<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1917 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1917</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1917</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-279410599</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.14507v1.pdf" target="_blank">Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training. While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional fine-tuning or specialized modules? We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert. Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average. This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning. By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios. Our code is available at https://github.com/oadamharoon/text2nav</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1917.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1917.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-BC-SigLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral cloning navigation policy using frozen SigLIP vision-language embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimalist, reactive navigation policy trained by behavioral cloning on joint image+text embeddings from a frozen SigLIP backbone; maps 1152-D joint embeddings to continuous wheel-velocity commands and is evaluated in simulated language-guided navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SigLIP + Behavioral Cloning Policy (frozen VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SigLIP image and text encoders project RGB images and language instructions into a shared 1152-dimensional latent space (sigmoid contrastive objective). A frozen SigLIP produces L2-normalized image and text features that are summed and re-normalized to form a joint embedding input to a small feedforward behavioral-cloning network that outputs continuous differential-drive wheel velocity commands (two-dimensional). The student policy is reactive (no explicit memory or mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs (SigLIP pretraining using a sigmoid contrastive objective)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>SigLIP variant referenced as So400M-Patch14-384 (pretrained on large-scale image-text pairs); such pretraining typically contains object descriptions and many natural image-text co-occurrences and therefore some implicit spatial relations (paper cites SigLIP but does not provide full pretraining corpus breakdown beyond the So400M identifier).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>language-guided navigation (semantic target navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Simulated Jetbot differential-drive navigation in NVIDIA Isaac Sim/Isaac Lab in a 3m × 3m arena with five colored 10cm spheres as potential goals. Agent receives egocentric RGB (256×256) and a natural-language instruction (color + relative spatial cue). Action space is continuous two-dimensional wheel velocity commands applied at 1/60 s; success is reaching the correct sphere within 0.1 m within up to 1000 timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper measures and discusses semantic alignment: SigLIP joint embeddings encode object semantics (color) and show measurable spatial sensitivity (distinguish same vs different grid cell placement). The language prompt includes relative spatial cues which increases alignment between language and perception embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Success rate: 74.0% (95% CI [65.4% - 82.6%]); average timesteps to reach goal (successful episodes): 369.4 timesteps (vs privileged expert 114.0). Behavioral cloning loss stabilized around ~0.06; cumulative rewards typically in range ≈ -8 to -13 indicating inefficiencies vs the expert.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct vision-only or randomly-initialized representation baseline reported. Privileged expert (state-aware, PPO trained with full state) achieves 100% success and average 114.0 timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Training data: ~500 expert trajectories (≈100 per target), averaging 5–8 seconds each (tens of thousands of state-action pairs). Paper reports the BC network 'converges quickly' but does not provide quantitative comparisons of sample complexity vs non-language baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map or saliency analysis reported for the VLM or student policy.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes — the paper ran a spatial sensitivity test on joint image-text embeddings: SigLIP showed a 26.5% cosine-distance separation between same-vs-different spatial grid cells (stronger spatial sensitivity than CLIP and ViLT), and the joint embedding was formed by L2-normalized image and text features summed and normalized.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect evidence: the policy grounds color-based language instructions to visual targets sufficiently often to achieve 74% success, demonstrating semantic grounding of object identity. However, there is no direct probing of verb/affordance grounding (e.g., mapping 'move' to a learned motor primitive); failures (inefficient paths, circling/timeouts) indicate weak grounding for temporal planning and exploration behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit multi-level (low-level vs high-level) feature decomposition or analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper identifies factors affecting transfer: (1) embedding dimensionality (higher dims correlated with better performance), (2) inherent spatial sensitivity of the VLM, and (3) prompt engineering (including relative spatial cues in language improved grounding). Transfer degrades for tasks requiring long-horizon planning or memory because the policy is reactive and the embeddings lack explicit spatial memory.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated — experiments use five colored spheres fixed to the environment (no explicit test of objects unseen in VLM pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No zero-shot/few-shot claims on embodied task — policy is trained with behavioral cloning from demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablation or probing of SigLIP layers; SigLIP weights are frozen throughout.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>The paper reports that a model fine-tuned for a non-embodied task (ViLT VQA) performed poorly on navigation, suggesting that task-specific fine-tuning (to VQA) can harm transfer to embodied navigation; no direct negative transfer quantified for SigLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct comparison to vision-only pretrained models (e.g., ImageNet-pretrained encoders) was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Behavioral cloning training dynamics: rapid initial convergence in BC loss followed by slower improvement and final stabilization (~0.06); policy performance still shows episodic variability during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Authors correlate embedding dimensionality with downstream performance: SigLIP (1152-d) outperformed ViLT (768-d) and CLIP (512-d); they argue richer representational capacity aids navigation despite higher BC loss for the higher-dimensional features.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1917.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-BC-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral cloning navigation policy using frozen CLIP (ViT-B/32) embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A behavioral cloning policy trained on frozen CLIP (ViT-B/32) joint image+text embeddings evaluated on the same simulated language-guided navigation task as the SigLIP-based policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (ViT-B/32) + Behavioral Cloning Policy (frozen VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CLIP provides image and text encoders with a 512-dimensional joint embedding space trained contrastively; the paper uses frozen CLIP encoders and sums L2-normalized image and text features to form the policy input, which is a feedforward BC network mapping to continuous wheel velocities.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs (contrastive softmax CLIP pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale internet image-text pairs typical of CLIP pretraining (paper does not enumerate corpus details), containing object descriptions and general semantics but paper does not document explicit affordance/action annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>language-guided navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same simulated Jetbot navigation task as other VLM-BC experiments: egocentric RGB + language instruction, continuous wheel-velocity outputs, five colored sphere targets in a 3m × 3m arena, evaluation in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Embedding spatial sensitivity lower than SigLIP (13.0% same/different grid cell cosine-distance difference), indicating weaker inherent encoding of spatial relations vs SigLIP; semantic object-color alignment present but less spatially sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Success rate: 62.0% (95% CI [52.5% - 71.5%]); average timesteps for successful episodes: 417.6 timesteps (worse efficiency than SigLIP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No non-VLM baseline; privileged expert achieves 100% success.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper uses the same demonstration dataset across VLMs; no per-model sample-efficiency curves or numeric comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention/saliency analysis reported for CLIP embeddings within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Spatial-sensitivity test performed: CLIP exhibited a 13.0% separation between same and different spatial positions in joint embeddings (less separation than SigLIP), which authors correlate with lower navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Only indirect via navigation success rate; no explicit probes linking CLIP embedding components to particular motor behaviors or affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Lower embedding dimensionality and lower spatial sensitivity correlated with poorer transfer to navigation. Prompting with relative spatial cues still required for reasonable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not tested.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported for CLIP specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>None provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>BC training loss for CLIP was lower than SigLIP, but lower imitation loss did not translate to superior navigation performance (authors note BC loss is not predictive of task performance).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>CLIP embeddings (512-d) yielded worse downstream performance than SigLIP (1152-d), consistent with an observed positive correlation between embedding dimensionality and navigation performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1917.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-BC-ViLT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral cloning navigation policy using frozen ViLT (VQA-finetuned) embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A behavioral cloning policy trained on frozen ViLT joint image+text embeddings (VQA-finetuned variant) evaluated on the same simulated navigation task; performed worst among tested VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViLT (VQA-finetuned) + Behavioral Cloning Policy (frozen VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ViLT is a vision-and-language transformer (no region supervision) producing joint embeddings (768-dimensional here for the VQA-finetuned variant). The model is used frozen; its embeddings feed a BC network mapping to continuous wheel velocities.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language (and further fine-tuned for VQA in the provided variant used)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining + VQA fine-tuning datasets (paper notes ViLT used was VQA-finetuned but does not detail datasets). The VQA fine-tuning appears to specialize ViLT for question-answering.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>language-guided navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same simulated Jetbot navigation task as other VLM-BC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper observes ViLT (VQA-finetuned) had weaker transfer — authors infer VQA fine-tuning specialized representations for discrete QA rather than continuous spatial navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Success rate: 40.0% (95% CI [30.4% - 49.6%]); average timesteps for successful episodes: 472.0 timesteps (worst efficiency among tested VLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>None provided; expert 100% success as privileged baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No quantitative per-model sample-efficiency analysis; same demonstration data used across models.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Spatial-sensitivity test showed ViLT had a 16.3% same/different spatial separation — better than CLIP in that test but downstream navigation performance was still poor, suggesting VQA fine-tuning altered features in ways detrimental to navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Only indirect via navigation success; no explicit grounding analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper suggests that task-specific fine-tuning on non-embodied datasets (VQA) can reduce transferability to embodied spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — ViLT fine-tuned for VQA performed substantially worse on embodied navigation, indicating negative transfer from non-embodied fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>BC training loss was lower for ViLT relative to SigLIP, but lower BC loss did not yield better navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>ViLT embedding dimension (768) lies between CLIP and SigLIP; despite moderate embedding dimensionality and spatial sensitivity, ViLT VQA-finetuning correlated with the worst navigation performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1917.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that fine-tunes vision-language models on robot action data to directly produce robot commands from visual and language inputs, demonstrating transfer of web-scale knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2 (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language-action model that fine-tunes pre-trained VLMs on robotic action datasets to map visual+language inputs to robot actions (cited as a contrast to the paper's frozen-VLM approach).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>initial vision-language pretraining followed by fine-tuning on action/robot datasets (per the referenced description in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; original RT-2 work uses web-scale image-text plus robot action datasets (outside scope of this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic control / language-conditioned robot actions</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Mentioned as systems that generate robot commands from vision + language, used in embodied settings (details are in the RT-2 original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper cites RT-2 as an approach that leverages web knowledge for robotic control, implying semantic alignment through fine-tuning on action data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Mentioned as fine-tuned on action data to produce commands — original RT-2 likely contains explicit action-grounding experiments (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1917.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robotic-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic-CLIP (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that fine-tunes CLIP on action data for robotic applications; cited here as an example of approaches that adapt VLMs rather than keeping them frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Robotic-CLIP (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A CLIP-based model fine-tuned on robot action datasets to support embodied tasks; referenced as a contrast to the frozen-VLM minimalist approach.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (CLIP) then fine-tuning on robot action data</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Original CLIP pretraining (internet image-text) plus robot action fine-tuning data (not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic applications (vision+language -> actions)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Mentioned in context of mapping VLMs to robotic control via fine-tuning on action data.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Implied that fine-tuning on action data improves alignment to robot tasks (per the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Referenced as using action data to ground visual-language features to robot actions (details in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1917.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP-Nav (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior approach that leverages CLIP embeddings combined with specialized navigation architectures to perform vision-and-language navigation, typically with mapping or spatial modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP-Nav (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines CLIP-based perception with navigation-specific modules (mapping, planning) rather than using frozen embeddings alone; cited as more complex contrast to the minimalist frozen-VLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for CLIP; downstream architecture integrates embeddings into navigation systems</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Embodied navigation tasks in which CLIP embeddings are used as perceptual inputs to mapping/planning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Used to leverage CLIP's semantic object understanding for navigation; spatial reasoning typically provided by additional modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1917.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLM (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that augments vision-language models with explicit spatial reasoning capabilities; cited as a different design choice relative to the frozen-VLM reactive policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLM (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A class of VLMs or methods that incorporate spatial reasoning mechanisms on top of vision-language representations to support embodied tasks requiring spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining with additional spatial reasoning augmentation (per referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>embodied spatial reasoning / navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Approaches that explicitly encode or learn spatial relationships for improved navigation/mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to improve spatial-semantic alignment relative to vanilla VLM embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1917.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLMaps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Language Maps (VLMaps) (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced approach that constructs spatial maps augmented with vision-language features for navigation and planning; contrasted with the paper's omission of mapping modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLMaps (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Systems that build spatial maps annotated with vision-language embeddings to support efficient planning and language-conditioned navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for the perceptual backbone; mapping modules built downstream</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>language-conditioned navigation with mapping</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Embodied navigation using explicit spatial memory/maps combined with language-conditioned semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Maps allow better alignment between semantic embeddings and spatial layout, addressing limitations identified in frozen-reactive policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1917.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZSON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ZSON (Zero-shot Object-Goal Navigation) (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that uses multimodal goal embeddings for zero-shot object-goal navigation; cited as related work in leveraging multimodal embeddings for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ZSON (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot approaches that use multimodal (image-text) goal embeddings to perform object-goal navigation without task-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on image-text pairs, applied in zero-shot manner to navigation</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>zero-shot object-goal navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Embodied navigation where a multimodal goal embedding specifies the target (no task-specific finetuning required).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to leverage alignment between vision and language for zero-shot transfer to navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Yes (zero-shot in the original ZSON work), but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1917.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R3M (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced representation-learning work (R3M) proposed for robot manipulation that aims to produce embodiment-aware representations for downstream robotic tasks; cited as a potential direction to improve embodiment alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R3M (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A universal visual representation for robot manipulation intended to be more embodiment-aware than generic VLMs; proposed as a complementary or alternative representation to generic VLMs for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>visual/robotic representation learning (paper does not specify exact pretraining in this document)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation / embodied representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Designed for manipulation tasks where embodiment-specific visual features and affordances are important.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Cited as more embodiment-aware and potentially better aligned to manipulation affordances than general VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1917.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1917.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenWorld-ObjManip</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Openworld object manipulation using pre-trained vision-language models (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced 2023 work demonstrating application of pre-trained vision-language models to open-world object manipulation; cited as an existing example of VLMs applied to embodied manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Openworld object manipulation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Work that applies pre-trained vision-language models to object manipulation in open-world settings; referenced to illustrate VLM usage beyond navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (pretrained VLMs used as perception/backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>object manipulation (open-world)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Embodied manipulation tasks leveraging pre-trained VLMs for object recognition/goal specification; details are in the referenced paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Implied that VLMs provide semantic object priors useful for manipulation; specifics not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Referenced as existing evidence that pre-trained VLMs can support manipulation, but no details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-2: Visionlanguage-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Robotic-CLIP: Fine-tuning CLIP on action data for robotic applications <em>(Rating: 2)</em></li>
                <li>Clip-nav: Using CLIP for zero-shot vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>SpatialVLM: Endowing vision-language models with spatial reasoning capabilities <em>(Rating: 2)</em></li>
                <li>Visual language maps for robot navigation <em>(Rating: 2)</em></li>
                <li>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings <em>(Rating: 2)</em></li>
                <li>R3M: A universal visual representation for robot manipulation <em>(Rating: 2)</em></li>
                <li>Openworld object manipulation using pre-trained visionlanguage models <em>(Rating: 2)</em></li>
                <li>VLM-Social-Nav: Socially aware robot navigation through scoring using vision-language models <em>(Rating: 1)</em></li>
                <li>NaVILA: Legged robot vision-language-action model for navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1917",
    "paper_id": "paper-279410599",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "VLM-BC-SigLIP",
            "name_full": "Behavioral cloning navigation policy using frozen SigLIP vision-language embeddings",
            "brief_description": "A minimalist, reactive navigation policy trained by behavioral cloning on joint image+text embeddings from a frozen SigLIP backbone; maps 1152-D joint embeddings to continuous wheel-velocity commands and is evaluated in simulated language-guided navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SigLIP + Behavioral Cloning Policy (frozen VLM)",
            "model_description": "SigLIP image and text encoders project RGB images and language instructions into a shared 1152-dimensional latent space (sigmoid contrastive objective). A frozen SigLIP produces L2-normalized image and text features that are summed and re-normalized to form a joint embedding input to a small feedforward behavioral-cloning network that outputs continuous differential-drive wheel velocity commands (two-dimensional). The student policy is reactive (no explicit memory or mapping).",
            "pretraining_type": "vision-language on image-text pairs (SigLIP pretraining using a sigmoid contrastive objective)",
            "pretraining_data_description": "SigLIP variant referenced as So400M-Patch14-384 (pretrained on large-scale image-text pairs); such pretraining typically contains object descriptions and many natural image-text co-occurrences and therefore some implicit spatial relations (paper cites SigLIP but does not provide full pretraining corpus breakdown beyond the So400M identifier).",
            "target_task_name": "language-guided navigation (semantic target navigation)",
            "target_task_description": "Simulated Jetbot differential-drive navigation in NVIDIA Isaac Sim/Isaac Lab in a 3m × 3m arena with five colored 10cm spheres as potential goals. Agent receives egocentric RGB (256×256) and a natural-language instruction (color + relative spatial cue). Action space is continuous two-dimensional wheel velocity commands applied at 1/60 s; success is reaching the correct sphere within 0.1 m within up to 1000 timesteps.",
            "semantic_alignment": "Paper measures and discusses semantic alignment: SigLIP joint embeddings encode object semantics (color) and show measurable spatial sensitivity (distinguish same vs different grid cell placement). The language prompt includes relative spatial cues which increases alignment between language and perception embeddings.",
            "performance_with_language_pretraining": "Success rate: 74.0% (95% CI [65.4% - 82.6%]); average timesteps to reach goal (successful episodes): 369.4 timesteps (vs privileged expert 114.0). Behavioral cloning loss stabilized around ~0.06; cumulative rewards typically in range ≈ -8 to -13 indicating inefficiencies vs the expert.",
            "performance_without_language_pretraining": "No direct vision-only or randomly-initialized representation baseline reported. Privileged expert (state-aware, PPO trained with full state) achieves 100% success and average 114.0 timesteps.",
            "sample_efficiency_comparison": "Training data: ~500 expert trajectories (≈100 per target), averaging 5–8 seconds each (tens of thousands of state-action pairs). Paper reports the BC network 'converges quickly' but does not provide quantitative comparisons of sample complexity vs non-language baselines.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention-map or saliency analysis reported for the VLM or student policy.",
            "embedding_space_analysis": "Yes — the paper ran a spatial sensitivity test on joint image-text embeddings: SigLIP showed a 26.5% cosine-distance separation between same-vs-different spatial grid cells (stronger spatial sensitivity than CLIP and ViLT), and the joint embedding was formed by L2-normalized image and text features summed and normalized.",
            "action_grounding_evidence": "Indirect evidence: the policy grounds color-based language instructions to visual targets sufficiently often to achieve 74% success, demonstrating semantic grounding of object identity. However, there is no direct probing of verb/affordance grounding (e.g., mapping 'move' to a learned motor primitive); failures (inefficient paths, circling/timeouts) indicate weak grounding for temporal planning and exploration behaviors.",
            "hierarchical_features_evidence": "No explicit multi-level (low-level vs high-level) feature decomposition or analysis reported.",
            "transfer_conditions": "Paper identifies factors affecting transfer: (1) embedding dimensionality (higher dims correlated with better performance), (2) inherent spatial sensitivity of the VLM, and (3) prompt engineering (including relative spatial cues in language improved grounding). Transfer degrades for tasks requiring long-horizon planning or memory because the policy is reactive and the embeddings lack explicit spatial memory.",
            "novel_vs_familiar_objects": "Not evaluated — experiments use five colored spheres fixed to the environment (no explicit test of objects unseen in VLM pretraining).",
            "zero_shot_or_few_shot": "No zero-shot/few-shot claims on embodied task — policy is trained with behavioral cloning from demonstrations.",
            "layer_analysis": "No layer-wise ablation or probing of SigLIP layers; SigLIP weights are frozen throughout.",
            "negative_transfer_evidence": "The paper reports that a model fine-tuned for a non-embodied task (ViLT VQA) performed poorly on navigation, suggesting that task-specific fine-tuning (to VQA) can harm transfer to embodied navigation; no direct negative transfer quantified for SigLIP.",
            "comparison_to_vision_only": "No direct comparison to vision-only pretrained models (e.g., ImageNet-pretrained encoders) was provided.",
            "temporal_dynamics": "Behavioral cloning training dynamics: rapid initial convergence in BC loss followed by slower improvement and final stabilization (~0.06); policy performance still shows episodic variability during evaluation.",
            "dimensionality_analysis": "Authors correlate embedding dimensionality with downstream performance: SigLIP (1152-d) outperformed ViLT (768-d) and CLIP (512-d); they argue richer representational capacity aids navigation despite higher BC loss for the higher-dimensional features.",
            "uuid": "e1917.0"
        },
        {
            "name_short": "VLM-BC-CLIP",
            "name_full": "Behavioral cloning navigation policy using frozen CLIP (ViT-B/32) embeddings",
            "brief_description": "A behavioral cloning policy trained on frozen CLIP (ViT-B/32) joint image+text embeddings evaluated on the same simulated language-guided navigation task as the SigLIP-based policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CLIP (ViT-B/32) + Behavioral Cloning Policy (frozen VLM)",
            "model_description": "CLIP provides image and text encoders with a 512-dimensional joint embedding space trained contrastively; the paper uses frozen CLIP encoders and sums L2-normalized image and text features to form the policy input, which is a feedforward BC network mapping to continuous wheel velocities.",
            "pretraining_type": "vision-language on image-text pairs (contrastive softmax CLIP pretraining)",
            "pretraining_data_description": "Large-scale internet image-text pairs typical of CLIP pretraining (paper does not enumerate corpus details), containing object descriptions and general semantics but paper does not document explicit affordance/action annotations.",
            "target_task_name": "language-guided navigation",
            "target_task_description": "Same simulated Jetbot navigation task as other VLM-BC experiments: egocentric RGB + language instruction, continuous wheel-velocity outputs, five colored sphere targets in a 3m × 3m arena, evaluation in simulation.",
            "semantic_alignment": "Embedding spatial sensitivity lower than SigLIP (13.0% same/different grid cell cosine-distance difference), indicating weaker inherent encoding of spatial relations vs SigLIP; semantic object-color alignment present but less spatially sensitive.",
            "performance_with_language_pretraining": "Success rate: 62.0% (95% CI [52.5% - 71.5%]); average timesteps for successful episodes: 417.6 timesteps (worse efficiency than SigLIP).",
            "performance_without_language_pretraining": "No non-VLM baseline; privileged expert achieves 100% success.",
            "sample_efficiency_comparison": "Paper uses the same demonstration dataset across VLMs; no per-model sample-efficiency curves or numeric comparisons are provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention/saliency analysis reported for CLIP embeddings within this work.",
            "embedding_space_analysis": "Spatial-sensitivity test performed: CLIP exhibited a 13.0% separation between same and different spatial positions in joint embeddings (less separation than SigLIP), which authors correlate with lower navigation performance.",
            "action_grounding_evidence": "Only indirect via navigation success rate; no explicit probes linking CLIP embedding components to particular motor behaviors or affordances.",
            "hierarchical_features_evidence": "None reported.",
            "transfer_conditions": "Lower embedding dimensionality and lower spatial sensitivity correlated with poorer transfer to navigation. Prompting with relative spatial cues still required for reasonable performance.",
            "novel_vs_familiar_objects": "Not tested.",
            "zero_shot_or_few_shot": "No.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Not reported for CLIP specifically.",
            "comparison_to_vision_only": "None provided.",
            "temporal_dynamics": "BC training loss for CLIP was lower than SigLIP, but lower imitation loss did not translate to superior navigation performance (authors note BC loss is not predictive of task performance).",
            "dimensionality_analysis": "CLIP embeddings (512-d) yielded worse downstream performance than SigLIP (1152-d), consistent with an observed positive correlation between embedding dimensionality and navigation performance.",
            "uuid": "e1917.1"
        },
        {
            "name_short": "VLM-BC-ViLT",
            "name_full": "Behavioral cloning navigation policy using frozen ViLT (VQA-finetuned) embeddings",
            "brief_description": "A behavioral cloning policy trained on frozen ViLT joint image+text embeddings (VQA-finetuned variant) evaluated on the same simulated navigation task; performed worst among tested VLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViLT (VQA-finetuned) + Behavioral Cloning Policy (frozen VLM)",
            "model_description": "ViLT is a vision-and-language transformer (no region supervision) producing joint embeddings (768-dimensional here for the VQA-finetuned variant). The model is used frozen; its embeddings feed a BC network mapping to continuous wheel velocities.",
            "pretraining_type": "vision-language (and further fine-tuned for VQA in the provided variant used)",
            "pretraining_data_description": "Pretraining + VQA fine-tuning datasets (paper notes ViLT used was VQA-finetuned but does not detail datasets). The VQA fine-tuning appears to specialize ViLT for question-answering.",
            "target_task_name": "language-guided navigation",
            "target_task_description": "Same simulated Jetbot navigation task as other VLM-BC experiments.",
            "semantic_alignment": "Paper observes ViLT (VQA-finetuned) had weaker transfer — authors infer VQA fine-tuning specialized representations for discrete QA rather than continuous spatial navigation.",
            "performance_with_language_pretraining": "Success rate: 40.0% (95% CI [30.4% - 49.6%]); average timesteps for successful episodes: 472.0 timesteps (worst efficiency among tested VLMs).",
            "performance_without_language_pretraining": "None provided; expert 100% success as privileged baseline.",
            "sample_efficiency_comparison": "No quantitative per-model sample-efficiency analysis; same demonstration data used across models.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None reported.",
            "embedding_space_analysis": "Spatial-sensitivity test showed ViLT had a 16.3% same/different spatial separation — better than CLIP in that test but downstream navigation performance was still poor, suggesting VQA fine-tuning altered features in ways detrimental to navigation.",
            "action_grounding_evidence": "Only indirect via navigation success; no explicit grounding analysis.",
            "hierarchical_features_evidence": "None.",
            "transfer_conditions": "Paper suggests that task-specific fine-tuning on non-embodied datasets (VQA) can reduce transferability to embodied spatial tasks.",
            "novel_vs_familiar_objects": "Not evaluated.",
            "zero_shot_or_few_shot": "No.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Yes — ViLT fine-tuned for VQA performed substantially worse on embodied navigation, indicating negative transfer from non-embodied fine-tuning.",
            "comparison_to_vision_only": "No.",
            "temporal_dynamics": "BC training loss was lower for ViLT relative to SigLIP, but lower BC loss did not yield better navigation performance.",
            "dimensionality_analysis": "ViLT embedding dimension (768) lies between CLIP and SigLIP; despite moderate embedding dimensionality and spatial sensitivity, ViLT VQA-finetuning correlated with the worst navigation performance.",
            "uuid": "e1917.2"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (referenced)",
            "brief_description": "Referenced prior work that fine-tunes vision-language models on robot action data to directly produce robot commands from visual and language inputs, demonstrating transfer of web-scale knowledge to robotic control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RT-2 (mentioned)",
            "model_description": "A vision-language-action model that fine-tunes pre-trained VLMs on robotic action datasets to map visual+language inputs to robot actions (cited as a contrast to the paper's frozen-VLM approach).",
            "pretraining_type": "initial vision-language pretraining followed by fine-tuning on action/robot datasets (per the referenced description in the paper)",
            "pretraining_data_description": "Not specified in this paper; original RT-2 work uses web-scale image-text plus robot action datasets (outside scope of this paper).",
            "target_task_name": "robotic control / language-conditioned robot actions",
            "target_task_description": "Mentioned as systems that generate robot commands from vision + language, used in embodied settings (details are in the RT-2 original paper).",
            "semantic_alignment": "Paper cites RT-2 as an approach that leverages web knowledge for robotic control, implying semantic alignment through fine-tuning on action data.",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "Mentioned as fine-tuned on action data to produce commands — original RT-2 likely contains explicit action-grounding experiments (not detailed here).",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.3"
        },
        {
            "name_short": "Robotic-CLIP",
            "name_full": "Robotic-CLIP (referenced)",
            "brief_description": "Referenced work that fine-tunes CLIP on action data for robotic applications; cited here as an example of approaches that adapt VLMs rather than keeping them frozen.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Robotic-CLIP (mentioned)",
            "model_description": "A CLIP-based model fine-tuned on robot action datasets to support embodied tasks; referenced as a contrast to the frozen-VLM minimalist approach.",
            "pretraining_type": "vision-language pretraining (CLIP) then fine-tuning on robot action data",
            "pretraining_data_description": "Original CLIP pretraining (internet image-text) plus robot action fine-tuning data (not specified in this paper).",
            "target_task_name": "robotic applications (vision+language -&gt; actions)",
            "target_task_description": "Mentioned in context of mapping VLMs to robotic control via fine-tuning on action data.",
            "semantic_alignment": "Implied that fine-tuning on action data improves alignment to robot tasks (per the referenced work).",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "Referenced as using action data to ground visual-language features to robot actions (details in original paper).",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.4"
        },
        {
            "name_short": "CLIP-Nav",
            "name_full": "CLIP-Nav (referenced)",
            "brief_description": "Referenced prior approach that leverages CLIP embeddings combined with specialized navigation architectures to perform vision-and-language navigation, typically with mapping or spatial modules.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CLIP-Nav (mentioned)",
            "model_description": "Combines CLIP-based perception with navigation-specific modules (mapping, planning) rather than using frozen embeddings alone; cited as more complex contrast to the minimalist frozen-VLM pipeline.",
            "pretraining_type": "vision-language pretraining for CLIP; downstream architecture integrates embeddings into navigation systems",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "vision-and-language navigation",
            "target_task_description": "Embodied navigation tasks in which CLIP embeddings are used as perceptual inputs to mapping/planning pipelines.",
            "semantic_alignment": "Used to leverage CLIP's semantic object understanding for navigation; spatial reasoning typically provided by additional modules.",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.5"
        },
        {
            "name_short": "SpatialVLM",
            "name_full": "SpatialVLM (referenced)",
            "brief_description": "Referenced work that augments vision-language models with explicit spatial reasoning capabilities; cited as a different design choice relative to the frozen-VLM reactive policy.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SpatialVLM (mentioned)",
            "model_description": "A class of VLMs or methods that incorporate spatial reasoning mechanisms on top of vision-language representations to support embodied tasks requiring spatial understanding.",
            "pretraining_type": "vision-language pretraining with additional spatial reasoning augmentation (per referenced work)",
            "pretraining_data_description": "Not detailed in this paper.",
            "target_task_name": "embodied spatial reasoning / navigation",
            "target_task_description": "Approaches that explicitly encode or learn spatial relationships for improved navigation/mapping.",
            "semantic_alignment": "Designed to improve spatial-semantic alignment relative to vanilla VLM embeddings.",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.6"
        },
        {
            "name_short": "VLMaps",
            "name_full": "Visual Language Maps (VLMaps) (referenced)",
            "brief_description": "Referenced approach that constructs spatial maps augmented with vision-language features for navigation and planning; contrasted with the paper's omission of mapping modules.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "VLMaps (mentioned)",
            "model_description": "Systems that build spatial maps annotated with vision-language embeddings to support efficient planning and language-conditioned navigation.",
            "pretraining_type": "vision-language pretraining for the perceptual backbone; mapping modules built downstream",
            "pretraining_data_description": "Not provided in this paper.",
            "target_task_name": "language-conditioned navigation with mapping",
            "target_task_description": "Embodied navigation using explicit spatial memory/maps combined with language-conditioned semantics.",
            "semantic_alignment": "Maps allow better alignment between semantic embeddings and spatial layout, addressing limitations identified in frozen-reactive policies.",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.7"
        },
        {
            "name_short": "ZSON",
            "name_full": "ZSON (Zero-shot Object-Goal Navigation) (referenced)",
            "brief_description": "Referenced prior work that uses multimodal goal embeddings for zero-shot object-goal navigation; cited as related work in leveraging multimodal embeddings for embodied tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ZSON (mentioned)",
            "model_description": "Zero-shot approaches that use multimodal (image-text) goal embeddings to perform object-goal navigation without task-specific training.",
            "pretraining_type": "vision-language pretraining on image-text pairs, applied in zero-shot manner to navigation",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "zero-shot object-goal navigation",
            "target_task_description": "Embodied navigation where a multimodal goal embedding specifies the target (no task-specific finetuning required).",
            "semantic_alignment": "Designed to leverage alignment between vision and language for zero-shot transfer to navigation.",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "Yes (zero-shot in the original ZSON work), but not evaluated in this paper.",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.8"
        },
        {
            "name_short": "R3M",
            "name_full": "R3M (referenced)",
            "brief_description": "Referenced representation-learning work (R3M) proposed for robot manipulation that aims to produce embodiment-aware representations for downstream robotic tasks; cited as a potential direction to improve embodiment alignment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "R3M (mentioned)",
            "model_description": "A universal visual representation for robot manipulation intended to be more embodiment-aware than generic VLMs; proposed as a complementary or alternative representation to generic VLMs for embodied tasks.",
            "pretraining_type": "visual/robotic representation learning (paper does not specify exact pretraining in this document)",
            "pretraining_data_description": "Not detailed in this paper.",
            "target_task_name": "robotic manipulation / embodied representation learning",
            "target_task_description": "Designed for manipulation tasks where embodiment-specific visual features and affordances are important.",
            "semantic_alignment": "Cited as more embodiment-aware and potentially better aligned to manipulation affordances than general VLMs.",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.9"
        },
        {
            "name_short": "OpenWorld-ObjManip",
            "name_full": "Openworld object manipulation using pre-trained vision-language models (referenced)",
            "brief_description": "Referenced 2023 work demonstrating application of pre-trained vision-language models to open-world object manipulation; cited as an existing example of VLMs applied to embodied manipulation tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Openworld object manipulation (mentioned)",
            "model_description": "Work that applies pre-trained vision-language models to object manipulation in open-world settings; referenced to illustrate VLM usage beyond navigation.",
            "pretraining_type": "vision-language pretraining (pretrained VLMs used as perception/backbone)",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "object manipulation (open-world)",
            "target_task_description": "Embodied manipulation tasks leveraging pre-trained VLMs for object recognition/goal specification; details are in the referenced paper.",
            "semantic_alignment": "Implied that VLMs provide semantic object priors useful for manipulation; specifics not discussed here.",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "Referenced as existing evidence that pre-trained VLMs can support manipulation, but no details provided in this paper.",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1917.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-2: Visionlanguage-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "Robotic-CLIP: Fine-tuning CLIP on action data for robotic applications",
            "rating": 2
        },
        {
            "paper_title": "Clip-nav: Using CLIP for zero-shot vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "SpatialVLM: Endowing vision-language models with spatial reasoning capabilities",
            "rating": 2
        },
        {
            "paper_title": "Visual language maps for robot navigation",
            "rating": 2
        },
        {
            "paper_title": "ZSON: Zero-shot object-goal navigation using multimodal goal embeddings",
            "rating": 2
        },
        {
            "paper_title": "R3M: A universal visual representation for robot manipulation",
            "rating": 2
        },
        {
            "paper_title": "Openworld object manipulation using pre-trained visionlanguage models",
            "rating": 2
        },
        {
            "paper_title": "VLM-Social-Nav: Socially aware robot navigation through scoring using vision-language models",
            "rating": 1
        },
        {
            "paper_title": "NaVILA: Legged robot vision-language-action model for navigation",
            "rating": 1
        }
    ],
    "cost": 0.0231325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?
17 Jun 2025</p>
<p>Nitesh Subedi 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Adam Haroon 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Shreyan Ganguly 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Samuel T K Tetteh 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Prajwal Koirala 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Cody Fleming 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Soumik Sarkar 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?
17 Jun 202533DDBF4F1B36C2CBE77673BD304488EEarXiv:2506.14507v1[cs.RO]Accepted to Robotics: Science and Systems (RSS) 2025 Workshop on RobotVision-Language ModelsBehavior CloningLanguage-Guided NavigationFrozen EmbeddingsFoundation ModelsRobot Learning
Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training.While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional finetuning or specialized modules?We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert.Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average.This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning.By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios.Our code is available at https://github.com/oadamharoon/text2nav</p>
<p>I. INTRODUCTION</p>
<p>The ability to follow natural language instructions is crucial for robots to operate effectively in human environments.Instructions like "go to the red ball" simultaneously specify both a goal object and the desired action, creating a flexible interface between humans and robots [14].However, bridging the gap between language understanding and physical action presents significant challenges, as the rich semantic knowledge embedded in natural language far exceeds what robots can learn from limited task-specific experience [22].</p>
<p>Traditional approaches to language-guided robot navigation typically require extensive task-specific training data and specialized architectures to ground language in visual observations.These approaches often suffer from limited generalization to novel commands or environments, as they can only recognize objects and follow instructions within predefined categories [18].The emergence of large-scale vision-language models (VLMs) like CLIP [18] and SigLIP [23] has opened new possibilities for robotics by providing rich semantic representations trained on diverse internet-scale data.</p>
<p>Recent work has demonstrated the benefits of integrating these foundation models into robotic systems.For example, approaches like RT-2 [24] and Robotic-CLIP [16] fine-tune VLMs on action data to directly generate robot commands from visual and language inputs.Other systems like CLIP-Nav [4] and VLM-Social-Nav [21] combine VLMs with specialized navigation architectures such as mapping modules or spatial awareness components.While these approaches show impressive capabilities, they often involve complex architectures, fine-tuning procedures, or additional perception modules tailored specifically for navigation.</p>
<p>This complexity raises a fundamental question for robotics researchers: are the rich semantic representations from pretrained VLMs already sufficient for basic navigation tasks, or are specialized architectures and fine-tuning essential?Answering this question is critical for several reasons:</p>
<p>• Resource efficiency: Complex architectures and fine-tuning require substantial computational resources that may be unnecessary if simpler approaches can achieve acceptable performance • Transfer learning: Understanding the intrinsic capabilities of frozen embeddings provides insights into their transferability across different embodied tasks • System design: Knowing the limitations of pretrained embeddings can guide more informed decisions about when to use specialized modules versus leveraging general-purpose representations</p>
<p>To address this question, we propose a minimalist approach that decouples the perception component (a Fig. 1: Training and deployment pipeline.A privileged expert policy π β collects demonstrations in simulation using full state observations.These are converted into joint vision-language embeddings using a pretrained VLM, forming the dataset D ′ .A policy π is then trained via behavioral cloning.At deployment, π receives only partial observations (camera image and instruction), which are encoded by the same VLM to select actions.frozen VLM) from the policy learning process.Rather than designing a complex end-to-end architecture, we investigate whether a simple behavior cloning policy trained on VLM embeddings can successfully navigate to language-specified targets.Our framework consists of two phases (Fig. 1): (1) training a privileged expert with full state access, and (2) distilling this expert's knowledge into a policy that operates solely on frozen vision-language embeddings.This minimalist approach differs fundamentally from previous work in three key technical aspects.First, unlike RT-2 [24] or Robotic-CLIP [16], we do not fine-tune the VLM on task-specific data, avoiding the computational cost and data requirements of adaptation.Second, unlike SpatialVLM [2], we do not augment the model with specialized spatial reasoning capabilities.Third, in contrast to approaches like VLMaps [7] or ZSON [12], we intentionally omit mapping or exploration modules that would provide spatial memory.Instead, we treat the VLM strictly as a frozen feature extractor, learning a reactive policy that must implicitly encode all navigationrelevant information from single-frame embeddings.</p>
<p>Our empirical evaluation reveals both the promise and limitations of this approach.The policy achieves a 74% success rate in navigating to language-specified targets, demonstrating that pretrained embeddings can indeed support basic grounding of language to visual targets.However, the policy takes significantly longer paths (3.2× on average) compared to the expert, indicating limitations in spatial reasoning, planning, and memory.These findings provide valuable insights for re- searchers seeking to balance simplicity and performance in language-guided robot navigation systems.</p>
<p>Our contributions include:</p>
<p>• A minimalist framework for language-guided navigation that uses frozen vision-language embeddings as the sole representation for policy learning • An empirical evaluation demonstrating that pretrained embeddings alone can achieve a 74% success rate in semantic target navigation • Analysis of the performance gap between our VLMguided policy and the privileged expert, revealing the strengths and limitations of pretrained embeddings for navigation • Insights into future directions for effectively leveraging foundation models in embodied robotics</p>
<p>II. METHODOLOGY</p>
<p>Our approach consists of two phases: (1) an expert demonstration phase using privileged information, and (2) a learning phase where a policy is trained on the expert data using vision-language representations.After training, the learned policy is deployed using only its onboard sensors and the language instruction.</p>
<p>A. Phase 1: Privileged Expert Policy</p>
<p>In Phase 1, we develop an expert behavioral policy π β (s, g) that can reliably navigate to a goal, where s denotes the full state and g denotes the goal specification.We represent g as a one-hot indicator of the target sphere's identity (among five possibilities) along with its position in the environment.The expert knows both which object is the target and its precise location, making navigation a straightforward control problem.We implement π β using Proximal Policy Optimization (PPO) [20].</p>
<p>The expert policy effectively functions as a perfect GPS-based navigator.In our preliminary evaluations, it achieves a 100% success rate, consistently reaching the correct sphere and stopping within 0.1m of the target.Importantly, π β receives the target identity directly rather than processing language instructions-this is acceptable since its role is solely to generate demonstrations, not to be deployed at test time.</p>
<p>Using π β , we collect a dataset of navigation trajectories.Each trajectory begins with the robot at a random location and orientation, with one of the five colored spheres randomly chosen as the goal.We generate natural language commands that include both semantic color identification and spatial positioning relative to the robot's current orientation (e.g., "The target is the red ball which is to your left.Move toward the ball."or "The target is the blue ball which is straight ahead.Move toward the ball.")for each target.These relative spatial cues are determined based on the angular position of the target relative to the robot's heading.The expert controls the robot until it reaches the goal, and we record:</p>
<p>• Camera images from the robot's perspective • Language instructions for the target • Expert actions (wheel velocity commands)</p>
<p>• Reward signals based on distance to goal The dataset D consists of approximately 500 trajectories (100 per target object), averaging 5-8 seconds of navigation each, resulting in tens of thousands of stateaction pairs for training.</p>
<p>B. Vision-Language Model Selection</p>
<p>To guide our choice of vision-language model, we first assessed whether pretrained VLMs encode not only semantic object information but also spatial relationships relevant to navigation tasks.We conducted an empirical comparison of three prominent models: ViLT (VQA-finetuned) [9], CLIP (ViT-B/32) [18], and SigLIP (So400M-Patch14-384) [1].</p>
<p>We designed a spatial distance test based on joint embeddings of images and language prompts.For each reference image containing a colored sphere, we generated a natural language instruction.Each 256×256 image was divided into an even 3×3 spatial grid, creating nine distinct regions for localizing objects.For each reference image, we selected two additional images containing the same colored object-one where the object appeared in the same spatial grid cell (same spatial semantics), and one where it appeared in a different cell (different spatial semantics).We controlled for background and lighting conditions to isolate the effect of spatial positioning.</p>
<p>We computed the cosine distance between the joint image-text embeddings of the reference image and each comparison image across x different configurations.Lower cosine distance indicates greater semantic and spatial similarity.If a vision-language model encodes spatial semantics, we expect lower distances when spatial Get expert action a t = π β (s t , g)
Image embedding v t = L2Norm(VLM img (I t )) 18: Text embedding u t = L2Norm(VLM text (l)) 19: Joint embedding: o t = L2Norm(v t + u t ) 20:
Add (o t , a t ) to D ′ 21: end for 22: Train policy π to minimize loss:
L(θ) = E (o,a)∼D ′ ∥π θ (o) − a∥ 2
23: return trained policy π positioning is consistent, even when the same object and instruction are present in both images.</p>
<p>As shown in Table I, all three models exhibited sensitivity to spatial positioning, with lower cosine distances for image pairs sharing the same spatial cell.Notably, SigLIP demonstrated the most pronounced distinction between same and different spatial contexts (26.5% difference), suggesting stronger spatial sensitivity in its joint embedding space compared to CLIP (13.0%difference) and ViLT (16.3% difference).</p>
<p>While this test does not fully assess complex spatial reasoning capabilities required for navigation planning, it confirms that pretrained VLMs inherently encode some degree of spatial information in their joint embeddings without explicit spatial training.This finding, combined with SigLIP's efficient architecture and strong performance on downstream tasks with fewer parameters [1], led us to select it as our vision-language backbone.</p>
<p>C. Vision-Language Embedding with SigLIP</p>
<p>Based on our comparative analysis, we leverage SigLIP [23] to bridge the gap between high-dimensional sensory inputs and the learning algorithm.SigLIP consists of an image encoder and a text encoder that project images and text into a shared 1152-dimensional latent space where semantically related concepts have high cosine similarity.The model's sigmoid-based contrastive loss function offers improved stability compared to CLIP's [18] softmax-based approach, making it particularly suitable for robotic applications.</p>
<p>For each timestep in our dataset, we compute a joint embedding that fuses visual and language information:</p>
<p>1) The image I and instruction L are processed through SigLIP's pretrained encoders 2) The resulting features are L2-normalized 3) The normalized features are summed and normalized again to produce a joint embedding vector o This vector o serves as a compact representation of both what the robot sees, what it has been instructed to do, and the spatial relationship between the robot and target.The inclusion of relative spatial descriptors in the language instructions enables the joint embedding to capture both semantic object identity and directional guidance.Critically, the SigLIP model remains completely frozen throughout our pipeline-we make no updates to its weights during training.</p>
<p>D. Behavioral Cloning Policy Learning</p>
<p>With the processed dataset D ′ = {(o t , a t )}, we train a policy π(a|o) via behavioral cloning [11].The policy is implemented as a feedforward neural network that takes the joint embedding vector o as input and outputs a two-dimensional action a = (ω 1 , ω 2 ) representing normalized wheel velocity commands.</p>
<p>We train the policy to minimize the mean squared error between its predictions and the expert's actions: During evaluation, the robot must navigate to a specific colored sphere based on language instructions that include both semantic color identification and relative spatial cues using only RGB camera input and pretrained vision-language embeddings.
L(θ) = E (o,a)∼D ′ ∥π θ (o) − a∥ 2
We optimize this loss using Adam with an initial learning rate of 1 × 10 −3 , decaying over time.The network converges quickly due to the large quantity of near-optimal demonstrations.</p>
<p>An important consideration is that the expert's actions are conditioned on privileged information that the student policy lacks.The student policy must learn behaviors like "turn towards the red object when seeing it" and "explore when the target is not visible" purely from the demonstration data, without ever receiving explicit state information.At deployment, the policy takes as input the current camera image and language instruction, processes them through SigLIP to obtain the joint embedding o, and outputs action commands in a closed-loop manner.</p>
<p>III. EXPERIMENTAL SETUP a) Simulation Environment:</p>
<p>We conduct all experiments in NVIDIA Isaac Sim [17] and NVIDIA Isaac Lab [13], a high-fidelity robotics simulation framework.Our code is available at https://github.com/oadamharoon/text2nav. The environment consists of a flat 3m × 3m arena with five colored spheres (red, green, blue, yellow, and pink) of 10cm diameter positioned at fixed locations for each episode.The robot starts at a random location and orientation.No obstacles are present, focusing the challenge on identifying the correct target and navigating efficiently toward it among visually similar distractors.</p>
<p>b) Robot Platform: The agent is modeled after the NVIDIA Jetbot, a small differential-drive robot with a forward-facing RGB camera.The camera provides a first-person view at 256×256 resolution.The robot's action space consists of two-dimensional angular velocity commands that directly control the wheels.Each action is applied for a short timestep (1/60 seconds) before the next observation is captured.c) Task Definition: At the start of each episode, one of the five colored balls is randomly chosen as the target.A natural language instruction is generated that includes both the target color and its relative spatial position based on the angular offset between the robot's heading and target location: "The target is the [color] ball which is to your [left/right/straight ahead].Move toward the ball."The robot must navigate to the correct target using only its egocentric RGB images and the language instruction.Success is defined as reaching within a small distance (0.1m) of the correct target within a maximum of 1000 timesteps.</p>
<p>d) Evaluation Protocol: We evaluate both the expert policy π β and the learned policy π on 100 test episodes with randomized starting positions.For each episode, we record:</p>
<p>• Success rate (reaching the correct target) • Number of timesteps to completion • Cumulative reward • Trajectory efficiency (compared to optimal paths) IV.RESULTS Our evaluation reveals a substantial performance gap between the expert policy with privileged state access and the vision-language embedding-based policy (Table II).The expert policy π β achieves a perfect 100% success rate, while the learned policy π succeeds in 74% of test episodes.The learned policy π using only vision-language embeddings achieves 74% success but requires significantly more timesteps than the privileged expert π β , even in successful cases.This highlights both the potential and limitations of using pretrained embeddings alone for navigation.</p>
<p>a) Success Rate Analysis: The 74% success rate of the vision-language policy is notable, as it demonstrates that pretrained embeddings alone can guide a robot to the correct target in a majority of cases without any architecture specifically designed for navigation or spatial reasoning.The policy correctly grounds language instructions in visual observations and generates appropriate actions to reach the target-all through the lens of a frozen vision-language model and a simple feedforward network.</p>
<p>b) Efficiency Gap: Despite its reasonable success rate, the learned policy exhibits significantly lower efficiency compared to the expert.When successful, π takes on average 369.4 timesteps to reach the goal-3.2×more than the expert's average of 114.0 timesteps.Even the best-case performance of π (216 timesteps) is approximately triple the expert's minimum (73 timesteps), while the worst case requires 828 timesteps compared to the expert's maximum of 154.</p>
<p>c) Reward and Consistency: The reward comparison (Fig. 4) further illustrates the performance gap.The learned policy's reward curve shows considerable episode-to-episode variation, with rewards ranging from approximately -8 to -13, indicating inconsistent performance across different starting configurations.In contrast, the expert maintains stable, high rewards across all test episodes.This variability suggests that the VLMbased policy struggles more with certain spatial arrangements or viewing angles than others.</p>
<p>d) Learning Dynamics: The behavioral cloning loss for SigLIP (Fig. 6) shows rapid initial convergence followed by more gradual improvement, eventually stabilizing around 0.06.This suggests that while the policy learns to approximate the expert's actions in many situations, there remains a persistent gap-likely corresponding to scenarios where the expert relied on privileged information not captured in the visual-linguistic embeddings.</p>
<p>A. Baseline Evaluation: Multi-Model Comparison</p>
<p>74.0%</p>
<p>Fig. 5: Multi-model performance comparison.Success rates across three vision-language models using identical BC training procedures (n=100 episodes each).SigLIP demonstrates superior performance, achieving 74.0% success rate compared to CLIP's 62.0% and ViLT's 40.0%.impact of different vision-language architectures, using identical training procedures and evaluation protocols across all three candidate models.We trained separate behavioral cloning policies using embeddings from ViLT (VQA-finetuned), CLIP (ViT-B/32), and SigLIP, evaluating each on 100 episodes for fair statistical comparison.</p>
<p>The results reveal a clear performance hierarchy: SigLIP (74.0%success) &gt; CLIP (62.0%) &gt; ViLT (40.0%), as shown in Figure 5. Remarkably, SigLIP achieved both the highest success rate and superior efficiency, requiring an average of only 369.4 timesteps for successful episodes compared to CLIP's 417.6 and ViLT's 472.0 timesteps.This dual superiority in both success and efficiency strongly validates our methodological choice of SigLIP as the vision-language backbone.</p>
<p>a) Embedding Dimensionality and Performance: The performance ranking aligns closely with embedding dimensionality: SigLIP's 1152-dimensional representations significantly outperform ViLT's 768-dimensional and CLIP's 512-dimensional embeddings.This suggests that richer representational capacity translates directly to better navigation performance, even when using frozen embeddings without task-specific adaptation.</p>
<p>b) BC Loss vs. Navigation Performance: Interestingly, the navigation performance ranking inversely correlates with behavioral cloning loss (Fig. 6).ViLT and CLIP achieved lower BC training losses, while SigLIP exhibited higher BC loss but superior actual navigation performance.This confirms our earlier hypothesis that BC loss measures imitation fidelity rather than task performance-SigLIP's richer 1152-dimensional embeddings are harder for the BC network to compress into action predictions, but they contain more navigationrelevant information that enables better spatial reasoning.c) Spatial Sensitivity Validation: These results validate our spatial sensitivity analysis from Section II-B, where SigLIP demonstrated the most pronounced spatial distinction (26.5% cosine distance difference between same/different spatial positions) compared to CLIP (13.0%) and ViLT (16.3%).The navigation performance hierarchy directly mirrors the spatial sensitivity ranking, confirming that inherent spatial understanding in visionlanguage embeddings translates to practical navigation capabilities.</p>
<p>d) Statistical Significance: The 95% confidence intervals reveal statistically significant differences between models: SigLIP (74.0%[65.4% -82.6%]),CLIP (62.0%[52.5% -71.5%]), and ViLT (40.0%[30.4% -49.6%]).The non-overlapping confidence intervals between SigLIP and ViLT, and minimal overlap between SigLIP and CLIP, demonstrate that the performance differences are statistically robust.</p>
<p>e) VQA Fine-tuning Limitations: ViLT's poor performance (40.0%success), despite being fine-tuned for visual question answering, suggests that task-specific fine-tuning on non-embodied datasets may not transfer effectively to robotic navigation.The VQA fine-tuning appears to have specialized ViLT for discrete questionanswering rather than the continuous spatial reasoning required for navigation, highlighting the importance of representation generality for embodied tasks.</p>
<p>These findings provide several key insights for using VLMs in navigation: (1) larger embedding dimensions generally improve performance when sufficient training data is available, (2) models with stronger inherent spatial sensitivity achieve better navigation results, (3) BC loss is not predictive of downstream task performance, and (4) general-purpose vision-language models may outperform task-specific fine-tuned variants for embodied applications requiring spatial reasoning.</p>
<p>V. DISCUSSION AND CONCLUSION</p>
<p>Our work addresses a fundamental question in embodied AI: can pretrained vision-language embeddings alone serve as a sufficient representation for navigation policies?The results provide a nuanced answer: while these embeddings enable basic language-guided navigation (74% success rate), they show clear limitations in efficiency and consistency compared to stateaware approaches.This performance gap offers valuable insights into both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks.SigLIP achieves superior navigation performance despite higher BC loss than ViLT and CLIP, suggesting that BC loss does not predict downstream task performance for embodied applications.</p>
<p>A. Semantic Grounding vs. Spatial Reasoning</p>
<p>Our results demonstrate that pretrained visionlanguage embeddings excel at semantic grounding-connecting language descriptions to visual observations.The policy successfully differentiates between colored targets based solely on the joint embeddings of images and instructions.This capability aligns with the core strength of models like SigLIP [23] and CLIP [18], which are trained specifically to align visual and linguistic concepts.</p>
<p>However, the efficiency gap reveals that these embeddings struggle with more complex spatial reasoning and planning.While our VLM selection study showed that SigLIP encodes basic spatial sensitivity (distinguishing same vs.different object positions), this level of spatial understanding appears insufficient for efficient navigation.The 3.2× longer paths taken by our policy suggest that while it can identify targets, it lacks the temporal memory and higher-order spatial reasoning needed for direct navigation.Unlike specialized approaches that incorporate mapping modules [7] or explicit spatial reasoning components [2], our minimalist policy must infer spatial relationships purely from a sequence of independent embeddings without explicit spatial representation.</p>
<p>This distinction between basic spatial sensitivity and complex spatial reasoning highlights an important consideration for robotics researchers: while foundation models offer powerful semantic representations and some inherent spatial understanding out-of-the-box, they may need to be complemented with dedicated spatial reasoning mechanisms for tasks requiring efficient path planning and exploration.Recent work in latent space planning [5,19] offers promising directions for bridging this gap by learning structured representations that support both semantic understanding and planning.</p>
<p>B. Prompt Engineering Sensitivity</p>
<p>An important consideration in our approach is the sensitivity to prompt design.Our initial experiments with simple color-only instructions (e.g., "Go to the red ball") yielded significantly lower performance.The inclusion of relative spatial cues ("The target is red ball which is to your left.Move toward the ball.")proved crucial for enabling the VLM embeddings to ground both semantic and spatial information effectively.This highlights the importance of prompt engineering when leveraging foundation models for embodied tasks, as the linguistic framing directly impacts the model's ability to extract navigation-relevant features from the joint embedding space.</p>
<p>C. Performance-Complexity Tradeoffs</p>
<p>Our minimalist approach highlights an important tradeoff in robotics: simplicity versus performance.More complex approaches like RT-2 [24] or NaVILA [3] can achieve higher performance through fine-tuning or specialized architectures but require substantial computational resources and engineering effort.In contrast, our approach using frozen embeddings and simple behavioral cloning demonstrates that reasonable performance can be achieved with minimal complexity.</p>
<p>This tradeoff is particularly relevant for resourceconstrained applications where training compute or specialized hardware may be limited.Our results suggest that for basic navigation tasks with clear visual targets, pretrained embeddings alone may provide sufficient capabilities without the need for extensive fine-tuning or complex architectures.However, for tasks requiring efficient navigation or complex reasoning, the additional complexity of specialized approaches may be justified.</p>
<p>Recent work in student-teacher distillation [10] and zero-shot policy transfer [8] offers promising directions for better balancing this tradeoff.By more effectively distilling the privileged expert's knowledge into a realizable student policy, it may be possible to achieve performance closer to the expert while maintaining the simplicity of our approach.</p>
<p>D. Implications for Future Research</p>
<p>Our findings suggest several promising directions for future research at the intersection of foundation models and robotics:</p>
<p>• Hybrid architectures: To address the repeated circling and target confusion failures we observed, combining the semantic richness of pretrained embeddings with explicit spatial memory could enable the policy to maintain consistent object identification across viewpoints.World models [6] offer a promising framework for integrating perception with spatial reasoning.• Task-specific representation learning: The confusion between visually similar targets suggests that generic vision-language embeddings may not optimally encode the distinctions most relevant for navigation.Approaches like R3M [15] could provide embodiment-aware representations that better capture navigation-relevant features while building on the spatial sensitivity already present in models like SigLIP.• Data-efficient adaptation: The timeout failures indicate that frozen embeddings struggle with systematic exploration.Lightweight adaptation techniques could align pretrained representations with navigation-specific requirements without the full computational cost of fine-tuning.</p>
<p>E. Conclusion</p>
<p>Our work provides an important empirical baseline for understanding the capabilities and limitations of pretrained vision-language embeddings in embodied navigation tasks.By demonstrating that these embeddings alone can achieve a 74% success rate in following language instructions, we highlight their potential as lightweight representations for robotics.However, the significant efficiency gap compared to privileged experts reveals the need for additional inductive biases or specialized components to achieve expert-level performance.</p>
<p>This minimalist approach provides robotics researchers with a practical baseline for evaluating when to use foundation models as-is, versus when to invest in specialized components.As foundation models continue to advance in capability and efficiency, understanding their intrinsic strengths and limitations becomes increasingly important for effective system design.By isolating and quantifying the specific contribution of vision-language embeddings to navigation performance, our work enables more informed architectural decisions at the intersection of foundation models and embodied robotics.</p>
<p>VI. ACKNOWLEDGMENT</p>
<p>This work is funded from NSF-USDA COALESCE grant #2021-67021-34418.</p>
<p>Fig. 2 :
2
Fig. 2: Expert policy training reward.Reward plot for the expert policy π β , which quickly learns to successfully navigate to specified targets using privileged state information.</p>
<p>Algorithm 1 3 :
13
Training Language-Conditioned Navigation Policy via Behavioral Cloning Require: Simulator S, expert policy π β , pretrained VLM (SigLIP), number of episodes N Ensure: Learned policy π that maps VLM joint embeddings to robot actions 1: Initialize dataset D ← ∅ 2: for i = 1 to N do Sample initial state s 0 in S</p>
<p>Fig. 3 :
3
Fig. 3: Simulation environment.Simulation environment showing the Jetbot with multiple colored spheres.During evaluation, the robot must navigate to a specific colored sphere based on language instructions that include both semantic color identification and relative spatial cues using only RGB camera input and pretrained vision-language embeddings.</p>
<p>Fig. 4 :
4
Fig. 4: Reward comparison.Cumulative rewards comparison between the expert (π β , blue) and VLM-based policy (π, orange).The learned policy shows greater variability and consistently lower rewards due to inefficient navigation and occasional failures.</p>
<p>While our main results demonstrate SigLIP's effectiveness, we conducted a controlled comparison to validate our VLM selection methodology and understand the</p>
<p>Fig. 6 :
6
Fig.6: Behavioral cloning loss across VLM architectures.SigLIP achieves superior navigation performance despite higher BC loss than ViLT and CLIP, suggesting that BC loss does not predict downstream task performance for embodied applications.</p>
<p>TABLE I :
I
VLM spatial sensitivity analysis.Cosine distance between joint image-text embeddings across spatial configurations.Lower values indicate greater embedding similarity given a task.All models show smaller distances for same spatial positions, but SigLIP shows the most pronounced separation.</p>
<p>TABLE II :
II
Performance comparison between policies.</p>
<p>Getting vit in shape: Scaling laws for compute-optimal model design. Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, Lucas Beyer, 2024</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia, 2024</p>
<p>Navila: Legged robot vision-language-action model for navigation. An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang, 2025</p>
<p>Clip-nav: Using clip for zeroshot vision-and-language navigation. Gunnar Vishnu Sashank Dorbala, Robinson Sigurdsson, Jesse Piramuthu, Gaurav S Thomason, Sukhatme, 2022</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, 2019</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, 2024</p>
<p>Visual language maps for robot navigation. Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard, 2023</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, 2022</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, 2021</p>
<p>Distilling realizable students from unrealizable teachers. Yujin Kim, Nathaniel Chin, Arnav Vasudev, Sanjiban Choudhury, 2025</p>
<p>Constrained behavior cloning for robotic learning. Wensheng Liang, Jun Xie, Zhicheng Wang, Jianwei Tan, Xiaoguang Ma, 2024</p>
<p>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings. Arjun Majumdar, Gunjan Aggarwal, Bhavika Suresh Devnani, Judy Hoffman, Dhruv Batra, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Orbit: A unified simulation framework for interactive robot learning environments. Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, Animesh Garg, 10.1109/LRA.2023.3270034IEEE Robotics and Automation Letters. 862023</p>
<p>Language-conditioned offline rl for multi-robot navigation. Steven Morad, Ajay Shankar, Jan Blumenkamp, Amanda Prorok, 2024</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, 2022</p>
<p>Robotic-clip: Fine-tuning clip on action data for robotic applications. Nghia Nguyen, Minh Nhat Vu, Tung D Ta, Baoru Huang, Thieu Vo, Ngan Le, Anh Nguyen, 2024</p>
<p>. Nvidia Isaac Sim, 2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021</p>
<p>Latent plans for task-agnostic offline reinforcement learning. Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, Wolfram Burgard, 2022</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 2017</p>
<p>Vlm-social-nav: Socially aware robot navigation through scoring using vision-language models. Daeun Song, Jing Liang, Amirreza Payandeh, Xuesu Amir Hossain Raj, Dinesh Xiao, Manocha, 2024</p>
<p>Openworld object manipulation using pre-trained visionlanguage models. Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, Karol Hausman, 2023</p>
<p>Sigmoid loss for language image pre-training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, 2023</p>
<p>RT-2: Visionlanguage-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Grecia Pannag R Sanketi, Salazar, Krista Michael S Ryoo, Kanishka Reymann, Karl Rao, Igor Pertsch, Henryk Mordatch, Yao Michalewski, Sergey Lu, Lisa Levine, Tsang-Wei Edward Lee, Isabel Lee, Yuheng Leal, Dmitry Kuang, Ryan Kalashnikov, Julian, J Nikhil, Alex Joshi, Jasmine Irpan, Alexander Hsu, Karol Herzog, Keerthana Hausman, Chuyuan Gopalakrishnan, Pete Fu, Chelsea Florence, Finn, Avinava Kumar, Danny Dubey, Tianli Driess, Ding, Montserrat Gonzalez Arenas, and Kehang Han. Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan20237th Annual Conference on Robot Learning</p>            </div>
        </div>

    </div>
</body>
</html>
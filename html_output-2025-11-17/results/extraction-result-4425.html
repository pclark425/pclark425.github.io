<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-274656829</p>
                <p><strong>Paper Title:</strong> Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) were used to assist four Commonwealth Scientific and Industrial Research Organisation (CSIRO) researchers to perform systematic literature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in these case studies. In each, we explore the impact of changing parameters on the accuracy of LLM responses. The LLM was tasked with extracting evidence from chosen academic papers to answer specific research questions. We evaluate the models' performance in faithfully reproducing quotes from the literature and subject experts were asked to assess the model performance in answering the research questions. We developed a semantic text highlighting tool to facilitate expert review of LLM responses. We found that state of the art LLMs were able to reproduce quotes from texts with greater than 95% accuracy and answer research questions with an accuracy of approximately 83%. We use two methods to determine the correctness of LLM responses; expert review and the cosine similarity of transformer embeddings of LLM and expert answers. The correlation between these methods ranged from 0.48 to 0.77, providing evidence that the latter is a valid metric for measuring semantic similarity.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CSIRO LLM-SLR Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CSIRO Large Language Model-assisted Systematic Literature Review Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An applied workflow developed and evaluated in this paper that uses GPT-3.5 Turbo and GPT-4 Turbo to assist domain researchers in performing Systematic Literature Review (SLR) tasks (searching, screening, extraction, synthesis), augmented with quote verification and semantic highlighting to help expert verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CSIRO LLM-SLR Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-step human-in-the-loop SLR pipeline where entire papers (converted from PDF to text) are provided directly to an LLM (no RAG used due to sufficiently large context windows). The LLM is prompted to (1) extract evidence quotes from papers, (2) provide final answers to specific research questions, and (3) perform screening decisions. Quote faithfulness is verified automatically by fuzzy string matching (thefuzz implementation of Levenshtein distance) and then the model is asked to produce final answers given verified quotes. Outputs are assisted by a semantic text highlighting algorithm (Algorithm 1) that highlights words in extracted evidence according to keyword sets (Entities/Relations/Properties) using SpaCy vector similarity and an extended Wu–Palmer (WordNet) similarity; transformer embeddings are used to compute cosine similarity between LLM and expert answers for automated semantic-similarity scoring. The workflow experiments with prompt variants (evidence-first vs direct), task batching (separate calls vs combined call), and model choice (GPT-3.5 Turbo vs GPT-4 Turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5 Turbo (referred to as GPT3) and GPT-4 Turbo (referred to as GPT4) via Microsoft Azure endpoints</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Direct-context prompting (full paper provided); explicit evidence-quote extraction prompts; fuzzy quote verification (Levenshtein/thefuzz); in-context learning with domain definitions; use of POS parsing (spaCy) to support highlighting.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single- or multi-call question-answering (LLM generates final answers from verified quotes); comparative similarity checks using transformer embedding cosine similarity; manual expert adjudication aggregated as final synthesis (human-in-the-loop aggregation rather than an automated claim-aggregation pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Case studies used: 10 papers (case study 1), 12 papers (case study 2), 60 papers (case study 3), plus screening datasets of 14 (arXiv) and 20 (PubMed) papers in case study 4.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Interdisciplinary system science: agri-food transitions, coordinated crisis response, sustainable transitions, and automated marking (education).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extracted quotes (evidence), per-paper answers to research questions, screening labels (relevant/irrelevant), highlighted evidence text to aid human review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Expert human review (binary correctness and qualitative ratings), fuzzy string matching (Levenshtein) for quote faithfulness, SpaCy semantic similarity, transformer-embedding cosine similarity, highlighting-rate correlation, false positive/false negative screening rates, token usage (prompt/completion) for cost analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Quote reproduction (faithfulness): GPT-3.5 Turbo ≈ 95% accuracy, GPT-4 Turbo ≈ 98% accuracy (fuzzy matching). Average final-answer (research-question) accuracy for GPT-4 Turbo ≈ 83%. Transformer similarity correlated with expert judgement with correlations reported between ~0.48 and 0.77 (case-dependent). Asking for evidence increased completion tokens ~4x and slightly reduced measured accuracy; grouping many tasks into a single call reduced prompt tokens ~10x but introduced frame-shift/jumbling errors and decreased quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert review (subject-matter experts) and model-to-model comparison (GPT-3.5 vs GPT-4); automated similarity metrics (SpaCy, transformer embeddings) compared against expert judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLMs reproduced quotes near-perfectly (95–98%) compared to human-provided quotes; final-answer accuracy (expert-rated) for GPT-4 was ≈83% on average, with near-100% on trivial tasks and much lower on highly nuanced tasks (down to ~10% in worst cases). Transformer-embedding similarity correlated better with expert judgements than SpaCy similarity (SpaCy sometimes had near-zero or negative correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing the whole paper as context and verifying extracted quotes with fuzzy matching yields very high quote fidelity; transformer-embedding cosine similarity is a stronger automated proxy for expert judgement than SpaCy similarity; asking LLMs to produce evidence increases cost and can slightly reduce accuracy; batching many tasks into one call reduces token usage but can produce ‘frameshift’ errors and reduce output quality; semantic highlighting helps human reviewers triage evidence but requires expert keyword calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination risk (mitigated by quote verification but still present); selection and domain biases; LLMs can lack domain nuance (mitigated by providing definitions); SpaCy similarity sensitive to capitalization; high uncertainty in highlighting-correlation metric; task-batching leads to frameshift/jumbled responses; computational cost scales with completion tokens; transformer similarity values cluster high (0.7–0.95) and need rescaling for human interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Observed that grouping multiple tasks into a single prompt reduced prompt-token count by ~10x (cost-saving) but degraded accuracy via jumbled answers; GPT-3.5's shorter context length sometimes prevented it from finding relevant quotes when entire paper did not fit, causing more errors compared to GPT-4; increasing the number of tasks the model must handle in one call tended to reduce performance (frameshift errors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic Highlighting (Algorithm 1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Text Highlighting Algorithm (Algorithm 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explainable, non-trained highlighting algorithm that marks words in retrieved evidence as semantically related to user-specified keyword sets (Entities, Relations, Properties) using SpaCy POS parsing, vector similarities, and an extended Wu–Palmer WordNet-based similarity function with tunable thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semantic Text Highlighting (Algorithm 1)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Algorithm requires domain-expert-supplied keyword sets partitioned into Entities (nouns), Relations (verbs), and Properties (adjectives/adverbs). Each word in candidate evidence is POS-parsed (spaCy); two similarity scores are computed against relevant keywords: (1) Wu–Palmer (WordNet) extended similarity over synsets/pertainyms/related forms with weighted contributions, and (2) spaCy word-vector cosine similarity as fallback. Thresholds (WUP-Threshold, VEC-Threshold) control highlighting. The algorithm can produce explanations for highlighted tokens and provides a highlighting-rate statistic used to compare LLM and expert outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not an LLM itself; uses spaCy word vectors and WordNet resources. The overall workflow pairs this algorithm with GPT-3.5/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not an extractor: it annotates LLM-retrieved evidence text (post-extraction) using POS parsing + WordNet/Wu–Palmer + spaCy vector similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Used as an aid to synthesis by quantifying overlap between highlighted words in expert vs LLM answers (highlighting-rate) and by visually marking semantically relevant tokens to speed human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied across the paper's case studies (examples in multiple case studies; used on evidence extracted from tens of papers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General-purpose for evidence texts in SLRs (demonstrated on interdisciplinary system science texts).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Highlighted evidence text (token-level annotations) and a highlighting-rate metric (fraction of tokens highlighted).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Highlighting correlation with human judgement (Pearson correlation against expert highlighted fractions), qualitative expert feedback, and empirical guidelines for target highlighting rates (~0.4 ± 0.1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Authors report the highlighting-rate correlates with human-judged accuracy in the expected direction but with large uncertainty; suggested useful target highlighting-rate for calibration is ~0.4 ± 0.1 on evidence texts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert highlighting and semantic-similarity metrics (SpaCy and transformer embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Transformer-embedding similarity correlated with expert judgement more strongly than SpaCy; highlighting correlation followed human-judged accuracy trends but had high uncertainty (no definitive replacement for expert review).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Wu–Palmer WordNet similarity often aligns better with human expectations for highlighting than spaCy vector similarity; vector similarity serves as fallback; the algorithm is explainable, requires no training data, and is helpful for triaging evidence when calibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires expert-in-the-loop keyword calibration (semi-automated); WordNet coverage and POS tagging limits; high uncertainty in highlighting-rate as automated metric; may highlight too many or too few tokens depending on keyword set and text density.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to be lightweight (no training) so scales with the number of evidence texts, but effectiveness depends on per-question keyword calibration; no formal study on very large corpora presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM: A Toolkit for Scientific Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit cited by the paper as an existing tool for assisting literature review with LLMs; the paper notes techniques are undisclosed but that such tools commonly use LLM APIs, prompt engineering, Retrieval-Augmented Generation (RAG) and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LitLLM: A Toolkit for Scientific Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as one among several commercial/academic tools for enhancing or automating literature review. The paper states the internal techniques of LitLLM remain undisclosed but that such tools appear to combine LLM API calls, in-context prompt engineering, RAG, and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (techniques undisclosed).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Reportedly appears to use LLM API calls with prompt engineering and Retrieval-Augmented Generation (RAG) and possibly fine-tuning (per the paper's literature summary).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper; likely generative summarization and retrieval-augmented summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review toolkit (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper (presumably summaries, extracted evidence, citations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper highlights that LitLLM (and similar tools) exist but lack transparency about internal techniques, making it difficult to determine best practices for AI-assisted SLR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Commercial secrecy and lack of transparency; unknown bias and reliability properties in their pipelines as noted by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4425.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature analytics tool cited as an existing product that enhances literature review; internal techniques are treated as commercial and undisclosed, but likely incorporate LLMs or NLP methods for evidence-scoring and citation contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scite</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of existing literature-review enhancement tools; the paper does not provide internal details, noting most commercial tools keep techniques undisclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper; likely citation-context extraction and NLP-based classification (paper only notes black-box nature).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scholarly literature analytics.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper (commonly: citation context, evidence classification).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Listed among tools; authors stress lack of transparency across such tools hinders reproducibility and best-practice identification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Commercial secrecy; unknown training data and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4425.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit (product)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A product cited as a tool being used to enhance literature review workflows; treated as a black-box in this paper with likely use of LLM APIs and prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Elicit (product review)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as an existing tool for literature review; the authors note its internal methods are undisclosed but it is representative of tools that employ LLM API calls, prompt engineering, RAG and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper; generally characterized as using LLM-based retrieval/QA (per the paper's summary of common strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Elicit exemplifies commercial/academic tools using LLM strategies, but specifics are not made public, impeding assessment of best-practice methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Lack of methodological transparency; unknown biases and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4425.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scopus AI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scopus AI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-search/product offering mentioned as an example of AI-enhanced literature review tools; internal methods are not disclosed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerating research processes with Scopus AI: A place branding case study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scopus AI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an example of vendor-provided AI tools for literature review; paper states that approaches of such tools are commercially undisclosed but appear to use LLM API calls, prompt engineering, RAG and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper; likely retrieval-augmented generation and NLP-based extraction per general characterization.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General academic literature; example in cited case study applied to place branding.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Listed among modern tools; lack of transparency in such systems is highlighted as a barrier to assessing and improving SLR methodologies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Commercial secrecy and unknown biases in system outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4425.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system (2024) that frames an LLM agent combined with human workflow guidance to produce comparative literature summaries; cited as related work but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited in the references as a prior work proposing an LLM agent that integrates human workflow guidance to produce comparative literature summaries. The current paper does not detail ChatCite's architecture; it is listed as an example of approaches that coordinate LLM agents and human workflows for comparative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper (title implies agent-guided comparative summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Comparative literature summarization (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative literature summaries (implied by title).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a relevant recent example of LLM-driven literature summarization with human workflow guidance; no details given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4425.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent SLR (Sami et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced 2024 work proposing a multi-agent AI architecture for performing systematic literature review tasks; cited as related literature but not used or evaluated by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Agent SLR (Sami et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in the references as a concept and empirical evaluation of a system using multiple AI agents to perform SLR tasks. The present paper only cites it as related work and does not describe internal components, but places it in the context of agent-based approaches to automate literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper (likely agent-coordinated retrieval and extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper (multi-agent aggregation implied).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature review processes (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper (presumably structured SLR outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as part of the rapidly evolving landscape of AI-assisted SLR systems that coordinate multiple agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4425.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid Semi-Auto Workflow (Ye et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced paper describing a hybrid semi-automated SLR workflow that integrates LLM analysis; cited in related work but not described in detail within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid Semi-Automated Workflow (Ye et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Listed in the references as a recent contribution proposing a hybrid semi-automated SLR pipeline that leverages LLMs for analysis; the present paper lists it among recent efforts optimizing LLM tools for literature review but does not summarize internals.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic and literature review processes (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work in efforts to semi-automate SLR with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4425.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Evidence Synthesis Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study (ACL 2023 short paper) that applied GPT-3 to medical evidence summarization and synthesis, showing mixed results; cited as prior art applying LLMs to synthesize evidence across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success).</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3 Evidence Synthesis Study (Shaib et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a prior application of GPT-3 to summarization/synthesis of medical evidence; the current paper cites it to indicate existing attempts at automating synthesis with LLMs and varying levels of success.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3 (as per the cited work title), details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper (implied population-level summarization and generation).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper (implied summarization/simplification/synthesis across multiple studies).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Medical evidence / biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries and synthesized medical evidence (per referenced title).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper; referenced as 'varying success' in original title.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to illustrate that LLM-based evidence synthesis has been attempted with mixed outcomes, motivating careful evaluation and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Original work reported mixed success; current paper reiterates general LLM limitations such as hallucination and domain nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4425.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4425.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Insights (ChatGPT case study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced case study (2024) applying ChatGPT for research paper analysis; cited as one of several contemporary efforts to use general-purpose LLMs for literature analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Insights (De Silva et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Listed among related studies that applied ChatGPT/large language models to analyze research papers. The present paper does not provide details on architecture or evaluation beyond citing the work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (ChatGPT/ChatGPT-derived model implied by title); specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Research paper analysis (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper (presumably analysis summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Included among recent case studies exploring ChatGPT's utility in research analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LitLLM: A Toolkit for Scientific Literature Review <em>(Rating: 2)</em></li>
                <li>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary <em>(Rating: 2)</em></li>
                <li>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation <em>(Rating: 2)</em></li>
                <li>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis <em>(Rating: 2)</em></li>
                <li>Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success). <em>(Rating: 2)</em></li>
                <li>AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis <em>(Rating: 1)</em></li>
                <li>Artificial Intelligence for Literature Reviews: Opportunities and Challenges. <em>(Rating: 2)</em></li>
                <li>Human-AI Collaboration to Identify Literature for Evidence Synthesis <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4425",
    "paper_id": "paper-274656829",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "CSIRO LLM-SLR Workflow",
            "name_full": "CSIRO Large Language Model-assisted Systematic Literature Review Workflow",
            "brief_description": "An applied workflow developed and evaluated in this paper that uses GPT-3.5 Turbo and GPT-4 Turbo to assist domain researchers in performing Systematic Literature Review (SLR) tasks (searching, screening, extraction, synthesis), augmented with quote verification and semantic highlighting to help expert verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CSIRO LLM-SLR Workflow",
            "system_description": "A multi-step human-in-the-loop SLR pipeline where entire papers (converted from PDF to text) are provided directly to an LLM (no RAG used due to sufficiently large context windows). The LLM is prompted to (1) extract evidence quotes from papers, (2) provide final answers to specific research questions, and (3) perform screening decisions. Quote faithfulness is verified automatically by fuzzy string matching (thefuzz implementation of Levenshtein distance) and then the model is asked to produce final answers given verified quotes. Outputs are assisted by a semantic text highlighting algorithm (Algorithm 1) that highlights words in extracted evidence according to keyword sets (Entities/Relations/Properties) using SpaCy vector similarity and an extended Wu–Palmer (WordNet) similarity; transformer embeddings are used to compute cosine similarity between LLM and expert answers for automated semantic-similarity scoring. The workflow experiments with prompt variants (evidence-first vs direct), task batching (separate calls vs combined call), and model choice (GPT-3.5 Turbo vs GPT-4 Turbo).",
            "llm_model_used": "GPT-3.5 Turbo (referred to as GPT3) and GPT-4 Turbo (referred to as GPT4) via Microsoft Azure endpoints",
            "extraction_technique": "Direct-context prompting (full paper provided); explicit evidence-quote extraction prompts; fuzzy quote verification (Levenshtein/thefuzz); in-context learning with domain definitions; use of POS parsing (spaCy) to support highlighting.",
            "synthesis_technique": "Single- or multi-call question-answering (LLM generates final answers from verified quotes); comparative similarity checks using transformer embedding cosine similarity; manual expert adjudication aggregated as final synthesis (human-in-the-loop aggregation rather than an automated claim-aggregation pipeline).",
            "number_of_papers": "Case studies used: 10 papers (case study 1), 12 papers (case study 2), 60 papers (case study 3), plus screening datasets of 14 (arXiv) and 20 (PubMed) papers in case study 4.",
            "domain_or_topic": "Interdisciplinary system science: agri-food transitions, coordinated crisis response, sustainable transitions, and automated marking (education).",
            "output_type": "Extracted quotes (evidence), per-paper answers to research questions, screening labels (relevant/irrelevant), highlighted evidence text to aid human review.",
            "evaluation_metrics": "Expert human review (binary correctness and qualitative ratings), fuzzy string matching (Levenshtein) for quote faithfulness, SpaCy semantic similarity, transformer-embedding cosine similarity, highlighting-rate correlation, false positive/false negative screening rates, token usage (prompt/completion) for cost analysis.",
            "performance_results": "Quote reproduction (faithfulness): GPT-3.5 Turbo ≈ 95% accuracy, GPT-4 Turbo ≈ 98% accuracy (fuzzy matching). Average final-answer (research-question) accuracy for GPT-4 Turbo ≈ 83%. Transformer similarity correlated with expert judgement with correlations reported between ~0.48 and 0.77 (case-dependent). Asking for evidence increased completion tokens ~4x and slightly reduced measured accuracy; grouping many tasks into a single call reduced prompt tokens ~10x but introduced frame-shift/jumbling errors and decreased quality.",
            "comparison_baseline": "Human expert review (subject-matter experts) and model-to-model comparison (GPT-3.5 vs GPT-4); automated similarity metrics (SpaCy, transformer embeddings) compared against expert judgements.",
            "performance_vs_baseline": "LLMs reproduced quotes near-perfectly (95–98%) compared to human-provided quotes; final-answer accuracy (expert-rated) for GPT-4 was ≈83% on average, with near-100% on trivial tasks and much lower on highly nuanced tasks (down to ~10% in worst cases). Transformer-embedding similarity correlated better with expert judgements than SpaCy similarity (SpaCy sometimes had near-zero or negative correlations).",
            "key_findings": "Providing the whole paper as context and verifying extracted quotes with fuzzy matching yields very high quote fidelity; transformer-embedding cosine similarity is a stronger automated proxy for expert judgement than SpaCy similarity; asking LLMs to produce evidence increases cost and can slightly reduce accuracy; batching many tasks into one call reduces token usage but can produce ‘frameshift’ errors and reduce output quality; semantic highlighting helps human reviewers triage evidence but requires expert keyword calibration.",
            "limitations_challenges": "Hallucination risk (mitigated by quote verification but still present); selection and domain biases; LLMs can lack domain nuance (mitigated by providing definitions); SpaCy similarity sensitive to capitalization; high uncertainty in highlighting-correlation metric; task-batching leads to frameshift/jumbled responses; computational cost scales with completion tokens; transformer similarity values cluster high (0.7–0.95) and need rescaling for human interpretation.",
            "scaling_behavior": "Observed that grouping multiple tasks into a single prompt reduced prompt-token count by ~10x (cost-saving) but degraded accuracy via jumbled answers; GPT-3.5's shorter context length sometimes prevented it from finding relevant quotes when entire paper did not fit, causing more errors compared to GPT-4; increasing the number of tasks the model must handle in one call tended to reduce performance (frameshift errors).",
            "uuid": "e4425.0",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Semantic Highlighting (Algorithm 1)",
            "name_full": "Semantic Text Highlighting Algorithm (Algorithm 1)",
            "brief_description": "An explainable, non-trained highlighting algorithm that marks words in retrieved evidence as semantically related to user-specified keyword sets (Entities, Relations, Properties) using SpaCy POS parsing, vector similarities, and an extended Wu–Palmer WordNet-based similarity function with tunable thresholds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Semantic Text Highlighting (Algorithm 1)",
            "system_description": "Algorithm requires domain-expert-supplied keyword sets partitioned into Entities (nouns), Relations (verbs), and Properties (adjectives/adverbs). Each word in candidate evidence is POS-parsed (spaCy); two similarity scores are computed against relevant keywords: (1) Wu–Palmer (WordNet) extended similarity over synsets/pertainyms/related forms with weighted contributions, and (2) spaCy word-vector cosine similarity as fallback. Thresholds (WUP-Threshold, VEC-Threshold) control highlighting. The algorithm can produce explanations for highlighted tokens and provides a highlighting-rate statistic used to compare LLM and expert outputs.",
            "llm_model_used": "Not an LLM itself; uses spaCy word vectors and WordNet resources. The overall workflow pairs this algorithm with GPT-3.5/GPT-4.",
            "extraction_technique": "Not an extractor: it annotates LLM-retrieved evidence text (post-extraction) using POS parsing + WordNet/Wu–Palmer + spaCy vector similarity.",
            "synthesis_technique": "Used as an aid to synthesis by quantifying overlap between highlighted words in expert vs LLM answers (highlighting-rate) and by visually marking semantically relevant tokens to speed human verification.",
            "number_of_papers": "Applied across the paper's case studies (examples in multiple case studies; used on evidence extracted from tens of papers).",
            "domain_or_topic": "General-purpose for evidence texts in SLRs (demonstrated on interdisciplinary system science texts).",
            "output_type": "Highlighted evidence text (token-level annotations) and a highlighting-rate metric (fraction of tokens highlighted).",
            "evaluation_metrics": "Highlighting correlation with human judgement (Pearson correlation against expert highlighted fractions), qualitative expert feedback, and empirical guidelines for target highlighting rates (~0.4 ± 0.1).",
            "performance_results": "Authors report the highlighting-rate correlates with human-judged accuracy in the expected direction but with large uncertainty; suggested useful target highlighting-rate for calibration is ~0.4 ± 0.1 on evidence texts.",
            "comparison_baseline": "Human expert highlighting and semantic-similarity metrics (SpaCy and transformer embeddings).",
            "performance_vs_baseline": "Transformer-embedding similarity correlated with expert judgement more strongly than SpaCy; highlighting correlation followed human-judged accuracy trends but had high uncertainty (no definitive replacement for expert review).",
            "key_findings": "Wu–Palmer WordNet similarity often aligns better with human expectations for highlighting than spaCy vector similarity; vector similarity serves as fallback; the algorithm is explainable, requires no training data, and is helpful for triaging evidence when calibrated.",
            "limitations_challenges": "Requires expert-in-the-loop keyword calibration (semi-automated); WordNet coverage and POS tagging limits; high uncertainty in highlighting-rate as automated metric; may highlight too many or too few tokens depending on keyword set and text density.",
            "scaling_behavior": "Designed to be lightweight (no training) so scales with the number of evidence texts, but effectiveness depends on per-question keyword calibration; no formal study on very large corpora presented.",
            "uuid": "e4425.1",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM: A Toolkit for Scientific Literature Review",
            "brief_description": "A toolkit cited by the paper as an existing tool for assisting literature review with LLMs; the paper notes techniques are undisclosed but that such tools commonly use LLM APIs, prompt engineering, Retrieval-Augmented Generation (RAG) and fine-tuning.",
            "citation_title": "LitLLM: A Toolkit for Scientific Literature Review",
            "mention_or_use": "mention",
            "system_name": "LitLLM",
            "system_description": "Mentioned as one among several commercial/academic tools for enhancing or automating literature review. The paper states the internal techniques of LitLLM remain undisclosed but that such tools appear to combine LLM API calls, in-context prompt engineering, RAG, and fine-tuning.",
            "llm_model_used": "Not specified in this paper (techniques undisclosed).",
            "extraction_technique": "Reportedly appears to use LLM API calls with prompt engineering and Retrieval-Augmented Generation (RAG) and possibly fine-tuning (per the paper's literature summary).",
            "synthesis_technique": "Not specified in this paper; likely generative summarization and retrieval-augmented summarization.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Scientific literature review toolkit (general).",
            "output_type": "Not specified in this paper (presumably summaries, extracted evidence, citations).",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Paper highlights that LitLLM (and similar tools) exist but lack transparency about internal techniques, making it difficult to determine best practices for AI-assisted SLR.",
            "limitations_challenges": "Commercial secrecy and lack of transparency; unknown bias and reliability properties in their pipelines as noted by the authors.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.2",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Scite",
            "name_full": "Scite",
            "brief_description": "A literature analytics tool cited as an existing product that enhances literature review; internal techniques are treated as commercial and undisclosed, but likely incorporate LLMs or NLP methods for evidence-scoring and citation contexts.",
            "citation_title": "Scite",
            "mention_or_use": "mention",
            "system_name": "Scite",
            "system_description": "Referenced as an example of existing literature-review enhancement tools; the paper does not provide internal details, noting most commercial tools keep techniques undisclosed.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Not specified in this paper; likely citation-context extraction and NLP-based classification (paper only notes black-box nature).",
            "synthesis_technique": "Not specified in this paper.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "General scholarly literature analytics.",
            "output_type": "Not specified in this paper (commonly: citation context, evidence classification).",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Listed among tools; authors stress lack of transparency across such tools hinders reproducibility and best-practice identification.",
            "limitations_challenges": "Commercial secrecy; unknown training data and biases.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.3",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Elicit",
            "name_full": "Elicit (product)",
            "brief_description": "A product cited as a tool being used to enhance literature review workflows; treated as a black-box in this paper with likely use of LLM APIs and prompt engineering.",
            "citation_title": "Elicit (product review)",
            "mention_or_use": "mention",
            "system_name": "Elicit",
            "system_description": "Mentioned in related work as an existing tool for literature review; the authors note its internal methods are undisclosed but it is representative of tools that employ LLM API calls, prompt engineering, RAG and fine-tuning.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Not specified in this paper; generally characterized as using LLM-based retrieval/QA (per the paper's summary of common strategies).",
            "synthesis_technique": "Not specified in this paper.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "General literature review.",
            "output_type": "Not specified in this paper.",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Elicit exemplifies commercial/academic tools using LLM strategies, but specifics are not made public, impeding assessment of best-practice methods.",
            "limitations_challenges": "Lack of methodological transparency; unknown biases and validation.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.4",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Scopus AI",
            "name_full": "Scopus AI",
            "brief_description": "A literature-search/product offering mentioned as an example of AI-enhanced literature review tools; internal methods are not disclosed in the paper.",
            "citation_title": "Accelerating research processes with Scopus AI: A place branding case study",
            "mention_or_use": "mention",
            "system_name": "Scopus AI",
            "system_description": "Mentioned as an example of vendor-provided AI tools for literature review; paper states that approaches of such tools are commercially undisclosed but appear to use LLM API calls, prompt engineering, RAG and fine-tuning.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Not specified in this paper; likely retrieval-augmented generation and NLP-based extraction per general characterization.",
            "synthesis_technique": "Not specified in this paper.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "General academic literature; example in cited case study applied to place branding.",
            "output_type": "Not specified in this paper.",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Listed among modern tools; lack of transparency in such systems is highlighted as a barrier to assessing and improving SLR methodologies.",
            "limitations_challenges": "Commercial secrecy and unknown biases in system outputs.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.5",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "brief_description": "A referenced system (2024) that frames an LLM agent combined with human workflow guidance to produce comparative literature summaries; cited as related work but not evaluated in this paper.",
            "citation_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "mention_or_use": "mention",
            "system_name": "ChatCite",
            "system_description": "Cited in the references as a prior work proposing an LLM agent that integrates human workflow guidance to produce comparative literature summaries. The current paper does not detail ChatCite's architecture; it is listed as an example of approaches that coordinate LLM agents and human workflows for comparative summaries.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Not specified in this paper.",
            "synthesis_technique": "Not specified in this paper (title implies agent-guided comparative summarization).",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Comparative literature summarization (general).",
            "output_type": "Comparative literature summaries (implied by title).",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Mentioned as a relevant recent example of LLM-driven literature summarization with human workflow guidance; no details given in this paper.",
            "limitations_challenges": "Not discussed in this paper.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.6",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Multi-Agent SLR (Sami et al.)",
            "name_full": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "brief_description": "A referenced 2024 work proposing a multi-agent AI architecture for performing systematic literature review tasks; cited as related literature but not used or evaluated by the authors.",
            "citation_title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "mention_or_use": "mention",
            "system_name": "Multi-Agent SLR (Sami et al.)",
            "system_description": "Described in the references as a concept and empirical evaluation of a system using multiple AI agents to perform SLR tasks. The present paper only cites it as related work and does not describe internal components, but places it in the context of agent-based approaches to automate literature review.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Not specified in this paper (likely agent-coordinated retrieval and extraction).",
            "synthesis_technique": "Not specified in this paper (multi-agent aggregation implied).",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Systematic literature review processes (general).",
            "output_type": "Not specified in this paper (presumably structured SLR outputs).",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Referenced as part of the rapidly evolving landscape of AI-assisted SLR systems that coordinate multiple agents.",
            "limitations_challenges": "Not discussed in this paper.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.7",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Hybrid Semi-Auto Workflow (Ye et al.)",
            "name_full": "A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis",
            "brief_description": "A referenced paper describing a hybrid semi-automated SLR workflow that integrates LLM analysis; cited in related work but not described in detail within this paper.",
            "citation_title": "A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis",
            "mention_or_use": "mention",
            "system_name": "Hybrid Semi-Automated Workflow (Ye et al.)",
            "system_description": "Listed in the references as a recent contribution proposing a hybrid semi-automated SLR pipeline that leverages LLMs for analysis; the present paper lists it among recent efforts optimizing LLM tools for literature review but does not summarize internals.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Not specified in this paper.",
            "synthesis_technique": "Not specified in this paper.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Systematic and literature review processes (general).",
            "output_type": "Not specified in this paper.",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Cited as related work in efforts to semi-automate SLR with LLMs.",
            "limitations_challenges": "Not discussed in this paper.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.8",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-3 Evidence Synthesis Study",
            "name_full": "Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",
            "brief_description": "A referenced study (ACL 2023 short paper) that applied GPT-3 to medical evidence summarization and synthesis, showing mixed results; cited as prior art applying LLMs to synthesize evidence across papers.",
            "citation_title": "Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success).",
            "mention_or_use": "mention",
            "system_name": "GPT-3 Evidence Synthesis Study (Shaib et al.)",
            "system_description": "Referenced as a prior application of GPT-3 to summarization/synthesis of medical evidence; the current paper cites it to indicate existing attempts at automating synthesis with LLMs and varying levels of success.",
            "llm_model_used": "GPT-3 (as per the cited work title), details not provided in this paper.",
            "extraction_technique": "Not specified in this paper (implied population-level summarization and generation).",
            "synthesis_technique": "Not specified in this paper (implied summarization/simplification/synthesis across multiple studies).",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Medical evidence / biomedical literature.",
            "output_type": "Summaries and synthesized medical evidence (per referenced title).",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper; referenced as 'varying success' in original title.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Cited to illustrate that LLM-based evidence synthesis has been attempted with mixed outcomes, motivating careful evaluation and human oversight.",
            "limitations_challenges": "Original work reported mixed success; current paper reiterates general LLM limitations such as hallucination and domain nuance.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.9",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "AI Insights (ChatGPT case study)",
            "name_full": "AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis",
            "brief_description": "A referenced case study (2024) applying ChatGPT for research paper analysis; cited as one of several contemporary efforts to use general-purpose LLMs for literature analysis.",
            "citation_title": "AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis.",
            "mention_or_use": "mention",
            "system_name": "AI Insights (De Silva et al.)",
            "system_description": "Listed among related studies that applied ChatGPT/large language models to analyze research papers. The present paper does not provide details on architecture or evaluation beyond citing the work.",
            "llm_model_used": "ChatGPT (ChatGPT/ChatGPT-derived model implied by title); specifics not provided in this paper.",
            "extraction_technique": "Not specified in this paper.",
            "synthesis_technique": "Not specified in this paper.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Research paper analysis (general).",
            "output_type": "Not specified in this paper (presumably analysis summaries).",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Included among recent case studies exploring ChatGPT's utility in research analysis.",
            "limitations_challenges": "Not discussed in this paper.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4425.10",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "rating": 2,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        },
        {
            "paper_title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "rating": 2,
            "sanitized_title": "system_for_systematic_literature_review_using_multiple_ai_agents_concept_and_an_empirical_evaluation"
        },
        {
            "paper_title": "A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis",
            "rating": 2,
            "sanitized_title": "a_hybrid_semiautomated_workflow_for_systematic_and_literature_review_processes_with_large_language_model_analysis"
        },
        {
            "paper_title": "Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success).",
            "rating": 2,
            "sanitized_title": "summarizing_simplifying_and_synthesizing_medical_evidence_using_gpt3_with_varying_success"
        },
        {
            "paper_title": "AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis",
            "rating": 1,
            "sanitized_title": "ai_insights_a_case_study_on_utilizing_chatgpt_intelligence_for_research_paper_analysis"
        },
        {
            "paper_title": "Artificial Intelligence for Literature Reviews: Opportunities and Challenges.",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_for_literature_reviews_opportunities_and_challenges"
        },
        {
            "paper_title": "Human-AI Collaboration to Identify Literature for Evidence Synthesis",
            "rating": 2,
            "sanitized_title": "humanai_collaboration_to_identify_literature_for_evidence_synthesis"
        }
    ],
    "cost": 0.0204475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science ⋆
16 Mar 2025</p>
<p>Lachlan Mcginness 
Australian National University</p>
<p>Peter Baumgartner 
Australian National University</p>
<p>Esther Onyango esther.onyango@csiro.au 
Australian National University</p>
<p>Zelalem Lema zelalem.moti@csiro.au 
Australian National University</p>
<p>Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science ⋆
16 Mar 2025F7B423E57F2980CA42B4159578A5E67510.1007/978-981-96-0348-0_3arXiv:2503.16515v1[cs.CL]Systematic Literature ReviewLarge Language ModelsHighlighting
Large Language Models (LLMs) were used to assist four Commonwealth Scientific and Industrial Research Organisation (CSIRO) researchers to perform systematic literature reviews (SLR).We evaluate the performance of LLMs for SLR tasks in these case studies.In each, we explore the impact of changing parameters on the accuracy of LLM responses.The LLM was tasked with extracting evidence from chosen academic papers to answer specific research questions.We evaluate the models' performance in faithfully reproducing quotes from the literature and subject experts were asked to assess the model performance in answering the research questions.We developed a semantic text highlighting tool to facilitate expert review of LLM responses.We found that state of the art LLMs were able to reproduce quotes from texts with greater than 95% accuracy and answer research questions with an accuracy of approximately 83%.We use two methods to determine the correctness of LLM responses; expert review and the cosine similarity of transformer embeddings of LLM and expert answers.The correlation between these methods ranged from 0.48 to 0.77, providing evidence that the latter is a valid metric for measuring semantic similarity.</p>
<p>Introduction</p>
<p>The scientific community is currently full of hype and hope for the use of Artificial Intelligence (AI) to accelerate research [28].Messeri and Crockett claim that scientists are too trusting and lack awareness of the biases and errors of Large Language Models (LLMs) [26].They present 'AI as Oracle' as a vision of the future where LLMs overcome the problem of too much literature to digest by efficiently searching and summarising information [26].Many research groups are optimising and improving LLM tools for literature review [7,18,2] including the CSIRO's (Commonwealth Scientific and Industrial Research Organisation's) Science Digital program [10].</p>
<p>Many tools exist to enhance or automate literature review, including LitLLM [1], Scite [7], Elicit [18] and Scopus AI [2].The techniques of these tools remain undisclosed as commercial secrets.However they all appear to use a combination of the same strategies: calling LLMs through APIs, prompt engineering (incontext learning), Retrieval Augmented Generation (RAG) and fine tuning [6].The lack of transparency about these tools make it difficult to determine and accelerate best practice in AI systematic review methods.Systematic Literature Review (SLR) was developed in the field of Evidence-Based Medicine as a method of reducing bias by sticking to strict protocols [6].It has since been adopted in many disciplines including social science, education and environmental science [6].SLR tasks include planning, searching, screening, extraction and synthesis [6].In the extraction phase, desired information is extracted from a set of selected studies.Few studies have attempted to objectively measure the capability of LLMs for SLR [36,41,11,32,33] and even fewer for the extraction and screening phases.Previous studies found that LLMs are unreliable SLR tools as they 'hallucinate' references that do not exist [35].</p>
<p>In this paper we evaluate the performance of GPT-3.5 Turbo, and GPT-4 Turbo, one of the best models currently available, on SLR tasks.We will refer to these models as GPT3 and GPT4 respectively.We present four case studies where LLMs were used to assist CSIRO interdisciplinary systems science researchers, including the last two authors of this paper, in different stages of SLR.In each we systematically explore the impact of changing a parameter on the accuracy of LLM responses.</p>
<p>Although automatically checking LLM responses is highly desirable, currently there are no tools that can perfectly check the correctness or semantic similarity of texts.An important and tedious step of using LLMs in SLR is verifying their responses.To make this task easier we contribute a highlighting algorithm, in analogy to highlighting with a text marker on paper.It aims to provide a human reader with visual clues to quickly scan generated text.The algorithm is driven by a small set of user-supplied keywords provided by a domain expert.We describe the algorithm and report on experiments and experiences with application to our case studies.</p>
<p>The rest of this paper is structured as follows.Section 2 summarizes our methodology for application to the case studies chosen.This includes statistical evaluation methods and LLM techniques.Section 3 introduces an algorithm for explainable text relevance in terms of semantic similarity, and communicating it through text highlighting.Section 4 reports on experimental results, and Section 5 discusses these results and Section 6 summarises the conclusions.</p>
<p>Methodology</p>
<p>We explore four case studies with interdisciplinary system scientists.All four studies used Microsoft Azure endpoints to call either GPT3 and GPT4.The first case study focuses on the health impacts of agri-food transitions.In the second case study, the researcher had performed an SLR extracting the enablers and constraints from twelve papers on coordinated responses to crises.They were interested to know if they had missed any key points when completing the SLR.The third case study verifies an SLR involving sixty papers on a sustainable transitions SLR task [27].The final case study focuses on screening papers for an SLR on the use of generative AI for marking student responses to exam questions.The studies investigate the impact on overall performance when using different models, asking LLMs to provide evidence for their answers, and splitting tasks into several calls.</p>
<p>An example research question from the first case study is "What are the health outcomes of an agri-food transition"?The LLM was tasked with answering the question and finding evidence from an academic paper to support its answer.Evidence would be a quote from the paper such as "An overabundance of food supply alone has been identified as a key cause of the obesity epidemic" and the LLM answer could be "Obesity".</p>
<p>Statistical and Evaluation Techniques</p>
<p>In this section we outline the statistical methods used to analyse the results.In cases where multiple similar data are available these results are summarised using mean (µ x ) and population standard deviation (σ x ) defined as usual.</p>
<p>In our analysis we use two methods to determine the correctness of LLM responses; expert review and automated similarity metrics of LLM and expert answers.The fist automated similarity metric is SpaCy Semantic Similarity [14].This method compares two strings the average embedding vector of the tokens in each of the strings is calculated and the cosine similarity is taken.The second method is the cosine similarity between transformer-based embeddings [30,38,19], which we will refer to as 'transformer similarity'.Transformer based embeddings have the ability to take order of words into account.</p>
<p>To compare these two metrics we use Pearson correlation coefficient shown in Equation 1.
Correlation = r = n i=1 (µ x − x i )(µ y − y i ) σ x σ y(1)
Uncertainty in correlation values was calculated using the Fisher transformation of correlation [13] with a 95% confidence interval (Z = 1.96) using Equation 2. In order to make these values more comparable to standard deviation they were divided by Z.
Correlation Uncertainty = ∆r = 1 Z tanh arctanh(r) ± Z √ n − 3(2)
One of the SLR tasks involves the using an LLM to screen papers and determine their relevance.If a paper is relevant we refer to this as positive.A true positive (TP) result is when a model classifies a relevant paper (as determined by experts) as relevant.A false positive (FP) occurs when an irrelevant paper is classified as relevant.True negatives (TN) and false negatives (FN) are defined similarly.False positive and False Negative rates in the normal way.</p>
<p>LLM Techniques</p>
<p>We converted the research papers from PDF to text with the pdftotext utility5 .Instead of using standard RAG techniques, we provided the entire paper to the LLM as the context windows were large enough.The research scientists were concerned by the biases of LLMs [5,31,4].Here we outline these and our approach to minimising them.</p>
<p>Selection bias: The LLM might favour, for example, well known authors or recent papers.To avoid this type of bias, the research scientist selected the papers for the SLRs.</p>
<p>Inability to understand nuance: There are many studies which note that Large Language Models are effective for general tasks but can struggle with domain specific knowledge and the nuance of specialised tasks [9,24,21].We were concerned that LLMs may lack the nuance to determine whether information relates to the study in the paper or referenced studies.To overcome this issue, researchers removed specific pages based on the task that the LLM needed to perform.For example, when asking about the geographic location of the paper, only the abstract and introduction were provided.We also addressed this issue by using in-context learning to provide the model with specific definitions provided by domain experts.</p>
<p>Lack of domain specific knowledge: The domain or even subject specific vocabulary in scientific literature poses challenges for LLM-based analysis.LLMs are trained on vast corpuses of which the subject specific matter is only a small portion.In agri-food transitions and co-ordinated responses to crises research litterature terms have very specific meanings and LLMs would miss the nuances of these words.We mitigated this problem by providing the LLMs with definitions of subject specific vocabulary as part of the prompts.</p>
<p>Hallucinations: It is well known that LLMs can 'make up' information [17].To manage this, tasks were broken into two steps.First, the Large Language Model was asked to find quotes that provide evidence of the desired information.Binary (yes/no) verification of the quotes failed because of trivial errors related to unicode characters.Therefore quotes were instead verified using fuzzy text matching using thefuzz implementation6 of the Levenshtein distance metric [20].The LLM was then given the quotes and asked for a final answer.</p>
<p>Semantic Text Highlighting</p>
<p>In this section we introduce a method for semantic text highlighting, or highlighting for short.The idea of semantic highlighting is not new and was originally formulated for information retrieval [16].In our context we apply highlighting to LLM retrieved evidence.Because the amount of retrieved evidence can still be overwhelming, further support is needed to aid a human reviewer.This is where highlighting comes in.</p>
<p>Like with a text marker on paper, highlighting does not need to be perfect to be useful.However, the highlighted words should be semantically related to a given specific SLR research question.Our method requires a set of keywords representing the research question.Highlighting then boils down to determining which words in a given sentence are semantically related to the keywords and conveying the findings in a useful way.For that, we rely on readily available information-theoretic similarity measures and a carefully curated corpus of English, which requires no training and is explainable.</p>
<p>Our method works as follows.The SLR researcher provides sets of keywords comprising of entities E, relations R and properties P as nouns, (possibly transitive) verbs and adjectives or adverbs, respectively.For example, in our first SLR case study on the topic influences of agri-food transitions on health outcomes the following were used:</p>
<p>Entities E: health, disease, outcome, food, lifestyle Relations R: explain, affect, improve, stimulate Properties P : environmental A word w in a given text is highlighted if it is deemed related to a keyword according to the procedure Similarity(w, C, t) in Algorithm 1, where C = E ∪R∪ P .The given text is first parsed with the part-of-speech (POS) parser SpaCy [15].It assigns a grammatic role t to every word w for determining the best suited subset of C for similarity.The algorithm computes two similarity scores between w and that subset in the range [0, 1].One of them is word vector similarity, as readily provided by spaCy, and the other is Wu-Palmer similarity [40,34] based on hypernym-reachability in WordNet [12].</p>
<p>Using WordNet for word similarity is an established and well-researched topic [23].In our algorithm, the search for similar words is broad and includes (onestep) synonyms, pertainyms, and related derived forms, weighted for each category.This was a design choice motivated by the case study in Section 4.2, where the researcher wants to ensure they did not miss any key points in their manual review.Similarity takes scores higher than given thresholds to determine whether w should be highlighted or not.We found that Wu-Palmer similarity often produces results more similar to what a human user expects from a highlighter.Hence, vector similarity acts only as a fall-back.Here are some examples for highlighted evidence text with the keywords above.
R ← ∅ // Result relation for s ∈ S do R ← R ∪ {s 1.0 −→ s} // R is reflexive for l ∈ WN-Lemmas(s) do R ← R ∪ {s P-Weight −→ WN-SynSet(v) | v ∈ WN-Pertainyms(l)} R ← R ∪ {s RF-Weight −→ WN-SynSet(v) | v ∈ WN-RelatedForms(l)} return R
2. Foodborne illnesses significantly influence individuals nutritional status.</p>
<ol>
<li>
<p>Changing lifestyles, mainly due to work commitment, have fuelled the increase in numbers eating out and the need for convenience foods.</p>
</li>
<li>
<p>Significant changes have occurred in food systems in the last decades that have contributed to widen such 'holes' in the barriers from phase to phase: agricultural intensification and industrialization causing major environmental deterioration, the increasing distance traveled by food in global markets, and the nutrition transition towards diets rich in ultra -processed food and animal protein are the three cornerstones of such changes.</p>
</li>
</ol>
<p>Entities (nouns, noun chunks) are colored red and relations (verbs) are colored blue.Additional colors are used for supporting words according to their grammatical roles.Properties (adjectives) of colored entities are purple.</p>
<p>For each word, the algorithm can provide an explanation of why it is highlighted.These explanations are helpful for customising parameter settings; for example, we get: In these annotations, NCP means 'NounChunkPart', and the similarity of the highlighted word(s) to keyword(s) is indicated as in SimilarTo(keyword, similarity, kind), where 'wup' is Wu-Palmer similarity.</p>
<p>Highlighting can be useful beyond marking up text excerpts.In one of our case studies below we take the 'highlighting rate' as a statistical measure to assess the similarity between the LLM's response and the researcher's benchmark evaluation.</p>
<p>Results</p>
<p>Case Study 1: Similarity Metrics in Agri-Food Transition SLR</p>
<p>In this first case study we determine the accuracy of the chosen metrics and explore if there is a significant difference in performance between GPT3 and GPT4.There were eight tasks (as shown in the first column of Table 1) and ten papers.GPT3 and GPT4 were given identical instructions to answer each task in separate calls.We compared the researcher's and models' answers.The models were asked to record three quotes (evidences), then give a final answer in a second call.Cases where the average fuzzy string similarity was less than 90% were manually investigated.There were 25 and 38 cases where this occurred for GPT4 and GPT3 respectively, resulting in overall error rates of 2% and 5%.</p>
<p>The semantic similarity between LLM and expert answers was calculated using transformer similarity and SpaCy similarity.An example of an LLM/expert answer could be "The Global context is Africa" which has a word count of 5.The correlation between the Transformer similarity score and the expert's judgement of the model answers was 0.48 ± 0.09 and there was almost no correlation (−0.07 ± 0.08) between the SpaCy similarity and expert judgment.</p>
<p>Case Study 2: Impact of Evidence on Coordinated Response to</p>
<p>Crisis SLR</p>
<p>In this case study we compare LLM output based on two methods.In the first method ('evidence') the LLM first obtains quotes to support its answer to the question and then writes it's answer.In the second method ('direct') the LLM writes an answer without searching for or providing evidence.</p>
<p>Providing evidence is a good way to increase the trustworthiness of LLM responses.However it will increase the number of completion tokens and therefore cost.We test our highlighting algorithm as an automated similarity metric by calculating the correlation between the highlighted fraction of each expert answer and model answers.</p>
<p>Table 2 shows that the evidence method results in slightly lower SpaCy Semantic Score, vector embedding cosine similarity, human judged accuracy and highlighting correlation.The highlighting correlation score changes by a more significant margin, but also has a greater uncertainty than the other measures.The number of prompt tokens is not significantly changed by asking the model for evidence, but the number of completion tokens increases fourfold.As the majority of tokens are prompt tokens, it might be expected that the number of completion tokens would have a small impact on the overall cost.However the computational cost of running a transformer in this case is proportional to the number of completion tokens.A technique to more effectively reduce the cost using an LLM for literature review is grouping multiple tasks into a single call as explored in the next case study.</p>
<p>Case Study 3: Impact of Combining Tasks on Sustainable</p>
<p>Transitions SLR</p>
<p>In this case study, we compare conditions that we name 'separate' and 'together'.</p>
<p>For the separate condition, GPT4 is called ten times.In each call the paper is provided and the LLM is asked to find an answer and evidence for a research question.In the together condition, the model is given all tasks in one call.The results for both experimental conditions are in Table 3.Note that papers were classified into three groups according to their geographical scale as requested by the researcher.There were twenty papers in each category.The researcher did not like the results of the together condition because GPT4 jumbled its responses resulting in a frame-shift.Despite the near tenfold increase in prompt tokens and cost, they preferred the separate condition.In the separate condition the model had no awareness of its answers to the other questions resulting in overlap of the ideas presented for each sub-question.</p>
<p>Case Study 4: Prompt Variations for Automated Screening</p>
<p>In the final case study the researcher wanted to automatically extract information from papers for the purpose of screening.The researcher had manually reviewed 14 papers from the arXiv and 20 from Pubmed of which 12 and 3 were relevant respectively.These were used as datasets to measure the performance of LLM information extraction.These datasets provide opposite extremes, one where the LLM needs to accept nearly all of the papers as relevant and another where it needs to reject nearly all the papers.This allowed for a study in simultaneously avoiding false positives and false negatives, see Table 4. Three prompts were used: 'prompt relevant' which indicated that the paper was likely to be relevant, 'prompt irrelevant', and a 'neutral prompt' which makes no indication of the paper's relevance.Table 4. Case Study 4: SpaCy and transformer similarity, expert verified accuracy, false positive and false negative rates for three different prompts on both datasets.The correlation between expert accuracy and the similarities were calculated to measure the quality of these metrics.The false positive and false negative rates in Table 4 show the LLM was likely to state that a paper was relevant when in fact it was not.The correlations show transformer similarity correlated better with expert review than SpaCy similarity.It was found that SpaCy similary is heavily impacted by changes in capitalisation.In some cases changes in capitalisation alone reduced the similarity to 0.3.</p>
<p>Discussion</p>
<p>In the first case study, GPT4 made less mistakes than GPT3 when extracting exact quotes from documents.This may be because of the quality of the model or GPT3's shorter maximum context length.The longest context length available for GPT3 models was 16,000 tokens which was not always sufficient to read an entire paper.When the paper was broken into multiple sections the model was less likely to find relevant quotes.We noticed that GPT3 was more likely to select quotes from the beginning of the context window than GPT4, whose quotes were more evenly distributed from the entire paper.</p>
<p>Across all studies it was found that transformer similarity correlates more strongly with expert opinion than than SpaCy similarity.This indicates that transformer embeddings are a better metric.This is expected given that it is able to take the positions of words into account.The correlation between amounts of text highlighted increased with increased human judged accuracy, showing the correct trend as a measure of semantic similarity.However the uncertainty values were very high and more work would need to be performed to determine if this is a suitable metric.</p>
<p>The highlighting tool's primary purpose is to aid a researcher in sifting through evidence and other LLM response text.The researchers anecdotally confirmed its value.</p>
<p>We originally anticipated that SpaCy's similarity would be a better measure of semantic similarity than transformer similarity because the transformer similarity scores only ranged between 0.7 and 1, while SpaCy similarities scores had much larger ranges.Contrary to our original expectations, transformer similarity ubiquitously correlated more strongly with expert opinion.To make transformer similarity more interpretable for humans we recommend scaling these values before interpretation.This is because a transformer similarity of 0.8 is actually low, despite normal human expectations.</p>
<p>The second case study found that when a model was asked to provide evidence for its claims, it was slightly less accurate on all metrics including expert judgement.This is an unexpected result as normally 'chain of thought' or asking a model to explain its reasoning improves performance [39,25].As trustworthiness is increased when the model provides verifiable evidence for its answers, this result indicates that there is an unfortunate trade off between accuracy and trustworthiness.The reason could be that the model is 'overloaded' when it needs to focus on multiple tasks at once.This was confirmed by the third case study (Section 4.3) where model performance also decreased when the number of tasks it was asked to complete in a single call increased.</p>
<p>In third third case study, as expected the number of prompt tokens is approximately ten times higher for the separate condition compared to the together condition.As far as the researcher was concerned the reduced number of errors in the results was worth the the extra cost.The major issue with the together condition was the possibility for tasks to be jumbled.One area for future work is to apply more advanced parsing techniques to avoid frameshift errors and therefore make the computationally cheaper technique more desirable for researchers.Another area for future work is to provide the model with specific keywords for each call to guide the LLM responses.</p>
<p>The fourth case study demonstrated that SpaCy semantic similarity can be heavily impacted by the capitalisation of words for medium sized models.This is not a desirable property for a system measuring semantic similarity; we argue that writing a sentence in all capital letters makes little change to the semantic meaning.</p>
<p>Readers may be tempted to think that scores such as SpaCy or transformer similarity could be used as better alternatives to subject expert review as they do not contain human biases.However, one needs to be careful in assuming that there is no bias when using metrics like these.There can be an illusion of objectivity, when in fact these models have been trained and validated on data which contains significant unknown biases.In this study, we highly value the opinion of experts who are part of the active research community and have observed that they demonstrate a strong awareness of their own biases.</p>
<p>False positive rates were consistently higher than false negative rates; GPT4 was more likely to think that an irrelevant paper was in fact relevant and most accurately screened papers when prompted to expect that papers may be irrelevant.</p>
<p>Overall GPT3 and GPT4 were able to find and correctly reproduce quotes from a text with 95% and 98% accuracy respectively.For low complexity tasks like finding the title or location of a paper, GPT4 performed with close to 100% accuracy, but accuracy was lower for more nuanced tasks such as identifying enablers in agri-food transitions.The overall approximate average accuracy of GPT4 in answering research questions was 83%.</p>
<p>Our highlighting workflow requires keyword calibration to determine a suitable set of keywords for each research question in an SLR.Currently, keyword calibration is semi-automated process with an expert in the loop.The expert proposes keywords, runs highlighting experiments on sample papers and adjusts the set of keywords according to their observations.</p>
<p>From our experiments we were able to derive some guidelines for calibration.A keywords set yielding a highlighting rate of around 0.4 ± 0.1 on evidence texts often seems to be a good compromise.If it is much higher, often too many irrelevant words are highlighted.If much lower, the domain has not been covered sufficiently and relevant keywords are missing.It is better to use evidence text than expert answers for keyword calibration as they are almost always proper English sentences.Expert assessments however can vary widely and sometimes are just lists of keywords.Hence the proportion of highlighted words is less reliable in this case, resulting in a less meaningful hit rate.</p>
<p>If the hit rate is unusually high this could be because of denser writing (less filler words) or because of denser information.The latter includes the possibility that surprising, additional insights have been unveiled.This helps to get a more complete picture of the problem.Conversely, we found that a much lower highlighting rate typically applies to irrelevant texts.</p>
<p>Conclusion</p>
<p>Large Language Models (LLMs) were used to assist interdisciplinary system scientists to conduct four Systematic Literature Reviews (SLR).The topics of the reviews were agri-food system transitions, coordinated responses to crises, sustainable transitions and automated marking.GPT-3.5 Turbo and GPT-4 Turbo had error rates of 5% and 2% when extracting exact quotes from research papers.Levenshtein distance accurately determined the faithfulness of quotes produced by LLMs and was robust to unexpected unicode characters and hyphenations.</p>
<p>When GPT-4 Turbo completed multiple tasks in a single prompt the number of prompt tokens decreased tenfold but with significant losses in accuracy in some cases due to frameshift errors.One area for future work would be to use more advanced parsing techniques to avoid frameshift or other 'jumbling' errors.</p>
<p>The accuracy of the models' answers was found to decrease with complexity of the task.For very simple tasks, expert rating of LLM answer correctness was close to 100% while for highly nuanced tasks it could be as low as 10%.On average it was found that GPT-4 Turbo was able to extract information from papers with approximately 83% accuracy.When screening papers it was found that GPT-4 Turbo was more likely to include irrelevant papers than exclude relevant papers.One area for future work is to provide the model with specific keywords to focus its answers, this may help a model focus on the desired ideas while trying to complete a nuanced task.</p>
<p>It was found that taking the cosine similarity of transformer embeddings of expert and LLM answers was a measure of accuracy that correlated more strongly with expert opinion than SpaCy's semantic similarity score.Nearly all of these transformer embedding cosine scores were in the range of 0.7 to 0.95.In order to make these cosine similarities more human interpretable, we recommend scaling them to take up the full range between 0 and 1.An area of future work would be to use cosine similarity of transformer embeddings for sentence-wise comparison of researcher and LLM answers in order to determine if any important pieces of information are missing.</p>
<p>Although highlighting is designed to assist researchers with manual checking of answers, correlation between amounts of highlighted text is showing some promise as an automated method for measuring the quality of LLM responses.Additional research would need to be conducted to see if this can be used as a valid similarity metric.Another idea for future work is to re-formulate the semantic similarity algorithm (Algorithm 1) with probabilistic logic programming.This would allow for a more flexible and expressive framework.In addition, weight parameters could be rephrased as probabilities and be learned from examples by maximum likelihood estimation.</p>
<p>Algorithm 1
1
Similarity Similarity(w, C, t) Input: w word, C keywords in canonical form (lemmas), t type of w (noun, verb, adjective ...) Output: Similarity score for w P-Weight ← 0.95 RF-Weight ← 0.95 WUP-Threshold ← 0.8 VEC-Threshold ← 0.95 bestwup ← max c∈C WUP-X(w, c, t) or else 0.0 bestvec ← max c∈C VEC(w, c) or else 0.0 if bestwup ≥ WUP-Threshold and bestwup ≥ bestvec then return bestwup elif bestvec ≥ VEC-Threshold then return bestvec else return 0.0 WUP-X(w, c, t) // Extended Wu-Palmer similarity, considers reachable words from w and c Sw = Extend(WN-Synsets(w, t)) // WN-Synsets returns synonyms of w Sc = Extend(WN-Synsets(c, t)) return max (sw ωw −→rw ,sc ωc −→rc)∈Sw ×Sc ωw • ωc • WN-WUP(rw, rc, t) // Wu-Palmer from WordNet VEC(w, c) // Vector similarity if w = c then return 1.0 elif both w and c have vector embeddings then return cosine-similarity of the embeddings of w and of c else return 0.0 Extend(S, t) Input: S a set of WordNet synsets Output: Weighted extension of S by pertainyms and derivationally related forms</p>
<p>2 .
2
Foodborne illnesses (NCP(Foodborne illnesses, [SimilarTo('disease', 0.95, 'wup')])) significantly influence (SimilarTo('affect, 0.84, 'wup')) individuals nutritional status (NCP(nutritional status, [SimilarTo('food', 0.91, 'wup')])).</p>
<p>Table 1 .
1
Case Study 1: Average and standard deviation for similarity scores, fuzzy matching scores, average word count and expert judged model accuracy are presented for GPT3 and GPT4.
Information Complexity ModelQuote Fuzzy Matching ScoreModel Average Word CountExpert Average Word CountTransformer SimilaritySpaCy SimilarityModel AccuracyGlobal ContextLowGPT-GPT-97 ± 5 98 ± 63.5 5.24.8 4.80.74 ± 0.25 0.58 ± 0.16 0.84 ± 0.06 0.57 ± 0.140.9 1.0Associated Health FocusLowGPT-GPT-97 ± 5 98 ± 45.6 71.6 1.60.82 ± 0.13 0.57 ± 0.16 0.75 0.85 ± 0.05 0.60 ± 0.14 0.94Transition PathwayGPT-Moderate GPT-95 ± 10 97 ± 813.4 235.4 5.40.81 ± 0.04 0.66 ± 0.08 0.65 0.85 ± 0.06 0.65 ± 0.21 1.0Agri-food BoundaryGPT-Moderate GPT-97 ± 4 98 ± 632 50.617 170.83 ± 0.04 0.77 ± 0.14 0.87 ± 0.03 0.79 ± 0.12 0.85 0.5Public Health RiskGPT-Moderate GPT-99 ± 7 97 ± 68.8 20.56.5 6.50.85 ± 0.05 0.59 ± 0.16 0.87 ± 0.06 0.74 ± 0.17 0.95 0.7SynergiesHighGPT-GPT-97 ± 5 98 ± 531.3 5826.4 26.40.83 ± 0.03 0.84 ± 0.07 0.25 0.81 ± 0.05 0.83 ± 0.07 0.1ConstraintsHighGPT-GPT-97 ± 5 98 ± 535 5918 180.82 ± 0.02 0.81 ± 0.08 0.44 0.84 ± 0.02 0.83 ± 0.07 1.0Integrated SolutionsHighGPT-GPT-97 ± 4 99 ± 328 5030 300.89 ± 0.04 0.90 ± 0.07 0.88 0.89 ± 0.05 0.89 ± 0.07 1.0</p>
<p>Table 2 .
2
Case Study 2: Prompt tokens, completion tokens, SpaCy similarity, transformer similarity, human judged accuracy and Highlighting Correlation are presented for comparison of direct and evidence-based conditions.
Experimental Prompt Tokens CompletionSpaCyTransformer Human Judged HighlightingCondition(×10 3 )TokensSimilaritySimilarityAccuracyCorrelationEvidence20.2 ± 5.7 842 ± 397 0.85 ± 0.06 0.87 ± 0.0669%−0.18 ± 0.24Direct20.0 ± 5.7 213 ± 64 0.88 ± 0.06 0.90 ± 0.0472%0.13 ± 0.25</p>
<p>Table 3 .
3
Case Study 3: Quote and final answer metrics for both conditions.The expert found significant errors in the first paper from the global scale responses for the together condition.They decided that the together method was not worth pursuing and did not evaluate the remaining together responses.
Experimental ConditionScale of paperPrompt Tokens ×10 5Completion Tokens ×10 3Fuzzy Text MatchingSpaCy SimilarityTransformer SimilarityInstances of failing to find quotesExpert Judged AccuracyGlobal1.1 ± 0.21.5 ± 0.2 98.4 ± 2.5 0.84 ± 0.4 0.85 ± 0.01082%SeparateInternational /national4.7 ± 1.71.3 ± 0.1 95.3 ± 6.8 0.79 ± 0.12 0.86 ± 0.01584%Subnational /Local2.2 ± 0.71.4 ± 0.2 99.4 ± 0.6 0.83 ± 0.03 0.84 ± 0.06275%Global0.12 ± 0.02 0.97 ± 0.37 99.5 ± 0.4 0.79 ± 0.5 0.83 ± 0.030N/ATogetherInternational /national0.48 ± 0.16 1.3 ± 0.2 93.1 ± 7.2 0.77 ± 0.16 0.84 ± 0.019N/ASubnational /Local0.22 ± 0.07 0.8 ± 0.2 98.0 ± 1.0 0.81 ± 0.03 0.84 ± 0.011N/A
https://www.xpdfreader.com/pdftotext-man.html
https://github.com/seatgeek/thefuzz
. It is also likely that climate change will contribute to novel occurrences of disease emergence and transmission.
Acknowledgements.This research was supported by funding from CSIRO Data61 and its Valuing Sustainability Future Science Platform initiative.We thank Enayat Moallemi for providing knowledge, expertise and data for the third case study.We thank Stephen Wan and Shima Khanehzar for helpful discussions.
LitLLM: A Toolkit for Scientific Literature Review. S Agarwal, I H Laradji, L Charlin, C Pal, 2024</p>
<p>Accelerating research processes with Scopus AI: A place branding case study. E Aguilera-Cora, C Lopezosa, J Fernández-Cavia, L Codina, 10.21555/rpc.v6i1.3088Revista Panamericana de Comunicación. 612024</p>
<p>Using LLM (Large Language Model) to Improve Efficiency in Literature Review for Undergraduate Research. S A Antu, H Chen, C K Richards, 2023WS on Empowering Education with LLMs</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, 10.1145/3442188.3445922Proc. FAccT '21. FAccT '21ACM2021</p>
<p>Language (Technology) is Power: A Critical Survey of "Bias" in NLP. S L Blodgett, S Barocas, Iii Daumé, H Wallach, H , 10.18653/v1/2020.acl-main.485Proc. 58th Annual Meeting of the ACL. ACL. 58th Annual Meeting of the ACL. ACL2020</p>
<p>F Bolanos, A Salatino, F Osborne, E Motta, arXiv:2402.08565Artificial Intelligence for Literature Reviews: Opportunities and Challenges. 2024</p>
<p>. S Brody, 10.5195/jmla.2021.1331Scite. Journal of the Medical Library Association. 10942021</p>
<p>Language Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Proc. NeurIPS. NeurIPS2020</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, W Ye, Y Zhang, Y Chang, P S Yu, Q Yang, X Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 1532024</p>
<p>CINTEL. Collaborative Intelligence Future Science Platform. 2024</p>
<p>De Silva, A Wijekoon, J L Liyanarachchi, R Panchendrarajan, R Rajapaksha, W , arXiv:2403.03293AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis. 2024</p>
<p>WordNet: An electronic lexical database. C Fellbaum, 1998Bradford Books</p>
<p>Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population. R A Fisher, 10.2307/2331838Biometrika. 1041915</p>
<p>M Honnibal, I Montani, Linguistic Features • spaCy Usage Documentation. </p>
<p>spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. M Honnibal, I Montani, 2017</p>
<p>Semantic highlighting. A Hussam, B Ford, J Hyde, A Merayyan, B Plummer, T Anderson, 10.1145/286498.286667CHI 98 Conference Summary on Human Factors in Computing Systems. CHI '98. ACM1998</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, 10.1145/3571730ACM Comput. Surv. 55122023</p>
<p>Journal of the Canadian Health Libraries Association. J Kung, 10.29173/jchla29657Journal de l'Association des bibliothèques de la santé du Canada. 4412023Elicit (product review)</p>
<p>Contextualized Embeddings based Transformer Encoder for Sentence Similarity Modeling in Answer Selection Task. M T R Laskar, J X Huang, E Hoque, Proc. Twelfth LREC. ELRA. Twelfth LREC. ELRA2020</p>
<p>Binary codes capable of correcting deletions, insertions and reversals. V I Levenshtein, Soviet Physics Doklady. 107071966</p>
<p>Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting. T O Li, W Zong, Y Wang, H Tian, Y Wang, S C Cheung, J Kramer, 10.1109/ASE56229.2023.0008938th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2023. 2023</p>
<p>Y Li, L Chen, A Liu, K Yu, L Wen, arXiv:2403.02574ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. 2024</p>
<p>Fuzzy word similarity: A semantic approach using WordNet. S Manna, B S U Mendis, 10.1109/FUZZY.2010.5584785International Conference on Fuzzy Systems. Barcelona, SpainIEEE2010</p>
<p>The application of large language models in medicine: A scoping review. X Meng, X Yan, K Zhang, D Liu, X Cui, Y Yang, M Zhang, C Cao, J Wang, X Wang, J Gao, Y G S Wang, J Ji, Z Qiu, M Li, C Qian, T Guo, S Ma, Z Wang, Z Guo, Y Lei, C Shao, W Wang, H Fan, Y D Tang, 10.1016/j.isci.2024.109713iScience. 2752024</p>
<p>Automated Theorem Provers Help Improve Large Language Model Reasoning. L Mcginness, P Baumgartner, 10.29007/2n9mProceedings of 25th Conference on Logic for Programming. N Bjørner, M Heule, A Voronkov, 25th Conference on Logic for ProgrammingEasyChair2024100</p>
<p>Artificial intelligence and illusions of understanding in scientific research. L Messeri, M J Crockett, 10.1038/s41586-024-07146-0Nature. 6272024</p>
<p>Entry points for accelerating transitions towards a more sustainable future. E Moallemi, M Miller, K Szetey, S Chakori, J Palmer, M Battaglia, B A Bryan, L Gao, A Hall, P Leith, R Raven, P M Reed, 10.31223/X5C68X2024EarthArXiv pre-print</p>
<p>Why scientists trust AI too much -and what to do about it. 10.1038/d41586-024-00639-yNature Editorial. 62780032024Nature</p>
<p>Predicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis. M Ormerod, J Martínez Del Rincón, B Devereux, JMIR Medical Informatics. 95e230992021</p>
<p>Changing the World by Changing the Data. A Rogers, 10.18653/v1/2021.acl-long.170Proc. 59th Annual Meeting of the ACL and the 11th IJCNLP. ACL. 59th Annual Meeting of the ACL and the 11th IJCNLP. ACL2021</p>
<p>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, K.-K Kemell, M Waseem, T Kilamo, M Saari, A N Duc, K Systä, P Abrahamsson, 2024</p>
<p>Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success). C Shaib, M Li, S Joseph, I Marshall, J J Li, B Wallace, Proc. 61st Annual Meeting of the ACL (Short Papers). 61st Annual Meeting of the ACL (Short Papers)ACL2023</p>
<p>A New Similarity measure for taxonomy based on edge counting. M Shenoy, 10.5121/ijwest.2012.3403International journal of Web &amp; Semantic Technology. 342012</p>
<p>Reviews and Reviewing: Approaches to Research Synthesis. An Annual Review of Information Science and Technology (ARIST) paper. L C Smith, 10.1002/asi.24851Journal of the ASIS&amp;T. 7532024</p>
<p>Human-AI Collaboration to Identify Literature for Evidence Synthesis. S Spillias, P Tuohy, M Andreotta, R Annand-Jones, F Boschetti, C Cvitanovic, J Duggan, E Fulton, D Karcher, C Paris, R Shellock, R Trebilco, 10.21203/rs.3.rs-3099291/v1Research Square. 2023</p>
<p>Artificial intelligence to automate the systematic review of scientific literature. J De La Torre-López, A Ramírez, J R Romero, 10.1007/s00607-023-01181-xComputing. 105102023</p>
<p>Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings. J Turton, R E Smith, D Vinson, 10.18653/v1/2021.repl4nlp-1.26Proc. RepL4NLP-2021. ACL. RepL4NLP-2021. ACL2021</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 35NeurIPS 2022. 2022</p>
<p>Verb Semantics and Lexical Selection. Z Wu, M Palmer, 10.3115/981732.98175132nd Annual Meeting of the ACL. ACL. 1994</p>
<p>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis. A Ye, A Maiti, M Schmidt, S J Pedersen, 10.3390/fi16050167Future Internet. 1651672024</p>
<p>Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents. Y Zou, H Liu, T Gui, J Wang, Q Zhang, M Tang, H Li, D Wang, Findings of the ACL. ACL. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1398 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1398</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1398</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-c4744a7c2bb298e4a52289a1e085c71cc3d37bc6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6" target="_blank">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.</p>
                <p><strong>Paper Abstract:</strong> Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1398.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1398.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-XL (Attentive Language Models Beyond a Fixed-Length Context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based autoregressive language model that introduces segment-level recurrence (state reuse as a memory) and a novel relative positional encoding to model very long-term dependency, resolve context fragmentation, and speed up evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based predictive model for autoregressive next-token prediction. Key mechanisms: (1) segment-level recurrence where hidden states from previous segments are cached and concatenated (with stop-gradient) to form a memory for keys/values; (2) relative positional encodings injected into attention computation via a reparameterized attention score (separate W_k,E and W_k,R, sinusoid R, and trainable u and v bias vectors). Uses masked self-attention and standard feed-forward layers to predict next tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive model (neural simulator / autoregressive sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>natural language modeling and text generation (word-level and character-level language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perplexity (PPL) on word-level benchmarks; bits-per-character (bpc) on character-level benchmarks; Relative Effective Context Length (RECL) for context-usage/fidelity of long-range dependency.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported state-of-the-art results in the paper: enwik8 bpc = 0.99 (24-layer Transformer-XL), text8 bpc = 1.08, WikiText-103 PPL = 18.3 (Large), One Billion Word PPL = 21.8 (Large), Penn Treebank PPL = 54.52 (no finetuning). RECL: Transformer-XL (151M) achieves RECL = 900 words at r=0.1 (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not presented as an interpretable/world-model; model is a neural black box (self-attention weights exist but the paper does not claim or demonstrate explicit interpretability of world states).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model sizes reported: standard ~151M parameters; other variants: 41M, 88M, 257M, 277M depending on depth/width. Training attention length examples: training attention length set to e.g. 384 (WikiText-103) and 784 (enwik8); evaluation attention lengths increased (e.g. 1,600; 3,800). Caching memory increases GPU memory usage (recurrence costs additional memory). Exact training GPU/time budget not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>During evaluation, due to state reuse, Transformer-XL is up to ~1,800x faster than the vanilla fixed-length Transformer (Table 9: e.g., 1,874x slower for Al-Rfou et al. vanilla Transformer at attn len 3,800). Also achieves similar or better performance with far fewer parameters than some deep vanilla Transformers (paper claims e.g. same result as 64-layer vanilla Transformer while using ~17% of parameter budget in one comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Substantial improvements on language-modeling tasks: new SoTA PPL/bpc numbers (see fidelity_performance). The model is also able to generate reasonably coherent long text articles with thousands of tokens when trained on WikiText-103 (qualitative generation capability reported).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Improved fidelity (lower PPL / lower bpc) translates to better predictive performance and qualitatively more coherent long-form generation; recurrence specifically resolves context fragmentation and helps even datasets without long-range dependency (One Billion Word) by improving short-term prediction. The paper argues task-relevant utility is improved (both short and long sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Recurrence + state caching increases effective context length and yields large evaluation speedups but incurs extra memory usage. Relative positional encoding is required to enable state reuse; without it state reuse causes temporal confusion and performance drop. Increasing evaluation attention length yields gains only when using the proposed relative encoding. Some improvements require larger evaluation memory even if backpropagation/training length is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key design choices include: segment-level recurrence with cached previous-segment hidden states (stop-gradient to limit backprop), relative sinusoid positional encodings R_{i-j}, separate weight matrices W_{k,E} and W_{k,R} for content and location keys, trainable global bias vectors u and v, caching memory length M (set equal to segment length in training and increased at evaluation), masked softmax autoregressive attention. These choices trade off memory for longer effective context and evaluation speed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to vanilla Transformer (Al-Rfou et al. 2018): Transformer-XL achieves lower perplexity/bpc and is far faster at evaluation with large contexts (up to ~1,800x). Compared to RNNs/LSTMs: achieves substantially longer effective context (RECL improvements reported: ~80% longer than RNNs and ~450% longer than vanilla Transformer per abstract and Table 8), and yields better perplexity on standard benchmarks. Compared to very large vanilla Transformer variants, Transformer-XL attains comparable or superior results with fewer parameters in some settings (paper reports matching a 64-layer vanilla Transformer's result with ~17% of its parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>The paper's experimental recommendation is to combine segment-level recurrence with the proposed relative positional encoding; set memory length M equal to segment length during training and increase during evaluation to extend effective context; larger evaluation attention lengths improve performance when using the proposed encoding. No single numerical 'optimal' is prescribed beyond these guidelines and empirical hyperparameter choices reported in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1398.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1398.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla Transformer (fixed-length)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer / vanilla Transformer language model (fixed-length segments, Al-Rfou et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard Transformer-based language model trained on fixed-length segments without state reuse across segments; positional information provided by absolute positional encodings added to token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vanilla Transformer (fixed-length segment LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-attention based autoregressive model applied to fixed-length segments; uses absolute positional encodings added to input embeddings; no segment-level recurrence or memory reuse across segments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive model (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>natural language modeling (character and word-level)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perplexity (PPL) and bits-per-character (bpc)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported baselines in paper: e.g., Al-Rfou et al. 12L Transformer bpc = 1.11 on enwik8; 64L Transformer bpc = 1.06 in Al-Rfou et al. (cited). In the paper's comparisons the vanilla Transformer variants generally have higher perplexity / bpc than Transformer-XL.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper; standard self-attention provides attention weights but no explicit interpretability analysis presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>During evaluation, recomputes full segment for each prediction; extremely expensive for long contexts — the paper reports vanilla Transformer being up to ~1,800x slower than Transformer-XL at large attention lengths (Table 9). Specific parameter counts vary (e.g., Al-Rfou 64L has ~235M parameters reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Much less efficient than Transformer-XL for long-context evaluation due to lack of state reuse; reported as orders-of-magnitude slower in per-token evaluation when large attention windows are used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Performs worse on long-range language modeling benchmarks compared to Transformer-XL (higher PPL / higher bpc).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Fixed-length training causes context fragmentation and upper bounds dependency length to segment length; can be mitigated at evaluation by shifting window and recomputing, but at huge computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity and straightforward training but limited context length, context fragmentation, and high evaluation cost for long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Absolute positional encodings added to embeddings; training on fixed-length segments; no cross-segment memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared unfavorably to Transformer-XL in both fidelity (PPL/bpc) and evaluation efficiency; compared to RNNs, vanilla Transformer can outperform RNNs on some tasks but is limited by fixed context during training unless made extremely deep.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed beyond the observation that vanilla Transformers require special handling (e.g., auxiliary losses, long depth) to partially compensate for lack of recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1398.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1398.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM / RNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network / Long Short-Term Memory (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence models that process tokens recurrently and maintain a hidden state; standard baselines for language modeling that can capture sequences but historically limited by vanishing/exploding gradients and shorter effective context in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long Short-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (RNN language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural network architecture with gated memory cells (LSTM) that updates a single hidden state sequentially and is trained with truncated BPTT for language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (latent state world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>natural language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perplexity (PPL); Effective Context Length / RECL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Example baseline numbers in paper: various LSTM-based models reported in tables (e.g., AWD-LSTM variants PPL ~55-59 on PTB in cited literature). The paper cites empirical finding (Khandelwal et al., 2018) that LSTM language models use ~200 context words on average.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper; LSTMs are treated as black-box sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Parameter counts for LSTM baselines vary (examples in tables include large models with hundreds of millions or billions of parameters in other works); RECL values in Table 8 show LSTM RECL ~400 (r=0.1) in the comparative grouping used.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Transformer-XL shows longer RECL (~80% longer than RNNs per abstract) and better/competitive perplexities; LSTMs generally have lower RECL and (in these experiments) higher perplexity than Transformer-XL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Historically strong baselines on language modeling but outperformed by Transformer-XL on the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>LSTMs capture medium-range dependencies (empirically ~200 words) but are harder to optimize for very long-term dependency; improvements in LSTMs (e.g., gated variants) exist but Transformer-XL extends effective context significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RNNs/LSTMs can be more memory-efficient per time-step but suffer from gradient issues and shorter practical context; Transformer-XL trades additional memory for much longer effective context and faster evaluation with caching.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Standard RNN/LSTM cell structure, truncated BPTT; auxiliary techniques exist in literature but are not the focus of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Transformer-XL outperforms RNN/LSTM baselines in both effective context length and many perplexity benchmarks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1398.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1398.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shaw relative pos.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-attention with relative position representations (Shaw et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior relative positional encoding method for self-attention that injects relative position information into attention; used as a comparator in the ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-attention with relative position representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Shaw et al. relative positional encodings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A formulation that modifies attention to include learned relative position embeddings between query and key positions (original Shaw formulation includes content and position terms but omits global bias terms (c) and (d) used in Transformer-XL's derivation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>relative positional encoding method for transformer attention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>natural language modeling / sequence modeling (used within self-attention architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perplexity when used in transformer LMs (as evaluated in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In ablation (WikiText-103), using Shaw et al. encoding with recurrence gave worse PPL than the Transformer-XL encoding (e.g., PPL 27.94 vs 26.77 in a smaller model setting in Table 6); also Shaw encoding generalized less well to longer evaluation attention lengths in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Shaw et al.'s implementation merges W_k and relative embeddings into a single trainable ̃R matrix (per the paper's discussion), which the authors argue reduces inductive bias; computational complexity is similar but design choices affect generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper reports that their proposed relative encoding (Transformer-XL) generalizes better to longer attention lengths and yields better performance; Shaw encoding underperforms in those generalization tests.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Inferior in the paper's ablations compared to Transformer-XL's relative encoding when combined with recurrence, as measured by PPL on WikiText-103 and One Billion Word.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shaw's method supplies relative positional information, but according to the paper loses some inductive bias (when collapsing R into a trainable matrix) and thus generalizes less well to longer memory lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Shaw's approach is simpler but in these experiments provided weaker generalization to longer contexts and slightly worse PPL; Transformer-XL's reparameterization retains sinusoid inductive bias and adds global bias terms, improving generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Shaw et al. implement relative positional encodings by merging positional multipliers into a learnable matrix; Transformer-XL instead separates W_k,E and W_k,R, uses sinusoid R (non-learnable) and adds global biases u and v.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly in ablation: Transformer-XL's encoding outperforms Shaw et al.'s variant in both generalization to longer contexts and PPL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1398.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1398.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory-augmented nets / NTM / MemNets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-augmented neural networks (e.g., Neural Turing Machines; Memory Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior architectures that augment neural nets with an external memory component and read/write mechanisms; cited as conceptually related to Transformer-XL's caching of past hidden states as memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Turing Machines / Memory Networks (memory-augmented neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that explicitly maintain an external memory matrix and use neural read/write mechanisms to store and retrieve information across long time spans; referenced as related work and conceptual antecedent to caching previous hidden states in Transformer-XL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>memory-augmented neural networks / explicit memory models</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general sequence modeling, algorithmic tasks, and long-range dependency tasks (varies by reference)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this paper; mentioned to draw conceptual connection (Transformer-XL caches hidden states and calls them memory).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Mentioned as conceptual background: Transformer-XL's cached hidden states can be viewed as a memory similar in spirit to memory-augmented networks, but Transformer-XL's mechanism is simpler (caching hidden states rather than learned read/write memory controllers).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper chooses a simpler cached-state memory (concatenation of stopped-gradient past hidden states) rather than a full read/write external memory with controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Transformer-XL's memory is analogous to memory-augmented models conceptually but implemented as cached hidden sequences per layer rather than a separate learnable external memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-attention with relative position representations <em>(Rating: 2)</em></li>
                <li>Character-level language modeling with deeper self-attention <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 2)</em></li>
                <li>Sharp nearby, fuzzy far away: How neural language models use context <em>(Rating: 2)</em></li>
                <li>Neural turing machines <em>(Rating: 1)</em></li>
                <li>Memory networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1398",
    "paper_id": "paper-c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Transformer-XL",
            "name_full": "Transformer-XL (Attentive Language Models Beyond a Fixed-Length Context)",
            "brief_description": "A transformer-based autoregressive language model that introduces segment-level recurrence (state reuse as a memory) and a novel relative positional encoding to model very long-term dependency, resolve context fragmentation, and speed up evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-XL",
            "model_description": "A transformer-based predictive model for autoregressive next-token prediction. Key mechanisms: (1) segment-level recurrence where hidden states from previous segments are cached and concatenated (with stop-gradient) to form a memory for keys/values; (2) relative positional encodings injected into attention computation via a reparameterized attention score (separate W_k,E and W_k,R, sinusoid R, and trainable u and v bias vectors). Uses masked self-attention and standard feed-forward layers to predict next tokens.",
            "model_type": "transformer-based predictive model (neural simulator / autoregressive sequence model)",
            "task_domain": "natural language modeling and text generation (word-level and character-level language modeling)",
            "fidelity_metric": "Perplexity (PPL) on word-level benchmarks; bits-per-character (bpc) on character-level benchmarks; Relative Effective Context Length (RECL) for context-usage/fidelity of long-range dependency.",
            "fidelity_performance": "Reported state-of-the-art results in the paper: enwik8 bpc = 0.99 (24-layer Transformer-XL), text8 bpc = 1.08, WikiText-103 PPL = 18.3 (Large), One Billion Word PPL = 21.8 (Large), Penn Treebank PPL = 54.52 (no finetuning). RECL: Transformer-XL (151M) achieves RECL = 900 words at r=0.1 (Table 8).",
            "interpretability_assessment": "Not presented as an interpretable/world-model; model is a neural black box (self-attention weights exist but the paper does not claim or demonstrate explicit interpretability of world states).",
            "interpretability_method": null,
            "computational_cost": "Model sizes reported: standard ~151M parameters; other variants: 41M, 88M, 257M, 277M depending on depth/width. Training attention length examples: training attention length set to e.g. 384 (WikiText-103) and 784 (enwik8); evaluation attention lengths increased (e.g. 1,600; 3,800). Caching memory increases GPU memory usage (recurrence costs additional memory). Exact training GPU/time budget not reported.",
            "efficiency_comparison": "During evaluation, due to state reuse, Transformer-XL is up to ~1,800x faster than the vanilla fixed-length Transformer (Table 9: e.g., 1,874x slower for Al-Rfou et al. vanilla Transformer at attn len 3,800). Also achieves similar or better performance with far fewer parameters than some deep vanilla Transformers (paper claims e.g. same result as 64-layer vanilla Transformer while using ~17% of parameter budget in one comparison).",
            "task_performance": "Substantial improvements on language-modeling tasks: new SoTA PPL/bpc numbers (see fidelity_performance). The model is also able to generate reasonably coherent long text articles with thousands of tokens when trained on WikiText-103 (qualitative generation capability reported).",
            "task_utility_analysis": "Improved fidelity (lower PPL / lower bpc) translates to better predictive performance and qualitatively more coherent long-form generation; recurrence specifically resolves context fragmentation and helps even datasets without long-range dependency (One Billion Word) by improving short-term prediction. The paper argues task-relevant utility is improved (both short and long sequences).",
            "tradeoffs_observed": "Recurrence + state caching increases effective context length and yields large evaluation speedups but incurs extra memory usage. Relative positional encoding is required to enable state reuse; without it state reuse causes temporal confusion and performance drop. Increasing evaluation attention length yields gains only when using the proposed relative encoding. Some improvements require larger evaluation memory even if backpropagation/training length is limited.",
            "design_choices": "Key design choices include: segment-level recurrence with cached previous-segment hidden states (stop-gradient to limit backprop), relative sinusoid positional encodings R_{i-j}, separate weight matrices W_{k,E} and W_{k,R} for content and location keys, trainable global bias vectors u and v, caching memory length M (set equal to segment length in training and increased at evaluation), masked softmax autoregressive attention. These choices trade off memory for longer effective context and evaluation speed.",
            "comparison_to_alternatives": "Compared to vanilla Transformer (Al-Rfou et al. 2018): Transformer-XL achieves lower perplexity/bpc and is far faster at evaluation with large contexts (up to ~1,800x). Compared to RNNs/LSTMs: achieves substantially longer effective context (RECL improvements reported: ~80% longer than RNNs and ~450% longer than vanilla Transformer per abstract and Table 8), and yields better perplexity on standard benchmarks. Compared to very large vanilla Transformer variants, Transformer-XL attains comparable or superior results with fewer parameters in some settings (paper reports matching a 64-layer vanilla Transformer's result with ~17% of its parameters).",
            "optimal_configuration": "The paper's experimental recommendation is to combine segment-level recurrence with the proposed relative positional encoding; set memory length M equal to segment length during training and increase during evaluation to extend effective context; larger evaluation attention lengths improve performance when using the proposed encoding. No single numerical 'optimal' is prescribed beyond these guidelines and empirical hyperparameter choices reported in experiments.",
            "uuid": "e1398.0",
            "source_info": {
                "paper_title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "Vanilla Transformer (fixed-length)",
            "name_full": "Transformer / vanilla Transformer language model (fixed-length segments, Al-Rfou et al. style)",
            "brief_description": "Standard Transformer-based language model trained on fixed-length segments without state reuse across segments; positional information provided by absolute positional encodings added to token embeddings.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "mention",
            "model_name": "Vanilla Transformer (fixed-length segment LM)",
            "model_description": "Self-attention based autoregressive model applied to fixed-length segments; uses absolute positional encodings added to input embeddings; no segment-level recurrence or memory reuse across segments.",
            "model_type": "transformer-based predictive model (autoregressive)",
            "task_domain": "natural language modeling (character and word-level)",
            "fidelity_metric": "Perplexity (PPL) and bits-per-character (bpc)",
            "fidelity_performance": "Reported baselines in paper: e.g., Al-Rfou et al. 12L Transformer bpc = 1.11 on enwik8; 64L Transformer bpc = 1.06 in Al-Rfou et al. (cited). In the paper's comparisons the vanilla Transformer variants generally have higher perplexity / bpc than Transformer-XL.",
            "interpretability_assessment": "Not discussed in this paper; standard self-attention provides attention weights but no explicit interpretability analysis presented here.",
            "interpretability_method": null,
            "computational_cost": "During evaluation, recomputes full segment for each prediction; extremely expensive for long contexts — the paper reports vanilla Transformer being up to ~1,800x slower than Transformer-XL at large attention lengths (Table 9). Specific parameter counts vary (e.g., Al-Rfou 64L has ~235M parameters reported in tables).",
            "efficiency_comparison": "Much less efficient than Transformer-XL for long-context evaluation due to lack of state reuse; reported as orders-of-magnitude slower in per-token evaluation when large attention windows are used.",
            "task_performance": "Performs worse on long-range language modeling benchmarks compared to Transformer-XL (higher PPL / higher bpc).",
            "task_utility_analysis": "Fixed-length training causes context fragmentation and upper bounds dependency length to segment length; can be mitigated at evaluation by shifting window and recomputing, but at huge computational cost.",
            "tradeoffs_observed": "Simplicity and straightforward training but limited context length, context fragmentation, and high evaluation cost for long contexts.",
            "design_choices": "Absolute positional encodings added to embeddings; training on fixed-length segments; no cross-segment memory.",
            "comparison_to_alternatives": "Compared unfavorably to Transformer-XL in both fidelity (PPL/bpc) and evaluation efficiency; compared to RNNs, vanilla Transformer can outperform RNNs on some tasks but is limited by fixed context during training unless made extremely deep.",
            "optimal_configuration": "Not discussed beyond the observation that vanilla Transformers require special handling (e.g., auxiliary losses, long depth) to partially compensate for lack of recurrence.",
            "uuid": "e1398.1",
            "source_info": {
                "paper_title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "LSTM / RNN",
            "name_full": "Recurrent Neural Network / Long Short-Term Memory (LSTM)",
            "brief_description": "Sequence models that process tokens recurrently and maintain a hidden state; standard baselines for language modeling that can capture sequences but historically limited by vanishing/exploding gradients and shorter effective context in practice.",
            "citation_title": "Long Short-Term Memory",
            "mention_or_use": "mention",
            "model_name": "LSTM (RNN language model)",
            "model_description": "Recurrent neural network architecture with gated memory cells (LSTM) that updates a single hidden state sequentially and is trained with truncated BPTT for language modeling.",
            "model_type": "recurrent neural network (latent state world model)",
            "task_domain": "natural language modeling",
            "fidelity_metric": "Perplexity (PPL); Effective Context Length / RECL",
            "fidelity_performance": "Example baseline numbers in paper: various LSTM-based models reported in tables (e.g., AWD-LSTM variants PPL ~55-59 on PTB in cited literature). The paper cites empirical finding (Khandelwal et al., 2018) that LSTM language models use ~200 context words on average.",
            "interpretability_assessment": "Not discussed in this paper; LSTMs are treated as black-box sequence models.",
            "interpretability_method": null,
            "computational_cost": "Parameter counts for LSTM baselines vary (examples in tables include large models with hundreds of millions or billions of parameters in other works); RECL values in Table 8 show LSTM RECL ~400 (r=0.1) in the comparative grouping used.",
            "efficiency_comparison": "Transformer-XL shows longer RECL (~80% longer than RNNs per abstract) and better/competitive perplexities; LSTMs generally have lower RECL and (in these experiments) higher perplexity than Transformer-XL.",
            "task_performance": "Historically strong baselines on language modeling but outperformed by Transformer-XL on the reported benchmarks.",
            "task_utility_analysis": "LSTMs capture medium-range dependencies (empirically ~200 words) but are harder to optimize for very long-term dependency; improvements in LSTMs (e.g., gated variants) exist but Transformer-XL extends effective context significantly.",
            "tradeoffs_observed": "RNNs/LSTMs can be more memory-efficient per time-step but suffer from gradient issues and shorter practical context; Transformer-XL trades additional memory for much longer effective context and faster evaluation with caching.",
            "design_choices": "Standard RNN/LSTM cell structure, truncated BPTT; auxiliary techniques exist in literature but are not the focus of this paper.",
            "comparison_to_alternatives": "Transformer-XL outperforms RNN/LSTM baselines in both effective context length and many perplexity benchmarks reported.",
            "uuid": "e1398.2",
            "source_info": {
                "paper_title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "Shaw relative pos.",
            "name_full": "Self-attention with relative position representations (Shaw et al., 2018)",
            "brief_description": "A prior relative positional encoding method for self-attention that injects relative position information into attention; used as a comparator in the ablation studies.",
            "citation_title": "Self-attention with relative position representations",
            "mention_or_use": "mention",
            "model_name": "Shaw et al. relative positional encodings",
            "model_description": "A formulation that modifies attention to include learned relative position embeddings between query and key positions (original Shaw formulation includes content and position terms but omits global bias terms (c) and (d) used in Transformer-XL's derivation).",
            "model_type": "relative positional encoding method for transformer attention",
            "task_domain": "natural language modeling / sequence modeling (used within self-attention architectures)",
            "fidelity_metric": "Perplexity when used in transformer LMs (as evaluated in ablations)",
            "fidelity_performance": "In ablation (WikiText-103), using Shaw et al. encoding with recurrence gave worse PPL than the Transformer-XL encoding (e.g., PPL 27.94 vs 26.77 in a smaller model setting in Table 6); also Shaw encoding generalized less well to longer evaluation attention lengths in these experiments.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": null,
            "computational_cost": "Shaw et al.'s implementation merges W_k and relative embeddings into a single trainable ̃R matrix (per the paper's discussion), which the authors argue reduces inductive bias; computational complexity is similar but design choices affect generalization.",
            "efficiency_comparison": "Paper reports that their proposed relative encoding (Transformer-XL) generalizes better to longer attention lengths and yields better performance; Shaw encoding underperforms in those generalization tests.",
            "task_performance": "Inferior in the paper's ablations compared to Transformer-XL's relative encoding when combined with recurrence, as measured by PPL on WikiText-103 and One Billion Word.",
            "task_utility_analysis": "Shaw's method supplies relative positional information, but according to the paper loses some inductive bias (when collapsing R into a trainable matrix) and thus generalizes less well to longer memory lengths.",
            "tradeoffs_observed": "Shaw's approach is simpler but in these experiments provided weaker generalization to longer contexts and slightly worse PPL; Transformer-XL's reparameterization retains sinusoid inductive bias and adds global bias terms, improving generalization.",
            "design_choices": "Shaw et al. implement relative positional encodings by merging positional multipliers into a learnable matrix; Transformer-XL instead separates W_k,E and W_k,R, uses sinusoid R (non-learnable) and adds global biases u and v.",
            "comparison_to_alternatives": "Compared directly in ablation: Transformer-XL's encoding outperforms Shaw et al.'s variant in both generalization to longer contexts and PPL.",
            "uuid": "e1398.3",
            "source_info": {
                "paper_title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "Memory-augmented nets / NTM / MemNets",
            "name_full": "Memory-augmented neural networks (e.g., Neural Turing Machines; Memory Networks)",
            "brief_description": "Prior architectures that augment neural nets with an external memory component and read/write mechanisms; cited as conceptually related to Transformer-XL's caching of past hidden states as memory.",
            "citation_title": "Neural turing machines",
            "mention_or_use": "mention",
            "model_name": "Neural Turing Machines / Memory Networks (memory-augmented neural networks)",
            "model_description": "Models that explicitly maintain an external memory matrix and use neural read/write mechanisms to store and retrieve information across long time spans; referenced as related work and conceptual antecedent to caching previous hidden states in Transformer-XL.",
            "model_type": "memory-augmented neural networks / explicit memory models",
            "task_domain": "general sequence modeling, algorithmic tasks, and long-range dependency tasks (varies by reference)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": null,
            "interpretability_method": null,
            "computational_cost": "Not specified in this paper; mentioned to draw conceptual connection (Transformer-XL caches hidden states and calls them memory).",
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Mentioned as conceptual background: Transformer-XL's cached hidden states can be viewed as a memory similar in spirit to memory-augmented networks, but Transformer-XL's mechanism is simpler (caching hidden states rather than learned read/write memory controllers).",
            "tradeoffs_observed": null,
            "design_choices": "Paper chooses a simpler cached-state memory (concatenation of stopped-gradient past hidden states) rather than a full read/write external memory with controllers.",
            "comparison_to_alternatives": "Transformer-XL's memory is analogous to memory-augmented models conceptually but implemented as cached hidden sequences per layer rather than a separate learnable external memory module.",
            "uuid": "e1398.4",
            "source_info": {
                "paper_title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
                "publication_date_yy_mm": "2019-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-attention with relative position representations",
            "rating": 2
        },
        {
            "paper_title": "Character-level language modeling with deeper self-attention",
            "rating": 2
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 2
        },
        {
            "paper_title": "Sharp nearby, fuzzy far away: How neural language models use context",
            "rating": 2
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 1
        },
        {
            "paper_title": "Memory networks",
            "rating": 1
        }
    ],
    "cost": 0.017447499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</h1>
<p>Zihang Dai ${ }^{12}$, Zhilin Yang ${ }^{12}$, Yiming Yang ${ }^{1}$, Jaime Carbonell ${ }^{1}$, Quoc V. Le ${ }^{2}$, Ruslan Salakhutdinov ${ }^{1}$<br>${ }^{1}$ Carnegie Mellon University, ${ }^{2}$ Google Brain<br>{dzihang, zhiliny, yiming, jgc, rsalakhu}@cs.cmu.edu, qvl@google.com</p>
<h4>Abstract</h4>
<p>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is $80 \%$ longer than RNNs and $450 \%$ longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to $1,800+$ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Language modeling is among the important problems that require modeling long-term dependency, with successful applications such as unsupervised pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). However, it has been a challenge to equip neural networks with the capability to model long-term dependency in sequential data. Recurrent neural networks (RNNs), in particular Long Short-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), have been a standard solution to language modeling and obtained strong results on multiple benchmarks. Despite the wide adaption, RNNs are difficult to optimize due to gradient vanishing and explosion (Hochreiter et al., 2001), and the introduction of gating in LSTMs and the gradient clipping technique (Graves, 2013) might not be sufficient to fully address this issue. Empirically, previous work has found that LSTM language models use 200 context words on average (Khandelwal et al., 2018), indicating room for further improvement.</p>
<p>On the other hand, the direct connections between long-distance word pairs baked in attention mechanisms might ease optimization and enable the learning of long-term dependency (Bahdanau et al., 2014; Vaswani et al., 2017). Recently, Al-Rfou et al. (2018) designed a set of auxiliary losses to train deep Transformer networks for character-level language modeling, which outperform LSTMs by a large margin. Despite the success, the LM training in Al-Rfou et al. (2018) is performed on separated fixed-length segments of a few hundred characters, without any information flow across segments. As a consequence of the fixed context length, the model cannot capture any longer-term dependency beyond the predefined context length. In addition, the fixed-length segments are created by selecting a consecutive chunk of symbols without respecting the sentence or any other semantic boundary. Hence, the model lacks necessary contextual information needed to well predict the first few symbols, leading to inefficient optimization and inferior performance. We refer to this problem as context fragmentation.</p>
<p>To address the aforementioned limitations of fixed-length contexts, we propose a new architecture called Transformer-XL (meaning extra long). We introduce the notion of recurrence into our</p>
<p>deep self-attention network. In particular, instead of computing the hidden states from scratch for each new segment, we reuse the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very longterm dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, passing information from the previous segment can also resolve the problem of context fragmentation. More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.</p>
<p>Transformer-XL obtained strong results on five datasets, varying from word-level to characterlevel language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.</p>
<p>Our main technical contributions include introducing the notion of recurrence in a purely selfattentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling.</p>
<h2>2 Related Work</h2>
<p>In the last few years, the field of language modeling has witnessed many significant advances, including but not limited to devising novel architectures to better encode the context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), improving regularization and optimization algorithms (Gal and Ghahramani, 2016) , speeding up the Softmax computation (Grave et al., 2016a) , and enriching the output distribution family (Yang et al., 2017).</p>
<p>To capture the long-range context in language modeling, a line of work directly feeds a representation of the wider context into the network
as an additional input. Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017).</p>
<p>More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from them, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency.</p>
<h2>3 Model</h2>
<p>Given a corpus of tokens $\mathbf{x}=\left(x_{1}, \ldots, x_{T}\right)$, the task of language modeling is to estimate the joint probability $P(\mathbf{x})$, which is often auto-regressively factorized as $P(\mathbf{x})=\prod_{t} P\left(x_{t} \mid \mathbf{x}<em _t="&lt;t">{&lt;t}\right)$. With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context $\mathbf{x}</em>$ into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token.</p>
<h3>3.1 Vanilla Transformer Language Models</h3>
<p>In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation. Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network. However, this is usually infeasible with the limited resource in practice.</p>
<p>One feasible but crude approximation is to split the entire corpus into shorter segments of man-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the vanilla model with a segment length 4.
ageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments. This is the idea adopted by Al-Rfou et al. (2018). We call it the vanilla model and visualize it in Fig. 1a. Under this training paradigm, information never flows across segments in either the forward or backward pass. There are two critical limitations of using a fixedlength context. First, the largest possible dependency length is upper bounded by the segment length, which is a few hundred on character-level language modeling (Al-Rfou et al., 2018). Therefore, although the self-attention mechanism is less affected by the vanishing gradient problem compared to RNNs, the vanilla model is not able to fully exploit this optimization advantage. Second, though it is possible to use padding to respect the sentence or other semantic boundaries, in practice it has been standard practice to simply chunk long text into fixed-length segments due to improved efficiency (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). However, simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem as discussed in Section 1.</p>
<p>During evaluation, at each step, the vanilla model also consumes a segment of the same length as in training, but only makes one prediction at the last position. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in Fig. 1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive. We will show that our proposed architecture is able to substantially improve the evaluation speed.</p>
<h3>3.2 Segment-Level Recurrence with State Reuse</h3>
<p>To address the limitations of using a fixed-length context, we propose to introduce a recurrence mechanism to the Transformer architecture. During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment, as shown in Fig. 2a. Although the gradient still remains within a segment, this additional input allows the network to exploit information in the history, leading to an ability of modeling longer-term dependency and avoiding context fragmentation. Formally, let the two consecutive segments of length $L$ be $\mathbf{s}<em 1="1" _tau_="\tau,">{\tau}=$ $\left[x</em>}, \cdots, x_{\tau, L}\right]$ and $\mathbf{s<em _tau_1_1="\tau+1,1">{\tau+1}=\left[x</em>}, \cdots, x_{\tau+1, L}\right]$ respectively. Denoting the $n$-th layer hidden state sequence produced for the $\tau$-th segment $\mathbf{s<em _tau="\tau">{\tau}$ by $\mathbf{h}</em>$ is produced (schematically) as follows,}^{n} \in \mathbb{R}^{L \times d}$, where $d$ is the hidden dimension. Then, the $n$-th layer hidden state for segment $\mathbf{s}_{\tau+1</p>
<p>$$
\begin{aligned}
&amp; \widetilde{\mathbf{h}}<em _tau="\tau">{\tau+1}^{n-1}=\left[\operatorname{SG}\left(\mathbf{h}</em>}^{n-1}\right) \circ \mathbf{h<em _tau_1="\tau+1">{\tau+1}^{n-1}\right] \
&amp; \mathbf{q}</em>}^{n}, \mathbf{k<em _tau_1="\tau+1">{\tau+1}^{n}, \mathbf{v}</em>}^{n}=\mathbf{h<em q="q">{\tau+1}^{n-1} \mathbf{W}</em>}^{\top}, \widetilde{\mathbf{h}<em k="k">{\tau+1}^{n-1} \mathbf{W}</em>}^{\top}, \widetilde{\mathbf{h}<em v="v">{\tau+1}^{n-1} \mathbf{W}</em> \
&amp; \mathbf{h}}^{\top<em _tau_1="\tau+1">{\tau+1}^{n}=\text { Transformer-Layer }\left(\mathbf{q}</em>}^{n}, \mathbf{k<em _tau_1="\tau+1">{\tau+1}^{n}, \mathbf{v}</em>\right)
\end{aligned}
$$}^{n</p>
<p>where the function $\mathrm{SG}(\cdot)$ stands for stop-gradient, the notation $\left[\mathbf{h}<em v="v">{u} \circ \mathbf{h}</em>}\right]$ indicates the concatenation of two hidden sequences along the length dimension, and $\mathbf{W}$. denotes model parameters. Compared to the standard Transformer, the critical difference lies in that the key $\mathbf{k<em _tau_1="\tau+1">{\tau+1}^{n}$ and value $\mathbf{v}</em>}^{n}$ are conditioned on the extended context $\widetilde{\mathbf{h}<em _tau="\tau">{\tau+1}^{n-1}$ and hence $\mathbf{h}</em>$ cached from the previous segment. We emphasize this particular design by the green paths in Fig. 2a.}^{n-1</p>
<p>With this recurrence mechanism applied to every two consecutive segments of a corpus, it essentially creates a segment-level recurrence in the hidden states. As a result, the effective context being utilized can go way beyond just two segments. However, notice that the recurrent dependency between $\mathbf{h}<em _tau="\tau">{\tau+1}^{n}$ and $\mathbf{h}</em>$ shifts one layer downwards}^{n-1</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the Transformer-XL model with a segment length 4.
per-segment, which differs from the same-layer recurrence in conventional RNN-LMs. Consequently, the largest possible dependency length grows linearly w.r.t. the number of layers as well as the segment length, i.e., $O(N \times L)$, as visualized by the shaded area in Fig. 2b. This is analogous to truncated BPTT (Mikolov et al., 2010), a technique developed for training RNNLMs. However, different from truncated BPTT, our method caches a sequence of hidden states instead of the last one, and should be applied together with the relative positional encoding technique described in Section 3.3.</p>
<p>Besides achieving extra long context and resolving fragmentation, another benefit that comes with the recurrence scheme is significantly faster evaluation. Specifically, during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model. In our experiments on enwiki8, Transformer-XL is up to 1,800+ times faster than the vanilla model during evaluation (see Section 4).</p>
<p>Finally, notice that the recurrence scheme does not need to be restricted to only the previous segment. In theory, we can cache as many previous segments as the GPU memory allows, and reuse all of them as the extra context when processing the current segment. Thus, we can cache a predefined length- $M$ old hidden states spanning (possibly) multiple segments, and refer to them as the memory $\mathbf{m}_{\tau}^{\mathbf{u}} \in \mathbb{R}^{M \times d}$, due to a clear connection to the memory augmented neural networks (Graves et al., 2014; Weston et al., 2014). In our experiments, we set $M$ equal to the segment length during training, and increase it by multiple times during evaluation.</p>
<h3>3.3 Relative Positional Encodings</h3>
<p>While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven't solved in or-
der to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as $\mathbf{U} \in \mathbb{R}^{L_{\max } \times d}$, where the $i$-th row $\mathbf{U}<em _max="{max" _text="\text">{i}$ corresponds to the $i$-th absolute position within a segment and $L</em>$ prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by}</p>
<p>$$
\begin{aligned}
\mathbf{h}<em _tau="\tau">{\tau+1} &amp; =f\left(\mathbf{h}</em>}, \mathbf{E<em _tau_1="\tau+1">{\mathbf{s}</em>}}+\mathbf{U<em _tau="\tau">{1: L}\right) \
\mathbf{h}</em>} &amp; =f\left(\mathbf{h<em _mathbf_s="\mathbf{s">{\tau-1}, \mathbf{E}</em><em 1:="1:" L="L">{\tau}}+\mathbf{U}</em>\right)
\end{aligned}
$$</p>
<p>where $\mathbf{E}<em _tau="\tau">{\mathbf{s}</em>}} \in \mathbb{R}^{L \times d}$ is the word embedding sequence of $\mathbf{s<em _mathbf_s="\mathbf{s">{\tau}$, and $f$ represents a transformation function. Notice that, both $\mathbf{E}</em><em _mathbf_s="\mathbf{s">{\tau}}$ and $\mathbf{E}</em><em 1:="1:" L="L">{\tau+1}}$ are associated with the same positional encoding $\mathbf{U}</em>$ for any $j=1, \ldots, L$, resulting in a sheer performance loss.}$. As a result, the model has no information to distinguish the positional difference between $x_{\tau, j}$ and $x_{\tau+1, j</p>
<p>In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or "bias" about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner. For instance, when a query vector $q_{\tau, i}$ attends on the key vectors $\mathbf{k}<em _tau_="\tau," j="j">{\tau, \leq i}$, it does not need to know the absolute position of each key vector to identify the temporal order of the segment. Instead, it suffices to know the relative distance between each key vector $k</em>$, i.e. $i-j$. Practically, one can create a set of relative posi-}$ and itself $q_{\tau, i</p>
<p>tional encodings $\mathbf{R} \in \mathbb{R}^{L_{\max } \times d}$, where the $i$-th row $\mathbf{R}<em _tau_="\tau," j="j">{i}$ indicates a relative distance of $i$ between two positions. By injecting the relative distance dynamically into the attention score, the query vector can easily distinguish the representations of $x</em>$ from their different distances, making the state reuse mechanism feasible. Meanwhile, we won't lose any temporal information, as the absolute position can be recovered recursively from relative distances.}$ and $x_{\tau+1, j</p>
<p>Previously, the idea of relative positional encodings has been explored in the context of machine translation (Shaw et al., 2018) and music generation (Huang et al., 2018). Here, we offer a different derivation, arriving at a new form of relative positional encodings, which not only has a one-to-one correspondence to its absolute counterpart but also enjoys much better generalization empirically (see Section 4). Firstly, in the standard Transformer (Vaswani et al., 2017), the attention score between query $q_{i}$ and key vector $k_{j}$ within the same segment can be decomposed as</p>
<p>$$
\begin{aligned}
\mathbf{A}<em x__i="x_{i">{i, j}^{\mathrm{sp}} &amp; =\underbrace{\mathbf{E}</em>}}^{\top} \mathbf{W<em k="k">{q}^{\top} \mathbf{W}</em>} \mathbf{E<em j="j">{x</em>}}<em x__i="x_{i">{(\alpha)}+\underbrace{\mathbf{E}</em>}}^{\top} \mathbf{W<em k="k">{q}^{\top} \mathbf{W}</em>} \mathbf{U<em _bar_k="(\bar{k">{j}}</em> \
&amp; +\underbrace{\mathbf{U}})<em q="q">{i}^{\top} \mathbf{W}</em>}^{\top} \mathbf{W<em x__j="x_{j">{k} \mathbf{E}</em>}}<em i="i">{(\bar{c})}+\underbrace{\mathbf{U}</em>}^{\top} \mathbf{W<em k="k">{q}^{\top} \mathbf{W}</em>} \mathbf{U<em _bar_d="(\bar{d">{j}}</em>
\end{aligned}
$$})</p>
<p>Following the idea of only relying on relative positional information, we propose to reparameterize the four terms as follows</p>
<p>$$
\begin{aligned}
\mathbf{A}<em x__i="x_{i">{i, j}^{\mathrm{st}} &amp; =\underbrace{\mathbf{E}</em>}}^{\top} \mathbf{W<em E="E" k_="k,">{q}^{\top} \mathbf{W}</em>} \mathbf{E<em j="j">{x</em>}}<em x__i="x_{i">{(\alpha)}+\underbrace{\mathbf{E}</em>}}^{\top} \mathbf{W<em R="R" k_="k,">{q}^{\top} \mathbf{W}</em>} \mathbf{R<em _bar_k="(\bar{k">{i-j}}</em> \
&amp; +\underbrace{u^{\top} \mathbf{W}})<em x__j="x_{j">{k, E} \mathbf{E}</em>}}<em R="R" k_="k,">{(\bar{c})}+\underbrace{v^{\top} \mathbf{W}</em>} \mathbf{R<em _bar_d="(\bar{d">{i-j}}</em>
\end{aligned}
$$})</p>
<ul>
<li>The first change we make is to replace all appearances of the absolute positional embedding $\mathbf{U}<em i-j="i-j">{j}$ for computing key vectors in term $(b)$ and $(d)$ with its relative counterpart $\mathrm{R}</em>$ is a sinusoid encoding matrix (Vaswani et al., 2017) without learnable parameters.}$. This essentially reflects the prior that only the relative distance matters for where to attend. Note that $\mathbf{R</li>
<li>Secondly, we introduce a trainable parameter $u \in \mathbb{R}^{d}$ to replace the query $\mathbf{U}<em q="q">{i}^{\top} \mathbf{W}</em>}^{\top}$ in term $(c)$. In this case, since the query vector is the same for all query positions, it suggests that the attentive bias towards different words should remain the same regardless of the query position. With a similar reasoning, a trainable parameter $v \in \mathbb{R}^{d}$ is added to substitute $\mathbf{U<em q="q">{i}^{\top} \mathbf{W}</em>$ in term $(d)$.}^{\top</li>
<li>Finally, we deliberately separate the two weight matrices $\mathbf{W}<em R="R" k_="k,">{k, E}$ and $\mathbf{W}</em>$ for producing the content-based key vectors and location-based key vectors respectively.
Under the new parameterization, each term has an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a contentdependent positional bias, term (c) governs a global content bias, and $(d)$ encodes a global positional bias.</li>
</ul>
<p>In comparison, the formulation in Shaw et al. (2018) only has terms $(a)$ and $(b)$, dropping the two bias terms $(c)$ and $(d)$. Moreover, Shaw et al. (2018) merge the multiplication $\mathbf{W}_{k} \mathbf{R}$ into a single trainable matrix $\tilde{\mathbf{R}}$, which abandons the inductive bias built into the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding $\mathbf{R}$ adapts the sinusoid formulation. As a benefit of the inductive bias, a model trained on a memory of some certain length can automatically generalize to a memory several times longer during evaluation.</p>
<p>Equipping the recurrence mechanism with our proposed relative positional embedding, we finally arrive at the Transformer-XL architecture. For completeness, we summarize the computational procedure for a $N$-layer Transformer-XL with a single attention head here. For $n=1, \ldots, N$ :</p>
<p>$$
\begin{aligned}
\widetilde{\mathbf{h}}<em r="r">{r}^{n-1}= &amp; {\left[\mathrm{SG}\left(\mathbf{m}</em>}^{n-1}\right) \circ \mathbf{h<em r="r">{r}^{n-1}\right] } \
\mathbf{q}</em>}^{n}, \mathbf{k<em r="r">{r}^{n}, \mathbf{v}</em>}^{n}= &amp; \mathbf{h<em q="q">{r}^{n-1} \mathbf{W}</em>}^{n \top}, \widetilde{\mathbf{h}<em E="E" k_="k,">{r}^{n-1} \mathbf{W}</em>}^{n \top}, \widetilde{\mathbf{h}<em v="v">{r}^{n-1} \mathbf{W}</em> \
\mathbf{A}}^{n \top<em i="i" r_="r,">{r, i, j}^{n}= &amp; \mathbf{q}</em>}^{n}{ }^{\top} \mathbf{k<em i="i" r_="r,">{r, j}^{n}+\mathbf{q}</em>}^{n}{ }^{\top} \mathbf{W<em i-j="i-j">{k, R}^{n} \mathbf{R}</em> \
&amp; +u^{\top} \mathbf{k}<em R="R" k_="k,">{r, j}+v^{\top} \mathbf{W}</em>}^{n} \mathbf{R<em r="r">{i-j} \
\mathbf{a}</em>}^{n}= &amp; \text { Masked-Softmax }\left(\mathbf{A<em r="r">{r}^{n}\right) \mathbf{v}</em> \
\mathbf{o}}^{n<em r="r">{r}^{n}= &amp; \text { LayerNorm }\left(\operatorname{Linear}\left(\mathbf{a}</em>}^{n}\right)+\mathbf{h<em r="r">{r}^{n-1}\right) \
\mathbf{h}</em>\right)
\end{aligned}
$$}^{n}= &amp; \text { Positionwise-Feed-Forward }\left(\mathbf{o}_{r}^{n</p>
<p>with $\mathbf{h}<em _mathbf_x="\mathbf{x">{r}^{0}:=\mathbf{E}</em><em R="R" k_="k,">{r}}$ defined as the word embedding sequence. In addition, it is worth mentioning that a naive way to compute $\mathbf{A}$ requires computing $\mathbf{W}</em>$ for all pairs $(i, j)$, whose cost is quadratic w.r.t. the sequence length. However, noticing that the value of $i-j$ only ranges from zero to the sequence length, we show a simple computation procedure in Appendix B, which reduces the cost to be linear w.r.t. the sequence length.}^{n} \mathbf{R}_{i-j</p>
<h2>4 Experiments</h2>
<h3>4.1 Main Results</h3>
<p>We apply Transformer-XL to a variety of datasets on both word-level and character-level language</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Param</th>
<th style="text-align: center;">PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Grave et al. (2016b) - LSTM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: left;">Bai et al. (2018) - TCN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: left;">Dauphin et al. (2016) - GCNN-8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: left;">Grave et al. (2016b) - Neural cache</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">Dauphin et al. (2016) - GCNN-14</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: left;">Merity et al. (2018) - QRNN</td>
<td style="text-align: center;">151 M</td>
<td style="text-align: center;">33.0</td>
</tr>
<tr>
<td style="text-align: left;">Rae et al. (2018) - Hebbian + Cache</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: left;">Ours - Transformer-XL Standard</td>
<td style="text-align: center;">151 M</td>
<td style="text-align: center;">$\mathbf{2 4 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Baevski and Auli (2018) - Adaptive Input*</td>
<td style="text-align: center;">247 M</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours - Transformer-XL Large</td>
<td style="text-align: center;">257 M</td>
<td style="text-align: center;">$\mathbf{1 8 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison with state-of-the-art results on WikiText-103. * indicates contemporary work.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Param</th>
<th style="text-align: center;">bpc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ha et al. (2016) - LN HyperNetworks</td>
<td style="text-align: center;">27 M</td>
<td style="text-align: center;">1.34</td>
</tr>
<tr>
<td style="text-align: left;">Chung et al. (2016) - LN HM-LSTM</td>
<td style="text-align: center;">35 M</td>
<td style="text-align: center;">1.32</td>
</tr>
<tr>
<td style="text-align: left;">Zilly et al. (2016) - RHN</td>
<td style="text-align: center;">46 M</td>
<td style="text-align: center;">1.27</td>
</tr>
<tr>
<td style="text-align: left;">Mujika et al. (2017) - FS-LSTM-4</td>
<td style="text-align: center;">47 M</td>
<td style="text-align: center;">1.25</td>
</tr>
<tr>
<td style="text-align: left;">Krause et al. (2016) - Large mLSTM</td>
<td style="text-align: center;">46 M</td>
<td style="text-align: center;">1.24</td>
</tr>
<tr>
<td style="text-align: left;">Knol (2017) - cmix v13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.23</td>
</tr>
<tr>
<td style="text-align: left;">Al-Rfou et al. (2018) - 12L Transformer</td>
<td style="text-align: center;">44 M</td>
<td style="text-align: center;">1.11</td>
</tr>
<tr>
<td style="text-align: left;">Ours - 12L Transformer-XL</td>
<td style="text-align: center;">41 M</td>
<td style="text-align: center;">$\mathbf{1 . 0 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Al-Rfou et al. (2018) - 64L Transformer</td>
<td style="text-align: center;">235 M</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: left;">Ours - 18L Transformer-XL</td>
<td style="text-align: center;">88 M</td>
<td style="text-align: center;">1.03</td>
</tr>
<tr>
<td style="text-align: left;">Ours - 24L Transformer-XL</td>
<td style="text-align: center;">277 M</td>
<td style="text-align: center;">$\mathbf{0 . 9 9}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison with state-of-the-art results on enwik8.
modeling to have a comparison with state-of-theart systems, including WikiText-103 (Merity et al., 2016), enwik8 (LLC, 2009), text8 (LLC, 2009), One Billion Word (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012).</p>
<p>WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28 K articles, with an average length of 3.6 K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018; Grave et al., 2016a). As shown in Table 1, Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3 , which demonstrates the superiority of the TransformerXL architecture.</p>
<p>The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in Table 2. Under the model size constraint, the 12-layer Transformer-XL achieves a new SoTA result, out-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Param</th>
<th style="text-align: center;">bpc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Cooijmans et al. (2016) - BN-LSTM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.36</td>
</tr>
<tr>
<td style="text-align: left;">Chung et al. (2016) - LN HM-LSTM</td>
<td style="text-align: center;">35 M</td>
<td style="text-align: center;">1.29</td>
</tr>
<tr>
<td style="text-align: left;">Zilly et al. (2016) - RHN</td>
<td style="text-align: center;">45 M</td>
<td style="text-align: center;">1.27</td>
</tr>
<tr>
<td style="text-align: left;">Krause et al. (2016) - Large mLSTM</td>
<td style="text-align: center;">45 M</td>
<td style="text-align: center;">1.27</td>
</tr>
<tr>
<td style="text-align: left;">Al-Rfou et al. (2018) - 12L Transformer</td>
<td style="text-align: center;">44 M</td>
<td style="text-align: center;">1.18</td>
</tr>
<tr>
<td style="text-align: left;">Al-Rfou et al. (2018) - 64L Transformer</td>
<td style="text-align: center;">235 M</td>
<td style="text-align: center;">1.13</td>
</tr>
<tr>
<td style="text-align: left;">Ours - 24L Transformer-XL</td>
<td style="text-align: center;">277 M</td>
<td style="text-align: center;">$\mathbf{1 . 0 8}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison with state-of-the-art results on text8.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Param</th>
<th style="text-align: center;">PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Shazeer et al. (2014) - Sparse Non-Negative</td>
<td style="text-align: center;">33 B</td>
<td style="text-align: center;">52.9</td>
</tr>
<tr>
<td style="text-align: left;">Chelba et al. (2013) - RNN-1024 + 9 Gram</td>
<td style="text-align: center;">20 B</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: left;">Kuchaiev and Ginsburg (2017) - G-LSTM-2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.0</td>
</tr>
<tr>
<td style="text-align: left;">Dauphin et al. (2016) - GCNN-14 bottleneck</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: left;">Jozefowicz et al. (2016) - LSTM</td>
<td style="text-align: center;">1.8 B</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: left;">Jozefowicz et al. (2016) - LSTM + CNN</td>
<td style="text-align: center;">1.04 B</td>
<td style="text-align: center;">30.0</td>
</tr>
<tr>
<td style="text-align: left;">Shazeer et al. (2017) - Low-Budget MoE</td>
<td style="text-align: center;">$\sim 5$ B</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: left;">Shazeer et al. (2017) - High-Budget MoE</td>
<td style="text-align: center;">$\sim 5$ B</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: left;">Shazeer et al. (2018) - Mesh Tensorflow</td>
<td style="text-align: center;">4.9 B</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: left;">Baevski and Auli (2018) - Adaptive Input*</td>
<td style="text-align: center;">0.46 B</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: left;">Baevski and Auli (2018) - Adaptive Input*</td>
<td style="text-align: center;">1.0 B</td>
<td style="text-align: center;">23.7</td>
</tr>
<tr>
<td style="text-align: left;">Ours - Transformer-XL Base</td>
<td style="text-align: center;">0.46 B</td>
<td style="text-align: center;">23.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours - Transformer-XL Large</td>
<td style="text-align: center;">0.8 B</td>
<td style="text-align: center;">$\mathbf{2 1 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison with state-of-the-art results on One Billion Word. * indicates contemporary work.
performing the 12-layer vanilla Transformer from Al-Rfou et al. (2018) by 0.05 , while both Transformer variants have a large margin over conventional RNN-based models. Notably, our 12-layer architecture achieves the same result as the 64layer network from Al-Rfou et al. (2018), using only $17 \%$ of the parameter budget. In order to see whether better performances can be obtained by increasing the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes. With the attention length 784 during training and 3,800 during evaluation, we obtained a new SoTA result and our method is the first to break through 1.0 on widely-studied characterlevel benchmarks. Different from Al-Rfou et al. (2018), Transformer-XL does not need any auxiliary losses, and thus all benefits are credited to a better architecture.</p>
<p>Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z , and space. Due to the similarity, we simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning. The compari-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Param</th>
<th style="text-align: center;">PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Inan et al. (2016) - Tied Variational LSTM</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: left;">Zilly et al. (2016) - Variational RHN</td>
<td style="text-align: center;">23 M</td>
<td style="text-align: center;">65.4</td>
</tr>
<tr>
<td style="text-align: left;">Zoph and Le (2016) - NAS Cell</td>
<td style="text-align: center;">25 M</td>
<td style="text-align: center;">64.0</td>
</tr>
<tr>
<td style="text-align: left;">Merity et al. (2017) - AWD-LSTM</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: left;">Pham et al. (2018) - Efficient NAS</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">58.6</td>
</tr>
<tr>
<td style="text-align: left;">Liu et al. (2018) - Differentiable NAS</td>
<td style="text-align: center;">23 M</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: left;">Yang et al. (2017) - AWD-LSTM-MoS</td>
<td style="text-align: center;">22 M</td>
<td style="text-align: center;">55.97</td>
</tr>
<tr>
<td style="text-align: left;">Melis et al. (2018) - Dropout tuning</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">55.3</td>
</tr>
<tr>
<td style="text-align: left;">Ours - Transformer-XL</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">$\mathbf{5 4 . 5 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Merity et al. (2017) - AWD-LSTM+Finetune ${ }^{\dagger}$</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: left;">Yang et al. (2017) - MoS+Finetune ${ }^{\dagger}$</td>
<td style="text-align: center;">22 M</td>
<td style="text-align: center;">$\mathbf{5 4 . 4 4}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison with state-of-the-art results on Penn Treebank. $\dagger$ indicates using two-step finetuning.
son with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SoTA result with a clear margin.</p>
<p>One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.</p>
<p>We also report the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we apply variational dropout and weight average to Transformer-XL. With proper regularization, Transformer-XL achieves a new SoTA result among models without two-step finetuning. Penn Treebank has only 1M training tokens, which implies that Transformer-XL also generalizes well even on small datasets.</p>
<h3>4.2 Ablation Study</h3>
<p>We conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme.</p>
<p>The first study is performed on WikiText-103, which requires modeling long-term dependency. The results are reported in Table 6. Among the compared encoding schemes, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou
et al. (2018) are absolute. "Full" and "half" losses refer to applying a cross entropy loss to all or the recent half positions in the segment. We found that absolute encodings only work well with half losses because half losses exclude positions with very short attention lengths during training for better generalization. Table 6 shows that both the recurrence mechanism and our encoding scheme are necessary to achieve the best performance, as well as generalizing to longer attention sequences during evaluation time. Although the backpropagation length during training is only 128 , with the two techniques the attention length can be increased to 640 at test time. In the standard setting with 151 M parameters, the perplexity decreases as the attention length increases.</p>
<p>Since the recurrence mechanism costs additional memory, we also compare Transformer-XL with baselines under the same GPU memory constraints. As shown in Table 10 in Appendix A, despite using a shorter backpropagation length, Transformer-XL remains superior to the baselines.</p>
<p>The second study targets at isolating the effects of resolving the context fragmentation problem from the benefit of capturing longer context length. In order to achieve this goal, we deliberately choose a dataset that does not require longterm dependency, so that any improvement from establishing the recurrence can be attributed to solving the context fragmentation. Specifically, we perform this controlled experiment on the One Billion Word dataset, which can only benefit from removing the context fragmentation. We train a 20-layer Transformer-XL with $\sim 0.3 \mathrm{~B}$ parameters for 400 K steps. As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences.</p>
<h3>4.3 Relative Effective Context Length</h3>
<p>Khandelwal et al. (2018) proposed a method to evaluate the Effective Context Length (ECL) of a sequence model. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower per-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Remark</th>
<th style="text-align: center;">Recurrence</th>
<th style="text-align: center;">Encoding</th>
<th style="text-align: center;">Loss</th>
<th style="text-align: center;">PPL init</th>
<th style="text-align: center;">PPL best</th>
<th style="text-align: center;">Attn Len</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Transformer-XL (128M)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">27.02</td>
<td style="text-align: center;">26.77</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Shaw et al. (2018)</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">27.94</td>
<td style="text-align: center;">27.94</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Half</td>
<td style="text-align: center;">28.69</td>
<td style="text-align: center;">28.33</td>
<td style="text-align: center;">460</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">29.59</td>
<td style="text-align: center;">29.02</td>
<td style="text-align: center;">260</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Half</td>
<td style="text-align: center;">30.10</td>
<td style="text-align: center;">30.10</td>
<td style="text-align: center;">120</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Shaw et al. (2018)</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">29.75</td>
<td style="text-align: center;">29.75</td>
<td style="text-align: center;">120</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Shaw et al. (2018)</td>
<td style="text-align: center;">Half</td>
<td style="text-align: center;">30.50</td>
<td style="text-align: center;">30.50</td>
<td style="text-align: center;">120</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Vaswani et al. (2017)</td>
<td style="text-align: center;">Half</td>
<td style="text-align: center;">30.97</td>
<td style="text-align: center;">30.97</td>
<td style="text-align: center;">120</td>
</tr>
<tr>
<td style="text-align: center;">Transformer (128M) ${ }^{\dagger}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Al-Rfou et al. (2018)</td>
<td style="text-align: center;">Half</td>
<td style="text-align: center;">31.16</td>
<td style="text-align: center;">31.16</td>
<td style="text-align: center;">120</td>
</tr>
<tr>
<td style="text-align: center;">Transformer-XL (151M)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">23.43</td>
<td style="text-align: center;">23.09</td>
<td style="text-align: center;">640</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.16</td>
<td style="text-align: center;">450</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.35</td>
<td style="text-align: center;">300</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M parameters). $\dagger$ indicates that the corresponding row is reduced to the same setting as the Transformer network in (Al-Rfou et al., 2018), except that two auxiliary losses are not implemented in our experiments. "PPL init" refers to using the same length as training. "PPL best" indicates the perplexity obtained by using the optimal length. "Attn Len" is the shortest possible attention length during evaluation to achieve the corresponding result (PPL best). Increasing the attention length during evaluation improves performance only when our positional encoding is used. The "Transformer-XL (151M)" setting uses a standard parameter budget as previous work (Merity et al., 2018), where we observe a similar effect when increasing the attention length during evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: left;">$\mathbf{2 5 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">With Shaw et al. (2018) encodings</td>
<td style="text-align: left;">25.7</td>
</tr>
<tr>
<td style="text-align: left;">Without recurrence</td>
<td style="text-align: left;">27.1</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study on One Billion Word, a dataset without long-term dependency.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$r=0.1$</th>
<th style="text-align: center;">$r=0.5$</th>
<th style="text-align: center;">$r=1.0$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformer-XL 151M</td>
<td style="text-align: center;">$\mathbf{9 0 0}$</td>
<td style="text-align: center;">$\mathbf{8 0 0}$</td>
<td style="text-align: center;">$\mathbf{7 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">QRNN</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Transformer-XL 128M</td>
<td style="text-align: center;">$\mathbf{7 0 0}$</td>
<td style="text-align: center;">$\mathbf{6 0 0}$</td>
<td style="text-align: center;">$\mathbf{5 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">- use Shaw et al. (2018) encoding</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">- remove recurrence</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
</tr>
</tbody>
</table>
<p>Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and $r$. The first three models and the last four models are compared as two model groups when we calculate RECL (RECL is computed on a model group rather than a single model). Each group has the same parameter budget.
plexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models. We instead propose a new metric called Relative Effective Context Length (RECL). RECL is defined on a model group instead of a single model, and the gain of a long context is measure by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair compari-
son. RECL also has a parameter $r$, which means constraining the comparison on top- $r$ hard examples. See Appedix C for more details about RECL. As shown in Table 8, Transformer-XL manages to model dependency of 900 words long on average with $r=0.1$. The RECL of TransformerXL is $80 \%$ and $450 \%$ longer than recurrent networks and Transformer respectively. Both the recurrence mechanism and our positional encodings contribute to a longer RECL. This further substantiates our argument that Transformer-XL is able to model longer-term dependency.</p>
<h3>4.4 Generated Text</h3>
<p>Trained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples.</p>
<h3>4.5 Evaluation Speed</h3>
<p>Finally, we compare the evaluation speed of our model with the vanilla Transformer model (AlRfou et al., 2018). As shown in Table 9, due to the state reuse scheme, Transformer-XL achieves an up to 1,874 times speedup during evaluation.</p>
<h2>5 Conclusions</h2>
<p>Transformer-XL obtains strong perplexity results, models longer-term dependency than RNNs and Transformer, achieves substantial speedup during</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attn Len</th>
<th style="text-align: center;">How much Al-Rfou et al. (2018) is slower</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3,800</td>
<td style="text-align: center;">$1,874 \mathrm{x}$</td>
</tr>
<tr>
<td style="text-align: center;">2,800</td>
<td style="text-align: center;">$1,409 \mathrm{x}$</td>
</tr>
<tr>
<td style="text-align: center;">1,800</td>
<td style="text-align: center;">773 x</td>
</tr>
<tr>
<td style="text-align: center;">800</td>
<td style="text-align: center;">363 x</td>
</tr>
</tbody>
</table>
<p>Table 9: Slowdown in terms of running time during evaluation. Evaluation is based on per-token time on one GPU.
evaluation, and is able to generate coherent text articles. We envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling.</p>
<h2>Acknowledgments</h2>
<p>ZD and YY were supported in part by National Science Foundation (NSF) under the grant IIS1546329 and by the DOE-Office of Science under the grant ASCR #KJ040201. ZY and RS were supported in part by the Office of Naval Research grant N000141812861, the NSF grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship.</p>
<h2>References</h2>
<p>Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.</p>
<p>Alexei Baevski and Michael Auli. 2018. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271.</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137-1155.</p>
<p>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.</p>
<p>Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. 2016. Hierarchical multiscale recurrent neural networks. arXiv preprint arXiv:1609.01704.</p>
<p>Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, and Aaron Courville. 2016. Recurrent batch normalization. arXiv preprint arXiv:1603.09025.</p>
<p>Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079-3087.</p>
<p>Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2016. Language modeling with gated convolutional networks. arXiv preprint arXiv:1612.08083.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Adji B Dieng, Chong Wang, Jianfeng Gao, and John Paisley. 2016. Topicrnn: A recurrent neural network with long-range semantic dependency. arXiv preprint arXiv:1611.01702.</p>
<p>Yarin Gal and Zoubin Ghahramani. 2016. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pages 1019-1027.</p>
<p>Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou. 2016a. Efficient softmax approximation for gpus. arXiv preprint arXiv:1609.04309.</p>
<p>Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016b. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426.</p>
<p>Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.</p>
<p>David Ha, Andrew Dai, and Quoc V Le. 2016. Hypernetworks. arXiv preprint arXiv:1609.09106.</p>
<p>Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al. 2001. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, and Douglas Eck.</p>
<ol>
<li>An improved relative self-attention mechanism for transformer with application to music generation. arXiv preprint arXiv:1809.04281.</li>
</ol>
<p>Hakan Inan, Khashayar Khosravi, and Richard Socher. 2016. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462.</p>
<p>Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. 2015. Document context language models. arXiv preprint arXiv:1511.03962.</p>
<p>Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410.</p>
<p>Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal, and Yoshua Bengio. 2018. Sparse attentive backtracking: Temporal credit assignment through reminding. In Advances in Neural Information Processing Systems, pages $7650-7661$.</p>
<p>Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. arXiv preprint arXiv:1805.04623.</p>
<p>Bryon Knol. 2017. cmix v13. http://www. byronknoll.com/cmix.html.</p>
<p>Ben Krause, Liang Lu, Iain Murray, and Steve Renals. 2016. Multiplicative lstm for sequence modelling. arXiv preprint arXiv:1609.07959.</p>
<p>Oleksii Kuchaiev and Boris Ginsburg. 2017. Factorization tricks for lstm networks. arXiv preprint arXiv:1703.10722.</p>
<p>Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. 2015. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941.</p>
<p>Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. 2018. Independently recurrent neural network (indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5457-5466.</p>
<p>Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.</p>
<p>MultiMedia LLC. 2009. Large text compression benchmark.</p>
<p>Gábor Melis, Charles Blundell, Tomáš Kočiskỳ, Karl Moritz Hermann, Chris Dyer, and Phil Blunsom. 2018. Pushing the bounds of dropout. arXiv preprint arXiv:1805.09208.</p>
<p>Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182.</p>
<p>Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.</p>
<p>Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association.</p>
<p>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. $S L T, 12(234-239): 8$.</p>
<p>Asier Mujika, Florian Meier, and Angelika Steger. 2017. Fast-slow recurrent neural networks. In Advances in Neural Information Processing Systems, pages 5915-5924.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.</p>
<p>Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. Efficient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf.</p>
<p>Jack W Rae, Chris Dyer, Peter Dayan, and Timothy P Lillicrap. 2018. Fast parametric learning with activation memorization. arXiv preprint arXiv:1803.10049.</p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155.</p>
<p>Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, pages 10434-10443.</p>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.</p>
<p>Noam Shazeer, Joris Pelemans, and Ciprian Chelba. 2014. Skip-gram language modeling using sparse non-negative matrix probability estimation. arXiv preprint arXiv:1412.1454.</p>
<p>Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. 2018. Learning longer-term dependencies in rnns with auxiliary losses. arXiv preprint arXiv:1803.00144.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008.</p>
<p>Tian Wang and Kyunghyun Cho. 2015. Largercontext language modelling. arXiv preprint arXiv:1511.03729.</p>
<p>Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and Lawrence Carin. 2017. Topic compositional neural language model. arXiv preprint arXiv:1712.09783.</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint arXiv:1410.3916.</p>
<p>Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. 2016. On multiplicative integration with recurrent neural networks. In Advances in neural information processing systems, pages 2856-2864.</p>
<p>Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. 2017. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953.</p>
<p>Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. 2016. Recurrent highway networks. arXiv preprint arXiv:1607.03474.</p>
<p>Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Equal contribution. Order determined by swapping the one in Yang et al. (2017).
${ }^{1}$ https://github.com/kimiyoung/ transformer-xl&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1829 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1829</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1829</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-4b530e7756a08af082c0ec2b242882b70873f753</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4b530e7756a08af082c0ec2b242882b70873f753" target="_blank">InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A novel perspective to evaluate the personality fidelity of RPAs with psychological scales is introduced, namely Interviewing Character agents for personality tests, and it is shown that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters.</p>
                <p><strong>Paper Abstract:</strong> Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to 80.7%.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1829.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1829.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert Rating (LLM-as-interviewer expert rating)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An assessment method in INCHARACTER where an LLM is prompted with all interview question-response pairs for a dimension and directly outputs a numerical personality score for that dimension, emulating clinician-style judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (expert rating)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, GPT-3.5, Gemini, Qwen-110B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>batch expert-rating: provide all (question,response) pairs for a dimension and request a final numeric score (no intermediate option conversion); anonymization of character names to avoid leakage</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>RPA interview responses (open-ended answers to psychological scale items)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>dialogue / natural language (role-playing agent responses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>personality dimension scores (e.g., BFI dimensions), measured alignment (accuracy, MAE) and consistency (std over runs and items)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Manual human annotations used as ground truth (PDb aggregated labels and invited annotators' scored labels; invited annotators were vetted for character knowledge and scored 73 dimensions per character)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>2-3 annotators per character for invited annotations (93 annotators total); PDb provides many crowd labels per character (varies by character)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>people familiar with the characters (university-student annotators vetted for understanding of BFI/16P); PDb crowd annotators (familiar fans)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (type-level), MAE, Pearson's r, Spearman's rho, Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>ER_batch with GPT-4: The Big Five Acc_Dim=76.6% / Acc_Full=31.2% , MAE=18.2 (Table 2);  In preliminary OC/ER validation vs human labels (100 sampled BFI cases): ER (batch) GPT-4: accuracy=89.0%, Pearson r=0.925, Spearman rho=0.927, Kendall tau=0.837 (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Using ER (batch) with strong interviewer LLMs (GPT-4/GPT-3.5) yields high alignment; anonymizing character names to avoid leakage; when RPAs' responses are consistent and non-contradictory; dimension-level batching (ER_batch) helps</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Contradictory RPA responses across items in a dimension reduce agreement; interviewer LLM mistakes (misinterpretation) produce mismatches; marginal/ambiguous dimensions are ignored which can influence measured alignment</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Ambiguous or out-of-scope scale questions for a character (i.e., questions beyond the character's knowledge or depiction) reduce agreement; no explicit notion of software artifact complexity applies—complexity of elicited responses (contradiction/ambiguity) lowers agreement</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear scale/dimension descriptions and providing interviewer LLM with scoring schema and dimension descriptions improved ER performance; ER benefits from the LLM's ability to weigh items intelligently versus equal-weight OC</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Preliminary ER/OC validation: 100 sampled RPA interview cases on BFI; Full experiments: 32 characters, 14 scales, interviews repeated 3 times; interview dataset: 18,304 dialogues</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Inter-annotator Cohen's quadratic-weighted kappa averaged 60.9% across 14 scales (higher for BFI & 16P) for invited annotators (Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>ER (GPT-4) showed high correlation with human labels in validation (Pearson r up to 0.925) and produced higher measured alignment than self-report baselines; human inter-annotator agreement (kappa ~60.9%) indicates reasonable but imperfect human consistency, while ER-LMM agreement with humans was high in preliminary validation</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No fine-tuning on human labels reported; improvements achieved via prompt engineering (batching, dimension descriptions), anonymization, and d-OC/ER methodological choices rather than supervised calibration on human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Expert Rating (ER) using strong interviewer LLMs (GPT-4/GPT-3.5) can simulate human expert interviewers well, producing high agreement with manual human labels (high Pearson/Spearman/Kendall correlations and high accuracy in preliminary tests); ER generally outperforms option-conversion and self-report baselines in aligning measured RPA personalities with human-annotated character personalities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Interviewer LLM errors and biases can lead to misratings (underestimating RPA fidelity); ER depends on LLM capability and may be affected by memorization/data leakage (mitigated by anonymization); ER cannot fully replace human experts, especially for ambiguous or contradictory interview responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1829.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1829.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>d-OC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dimension-specific Option Conversion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An option-conversion strategy where LLMs convert open-ended RPA responses to Likert-style options using dimension-specific, more descriptive option prompts (e.g., '4 (Extraverted)') to improve mapping accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (option conversion to Likert-style rating)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, GPT-3.5, Gemini, Qwen-110B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>single-pair conversion but with dimension-specific descriptive options in prompts (d-OC) rather than raw Likert labels; conversion of each (question,response) to an option, then aggregate via scale scoring</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>RPA interview responses (open-ended answers to psychological scale items)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>dialogue / natural language (role-playing agent responses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>option-level conversion accuracy, aggregated dimension scores (MAE, accuracy), and consistency (std over runs/items)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels collected for sample cases (100) for validation; larger ground-truth via PDb and invited annotators across characters for full experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>For validation sample: manual labels (count unspecified); for full ground truth: 2-3 annotators per character (93 annotators total) for invited annotations</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators familiar with characters, vetted for understanding of BFI/16P; PDb crowd annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (right/close/wrong threshold defined by within-1-point), Pearson r, Spearman rho, Kendall tau, MAE, Acc_Dim/Acc_Full</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Preliminary validation (100 BFI cases): d-OC GPT-4: accuracy=82.0%, Pearson r=0.847, Spearman rho=0.853, Kendall tau=0.806 (Table 1). Full experiments (Table 2) show d-OC with GPT-4: BFI Acc_Dim=72.2%, Acc_Full=14.6%, MAE=18.6; 16P Acc_Dim=80.2%, Acc_Full=45.8%, MAE=21.2</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Replacing raw Likert labels with dimension-descriptive options in prompts substantially improved conversion accuracy; high-capability interviewer LLMs (GPT-4/GPT-3.5) further increased agreement</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Standard OC (non-dimension-specific) showed significant inaccuracies; ambiguous or context-dependent responses are harder to map to discrete options</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Responses that are nuanced, long-form, or context-dependent are harder to convert reliably to discrete options, lowering agreement; clearer, directly relevant answers yield higher agreement</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>More descriptive, dimension-specific option wording in prompts (d-OC) increased LLM conversion accuracy compared to generic Likert prompts; thus clarity of target option semantics improved alignment</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Validation: 100 cases; Full experiments: 32 characters, multiple runs; detailed counts in Tables 1 and 2</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Inter-annotator kappa (invited annotators) avg 60.9% across scales used as reference for alignment; not specific to d-OC</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>d-OC substantially improved LLM-human agreement versus vanilla OC (e.g., GPT-4 d-OC Pearson r=0.847 vs OC r=0.600 in preliminary validation), and achieved better measured alignment with human-labeled character personalities in full experiments</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No supervised training on human labels reported; improvement achieved via prompt engineering (dimension-specific option wording) and use of stronger LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dimension-specific option conversion (d-OC) greatly improves the reliability of mapping open-ended RPA responses to Likert-style options, narrowing the gap between automated conversion and human labeling, especially when combined with high-capacity LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Even with d-OC, LLMs can still err, particularly on nuanced or contradictory responses; option conversion treats items with equal weight whereas ER can weigh items intelligently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1829.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1829.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Option Conversion (OC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that uses LLMs to map each open-ended interview response to a Likert-style option for the corresponding scale item, then aggregates options per the scale scoring scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (option conversion to Likert-style rating)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, GPT-3.5, Gemini, Qwen-110B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>single question-response to single Likert option mapping (one-by-one OC)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>RPA interview responses (open-ended answers to psychological scale items)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>dialogue / natural language (role-playing agent responses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>item-level option conversion accuracy, aggregated dimension scores (MAE, accuracy), consistency measures (Std_Item, Std_Dim, Std_Score)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels produced for validation (100 sampled cases) and larger ground truth via PDb/invited annotators for alignment measurement</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>Validation labels: manual (count unspecified); invited annotators for full dataset: 2-3 per character (93 annotators total)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators familiar with characters; PDb crowd annotations</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy, Pearson r, Spearman rho, Kendall's tau, MAE</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Preliminary validation (100 cases, BFI): OC GPT-4: accuracy=71.0%, Pearson r=0.600, Spearman rho=0.643, Kendall tau=0.595 (Table 1). Full experiments show OC with GPT-4: BFI Acc_Dim=64.3%, MAE=21.6 (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Works better when responses are concise and clearly map to a Likert option; using strong LLMs improves OC performance somewhat</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>OC shows notable inaccuracies for long-form or nuanced responses; ambiguous item phrasing or context effects reduce mapping quality</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Longer, more complex, or indirect responses reduce OC reliability; OC assumes straightforward mapping which fails on nuanced discourse</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>OC is sensitive to how clearly item intent is conveyed; less robust than ER to ambiguous item responses</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Validation: 100 cases; Full experiments across 32 characters and 14 scales</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Not specific to OC; human inter-annotator kappa for ground truth averaged 60.9% across scales</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>OC underperformed relative to ER and d-OC in matching human labels; d-OC and ER show better LLM-human agreement metrics</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No supervised calibration reported; improvements attempted via prompt tweaks and the d-OC variant</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla OC is less accurate than ER and d-OC for converting open-ended RPA answers to Likert ratings; it is more error-prone particularly for nuanced responses and yields lower correlations with human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>OC's one-by-one mapping lacks the context-aware weighing that ER provides; default Likert prompts are less semantically informative, producing lower reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1829.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1829.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SR / SR-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Report (SR) and Self-Report with Chain-of-Thought (SR-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines where RPAs are directly prompted to provide Likert choices for scale items (SR), and a variant that asks RPAs to produce chain-of-thought before choosing (SR-CoT); if RPAs don't produce exact choices, interviewer LLMs extract the options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Likert-style (self-report) automated extraction (with optional chain-of-thought elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Qwen-110B, Gemini, GPT-3.5, GPT-4 as either the RPA foundation model or as interviewer extractor</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Direct self-report prompting of RPAs to choose Likert options; SR-CoT: ask for rationale (chain-of-thought) before selecting; if non-exact answers, interviewer LLM extracts options</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>RPA self-reported choices to psychological scale items (textual responses)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>dialogue / natural language (role-playing agent responses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (Acc_Dim, Acc_Full), MAE, consistency (Std_Item), distinctiveness across RPAs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Comparison against PDb labels and invited annotator scores as ground truth; human annotators (2-3 per character) produced reference labels</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>2-3 annotators per character for invited annotations (93 annotators total); PDb crowdsourced labels available</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators familiar with characters, vetted for scale understanding; PDb crowd labels</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy, MAE, Std_Item, Pearson/Spearman sometimes for interviewer-extractor LLM comparison</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>SR (various interviewer/extractor LLMs) BFI Acc_Dim ~63.3-63.7%, MAE ~23.2-23.6 (Table 2). SR-CoT modestly improved Acc_Dim to ~65-67% but showed poorer Std_Item (higher variance) and overall less improvement than interview-based methods</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When RPAs comply and provide exact choices; in-context prompting can slightly improve SR performance</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Role-playing instructions can conflict with self-report prompting causing RPAs to refuse or answer noncompliantly; SR often underestimates RPA personalities versus interview-based methods; SR-CoT introduces instability (worse Std_Item) for smaller models</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>SR struggles when the role agent resists meta-task instructions; SR is brittle for models with limited capacity to follow self-report tasks</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Chain-of-thought did not substantially improve alignment; explicit interview questions (INCHARACTER) elicit more indicative responses than direct self-report</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Full experiments across 32 characters and 14 scales; specific SR validation reported in Table 2</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Human inter-annotator kappa averaged 60.9% (for ground truth labels used to compare SR results)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>SR and SR-CoT produced lower alignment with human labels than INCHARACTER methods (ER and d-OC); SR-CoT improved marginally over SR but introduced greater item-level variance</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No supervised calibration reported; SR-CoT is a prompting strategy only</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-report baselines are less aligned with human-annotated character personalities than interview-based INCHARACTER methods; SR-CoT yields limited improvement and can reduce response consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Self-report conflicts with role-playing instructions and can produce noncompliant/neutral answers; SR's implicit option-conversion burden on RPAs makes it weak for smaller foundation models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-reported versus clinician-rated symptoms of depression as outcome measures in psychotherapy research on depression: a meta-analysis <em>(Rating: 2)</em></li>
                <li>Self-report and clinician-rated measures of depression severity: can one replace the other? <em>(Rating: 2)</em></li>
                <li>Structured clinical interview for the dsm (scid) <em>(Rating: 2)</em></li>
                <li>Who is chatgpt? benchmarking llms' psychological portrayal using psychobench <em>(Rating: 2)</em></li>
                <li>Revisiting the reliability of psychological scales on large language models <em>(Rating: 1)</em></li>
                <li>Personality traits in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1829",
    "paper_id": "paper-4b530e7756a08af082c0ec2b242882b70873f753",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "ER",
            "name_full": "Expert Rating (LLM-as-interviewer expert rating)",
            "brief_description": "An assessment method in INCHARACTER where an LLM is prompted with all interview question-response pairs for a dimension and directly outputs a numerical personality score for that dimension, emulating clinician-style judgement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (expert rating)",
            "llm_judge_model": "GPT-4, GPT-3.5, Gemini, Qwen-110B",
            "llm_judge_prompt_approach": "batch expert-rating: provide all (question,response) pairs for a dimension and request a final numeric score (no intermediate option conversion); anonymization of character names to avoid leakage",
            "artifact_type": "RPA interview responses (open-ended answers to psychological scale items)",
            "artifact_domain": "dialogue / natural language (role-playing agent responses)",
            "evaluation_criteria": "personality dimension scores (e.g., BFI dimensions), measured alignment (accuracy, MAE) and consistency (std over runs and items)",
            "human_evaluation_setup": "Manual human annotations used as ground truth (PDb aggregated labels and invited annotators' scored labels; invited annotators were vetted for character knowledge and scored 73 dimensions per character)",
            "human_expert_count": "2-3 annotators per character for invited annotations (93 annotators total); PDb provides many crowd labels per character (varies by character)",
            "human_expert_expertise": "people familiar with the characters (university-student annotators vetted for understanding of BFI/16P); PDb crowd annotators (familiar fans)",
            "agreement_metric": "Accuracy (type-level), MAE, Pearson's r, Spearman's rho, Kendall's tau",
            "agreement_score": "ER_batch with GPT-4: The Big Five Acc_Dim=76.6% / Acc_Full=31.2% , MAE=18.2 (Table 2);  In preliminary OC/ER validation vs human labels (100 sampled BFI cases): ER (batch) GPT-4: accuracy=89.0%, Pearson r=0.925, Spearman rho=0.927, Kendall tau=0.837 (Table 1)",
            "high_agreement_conditions": "Using ER (batch) with strong interviewer LLMs (GPT-4/GPT-3.5) yields high alignment; anonymizing character names to avoid leakage; when RPAs' responses are consistent and non-contradictory; dimension-level batching (ER_batch) helps",
            "low_agreement_conditions": "Contradictory RPA responses across items in a dimension reduce agreement; interviewer LLM mistakes (misinterpretation) produce mismatches; marginal/ambiguous dimensions are ignored which can influence measured alignment",
            "artifact_complexity_effect": "Ambiguous or out-of-scope scale questions for a character (i.e., questions beyond the character's knowledge or depiction) reduce agreement; no explicit notion of software artifact complexity applies—complexity of elicited responses (contradiction/ambiguity) lowers agreement",
            "criteria_clarity_effect": "Clear scale/dimension descriptions and providing interviewer LLM with scoring schema and dimension descriptions improved ER performance; ER benefits from the LLM's ability to weigh items intelligently versus equal-weight OC",
            "sample_size": "Preliminary ER/OC validation: 100 sampled RPA interview cases on BFI; Full experiments: 32 characters, 14 scales, interviews repeated 3 times; interview dataset: 18,304 dialogues",
            "inter_human_agreement": "Inter-annotator Cohen's quadratic-weighted kappa averaged 60.9% across 14 scales (higher for BFI & 16P) for invited annotators (Table 6)",
            "proxy_vs_human_comparison": "ER (GPT-4) showed high correlation with human labels in validation (Pearson r up to 0.925) and produced higher measured alignment than self-report baselines; human inter-annotator agreement (kappa ~60.9%) indicates reasonable but imperfect human consistency, while ER-LMM agreement with humans was high in preliminary validation",
            "calibration_or_training": "No fine-tuning on human labels reported; improvements achieved via prompt engineering (batching, dimension descriptions), anonymization, and d-OC/ER methodological choices rather than supervised calibration on human judgments",
            "key_findings": "Expert Rating (ER) using strong interviewer LLMs (GPT-4/GPT-3.5) can simulate human expert interviewers well, producing high agreement with manual human labels (high Pearson/Spearman/Kendall correlations and high accuracy in preliminary tests); ER generally outperforms option-conversion and self-report baselines in aligning measured RPA personalities with human-annotated character personalities.",
            "limitations_noted": "Interviewer LLM errors and biases can lead to misratings (underestimating RPA fidelity); ER depends on LLM capability and may be affected by memorization/data leakage (mitigated by anonymization); ER cannot fully replace human experts, especially for ambiguous or contradictory interview responses.",
            "uuid": "e1829.0",
            "source_info": {
                "paper_title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "d-OC",
            "name_full": "Dimension-specific Option Conversion",
            "brief_description": "An option-conversion strategy where LLMs convert open-ended RPA responses to Likert-style options using dimension-specific, more descriptive option prompts (e.g., '4 (Extraverted)') to improve mapping accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (option conversion to Likert-style rating)",
            "llm_judge_model": "GPT-4, GPT-3.5, Gemini, Qwen-110B",
            "llm_judge_prompt_approach": "single-pair conversion but with dimension-specific descriptive options in prompts (d-OC) rather than raw Likert labels; conversion of each (question,response) to an option, then aggregate via scale scoring",
            "artifact_type": "RPA interview responses (open-ended answers to psychological scale items)",
            "artifact_domain": "dialogue / natural language (role-playing agent responses)",
            "evaluation_criteria": "option-level conversion accuracy, aggregated dimension scores (MAE, accuracy), and consistency (std over runs/items)",
            "human_evaluation_setup": "Human labels collected for sample cases (100) for validation; larger ground-truth via PDb and invited annotators across characters for full experiments",
            "human_expert_count": "For validation sample: manual labels (count unspecified); for full ground truth: 2-3 annotators per character (93 annotators total) for invited annotations",
            "human_expert_expertise": "annotators familiar with characters, vetted for understanding of BFI/16P; PDb crowd annotators",
            "agreement_metric": "Accuracy (right/close/wrong threshold defined by within-1-point), Pearson r, Spearman rho, Kendall tau, MAE, Acc_Dim/Acc_Full",
            "agreement_score": "Preliminary validation (100 BFI cases): d-OC GPT-4: accuracy=82.0%, Pearson r=0.847, Spearman rho=0.853, Kendall tau=0.806 (Table 1). Full experiments (Table 2) show d-OC with GPT-4: BFI Acc_Dim=72.2%, Acc_Full=14.6%, MAE=18.6; 16P Acc_Dim=80.2%, Acc_Full=45.8%, MAE=21.2",
            "high_agreement_conditions": "Replacing raw Likert labels with dimension-descriptive options in prompts substantially improved conversion accuracy; high-capability interviewer LLMs (GPT-4/GPT-3.5) further increased agreement",
            "low_agreement_conditions": "Standard OC (non-dimension-specific) showed significant inaccuracies; ambiguous or context-dependent responses are harder to map to discrete options",
            "artifact_complexity_effect": "Responses that are nuanced, long-form, or context-dependent are harder to convert reliably to discrete options, lowering agreement; clearer, directly relevant answers yield higher agreement",
            "criteria_clarity_effect": "More descriptive, dimension-specific option wording in prompts (d-OC) increased LLM conversion accuracy compared to generic Likert prompts; thus clarity of target option semantics improved alignment",
            "sample_size": "Validation: 100 cases; Full experiments: 32 characters, multiple runs; detailed counts in Tables 1 and 2",
            "inter_human_agreement": "Inter-annotator kappa (invited annotators) avg 60.9% across scales used as reference for alignment; not specific to d-OC",
            "proxy_vs_human_comparison": "d-OC substantially improved LLM-human agreement versus vanilla OC (e.g., GPT-4 d-OC Pearson r=0.847 vs OC r=0.600 in preliminary validation), and achieved better measured alignment with human-labeled character personalities in full experiments",
            "calibration_or_training": "No supervised training on human labels reported; improvement achieved via prompt engineering (dimension-specific option wording) and use of stronger LLMs",
            "key_findings": "Dimension-specific option conversion (d-OC) greatly improves the reliability of mapping open-ended RPA responses to Likert-style options, narrowing the gap between automated conversion and human labeling, especially when combined with high-capacity LLMs.",
            "limitations_noted": "Even with d-OC, LLMs can still err, particularly on nuanced or contradictory responses; option conversion treats items with equal weight whereas ER can weigh items intelligently.",
            "uuid": "e1829.1",
            "source_info": {
                "paper_title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "OC",
            "name_full": "Option Conversion (OC)",
            "brief_description": "A procedure that uses LLMs to map each open-ended interview response to a Likert-style option for the corresponding scale item, then aggregates options per the scale scoring scheme.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (option conversion to Likert-style rating)",
            "llm_judge_model": "GPT-4, GPT-3.5, Gemini, Qwen-110B",
            "llm_judge_prompt_approach": "single question-response to single Likert option mapping (one-by-one OC)",
            "artifact_type": "RPA interview responses (open-ended answers to psychological scale items)",
            "artifact_domain": "dialogue / natural language (role-playing agent responses)",
            "evaluation_criteria": "item-level option conversion accuracy, aggregated dimension scores (MAE, accuracy), consistency measures (Std_Item, Std_Dim, Std_Score)",
            "human_evaluation_setup": "Human labels produced for validation (100 sampled cases) and larger ground truth via PDb/invited annotators for alignment measurement",
            "human_expert_count": "Validation labels: manual (count unspecified); invited annotators for full dataset: 2-3 per character (93 annotators total)",
            "human_expert_expertise": "annotators familiar with characters; PDb crowd annotations",
            "agreement_metric": "Accuracy, Pearson r, Spearman rho, Kendall's tau, MAE",
            "agreement_score": "Preliminary validation (100 cases, BFI): OC GPT-4: accuracy=71.0%, Pearson r=0.600, Spearman rho=0.643, Kendall tau=0.595 (Table 1). Full experiments show OC with GPT-4: BFI Acc_Dim=64.3%, MAE=21.6 (Table 2)",
            "high_agreement_conditions": "Works better when responses are concise and clearly map to a Likert option; using strong LLMs improves OC performance somewhat",
            "low_agreement_conditions": "OC shows notable inaccuracies for long-form or nuanced responses; ambiguous item phrasing or context effects reduce mapping quality",
            "artifact_complexity_effect": "Longer, more complex, or indirect responses reduce OC reliability; OC assumes straightforward mapping which fails on nuanced discourse",
            "criteria_clarity_effect": "OC is sensitive to how clearly item intent is conveyed; less robust than ER to ambiguous item responses",
            "sample_size": "Validation: 100 cases; Full experiments across 32 characters and 14 scales",
            "inter_human_agreement": "Not specific to OC; human inter-annotator kappa for ground truth averaged 60.9% across scales",
            "proxy_vs_human_comparison": "OC underperformed relative to ER and d-OC in matching human labels; d-OC and ER show better LLM-human agreement metrics",
            "calibration_or_training": "No supervised calibration reported; improvements attempted via prompt tweaks and the d-OC variant",
            "key_findings": "Vanilla OC is less accurate than ER and d-OC for converting open-ended RPA answers to Likert ratings; it is more error-prone particularly for nuanced responses and yields lower correlations with human labels.",
            "limitations_noted": "OC's one-by-one mapping lacks the context-aware weighing that ER provides; default Likert prompts are less semantically informative, producing lower reliability.",
            "uuid": "e1829.2",
            "source_info": {
                "paper_title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SR / SR-CoT",
            "name_full": "Self-Report (SR) and Self-Report with Chain-of-Thought (SR-CoT)",
            "brief_description": "Baselines where RPAs are directly prompted to provide Likert choices for scale items (SR), and a variant that asks RPAs to produce chain-of-thought before choosing (SR-CoT); if RPAs don't produce exact choices, interviewer LLMs extract the options.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Likert-style (self-report) automated extraction (with optional chain-of-thought elicitation)",
            "llm_judge_model": "Qwen-110B, Gemini, GPT-3.5, GPT-4 as either the RPA foundation model or as interviewer extractor",
            "llm_judge_prompt_approach": "Direct self-report prompting of RPAs to choose Likert options; SR-CoT: ask for rationale (chain-of-thought) before selecting; if non-exact answers, interviewer LLM extracts options",
            "artifact_type": "RPA self-reported choices to psychological scale items (textual responses)",
            "artifact_domain": "dialogue / natural language (role-playing agent responses)",
            "evaluation_criteria": "Accuracy (Acc_Dim, Acc_Full), MAE, consistency (Std_Item), distinctiveness across RPAs",
            "human_evaluation_setup": "Comparison against PDb labels and invited annotator scores as ground truth; human annotators (2-3 per character) produced reference labels",
            "human_expert_count": "2-3 annotators per character for invited annotations (93 annotators total); PDb crowdsourced labels available",
            "human_expert_expertise": "annotators familiar with characters, vetted for scale understanding; PDb crowd labels",
            "agreement_metric": "Accuracy, MAE, Std_Item, Pearson/Spearman sometimes for interviewer-extractor LLM comparison",
            "agreement_score": "SR (various interviewer/extractor LLMs) BFI Acc_Dim ~63.3-63.7%, MAE ~23.2-23.6 (Table 2). SR-CoT modestly improved Acc_Dim to ~65-67% but showed poorer Std_Item (higher variance) and overall less improvement than interview-based methods",
            "high_agreement_conditions": "When RPAs comply and provide exact choices; in-context prompting can slightly improve SR performance",
            "low_agreement_conditions": "Role-playing instructions can conflict with self-report prompting causing RPAs to refuse or answer noncompliantly; SR often underestimates RPA personalities versus interview-based methods; SR-CoT introduces instability (worse Std_Item) for smaller models",
            "artifact_complexity_effect": "SR struggles when the role agent resists meta-task instructions; SR is brittle for models with limited capacity to follow self-report tasks",
            "criteria_clarity_effect": "Chain-of-thought did not substantially improve alignment; explicit interview questions (INCHARACTER) elicit more indicative responses than direct self-report",
            "sample_size": "Full experiments across 32 characters and 14 scales; specific SR validation reported in Table 2",
            "inter_human_agreement": "Human inter-annotator kappa averaged 60.9% (for ground truth labels used to compare SR results)",
            "proxy_vs_human_comparison": "SR and SR-CoT produced lower alignment with human labels than INCHARACTER methods (ER and d-OC); SR-CoT improved marginally over SR but introduced greater item-level variance",
            "calibration_or_training": "No supervised calibration reported; SR-CoT is a prompting strategy only",
            "key_findings": "Self-report baselines are less aligned with human-annotated character personalities than interview-based INCHARACTER methods; SR-CoT yields limited improvement and can reduce response consistency.",
            "limitations_noted": "Self-report conflicts with role-playing instructions and can produce noncompliant/neutral answers; SR's implicit option-conversion burden on RPAs makes it weak for smaller foundation models.",
            "uuid": "e1829.3",
            "source_info": {
                "paper_title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-reported versus clinician-rated symptoms of depression as outcome measures in psychotherapy research on depression: a meta-analysis",
            "rating": 2
        },
        {
            "paper_title": "Self-report and clinician-rated measures of depression severity: can one replace the other?",
            "rating": 2
        },
        {
            "paper_title": "Structured clinical interview for the dsm (scid)",
            "rating": 2
        },
        {
            "paper_title": "Who is chatgpt? benchmarking llms' psychological portrayal using psychobench",
            "rating": 2
        },
        {
            "paper_title": "Revisiting the reliability of psychological scales on large language models",
            "rating": 1
        },
        {
            "paper_title": "Personality traits in large language models",
            "rating": 1
        }
    ],
    "cost": 0.018082,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews</h1>
<p>Xintao Wang ${ }^{1}$, Yunze Xiao ${ }^{2}$, Jen-tse Huang ${ }^{3}$, Siyu Yuan ${ }^{4}$, Rui Xu ${ }^{1}$, Haoran Guo ${ }^{5}$, Quan $\mathbf{T u}^{6}$, Yaying $\mathrm{Fei}^{7}$, Ziang Leng ${ }^{8}$, Wei Wang ${ }^{1}$, Jiangjie Chen ${ }^{1}$, Cheng $\mathbf{L i}^{9}$, Yanghua Xiao ${ }^{* 1}$ ${ }^{1}$ Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University ${ }^{2}$ Carnegie Mellon University ${ }^{3}$ The Chinese University of Hong Kong ${ }^{4}$ School of Data Science, Fudan University ${ }^{5}$ RhineAI ${ }^{6}$ Renmin University of China ${ }^{7}$ Beijing University of Technology ${ }^{8}$ Boston University ${ }^{9}$ SenseTime<br>{xtwang21, syyuan21, ruixu21}@m.fudan.edu.cn, {jjchen19, weiwang1, shawyh}@fudan.edu.cn<br>yunzex@andrew.cmu.edu, jthuang@cse.cuhk.edu.hk, chengli@sensetime.com</p>
<h4>Abstract</h4>
<p>Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to $80.7 \%{ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Recent advancements in large language models (LLMs) have catalyzed the emergence of roleplaying agents (RPAs). RPAs are interactive AI systems simulating diverse roles or characters. RPA applications have been extended to diverse contexts, such as AI agents of fictional characters (Li et al., 2023), digital clones for humans (Gao et al., 2023), and AI non-player characters in video games (Wang et al., 2023a). Recent research trends have increasingly focused on the development of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The procedure of personality tests on RPAs. To evaluate the personality fidelity of RPAs, we apply various scales to measure their personalities and compare the results with the personality labels of the characters.</p>
<p>RPAs, including building RPAs for specific characters (Li et al., 2023; Wang et al., 2023b) and improving the role-playing abilities of foundation models (Zhou et al., 2023).</p>
<p>However, evaluating character fidelity in RPAs remains a relatively underexplored area. Prior research mainly concentrates on the replication of knowledge, experience, and linguistic patterns of characters (Shao et al., 2023a; Zhou et al., 2023), which manifests in two primary issues: (1) They necessitate character-specific datasets, thereby complicating the evaluation of new characters. (2) They overlook evaluating RPAs' thoughts and underlying mindsets. Towards these issues, we propose to evaluate if RPAs faithfully reproduce the personalities of target characters, i.e., personality fidelity, as depicted in Figure 1. Personality tests, administered by psychological scales, measure an individual's interrelated behavioral, cognitive, and emotional patterns (Barrick and Mount, 1991; Bem, 1981). By measuring the personalities of RPAs and com-</p>
<p>paring them with the personalities of the characters, we can attain a more nuanced understanding of RPAs' character fidelity.</p>
<p>Prior studies on LLM personalities are mainly based on self-report scales, which prompt LLMs to select options or assign ratings to specific items (Tu et al., 2023; Huang et al., 2023b). However, this method suffers from several limitations for RPAs. (1) The instruction to complete scales contradicts role-playing instructions, leading to RPAs' reluctance or inability to engage with personality tests. (2) More importantly, the selected options may conflict with the actual behaviors of RPAs, making the test results unindicative of their true personalities. RPAs might underperform owing to an inadequate understanding of scale instructions and the biases inherent in the training data.</p>
<p>Therefore, we propose INCHARACTER, a novel approach to Interviews Character agents for personality tests. While self-report scales are popular in humans for their cost-effectiveness, interviewbased scales evaluated by experts offer a more comprehensive analysis (Uher et al., 2012; Rush et al., 1987). Self-reports are sometimes influenced by an individual's lack of insight, denial, or bias. In contrast, an interviewer can be a guide to elicit thoughts of individuals, effectively identifying and addressing the nuances via conversations to overcome the previously mentioned limitations. INCHARACTER employs this interview-based procedure (Trull et al., 1998) on RPAs, which includes two stages: (1) Interview: RPAs are engaged with open-ended questions derived from psychological scales to elicit RPAs' mindsets and behaviors. (2) Assessment: We utilize LLMs to interpret the responses collected from the first stage. This can involve converting the responses to Likert levels or using LLMs to simulate a psychiatrist's role in judging RPA personalities.</p>
<p>We apply InCharacter to various RPAs on 14 personality tests, including the Big Five Inventory (BFI), 16Personalities ${ }^{2}$ (16P), and Dark Triad Dirty Dozen (DTDD). The personality labels for the BFI and 16P are accessible through the Personality Database (PDb) ${ }^{3}$. Additionally, we engage human annotators familiar with the characters to label them on other scales, thereby creating a comprehensive benchmark for evaluating RPA personalities. Our experiments include various types of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>existing RPAs. The results demonstrate that the INCHARACTER effectively simulates interview-based tests conducted by human experts and yields RPAs personality measurement better aligned with the characters than self-report methods. Additionally, we find that the conversation data generated in our interview phase are of high quality and ideal for fine-tuning foundation models for RPAs. Hence, we release a dataset comprising 18,304 interview dialogues to facilitate future research.</p>
<p>The contributions of this paper are mainly threefold: 1) We introduce a novel aspect for RPA evaluation, i.e., personality fidelity, based on psychological scales. 2) We propose INCHARACTER, an interview-based framework for personality tests on RPAs and collect the first benchmark for RPA personality evaluation, facilitating future research on developing better RPAs. 3) Our experiments on various RPAs and psychological scales demonstrate the efficacy of INCHARACTER.</p>
<h2>2 Preliminaries</h2>
<h3>2.1 Role-Playing Agents</h3>
<p>Recent advancements have led to the emergence and evolution of several pivotal abilities in LLMs to facilitate the development of RPAs, including in-context learning (Brown et al., 2020), instruction following (Ouyang et al., 2022), step-by-step reasoning (Wei et al., 2022), and human-like traits such as empathy (Sorin et al., 2023). RPAs are interactive AI systems that act as assigned personas, from fictional characters to celebrities. RPAs utilize persona data to simulate characters, drawing from training datasets, prompted contexts, or external databases. Typically, existing work develops RPAs by setting character descriptions as system prompts (Zhou et al., 2023; Shao et al., 2023a) and crafting memory modules with character dialogues (Li et al., 2023; Wang et al., 2023b).</p>
<h3>2.2 Psychological Scales</h3>
<p>Usually rated on Likert levels, psychological scales are commonly used for personality tests. Selfreport scales require participants to respond to a series of items analyzed through a specific scoring scheme to determine their personality traits. A scale rated on Likert levels, denoted as $\mathcal{L}=$ $(\mathcal{P}, \mathcal{D}, \mathcal{O}, f)$, comprises a set of items $\mathcal{P}$ (i.e., a questionnaire), a list of dimensions $\mathcal{D}$, a set of response options $\mathcal{O}$, and a scoring scheme $f$. Each item $p \in \mathcal{P}$ is a statement or question, positively</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The framework of InCharacter for personality tests on RPAs. Left: Previous methods use self-report scales, which prompt LLMs to select an option directly. Right: InCharacter adopts an interview-based approach comprising two phases: the interview and assessment phases. The interview phase elicits the behavioral, cognitive, and emotional patterns of RPAs that reflect their underlying mindsets. The assessment phase measures personalities based on interview results, with two alternative methodologies: option conversion and expert rating.
or negatively corresponding to a dimension $d \in \mathcal{D}$. For example, the item "Values artistic, aesthetic experiences." is positively related to the Openness dimension in the BFI. Participants select an ordinal response $o \in \mathcal{O}$ for each item, such as Agree. Typically, these options are numerically coded, e.g., " 1 " for Strongly Disagree and " 5 " for Strongly Agree. This process generates a response array $\mathcal{A}$. The scoring schema $f$ usually includes item-dimension mapping, identification of positive and negative items, conversion of options to scores, and an aggregation method (e.g., average or sum). Finally, the participant's personality scores $\mathcal{S}$ is derived as $\mathcal{S}=f(\mathcal{A})$, where $\mathcal{S}=\left(s_{d_{1}}, s_{d_{2}}, \ldots, s_{d_{|\mathcal{D}|}}\right)$ represents scores across each dimension. We summarize the notations used in this paper in Table 4.</p>
<h2>3 INCHARACTER</h2>
<p>This section introduces InCharacter, a novel personality assessment methodology designed explicitly for RPAs, utilizing an interview-based procedure. Figure 2 illustrates our two-stage framework. The interview stage is detailed in $\S 3.1$, followed by an elaboration of the assessment stage in §3.2.</p>
<h3>3.1 Interview</h3>
<p>INCHARACTER draws inspiration from the Structured Interview approach used in psychological testing (Trull et al., 1998). For a given scale, it transforms scale items into a series of open-ended questions, forming the basis for a structured interview. Then, our framework interviews RPAs using
these open-ended questions to elicit their perspectives on topics indicative of personality traits.</p>
<p>Constructing Question List We develop the structured interview question list based on items of the scale. Specifically, each item $p \in \mathcal{P}$ is transformed into an open-ended question $q$ via LLMs and manually checked. Consequently, the question list $\mathcal{Q}$ comprises $|\mathcal{P}|$ questions. For instance, in the BFI, the item "Values artistic, aesthetic experiences." is rephrased as "Do you values artistic, aesthetic experiences?"</p>
<p>Interviewing RPAs We interview an RPA $C$ of character $c$, by presenting each question $q \in \mathcal{Q}$ and recording its corresponding response $r$. To avoid context effects (Nikolić, 2010), each question is posed in an isolated context, thereby avoiding potential interference among the questions.</p>
<h3>3.2 Assessment</h3>
<p>Based on the interview results, the assessment phase quantitatively evaluates the score $s_{d}$ of the RPA $C$ across each dimension $d \in \mathcal{D}$. To this end, we introduce two distinct methodologies for measuring and analyzing RPA personalities leveraging LLMs: option conversion (OC) and expert rating (ER).</p>
<p>Option Conversion This technique leverages LLMs to convert a response $r$ for a question $q$ into a corresponding answer option $a \in \mathcal{O}$ for item $p$, effectively bridging the gap between closed-ended and open-ended question formats. The idea follows</p>
<p>the clinician-rated scales used in clinical psychiatry <em>Cuijpers et al. (2010); Uher et al. (2012)</em>, where professional clinicians assign ratings to each scale item based on their observations during patient interviews and compute the final scores following the scale’s scoring scheme. For example, a response “I believe that art transcends reality…” is converted to “5 (Strongly Agree)” for the item. Afterward, the answer list $\mathcal{A}$ is input to the scoring scheme $f$ to compute the final personality scores. In practice, we observe that even state-of-the-art LLMs like GPT-4 <em>OpenAI (2023)</em> exhibit notable inaccuracies in categorizing the attitudes of RPAs. Therefore, we further introduce a dimensional-specific option conversion (d-OC) strategy, which divides $(q,r)$ pairs according to dimensions and substitutes Likert levels, such as “4 (Agree)” and “2 (Disagree)”, with more descriptive options like “4 (Extroverted)” and “2 (Introverted)” in the prompts for LLMs.</p>
<p>Expert Rating In contrast with the one-by-one question conversion in OC, this method applies LLMs to directly evaluate personality scores of RPAs in each dimension, considering all corresponding $(q,r)$ pairs. This idea draws inspiration from the structured clinical interview in clinical psychiatry <em>First (2014)</em>, where clinicians assess patients using a predefined question list and derive final scores based on the responses without intermediate ratings or scoring schemes. The interviewer LLM is prompted with comprehensive descriptions of the scale, dimension, and score range. It then generates the final personality score for each dimension based on the pertinent responses. The advantage of ER is that it re-implements the scoring schema with the interviewer LLM, which can intelligently weigh individual $(q,r)$ pairs instead of using equal weights in OC. Hence, it better recognizes personality-indicative responses from RPAs.</p>
<p>Details of our prompts for OC and ER are available in §G in the appendix. To prevent the influence of data leakage in ER and d-OC, i.e., the interviewer LLM might have memorized the characters’ personality types. Hence, we anonymize the character names in the input prompts.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Preliminary Study</h3>
<h4>Can LLMs Simulate Human Interviewers?</h4>
<p>First, we validate the capability of interviewer</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>|  LLMs | Acc. | Pearson’s $r$ | Spearman’s $\rho$ | Kendall’s $\tau$  |
| --- | --- | --- | --- | --- |
|  <em>Option Conversation</em> |  |  |  |   |
|  Gemini | 69.5 | 54.5 | 55.9 | 53.2  |
|  GPT-3.5 | 57.5 | 34.6 | 36.2 | 32.4  |
|  GPT-4 | 71.0 | 60.0 | 64.3 | 59.5  |
|  <em>Dimension-specific Option Conversation</em> |  |  |  |   |
|  Gemini | 79.0 | 79.6 | 80.6 | 75.9  |
|  GPT-3.5 | 76.5 | 79.2 | 81.7 | 74.5  |
|  GPT-4 | 82.0 | 84.7 | 85.3 | 80.6  |
|  <em>Expert Rating (batch)</em> |  |  |  |   |
|  Gemini | 84.0 | 83.9 | 85.7 | 76.6  |
|  GPT-3.5 | 84.0 | 90.6 | 89.9 | 80.4  |
|  GPT-4 | 89.0 | 92.5 | 92.7 | 83.7  |</p>
<p>Table 1: The accuracy (Acc.) and consistency measurements of interviewer LLMs on the OC or ER tasks, compared with human labels.</p>
<p>LLMs on the OC and ER tasks, given the interview results of RPAs. We compare LLM predictions with human judgments. For each task, we sample 100 cases of state-of-the-art RPAs on the BFI and manually label them. For OC, the input is one question-response pair $(q,a)$ and the output is an option. For ER, the input is composed of multiple $(q,a)$ pairs, and the output is a score. Examples are shown in the right of Figure 2. We experiment with three LLMs, including GPT-4, GPT-3.5 and Gemini. The RPAs are detailed in §4.2. We report the Pearson’s $r$ <em>Pearson (1920)</em>, Spearman’s $\rho$ <em>Spearman (1961)</em> and Kendall’s $\tau$ <em>Kendall (1938)</em> correlations between human annotations and interviewer LLMs, as well as the accuracy. We consider LLM predictions varying from human labels by less than 1 point, exactly 1, or more than 1, as <em>right</em>, <em>close</em> (half-correct) or <em>wrong</em>, for accuracy calculation. More details can be found in §F.1.</p>
<p>The results presented in Table 1 lead to several findings. First, for ER, state-of-the-art LLMs can adequately rate participants’ personalities based on interview results. We observe that GPT-4 makes only 4% wrong cases in ER, primarily when RPAs give contradictory responses. Second, for OC, the LLMs show significant inaccuracy, while replacing Likert-level options with dimension-descriptive ones (d-OC) largely improves LLMs in this task. Considering the consistency measurements, state-of-the-art LLMs achieve acceptable performance in simulating human interviewers to assess RPA personalities through ER or d-OC.</p>
<h3>4.2 Experimental Settings</h3>
<p>The experiments in the subsequent part of this paper are based on the following settings:</p>
<p>RPAs and Characters This work primarily focuses on RPAs built on character data curated by ChatHaruhi (Li et al., 2023) and RoleLLM (Wang et al., 2023b). We select 32 widely-known characters, 16 from ChatHaruhi ${ }^{5}$ and 16 from RoleLLM. The characters are mainly from popular fictional works, such as Harry Potter, The Big Bang Theory and Genshin Impact. Please refer to §C for the detailed character selection process. The character data from ChatHaruhi and RoleLLM includes descriptions and dialogues used for system prompts and memory modules. To implement RPAs, we apply the Chat-Haruhi-Suzumiya ${ }^{6}$ library, and adopt GPT-3.5 (OpenAI, 2022) as the foundation LLM by default.</p>
<p>Psychological Scales We consider 14 psychological scales, including the BFI, the 16P, and 12 other scales following PsychoBench ${ }^{7}$ (Huang et al., 2024) to evaluate RPAs. Most scales apply scoring schemes like average and sum, while the 16 P is close-source and accessed via its API. Detailed introduction of these scales can be found in §B. Due to page limitations, the main body presents results for the BFI and 16P, while additional findings are detailed in the Appendix.</p>
<p>Personality Labels We collect labels for character personalities in the form of both scores and types, contributed by people familiar with these characters. From the PDb, an online platform for character personality annotation, we derive scores of the BFI and 16P on each dimension from its label percentage (e.g., $60 \%$ Extroverted). We then categorize it into a type of either positive, negative, or marginal if it is above $60 \%$, under $40 \%$, or otherwise. Then, we invite human annotators for comprehensive personality labels on all 14 scales. To select qualified annotators, we examine their character understanding of the BFI and 16P, matching with labels from the PDb. We invite two to three annotators for each character ( 93 in total for 32 characters) and average their results for improved reliability and objectivity. The scores are re-scaled into</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the unit interval $[0,1]$ and categorized into types similarly. We measure the inter-annotator consistency via Cohen's kappa coefficient (Cohen, 1968), and find the average coefficient across 14 scales $60.9 \%$. For the BFI and 16P, we adopt types from the PDb and scores from our invited annotators. The details about PDb annotations, our human annotation process, intra-annotator consistency, and other statistics can be found in §D.</p>
<p>Interviewer LLMs We use LLMs to accomplish the OC, d-OC and ER tasks in the assessment phase of INCHARACTER, or to extract selected options from RPA responses in self-report methods if RPAs do not provide exactly the choice. We consider four widely-acknowledged LLMs, including GPT-3.5, GPT-4, Gemini and Qwen1.5-110B ${ }^{8}$.</p>
<p>Metrics We consider two sets of metrics, namely: (1) Measured alignment (MA) compares the measured personalities of RPAs and human-annotated personalities of characters. It depends both on the performance of RPAs and the effectiveness of personality test methods. We categorize RPAs as positive or negative on each dimension if the scores are above or below the median of the scoring range. Then, we calculate mean absolute error (MAE) and accuracy to measure alignment at the score and type level, respectively. We re-scale MAE by dividing it with the scoring range length. For accuracy, we report the average $\mathbf{A c c}<em _Full="{Full" _text="\text">{\text {Dim }}$ and $\mathbf{A c c}</em>}}$, where correctness is judged on individual or all dimensions of each scale. The marginal dimensions of each character are ignored due to their ambiguity. (2) Personality consistency (PC) indicates whether the measured personality of RPAs is consistent across various settings. We analyze the standard variance at the item-level ( $\mathbf{S t d<em _Dim="{Dim" _text="\text">{\text {Item }}$ ), dimensionlevel (Std $</em>$ ), and score-level (Std $}<em _Item="{Item" _text="\text">{\text {Score }}$ ). Std $</em>}}$ and $\mathbf{S t d<em _Item="{Item" _text="\text">{\text {Dim }}$ measure the consistency of an RPA's scores on individual items. For INCHARACTER, we experiment with OC and d-OC to convert responses into scores. Std $</em>}}$ measures an RPA's consistency on the same item across multiple runs. $\mathbf{S t d<em _Score="{Score" _text="\text">{\text {Dim }}$ compares an RPA's responses across different items in the same dimension. $\mathbf{S t d}</em>$ denotes the variance of an RPA's score on each dimension across multiple runs. We divide these metrics by the length of the corresponding scoring range to re-scale them into the unit interval.}</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>5 Experimental Results</p>
<h3>5.1 Personality Tests on RPAs</h3>
<p>Baselines For InCharacter, we experiment with the ER, OC, and d-OC. For ER, we consider two settings, $\mathrm{ER}<em _batch="{batch" _text="\text">{\text {all }}$ and $\mathrm{ER}</em>$. For selfreport (SR) baselines, we follow previous work on LLM Psychometrics (Huang et al., 2024) to prompt RPAs to provide exactly a choice for each scale item. If their responses are not exactly the choices, we use interviewer LLMs to extract the choices. Then, the numbers are aggregated via the scoring schema to get the results. Besides, we experiment with SR-CoT, which enhances SR with chain-of-thought reasoning, i.e., explicitly asking RPAs to articulate their thoughts before choosing the options.}}$, where questionresponse pairs in one dimension are inputted into interviewer LLMs all-at-once or in-batch ${ }^{9</p>
<p>We compare these methods on the BFI and 16P. The experiments are repeated three times, including both the interview phase and the assessment phase. We report the average results of the three runs for MA metrics and $\mathbf{S t d}<em _Item="{Item" _text="\text">{\text {Dim }}$, and calculate $\mathbf{S t d}</em>$ across the three runs.}}$ and $\mathbf{S t d}_{\text {Score }</p>
<p>Alignment between RPAs' Measured Personalities and Characters' Labeled Personalities Then, we apply InCharacter to measure RPA personalities. According to the results in Table 2, we have the following analyses: (1) Using InCharacter with ER and GPT-4, the measured RPA personalities are highly aligned with ground truth labels of corresponding characters. This suggests that state-of-the-art RPAs well reproduce many of the characters' personality traits, and our methods accurately measure their personalities. (2) RPA personalities measured via InCharacter are better aligned with the characters than SR baselines. This validates the advantage of InCharACTER over self-report for personality tests on RPAs, which will be further discussed. (3) The alignment measured via INCHARACTER correlates with the interviewer LLMs' capability on the assessment tasks. For the interviewer LLMs, GPT-4 achieves the best metrics, while GPT-3.5, Gemini, and Qwen-110B also demonstrate satisfactory performance. For the assessment methods, InCharACTER with ER generally achieves better MA met-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Visualization of 32 RPAs' personalities on the BFI measured by different methods. We use principal component analysis (PCA) to map the results into 2D spaces. Black points represent the personality of GPT3.5 measured by corresponding methods.
rics than d-OC, while d-OC surpasses OC. However, Table 1 shows that interviewers LLMs still make mistakes on the ER and OC tasks, leading to potential inaccuracies in INCHARACTER and may underestimate the personality alignment of RPAs.</p>
<p>Robustness, Consistency and Distinctiveness of RPA Personalities Generally, the measured RPA personalities are robust across our observations. The $\mathbf{S t d}<em _Item="{Item" _text="\text">{\text {Score }}$ across three runs remain below 6\% in various settings, which underlines the reliability of personality tests and the robustness of RPA personalities. Then, we study the consistency at the item-level $\left(\mathbf{S t d}</em>}}\right)$ and dimension-level $\left(\mathbf{S t d<em _batch="{batch" _text="\text">{\text {Dim }}\right)$. With InCharacter, after converting the interview results into scores via d-OC and GPT-4, We observe that RPAs respond to the same items consistently across multiple runs and exhibit a relatively consistent personality across different items on the same dimension. We visualize the distribution of RPA personalities on the BFI in Figure 3, and find that RPAs exhibit distinct personalities, especially when measured by INCHARACTER with $\mathrm{ER}</em>$ and GPT-4.}</p>
<p>Self-report v.s. Interview-based Methods As shown in Table 2, the personalities measured by InCharacter are more aligned with the characters, compared with self-report. Meanwhile, in interview-based tests, RPAs exhibit more consistent personalities across different questions, as well as greater distinctiveness, shown in Figure 3. These findings confirm the advantages of interview-based tests over self-report in measuring RPA personalities. Although SR-CoT attempts to enhance SR with the thought process, its improvement over SR is limited, and it encounters poor $\mathbf{S t d}_{\text {Item }}$. Further analyses and comparisons are detailed in §F.2. We also experiment with enhancing self-report methods with in-context learning in §F.8, which under-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Interviewer Model</th>
<th style="text-align: center;">The Big Five Inventory</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">The 16 Personalities</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc $_{\text {Dim }}$</td>
<td style="text-align: center;">Acc $_{\text {Full }}$</td>
<td style="text-align: center;">MAE $\downarrow$</td>
<td style="text-align: center;">Std $_{\text {Item }}$</td>
<td style="text-align: center;">Std $_{\text {Dim }}$</td>
<td style="text-align: center;">Std $_{\text {Score }}$</td>
<td style="text-align: center;">Acc $_{\text {Dim }}$</td>
<td style="text-align: center;">Acc $_{\text {Full }}$</td>
<td style="text-align: center;">MAE $\downarrow$</td>
<td style="text-align: center;">Std $_{\text {Item }}$</td>
<td style="text-align: center;">Std $_{\text {Dim }}$</td>
<td style="text-align: center;">Std $_{\text {Score }}$</td>
</tr>
<tr>
<td style="text-align: center;">Self-report Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">Qwen-110B</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">1.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;">SR-CoT</td>
<td style="text-align: center;">Qwen-110B</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">5.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">5.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">4.8</td>
</tr>
<tr>
<td style="text-align: center;">INCHARACTER: Interview-based Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OC</td>
<td style="text-align: center;">Qwen-110B</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: center;">d-OC</td>
<td style="text-align: center;">Qwen-110B</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">2.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">4.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{ER}_{\text {all }}$</td>
<td style="text-align: center;">Qwen-110B</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.4</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{ER}_{\text {batch }}$</td>
<td style="text-align: center;">Qwen-110B</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Metrics on personalities of the selected RPAs were measured via various personality test methods on the big five inventory and 16 personalities. For MA metrics, the best results are bolded, and the second best ones are underlined. $\mathbf{S t d}<em _Dim="{Dim" _text="\text">{\text {Item }}$ and $\mathbf{S t d}</em>$ are derived from scores of individual items, and are hence inapplicable for ER.
mines the measurements instead.}</p>
<h2>Comprehensive Personality Tests on 14 Scales</h2>
<p>We extend personality tests on RPAs to 14 psychological scales, using INCHARACTER with $\mathrm{ER}<em _Dim="{Dim" _text="\text">{\text {batch }}$ and GPT-3.5. In Figure 4, we demonstrate the Acc $</em>$ of $78.9 \%$, covering personality traits (BFI, 16P), dark personalities (DTDD), interpersonal relationships (BSRI, ECR-R), basic interests (CABIN), motivation (GSE, LMS) and emotional intelligence (EIS, WLEIS), etc. The detailed metrics on individual scales and individual dimensions are listed in §F.5.}}$ of state-of-the-art RPAs across each scale. Overall, we observe that the RPAs exhibit personalities align with the target characters in comprehensive aspects with an average Acc $_{\text {Dim }</p>
<p>More Results and Analyses Besides, we study the importance of using personality scales in §F.6, and find that scale questions are crucial for eliciting personality-indicative responses from RPAs. We also observe that some scale questions go beyond the knowledge scope of certain characters in §F.7,
and adapting these questions for each character yields more accurate measurements.</p>
<h3>5.2 Personality Fidelity of Different RPAs</h3>
<p>With INCHARACTER, we compare the personality fidelity of various types of RPAs, covering different character data and foundation models. We apply INCHARACTER with $\mathrm{ER}_{\text {batch }}$ and use GPT-3.5 as the interviewer LLM for personality tests. We report the MA metrics on the BFI and 16P in Table 3.</p>
<p>Character Data for RPAs Typically, existing RPAs utilize two types of character data: descriptions and memories. Character descriptions serve as the system prompts for RPAs, while memories consist of characters' experiences and dialogues used for retrieval. With GPT-3.5, we evaluate RPAs with only descriptions (D), only memories $(\mathrm{M})$, and a combination of both $(\mathrm{D}+\mathrm{M})$. The results in Table 3 reveal that: (1) With only description, RPAs achieve MA metrics close to the full $\mathrm{D}+\mathrm{M}$ setup, highlighting the importance of character de-</p>
<table>
<thead>
<tr>
<th>Agent Types</th>
<th></th>
<th>The Big Five Inventory</th>
<th></th>
<th></th>
<th>The 16 Personalities</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>LLMs</td>
<td>Data</td>
<td>$\mathbf{A c c}_{\text {B1 } \mathbf{8}}$</td>
<td>$\mathbf{A c c}_{\text {ru11 }}$</td>
<td>MAE $\downarrow$</td>
<td>$\mathbf{A c c}_{\text {B1 } \mathbf{8}}$</td>
<td>$\mathbf{A c c}_{\text {ru11 }}$</td>
<td>MAE $\downarrow$</td>
</tr>
<tr>
<td>w/ General Open-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen 7B</td>
<td>D+M</td>
<td>60.5</td>
<td>9.4</td>
<td>24.3</td>
<td>67.8</td>
<td>21.9</td>
<td>27.9</td>
</tr>
<tr>
<td>OpenChat-3.5 7B</td>
<td>D+M</td>
<td>63.1</td>
<td>6.2</td>
<td>23.1</td>
<td>76.9</td>
<td>40.6</td>
<td>24.6</td>
</tr>
<tr>
<td>Mistral-2 7B</td>
<td>D+M</td>
<td>66.2</td>
<td>18.8</td>
<td>21.3</td>
<td>68.6</td>
<td>21.9</td>
<td>26.0</td>
</tr>
<tr>
<td>LLaMa-2-Chat 13B</td>
<td>D+M</td>
<td>66.9</td>
<td>12.5</td>
<td>26.8</td>
<td>66.9</td>
<td>28.1</td>
<td>27.7</td>
</tr>
<tr>
<td>Mixtral 8x7B</td>
<td>D+M</td>
<td>68.2</td>
<td>15.6</td>
<td>20.8</td>
<td>71.9</td>
<td>31.2</td>
<td>25.3</td>
</tr>
<tr>
<td>w/ Specialized Open-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CharacterGLM 6B</td>
<td>D+M</td>
<td>54.1</td>
<td>0.0</td>
<td>25.8</td>
<td>52.1</td>
<td>15.6</td>
<td>29.7</td>
</tr>
<tr>
<td>RP-Qwen 7B</td>
<td>D+M</td>
<td>60.5</td>
<td>0.0</td>
<td>23.8</td>
<td>64.5</td>
<td>15.6</td>
<td>28.6</td>
</tr>
<tr>
<td>RP-Mistral-2 7B</td>
<td>D+M</td>
<td>70.1</td>
<td>18.8</td>
<td>21.7</td>
<td>69.4</td>
<td>28.1</td>
<td>26.1</td>
</tr>
<tr>
<td>w/ Close-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>character.ai</td>
<td>D*</td>
<td>52.2</td>
<td>9.4</td>
<td>31.2</td>
<td>52.9</td>
<td>21.9</td>
<td>31.6</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>D</td>
<td>71.3</td>
<td>21.9</td>
<td>21.1</td>
<td>78.5</td>
<td>43.8</td>
<td>22.0</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>M</td>
<td>71.3</td>
<td>18.8</td>
<td>21.8</td>
<td>71.9</td>
<td>31.2</td>
<td>26.0</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>D+M</td>
<td>72.0</td>
<td>21.9</td>
<td>18.8</td>
<td>79.3</td>
<td>43.8</td>
<td>22.6</td>
</tr>
<tr>
<td>GPT-4</td>
<td>D+M</td>
<td>73.9</td>
<td>25.0</td>
<td>19.8</td>
<td>76.0</td>
<td>43.8</td>
<td>23.2</td>
</tr>
</tbody>
</table>
<p>Table 3: Measured alignment (\%) of RPAs with different foundation models and character data. D and M represent descriptions and memories respectively, and D* denote private descriptions of character.ai.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Measured alignment ( $\mathbf{A c c}_{\text {B1 } \mathbf{8}}, \%$ ) of state-of-the-art RPAs on 14 scales.
scription in shaping RPA personalities. (2) RPAs can well mimic character personalities exhibited in their past experiences, e.g., extraversion and openness, even if the experiences are not directly related to scale questions. Additionally, we compare RPAs with character data from ChatHaruhi, RoleLLM and character.ai (c.ai) in §F.3.</p>
<p>Foundation Models for RPAs We consider three types of LLMs: (1) General open-source models, including Qwen-7B (Bai et al., 2023), OpenChat3.5 7B (Wang et al., 2024), Mistral-2 7B (Jiang et al., 2023), Llama-2-chat 13B (Touvron et al., 2023) and Mixtral 8x7B (Jiang et al., 2024). (2) Specialized open-source models for RPAs, including Character-GLM 6B (Zhou et al., 2023), RPQwen $7 \mathrm{~B}^{10}$, and RP-Mistral-2 7B. We train RP-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Mistral-2 7B with details shown in §E.2. (3) Closesource models: GPT-3.5 and GPT-4.</p>
<p>The results are shown in Table 10. We observe that, (1) RPAs with GPT-3.5 and GPT-4 achieve the best personality fidelity, and GPT-4 does not significantly surpass GPT-3.5. (2) With state-of-theart open-source LLMs, RPAs can also reproduce character personalities. However, such capacity depends largely on their ability to use specific languages, shown in §F.4. (3) Incremental fine-tuning on open-source LLMs with role-playing datasets brings limited improvement in personality fidelity, especially when they are already equipped with excellent role-playing ability.</p>
<p>Close-source RPAs We also experiment with close-source RPAs from character.ai, which are based on their private foundation models and data. As shown in Table 10, these RPAs barely reproduce character personalities, significantly underperforming GPT-3.5 (D) which shares a similar framework. According to our observation, while character.ai RPAs provide human-like answers, their answers tend to be compliant and pleasing to users, instead of reproducing the target characters. Examples and further analysis are shown in §H.2.</p>
<h2>6 Related Work</h2>
<p>Role-Playing Agents RPAs learn and leverage character data in various ways, including training on raw scripts or dialogues (Shao et al., 2023b), prompting with character descriptions (Zhou et al., 2023), and retrieval from character experiences ( Li</p>
<p>et al., 2023). Existing efforts mainly focus on developing character-specific RPAs or foundation models for RPAs. The former includes ChatHaruhi (Li et al., 2023) and RoleLLM (Wang et al., 2023b), which target well-established fictional characters. The latter includes character.ai and CharacterGLM (Zhou et al., 2023). For evaluation, prior research mainly concentrates on two facets: 1) Character-independent capabilities, which include conversational abilities (Duan et al., 2023), human-likeness (Tu et al., 2024), multi-turn consistency (Shao et al., 2023a), and attractiveness (Zhou et al., 2023); 2) Character fidelity, including the characters' knowledge, experience, and linguistic patterns (Wang et al., 2023b; Shao et al., 2023a). Overall, these methods generally require test sets for each character, and neglect the evaluation of RPAs' underlying mindset.
Psychological Analysis on LLMs Recent studies conducted personality tests using the BFI (Romero et al., 2023; Karra et al., 2022; Li et al., 2022; Jiang et al., 2022; Safdari et al., 2023; Bodroza et al., 2023), the MBTI (Rutinowski et al., 2024; Pan and Zeng, 2023) on various LLMs. Notably, Huang et al. (2023c) verified the reliability of the BFI on GPT-3.5, while Safdari et al. (2023) demonstrated the construct validity of the BFI on the PaLM model family. Other studies also investigates other mental perspectives, such as emotions (Huang et al., 2023a), values (Miotto et al., 2022; Rutinowski et al., 2024; Hartmann et al., 2023), consciousness (Butlin et al., 2023), and mental illness (Coda-Forno et al., 2023). Our research diverges by employing personality tests as an innovative approach to assessing character fidelity in RPAs.</p>
<h2>7 Conclusion</h2>
<p>In this study, we investigate the personality fidelity in RPAs, i.e., whether RPAs reproduce personalities of their intended characters. Addressing the shortcomings of previous methods on RPAs, we propose INCHARACTER, an interview-based approach that accurately measures RPA personalities based on their elicited mindsets and behaviors. Our experiments span various types of RPAs, covering 32 characters on 14 psychological scales. The results validate the effectiveness of INCHARACTER in measuring RPA personalities. Afterward, with INCHARACTER, we comprehensively evaluate personality fidelity in existing RPAs, discovering that
state-of-the-art RPAs successfully portray many personality traits of the characters.</p>
<h2>Limitations</h2>
<p>There are several limitations in this study. First, the personality measurement in this paper relies on the interviewer LLMs. Consequently, the accuracy of the measured results may be compromised by potential errors or biases inherent in LLMs, potentially leading to an underestimation of the personality fidelity in RPAs. Second, the personalities of humans or fictional characters can change over time. Since we use one static personality label for a specific character, there may be noise in our evaluation. For instance, the character of James Bond has experienced significant development over the past two decades across various films and television series. Our character annotations are derived from a singular, fixed time point in his storyline. Additionally, the progressive changes in RPA personalities remain unexplored within existing literature. We leave the study of RPA personality dynamics for future research.</p>
<h2>Ethical Statement</h2>
<p>We hereby acknowledge that all authors of this work are aware of the provided ACL Code of Ethics and honor the code of conduct.</p>
<p>Use of Human Annotations In conducting our research, we have employed a methodology that incorporates personality labels, which were gathered through the online platform and by engaging a group of annotators. These annotators, who are university students, play a crucial role in our research process. To ensure fair treatment and to value their contribution, we offer them compensation that significantly exceeds the local minimum wage standards. Moreover, we maintain transparency regarding the application and purpose of their annotations, securing their informed consent for the use of these annotations in our research endeavors. Additionally, we are committed to upholding the privacy rights of our annotators throughout the annotation process, ensuring a respectful and ethical research environment.</p>
<p>Risks In this paper, we introduce a novel approach, referred to as INCHARACTER, designed to assess the personalities of Role-Play Agent (RPA) entities. An integral component of our evaluation</p>
<p>process involves the use of interviewer Large Language Models (LLMs), which, while innovative, could potentially introduce bias into the assessment outcomes. It is important to acknowledge this limitation as LLMs may reflect the inherent biases present in their training data. Furthermore, our evaluation encompasses a comprehensive analysis across 14 personality scales, notably including the Dark Triad of Personality (DTDD) scale, which focuses on darker personality traits. While this inclusion is aimed at providing a thorough understanding of RPA personalities, it raises ethical concerns regarding the potential for generating harmful content. This aspect underscores the need for careful consideration and implementation of safeguards to mitigate the risks associated with exploring dark personality traits in RPAs.</p>
<h2>Acknowledgment</h2>
<p>This work is funded by the Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902). This work originates from a project on Chat-Haruhi-Suzumiya proposed by Cheng Li. We owe thanks to the early contributors. We are thankful for the support provided by Zheli Xuan at Wuhan University, and Dingding Hu at the Institute of Psychology, Chinese Academy of Sciences, who offered invaluable assistance as psychology researchers. Our gratitude extends to our invited annotators, primarily from Fudan University, for their contribution of high-quality annotations of character personalities. We also acknowledge the assistance provided by Rui Fu and Wenxin Gao at Fudan University during the annotation process. Finally, we express our sincere gratitude for the precious comments and suggestions from Yikai Zhang, Xinfeng Yuan, and Shuang Li at Fudan University.</p>
<h2>References</h2>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. ArXiv preprint, abs/2309.16609.</p>
<p>Murray R Barrick and Michael K Mount. 1991. The big five personality dimensions and job performance: a meta-analysis. Personnel psychology, 44(1):1-26.</p>
<p>Sandra L Bem. 1981. Bem sex role inventory. Journal of personality and social psychology.</p>
<p>Bojana Bodroza, Bojana M Dinic, and Ljubisa Bojic. 2023. Personality testing of gpt-3: Limited temporal
reliability, but highlighted social desirability of gpt3's personality instruments results. ArXiv preprint, abs/2306.04308.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M Fleming, Chris Frith, Xu Ji, et al. 2023. Consciousness in artificial intelligence: Insights from the science of consciousness. ArXiv preprint, abs/2308.08708.</p>
<p>Julian Coda-Forno, Kristin Witte, Akshay K Jagadish, Marcel Binz, Zeynep Akata, and Eric Schulz. 2023. Inducing anxiety in large language models increases exploration and bias. ArXiv preprint, abs/2304.11111.</p>
<p>Jacob. Cohen. 1968. Weighed kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. Psychological Bulletin, 70(4):213220.</p>
<p>Pim Cuijpers, Juan Li, Stefan G Hofmann, and Gerhard Andersson. 2010. Self-reported versus clinicianrated symptoms of depression as outcome measures in psychotherapy research on depression: a metaanalysis. Clinical psychology review, 30(6):768-778.</p>
<p>Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, and Kai Chen. 2023. Botchat: Evaluating llms' capabilities of having multi-turn dialogues. ArXiv preprint, abs/2310.13650.</p>
<p>Michael B First. 2014. Structured clinical interview for the dsm (scid). The encyclopedia of clinical psychology, pages 1-6.</p>
<p>Jingsheng Gao, Yixin Lian, Ziyi Zhou, Yuzhuo Fu, and Baoyuan Wang. 2023. LiveChat: A large-scale personalized dialogue dataset automatically constructed from live streaming. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1538715405, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The political ideology of conversational ai: Converging evidence on chatgpt's pro-</p>
<p>environmental, left-libertarian orientation. Available at SSRN 4316084.</p>
<p>Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. 2023a. Emotionally numb or empathetic? evaluating how llms feel using emotionbench. ArXiv preprint, abs/2308.03656.</p>
<p>Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, and Michael R Lyu. 2023b. Chatgpt an enfj, bard an istj: Empirical study on personalities of large language models. ArXiv preprint, abs/2305.19926.</p>
<p>Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, and Michael R Lyu. 2023c. Revisiting the reliability of psychological scales on large language models. ArXiv preprint, abs/2305.19926.</p>
<p>Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. 2024. Who is chatgpt? benchmarking llms' psychological portrayal using psychobench. In Proceedings of the Twelfth International Conference on Learning Representations.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. ArXiv preprint, abs/2310.06825.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. ArXiv preprint, abs/2401.04088.</p>
<p>Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, and Yixin Zhu. 2022. Mpi: Evaluating and inducing personality in pre-trained language models. ArXiv preprint, abs/2206.07550.</p>
<p>Saketh Reddy Karra, Son The Nguyen, and Theja Tulabandhula. 2022. Estimating the personality of white-box language models. ArXiv preprint, abs/2204.12000.</p>
<p>Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81-93.</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, et al. 2023. Chatharuhi: Reviving anime character in reality via large language model. ArXiv preprint, abs/2308.09597.</p>
<p>Xingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei Huang, Lin Qiu, and Lidong Bing. 2022. Does gpt-3 demonstrate psychopathy? evaluating large language models from a psychological perspective. ArXiv preprint, abs/2212.10529.</p>
<p>Marilù Miotto, Nicola Rossberg, and Bennett Kleinberg. 2022. Who is GPT-3? an exploration of personality, values and demographics. In Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS), pages 218-227, Abu Dhabi, UAE. Association for Computational Linguistics.</p>
<p>Danko Nikolić. 2010. The brain is a context machine. Review of psychology, 17(1):33-38.</p>
<p>OpenAI. 2022. Openai: Introducing chatgpt.
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.</p>
<p>Keyu Pan and Yawen Zeng. 2023. Do llms possess a personality? making the mbti test an amazing evaluation for large language models. ArXiv preprint, abs/2307.16180.</p>
<p>Karl Pearson. 1920. Notes on the history of correlation. Biometrika, 13(1):25-45.</p>
<p>Peter Romero, Stephen Fitz, and Teruo Nakatsuma. 2023. Do gpt language models suffer from split personality disorder? the advent of substrate-free psychometrics. ResearchSquare preprint.</p>
<p>A John Rush, William Hiser, and Donna E Giles. 1987. A comparison of self-reported versus clinicianrelated symptoms in depression. The Journal of clinical psychiatry, 48(6):246-248.</p>
<p>Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, Moritz Roidl, and Markus Pauly. 2024. The self-perception and political biases of chatgpt. Human Behavior and Emerging Technologies, 2024(1):7115633.</p>
<p>Mustafa Safdari, Greg Serapio-García, Clément Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, and Maja Matarić. 2023. Personality traits in large language models. ArXiv preprint, abs/2307.00184.</p>
<p>Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023a. Character-LLM: A trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13153-13187, Singapore. Association for Computational Linguistics.</p>
<p>Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023b. Character-LLM: A trainable agent for roleplaying. In Proceedings of the 2023 Conference on</p>
<p>Empirical Methods in Natural Language Processing, pages 13153-13187, Singapore. Association for Computational Linguistics.</p>
<p>Vera Sorin, Danna Brin, Yiftach Barash, Eli Konen, Alexander Charney, Girish Nadkarni, and Eyal Klang. 2023. Large language models (llms) and empathy-a systematic review. medRxiv, pages 2023-08.</p>
<p>Charles Spearman. 1961. The proof and measurement of association between two things.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.</p>
<p>Timothy J Trull, Thomas A Widiger, J David Useda, Jay Holcomb, Bao-Tran Doan, Seth R Axelrod, Barry L Stern, and Beth S Gershuny. 1998. A structured interview for the assessment of the five-factor model of personality. Psychological assessment, 10(3):229.</p>
<p>Quan Tu, Chuanqi Chen, Jinpeng Li, Yanran Li, Shuo Shang, Dongyan Zhao, Ran Wang, and Rui Yan. 2023. Characterchat: Learning towards conversational ai with personalized social support. ArXiv preprint, abs/2308.10278.</p>
<p>Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. 2024. Charactereval: A chinese benchmark for role-playing conversational agent evaluation. ArXiv preprint, abs/2401.01275.</p>
<p>Rudolf Uher, Roy H Perlis, Anna Placentino, Mojca Zvezdana Dernovšek, Neven Henigsberg, Ole Mors, Wolfgang Maier, Peter McGuffin, and Anne Farmer. 2012. Self-report and clinician-rated measures of depression severity: can one replace the other? Depression and anxiety, 29(12):1043-1049.</p>
<p>Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024. Openchat: Advancing open-source language models with mixed-quality data. In The Twelfth International Conference on Learning Representations.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291.</p>
<p>Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. 2023b. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. ArXiv preprint, abs/2310.00746.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In</p>
<p>Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.</p>
<p>Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. 2023. Characterglm: Customizing chinese conversational ai characters with large language models. ArXiv preprint, abs/2311.16832.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task Formulation</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{L}$</td>
<td style="text-align: center;">A Likert Scale, composed of $\mathcal{L}=(\mathcal{P}, \mathcal{D}, \mathcal{O}, f)$.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{P}$</td>
<td style="text-align: center;">A set of items, where $p \in \mathcal{P}$ is an item.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}$</td>
<td style="text-align: center;">A list of dimensions, where $d \in \mathcal{D}$ is a dimension.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{O}$</td>
<td style="text-align: center;">A set of options, where $o \in \mathcal{O}$ is an option for a $p$.</td>
</tr>
<tr>
<td style="text-align: center;">$f$</td>
<td style="text-align: center;">A scoring scheme, i.e., a function.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{A}$</td>
<td style="text-align: center;">A response list, where $a \in \mathcal{A}$ is a response for a $p$.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">The participant's personality scores across each dimension, i.e., $\mathcal{S}=\left(s_{d_{1}}, s_{d_{2}}, \ldots, s_{d_{(D)}}\right)$.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{Q}$</td>
<td style="text-align: center;">A set of questions, where each $q \in \mathcal{Q}$ is transformed from a $p$.</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">An RPA of a character $c$.</td>
</tr>
<tr>
<td style="text-align: center;">Methods</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">Self-report baseline.</td>
</tr>
<tr>
<td style="text-align: center;">SR-CoT</td>
<td style="text-align: center;">SR with chain-of-thought reasoning.</td>
</tr>
<tr>
<td style="text-align: center;">ER</td>
<td style="text-align: center;">INCHARACTER with expert rating.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{ER}_{\text {batch }}$</td>
<td style="text-align: center;">ER where question-response pairs in one dimension are inputted in-batch.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{ER}_{\text {all }}$</td>
<td style="text-align: center;">ER where question-response pairs in one dimension are inputted all-at-once.</td>
</tr>
<tr>
<td style="text-align: center;">OC</td>
<td style="text-align: center;">INCHARACTER with option conversion.</td>
</tr>
<tr>
<td style="text-align: center;">d-OC</td>
<td style="text-align: center;">OC with dimension-specific options.</td>
</tr>
<tr>
<td style="text-align: center;">Metrics</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">Mean absolute error.</td>
</tr>
<tr>
<td style="text-align: center;">Acc $_{\text {Sta }}$</td>
<td style="text-align: center;">Accuracy on individual dimensions.</td>
</tr>
<tr>
<td style="text-align: center;">Acc $_{\text {all }}$</td>
<td style="text-align: center;">Accuracy on all dimensions of a scale.</td>
</tr>
<tr>
<td style="text-align: center;">Std $_{\text {Iten }}$</td>
<td style="text-align: center;">Standard variance of an RPA's responses (converted to scores) on the same item across multiple runs.</td>
</tr>
<tr>
<td style="text-align: center;">Std $_{\text {Sta }}$</td>
<td style="text-align: center;">Standard variance of an RPA's responses (converted to scores) across different items in the same dimension.</td>
</tr>
<tr>
<td style="text-align: center;">Std $_{\text {Iten }}$</td>
<td style="text-align: center;">Standard variance of an RPA's score on each dimension across multiple runs.</td>
</tr>
</tbody>
</table>
<p>Table 4: A notation table.</p>
<p>In Table 4, we list the notations and abbreviations in this paper, along with their definitions.</p>
<h2>B Psychological Scales</h2>
<p>Big Five Inventory The BFI serves as a prominent instrument for assessing personality dimensions. This model, often encapsulated by the acronym "OCEAN," encompasses five critical traits: (1) Openness to Experience ( $O$ ), which highlights a person's curiosity, inventiveness, and appreciation for art, emotion, adventure, and novel concepts. (2) Conscientiousness (C), indicating how much an individual exhibits organization, reliability, and responsibility. (3) Extraversion (E), denoting the level to which a person is sociable and energized by interactions with others. (4) Agreeableness $(A)$, assessing an individual's kindness, empathy, and ability to cooperate with others. (5) Neuroticism $(N)$, gauging the tendency of an individual to experience negative feelings such as anxi-
ety, anger, and sadness, as opposed to being more emotionally resilient and less stress-susceptible.</p>
<p>Eysenck Personality Questionnaire (Revised) The Revised Eysenck Personality Questionnaire (EPQ-R) serves as a psychological instrument for gauging distinct personality trait variances in individuals. It identifies three principal traits: (1) Extraversion ( $E$ ), which assesses whether a person tends to be more sociable, energetic, and outgoing as opposed to being introverted, quiet, and reserved. (2) Neuroticism ( $N$ ), which gauges emotional steadiness. These dimensions (i.e., E and N) share similarities with those found in the BFI. (3) Psychoticism $(P)$, which is indicative of a person's inclination towards solitude, a lack of empathy, and a propensity for aggression or a tough-minded attitude. This trait is crucial to understand as indicative of personality characteristics rather than serious mental health conditions. (4) Beyond these primary scales, the EPQ-R also incorporates a $L y$ ing Scale ( $L$ ) intended to identify responses aimed at social desirability. This scale evaluates the extent to which an individual may attempt to portray themselves in a more favorable light.</p>
<p>Dark Triad Dirty Dozen The DTDD is identified as a brief, 12 -item measure crafted to evaluate the trio of principal personality characteristics known as the Dark Triad, encompassing: (1) Narcissism $(N)$, characterized by an exaggerated sense of one's own significance, an obsession with dreams of boundless success, and a craving for undue admiration. (2) Machiavellianism (M), indicative of a deceitful approach in social interactions and a skeptical indifference to ethical principles. (3) Psychopathy $(P)$, which includes tendencies towards impulsiveness, a deficiency in empathy, and hostile relations with others. These Dark Triad personality dimensions are typically viewed as the antithesis of the characteristics measured by the BFI or the EPQ-R, which represent "Light" traits.</p>
<p>The NERIS Type Explorer The 16Personalities utilizes the acronym format introduced by MyersBriggs for its simplicity and convenience, with an additional letter to accommodate five rather than four scales. However, unlike Myers-Briggs or other theories based on the Jungian model, the incorporation of Jungian concepts such as cognitive functions, or their prioritization, has not been undertaken. Instead, they rework and rebalance the dimensions of personality in the BFI personality</p>
<p>traits. The personality types are based on five independent spectrums, with all letters in the type code (e.g., INFJ-A) referring to one of the two sides of the corresponding spectrum.</p>
<p>Bem's Sex Role Inventory The BSRI assesses the degree to which individuals identify with traditionally masculine and feminine characteristics. Rather than focusing on behaviors, such as participation in sports or cooking, this tool evaluates psychological characteristics, including assertiveness and gentleness. Participants are divided into four groups based on whether their average scores exceed the median for each component. These groups are designated as Masculine (M: Yes; F: No), Feminine (M: No; F: Yes), Androgynous (M: Yes; F: Yes), and Undifferentiated (M: No; F: No).</p>
<h2>Comprehensive Assessment of Basic Interests</h2>
<p>The CABIN provides an exhaustive evaluation for identifying 41 essential dimensions of vocational interest. Following this evaluation, the researchers introduce a model of interest consisting of eight dimensions, named SETPOINT. This model includes dimensions such as Health Science, Creative Expression, Technology, People, Organization, Influence, Nature, and Things. These core dimensions are also adaptable to a six-dimension framework, which is prevalently recognized within the interest research community. This framework aligns with Holland's RIASEC model, which features the dimensions: Realistic, Investigate, Artistic, Social, Enterprising, and Conventional.</p>
<p>Implicit Culture Belief The ICB scale measures the extent to which individuals think a person's ethnic culture influences their development. Scoring higher on this scale indicates a firm belief that a person's ethnic culture is the main factor shaping their identity, values, and perspective on the world. On the other hand, a lower score on the scale denotes a belief in the ability of an individual to shape their own identity through hard work, commitment, and education.</p>
<h2>Experiences in Close Relationships (Revised)</h2>
<p>The ECR-R is a self-assessment tool crafted to gauge variations in adult attachment styles, particularly within the realm of romantic relationships. As an enhanced iteration of the original ECR scale, the ECR-R introduces refinements in quantifying attachment tendencies. It assesses two primary aspects: (1) Attachment Anxiety indicates the degree to which a person fears rejection or abandonment
by their romantic partners. (2) Attachment Avoidance assesses the degree to which a person prefers to keep emotional and physical distance from their partners, often stemming from unease with closeness or reliance.</p>
<p>General Self-Efficacy The GSE Scale evaluates a person's confidence in their capacity to address diverse demanding situations in life. This confidence, known as "self-efficacy," plays a pivotal role in social cognitive theory and is associated with numerous health outcomes, motivational levels, and performance measures. An elevated score on this scale indicates a person's strong belief in their ability to confront and manage challenging circumstances, undertake new or complex tasks, and navigate through the resultant difficulties. On the flip side, a lower score on the scale suggests a lack of self-assurance in handling challenges, rendering individuals more susceptible to experiencing helplessness, anxiety, or engaging in avoidance behaviors when encountering hardships.</p>
<p>Life Orientation Test (Revised) The LOT-R is designed to assess variations in optimism and pessimism among individuals. It includes ten questions, with an interesting aspect being that only six of these questions contribute to the test's score. The other four are designed as filler items, cleverly integrated to obscure the test's primary focus. Within the scored questions, equal numbers are dedicated to evaluating optimism and pessimism-three for each. A tendency towards higher scores in optimism and lower in pessimism signifies a predominantly optimistic outlook.</p>
<p>Love of Money Scale The LMS evaluates the perspectives and feelings of people regarding money. This tool aims to quantify the degree to which people perceive money as a symbol of power, success, and liberty, along with its significance in influencing behaviors and choices. The LMS identifies three key dimensions: (1) Rich reflects the degree to which people link money with success and accomplishment. (2) Motivator determines the extent to which money serves as an incentive in someone's life, i.e., how much individuals are motivated by monetary rewards in their decisions and behaviors. (3) Important assesses the level of importance people attribute to money, affecting their principles, objectives, and perspective of the world.</p>
<p>Emotional Intelligence Scale The EIS serves as a self-assessment tool for evaluating multiple as-</p>
<p>pects of emotional intelligence. This instrument emphasizes various elements of emotional intelligence, notably the perception, management, and application of emotions. It is extensively utilized in the field of psychology to investigate how emotional intelligence influences different outcomes, including personal well-being, professional performance, and social interactions.</p>
<h1>Wong and Law Emotional Intelligence Scale</h1>
<p>Similar to EIS, the WLEIS is also a self-report instrument designed for evaluating emotional intelligence. However, it distinctly includes four subscales that represent the primary aspects of emotional intelligence: (1) Self-emotion appraisal (SEA) focuses on an individual's proficiency in identifying and understanding their emotions. (2) Others' emotion appraisal (OEA) is about the skill of recognizing and comprehending the emotions of others. (3) Use of emotion (UOE) deals with the ability to employ emotions to aid various mental processes, like reasoning and problem-solving. (4) Regulation of emotion (ROE) is concerned with the ability to control and adjust emotions within oneself and in others.</p>
<p>Empathy Scale Empathy, defined as the capacity to perceive and resonate with the emotions of another, is traditionally divided into cognitive and emotional empathy. Cognitive empathy, also known as "perspective-taking," entails the mental faculty to identify and comprehend the thoughts, beliefs, or feelings of someone else. Conversely, emotional empathy involves the vicarious experience of the emotions felt by another individual.</p>
<h2>C Character Selection</h2>
<p>When selecting characters for RPAs, we consider the following factors: (1) There exist multiple RPAs for the characters, e.g., both ChatHaruhi and character.ai have their RPA for Hermione Granger. (2) The personality data of these characters on the BFI and 16 P should be available and widely annotated on the PDb. (3) The selected characters should possess diversified personalities. Hence, the pipeline of our character selection process is composed of the following steps:</p>
<ol>
<li>Initially, we collect characters with RPAs and character data curated by ChatHaruhi and RoleLLM. Then, we search for their counterparts in character.ai, and keep only those with character.ai RPAs.</li>
<li>We collect personality data for these characters from the PDb. Characters with less than ten annotations on either the BFI or the 16P are discarded.</li>
<li>We categorize the remaining characters based on their BFI personality types (5-letter code such as SLOAI). Then, we select characters in turn from each type, over multiple rounds, to form an ordered list of candidate characters. If a certain type has no characters left, we skip it.</li>
<li>Finally, We pick the first 32 characters in the candidate list. Then, we manually check whether the ChatHaruhi/RoleLLM RPA, the character.ai RPA, and the PDb annotation refer to the same character. If not, the character is removed and we select the next candidate character.</li>
</ol>
<p>The selected characters and their sources are: Hermione Granger, Harry Potter, Ron Weasley, Luna Lovegood, Draco Malfoy, Albus Dumbledore, Minerva McGonagall, Severus Snape (Harry Potter Series), Zhong Li, Hu Tao, Raiden Shougun, Ayaka Kamisato, Wanderer (Genshin Impact), Thor, Lucifer Morningstar, Rorschach (DC Comics) Sheldon Cooper, Raj Koothrappali (The Big Bang Theory), Gaston (Beauty and the Beast), Klaus Mikaelson (The Vampirie Diaries), Jigsaw (Saw Series), James Bond (James Bond Film Series), Twilight Sparkle (My Little Pony: Friendship Is Magic), John Keating (Dead Poets Society), Michael Scott (The Office), Shrek (Shrek), Jeffrey Lebowski (The Dude), Walk Kowalski (Gran Torino), Lestat de</p>
<p>Lioncourt (Interview with the Vampire), Blair Waldorf (Gossip Girl), Haruhi Suzumiya (The Melancholy of Haruhi Suzumiya), and Jim Morrison (Celebrities).</p>
<p>We illustrate these characters' personality scores on the BFI, together with the measured personalities of their RPAs in Figure 6. Besides, we demonstrate some examples of system prompts of the RPAs in Table 5.</p>
<table>
<thead>
<tr>
<th>System Prompts for RPAs</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Zhong Li (ChatHaruhi)</td>
<td>Please be aware that your codename in this conversation is 'Zhongli'. Others call you 'Zhongli', 'Guest', 'Emperor', or 'Rex Lapis'. The preceding text provided some classic scenes from the game. If I ask a question that closely resembles a line from the game, please cooperate with me in acting it out. If I ask a question related to events in the game, please respond based on the game's content. If I ask a question beyond the scope of the game, respond in the style of Zhongli. You have the appearance of an adult male, with a short haircut and a long, thin braid reaching down to your hips. Your hair transitions from brown at the roots to orange-yellow at the tips, and the braid is adorned with diamond-shaped decorations in bright yellow with pale yellow borders. Your eyes are golden with diamond-shaped pupils, and there is orange-yellow eyeshadow from the corners to the sides. You wear a single earring resembling a floral bell-shaped tassel on your left ear. Your attire is a blend of a suit and a robe, divided into three layers. ... (695 words)</td>
<td></td>
</tr>
<tr>
<td>Hermione Granger (ChatHaruhi)</td>
<td>I want you to act like Hermione from Harry Potter. You are now cosplay Hermione Granger If others' questions are related with the novel, please try to reuse the original lines from the novel. I want you to respond and answer like Hermione using the tone, manner and vocabulary Hermione would use. You must know all of the knowledge of Hermione. Hermione Granger is a smart, diligent, and confident young witch with a high pursuit of learning and knowledge. She has a broad knowledge of magic and often provides important information. Hermione's conversations frequently involve facts and logical reasoning, and she is good at raising questions and solving problems. (109 words)</td>
<td></td>
</tr>
<tr>
<td>Thor (RoleLLM)</td>
<td>I want you to act like Thor from Thor-Ragnarok If others' questions are related with the novel, please try to reuse the original lines from the novel. I want you to respond and answer like Thor using the tone, manner and vocabulary Thor would use. You must know all of the knowledge of Thor. You are a powerful and godlike being, the crown prince of Asgard who wields a mighty hammer. Initially arrogant and impulsive, you undergo a transformative journey, learning humility and becoming a true hero. Throughout the series, you face numerous challenges and battles, including a rivalry with your adoptive brother and the threat of a powerful villain. Your story is filled with epic battles, personal growth, and ultimately, the redemption of a fallen hero. (127 words)</td>
<td></td>
</tr>
<tr>
<td>James Bond (RoleLLM)</td>
<td>I want you to act like James Bond from Tomorrow-Never-Dies. If others' questions are related with the novel, please try to reuse the original lines from the novel. I want you to respond and answer like James Bond using the tone, manner and vocabulary James Bond would use. You must know all of the knowledge of James Bond. A suave and skilled British secret agent with a license to kill, you are known for your impeccable style, charm, and wit. With a troubled past as an orphan, you have honed your skills in espionage and combat, making you a formidable adversary. Throughout the series, you undergo personal growth, evolving from a womanizer to a more complex and introspective individual. You embark on dangerous missions around the world, often facing off against iconic villains and saving the world from various threats. Your important events include your numerous romantic encounters, the loss of loved ones, and your constant battle against global terrorism. (160 words)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: Examples of system prompts for RPAs in ChatHaruhi and RoleLLM. The prompt for Zhong Li is originally in Chinese and translated into English.</p>
<h2>D Human Annotations</h2>
<p>We collect groundtruth labels for character personalities annotated by people familiar with the characters, from both the PDb and our invited annotators.</p>
<p>The Personality Database PDb collects and offers categorical personality annotations of massive fictional characters on the BFI (e.g., "RCUAI"11) and 16P (e.g., "ENTJ"). Each character $c$ is labeled by plentiful human annotators familiar with $c$, and PDb offers detailed numbers of annotations of each label. For the selected characters, we calculate the label-percentage of the positive type on each dimension ${ }^{12}$ as the scores. Then, we categorize each score into the positive, negative or marginal type if it is above $60 \%$, under $40 \%$ or otherwise. Marginal types indicate ambiguity and would be ignored for alignment calculation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Human Annotation Example Prompt</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">InST</td>
<td style="text-align: left;">Please rate <character> on the "Openness" dimension of</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BFI personality.</td>
</tr>
<tr>
<td style="text-align: left;">Scoring</td>
<td style="text-align: left;">Each option is a number from 1 to 5.1 represents "very un-</td>
</tr>
<tr>
<td style="text-align: left;">Scale</td>
<td style="text-align: left;">curious", 2 represents "uncurious", 3 represents "neutral",</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">4 represents "curious", and 5 represents "very curious."</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">If you think this dimension is irrelevant to the character,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">or it is difficult to judge the character on this dimension,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">please answer "X."</td>
</tr>
<tr>
<td style="text-align: left;">EXAMPLAR 1. Is original, comes up with new ideas. (High score)</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ITEMS</td>
<td style="text-align: left;">2. Is curious about many different things. (High score)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">3. Is ingenious, a deep thinker. (High Score)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">4. Prefers work that is routine.. (Low score)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">5. Has few artistic interests.. (Low score)</td>
</tr>
</tbody>
</table>
<p>Human Annotations After that, we invite human annotators familiar with these characters to score them on a broader range of psychological scales. Each annotation contains 73 dimensions on 14 scales for a character. We provide detailed annotation prompts of each dimension, including the scoring instruction and examplar positively and negatively related items.</p>
<p>We first examine their understanding of the characters by matching their type annotations on the BFI and 16P with the PDb labels. If one or two differences exist, we ask them to explain their answers. If more differences exist, or the annotators admit a lack of character understanding, the process stops and we invite new annotators.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| BFI | 16P | BSRI | Empt. | EPQ-R | LMS | DTDD |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| 75.9 | 77.0 | 71.4 | 64.5 | 43.8 | 65.6 | 51.7 |
| ECR-R | GSE | ICB | LOT-R | EIS | WLEIS | CABIN |
| 47.3 | 64.8 | 62.2 | 71.9 | 50.9 | 58.8 | 46.3 |</p>
<p>Table 6: The kappa coefficient (\%) on 14 scales. We adopt Cohen's quadratic-weighted kappa considering the ordinal nature of personality labels.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>PDb Labels</th>
<th></th>
<th></th>
<th></th>
<th>Annotator Labels</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BFI</td>
<td>16P</td>
<td>BFI</td>
<td>16P</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Acc</td>
<td>MSE</td>
<td>Acc</td>
<td>MSE</td>
<td>Acc</td>
<td>MSE</td>
<td>Acc</td>
<td>MSE</td>
</tr>
<tr>
<td>SR</td>
<td>63.3</td>
<td>15.8</td>
<td>65.6</td>
<td>15.2</td>
<td>60.1</td>
<td>7.6</td>
<td>59.2</td>
<td>9.1</td>
</tr>
<tr>
<td>SR-CoT</td>
<td>65.0</td>
<td>16.9</td>
<td>69.7</td>
<td>14.7</td>
<td>60.7</td>
<td>7.9</td>
<td>65.0</td>
<td>8.6</td>
</tr>
<tr>
<td>OC</td>
<td>64.3</td>
<td>16.1</td>
<td>75.5</td>
<td>12.7</td>
<td>63.1</td>
<td>7.2</td>
<td>69.4</td>
<td>7.2</td>
</tr>
<tr>
<td>d-OC</td>
<td>72.2</td>
<td>13.1</td>
<td>80.2</td>
<td>10.1</td>
<td>65.6</td>
<td>6.4</td>
<td>72.5</td>
<td>6.7</td>
</tr>
<tr>
<td>$\mathrm{ER}_{\text {n11 }}$</td>
<td>76.7</td>
<td>12.4</td>
<td>79.6</td>
<td>9.3</td>
<td>67.9</td>
<td>6.5</td>
<td>72.7</td>
<td>6.8</td>
</tr>
<tr>
<td>$\mathrm{ER}_{\text {Battb }}$</td>
<td>76.7</td>
<td>12.2</td>
<td>80.7</td>
<td>9.7</td>
<td>67.5</td>
<td>5.9</td>
<td>73.6</td>
<td>6.4</td>
</tr>
</tbody>
</table>
<p>Table 7: Measured alignment (\%) on the BFI and 16P with labels from the PDb or invited annotators, with GPT-4 as the interviewer LLM. Acc denotes dimensional accuracy.</p>
<p>Then, we ask them to score the characters on the 73 dimensions according to corresponding scoring ranges and instructions. Annotators are allowed to mark a character on a dimension as " X " which indicates unrelatedness or ambiguity. We collected two to three annotations for each character (in total, 93 annotations for 32 characters) and took their average scores as the final score. Finally, we linearly re-scale the scores into the unit interval $[0,1]$, and similarly categorize the scores into types. If more than one annotators mark a dimension as " X " for a character, this dimension will also be treated as a marginal type.</p>
<p>We measure the inter-annotator consistency via Cohen's kappa coefficient (Cohen, 1968) on each of the scales. Specifically, we calculate quadraticweighted kappa (Cohen, 1968), since the classes are ordinal. The average kappa coefficient across 14 scales is $60.9 \%$. The detailed kappa coefficients on 14 scales are listed in Table 6. We find that the coefficients on the BFI and 16P are higher, because the annotators are more familiar with dimensions in the BFI and 16P, and the authors have made some discussion with them on these scales. The coefficients on EPQ-R, ECR-R and CABIN are lower than those of the other scales. For ECR-R and CABIN, this is probably because the characters' personalities on close relationships (ECR-R)</p>
<p>and various career interests (CABIN) are rarely depicted in the original works. For EPQ-R, this is in part influenced by ambiguity of the Lying dimension, where high scores actually stand for Honesty.</p>
<p>On the BFI and 16P, we experimented with both labels from the PDb and our invited annotators. Experimental results in Table 7 show that, across various settings, the PDb labels always yield higher accuracy while labels from invited annotators yield better MSE. The reasons are that, the PDb labels are contributed by massive online annotators, and are hence more accurate. However, they offer only the personality types and corresponding label numbers instead of detailed personality scores, which cannot be well represented by the label-percentage. For example, even if all the annotators mark a character $c$ as Extravorted, we cannot assume $c$ as completely Extroverted.</p>
<h2>E Implementation Details</h2>
<h3>E.1 RPA Inference and Post-processing</h3>
<p>Our implementation of RPAs is based on ChatHaruhi-Suzumiya and LangChain ${ }^{13}$. Hence, we invoke foundation LLMs in RPAs with the default temperature 0.7 of LangChain. When experimenting with open-source LLMs as foundation models, we observe that LLMs may generate unexpected multi-round conversations or repeated content. In such cases, we remove the extra rounds or repetition.</p>
<p>In self-report tests, RPAs may refuse to participate in the tests and provide their choices, interestingly, because they are role-playing characters with noncompliant personalities. In these cases, the responses are categorized as "Neutral" for completeness of the results.</p>
<p>For interviewer LLMs, we request responses in JSON formats. If a response cannot be parsed into JSON data or the results of any samples in the batch are missing, we prompt interviewer LLMs to regenerate. The temperature is set as 0 for the initial generation, and 0.2 for regeneration. For GPT-3.5, if samples in one input exceed the token limit, we split the input into smaller batches. For Gemini, there are several cases where responses repeatedly fail to be parsed into JSON format or are blocked, In such cases, we resort to GPT-4 instead to ensure the completeness of the assessment process.</p>
<h3>E.2 Fine-Tuning</h3>
<p>We fine-tune the Mistral-2 7B model on the ChatHaruhi-English-62K dataset ${ }^{14}$, sourced from both ChatHaruhi (Li et al., 2023) and RoleLLM (Wang et al., 2023b). Our implementation is based on LLaMA-Factory ${ }^{15}$. We adopt LoRA tuning, and configure the training with a batch size of 16 , a learning rate of $5 e-5$, across three epochs, using the "fp16" option.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>F Additional Results</p>
<h3>F.1 Interviewer LLMs</h3>
<p>For the OC and ER tasks, we report the detailed numbers of right, close and wrong cases of interviewer LLMs in Table 8. For ER, GPT-4 rates 82\% of cases highly consistent with humans, and only $4 \%$ of cases notably different compared with humans. Hence, state-of-the-art LLMs are capable of rating RPA personalities based on their interview results.</p>
<h3>F.2 Self-report v.s. Interview-based Methods</h3>
<p>The advantages of INCHARACTER over SR originate from the fact that INCHARACTER elicits the thoughts and behaviors of RPAs for personality assessments. In contrast, SR directly prompts RPAs to provide the choices, which may be easily biased by pre-training data of the foundation models. This difference enables INCHARACTER to yield more distinct personality measurements than self-report, as is evidenced in Figure 3. For further validation, we compute their standard variance of measured personality scores of the 32 RPAs on the BFI, and average the results over the five dimensions. We obtain a result of 1.03 for INCHARACTER with $\mathrm{ER}_{\text {batch }}$ and GPT-4, 0.71 for SR and 0.68 for SRCoT.</p>
<p>While SR-CoT also attempts to elicit thoughts of the RPAs, its improvement over SR is limited. Furthermore, SR-CoT has two disadvantages compared with INCHARACTER. First, SR-CoT indeed requires RPAs themselves to perform the OC task implicitly, while INCHARACTER can apply other methodologies such as d-OC and ER. As is shown in Table 1, existing LLMs generally perform better on ER and d-OC, rather than OC. Second, the implicit OC task in SR-CoT would be a challenge for RPAs based on small foundation models, while in INCHARACTER, we can decouple the interviewer LLMs and foundation models for RPAs.</p>
<h3>F.3 RPAs from Different Works</h3>
<p>Many recent efforts have been committed to developing RPAs for specific characters in various ways. We compare RPAs contributed by ChatHaruhi (Li et al., 2023), RoleLLM (Wang et al., 2023b) and character.ai ${ }^{16}$. They craft character data in different methods. ChatHaruhi and RoleLLM share few characters in common, and their character data are</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 8: Detailed numbers of right, close, and wrong cases in human evaluation of interviewer LLMs on the option conversion task and the expert rating tasks. #Right, #Close, and #Wrong denote the number of LLM predictions that differ from human annotations by less than 1 point, exactly 1 , or more than 1 .
thus less comparable. character.ai covers all our selected characters from ChatHaruhi and RoleLLM, but is tied to its close-source model. Hence, we distinguish three groups of characters, including the 16 ChatHaruhi characters (CH, the 16 RoleLLM characters (RL) and their union (CH+RL). We implement CH and RL RPAs with GPT-3.5 as the foundation model using character data from ChatHaruhi and RoleLLM. We conduct personality tests using INCHARACTER, with $\mathrm{ER}_{\text {batch }}$ and GPT-3.5 as the interviewer LLM.</p>
<p>The results are shown in Table 9. According to the results, we observe that GPT-3.5-based RPAs with character data from both ChatHaruhi and RoleLLM achieve high personality fidelity, demonstrating the quality of their character data. However, character.ai RPAs exhibit low personality fidelity regarding the characters, even if we compare it with GPT-3.5-based RPAs using only character descriptions from ChatHaruhi and RoleLLM.</p>
<h3>F.4 Foundation Models for RPAs</h3>
<p>Results on Different Languages As LLMs have different capabilities in different languages, we distinguish characters with data in English or Chinese, and report the results in $\S$ F.4. The results demonstrate that, the personality fidelity of RPAs largely depends on LLMs' capacity on the language. While LLaMa-2-Chat 13B shows competitive performance on English-based characters, its performance on Chinese-based characters is unsatisfying.</p>
<p>| RPA Type | CH | | | | | | | RL | | | | | | | CH+RL | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ Our implementation is based on https://github.com/ kramcat/CharacterAI.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{8}$ The versions in this paper are gpt-3.5-turbo-1106, gpt-4-1106-preview, gemini-pro, Qwen1.5-110B-Chat respectively.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
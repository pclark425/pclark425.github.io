<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1370 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1370</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1370</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-d35b05f440b5ba00d9429139edef7182bf9f7ce7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d35b05f440b5ba00d9429139edef7182bf9f7ce7" target="_blank">Learning to Navigate in Complex Environments</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work considers jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks and shows that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs.</p>
                <p><strong>Paper Abstract:</strong> Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1370.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1370.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>I-Maze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>I-Maze (T-maze / I-maze layout used in DeepMind Lab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed-layout maze inspired by rodent T-mazes: a central corridor with four arms each ending in an alcove; the goal is hidden in an alcove and remains fixed within an episode but varies across episodes, requiring memory to return efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>I-Maze</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>First-person 3D maze (DeepMind Lab) inspired by T-maze; central corridor with four arms and alcoves (one contains the goal); visually rich with sparse cues.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Grid-like 2D maze topology embedded in 3D rendering: corridors and arms produce sparse local connectivity (not fully connected, path-constrained).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>77 discretized locations (as used for the position decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nav A3C + D2 (stacked LSTM A3C with depth prediction from the policy LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Asynchronous Advantage Actor-Critic with a convnet encoder, two-layer stacked LSTM (policy/value from top LSTM), extra inputs (agent-relative velocity, previous action, previous reward), and auxiliary depth-prediction target from the top LSTM (D2).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>AUC (area under learning curve), latency to first goal and subsequent goals, number of goals reached per episode, position decoding accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>AUC 203.5 (Nav A3C + D2, mean over top-5 hyperparameter runs); Latency to first/subsequent goal reported as '8.8 c : 2.5 s' (table formatting preserved); Goals: 100/100 (episodes with â‰¥1 goal).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>100/100 episodes had at least one goal (100%) for Nav A3C + D2</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based policy (stacked LSTM with auxiliary tasks); a policy that uses memory of goal location to take the direct return route is optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Symmetry and alcove dead-ends shape policy: the I-maze's symmetric arms allow equivalent action sequences to opposite arms, enabling a compressed policy representation (Nav A3C forms sub-policies for diagonally opposite arms). Position decoding accuracy correlates with higher task reward; auxiliary tasks (depth prediction from LSTM, loop-closure) improve localization and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Compared to other mazes, the I-maze required memory of the hidden alcove location; Nav A3C + D2 achieved highest AUC and high goal rates, showing auxiliary depth prediction from the policy LSTM particularly benefits environments where memory of goal location and disambiguation of symmetric branches matter.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policy representations compress symmetric goals into fewer sub-policies (t-SNE clusters show two main clusters in Nav A3C corresponding to diagonally opposite arms), indicating efficient policy structure that exploits action equivalences; LSTM cell activations reflect goal-conditioning and memory-driven routing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Navigate in Complex Environments', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1370.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1370.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static Maze (Small, Static 1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static Maze 1 (small 5x10 fixed-goal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visually rich small 3D maze (5x10 grid) with a fixed goal location and fixed fruit locations, start positions randomized; rewards encourage reaching the goal repeatedly within long episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static 1 (small 5x10 maze)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>First-person 3D maze (DeepMind Lab) sized 5x10 grid; fixed goal and fruit positions across episodes; start position/orientation randomized each respawn.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>2D grid-like maze with corridors and walls producing constrained sparse connectivity between neighboring cells.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>5 x 10 grid = 50 discretized locations</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nav A3C + D2 (stacked LSTM with depth prediction from LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A3C agent with conv encoder and stacked LSTM; inputs include velocity, previous action and reward; auxiliary objective: depth classification from top LSTM activations.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>AUC, time-to-goal (latency), goals-per-episode, position-decoding accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>AUC 104.3 (Nav A3C + D2); Score 119 mean (top-5); Position decoding accuracy 95.4%; Latency reported as '5.9 c : 5.4 s' (first : subsequent).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>100/100 episodes had one or more goals (100%) for Nav A3C + D2</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Reactive policies can perform well in static small mazes (feedforward A3C is competitive), but best performance achieved by memory-augmented policy with auxiliary depth prediction for faster/data-efficient learning.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>On small, fixed-goal mazes, reactive/encoder-memorized strategies can succeed because observations suffice to infer location; nevertheless, auxiliary tasks (depth from LSTM) greatly speed learning and improve asymptotic performance, indicating that topological constraints (corridors, limited ambiguity) allow reactive strategies but benefit from geometry-aware representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Auxiliary tasks have the strongest accelerating effect on static mazes; depth prediction from policy LSTM yields largest gains relative to baseline, suggesting geometry-focused auxiliary losses bootstrap representation learning in topologies with stable goal placements.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Feedforward agents can memorize position in weights for small static mazes; stacked LSTM + auxiliary tasks learn more robust localized representations faster (position entropy decreases after respawn), enabling quicker exploitation of known goal location.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Navigate in Complex Environments', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1370.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1370.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static Maze (Large, Static 2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static Maze 2 (large 9x15 fixed-goal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger fixed-goal 3D maze (9x15 grid) with visually rich cues; episodes longer and the environment more challenging, but goal/fruit positions remain fixed across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static 2 (large 9x15 maze)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>First-person 3D maze (DeepMind Lab) sized 9x15; fixed goal and fruit positions; random start position and orientation; longer episodes to allow repeated goal acquisitions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Large 2D grid maze with corridors/walls; sparse local connectivity and longer path lengths between distant locations compared to small maze.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>9 x 15 grid = 135 discretized locations</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nav A3C + D2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stacked LSTM A3C with convnet encoder and auxiliary depth-classification loss formulated on top-LSTM activations, plus agent-relative velocity, prev action and reward inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>AUC, latency to first/subsequent goal, goals per episode, position-decoding accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>AUC 157.6 (Nav A3C + D2); Score 200 mean (top-5); Position decoding accuracy 94.0%; Latency reported as '10.9 c : 11.0 s'.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>100/100 episodes had at least one goal (100%) for Nav A3C + D2</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-augmented policy with geometry-aware auxiliary tasks; planning-like behavior emerges implicitly via learned representations rather than explicit graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Larger environment size increases difficulty (longer latencies, more data needed); auxiliary depth prediction yields large performance gains and better position decoding, indicating representation of geometry helps exploration and exploitation in higher-diameter environments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Auxiliary tasks (depth from LSTM) deliver larger absolute gains on larger maps compared to baseline; pure reactive policies fare worse as map size increases, reinforcing that higher effective diameter/topological complexity favors memory-based policies.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Increased topology complexity (size) requires richer internal state (LSTM) and explicit signals (depth/loop closure) to reduce position uncertainty and lower latency to subsequent goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Navigate in Complex Environments', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1370.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1370.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Goal Maze 1 (small)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Goal Maze 1 (small 5x10, dynamic goals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small 5x10 maze variant where goal and fruits are randomly placed each episode; within an episode the goal is fixed but agents must explore then exploit repeated goal returns after respawn.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Random Goal 1 (small 5x10 maze)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>First-person 3D maze (DeepMind Lab) small map; goal and fruit positions randomized each episode, encouraging an explore-then-exploit strategy requiring short-term memory of discovered goal locations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Grid-like maze topology with corridors; topology similar to small static maze but goal placement varies, increasing effective exploration difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>5 x 10 grid = 50 discretized locations</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nav A3C + D2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stacked LSTM A3C with conv encoder and auxiliary top-LSTM depth classification, plus velocity, prev action and reward inputs to second LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>AUC, latency to first and subsequent goals, position-decoding accuracy, goals-per-episode</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>AUC 71.1 (Nav A3C + D2); Score 96 mean (top-5); Position decoding accuracy 85.5%; Latency reported as '14.0 c : 7.1 s' (first : subsequent).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>100/100 episodes had at least one goal (100%) for Nav A3C + D2</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based (must remember discovered goal); policies that combine exploration for discovery and then exploit stored goal location are optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Because the topology is small, exploration is feasible; auxiliary tasks (depth prediction from LSTM) substantially improve exploration efficiency and reduce latency for subsequent finds, demonstrating that geometry and loop-closure signals speed up learning of explore-then-exploit behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Random (dynamic) goals increase need for memory and exploration; relative gains from auxiliary tasks are substantial compared to baseline recurrent and feedforward agents, but overall AUC is lower than for fixed-goal static mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Agents learn an explore-then-exploit policy: initial exploration to find goal, then direct returns after respawn; stacked LSTM policies with auxiliary inputs and D2 support fast localization and low-latency returns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Navigate in Complex Environments', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1370.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1370.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Goal Maze 2 (large)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Goal Maze 2 (large 9x15, dynamic goals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large 9x15 maze where goals and fruits are randomized each episode; longer episodes and larger map increase exploration difficulty and memory demands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Random Goal 2 (large 9x15 maze)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>First-person 3D maze (DeepMind Lab) large map; goal and fruit placements randomized per episode; requires exploration and memory for repeated goal returns in a larger topology.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Large grid-like maze with corridors and walls; longer path lengths and more ambiguous observations increase topological complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>9 x 15 grid = 135 discretized locations</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nav A3C + D2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stacked LSTM A3C with convnet encoder and auxiliary depth-classification from top LSTM; includes velocity, previous action and reward as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>AUC, latency to first/subsequent goal, goals per episode, position decoding accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>AUC 82.1 (Nav A3C + D2); Score 103 mean (top-5); Position decoding accuracy 72.4%; Latency '15.4 c : 15.0 s' (first : subsequent); Goals 79/100 episodes had at least one goal for Nav A3C + D2.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>79/100 episodes had at least one goal (79%) for Nav A3C + D2 (evaluation single best agent)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based with strong geometry representation; larger diameter/topological complexity increases the need for persistent memory and better representations (auxiliary tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Larger topological size reduces the relative improvement from auxiliary tasks on subsequent-latency (for Random Goal 2 none of the agents achieved lower latency after initial acquisition), indicating that high diameter/complexity limits how well short-term memory can support fast returns without more capacity or external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Performance drops (lower success rate and position accuracy) as topology size and complexity increase; Nav A3C + D2 still outperforms baselines but absolute performance is constrained by map size and ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies require longer-term memory for larger maps; position decoding accuracy is lower compared to smaller maps, suggesting increased uncertainty and need for richer memory architectures (external memory suggested for future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Navigate in Complex Environments', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1370.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1370.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MazeBase gridworld (reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MazeBase gridworld (referenced work: Kulkarni et al., 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2D gridworld environment used in prior work for testing successor representations and bottleneck detection; referenced as prior work demonstrating analysis of bottlenecks and learned representations in discrete mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep successor reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MazeBase gridworld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D grid-based simulated environment (gridworld) used for tasks including navigation, bottleneck detection and skill learning; domain: abstract/grid puzzles rather than first-person 3D.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Discrete grid topology; prior work used it to study successor representations and bottlenecks (graph-theoretic structures).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep successor representation agents (from Kulkarni et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Feedforward architecture learning successor features to enable behavioral flexibility to reward changes and to detect structural bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Mentioned as prior work showing detection of bottlenecks in VizDoom and use of successor features to capture topological structure; cited but not analysed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Referenced as demonstrating that learned representations can reveal graph bottlenecks and support flexible behavior under reward changes (not analysed in current paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Navigate in Complex Environments', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1370.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1370.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-based games (reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced paper that applies deep RL to language-based/text-adventure games, focusing on language understanding in text-only interactive environments (EMNLP 2015).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games (general, referenced study)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-adventure style environments where observations and actions are text strings; domain: language-grounded navigation and interaction rather than first-person visual mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Typically discrete state graphs induced by text states and actions; paper references this line of work but does not analyse graph metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reinforcement learning agents for text-based games (Narasimhan et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL agents augmented for language understanding to act in text-only game environments; cited as related work but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Mentioned in related work; this paper does not analyse text-world graph topology or its impact on policy structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Referenced as an example of navigation in text-based worlds that requires language understanding; no direct findings in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Navigate in Complex Environments', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep successor reinforcement learning <em>(Rating: 2)</em></li>
                <li>Playing FPS games with deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Reinforcement learning with unsupervised auxiliary tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1370",
    "paper_id": "paper-d35b05f440b5ba00d9429139edef7182bf9f7ce7",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "I-Maze",
            "name_full": "I-Maze (T-maze / I-maze layout used in DeepMind Lab)",
            "brief_description": "A fixed-layout maze inspired by rodent T-mazes: a central corridor with four arms each ending in an alcove; the goal is hidden in an alcove and remains fixed within an episode but varies across episodes, requiring memory to return efficiently.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "I-Maze",
            "environment_description": "First-person 3D maze (DeepMind Lab) inspired by T-maze; central corridor with four arms and alcoves (one contains the goal); visually rich with sparse cues.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Grid-like 2D maze topology embedded in 3D rendering: corridors and arms produce sparse local connectivity (not fully connected, path-constrained).",
            "environment_size": "77 discretized locations (as used for the position decoder)",
            "agent_name": "Nav A3C + D2 (stacked LSTM A3C with depth prediction from the policy LSTM)",
            "agent_description": "Asynchronous Advantage Actor-Critic with a convnet encoder, two-layer stacked LSTM (policy/value from top LSTM), extra inputs (agent-relative velocity, previous action, previous reward), and auxiliary depth-prediction target from the top LSTM (D2).",
            "exploration_efficiency_metric": "AUC (area under learning curve), latency to first goal and subsequent goals, number of goals reached per episode, position decoding accuracy",
            "exploration_efficiency_value": "AUC 203.5 (Nav A3C + D2, mean over top-5 hyperparameter runs); Latency to first/subsequent goal reported as '8.8 c : 2.5 s' (table formatting preserved); Goals: 100/100 (episodes with â‰¥1 goal).",
            "success_rate": "100/100 episodes had at least one goal (100%) for Nav A3C + D2",
            "optimal_policy_type": "Memory-based policy (stacked LSTM with auxiliary tasks); a policy that uses memory of goal location to take the direct return route is optimal.",
            "topology_performance_relationship": "Symmetry and alcove dead-ends shape policy: the I-maze's symmetric arms allow equivalent action sequences to opposite arms, enabling a compressed policy representation (Nav A3C forms sub-policies for diagonally opposite arms). Position decoding accuracy correlates with higher task reward; auxiliary tasks (depth prediction from LSTM, loop-closure) improve localization and performance.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Compared to other mazes, the I-maze required memory of the hidden alcove location; Nav A3C + D2 achieved highest AUC and high goal rates, showing auxiliary depth prediction from the policy LSTM particularly benefits environments where memory of goal location and disambiguation of symmetric branches matter.",
            "policy_structure_findings": "Policy representations compress symmetric goals into fewer sub-policies (t-SNE clusters show two main clusters in Nav A3C corresponding to diagonally opposite arms), indicating efficient policy structure that exploits action equivalences; LSTM cell activations reflect goal-conditioning and memory-driven routing.",
            "uuid": "e1370.0",
            "source_info": {
                "paper_title": "Learning to Navigate in Complex Environments",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "Static Maze (Small, Static 1)",
            "name_full": "Static Maze 1 (small 5x10 fixed-goal)",
            "brief_description": "A visually rich small 3D maze (5x10 grid) with a fixed goal location and fixed fruit locations, start positions randomized; rewards encourage reaching the goal repeatedly within long episodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Static 1 (small 5x10 maze)",
            "environment_description": "First-person 3D maze (DeepMind Lab) sized 5x10 grid; fixed goal and fruit positions across episodes; start position/orientation randomized each respawn.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "2D grid-like maze with corridors and walls producing constrained sparse connectivity between neighboring cells.",
            "environment_size": "5 x 10 grid = 50 discretized locations",
            "agent_name": "Nav A3C + D2 (stacked LSTM with depth prediction from LSTM)",
            "agent_description": "A3C agent with conv encoder and stacked LSTM; inputs include velocity, previous action and reward; auxiliary objective: depth classification from top LSTM activations.",
            "exploration_efficiency_metric": "AUC, time-to-goal (latency), goals-per-episode, position-decoding accuracy",
            "exploration_efficiency_value": "AUC 104.3 (Nav A3C + D2); Score 119 mean (top-5); Position decoding accuracy 95.4%; Latency reported as '5.9 c : 5.4 s' (first : subsequent).",
            "success_rate": "100/100 episodes had one or more goals (100%) for Nav A3C + D2",
            "optimal_policy_type": "Reactive policies can perform well in static small mazes (feedforward A3C is competitive), but best performance achieved by memory-augmented policy with auxiliary depth prediction for faster/data-efficient learning.",
            "topology_performance_relationship": "On small, fixed-goal mazes, reactive/encoder-memorized strategies can succeed because observations suffice to infer location; nevertheless, auxiliary tasks (depth from LSTM) greatly speed learning and improve asymptotic performance, indicating that topological constraints (corridors, limited ambiguity) allow reactive strategies but benefit from geometry-aware representations.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Auxiliary tasks have the strongest accelerating effect on static mazes; depth prediction from policy LSTM yields largest gains relative to baseline, suggesting geometry-focused auxiliary losses bootstrap representation learning in topologies with stable goal placements.",
            "policy_structure_findings": "Feedforward agents can memorize position in weights for small static mazes; stacked LSTM + auxiliary tasks learn more robust localized representations faster (position entropy decreases after respawn), enabling quicker exploitation of known goal location.",
            "uuid": "e1370.1",
            "source_info": {
                "paper_title": "Learning to Navigate in Complex Environments",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "Static Maze (Large, Static 2)",
            "name_full": "Static Maze 2 (large 9x15 fixed-goal)",
            "brief_description": "A larger fixed-goal 3D maze (9x15 grid) with visually rich cues; episodes longer and the environment more challenging, but goal/fruit positions remain fixed across episodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Static 2 (large 9x15 maze)",
            "environment_description": "First-person 3D maze (DeepMind Lab) sized 9x15; fixed goal and fruit positions; random start position and orientation; longer episodes to allow repeated goal acquisitions.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Large 2D grid maze with corridors/walls; sparse local connectivity and longer path lengths between distant locations compared to small maze.",
            "environment_size": "9 x 15 grid = 135 discretized locations",
            "agent_name": "Nav A3C + D2",
            "agent_description": "Stacked LSTM A3C with convnet encoder and auxiliary depth-classification loss formulated on top-LSTM activations, plus agent-relative velocity, prev action and reward inputs.",
            "exploration_efficiency_metric": "AUC, latency to first/subsequent goal, goals per episode, position-decoding accuracy",
            "exploration_efficiency_value": "AUC 157.6 (Nav A3C + D2); Score 200 mean (top-5); Position decoding accuracy 94.0%; Latency reported as '10.9 c : 11.0 s'.",
            "success_rate": "100/100 episodes had at least one goal (100%) for Nav A3C + D2",
            "optimal_policy_type": "Memory-augmented policy with geometry-aware auxiliary tasks; planning-like behavior emerges implicitly via learned representations rather than explicit graph search.",
            "topology_performance_relationship": "Larger environment size increases difficulty (longer latencies, more data needed); auxiliary depth prediction yields large performance gains and better position decoding, indicating representation of geometry helps exploration and exploitation in higher-diameter environments.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Auxiliary tasks (depth from LSTM) deliver larger absolute gains on larger maps compared to baseline; pure reactive policies fare worse as map size increases, reinforcing that higher effective diameter/topological complexity favors memory-based policies.",
            "policy_structure_findings": "Increased topology complexity (size) requires richer internal state (LSTM) and explicit signals (depth/loop closure) to reduce position uncertainty and lower latency to subsequent goals.",
            "uuid": "e1370.2",
            "source_info": {
                "paper_title": "Learning to Navigate in Complex Environments",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "Random Goal Maze 1 (small)",
            "name_full": "Random Goal Maze 1 (small 5x10, dynamic goals)",
            "brief_description": "Small 5x10 maze variant where goal and fruits are randomly placed each episode; within an episode the goal is fixed but agents must explore then exploit repeated goal returns after respawn.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Random Goal 1 (small 5x10 maze)",
            "environment_description": "First-person 3D maze (DeepMind Lab) small map; goal and fruit positions randomized each episode, encouraging an explore-then-exploit strategy requiring short-term memory of discovered goal locations.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Grid-like maze topology with corridors; topology similar to small static maze but goal placement varies, increasing effective exploration difficulty.",
            "environment_size": "5 x 10 grid = 50 discretized locations",
            "agent_name": "Nav A3C + D2",
            "agent_description": "Stacked LSTM A3C with conv encoder and auxiliary top-LSTM depth classification, plus velocity, prev action and reward inputs to second LSTM.",
            "exploration_efficiency_metric": "AUC, latency to first and subsequent goals, position-decoding accuracy, goals-per-episode",
            "exploration_efficiency_value": "AUC 71.1 (Nav A3C + D2); Score 96 mean (top-5); Position decoding accuracy 85.5%; Latency reported as '14.0 c : 7.1 s' (first : subsequent).",
            "success_rate": "100/100 episodes had at least one goal (100%) for Nav A3C + D2",
            "optimal_policy_type": "Memory-based (must remember discovered goal); policies that combine exploration for discovery and then exploit stored goal location are optimal.",
            "topology_performance_relationship": "Because the topology is small, exploration is feasible; auxiliary tasks (depth prediction from LSTM) substantially improve exploration efficiency and reduce latency for subsequent finds, demonstrating that geometry and loop-closure signals speed up learning of explore-then-exploit behavior.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Random (dynamic) goals increase need for memory and exploration; relative gains from auxiliary tasks are substantial compared to baseline recurrent and feedforward agents, but overall AUC is lower than for fixed-goal static mazes.",
            "policy_structure_findings": "Agents learn an explore-then-exploit policy: initial exploration to find goal, then direct returns after respawn; stacked LSTM policies with auxiliary inputs and D2 support fast localization and low-latency returns.",
            "uuid": "e1370.3",
            "source_info": {
                "paper_title": "Learning to Navigate in Complex Environments",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "Random Goal Maze 2 (large)",
            "name_full": "Random Goal Maze 2 (large 9x15, dynamic goals)",
            "brief_description": "Large 9x15 maze where goals and fruits are randomized each episode; longer episodes and larger map increase exploration difficulty and memory demands.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Random Goal 2 (large 9x15 maze)",
            "environment_description": "First-person 3D maze (DeepMind Lab) large map; goal and fruit placements randomized per episode; requires exploration and memory for repeated goal returns in a larger topology.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Large grid-like maze with corridors and walls; longer path lengths and more ambiguous observations increase topological complexity.",
            "environment_size": "9 x 15 grid = 135 discretized locations",
            "agent_name": "Nav A3C + D2",
            "agent_description": "Stacked LSTM A3C with convnet encoder and auxiliary depth-classification from top LSTM; includes velocity, previous action and reward as inputs.",
            "exploration_efficiency_metric": "AUC, latency to first/subsequent goal, goals per episode, position decoding accuracy",
            "exploration_efficiency_value": "AUC 82.1 (Nav A3C + D2); Score 103 mean (top-5); Position decoding accuracy 72.4%; Latency '15.4 c : 15.0 s' (first : subsequent); Goals 79/100 episodes had at least one goal for Nav A3C + D2.",
            "success_rate": "79/100 episodes had at least one goal (79%) for Nav A3C + D2 (evaluation single best agent)",
            "optimal_policy_type": "Memory-based with strong geometry representation; larger diameter/topological complexity increases the need for persistent memory and better representations (auxiliary tasks).",
            "topology_performance_relationship": "Larger topological size reduces the relative improvement from auxiliary tasks on subsequent-latency (for Random Goal 2 none of the agents achieved lower latency after initial acquisition), indicating that high diameter/complexity limits how well short-term memory can support fast returns without more capacity or external memory.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Performance drops (lower success rate and position accuracy) as topology size and complexity increase; Nav A3C + D2 still outperforms baselines but absolute performance is constrained by map size and ambiguity.",
            "policy_structure_findings": "Policies require longer-term memory for larger maps; position decoding accuracy is lower compared to smaller maps, suggesting increased uncertainty and need for richer memory architectures (external memory suggested for future work).",
            "uuid": "e1370.4",
            "source_info": {
                "paper_title": "Learning to Navigate in Complex Environments",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "MazeBase gridworld (reference)",
            "name_full": "MazeBase gridworld (referenced work: Kulkarni et al., 2016)",
            "brief_description": "A 2D gridworld environment used in prior work for testing successor representations and bottleneck detection; referenced as prior work demonstrating analysis of bottlenecks and learned representations in discrete mazes.",
            "citation_title": "Deep successor reinforcement learning",
            "mention_or_use": "mention",
            "environment_name": "MazeBase gridworld",
            "environment_description": "A 2D grid-based simulated environment (gridworld) used for tasks including navigation, bottleneck detection and skill learning; domain: abstract/grid puzzles rather than first-person 3D.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Discrete grid topology; prior work used it to study successor representations and bottlenecks (graph-theoretic structures).",
            "environment_size": null,
            "agent_name": "Deep successor representation agents (from Kulkarni et al.)",
            "agent_description": "Feedforward architecture learning successor features to enable behavioral flexibility to reward changes and to detect structural bottlenecks.",
            "exploration_efficiency_metric": null,
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Mentioned as prior work showing detection of bottlenecks in VizDoom and use of successor features to capture topological structure; cited but not analysed in this paper.",
            "comparison_across_topologies": null,
            "topology_comparison_results": null,
            "policy_structure_findings": "Referenced as demonstrating that learned representations can reveal graph bottlenecks and support flexible behavior under reward changes (not analysed in current paper).",
            "uuid": "e1370.5",
            "source_info": {
                "paper_title": "Learning to Navigate in Complex Environments",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "Text-based games (reference)",
            "name_full": "Language understanding for text-based games using deep reinforcement learning",
            "brief_description": "A referenced paper that applies deep RL to language-based/text-adventure games, focusing on language understanding in text-only interactive environments (EMNLP 2015).",
            "citation_title": "Language understanding for text-based games using deep reinforcement learning",
            "mention_or_use": "mention",
            "environment_name": "Text-based games (general, referenced study)",
            "environment_description": "Text-adventure style environments where observations and actions are text strings; domain: language-grounded navigation and interaction rather than first-person visual mazes.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Typically discrete state graphs induced by text states and actions; paper references this line of work but does not analyse graph metrics.",
            "environment_size": null,
            "agent_name": "Reinforcement learning agents for text-based games (Narasimhan et al., 2015)",
            "agent_description": "Deep RL agents augmented for language understanding to act in text-only game environments; cited as related work but not used in experiments.",
            "exploration_efficiency_metric": null,
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Mentioned in related work; this paper does not analyse text-world graph topology or its impact on policy structure.",
            "comparison_across_topologies": null,
            "topology_comparison_results": null,
            "policy_structure_findings": "Referenced as an example of navigation in text-based worlds that requires language understanding; no direct findings in the current paper.",
            "uuid": "e1370.6",
            "source_info": {
                "paper_title": "Learning to Navigate in Complex Environments",
                "publication_date_yy_mm": "2016-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Deep successor reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Playing FPS games with deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Reinforcement learning with unsupervised auxiliary tasks",
            "rating": 2
        }
    ],
    "cost": 0.0193125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS</h1>
<p>Piotr Mirowski<em>, Razvan Pascanu</em>, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell<br>DeepMind<br>London, UK<br>{piotrmirowski, razp, fviola, soyer, aybd, abanino, mdenil, goroshin, sifre, korayk, dkumaran, raia} @google.com</p>
<h4>Abstract</h4>
<p>Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour ${ }^{1}$, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.</p>
<h2>1 INTRODUCTION</h2>
<p>The ability to navigate efficiently within an environment is fundamental to intelligent behavior. Whilst conventional robotics methods, such as Simultaneous Localisation and Mapping (SLAM), tackle navigation through an explicit focus on position inference and mapping (Dissanayake et al., 2001), here we follow recent work in deep reinforcement learning (Mnih et al., 2015; 2016) and propose that navigational abilities could emerge as the by-product of an agent learning a policy that maximizes reward. One advantage of an intrinsic, end-to-end approach is that actions are not divorced from representation, but rather learnt together, thus ensuring that task-relevant features are present in the representation. Learning to navigate from reinforcement learning in partially observable environments, however, poses several challenges.</p>
<p>First, rewards are often sparsely distributed in the environment, where there may be only one goal location. Second, environments often comprise dynamic elements, requiring the agent to use memory at different timescales: rapid one-shot memory for the goal location, together with short term memory subserving temporal integration of velocity signals and visual observations, and longer term memory for constant aspects of the environment (e.g. boundaries, cues).</p>
<p>To improve statistical efficiency we bootstrap the reinforcement learning procedure by augmenting our loss with auxiliary tasks that provide denser training signals that support navigation-relevant representation learning. We consider two additional losses: the first one involves reconstruction of a low-dimensional depth map at each time step by predicting one input modality (the depth channel) from others (the colour channels). This auxiliary task concerns the 3D geometry of the environment, and is aimed to encourage the learning of representations that aid obstacle avoidance and short-term trajectory planning. The second task directly invokes loop closure from SLAM: the agent is trained to predict if the current location has been previously visited within a local trajectory.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Views from a small $5 \times 10$ maze, a large $9 \times 15$ maze and an I-maze, with corresponding maze layouts and sample agent trajectories. The mazes, which will be made public, have different textures and visual cues as well as exploration rewards and goals (shown right).</p>
<p>To address the memory requirements of the task we rely on a stacked LSTM architecture (Graves et al., 2013; Pascanu et al., 2013). We evaluate our approach using five 3D maze environments and demonstrate the accelerated learning and increased performance of the proposed agent architecture. These environments feature complex geometry, random start position and orientation, dynamic goal locations, and long episodes that require thousands of agent steps (see Figure 1). We also provide detailed analysis of the trained agent to show that critical navigation skills are acquired. This is important as neither position inference nor mapping are directly part of the loss; therefore, raw performance on the goal finding task is not necessarily a good indication that these skills are acquired. In particular, we show that the proposed agent resolves ambiguous observations and quickly localizes itself in a complex maze, and that this localization capability is correlated with higher task reward.</p>
<h1>2 APPROACH</h1>
<p>We rely on a end-to-end learning framework that incorporates multiple objectives. Firstly it tries to maximize cumulative reward using an actor-critic approach. Secondly it minimizes an auxiliary loss of inferring the depth map from the RGB observation. Finally, the agent is trained to detect loop closures as an additional auxiliary task that encourages implicit velocity integration.
The reinforcement learning problem is addressed with the Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) that relies on learning both a policy $\pi\left(a_{t} \mid s_{t} ; \theta\right)$ and value function $V\left(s_{t} ; \theta_{V}\right)$ given a state observation $s_{t}$. Both the policy and value function share all intermediate representations, both being computed using a separate linear layer from the topmost layer of the model. The agent setup closely follows the work of (Mnih et al., 2016) and we refer to this work for the details (e.g. the use of a convolutional encoder followed by either an MLP or an LSTM, the use of action repetition, entropy regularization to prevent the policy saturation, etc.). These details can also be found in the Appendix B.</p>
<p>The baseline that we consider in this work is an A3C agent (Mnih et al., 2016) that receives only RGB input from the environment, using either a recurrent or a purely feed-forward model (see Figure 2a,b). The encoder for the RGB input (used in all other considered architectures) is a 3 layer convolutional network. To support the navigation capability of our approach, we also rely on the Nav A3C agent (Figure 2c) which employs a two-layer stacked LSTM after the convolutional encoder. We expand the observations of the agents to include agent-relative velocity, the action sampled from the stochastic policy and the immediate reward, from the previous time step. We opt to feed the velocity and previously selected action directly to the second recurrent layer, with the first layer only receiving the reward. We postulate that the first layer might be able to make associations between reward and visual observations that are provided as context to the second layer from which the policy is computed. Thus, the observation $s_{t}$ may include an image $\mathbf{x}_{t} \in \mathbb{R}^{3 \times W \times H}$ (where $W$ and $H$ are the width and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Different architectures: (a) is a convolutional encoder followed by a feedforward layer and policy ( $\pi$ ) and value function outputs; (b) has an LSTM layer; (c) uses additional inputs (agent-relative velocity, reward, and action), as well as a stacked LSTM; and (d) has additional outputs to predict depth and loop closures.
height of the image), the agent-relative lateral and rotational velocity $\mathbf{v}<em t-1="t-1">{t} \in \mathbb{R}^{6}$, the previous action $\mathbf{a}</em>$.
Figure 2d shows the augmentation of the Nav A3C with the different possible auxiliary losses. In particular we consider predicting depth from the convolutional layer (we will refer to this choice as $D_{1}$ ), or from the top LSTM layer ( $D_{2}$ ) or predicting loop closure ( $L$ ). The auxiliary losses are computed on the current frame via a single layer MLP. The agent is trained by applying a weighted sum of the gradients coming from A3C, the gradients from depth prediction (multiplied with $\beta_{d_{1}}, \beta_{d_{2}}$ ) and the gradients from the loop closure (scaled by $\beta_{l}$ ). More details of the online learning algorithm are given in Appendix B.} \in \mathbb{R}^{N_{A}}$, and the previous reward $r_{t-1} \in \mathbb{R</p>
<h1>2.1 DEPTH PREDICTION</h1>
<p>The primary input to the agent is in the form of RGB images. However, depth information, covering the central field of view of the agent, might supply valuable information about the 3D structure of the environment. While depth could be directly used as an input, we argue that if presented as an additional loss it is actually more valuable to the learning process. In particular if the prediction loss shares representation with the policy, it could help build useful features for RL much faster, bootstrapping learning. Since we know from (Eigen et al., 2014) that a single frame can be enough to predict depth, we know this auxiliary task can be learnt. A comparison between having depth as input versus as an additional loss is given in Appendix C, which shows significant gain for depth as a loss.</p>
<p>Since the role of the auxiliary loss is just to build up the representation of the model, we do not necessarily care about the specific performance obtained or nature of the prediction. We do care about the data efficiency aspect of the problem and also computational complexity. If the loss is to be useful for the main task, we should converge faster on it compared to solving the RL problem (using less data samples), and the additional computational cost should be minimal. To achieve this we use a low resolution variant of the depth map, reducing the screen resolution to 4 x 16 pixels $^{2}$.
We explore two different variants for the loss. The first choice is to phrase it as a regression task, the most natural choice. While this formulation, combined with a higher depth resolution, extracts the most information, mean square error imposes a unimodal distribution (van den Oord et al., 2016). To address this possible issue, we also consider a classification loss, where depth at each position is discretised into 8 different bands. The bands are non-uniformally distributed such that we pay more attention to far-away objects (details in Appendix B). The motivation for the classification formulation is that while it greatly reduces the resolution of depth, it is more flexible from a learning perspective and can result in faster convergence (hence faster bootstrapping).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.2 LOOP CLOSURE PREDICTION</h1>
<p>Loop closure, like depth, is valuable for a navigating agent, since can be used for efficient exploration and spatial reasoning. To produce the training targets, we detect loop closures based on the similarity of local position information during an episode, which is obtained by integrating 2D velocity over time. Specifically, in a trajectory noted $\left{p_{0}, p_{1}, \ldots, p_{T}\right}$, where $p_{t}$ is the position of the agent at time $t$, we define a loop closure label $l_{t}$ that is equal to 1 if the position $p_{t}$ of the agent is close to the position $p_{t^{\prime}}$ at an earlier time $t^{\prime}$. In order to avoid trivial loop closures on consecutive points of the trajectory, we add an extra condition on an intermediary position $p_{t^{\prime \prime}}$ being far from $p_{t}$. Thresholds $\eta_{1}$ and $\eta_{2}$ provide these two limits. Learning to predict the binary loop label is done by minimizing the Bernoulli loss $\mathcal{L}<em t="t">{l}$ between $l</em>$ of the last hidden layer of the model, followed by a sigmoid activation.}$ and the output of a single-layer output from the hidden representation $h_{t</p>
<h2>3 RELATED WORK</h2>
<p>There is a rich literature on navigation, primarily in the robotics literature. However, here we focus on related work in deep RL. Deep Q-networks (DQN) have had breakthroughs in extremely challenging domains such as Atari (Mnih et al., 2015). Recent work has developed on-policy RL methods such as advantage actor-critic that use asynchronous training of multiple agents in parallel (Mnih et al., 2016). Recurrent networks have also been successfully incorporated to enable state disambiguation in partially observable environments (Koutnik et al., 2013; Hausknecht \&amp; Stone, 2015; Mnih et al., 2016; Narasimhan et al., 2015).</p>
<p>Deep RL has recently been used in the navigation domain. Kulkarni et al. (2016) used a feedforward architecture to learn deep successor representations that enabled behavioral flexibility to reward changes in the MazeBase gridworld, and provided a means to detect bottlenecks in 3D VizDoom. Zhu et al. (2016) used a feedforward siamese actor-critic architecture incorporating a pretrained ResNet to support navigation to a target in a discretised 3D environment. Oh et al. (2016) investigated the performance of a variety of networks with external memory (Weston et al., 2014) on simple navigation tasks in the Minecraft 3D block world environment. Tessler et al. (2016) also used the Minecraft domain to show the benefit of combining feedforward deep-Q networks with the learning of resuable skill modules (cf options: (Sutton et al., 1999)) to transfer between navigation tasks. Tai \&amp; Liu (2016) trained a convnet DQN-based agent using depth channel inputs for obstacle avoidance in 3D environments. Barron et al. (2016) investigated how well a convnet can predict the depth channel from RGB in the Minecraft environment, but did not use depth for training the agent.</p>
<p>Auxiliary tasks have often been used to facilitate representation learning (Suddarth \&amp; Kergosien, 1990). Recently, the incorporation of additional objectives, designed to augment representation learning through auxiliary reconstructive decoding pathways (Zhang et al., 2016; Rasmus et al., 2015; Zhao et al., 2015; Mirowski et al., 2010), has yielded benefits in large scale classification tasks. In deep RL settings, however, only two previous papers have examined the benefit of auxiliary tasks. Specifically, Li et al. (2016) consider a supervised loss for fitting a recurrent model on the hidden representations to predict the next observed state, in the context of imitation learning of sequences provided by experts, and Lample \&amp; Chaplot (2016) show that the performance of a DQN agent in a first-person shooter game in the VizDoom environment can be substantially enhanced by the addition of a supervised auxiliary task, whereby the convolutional network was trained on an enemy-detection task, with information about the presence of enemies, weapons, etc., provided by the game engine.</p>
<p>In contrast, our contribution addresses fundamental questions of how to learn an intrinsic representation of space, geometry, and movement while simultaneously maximising rewards through reinforcement learning. Our method is validated in challenging maze domains with random start and goal locations.</p>
<h2>4 EXPERIMENTS</h2>
<p>We consider a set of first-person 3D mazes from the DeepMind Lab environment (Beattie et al., 2016) (see Fig. 1) that are visually rich, with additional observations available to the agent such as inertial</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Rewards achieved by the agents on 5 different tasks: two static mazes (small and large) with fixed goals, two static mazes with comparable layout but with dynamic goals and the I-maze. Results are averaged over the top 5 random hyperparameters for each agent-task configuration. Star in the label indicates the use of reward clipping. Please see text for more details.
information and local depth information. ${ }^{3}$ The action space is discrete, yet allows finegrained control, comprising 8 actions: the agent can rotate in small increments, accelerate forward or backward or sideways, or induce rotational acceleration while moving. Reward is achieved in these environments by reaching a goal from a random start location and orientation. If the goal is reached, the agent is respawned to a new start location and must return to the goal. The episode terminates when a fixed amount of time expires, affording the agent enough time to find the goal several times. There are sparse 'fruit' rewards which serve to encourage exploration. Apples are worth 1 point, strawberries 2 points and goals are 10 points. Videos of the agent solving the maze are linked in Appendix A.</p>
<p>In the static variant of the maze, the goal and fruit locations are fixed and only the agent's start location changes. In the dynamic (Random Goal) variant, the goal and fruits are randomly placed on every episode. Within an episode, the goal and apple locations stay fixed until the episode ends. This encourages an explore-exploit strategy, where the agent should initially explore the maze, then retain the goal location and quickly refind it after each respawn. For both variants (static and random goal) we consider a small and large map. The small mazes are $5 \times 10$ and episodes last for 3600 timesteps, and the large mazes are $9 \times 15$ with 10800 steps (see Figure 1). The RGB observation is $84 \times 84$.</p>
<p>The I-Maze environment (see Figure 1, right) is inspired by the classic T-maze used to investigate navigation in rodents (Olton et al., 1979): the layout remains fixed throughout, the agent spawns in the central corridor where there are apple rewards and has to locate the goal which is placed in the alcove of one of the four arms. Because the goal is hidden in the alcove, the optimal agent behaviour must rely on memory of the goal location in order to return to the goal using the most direct route. Goal location is constant within an episode but varies randomly across episodes.</p>
<p>The different agent architectures described in Section 2 are evaluated by training on the five mazes. Figure 3 shows learning curves (averaged over the 5 top performing agents). The agents are a feedforward model (FF A3C), a recurrent model (LSTM A3C), the stacked LSTM version with velocity, previous action and reward as input (Nav A3C), and Nav A3C with depth prediction from the convolution layer (Nav A3C+ $D_{1}$ ), Nav A3C with depth prediction from the last LSTM layer (Nav A3C+ $D_{2}$ ), Nav A3C with loop closure prediction (Nav A3C+L) as well as the Nav A3C with</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: left: Example of depth predictions (pairs of ground truth and predicted depths), sampled every 40 steps. right: Example of loop closure prediction. The agent starts at the gray square and the trajectory is plotted in gray. Blue dots correspond to true positive outputs of the loop closure detector; red cross correspond to false positives and green cross to false negatives. Note the false positives that occur when the agent is actually a few squares away from actual loop closure.
all auxiliary losses considered together (Nav A3C+ $D_{1} D_{2} L$ ). In each case we ran 64 experiments with randomly sampled hyper-parameters (for ranges and details please see the appendix). The mean over the top 5 runs as well as the top 5 curves are plotted. Expert human scores, established by a professional game player, are compared to these results. The Nav A3C+ $D_{2}$ agents reach human-level performance on Static 1 and 2, and attain about $91 \%$ and $59 \%$ of human scores on Random Goal 1 and 2 .</p>
<p>In Mnih et al. (2015) reward clipping is used to stabilize learning, technique which we employed in this work as well. Unfortunately, for these particular tasks, this yields slightly suboptimal policies because the agent does not distinguish apples ( 1 point) from goals ( 10 points). Removing the reward clipping results in unstable behaviour for the base A3C agent (see Appendix C). However it seems that the auxiliary signal from depth prediction mediates this problem to some extent, resulting in stable learning dynamics (e.g. Figure 3f, Nav A3C $+D_{1}$ vs Nav A3C* $+D_{1}$ ). We clearly indicate whether reward clipping is used by adding an asterisk to the agent name.</p>
<p>Figure 3 f also explores the difference between the two formulations of depth prediction, as a regression task or a classification task. We can see that the regression agent (Nav A3C<em> $+D_{1}[\mathrm{MSE}]$ ) performs worse than one that does classification (Nav A3C</em> $+D_{1}$ ). This result extends to other maps, and we therefore only use the classification formulation in all our other results ${ }^{4}$. Also we see that predicting depth from the last LSTM layer (hence providing structure to the recurrent layer, not just the convolutional ones) performs better.</p>
<p>We note some particular results from these learning curves. In Figure 3 (a and b), consider the feedforward A3C model (red curve) versus the LSTM version (pink curve). Even though navigation seems to intrinsically require memory, as single observations could often be ambiguous, the feedforward model achieves competitive performance on static mazes. This suggest that there might be good strategies that do not involve temporal memory and give good results, namely a reactive policy held by the weights of the encoder, or learning a wall-following strategy. This motivates the dynamic environments that encourage the use of memory and more general navigation strategies.</p>
<p>Figure 3 also shows the advantage of adding velocity, reward and action as an input, as well as the impact of using a two layer LSTM (orange curve vs red and pink). Though this agent (Nav A3C) is better than the simple architectures, it is still relatively slow to train on all of the mazes. We believe that this is mainly due to the slower, data inefficient learning that is generally seen in pure RL approaches. Supporting this we see that adding the auxiliary prediction targets of depth and loop closure (Nav A3C $+D_{1} D_{2} L$, black curve) speeds up learning dramatically on most of the mazes (see Table 1: AUC metric). It has the strongest effect on the static mazes because of the accelerated learning, but also gives a substantial and lasting performance increase on the random goal mazes.</p>
<p>Although we place more value on the task performance than on the auxiliary losses, we report the results from the loop closure prediction task. Over 100 test episodes of 2250 steps each, within a large maze (random goal 2), the Nav A3C* $+D_{1} L$ agent demonstrated very successful loop detection, reaching an F-1 score of 0.83 . A sample trajectory can be seen in Figure 4 (right).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Mean over top 5 agents</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Highest reward agent</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Maze</td>
<td style="text-align: left;">Agent</td>
<td style="text-align: center;">AUC</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">\% Human</td>
<td style="text-align: center;">Goals</td>
<td style="text-align: center;">Position Acc</td>
<td style="text-align: center;">Latency 1:&gt;1</td>
<td style="text-align: center;">Score</td>
</tr>
<tr>
<td style="text-align: left;">I-Maze</td>
<td style="text-align: left;">FF A3C*</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94/100</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">$9.3 \mathrm{c}: 9.0 \mathrm{~s}$</td>
<td style="text-align: center;">102</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LSTM A3C*</td>
<td style="text-align: center;">112.4</td>
<td style="text-align: center;">244</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1 0 0 / 1 0 0}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 8}$</td>
<td style="text-align: center;">$15.3 \mathrm{c}: 3.2 \mathrm{~s}$</td>
<td style="text-align: center;">203</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C* $+D_{1} L$</td>
<td style="text-align: center;">169.7</td>
<td style="text-align: center;">266</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1 0 0 / 1 0 0}$</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">$10.7 \mathrm{c}: 2.7 \mathrm{~s}$</td>
<td style="text-align: center;">252</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{2}$</td>
<td style="text-align: center;">$\mathbf{2 0 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{2 6 8}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1 0 0 / 1 0 0}$</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">$\mathbf{8 . 8 \mathrm{c}: 2 . 5 \mathrm{~s}}$</td>
<td style="text-align: center;">$\mathbf{2 6 9}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{1} D_{2} L$</td>
<td style="text-align: center;">199.9</td>
<td style="text-align: center;">258</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1 0 0 / 1 0 0}$</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">$\mathbf{9 . 9 \mathrm{c}: 2 . 5 \mathrm{~s}}$</td>
<td style="text-align: center;">251</td>
</tr>
<tr>
<td style="text-align: left;">Static 1</td>
<td style="text-align: left;">FF A3C*</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">$8.8 \mathrm{c}: 8.7 \mathrm{~s}$</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LSTM A3C*</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">$6.1 \mathrm{c}: 5.9 \mathrm{~s}$</td>
<td style="text-align: center;">110</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{2}$</td>
<td style="text-align: center;">$\mathbf{1 0 4 . 3}$</td>
<td style="text-align: center;">$\mathbf{1 1 9}$</td>
<td style="text-align: center;">$\mathbf{1 2 5}$</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">$\mathbf{9 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{5 . 9 \mathrm{c}: 5 . 4 \mathrm{~s}}$</td>
<td style="text-align: center;">122</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{1} D_{2} L$</td>
<td style="text-align: center;">102.3</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">$\mathbf{5 . 9 \mathrm{c}: 5 . 4 \mathrm{~s}}$</td>
<td style="text-align: center;">$\mathbf{1 2 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Static 2</td>
<td style="text-align: left;">FF A3C*</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">$24.2 \mathrm{c}: 22.9 \mathrm{~s}$</td>
<td style="text-align: center;">111</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LSTM A3C*</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">153</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">$15.5 \mathrm{c}: 14.9 \mathrm{~s}$</td>
<td style="text-align: center;">155</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{2}$</td>
<td style="text-align: center;">$\mathbf{1 5 7 . 6}$</td>
<td style="text-align: center;">$\mathbf{2 0 0}$</td>
<td style="text-align: center;">$\mathbf{1 1 6}$</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">$\mathbf{9 4 . 0}$</td>
<td style="text-align: center;">$10.9 \mathrm{c}: 11.0 \mathrm{~s}$</td>
<td style="text-align: center;">$\mathbf{2 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{1} D_{2} L$</td>
<td style="text-align: center;">156.1</td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">$11.1 \mathrm{c}: 12.0 \mathrm{~s}$</td>
<td style="text-align: center;">192</td>
</tr>
<tr>
<td style="text-align: left;">Random Goal 1</td>
<td style="text-align: left;">FF A3C*</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">88/100</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">$11.0: 9.9 \mathrm{~s}$</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LSTM A3C*</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">85/100</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">$11.1 \mathrm{c}: 9.2 \mathrm{~s}$</td>
<td style="text-align: center;">66</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{2}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 1}$</td>
<td style="text-align: center;">$\mathbf{9 6}$</td>
<td style="text-align: center;">$\mathbf{9 1}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 / 1 0 0}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 0 \mathrm{c}: 7 . 1 \mathrm{~s}}$</td>
<td style="text-align: center;">$\mathbf{9 1}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{1} D_{2} L$</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">81/100</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">$11.5 \mathrm{c}: 7.2 \mathrm{~s}$</td>
<td style="text-align: center;">74.6</td>
</tr>
<tr>
<td style="text-align: left;">Random Goal 2</td>
<td style="text-align: left;">FF A3C*</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">$\mathbf{9 3 / 1 0 0}$</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">$27.3 \mathrm{c}: 28.2 \mathrm{~s}$</td>
<td style="text-align: center;">77</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LSTM A3C*</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">74/100</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$21.5 \mathrm{c}: 29.7 \mathrm{~s}$</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C* $+D_{1} L$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">90/100</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">$17.9 \mathrm{c}: 18.4 \mathrm{~s}$</td>
<td style="text-align: center;">106</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{2}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 1}$</td>
<td style="text-align: center;">$\mathbf{1 0 3}$</td>
<td style="text-align: center;">$\mathbf{5 9}$</td>
<td style="text-align: center;">79/100</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">$\mathbf{1 5 . 4 \mathrm{c}: 1 5 . 0 \mathrm{~s}}$</td>
<td style="text-align: center;">$\mathbf{1 0 9}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Nav A3C $+D_{1} D_{2} L$</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">74/100</td>
<td style="text-align: center;">$\mathbf{8 1 . 5}$</td>
<td style="text-align: center;">$15.9 \mathrm{c}: 16.0 \mathrm{~s}$</td>
<td style="text-align: center;">102</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of four agent architectures over five maze configurations, including random and static goals. $A U C$ (Area under learning curve), Score, and \% Human are averaged over the best 5 hyperparameters. Evaluation of a single best performing agent is done through analysis on 100 test episodes. Goals gives the number of episodes where the goal was reached one more more times. Position Accuracy is the classification accuracy of the position decoder. Latency $1:&gt;1$ is the average time to the first goal acquisition vs. the average time to all subsequent goal acquisitions. Score is the mean score over the 100 test episodes.</p>
<h1>5 ANALYSIS</h1>
<h3>5.1 POSITION DECODING</h3>
<p>In order to evaluate the internal representation of location within the agent (either in the hidden units $h_{t}$ of the last LSTM, or, in the case of the FF A3C agent, in the features $f_{t}$ on the last layer of the conv-net), we train a position decoder that takes that representation as input, consisting of a linear classifier with multinomial probability distribution over the discretized maze locations. Small mazes $(5 \times 10)$ have 50 locations, large mazes $(9 \times 15)$ have 135 locations, and the I-maze has 77 locations. Note that we do not backpropagate the gradients from the position decoder through the rest of the network. The position decoder can only see the representation exposed by the model, not change it.</p>
<p>An example of position decoding by the Nav A3C $+D_{2}$ agent is shown in Figure 6, where the initial uncertainty in position is improved to near perfect position prediction as more observations are acquired by the agent. We observe that position entropy spikes after a respawn, then decreases once the agent acquires certainty about its location. Additionally, videos of the agent's position decoding are linked in Appendix A. In these complex mazes, where localization is important for the purpose of reaching the goal, it seems that position accuracy and final score are correlated, as shown in Table 1. A pure feed-forward architecture still achieves $64.3 \%$ accuracy in a static maze with static goal, suggesting that the encoder memorizes the position in the weights and that this small maze is solvable by all the agents, with sufficient training time. In Random Goal 1, it is Nav A3C $+D_{2}$ that achieves the best position decoding performance ( $85.5 \%$ accuracy), whereas the FF A3C and the LSTM A3C architectures are at approximately $50 \%$.</p>
<p>In the I-maze, the opposite branches of the maze are nearly identical, with the exception of very sparse visual cues. We observe that once the goal is first found, the Nav A3C* $+D_{1} L$ agent is capable of directly returning to the correct branch in order to achieve the maximal score. However, the linear position decoder for this agent is only $68.5 \%$ accurate, whereas it is $87.8 \%$ in the plain LSTM A3C agent. We hypothesize that the symmetry of the I-maze will induce a symmetric policy that need not be sensitive to the exact position of the agent (see analysis below).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Trajectories of the Nav A3C* $+D_{1} L$ agent in the I-maze (left) and of the Nav A3C $+D_{2}$ random goal maze 1 (right) over the course of one episode. At the beginning of the episode (gray curve on the map), the agent explores the environment until it finds the goal at some unknown location (red box). During subsequent respawns (blue path), the agent consistently returns to the goal. The value function, plotted for each episode, rises as the agent approaches the goal. Goals are plotted as vertical red lines.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Trajectory of the Nav A3C $+D_{2}$ agent in the random goal maze 1, overlaid with the position probability predictions predicted by a decoder trained on LSTM hidden activations, taken at 4 steps during an episode. Initial uncertainty gives way to accurate position prediction as the agent navigates.</p>
<p>A desired property of navigation agents in our Random Goal tasks is to be able to first find the goal, and reliably return to the goal via an efficient route after subsequent re-spawns. The latency column in Table 1 shows that the Nav A3C $+D_{2}$ agents achieve the lowest latency to goal once the goal has been discovered (the first number shows the time in seconds to find the goal the first time, and the second number is the average time for subsequent finds). Figure 5 shows clearly how the agent finds the goal, and directly returns to that goal for the rest of the episode. For Random Goal 2, none of the agents achieve lower latency after initial goal acquisition; this is presumably due to the larger, more challenging environment.</p>
<h1>5.2 STACKED LSTM GOAL ANALYSIS</h1>
<p>Figure 7(a) shows shows the trajectories traversed by an agent for each of the four goal locations. After an initial exploratory phase to find the goal, the agent consistently returns to the goal location. We visualize the agent's policy by applying tSNE dimension reduction (Maaten \&amp; Hinton, 2008) to the cell activations at each step of the agent for each of the four goal locations. Whilst clusters corresponding to each of the four goal locations are clearly distinct in the LSTM A3C agent, there are 2 main clusters in the Nav A3C agent - with trajectories to diagonally opposite arms of the maze represented similarly. Given that the action sequence to opposite arms is equivalent (e.g. straight, turn left twice for top left and bottom right goal locations), this suggests that the Nav A3C policy-dictating LSTM maintains an efficient representation of 2 sub-policies (i.e. rather than 4 independent policies) - with critical information about the currently relevant goal provided by the additional LSTM.</p>
<h3>5.3 INVESTIGATING DIFFERENT COMBINATIONS OF AUXILIARY TASKS</h3>
<p>Our results suggest that depth prediction from the policy LSTM yields optimal results. However, several other auxiliary tasks have been concurrently introduced in (Jaderberg et al., 2017), and thus we provide a comparison of reward prediction against depth prediction. Following that paper, we implemented two additional agent architectures, one performing reward prediction from the convnet using a replay buffer, called Nav A3C<em> $+R$, and one combining reward prediction from the convnet and depth prediction from the LSTM (Nav A3C $+R D_{2}$ ). Table 2 suggests that reward prediction (Nav A3C</em> $+R$ ) improves upon the plain stacked LSTM architecture (Nav A3C*) but not as much as depth prediction from the policy LSTM (Nav A3C $+D_{2}$ ). Combining reward prediction and depth prediction (Nav A3C $+R D_{2}$ ) yields comparable results to depth prediction alone (Nav A3C $+D_{2}$ ); normalised average AUC values are respectively 0.995 vs. 0.981 . Future work will explore other auxiliary tasks.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: LSTM cell activations of LSTM A3C and Nav A3C* $+D_{1} L$ agents from the I-Maze collected over multiple episodes and reduced to 2 dimensions using tSNE, then coloured to represent the goal location. Policy-dictating LSTM of Nav A3C agent shown.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Navigation agent architecture</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Maze</td>
<td style="text-align: center;">Nav A3C*</td>
<td style="text-align: center;">Nav A3C $+D_{1}$</td>
<td style="text-align: center;">Nav A3C $+D_{2}$</td>
<td style="text-align: center;">Nav A3C $+D_{1} D_{2}$</td>
<td style="text-align: center;">Nav A3C* $+R$</td>
<td style="text-align: center;">Nav A3C $+R D_{2}$</td>
</tr>
<tr>
<td style="text-align: left;">I-Maze</td>
<td style="text-align: center;">143.3</td>
<td style="text-align: center;">196.7</td>
<td style="text-align: center;">$\mathbf{2 0 3 . 5}$</td>
<td style="text-align: center;">197.2</td>
<td style="text-align: center;">128.2</td>
<td style="text-align: center;">191.8</td>
</tr>
<tr>
<td style="text-align: left;">Static 1</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">103.2</td>
<td style="text-align: center;">104.3</td>
<td style="text-align: center;">100.3</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">$\mathbf{1 0 5 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Static 2</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">153.1</td>
<td style="text-align: center;">$\mathbf{1 5 7 . 6}$</td>
<td style="text-align: center;">151.6</td>
<td style="text-align: center;">100.6</td>
<td style="text-align: center;">155.5</td>
</tr>
<tr>
<td style="text-align: left;">Random Goal 1</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">$\mathbf{7 2 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Random Goal 2</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">$\mathbf{8 2 . 1}$</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">80.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of five navigation agent architectures over five maze configurations with random and static goals, including agents performing reward prediction Nav A3C* $+R$ and Nav A3C $+R D_{2}$, where reward prediction is implemented following (Jaderberg et al., 2017). We report the $A U C$ (Area under learning curve), averaged over the best 5 hyperparameters.</p>
<h1>6 CONCLUSION</h1>
<p>We proposed a deep RL method, augmented with memory and auxiliary learning targets, for training agents to navigate within large and visually rich environments that include frequently changing start and goal locations. Our results and analysis highlight the utility of un/self-supervised auxiliary objectives, namely depth prediction and loop closure, in providing richer training signals that bootstrap learning and enhance data efficiency. Further, we examine the behavior of trained agents, their ability to localise, and their network activity dynamics, in order to analyse their navigational abilities.</p>
<p>Our approach of augmenting deep RL with auxiliary objectives allows end-end learning and may encourage the development of more general navigation strategies. Notably, our work with auxiliary losses is related to (Jaderberg et al., 2017) which independently looks at data efficiency when exploiting auxiliary losses. One difference between the two works is that our auxiliary losses are online (for the current frame) and do not rely on any form of replay. Also the explored losses are very different in nature. Finally our focus is on the navigation domain and understanding if navigation emerges as a bi-product of solving an RL problem, while Jaderberg et al. (2017) is concerned with data efficiency for any RL-task.
Whilst our best performing agents are relatively successful at navigation, their abilities would be stretched if larger demands were placed on rapid memory (e.g. in procedurally generated mazes), due to the limited capacity of the stacked LSTM in this regard. It will be important in the future to combine visually complex environments with architectures that make use of external memory (Graves et al., 2016; Weston et al., 2014; Olton et al., 1979) to enhance the navigational abilities of agents. Further, whilst this work has focused on investigating the benefits of auxiliary tasks for developing the ability to navigate through end-to-end deep reinforcement learning, it would be interesting for future work to compare these techniques with SLAM-based approaches.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We would like to thank Alexander Pritzel, Thomas Degris and Joseph Modayil for useful discussions, Charles Beattie, Julian Schrittwieser, Marcus Wainwright, and Stig Petersen for environment design and development, and Amir Sadik and Sarah York for expert human game testing.</p>
<h1>REFERENCES</h1>
<p>Trevor Barron, Matthew Whitehead, and Alan Yeung. Deep reinforcement learning in a 3-d blockworld environment. In Deep Reinforcement Learning: Frontiers and Challenges, IJCAI, 2016.</p>
<p>Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich KÃ„ijttler, Andrew Lefrancq, Simon Green, Victor Valdes, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. In arXiv, 2016. URL https://arxiv.org/ abs/1612.03801.</p>
<p>MWM Gamini Dissanayake, Paul Newman, Steve Clark, Hugh F. Durrant-Whyte, and Michael Csorba. A solution to the simultaneous localization and map building (slam) problem. IEEE Transactions on Robotics and Automation, 17(3):229-241, 2001.</p>
<p>David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In Proc. of Neural Information Processing Systems, NIPS, 2014.</p>
<p>Alex Graves, Mohamed Abdelrahman, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2013.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwiÅ„ska, Sergio GÃ³mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 2016.</p>
<p>Matthew J. Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. Proc. of Conf. on Artificial Intelligence, AAAI, 2015.</p>
<p>Max Jaderberg, Volodymir Mnih, Wojciech Czarnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In Submitted to Int'l Conference on Learning Representations, ICLR, 2017.</p>
<p>Jan Koutnik, Giuseppe Cuccu, JÃ„ijrgen Schmidhuber, and Faustino Gomez. Evolving large-scale neural networks for vision-based reinforcement learning. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, GECCO, 2013.</p>
<p>Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J. Gershman. Deep successor reinforcement learning. CoRR, abs/1606.02396, 2016. URL http://arxiv.org/abs/1606. 02396 .</p>
<p>Guillaume Lample and Devendra Singh Chaplot. Playing FPS games with deep reinforcement learning. CoRR, 2016. URL http://arxiv.org/abs/1609.05521.</p>
<p>Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, and Ji He. Recurrent reinforcement learning: A hybrid approach. In Proceedings of the International Conference on Learning Representations, ICLR, 2016. URL https://arxiv.org/abs/1509.03044.</p>
<p>Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579-2605, 2008.</p>
<p>Piotr Mirowski, Marc'Aurelio Ranzato, and Yann LeCun. Dynamic auto-encoders for semantic indexing. In NIPS Deep Learning and Unsupervised Learning Workshop, 2010.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, et al. Human-level control through deep reinforcement learning. Nature, 518:529-533, 2015.</p>
<p>Volodymyr Mnih, AdriÃƒÃ£ PuigdomÃƒÃnech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proc. of Int'l Conf. on Machine Learning, ICML, 2016.</p>
<p>Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, et al. Massively parallel methods for deep reinforcement learning. In Proceedings of the International Conference on Machine Learning Deep Learning Workshop, ICML, 2015.</p>
<p>Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In Proc. of Empirical Methods in Natural Language Processing, EMNLP, 2015.</p>
<p>Junhyuk Oh, Valliappa Chockalingam, Satinder P. Singh, and Honglak Lee. Control of memory, active perception, and action in minecraft. In Proc. of International Conference on Machine Learning, ICML, 2016.</p>
<p>David S Olton, James T Becker, and Gail E Handelmann. Hippocampus, space, and memory. Behavioral and Brain Sciences, 2(03):313-322, 1979.</p>
<p>Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.</p>
<p>Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems, NIPS, 2015.</p>
<p>Steven C Suddarth and YL Kergosien. Rule-injection hints as a means of improving network performance and learning time. In Neural Networks, pp. 120-129. Springer, 1990.</p>
<p>Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1):181-211, 1999.</p>
<p>Lei Tai and Ming Liu. Towards cognitive exploration through deep reinforcement learning for mobile robots. In arXiv, 2016. URL https://arxiv.org/abs/1610.01733.</p>
<p>Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, and Shie Mannor. A deep hierarchical approach to lifelong learning in minecraft. CoRR, abs/1604.07255, 2016. URL http://arxiv.org/abs/1604.07255.</p>
<p>Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. In Coursera: Neural Networks for Machine Learning, volume 4, 2012.
A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. 2016.</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.</p>
<p>Yuting Zhang, Kibok Lee, and Honglak Lee. Augmenting supervised neural networks with unsupervised objectives for large-scale image classification. In Proc. of International Conference on Machine Learning, ICML, 2016.</p>
<p>Junbo Zhao, MichaÃ«l Mathieu, Ross Goroshin, and Yann LeCun. Stacked what-where auto-encoders. Int'l Conf. on Learning Representations (Workshop), ICLR, 2015. URL http://arxiv.org/ abs/1506.02351.</p>
<p>Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. CoRR, abs/1609.05143, 2016. URL http://arxiv.org/abs/1609.05143.</p>
<h1>Supplementary Material</h1>
<h2>A VIDEOS OF TRAINED NAVIGATION AGENTS</h2>
<p>We show the behaviour of Nav A3C* $+D_{1} L$ agent in 5 videos, corresponding to the 5 navigation environments: I-maze ${ }^{5}$, (small) static maze ${ }^{6}$, (large) static maze ${ }^{7}$, (small) random goal maze ${ }^{8}$ and (large) random goal maze ${ }^{9}$. Each video shows a high-resolution video (the actual inputs to the agent are down-sampled to $84 \times 84$ RGB images), the value function over time (with fruit reward and goal acquisitions), the layout of the mazes with consecutive trajectories of the agent marked in different colours and the output of the trained position decoder, overlayed on top of the maze layout.</p>
<h2>B Network Architecture and Training</h2>
<h2>B. 1 THE ONLINE MULTI-LEARNER ALGORITHM FOR MULTI-TASK LEARNING</h2>
<p>We introduce a class of neural network-based agents that have modular structures and that are trained on multiple tasks, with inputs coming from different modalities (vision, depth, past rewards and past actions). Implementing our agent architecture is simplified by its modular nature. Essentially, we construct multiple networks, one per task, using shared building blocks, and optimise these networks jointly. Some modules, such as the conv-net used for perceiving visual inputs, or the LSTMs used for learning the navigation policy, are shared among multiple tasks, while other modules, such as depth predictor $g_{d}$ or loop closure predictor $g_{l}$, are task-specific. The navigation network that outputs the policy and the value function is trained using reinforcement learning, while the depth prediction and loop closure prediction networks are trained using self-supervised learning.
Within each thread of the asynchronous training environment, the agent plays on its own episode of the game environment, and therefore sees observation and reward pairs $\left{\left(s_{t}, r_{t}\right)\right}$ and takes actions that are different from those experienced by agents from the other, parallel threads. Within a thread, the multiple tasks (navigation, depth and loop closure prediction) can be trained at their own schedule, and they add gradients to the shared parameter vector as they arrive. Within each thread, we use a flag-based system to subordinate gradient updates to the A3C reinforcement learning procedure.</p>
<h2>B. 2 NETWORK AND TRAINING DETAILS</h2>
<p>For all the experiments we use an encoder model with 2 convolutional layers followed by a fully connected layer, or recurrent layer(s), from which we predict the policy and value function. The architecture is similar to the one in (Mnih et al., 2016). The convolutional layers are as follows. The first convolutional layer has a kernel of size 8 x 8 and a stride of 4 x 4 , and 16 feature maps. The second layer has a kernel of size 4 x 4 and a stride of 2 x 2 , and 32 feature maps. The fully connected layer, in the FF A3C architecture in Figure 2a has 256 hidden units (and outputs visual features $f_{t}$ ). The LSTM in the LSTM A3C architecture has 256 hidden units (and outputs LSTM hidden activations $h_{t}$ ). The LSTMs in Figure 2c and 2d are fed extra inputs (past reward $r_{t-1}$, previous action $\mathbf{a}<em t="t">{t}$ expressed as a one-hot vector of dimension 8 and agent-relative lateral and rotational velocity $\mathbf{v}</em>$ are all single-layer MLPs with 128 hidden units. The depth MLPs are followed by 64 independent 8-dimensional softmax outputs (one per depth pixel). The loop closure MLP is followed by a 2-dimensional softmax output. We illustrate on Figure 8 the architecture of the Nav A3C+D+L+Dr agent.}$ encoded by a 6-dimensional vector), which are all concatenated to vector $f_{t}$. The Nav A3C architectures (Figure 2c,d) have a first LSTM with 64 or 128 hiddens and a second LSTM with 256 hiddens. The depth predictor modules $g_{d}, g_{d}^{\prime}$ and the loop closure detection module $g_{l</p>
<p>Depth is taken as the Z-buffer from the Labyrinth environment (with values between 0 and 255), divided by 255 and taken to power 10 to spread the values in interval $[0,1]$. We empirically decided to use the following quantization: ${0,0.05,0.175,0.3,0.425,0.55,0.675,0.8,1}$ to ensure a uniform</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Details of the architecture of the Nav A3C+D+L+Dr agent, taking in RGB visual inputs $\mathbf{x}<em t-1="t-1">{t}$, past reward $r</em>}$, previous action $\mathbf{a<em t="t">{t-1}$ as well as agent-relative velocity $\mathbf{v}</em>}$, and producing policy $\pi$, value function $V$, depth predictions $g_{d}\left(\mathbf{f<em d="d">{t}\right)$ and $g</em>}^{\prime}\left(\mathbf{h<em l="l">{t}\right)$ as well as loop closure detection $g</em>}\left(\mathbf{h<em d="d">{t}\right)$.
binning across 8 classes. The previous version of the agent had a single depth prediction MLP $g</em>$.
The parameters of each of the modules point to a subset of a common vector of parameters. We optimise these parameters using an asynchronous version of RMSProp (Tieleman \&amp; Hinton, 2012). (Nair et al., 2015) was a recent example of asynchronous and parallel gradient updates in deep reinforcement learning; in our case, we focus on the specific Asynchronous Advantage Actor Critic (A3C) reinforcement learning procedure in (Mnih et al., 2016).}$ for regressing $8 \times 16=128$ depth pixels from the convnet outputs $\mathbf{f}_{t</p>
<p>Learning follows closely the paradigm described in (Mnih et al., 2016). We use 16 workers and the same RMSProp algorithm without momentum or centering of the variance. Gradients are computed over non-overlaping chunks of the episode. The score for each point of a training curve is the average over all the episodes the model gets to finish in $5 e 4$ environment steps.</p>
<p>The whole experiments are run for a maximum of $1 e 8$ environment step. The agent has an action repeat of 4 as in (Mnih et al., 2016), which means that for 4 consecutive steps the agent will use the same action picked at the beginning of the series. For this reason through out the paper we actually report results in terms of agent perceived steps rather than environment steps. That is, the maximal number of agent perceived step that we do for any particular run is $2.5 e 7$.</p>
<p>In our grid we sample hyper-parameters from categorical distributions:</p>
<ul>
<li>Learning rate was sampled from $\left[10^{-4}, 5 \cdot 10^{-4}\right]$.</li>
<li>Strength of the entropy regularization from $\left[10^{-4}, 10^{-3}\right]$.</li>
<li>Rewards were not scaled and not clipped in the new set of experiments. In our previous set of experiments, rewards were scaled by a factor from ${0.3,0.5}$ and clipped to 1 prior to back-propagation in the Advantage Actor-Critic algorithm.</li>
<li>Gradients are computed over non-overlaping chunks of 50 or 75 steps of the episode. In our previous set of experiments, we used chunks of 100 steps.
The auxiliary tasks, when used, have hyperparameters sampled from:</li>
<li>Coefficient $\beta_{d}$ of the depth prediction loss from convnet features $\mathcal{L}_{d}$ sampled from ${3.33,10,33}$.</li>
<li>Coefficient $\beta_{d}^{\prime}$ of the depth prediction loss from LSTM hiddens $\mathcal{L}_{d^{\prime}}$ sampled from ${1,3.33,10}$.</li>
<li>Coefficient $\beta_{l}$ of the loop closure prediction loss $\mathcal{L}_{l}$ sampled from ${1,3.33,10}$.</li>
</ul>
<p>Loop closure uses the following thresholds: maximum distance for position similarity $\eta_{1}=1$ square and minimum distance for removing trivial loop-closures $\eta_{2}=2$ squares.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Results are averaged over the top 5 random hyperparameters for each agent-task configuration. Star in the label indicates the use of reward clipping. Please see text for more details.</p>
<h1>C ADDITIONAL RESULTS</h1>
<h2>C. 1 REWARD CLIPPING</h2>
<p>Figure 9 shows additional learning curves. In particular in the left plot we show that the baselines (A3C FF and A3C LSTM) as well as Nav A3C agent without auxiliary losses, perform worse without reward clipping than with reward clipping. It seems that removing reward clipping makes learning unstable in absence of auxiliary tasks. For this particular reason we chose to show the baselines with reward clipping in our main results.</p>
<h2>C. 2 DEPTH PREDICTION AS REGRESSION OR CLASSIFICATION TASKS</h2>
<p>The right subplot of Figure 9 compares having depth as an input versus as a target. Note that using RGBD inputs to the Nav A3C agent performs even worse than predicting depth as a regression task, and in general is worse than predicting depth as a classification task.</p>
<h2>C. 3 NON-NAVIGATION TASKS IN 3D MAZE ENVIRONMENTS</h2>
<p>We have evaluated the behaviour of the agents introduced in this paper, as well as agents with reward prediction, introduced in (Jaderberg et al., 2017) (Nav A3C* $+R$ ) and with a combination of reward prediction from the convnet and depth prediction from the policy LSTM (Nav A3C $+R D_{2}$ ), on different 3D maze environments with non-navigation specific tasks. In the first environment, Seek-Avoid Arena, there are apples (yielding 1 point) and lemons (yielding -1 point) disposed in an arena, and the agents needs to pick all the apples before respawning; episodes last 20 seconds. The second environment, Stairway to Melon, is a thin square corridor; in one direction, there is a lemon followed by a stairway to a melon ( 10 points, resets the level) and in the other direction are 7 apples and a dead end, with the melon visible but not reachable. The agent spawns between the lemon and the apples with a random orientation. Both environments have been released in DeepMind Lab (Beattie et al., 2016). These environments do not require navigation skills such as shortest path planning, but a simple reward identification (lemon vs. apple or melon) and persistent exploration. As Figure 10 shows, there is no major difference between auxiliary tasks related to depth prediction or reward prediction. Depth prediction boosts the performance of the agent beyond that of the stacked LSTM architecture, hinting at a more general applicability of depth prediction beyond navigation tasks.</p>
<h2>C. 4 SENSITIVITY TOWARDS HYPER-PARAMETER SAMPLING</h2>
<p>For each of the experiments in this paper, 64 replicas were run with hyperparameters (learning rate, entropy cost) sampled from the same interval. Figure 11 shows that the Nav architectures with</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Comparison of agent architectures over non-navigation maze configurations, Seek-Avoid Arena and Stairway to Melon, described in details in (Beattie et al., 2016). Image credits for (c) and (d): (Jaderberg et al., 2017).
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Plot of the Area Under the Curve (AUC) of the rewards achieved by the agents, across different experiments and on 3 different tasks: large static maze with fixed goals, large static maze with comparable layout but with dynamic goals, and the I-maze. The reward AUC values are computed for each replica; 64 replicas were run per experiment and the reward AUC values are sorted by decreasing value.
auxiliary tasks achieve higher results for a comparatively larger number of replicas, hinting at the fact that auxiliary tasks make learning more robust to the choice of hyperparameters.</p>
<h1>C. 5 ASYMPTOTIC PERFORMANCE OF THE AGENTS</h1>
<p>Finally, we compared the asymptotic performance of the agents, both in terms of navigation (final rewards obtained at the end of the episode) and in terms of their representation in the policy LSTM. Rather than visualising the convolutional filters, we quantify the change in representation, with and</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Agent architecture</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Frames</td>
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">LSTM A3C*</td>
<td style="text-align: center;">Nav A3C+ $D_{2}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{1 2 0 M}$</td>
<td style="text-align: center;">Score (mean top 5)</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">$\mathbf{1 0 3}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Position Acc</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$\mathbf{7 2 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{2 4 0 M}$</td>
<td style="text-align: center;">Score (mean top 5)</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">$\mathbf{1 1 4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Position Acc</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">$\mathbf{8 0 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Asymptotic performance analysis of two agents in the Random Goal 2 maze, comparing training for 120M Labyrinth frames vs. 240M frames.
without auxiliary task, in terms of position decoding, following the approach explained in Section 5.1. Specifically, we compare the baseline agent (LSTM A3C*) to a navigation agent with one auxiliary task (depth prediction), that gets about twice as many gradient updates for the same number of frames seen in the environment: once for the RL task and once for the auxiliary depth prediction task. As Table 3 shows, the performance of the baseline agent as well as the position decoding accuracy do significantly increase after twice the number of training steps (going from 57 points to 90 points, and from $33.4 \%$ to $66.5 \%$, but do not reach the performance and position decoding accuracy of the Nav $\mathrm{A} 3 \mathrm{C}+D_{2}$ agent after half the number of training frames. For this reason, we believe that the auxiliary task do more than simply accelerate training.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Video of the Nav A3C<em> $+D_{1} L$ agent on the I-maze: https://youtu.be/PS4iJ7Hk_BU
${ }^{6}$ Video of the Nav A3C</em> $+D_{1} L$ agent on static maze 1: https://youtu.be/-HsjQoIou_c
${ }^{7}$ Video of the Nav A3C<em> $+D_{1} L$ agent on static maze 2: https://youtu.be/kHlAvRAYkbi
${ }^{8}$ Video of the Nav A3C</em> $+D_{1} L$ agent on random goal maze 1: https://youtu.be/5IBT2UADJYO
${ }^{9}$ Video of the Nav A3C* $+D_{1} L$ agent on random goal maze 2: https://youtu.be/e10mXgBG9yo&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
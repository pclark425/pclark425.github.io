<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4283 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4283</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4283</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-276579678</p>
                <p><strong>Paper Title:</strong> A large language model framework for literature-based disease–gene association prediction</p>
                <p><strong>Paper Abstract:</strong> Abstract With the exponential growth of biomedical literature, leveraging Large Language Models (LLMs) for automated medical knowledge understanding has become increasingly critical for advancing precision medicine. However, current approaches face significant challenges in reliability, verifiability, and scalability when extracting complex biological relationships from scientific literature using LLMs. To overcome the obstacles of LLM development in biomedical literature understating, we propose LORE, a novel unsupervised two-stage reading methodology with LLM that models literature as a knowledge graph of verifiable factual statements and, in turn, as semantic embeddings in Euclidean space. LORE captured essential gene pathogenicity information when applied to PubMed abstracts for large-scale understanding of disease–gene relationships. We demonstrated that modeling a latent pathogenic flow in the semantic embedding with supervision from the ClinVar database led to a 90% mean average precision in identifying relevant genes across 2097 diseases. This work provides a scalable and reproducible approach for leveraging LLMs in biomedical literature analysis, offering new opportunities for researchers to identify therapeutic targets efficiently.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4283.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4283.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation extraction and Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage literature-semantics framework that uses LLMs to extract atomic factual relations from scientific articles (LLM-ORE) and to produce dense literature-semantic embeddings (LLM-EMB) for downstream modeling and discovery of domain-level patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613, Llama-8B, OpenAI text-embedding-3-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LORE (two-stage: LLM-ORE + LLM-EMB)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Two-stage process: (1) LLM-ORE — prompt-based open relation extraction applied at the article (title+abstract) level using text-continuation prompts and demonstrations to produce atomic <subject, predicate, object> triplets for target entity pairs; (2) LLM-EMB — read all extracted relations for each entity pair, concatenate into (sub)documents, and encode those documents into fixed-length dense vectors (512-d) using an embedding LLM API (text-embedding-3-large). The extracted relations are compiled into a verifiable knowledge graph; embeddings are used for manifold analyses and supervised ranking (ML-Ranker). The pipeline includes splitting long relation-documents into sub-documents if exceeding LLM context, lemmatization of predicates, automated tagging by curated key semantics, and optional use of smaller open models (Llama-8B) for large-scale processing.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to multiple scales: primary experiments on a subset (1,745,538 articles → 11,285,095 relations); reported extraction of 11,000,000 relations from 1.7 million abstracts with GPT-3.5; extended processing of 3,997,496 abstracts using Llama-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical literature (disease-gene relationships, clinical genetics, precision medicine).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical semantic relations (atomic factual statements) and higher-level empirical patterns/generalizations about pathogenicity (e.g., latent manifold organization and cross-disease 'pathogenic flow').</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) Sentential relations such as 'gene X mutation cosegregates with disease Y' and 'gene X causes disease Y'. 2) Higher-level empirical pattern: a smooth cross-disease directional 'pathogenic flow' in the literature-semantic embedding where literature evidence indicating pathogenicity increases monotonically along certain manifold axes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Downstream supervised evaluation vs. ClinVar expert-curated pathogenic labels using ranking: leave-one-disease-out cross-validation and 5-fold gene-disjoint CV; comparison to baselines (paper co-occurrence counting, LLM-ORE counts, direct LLM questioning such as GPT-4o and GPT-3.5 answers, linear regression on embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ML-Ranker (using LORE embeddings/features) achieved Mean Average Precision (MAP) = 79.9% across 2097 diseases (all DGs), MAP = 90.0% when restricted to DGs with LLM-ORE annotations; co-occurrence paper counting MAP = 69.4%; LLM-EMB linear regression MAP = 87.7%; direct GPT-4o questioning MAP = 31.7%. Using Llama-8B to process 3,997,496 abstracts produced 74,132,940 relations and an ML-Ranker MAP = 81.6% (when extended scope). Coverage: LLM-ORE relations covered 71.4% of ClinVar DGs for the main subset; paper co-occurrence covered 94.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines included naive paper co-occurrence counting (#paper), simple relation count from LLM-ORE (LLM-ORE counts and key-semantics counts), LLM-EMB linear regression, and direct LLM Q&A (GPT-3.5 and GPT-4o). ML-Ranker outperformed #paper and GPT-4o; LLM-EMB regression was strong (87.7%) but ML-Ranker (90.0%) provided higher performance when relations present.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An LLM-driven two-stage extraction + embedding pipeline can create a verifiable, literature-scale knowledge graph and embeddings that capture latent semantic structure relevant to pathogenicity; embeddings reveal a smooth, cross-disease consistent 'pathogenic flow' enabling disease-wise ranking; smaller open models (Llama-8B) can scale extraction with modest performance loss; constraining LLM output to explicit extracted relations improves verifiability and mitigates hallucination compared to directly querying LLMs for domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM hallucination and opacity of parametric memory motivate extraction-to-KG approach; LORE still depends on article abstracts (not full text), so some ClinVar entries lacking abstract co-occurrence remain unreachable (~1307 ClinVar DGs without pubmedKB co-occurrence); LLM-ORE coverage (71.4%) lower than simple co-occurrence; curated key semantics used ClinVar during curation (possible bias); retrieval and context-window limits require splitting long relation-documents; scalability requires using smaller/open models to process full corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4283.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4283.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-ORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt-driven open relation extraction method that uses LLMs to read individual article texts and emit atomic structured relation triplets (<subject, predicate, object>) describing relationships between target entities at article level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (primary), Llama-8B (alternative for scale)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-ORE (prompted open relation extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Task-agnostic text-continuation prompts with a demonstration section specifying desired behavior (examples and counterexamples) are used so the LLM rewrites article-level understanding into concise factual triplets. The prompt enforces constraints: subject contains first entity, object contains second entity, predicate is concise and self-contained, and encourages abstract understanding beyond sentence-level syntax. Output is a list of relational triplets; lemmatization and later tagging map predicates to 'linguistic lemmas' and curated key-semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied on 1.7 million abstracts to produce ~11 million relations in one run; on a bootstrapped subset of 1,745,538 articles they produced 11,285,095 relations; Llama-8B used to process ~3,997,496 abstracts producing 74,132,940 relations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical literature (disease-gene and variant relations).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Atomic factual relations (sentential assertions) about entities — e.g., causal/associative claims, mutation mentions, cohort observations. These are qualitative relational statements rather than mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Examples include 'gene X causes disease Y', 'mutation in gene X is frequently encountered in disease Y patients', 'mutation X cosegregates with disease Y'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Indirect evaluation via downstream ranking performance (ML-Ranker) against ClinVar labels; coverage statistics compared to simple co-occurrence; manual curation of lemmas (sampling 10 relations from pathogenic DGs and 10 from other DGs) during key-semantics taxonomy construction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LLM-ORE relations covered 71.4% of ClinVar DGs in the main subset; when counting tagged key-semantics relations per DG as a predictor, performance (MAP) was lower than full ML-Ranker but provided useful evidence indexing (precise MAP value for tag-count baseline reported lower than ML-Ranker and co-occurrence baseline — co-occurrence MAP = 69.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to naive co-occurrence (#paper) counting and to direct LLM questioning; LLM-ORE provides verifiable relations while direct LLM Q&A (GPT-4o) had poor MAP (31.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompted LLMs can reliably extract article-level atomic relations when guided by demonstrations and constraints, producing a concise knowledge graph that preserves verifiability and supports downstream embedding and ranking tasks; open models can be substituted for scale with modest performance changes.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Coverage less than raw co-occurrence (not all co-occurring DGs yield extractable relations), reliance on prompts and demonstration quality, potential for inconsistent phrasing across relations (necessitating heavy lemmatization and taxonomy curation), and the need to manage hallucinated or imprecise extractions via human review or conservative tagging rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4283.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4283.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-EMB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Embedding of Literature Semantics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that encodes all extracted relations between an entity pair (from LLM-ORE) into a fixed-length dense vector embedding representing the literature knowledge about that pair; these embeddings are used for manifold analysis and supervised ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI text-embedding-3-large (embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-EMB (document-level embedding of relations)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each entity pair, concatenate all relation triplets into a document (split into sub-documents if exceeding model context), then encode the document(s) with an embedding LLM to produce a 512-dimensional dense vector for the pair; if multiple sub-documents exist, aggregate embeddings weighted by token count. Embeddings serve as features for unsupervised visualization (UMAP, PCA) and as inputs to supervised ML-Ranker.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Embeddings constructed for entity pairs derived from the subset (1,745,538 articles → 11,285,095 relations) and for larger processed corpora (1.7M and ~4M abstracts as described).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical literature (disease-gene relations).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Latent empirical patterns and manifold structures (e.g., clustering of pathogenic DGs and the 'pathogenic flow' directional field in embedding space); embeddings summarize qualitative relations into continuous vectors that reveal empirical organization.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Observed that ClinVar-known pathogenic DGs cluster in a subspace aligned with semantics 'mutation' and 'cause'; discovered a curved arm manifold where literature semantics transition from non-pathogenic to pathogenic, forming a directional 'pathogenic flow'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Visualization (UMAP, PCA, ridge regression axes) to reveal latent structures; supervised regression/ranking with ClinVar labels (linear regression on embeddings achieved MAP = 87.7%; ML-Ranker using embeddings achieved higher MAP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LLM-EMB linear regression MAP = 87.7% (as a baseline for embedding-only supervised modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to paper co-occurrence counting and raw LLM-ORE relation counts; embeddings provided superior predictive features and captured latent semantics enabling high MAP.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embeddings of extracted relation-documents capture latent semantic structure strongly associated with pathogenicity; embedding space reveals smooth, interpretable manifolds and consistent directional fields useful for disease-wise ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Embedding quality depends on the completeness and correctness of relation extraction; context-window limitations can necessitate splitting longer relation sets; embeddings summarize but can obscure provenance unless linked back to KG relations and source article IDs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4283.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4283.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pathogenic flow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pathogenic flow (literature-semantic directional field)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discovered empirical pattern in the literature-semantic embedding: a smooth, cross-disease consistent vector field indicating the direction and magnitude from non-pathogenic to pathogenic evidence across embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Derived from LLM-EMB embeddings (OpenAI text-embedding-3-large) and LLM-ORE relations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Pathogenic flow analysis (disease-wise flow vectors aggregated in spatial cubes)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute per-DG unit flow vectors pointing from non-pathogenic DG embeddings toward average pathogenic DG embeddings for the same disease (and vice versa), then quantize/average these vectors within spatial cubes of embedding space to obtain a vector field u_L that reflects cross-disease consistency and directionality of pathogenic evidence. Visualize with arrows where length reflects consistency magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Pattern derived from embeddings built on relations extracted from the literature subset (1,745,538 articles; 11,285,095 relations) and larger processed corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical genetics / literature mining.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical generalization / pattern: a cross-disease semantic gradient (directional flow) correlating literature semantics with pathogenicity labels.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) Along a curved arm manifold in embedding space, semantic indicators like 'mutation' and 'cause' increase and correspond to higher pathogenicity scores. 2) Even if pathogenic and non-pathogenic DGs from different diseases mix locally, the aggregated flow direction allows consistent disease-wise ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative visualization (UMAP, PCA, 3D linear subspaces), and quantitative validation via improvement in disease-wise ranking when ML-Ranker models the disease-wise flow (LambdaRank with GBDT) against ClinVar labels (AP/MAP metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using ML-Ranker (which explicitly models disease-wise flow) yielded MAP = 90.0% on DGs with LLM-ORE annotations and 79.9% overall, demonstrating the utility of flow modeling versus point-wise approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually and empirically to point-wise absolute label modeling (which requires complex nonlinear separation) and to embedding-only linear regression; modeling disease-wise flow enabled better and more generalizable ranking across diseases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The pathogenic flow is a robust, cross-disease consistent semantic gradient that makes disease-wise ranking more tractable and generalizable, even when local mixtures of pathogenic/non-pathogenic points exist in the embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Flow estimation relies on having both pathogenic and non-pathogenic labeled DGs per disease to compute directions; for diseases with sparse ClinVar annotations the flow estimate may be noisy; the method aggregates across diseases and may hide disease-specific subtleties.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4283.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4283.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Key semantics taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curated taxonomy of key pathogenicity semantics (105 key semantics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually curated taxonomy of 105 key semantics (grouped into 4 classes and 15 subclasses) mapped from linguistic lemmas extracted from LLM-ORE relations to tag relations with pathogenicity-relevant semantic types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-ORE outputs (GPT-3.5 / Llama-8B used to produce source relations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Key semantics curation (lemma extraction + coverage/precision filtering + manual taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Three-step pipeline: (1) lemmatize predicates from LLM-ORE sentential relations into linguistic lemmas (normalize inflections, remove entity names), (2) filter important lemmas by coverage (appear in relations of at least n=100 DGs) and precision (>=50% relations associated with ClinVar-known pathogenic DGs), yielding 282 important lemmas, (3) manual curation to produce a 105-key-semantic taxonomy; sample relations inspected (10 pathogenic + 10 others) per lemma; taxonomy tags are applied to relations to index KG and support evidence retrieval and simple tag-count-based scoring baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the set of LLM-ORE relations created from the literature subset (1,745,538 articles → 11,285,095 relations) and larger corpora processed by Llama-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical literature (pathogenicity semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Categorical semantic labels capturing qualitative notions (Relation, Mutation, Disease, Cohort/misc) — effectively distilled linguistic patterns that index types of evidence relevant to pathogenicity.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Tag examples: 'mutation cosegregates with disease' (Relation class), 'loss-of-function' or 'haploinsufficiency' (Mutation class), 'familial aggregation' or 'observed in patients' (Disease/Cohort classes).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Effectiveness assessed by using tag-count per DG as a naive pathogenicity predictor (compared to ClinVar labels) and by manual inspection of lemma-relation samples during curation; note curation used ClinVar info so results not directly comparable to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not presented as a standalone high-level metric; tag-count baseline performed worse than ML-Ranker (MAP lower than 79.9%) but provided useful evidence indexing. Coverage: after taxonomy tagging, key semantics served to annotate relations automatically across the KG (282 lemmas condensed to 105 key semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared as a simple method (counting tagged relations) against co-occurrence counting, LLM-EMB regression, and ML-Ranker; tag-count approach is weaker but useful for rapid evidence triage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatically tagging LLM-extracted relations with a curated taxonomy provides actionable indexes for KG browsing and evidence retrieval and helps highlight semantics strongly associated with pathogenicity (e.g., 'cause', 'mutation').</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Curation used ClinVar as a reference (introduces bias); lemma filtering parameters (coverage n and precision r) influence which semantics are kept; tag-counting as a predictive method is limited and not independent of the curation supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4283.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4283.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational large language model from OpenAI used in this study as the primary LLM for article-level open relation extraction (LLM-ORE) to convert abstracts into atomic relation triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Used as the LLM-ORE engine for prompt-based open relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Text-continuation prompts with a demonstration and explicit constraints were used; GPT-3.5 parsed title+abstract into lists of <subject,predicate,object> triplets for each target entity pair; outputs were then lemmatized and aggregated into a KG.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Produced ~11 million relations across ~1.7 million abstracts in the main run described.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical literature / disease-gene relations.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Atomic sentential relations and input to downstream extraction of higher-level patterns (e.g., pathogenic flow).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>See LLM-ORE examples: 'e1 causes e2', 'e1 mutations are frequently encountered in e2 patients', 'e1 haploinsufficiency results in e2'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Performance of the full pipeline using GPT-3.5-produced relations evaluated by downstream ranking accuracy (ML-Ranker MAP) vs. ClinVar; coverage statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using GPT-3.5 for relation extraction contributed to ML-Ranker achieving MAP up to 90.0% on DGs with relations; 11M relations from 1.7M abstracts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with using open Llama-8B for extraction at scale (marginal performance loss but much greater coverage for full corpora) and compared with direct questioning of models like GPT-4o (which performed poorly on direct Q&A).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 is competent for article-level comprehension and reliable relation extraction when guided by prompts and demonstrations; extracting relations (rather than directly asking model-level facts) improves verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Model parameterization is opaque; possible hallucinations in unconstrained outputs mitigated by enforcing structured triplet format and linking to source abstracts; scaling GPT-3.5 across entire corpora is costly, motivating use of smaller/open models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4283.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4283.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B-Instruct (Llama-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, instruction-tuned LLaMA-family model with ~8B parameters used here as a smaller alternative to GPT-3.5 for large-scale LLM-ORE processing of PubMed abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (Llama-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Used as an alternative LLM-ORE engine for scalable extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Same prompt-based LLM-ORE prompt structure applied to Llama-8B to extract relation triplets at article level; used to process the full set of ~3.997 million pubmedKB abstracts for DG co-occurrence to produce 74,132,940 relations at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Processed ~3,997,496 pubmedKB abstracts with DG co-occurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical literature (disease-gene relations).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Atomic relations and support for extracting the same higher-level patterns (semantic embeddings and pathogenic flow) when scaled.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Same kinds of relation triplets as with GPT-3.5; used to support construction of KG and embeddings at larger corpus scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Downstream evaluation via ML-Ranker ranking performance and coverage compared to ClinVar labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using Llama-8B to scale extraction produced 74,132,940 relations covering 91.3% of ClinVar DGs and enabled ML-Ranker MAP = 81.6% on the larger scope; stand-alone Llama-8B-based ML-Ranker MAP = 79.5% in earlier subset comparison (comparable to GPT-3.5-based 79.9% MAP on full set).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-3.5 extraction (marginal performance loss) and to direct LLM Q&A baselines (GPT-4o), and to naive co-occurrence counting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller open models (8B) can perform article-level comprehension sufficiently well to support large-scale KG construction, with modest loss in ranking performance but major gains in scalability and accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Smaller model capacity can yield marginal performance loss; requires careful prompt engineering; quality trade-offs exist but are acceptable for large-scale corpus processing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models encode clinical knowledge <em>(Rating: 2)</em></li>
                <li>Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature? <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>The impact of large language models on scientific discovery: a preliminary study using GPT-4 <em>(Rating: 2)</em></li>
                <li>Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4283",
    "paper_id": "paper-276579678",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "LORE",
            "name_full": "LLM-based Open Relation extraction and Embedding",
            "brief_description": "A two-stage literature-semantics framework that uses LLMs to extract atomic factual relations from scientific articles (LLM-ORE) and to produce dense literature-semantic embeddings (LLM-EMB) for downstream modeling and discovery of domain-level patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613, Llama-8B, OpenAI text-embedding-3-large",
            "model_size": null,
            "method_name": "LORE (two-stage: LLM-ORE + LLM-EMB)",
            "method_description": "Two-stage process: (1) LLM-ORE — prompt-based open relation extraction applied at the article (title+abstract) level using text-continuation prompts and demonstrations to produce atomic &lt;subject, predicate, object&gt; triplets for target entity pairs; (2) LLM-EMB — read all extracted relations for each entity pair, concatenate into (sub)documents, and encode those documents into fixed-length dense vectors (512-d) using an embedding LLM API (text-embedding-3-large). The extracted relations are compiled into a verifiable knowledge graph; embeddings are used for manifold analyses and supervised ranking (ML-Ranker). The pipeline includes splitting long relation-documents into sub-documents if exceeding LLM context, lemmatization of predicates, automated tagging by curated key semantics, and optional use of smaller open models (Llama-8B) for large-scale processing.",
            "number_of_papers": "Applied to multiple scales: primary experiments on a subset (1,745,538 articles → 11,285,095 relations); reported extraction of 11,000,000 relations from 1.7 million abstracts with GPT-3.5; extended processing of 3,997,496 abstracts using Llama-8B.",
            "domain_or_field": "Biomedical literature (disease-gene relationships, clinical genetics, precision medicine).",
            "type_of_laws_extracted": "Empirical semantic relations (atomic factual statements) and higher-level empirical patterns/generalizations about pathogenicity (e.g., latent manifold organization and cross-disease 'pathogenic flow').",
            "example_laws_extracted": "1) Sentential relations such as 'gene X mutation cosegregates with disease Y' and 'gene X causes disease Y'. 2) Higher-level empirical pattern: a smooth cross-disease directional 'pathogenic flow' in the literature-semantic embedding where literature evidence indicating pathogenicity increases monotonically along certain manifold axes.",
            "evaluation_method": "Downstream supervised evaluation vs. ClinVar expert-curated pathogenic labels using ranking: leave-one-disease-out cross-validation and 5-fold gene-disjoint CV; comparison to baselines (paper co-occurrence counting, LLM-ORE counts, direct LLM questioning such as GPT-4o and GPT-3.5 answers, linear regression on embeddings).",
            "performance_metrics": "ML-Ranker (using LORE embeddings/features) achieved Mean Average Precision (MAP) = 79.9% across 2097 diseases (all DGs), MAP = 90.0% when restricted to DGs with LLM-ORE annotations; co-occurrence paper counting MAP = 69.4%; LLM-EMB linear regression MAP = 87.7%; direct GPT-4o questioning MAP = 31.7%. Using Llama-8B to process 3,997,496 abstracts produced 74,132,940 relations and an ML-Ranker MAP = 81.6% (when extended scope). Coverage: LLM-ORE relations covered 71.4% of ClinVar DGs for the main subset; paper co-occurrence covered 94.8%.",
            "comparison_baseline": "Baselines included naive paper co-occurrence counting (#paper), simple relation count from LLM-ORE (LLM-ORE counts and key-semantics counts), LLM-EMB linear regression, and direct LLM Q&A (GPT-3.5 and GPT-4o). ML-Ranker outperformed #paper and GPT-4o; LLM-EMB regression was strong (87.7%) but ML-Ranker (90.0%) provided higher performance when relations present.",
            "key_findings": "An LLM-driven two-stage extraction + embedding pipeline can create a verifiable, literature-scale knowledge graph and embeddings that capture latent semantic structure relevant to pathogenicity; embeddings reveal a smooth, cross-disease consistent 'pathogenic flow' enabling disease-wise ranking; smaller open models (Llama-8B) can scale extraction with modest performance loss; constraining LLM output to explicit extracted relations improves verifiability and mitigates hallucination compared to directly querying LLMs for domain knowledge.",
            "challenges_limitations": "LLM hallucination and opacity of parametric memory motivate extraction-to-KG approach; LORE still depends on article abstracts (not full text), so some ClinVar entries lacking abstract co-occurrence remain unreachable (~1307 ClinVar DGs without pubmedKB co-occurrence); LLM-ORE coverage (71.4%) lower than simple co-occurrence; curated key semantics used ClinVar during curation (possible bias); retrieval and context-window limits require splitting long relation-documents; scalability requires using smaller/open models to process full corpora.",
            "uuid": "e4283.0",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-ORE",
            "name_full": "LLM-based Open Relation Extraction",
            "brief_description": "Prompt-driven open relation extraction method that uses LLMs to read individual article texts and emit atomic structured relation triplets (&lt;subject, predicate, object&gt;) describing relationships between target entities at article level.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613 (primary), Llama-8B (alternative for scale)",
            "model_size": null,
            "method_name": "LLM-ORE (prompted open relation extraction)",
            "method_description": "Task-agnostic text-continuation prompts with a demonstration section specifying desired behavior (examples and counterexamples) are used so the LLM rewrites article-level understanding into concise factual triplets. The prompt enforces constraints: subject contains first entity, object contains second entity, predicate is concise and self-contained, and encourages abstract understanding beyond sentence-level syntax. Output is a list of relational triplets; lemmatization and later tagging map predicates to 'linguistic lemmas' and curated key-semantics.",
            "number_of_papers": "Applied on 1.7 million abstracts to produce ~11 million relations in one run; on a bootstrapped subset of 1,745,538 articles they produced 11,285,095 relations; Llama-8B used to process ~3,997,496 abstracts producing 74,132,940 relations.",
            "domain_or_field": "Biomedical literature (disease-gene and variant relations).",
            "type_of_laws_extracted": "Atomic factual relations (sentential assertions) about entities — e.g., causal/associative claims, mutation mentions, cohort observations. These are qualitative relational statements rather than mathematical laws.",
            "example_laws_extracted": "Examples include 'gene X causes disease Y', 'mutation in gene X is frequently encountered in disease Y patients', 'mutation X cosegregates with disease Y'.",
            "evaluation_method": "Indirect evaluation via downstream ranking performance (ML-Ranker) against ClinVar labels; coverage statistics compared to simple co-occurrence; manual curation of lemmas (sampling 10 relations from pathogenic DGs and 10 from other DGs) during key-semantics taxonomy construction.",
            "performance_metrics": "LLM-ORE relations covered 71.4% of ClinVar DGs in the main subset; when counting tagged key-semantics relations per DG as a predictor, performance (MAP) was lower than full ML-Ranker but provided useful evidence indexing (precise MAP value for tag-count baseline reported lower than ML-Ranker and co-occurrence baseline — co-occurrence MAP = 69.4%).",
            "comparison_baseline": "Compared implicitly to naive co-occurrence (#paper) counting and to direct LLM questioning; LLM-ORE provides verifiable relations while direct LLM Q&A (GPT-4o) had poor MAP (31.7%).",
            "key_findings": "Prompted LLMs can reliably extract article-level atomic relations when guided by demonstrations and constraints, producing a concise knowledge graph that preserves verifiability and supports downstream embedding and ranking tasks; open models can be substituted for scale with modest performance changes.",
            "challenges_limitations": "Coverage less than raw co-occurrence (not all co-occurring DGs yield extractable relations), reliance on prompts and demonstration quality, potential for inconsistent phrasing across relations (necessitating heavy lemmatization and taxonomy curation), and the need to manage hallucinated or imprecise extractions via human review or conservative tagging rules.",
            "uuid": "e4283.1",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-EMB",
            "name_full": "LLM-based Embedding of Literature Semantics",
            "brief_description": "An approach that encodes all extracted relations between an entity pair (from LLM-ORE) into a fixed-length dense vector embedding representing the literature knowledge about that pair; these embeddings are used for manifold analysis and supervised ranking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI text-embedding-3-large (embedding model)",
            "model_size": null,
            "method_name": "LLM-EMB (document-level embedding of relations)",
            "method_description": "For each entity pair, concatenate all relation triplets into a document (split into sub-documents if exceeding model context), then encode the document(s) with an embedding LLM to produce a 512-dimensional dense vector for the pair; if multiple sub-documents exist, aggregate embeddings weighted by token count. Embeddings serve as features for unsupervised visualization (UMAP, PCA) and as inputs to supervised ML-Ranker.",
            "number_of_papers": "Embeddings constructed for entity pairs derived from the subset (1,745,538 articles → 11,285,095 relations) and for larger processed corpora (1.7M and ~4M abstracts as described).",
            "domain_or_field": "Biomedical literature (disease-gene relations).",
            "type_of_laws_extracted": "Latent empirical patterns and manifold structures (e.g., clustering of pathogenic DGs and the 'pathogenic flow' directional field in embedding space); embeddings summarize qualitative relations into continuous vectors that reveal empirical organization.",
            "example_laws_extracted": "Observed that ClinVar-known pathogenic DGs cluster in a subspace aligned with semantics 'mutation' and 'cause'; discovered a curved arm manifold where literature semantics transition from non-pathogenic to pathogenic, forming a directional 'pathogenic flow'.",
            "evaluation_method": "Visualization (UMAP, PCA, ridge regression axes) to reveal latent structures; supervised regression/ranking with ClinVar labels (linear regression on embeddings achieved MAP = 87.7%; ML-Ranker using embeddings achieved higher MAP).",
            "performance_metrics": "LLM-EMB linear regression MAP = 87.7% (as a baseline for embedding-only supervised modeling).",
            "comparison_baseline": "Compared to paper co-occurrence counting and raw LLM-ORE relation counts; embeddings provided superior predictive features and captured latent semantics enabling high MAP.",
            "key_findings": "Embeddings of extracted relation-documents capture latent semantic structure strongly associated with pathogenicity; embedding space reveals smooth, interpretable manifolds and consistent directional fields useful for disease-wise ranking.",
            "challenges_limitations": "Embedding quality depends on the completeness and correctness of relation extraction; context-window limitations can necessitate splitting longer relation sets; embeddings summarize but can obscure provenance unless linked back to KG relations and source article IDs.",
            "uuid": "e4283.2",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Pathogenic flow",
            "name_full": "Pathogenic flow (literature-semantic directional field)",
            "brief_description": "A discovered empirical pattern in the literature-semantic embedding: a smooth, cross-disease consistent vector field indicating the direction and magnitude from non-pathogenic to pathogenic evidence across embedding space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Derived from LLM-EMB embeddings (OpenAI text-embedding-3-large) and LLM-ORE relations",
            "model_size": null,
            "method_name": "Pathogenic flow analysis (disease-wise flow vectors aggregated in spatial cubes)",
            "method_description": "Compute per-DG unit flow vectors pointing from non-pathogenic DG embeddings toward average pathogenic DG embeddings for the same disease (and vice versa), then quantize/average these vectors within spatial cubes of embedding space to obtain a vector field u_L that reflects cross-disease consistency and directionality of pathogenic evidence. Visualize with arrows where length reflects consistency magnitude.",
            "number_of_papers": "Pattern derived from embeddings built on relations extracted from the literature subset (1,745,538 articles; 11,285,095 relations) and larger processed corpora.",
            "domain_or_field": "Biomedical genetics / literature mining.",
            "type_of_laws_extracted": "Empirical generalization / pattern: a cross-disease semantic gradient (directional flow) correlating literature semantics with pathogenicity labels.",
            "example_laws_extracted": "1) Along a curved arm manifold in embedding space, semantic indicators like 'mutation' and 'cause' increase and correspond to higher pathogenicity scores. 2) Even if pathogenic and non-pathogenic DGs from different diseases mix locally, the aggregated flow direction allows consistent disease-wise ranking.",
            "evaluation_method": "Qualitative visualization (UMAP, PCA, 3D linear subspaces), and quantitative validation via improvement in disease-wise ranking when ML-Ranker models the disease-wise flow (LambdaRank with GBDT) against ClinVar labels (AP/MAP metrics).",
            "performance_metrics": "Using ML-Ranker (which explicitly models disease-wise flow) yielded MAP = 90.0% on DGs with LLM-ORE annotations and 79.9% overall, demonstrating the utility of flow modeling versus point-wise approaches.",
            "comparison_baseline": "Compared conceptually and empirically to point-wise absolute label modeling (which requires complex nonlinear separation) and to embedding-only linear regression; modeling disease-wise flow enabled better and more generalizable ranking across diseases.",
            "key_findings": "The pathogenic flow is a robust, cross-disease consistent semantic gradient that makes disease-wise ranking more tractable and generalizable, even when local mixtures of pathogenic/non-pathogenic points exist in the embedding space.",
            "challenges_limitations": "Flow estimation relies on having both pathogenic and non-pathogenic labeled DGs per disease to compute directions; for diseases with sparse ClinVar annotations the flow estimate may be noisy; the method aggregates across diseases and may hide disease-specific subtleties.",
            "uuid": "e4283.3",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Key semantics taxonomy",
            "name_full": "Curated taxonomy of key pathogenicity semantics (105 key semantics)",
            "brief_description": "A manually curated taxonomy of 105 key semantics (grouped into 4 classes and 15 subclasses) mapped from linguistic lemmas extracted from LLM-ORE relations to tag relations with pathogenicity-relevant semantic types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM-ORE outputs (GPT-3.5 / Llama-8B used to produce source relations)",
            "model_size": null,
            "method_name": "Key semantics curation (lemma extraction + coverage/precision filtering + manual taxonomy)",
            "method_description": "Three-step pipeline: (1) lemmatize predicates from LLM-ORE sentential relations into linguistic lemmas (normalize inflections, remove entity names), (2) filter important lemmas by coverage (appear in relations of at least n=100 DGs) and precision (&gt;=50% relations associated with ClinVar-known pathogenic DGs), yielding 282 important lemmas, (3) manual curation to produce a 105-key-semantic taxonomy; sample relations inspected (10 pathogenic + 10 others) per lemma; taxonomy tags are applied to relations to index KG and support evidence retrieval and simple tag-count-based scoring baselines.",
            "number_of_papers": "Applied to the set of LLM-ORE relations created from the literature subset (1,745,538 articles → 11,285,095 relations) and larger corpora processed by Llama-8B.",
            "domain_or_field": "Biomedical literature (pathogenicity semantics).",
            "type_of_laws_extracted": "Categorical semantic labels capturing qualitative notions (Relation, Mutation, Disease, Cohort/misc) — effectively distilled linguistic patterns that index types of evidence relevant to pathogenicity.",
            "example_laws_extracted": "Tag examples: 'mutation cosegregates with disease' (Relation class), 'loss-of-function' or 'haploinsufficiency' (Mutation class), 'familial aggregation' or 'observed in patients' (Disease/Cohort classes).",
            "evaluation_method": "Effectiveness assessed by using tag-count per DG as a naive pathogenicity predictor (compared to ClinVar labels) and by manual inspection of lemma-relation samples during curation; note curation used ClinVar info so results not directly comparable to other methods.",
            "performance_metrics": "Not presented as a standalone high-level metric; tag-count baseline performed worse than ML-Ranker (MAP lower than 79.9%) but provided useful evidence indexing. Coverage: after taxonomy tagging, key semantics served to annotate relations automatically across the KG (282 lemmas condensed to 105 key semantics).",
            "comparison_baseline": "Compared as a simple method (counting tagged relations) against co-occurrence counting, LLM-EMB regression, and ML-Ranker; tag-count approach is weaker but useful for rapid evidence triage.",
            "key_findings": "Automatically tagging LLM-extracted relations with a curated taxonomy provides actionable indexes for KG browsing and evidence retrieval and helps highlight semantics strongly associated with pathogenicity (e.g., 'cause', 'mutation').",
            "challenges_limitations": "Curation used ClinVar as a reference (introduces bias); lemma filtering parameters (coverage n and precision r) influence which semantics are kept; tag-counting as a predictive method is limited and not independent of the curation supervision.",
            "uuid": "e4283.4",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-3.5 (gpt-3.5-turbo-0613)",
            "name_full": "OpenAI GPT-3.5 (gpt-3.5-turbo-0613)",
            "brief_description": "A conversational large language model from OpenAI used in this study as the primary LLM for article-level open relation extraction (LLM-ORE) to convert abstracts into atomic relation triplets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_size": null,
            "method_name": "Used as the LLM-ORE engine for prompt-based open relation extraction",
            "method_description": "Text-continuation prompts with a demonstration and explicit constraints were used; GPT-3.5 parsed title+abstract into lists of &lt;subject,predicate,object&gt; triplets for each target entity pair; outputs were then lemmatized and aggregated into a KG.",
            "number_of_papers": "Produced ~11 million relations across ~1.7 million abstracts in the main run described.",
            "domain_or_field": "Biomedical literature / disease-gene relations.",
            "type_of_laws_extracted": "Atomic sentential relations and input to downstream extraction of higher-level patterns (e.g., pathogenic flow).",
            "example_laws_extracted": "See LLM-ORE examples: 'e1 causes e2', 'e1 mutations are frequently encountered in e2 patients', 'e1 haploinsufficiency results in e2'.",
            "evaluation_method": "Performance of the full pipeline using GPT-3.5-produced relations evaluated by downstream ranking accuracy (ML-Ranker MAP) vs. ClinVar; coverage statistics reported.",
            "performance_metrics": "Using GPT-3.5 for relation extraction contributed to ML-Ranker achieving MAP up to 90.0% on DGs with relations; 11M relations from 1.7M abstracts reported.",
            "comparison_baseline": "Compared with using open Llama-8B for extraction at scale (marginal performance loss but much greater coverage for full corpora) and compared with direct questioning of models like GPT-4o (which performed poorly on direct Q&A).",
            "key_findings": "GPT-3.5 is competent for article-level comprehension and reliable relation extraction when guided by prompts and demonstrations; extracting relations (rather than directly asking model-level facts) improves verifiability.",
            "challenges_limitations": "Model parameterization is opaque; possible hallucinations in unconstrained outputs mitigated by enforcing structured triplet format and linking to source abstracts; scaling GPT-3.5 across entire corpora is costly, motivating use of smaller/open models.",
            "uuid": "e4283.5",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama-8B",
            "name_full": "Llama-3.1-8B-Instruct (Llama-8B)",
            "brief_description": "An open-source, instruction-tuned LLaMA-family model with ~8B parameters used here as a smaller alternative to GPT-3.5 for large-scale LLM-ORE processing of PubMed abstracts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (Llama-8B)",
            "model_size": "8B",
            "method_name": "Used as an alternative LLM-ORE engine for scalable extraction",
            "method_description": "Same prompt-based LLM-ORE prompt structure applied to Llama-8B to extract relation triplets at article level; used to process the full set of ~3.997 million pubmedKB abstracts for DG co-occurrence to produce 74,132,940 relations at scale.",
            "number_of_papers": "Processed ~3,997,496 pubmedKB abstracts with DG co-occurrence.",
            "domain_or_field": "Biomedical literature (disease-gene relations).",
            "type_of_laws_extracted": "Atomic relations and support for extracting the same higher-level patterns (semantic embeddings and pathogenic flow) when scaled.",
            "example_laws_extracted": "Same kinds of relation triplets as with GPT-3.5; used to support construction of KG and embeddings at larger corpus scale.",
            "evaluation_method": "Downstream evaluation via ML-Ranker ranking performance and coverage compared to ClinVar labels.",
            "performance_metrics": "Using Llama-8B to scale extraction produced 74,132,940 relations covering 91.3% of ClinVar DGs and enabled ML-Ranker MAP = 81.6% on the larger scope; stand-alone Llama-8B-based ML-Ranker MAP = 79.5% in earlier subset comparison (comparable to GPT-3.5-based 79.9% MAP on full set).",
            "comparison_baseline": "Compared to GPT-3.5 extraction (marginal performance loss) and to direct LLM Q&A baselines (GPT-4o), and to naive co-occurrence counting.",
            "key_findings": "Smaller open models (8B) can perform article-level comprehension sufficiently well to support large-scale KG construction, with modest loss in ranking performance but major gains in scalability and accessibility.",
            "challenges_limitations": "Smaller model capacity can yield marginal performance loss; requires careful prompt engineering; quality trade-offs exist but are acceptable for large-scale corpus processing.",
            "uuid": "e4283.6",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature?",
            "rating": 2,
            "sanitized_title": "retrieve_summarize_and_verify_how_will_chatgpt_affect_information_seeking_from_the_medical_literature"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine",
            "rating": 1,
            "sanitized_title": "benefits_limits_and_risks_of_gpt4_as_an_ai_chatbot_for_medicine"
        }
    ],
    "cost": 0.01788225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction</p>
<p>Peng-Hsuan Li 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Yih-Yun Sun 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Hsueh-Fen Juan 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Department of Life Science
National Taiwan University
No. 1, Sec. 4, Roosevelt Rd10617TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Chien-Yu Chen 0000-0002-6940-6389
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Department of Biomechatronics Engineering
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Huai-Kuang Tsai 0000-0002-4200-8137
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Institute of Information Science
Academia Sinica
No. 128, Academia Road, Section 211529NankangTaipeiTaiwan</p>
<p>Jia-Hsin Huang jiahsin.huang@gmail.com 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction
AED27303585B563AB05DD7EDBA7B860210.1093/bib/bbaf070Received: November 15, 2024. Revised: January 9, 2025. Accepted: February 6, 2025literature miningbiomedical relation extractionNLPknowledge graphlarge language model
This study explores biomedical informatics and artificial intelligence, leveraging large language models and knowledge graphs to advance precision medicine and enhance the discovery of disease-gene relationships.</p>
<p>Introduction</p>
<p>Knowledge curation from scientific literature is fundamental to the advancement of research across disciplines [1][2][3].Traditionally, domain-specific knowledge accumulates incrementally and relies heavily on human expert review processes.The biomedical literature, for instance, is a key resource for identifying causal genetic elements associated with diseases and offering insights into clinical practice.Several expert-curated databases, such as ClinVar [4], COSMIC [5], OMIM [6], and PharmGKB [7], provide invaluable assessments of literature evidence.However, such resources are limited in scale because of the broad scope and the rapid expansion of scientific publications [8,9].Many computational approaches have been applied to enhance automation in biomedical literature-based discovery for different tasks, such as gene-disease association prediction [10][11][12], text mining and curation [13][14][15][16][17], and biomedical entity relation extraction [18][19][20][21].However, these methods primarily focus on extracting isolated sentences or paragraphs containing entities of interest rather than synthesizing comprehensive information across multiple sources and contexts.Thus, substantial efforts are required to create task-specific datasets and train models for each literature domain.</p>
<p>On the other hand, Machine Reading Comprehension (MRC) [22], wherein machines answer questions based on textual context, serves as a promising complement to human expertise in reading vast amounts of literature.Recent advancements in natural language processing, particularly the development of Large Language Models (LLMs), have significantly enhanced MRC capabilities to potentially accelerate knowledge synthesis [23][24][25].Recent LLMs, such as GPT-4, have demonstrated remarkable capabilities in textual comprehension across diverse domains; however, LLMs face challenges when it comes to reliability and verifiability [23,26].Specifically, LLMs are prone to hallucination, a phenomenon whereby plausible but factually incorrect information is generated.Moreover, the opaque parametric memory of LLMs poses a substantial obstacle to traceabilitysources of evidence supporting their statements are often unclear.To mitigate these concerns, researchers have applied Retrieval Augmented Generation (RAG) in LLM-based chatbots [27,28].RAG restricts the information source of an LLM to an explicit but small set of retrieved texts per user query.However, owing to scalability constraints, fast but shallow sentence similarity-based retrieval is required, leading to incomplete information capture as nuanced content and relevant articles are often missing.Figure 1.Overview of the literature-semantics framework.Given a large body of literature containing expert domain knowledge, the proposed framework creates a comprehensive unsupervised knowledge graph and numerical embeddings between entities.This enables large-scale supervised modeling of downstream tasks, where model predictions are accompanied by verifiable evidence of relations that can be traced back to the original articles.(a) The framework is applied to PubMed literature, and a knowledge graph containing semantic relations between diseases and genes and their mutations is created.(b) An embedding for disease-gene relationships, where each point in space contains the literature-semantic knowledge of a DG, is created.(c) The embedding is shown to contain a latent structure of DG pathogenicity.(d) We further analyze the disease-wise pathogenic f low and find that it is consistent across diseases and even smoother than the point-wise distribution.(e) An ML-ranker is trained to model the f low and to predict pathogenic genes for each disease, where the prediction scope is 200× larger than expert-curated supervision.(f) We curated 105 key semantics about DG pathogenicity with linguistic lemmas to automatically tag relations.(G) With the proposed framework, we facilitate future research on DG pathogenicity with a literature-scale knowledge base of predicted DG scores and supporting evidence.</p>
<p>In essence, scalable knowledge curation calls for a computational method that is inductive across domains and is capable of capturing nuanced textual context for knowledge synthesis, all while maintaining essential reliability and verifiability.To this end, we introduce LORE (LLM-based Open Relation extraction and Embedding), a novel literature semantics framework that encompasses the best of both worlds and is tested true in capturing disease-gene relationships across the PubMed literature (Fig. 1).</p>
<p>LORE leverages a two-stage reading methodology comprising LLM-based Open Relation Extraction (LLM-ORE) and LLM-based embedding (LLM-EMB) (Fig. 1a).First, LORE employs LLM-ORE to comprehend each article and generate atomic statements about entity relations therein.Curated knowledge is explicitly derived from individual articles, making the generated relations reliable and verifiable.Furthermore, this operation creates a comprehensive unsupervised knowledge graph of the literature, the conciseness of which makes it possible for LORE to then use LLM-EMB to encode the full relation knowledge between each entity pair and create numerical embeddings for downstream task-specific applications.In this work, we applied LORE to curate disease-gene relationship knowledge in the PubMed literature, using disease, gene, and variant annotations from pubmedKB [2].</p>
<p>The ClinVar [4] database is an important annotation repository of the relationships between genes and diseases.Using key disease-gene pairs from ClinVar as references, we evaluated the effectiveness of LORE to explore the latent space of gene-wise pathogenicity and disease-wise pathogenic f low across genes (Fig. 1b).In addition, we constructed machine learning models with the supervision of pathogenic genes from ClinVar to rank the relevance of gene pathogenicity using the semantic embeddings (Fig. 1b).</p>
<p>Finally, we curated a taxonomy of key semantics to use as tags for relations (Fig. 1c).We created PMKB-CV (pubmedKB-ClinVar) dataset, a novel resource that expands the scope of disease-gene relationship data.Notably, PMKB-CV encompasses more than 2097 diseases and covers disease-gene pairs (DGs) at a scale 200 times larger than that covered by ClinVar.Moreover, PMKB-CV provides rich annotations, including semantic embeddings, predicted DG scores, and verifiable knowledge graph relations with tags and source article IDs (Fig. 1c).In summary, our LORE framework harnesses the power of LLM-based MRC and enables literaturescale knowledge graph construction and downstream modeling.Importantly, PMKB-CV further bridges the gap between largescale computational analysis and human assessment, helping to advance our understanding of disease genetics and potential therapeutic targets.</p>
<p>Results</p>
<p>LLM-ORE curates semantic relations knowledge from literature</p>
<p>We applied LLM-ORE to PubMed abstracts for annotating diseasegene relationships and created a comprehensive unsupervised knowledge graph (Fig. 1a).A total of 11 million relations across 1.7 million abstracts were obtained by prompting GPT-3.5 [29].Using text-continuation prompts [30], we employed LLMs to analyze individual articles and extract atomic statements describing relations between pairs of entities.Figure 2 illustrates the prompt structure used to guide GPT-3.5 in this task.This prompt is composed of two key sections.The first section demonstrates a domain-agnostic Open Relation Extraction (ORE) [31].The text serves as a primer, independent of the specific biomedical context, to establish the expected format and level of detail for the extraction (see more details in Methods).Following the same structure as the demonstration, the second section applies the ORE process to the target article and entities under investigation.Notably, this approach allows for a generalized ORE from the literature, which is not constrained to predefined relation types or entity pairs.A total of 358,888 distinct semantic lemmas are present across the 11 million disease-gene relations.We reviewed 282 highcoverage lemmas in the knowledge graph and curated a taxonomy of 105 key semantics about pathogenicity (Fig. 3).The key semantics, automatically tagged to relations, served as a set of indexes to access the knowledge graph.We grouped the key pathogenicity semantics into four main classes.</p>
<p>Class 1, Relation, includes key semantics that directly describe the relation between a gene and a disease.For example, the key semantic 'mutation cosegregates with disease' in Class 1.3 describes the correlation between occurrence of a certain genetic mutation and whether the genome is from a patient of a certain disease.</p>
<p>Class 2, Mutation, conveys information about genetic mutations.This type of information implicates that the corresponding gene has a role in a certain disease.</p>
<p>Class 3, Disease, conveys information on the genetic aspects of diseases.This semantic implies that a certain gene is at play.Class 4 contains cohort and miscellaneous information that entails or hints at pathogenicity.</p>
<p>LLM-EMB embeddings capture underlying gene pathogenicity</p>
<p>In the second stage of LORE, we applied LLM-EMB to the PubMed DG knowledge graph created in the first stage using LLM-ORE.For each pair of entities, all their relations across all articles were encoded to a single vector, which contained the literature knowledge about their relationship.A dense 512-dimensional representation was created for each DG.</p>
<p>To analyze the latent pathogenicity structure within the literature-semantic embedding, we first displayed the embedding space using 2D UMAP [32] (Fig. 4a-d).Each point represented a DG.When points were colored by co-occurrence frequency in PubMed abstracts, high-frequency DGs were observed to be distributed throughout the space (Fig. 4a).When points were colored by ClinVar pathogenicity labels, pathogenic DGs were observed to cluster in a subspace (Fig. 4b).This subspace was well-captured by the pathogenic score prediction of ML-Ranker (See more details about the ML-Ranker in the following sections and in Methods).With an optimal pathogenic score threshold to split the DGs by color into red or gray, the distribution was close to that of the ClinVar labels (Fig. 4d).In contrast to the ClinVar labels, which were human-curated and sparse, the stratified pathogenic score predicted by ML-Ranker delineated a smooth landscape of pathogenicity (Fig. 4c).Thus, high-score DGs not curated yet in ClinVar will be of high interest to the biomedical community.</p>
<p>Next, we displayed the 3D linear subspaces of the LLM-EMB embedding and observed the DG pathogenicity distribution (Fig. 4e-g).The axes were calculated using Principal Component Analysis (PCA) or ridge regression, a regularized version of ordinary least squares regression, against ClinVar pathogenicity labels.For the PCA subspace (Fig. 4e), the top three dimensions explained 12.8% of the variance of the embedding.A latent structure of two manifolds was observed-a dense spherical cluster of gray DG points without ClinVar annotations and a curved pattern of gradual transition from gray to red (i.e., known pathogenic DGs annotated in ClinVar).For the subspaces spanned by a combination of PCA axes and regression axes (Fig. 4f,g), clearer manifolds were observed when more regression axes were included.With two regression axes, a distribution pattern aligned with ClinVar annotations, signifying an association with pathogenicity, was revealed.We noted that the regression axes spanned a smaller subspace of the embedding compared with the PCA axes.Nevertheless, they provided an analytical pathogenicity perspective of the unsupervised embedding.</p>
<p>We further analyzed the literature-semantics structure by visualizing the distribution of important semantic lemmas in the 3D PCA subspace (Fig. 4h-j).The frequency of each semantic lemma, such as 'cause,' was represented in log scale per DG using a cool-blue-to-warm-red color gradient.Distinct patterns emerged for various lemmas, particularly those lemmas with a f latter distribution, such as 'associate,' 'mutation,' and 'cause.' The semantic 'associate' (Fig. 4h) was observed to increase along a linear axis but was more prevalent (indicated by greener colors) in the curved arm than in the sparse parts (bluer).The semantic 'mutation' (Fig. 4i) was primarily distributed along the curved arm.Similarly, the semantic 'cause' (Fig. 4j) was concentrated along the curved arm, with a significant presence at the space correlated with ClinVar annotations.This visualization of semantic lemmas revealed a connection between the intrinsic semantic structure of LLM-EMB and the latent DG point-wise pathogenicity structure.</p>
<p>In short, our analyses revealed the connection between the literature semantics captured by LORE and the known pathogenic DGs in the ClinVar database, as illustrated in Fig. 4. Using both UMAP and PCA visualization techniques, we examined how these known pathogenic DGs are distributed within the semantic LLM-EMB embedding space.Notably, these pathogenic DGs clustered in specific subspaces that aligned with disease-related semantic concepts -particularly terms like 'mutation' and 'cause'.This clustering pattern demonstrates the potential value of LLM-EMB embeddings as robust features for developing our ML-Ranker system for pathogenicity prediction.</p>
<p>Pathogenic flow in the literature-semantic space</p>
<p>In this section, we explore the embedding space underlying the DG pathogenicity distribution (Fig. 5).First, to model pathogenicity, a straightforward approach would be to model the distribution of pathogenic DGs (i.e., red points) in the embedding space (Fig. 5a,b).Clear subspaces of pathogenicity and nonpathogenicity were evident in the literature-semantic space visualized using 3D linear axes (Fig. 4e-g).These low-rank subspaces can be seen in the gray balls and the ends of the gray-to-red arm.However, the gray and red dots co-occur in some places, such as the center of the arm.To distinguish between gray and red dots, high-dimensional hyperplanes or complex nonlinear subspaces are needed, requiring more data and hampering generalization.</p>
<p>However, the fundamental task is to predict the most relevant pathogenic genes for each disease.If the gray and red dots for each disease are distributed separately to the two ends of a linear axis or, more generally, a smooth curve, pathogenic and nonpathogenic genes can be distinguished by splitting the curve.Furthermore, if the curves are consistent across diseases, the relevant pathogenic genes for every disease can be identified by monotonically raising the predicted relevance along the curves.Under this condition, perfect modeling can be achieved even if nonpathogenic and pathogenic DGs from different diseases are mixed in the embedding space (Fig. 5c).</p>
<p>In this study, we defined pathogenic f low as the direction and magnitude of nonpathogenicity to pathogenicity at each location in space.We started by calculating the disease-wise f low direction at each DG.Then, we quantized the f low direction at each location in space, using f low magnitude to ref lect consistency.We observed a smooth, cross-disease consistent field of pathogenic f low in the literature-semantic embedding (Fig. 5d).A consistent f low of nonpathogenicity to pathogenicity was noted along the arm-shaped manifold of the gray-to-red transition, implying that although DG point-wise modeling was difficult in the central mixture of the arm, disease-wise modeling was smoother and much more linear.</p>
<p>Literature-scale pathogenicity prediction by ML-ranker</p>
<p>We built an ML-Ranker that can model disease-wise pathogenic f low and score DGs that co-occur in PubMed abstracts.We applied the lambda objective with Gradient-Boosted Decision Trees (GBDT) [33] to directly model disease-wise pathogenic f low.</p>
<p>We compiled the PMKB-CV dataset to validate the proposed approach (Fig. 6a).PMKB-CV contains 2097 diseases that are present in both pubmedKB and ClinVar.For these diseases, 652 701 DGs co-occurred in PubMed abstracts, whereas only 3004 DGs were human-curated by ClinVar.Paper abstract co-occurrence covered 94.8% of the 3004 known pathogenic DGs, whereas LLM-ORE relations covered 71.4% (Fig. 6b).We included pubmedKB annotations, such as the number of paper abstracts in which a DG co-occurs in the dataset, and also used them as model features.Moreover, we used zero embedding for DGs without relations.Consequently, all DGs in the dataset can be uniformly used for training and prediction.We used leave-one-disease-out cross-validation to evaluate the performance of ML-Ranker for predicting pathogenic genes for each disease iteratively.An average precision (AP) score was calculated for each disease and Mean Average Precision (MAP) was used to evaluate the overall performance of the ranker.As a baseline, we directly asked GPT-3.5 (ver.2023-06-13) and GPT-4o (ver.2024-05-13) about the pathogenicity of each DG.In addition, to put into perspective the effectiveness of the curated key semantics in identifying crucial literature evidence, we tested a DG pathogenicity prediction method of simply counting the number of tagged relations per DG.Of note, the curated key semantics were only used in the experiment 'LLM-ORE (key semantics)' (Fig. 6b).</p>
<p>For all DGs in PMKB-CV, ML-Ranker achieved an MAP of 79.9%, which was a significant enhancement over the 69.4% MAP of co-occurrence paper counting and the 31.7%MAP of GPT-4o (Fig. 6b).Similar performance (MAP = 79.2%) was observed when applying 5-fold cross-validation of disjoint genes subsets (Supplementary Fig. 1).</p>
<p>When focusing specifically on those DGs with LLM-ORE annotations, ML-Ranker yielded a remarkable MAP of 90.0% (Fig. 6b).The predictive performance (AP) of ML-Ranker was statistically significantly better than other methods including co-occurrence paper counting (LLM-#paper), literature-semantic relation counting (LLM-ORE), and latent pathogenic f low modeling (LLM-EMB) (Fig. 6c).For the highly prevalent diseases that co-occurred with the highest number of genes in pubmedKB, ML-Ranker achieved a robust MAP of 81%, compared with the 67% MAP of co-occurrence paper counting (Fig. 6d).GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>Furthermore, we extended LORE to use the open-source Llama-8B model (ver.Llama-3.1-8B-Instruct).Using a much smaller and accessible model, ML-Ranker achieved a comparable 79.5% MAP to the 79.9% MAP of using GPT-3.5.Leveraging Llama-8B, we processed all 3,997,496 pubmedKB abstracts with DG co-occurrence and curated 74,132,940 relations.The resulting relations covered 91.3% of ClinVar DGs, enabling ML-Ranker to achieve a notable ranking performance of 81.6% MAP (Fig. 6e).</p>
<p>Finally, the DG scores and ranking provided by ML-Ranker are accompanied by literature evidence (Fig. 6f).Our approach facilitates future expert assessment of DG pathogenicity by a quick grasp of literature knowledge with key semantics relations and relevant articles.</p>
<p>Discussion</p>
<p>Recent advancements in LLMs aim to automate complex sensemaking as human endeavors in reading and connecting information across large collections of scientific literature [34].Our study introduces LORE, a novel literature semantics framework that fundamentally reframes how we leverage LLMs to extract and use knowledge from scientific literature.</p>
<p>The LORE framework offers several key advantages.First, knowledge synthesis using the LORE approach constructs a literature knowledge graph of verifiable factual statements linked to the sources.Second, LORE offers a scalable framework for knowledge synthesis from large amounts of article texts.LORE extracts original article texts and transforms them into a concise knowledge graph.The approach is more efficient than traditional retrieval augmented generation approaches that select only a small set of articles for an LLM to read.The knowledge graph is much more concise compared with the original articles.This reduction in size and complexity allows for a more efficient representation of information; all the relevant knowledge can then be embedded for downstream tasks.In addition, LORE allows new publications to be annotated, thereby continually expanding the knowledge graph.Third, this approach places much less demand on the capability of LLMs, compared with directly asking LLMs expert domain questions.Using LORE, we have captured gene pathogenicity with GPT-3.5 and Llama-8B (Fig. 6e), a feat far from being achieved by directly asking GPT-4o (Fig. 6b).Indeed, small and open-source LLMs have been demonstrated to be competent for article-level comprehension [35,36], hence the methodology is not constrained to enterprise LLMs.Finally, our framework demonstrates remarkable efficacy in capturing disease-gene relationships through unsupervised relationship extraction and embedding, and users can also employ LORE with prompt engineering and fine-tuning to annotate task-specific knowledge across various domains of scientific inquiry [37,38].</p>
<p>When applying LORE to the complex landscape of DG relationships, we demonstrated the presence of a latent smooth field of cross-disease consistent pathogenic f low in the unsupervised literature-semantic embeddings.This discovery reveals that although pathogenic and nonpathogenic DGs from different diseases may occupy similar locations in the embedding space, a consistent directional f low of pathogenicity exists in terms of semantics.To illustrate, consider a simplified one-dimensional embedding axis where rare and common diseases coexist (Supplementary Fig. 2).Suppose that D1 is a rare disease reported in a few studies and that D2 is a common disease whose association with many genes is discussed in a multitude of papers; in this scenario, the following literature annotations, DG locations, and pathogenicity labels are possible:</p>
<p>G1 is not related to D1. (x = 0, y = non-pathogenic).</p>
<p>G2 mutation is found in a D1 patient. (x = 1, y = pathogenic). G3 mutation is found in a D2 patient; G3 is associated with D2. (x = 2, y = non-pathogenic). G4 mutation is found in a D2 patient; G4 causes D2. (x = 3, y = pathogenic).</p>
<p>Although the pathogenic D1G2 and the non-pathogenic D2G3 are mixed in the center of the axis, literature evidence about pathogenicity consistently increases along the axis.As a result, even when pathogenic and non-pathogenic associations are interspersed due to inter-disease differences such as popularity, the literature-semantic axis provides for a cross-disease consistent linear f low, enabling accurate pathogenicity modeling across diseases.</p>
<p>Our initial analyses of this pathogenic f low revealed clusters of disease-specific pathogenic curves.Notably, we found that these clusters often form a continuum, with the endpoint of one cluster serving as the starting point for another.This observation suggests a broader, interconnected field of pathogenic relationships across diseases, offering new perspectives on the complex landscape of genetic pathogenicity.</p>
<p>LORE curates knowledge for entity pairs that co-occur in literature articles.For the study of disease-gene pathogenicity, we noted that the potential curation scope was larger than the PMKB-CV dataset.PMKB-CV contained 2097 diseases that had both pubmedKB DG co-occurrence and known ClinVar pathogenic DGs (Fig. 6a); the full pubmedKB contained 3 128 402 DGs co-occurred in abstracts, spanning 8894 diseases (Supplementary Fig. 3).In this study, we focused on those 2097 diseases that could be validated by ClinVar, but the potential curation scope was as large as the 3 128 402 DGs.On the other hand, we also noted that the full ClinVar contained 4311 known pathogenic DGs, 1307 of which had no pubmedKB abstract co-occurrence.This was the inherent limitation to article abstract-based MRC.</p>
<p>Conclusion</p>
<p>In summary, our study makes three significant contributions to the field.First, it presents a novel literature semantics framework that addresses the long-standing challenges of comprehensiveness, reliability, and verifiability in machine reading comprehension.Second, it demonstrates the efficacy of LORE in capturing complex pathogenic relationships across diseases to reveal new insights into pathogenic f low.Finally, it provides a literature-scale dataset that not only complements existing resources such as ClinVar but also offers a knowledge graph of DG relationships with graded pathogenicity scores for genetic prioritization in clinical practice.The methodology of LORE is a general improvement on LLM-based machine reading comprehension, paving the way for bridging vast literature resources and actionable scientific knowledge to realize accelerated discoveries across scientific disciplines.</p>
<p>Methods</p>
<p>Two-stage reading comprehension of LORE</p>
<p>In the first stage of LORE, we applied LLM-ORE by prompting LLMs. Figure 2 shows the actual prompt we used.The demonstration consists of an article with a topic as Martin Likes Fish, target entity pair 'Martin' and 'fish', along with lists of unwanted and desired annotations.This section implicitly specifies the ORE task and the following required properties.</p>
<p>(1) The annotations should be concrete statements of fact implied by the article.</p>
<p>(2) The annotations should follow a structured format, specifically a list of relational triplets.</p>
<p>(3) Each relational triplet should be &lt; 'subject', 'predicate', 'object' &gt; .</p>
<p>(4) The subject should contain the first entity, and the first entity should only appear in the subject.</p>
<p>(5) The object should contain the second entity, and the second entity should only appear in the object.</p>
<p>(6) The predicate should be a concise, self-contained description of the relation between the subject and the object.</p>
<p>(7) Abstract understanding of the article is allowed beyond sentencelevel syntax.</p>
<p>The approach allows for abstract understanding beyond sentence-level syntax.The requirements are better understood through demonstration rather than explicit definitions.For instance, instead of explaining the terms 'subject,' 'predicate,' and 'object,' the examples and counterexamples in the demonstration make the concept clear.In addition, the examples illustrate behaviors that are easier to grasp instinctively rather than by complex rules.Examples include the removal of 'also' from 'Martin also loves eating fish', the rewriting of 'large fish scare him' to 'scare of', and the digested understanding of 'Martin dreamed about fishing' from 'he has a dream.The dream is about fishing'.Finally, we note that the ending '-' is important in ensuring that LLM follows the desired list format.</p>
<p>Formally, the desired generation
g = f LLM−ORE p, e 1 , e 2 ; m
where p is the target article, e 1 and e 2 are the names of the target entities, and m is the LLM model.In this work, we use the paper title and abstract as p.For m, gpt-3.5-turbo-0613 is used.The function maps &lt; p, e 1 , e 2 &gt; to g, the list of parsed relational triplets, using the text-continuation prompt shown in Fig. 2.</p>
<p>In the second stage of LORE, LLM reads all relations between an entity pair and produces a numerical representation of knowledge about their relationships.For example, the following relations between the entity e 1 and the entity e 2 are read by LLM as the following document.'e1', 'causes', 'e2'.'e1 mutations', 'are frequently encountered in', 'e2 patients'.'e1 haploinsufficiency', 'results in', 'e2'.Document: 'e1 causes e2.e1 mutations are frequently encountered in e2 patients.e1 haploinsufficiency results in e2.'.</p>
<p>If the document is larger than the allowed context of an LLM, it is split into multiple sub-documents, each of which contains as many relations as possible.</p>
<p>Formally, the embedding vector of an entity pair is given by
v = f LLM−EMB D e1,e2 ; m =</p>
<p>Modeling the pathogenic flow with ML-ranker</p>
<p>To visualize the disease-wise pathogenic f low, we define the f low as a unit vector at each DG that points to the pathogenic direction of disease D at the embedding location of DG.Formally, the f low vector is given by
u DG = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ v DG − 1 |S 0 D | DG ∈S 0 D v DG if P(DG) = 1 1 |S 1 D | DG ∈S 1 D v DG − v DG if P(DG) = 0 u DG = u DG | u DG |
where v denotes the embedding vector by LLM-EMB, S 0 D and S 1 D denote the sets of non-pathogenic and pathogenic diseasegene pairs of D respectively, and P maps non-pathogenic and pathogenic pairs to 0 and 1, respectively.In other words, each non-pathogenic DG has a unit f low vector directed at the average embedding of the pathogenic DGs of the same disease, and each pathogenic DG has a unit f low vector directed from the average embedding of the non-pathogenic DGs of the same disease.Then, we quantize the f low vectors for each cube in space by averaging them.Formally, a quantized f low vector is given by
u L = 1 | L | DG∈L u DG
where L is the set of DGs in a cube subspace.Finally, we show the field of pathogenic f low in Fig. 5d, where each arrow corresponds to a quantized f low vector.The arrow direction is the aggregated disease-wise f low direction from non-pathogenicity to pathogenicity, and the arrow length, proportional to | u L |, reflects the degree of cross-disease consistency at that location.</p>
<p>As shown in Fig. 5c, the essence of modeling the pathogenic f low is to model the difference between non-pathogenic and pathogenic genes per disease.Specifically, suppose DG1 and DG2 correspond to the same disease but P(DG1) = 1 and P(DG2) = 0. Let s denote the predicted pathogenicity score.Then one would want to maximize the probability Pr (P(DG1) &gt; P(DG2)) by minimizing the following RankNet [39]  To make sure the most relevant genes are ranked on top for each disease, the final gradient, λ, is the gradient of the RankNet loss multiplied by the change in Normalized Discounted Cumulative Gain (NDCG) [40].Formally, this LambdaRank [41] gradient is given by λ DG1,DG2 = ∂L ∂s DG1</p>
<p>• NDCG (DG1, DG2)</p>
<p>In this work, we use λGBDT, the lambda objective combined with GBDT [33], as the ML-Ranker to explicitly model the pathogenic f low in the literature-semantic space.</p>
<p>To evaluate the ranking performance for each disease, AP is used to see if the known pathogenic genes for a disease are ranked on top.Formally, for each disease,
AP = 1 R N k=1 known(k) × precision(k)
where N is the number of ranked genes for that disease, and R is the number of ranked known pathogenic genes for that disease.</p>
<p>known(k) = 1 if the gene ranked at k is a known pathogenic gene for that disease; otherwise known(k) = 0. precision(k) is the percentage of known pathogenic genes among genes ranked top-k for that disease.</p>
<p>Key semantics curation</p>
<p>The curation process consists of three main steps: linguistic lemma extraction, important lemma identification, and manual taxonomy construction.In the first step, the sentential relations extracted by LLM-ORE are tokenized to bags of words, and word inf lections are lemmatized to dictionary form.For example, cause, causes, caused, and causing all correspond to the same linguistic lemma.At this stage, entity names are also filtered out.</p>
<p>In the second step, important lemmas are identified using a coverage filter and a precision filter.The coverage filter demands a lemma to appear in LLM-ORE relations of at least n DGs.The precision filter requires that a certain proportion r of all the relations involving a lemma be from known pathogenic DGs, as indicated by ClinVar.The parameters can be adjusted according to the desired scope of key semantics.In this work, we selected a coverage parameter n = 100 and a precision parameter r = 50% and resulted in 282 important lemmas.</p>
<p>The final step involves manually curating a taxonomy of 105 key semantics by examining the LLM-ORE relations associated with these important lemmas During curation, we sampled 10 relations from known pathogenic DGs and 10 relations from other DGs to inspect for each lemma.The resulting key semantics were then used to tag all relevant relations in the respective lemmas.As a result, the LLM-ORE knowledge graph contains sentential relations linked to the semantic taxonomy of pathogenicity.</p>
<p>Furthermore, we experimented with a DG pathogenicity prediction method where the number of tagged relations for each DG is directly used as its pathogenic score.We note that the curation process has used the ClinVar information, so the generalizability of the ranking performance of this method is not directly comparable to other methods.Nevertheless, we put the effectiveness of the curation method into perspective, showing what performance and scope DG pathogenicity researchers can expect when using the key semantics tags to grasp the literature knowledge and identify relevant relations and articles for their DGs of interest.</p>
<p>Constructing the PMKB-CV dataset</p>
<p>We constructed the PMKB-CV dataset as a large-scale complement of ClinVar and an evaluation benchmark for our proposed methodology.The scope of the dataset is defined using disease IDs from Medical Subject Headings (MeSH), a vocabulary thesaurus maintained by the Nation Library of Medicine (NLM), and Homo sapiens protein-coding gene IDs from the National Center for Biotechnology Information (NCBI).</p>
<p>The PMKB-CV dataset comprises two main components including literature-based and expert database parts.For the literature part, we utilized the annotations from pubmedKB [2].The gene and variant mentions are both indexed by NCBI gene IDs, and we considered the occurrence of either a gene or its variant as an occurrence.This approach yielded 3,128,402 DG pairs with co-occurrence within abstracts, encompassing 8894 diseases and 18,393 genes.In addition, we extracted a subset of PubMed articles as the most relevant literature for the study of diseasegene relationships via a bootstrapping iteration with pubmedKB annotations as features.Using leave-one-disease-out training, we predicted a bootstrap score for each DG.Then, the literature subset is formed by the articles associated with the top three genes for each disease, the top three diseases for each gene, and the top 15 K DGs.The resulting subset contains 1,745,538 articles, for which we applied LORE and curated 11,285,095 relations.</p>
<p>The expert database part was derived from ClinVar [4], which provides gene-disease relationship data.As ClinVar uses OMIM (Online Mendelian Inheritance in Man) [6] numbers for disease indexing, we applied UMLS (Unified Medical Language System) [42], a vocabulary alignment dataset maintained by NLM, to map OMIM numbers to MeSH IDs.This process resulted in 4311 known pathogenic DGs, spanning 3175 distinct diseases and 2416 distinct genes.</p>
<p>The final PMKB-CV dataset was created by including those diseases that have both pubmedKB co-occurrence DGs and Clin-Var known pathogenic DGs.Statistics of the resulting dataset are shown in Fig. 6a.</p>
<p>Key Points</p>
<p>• We present a scalable framework that achieves 90% mean AP in identifying pathogenic gene associations across 2097 diseases, demonstrating remarkable accuracy in automated literature interpretation while effectively mitigating LLM hallucination risks.• Our analysis of literature-based semantic embeddings revealed a consistent directional pattern in how pathogenic genes are represented across different diseases.While both pathogenic and non-pathogenic disease-gene pairs cluster similarly in the embedding space, we discovered a distinct semantic f low that indicates pathogenicity.This pattern could help automate the identification of disease-causing genes from scientific literature.• The framework provides a reproducible methodology for leveraging LLMs in biomedical literature analysis, offering a valuable tool for researchers and clinicians in understanding disease mechanisms and identifying potential therapeutic targets.</p>
<p>Figure 2 .
2
Figure 2. Annotating articles with LLM-ORE (open relation extraction).Text-continuation prompts are used to make LLM write down its understanding of an article in the form of atomic factual statements.The ORE task is crafted to extract concise relations between entities at the article level.For example, 'Martin dreamed about fishing' requires comprehension and rewriting of several sentences.Also, common unwanted behaviors are avoided by providing examples of bad relations.We applied the task-agnostic prompt to extract an open set of diverse and comprehensive entity relationships for academic literature.</p>
<p>Figure 3 .
3
Figure 3. Curated key semantics.We created a taxonomy of four main classes, 15 subclasses, and 105 key semantics about disease-gene pathogenicity.With their corresponding linguistic lemmas, tags are added to relations automatically.Here, two key semantics and sample relations are shown for every subclass.</p>
<p>Figure 4 .
4
Figure 4. LLM-EMB literature-semantic embedding visualization.(a-d) Visualization with UMAP to analyze the latent pathogenicity structure within the literature-semantic embedding.Points are colored by the number of papers (#paper) (a), ClinVar pathogenicity labels (b), graded ML-ranker prediction (c), and binary ML-ranker prediction (d).The sparse ClinVar-curated red pathogenic DGs are seen clustering toward a subspace, captured well by ML-ranker, which also provides a smooth landscape of graded predictions for uncurated DGs.(e-g) Visualization with linear axes calculated using PCA (e), ridge (g), and their combination (f).A point is colored red if it is a known pathogenic DG in ClinVar, and DGs with unknown pathogenicity are colored gray.A latent structure of two manifolds-a gray ball of nonpathogenicity and a curved arm of transition from nonpathogenicity to pathogenicity-resides in the semantic space.(h-j) PCA visualization colored by distributions of literature semantics 'associate' (h), 'mutation' (i), and 'cause' (j).The connection between the smooth semantics distribution and the sparsely-curated ClinVar pathogenicity distribution can be seen.</p>
<p>Figure 5 .
5
Figure 5. Pathogenic f low in the literature-semantic space.(a) Suppose in the semantic embedding, nine DGs from three different diseases reside on a curve.(b) For the DG point-wise objective, the disease group information is not used, and absolute zero-one labels are the prediction target.As a result, a non-linear function along the curve must be learned.(c) For the disease-wise f low objective, DGs are grouped by disease, and relative f low directions are the prediction target.Because of the cross-disease consistency of the f low, a linear function along the curve will be learned to rank DGs for every disease perfectly.(d) Visualization of the actual pathogenic f low.A smooth, cross-disease consistent field of pathogenic f low is seen residing in the literature-semantic space.</p>
<p>Figure 6 .
6
Figure 6.PMKB-CV dataset and ranking performance.(a) Statistics of the PMKB-CV dataset.For 2097 diseases, the literature semantics framework has a 200× prediction scope against curated DGs.(b) Mean average precision (MAP) of the ranking performance and the ClinVar coverage of different methods.GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>d∈De 1 ,e 2
2
emb d; m × | d | d∈De 1 ,e 2 | d | where e 1 and e 2 are the target entities, D e1,e2 is the set of all sub-documents, usually just one full document, containing the relations between e 1 and e 2 , and m is the LLM model.In this work, we employed the text-embedding-3-large from OpenAI for m, and the length of each sub-document | d | is its number of tokens according to m.</p>
<p>loss.L DG1,DG2 = − log Pr (P(DG1) &gt; P(DG2)) = log 1 + e −σ (sDG1−sDG2)</p>
<p>AcknowledgementsThis manuscript was edited by Wallace Academic Editing.The authors also thank Dau-Ming Niu and Yun-Ru Chen at the Taipei Veterans General Hospital in Taiwan for their support (NSTC 113-2634-F-A49-003).Code availabilityThe code supporting the conclusions of this study is available on GitHub at https://github.com/ailabstw/LORE.Data availabilityThe PMKB-CV datasets supporting the findings of this study are available at https://doi.org/10.5281/zenodo.14607639.FundingThis work was supported in part by the Center for Advanced Computing and Imaging in Biomedicine (NTU-113 L900701) from The Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education in Taiwan.Author contributionsSupplementary dataSupplementary data are available at Briefings in Bioinformatics online.Conf lict of interest: None declared.
How user intelligence is improving PubMed. N Fiorini, R Leaman, D J Lipman, 10.1038/nbt.4267Nat Biotechnol. 362018</p>
<p>pubmedKB: an interactive web server for exploring biomedical entity relations in the biomedical literature. P H Li, T F Chen, J Y Yu, 10.1093/nar/gkac310Nucleic Acids Res. 502022</p>
<p>PubMed and beyond: biomedical literature search in the age of artificial intelligence. Q Jin, R Leaman, Z Lu, 10.1016/j.ebiom.2024.104988EBioMedicine. 1001049882024</p>
<p>ClinVar: public archive of interpretations of clinically relevant variants. M J Landrum, J M Lee, M Benson, 10.1093/nar/gkv1222Nucleic Acids Res. 442016</p>
<p>COSMIC: the catalogue of somatic mutations In cancer. J G Tate, S Bamford, H C Jubb, 10.1093/nar/gky1015Nucleic Acids Res. 472019</p>
<p>OMIM.org: leveraging knowledge across phenotype-gene relationships. J S Amberger, C A Bocchini, A F Scott, 10.1093/nar/gky1151Nucleic Acids Res. 472019</p>
<p>An evidence-based framework for evaluating pharmacogenomics knowledge for personalized medicine. M Whirl-Carrillo, R Huddart, L Gong, 10.1002/cpt.2350Clin Pharmacol Ther. 1102021</p>
<p>Biomedical language processing: what's beyond PubMed?. L Hunter, K B Cohen, 10.1016/j.molcel.2006.02.012Mol Cell. 212006</p>
<p>Manual curation is not sufficient for annotation of genomic databases. W A Baumgartner, Jr, K B Cohen, L M Fox, 10.1093/bioinformatics/btm229Bioinformatics. 232007</p>
<p>Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. A Bravo, J Pinero, N Queralt-Rosinach, 10.1186/s12859-015-0472-9BMC Bioinform. 16552015</p>
<p>DisGeNET: a comprehensive platform integrating information on human diseaseassociated genes and variants. J Pinero, À Bravo, N Queralt-Rosinach, 10.1093/nar/gkw943Nucleic Acids Res. 452017</p>
<p>RENET: a deep learning approach for extracting gene-disease associations from literature. Y Wu, R Luo, Hcm Leung, 10.1007/978-3-030-17083-7_17Research in Computational Molecular Biology. 114672019</p>
<p>PGxCorpus, a manually annotated corpus for pharmacogenomics. J Legrand, R Gogdemir, C Bousquet, 10.1038/s41597-019-0342-9Sci Data. 732020</p>
<p>ACE2 expression is increased in the lungs of patients with comorbidities associated with severe COVID-19. Bgg Pinto, Aer Oliveira, Y Singh, 10.1093/infdis/jiaa332J Infect Dis. 2222020</p>
<p>Molecular and networklevel mechanisms explaining individual differences in autism spectrum disorder. A M Buch, P E Vértes, J Seidlitz, 10.1038/s41593-023-01259-xNat Neurosci. 262023</p>
<p>Graph embedding-based link prediction for literature-based discovery in Alzheimer's disease. Y Pu, D Beck, K Verspoor, 10.1016/j.jbi.2023.104464J Biomed Inform. 1451044642023</p>
<p>The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest. D Szklarczyk, R Kirsch, M Koutrouli, 10.1093/nar/gkac1000Nucleic Acids Res. 512023</p>
<p>DTMiner: identification of potential disease targets through biomedical literature mining. D Xu, M Zhang, Y Xie, 10.1093/bioinformatics/btw503201632Bioinformatics</p>
<p>A global network of biomedical relationships derived from text. B Percha, R B Altman, 10.1093/bioinformatics/bty114Bioinformatics. 342018</p>
<p>BioREx: improving biomedical relation extraction by leveraging heterogeneous datasets. P T Lai, C H Wei, L Luo, 10.1016/j.jbi.2023.104487J Biomed Inform. 1461044872023</p>
<p>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge. C H Wei, A Allot, P T Lai, 10.1093/nar/gkae235Nucleic Acids Res. 522024</p>
<p>Neural machine reading comprehension: methods and trends. S Liu, X Zhang, S Zhang, 10.3390/app9183698Appl Sci. 936982019</p>
<p>Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine. P Lee, S Bubeck, J Petro, 10.1056/NEJMsr2214184N Engl J Med. 3882023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>Assessing GPT-4 for cell type annotation in singlecell RNA-seq analysis. W Hou, Ji Z , 10.1038/s41592-024-02235-4Nat Methods. 212024</p>
<p>Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature?. Q Jin, R Leaman, Z Lu, 10.1681/ASN.0000000000000166J Am Soc Nephrol. 342023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, F Petroni, V Karpukhin, Adv Neural Inf Process Syst. 332020</p>
<p>Retrieval-augmented generation for large language models: a survey. Y Gao, Y Xiong, X Gao, 10.48550/arXiv.2312.10997arXiv:2312.109972023arXiv Preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, Adv Neural Inf Process Syst. 352022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Adv Neural Inf Process Syst. 332020</p>
<p>Effectiveness and efficiency of open relation extraction. F Mesquita, J Schmidek, D Barbosa, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational Linguistics2013</p>
<p>UMAP: uniform manifold approximation and projection. L Mcinnes, J Healy, N Saul, 10.21105/joss.00861J Open Source Softw. 38612018</p>
<p>Adapting boosting for information retrieval measures. Q Wu, Cjc Burges, K M Svore, 10.1007/s10791-009-9112-1Inf Retr. 132009</p>
<p>The impact of large language models on scientific discovery: a preliminary study using. M Research Ai4science, Azure Quantum, M , 10.48550/arXiv.2311.07361arXiv:2311.073612023GPT-4. arXiv Preprint</p>
<p>Benchmarking large language models for news summarization. T Zhang, F Ladhak, E Durmus, 10.1162/tacl_a_00632Trans Assoc Comput Linguist. 122024</p>
<p>G Team, T Mesnard, C Hardin, 10.48550/arXiv.2403.08295arXiv:2403.08295open models based on Gemini research and technology. 2024arXiv Preprint</p>
<p>QLoRA: efficient Finetuning of quantized LLMs. T Dettmers, A Pagnoni, A Holtzman, Adv Neural Inf Process Syst. 362023</p>
<p>MEDITRON-70B: scaling medical Pretraining for large language models. Z Chen, A H Cano, A Romanou, 10.48550/arXiv.2311.16079arXiv:2311.160792023arXiv Preprint</p>
<p>Learning to rank using gradient descent. C Burges, T Shaked, E Renshaw, Proceedings of the 22nd international conference on Machine learning -ICML '05. the 22nd international conference on Machine learning -ICML '05New York, NY, USAAssociation for Computing Machinery2005</p>
<p>Cumulated gain-based evaluation of IR techniques. K Järvelin, J Kekäläinen, 10.1145/582415.582418ACM Trans Inf Syst. 202002</p>
<p>Learning to rank with nonsmooth cost functions. C Burges, R Ragno, Q Le, 10.7551/mitpress/7503.003.0029Adv Neural Inf Process Syst. 192006</p>
<p>The unified medical language system (UMLS): integrating biomedical terminology. O Bodenreider, 10.1093/nar/gkh061Nucleic Acids Res. 322004</p>
<p>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup. Author The, 10.1093/bib/bbaf070Briefings in Bioinformatics. 26120252025Oxford University PressThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License</p>
<p>. Problem Solving Protocol. </p>            </div>
        </div>

    </div>
</body>
</html>
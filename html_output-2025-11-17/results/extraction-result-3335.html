<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3335 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3335</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3335</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4a56f72b9c529810ba4ecfe9eac522d87f6db81d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4a56f72b9c529810ba4ecfe9eac522d87f6db81d" target="_blank">Explaining Answers with Entailment Trees</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> ENTAILMENTBANK is created, the first dataset to contain multistep entailment trees, providing a new type of dataset (multistep entails) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</p>
                <p><strong>Paper Abstract:</strong> Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a “rationale”). If this could be done, new opportunities for understanding and debugging the system’s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3335.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3335.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntailmentWriter (T5-11B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EntailmentWriter (T5-based generative entailment-tree model, 11B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-based generative model fine-tuned to produce multistep entailment trees (linearized proofs) from given inputs; trained in an "all-at-once" seq2seq fashion to output leaf identifiers, intermediate conclusions, and step structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (fine-tuned EntailmentWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Built on the text-to-text T5 transformer; inputs are labeled sentences (sentN) and hypothesis (hypot) and the model generates a linearized entailment tree (sent* & sent* -> intX: sentence ... -> hypot). For Task3 a retrieval step supplies 25 retrieved sentences as input. Fine-tuned on EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['All-at-once generative multistep entailment (seq2seq)', 'Retrieval-augmented generation (Task 3)', 'No-context generation (zero-context full-tree generation — variant)', 'Iterative/one-step generation (shredded one-deep model — variant, interactive use)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>All-at-once: the model receives all (or retrieved) context and generates the entire linearized proof in one sequence (identifiers + new intermediate sentences). Retrieval-augmented: for the full-corpus task the system first retrieves 25 sentences from the corpus (RoBERTa-based ranker) and feeds them to the T5 model. No-context variant: model inputs only the QA and generates both leaves and intermediates. One-step/iterative variant: model trained on 'shredded' one-deep trees to generate single-step entailments that can be iteratively expanded by user selection.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses similar generative seq2seq style across settings (all-at-once) but combined with auxiliary methods (retrieval for Task3) and variants (no-context and shredded one-step models). The core approach is a single generative reasoning style rather than a heterogeneous mix of distinct inference algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank explanation tasks (Task1 no-distractor, Task2 distractor, Task3 full-corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate a valid multistep entailment tree proving a hypothesis (declarative QA pair) from (Task1) gold leaf sentences, (Task2) gold leaves plus distractors, or (Task3) a large corpus (with retrieval). Trees are multi-premise entailment steps chained to form the proof.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Test set (T5-11B EntailmentWriter): Task1 (no-distractor): Leaves F1 99.0, Leaves AllCorrect 89.4, Steps F1 51.5, Steps AllCorrect 38.2, Intermediates F1 (BLEURT) 71.2, Intermediates AllCorrect 52.9, Overall AllCorrect 35.6%. Task2 (distractor): Leaves F1 89.1, Leaves AllCorrect 48.8, Steps F1 41.4, Steps AllCorrect 27.7, Intermediates F1 66.2, Intermediates AllCorrect 53.2, Overall AllCorrect 25.6%. Task3 (full-corpus, retrieval+model): Leaves F1 39.7, Leaves AllCorrect 3.8, Steps F1 7.8, Steps AllCorrect 2.9, Intermediates F1 36.4, Intermediates AllCorrect 13.2, Overall AllCorrect 2.9%. Retrieval retrieved on average 66.1% of gold leaf sentences for Task3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparisons across the three tasks show the same generative method performs well when the gold leaves are provided (Task1) but degrades when distractors are present (Task2) and collapses when retrieval is required (Task3). The paper also compares a no-context zero-shot variant run on an external dataset (Challenge300) where ~35% of generated trees in a manual sample were valid, suggesting some OOD transfer for the no-context generation setting. No explicit ablation switching between fundamentally different reasoning algorithms (e.g., symbolic vs neural) is conducted; comparisons are across input-setting (gold leaves vs distractors vs retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A single generative seq2seq model (EntailmentWriter) can produce correct multistep entailment trees frequently when provided the necessary raw facts (Task1: ~36% perfect trees; human evaluation suggests performance may be ~20% higher due to valid alternative structures). Performance degrades with distractors and especially with retrieval from a large corpus. The dataset requires diverse forms of reasoning (substitution, rule application, conjunction, class-inference, property inheritance, sequential inference), and a single all-at-once generative approach struggles on larger proofs (performance drops rapidly with number of steps). Iterative/one-step generation is proposed and explored as a promising alternative for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Failure modes include high rates of repeated copying of input in intermediates (41% of invalid steps), invalid entailments that rely on facts not present in that step (47%), and mis-evaluation due to valid but differently-structured proofs. For Task3, retrieval failures (only ~66.1% gold leaves retrieved) and the short token input window lead to very low overall success (Overall AllCorrect 2.9%). Scores drop sharply as gold proof length increases (Table A3: Overall AllCorrect approaches 0% for proofs with ≥3-5 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3335.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-large baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-large EntailmentWriter baseline (T5-large fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller T5 checkpoint fine-tuned in the same manner as EntailmentWriter to establish a baseline; performs worse than the 11B model but follows the same all-at-once generation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (fine-tuned baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-large (text-to-text transformer) fine-tuned on the same EntailmentBank training data and tasks, using the same linearized tree encoding and training procedure as the main EntailmentWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['All-at-once generative multistep entailment (seq2seq)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same linearized proof generation approach as the 11B EntailmentWriter but with the T5-large checkpoint and different training schedule/hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single generative style (same as T5-11B) — no additional reasoning styles applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank tasks (Task1/Task2/Task3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multistep entailment tree generation tasks used for the main model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Test set (T5-large): Task1: Leaves F1 98.7, Leaves AllCorrect 86.2, Steps F1 50.5, Steps AllCorrect 37.7, Intermediates F1 67.6, Intermediates AllCorrect 50.3, Overall AllCorrect 34.4%. Task2: Leaves F1 84.3, Leaves AllCorrect 38.5, Steps F1 35.7, Steps AllCorrect 23.5, Intermediates F1 62.6, Intermediates AllCorrect 50.9, Overall AllCorrect 22.4%. Task3: Leaves F1 35.2, Leaves AllCorrect 2.9, Steps F1 6.2, Steps AllCorrect 2.4, Intermediates F1 33.0, Intermediates AllCorrect 13.2, Overall AllCorrect 2.4%. (See Appendix E / Table A4.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to the 11B model, T5-large shows lower but broadly similar trends: high leaf selection performance when gold leaves are provided, degraded step/intermediate correctness with distractors, and a collapse in full-corpus task performance due to retrieval and input-size limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller checkpoint obtains qualitatively similar behavior but lower absolute performance; confirms scaling benefits (T5-11B outperforms T5-large across metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Same failure modes as larger model; absolute performance notably lower, especially on Steps and Overall AllCorrect metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3335.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-based retriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-based relevant-sentence retriever (used for Task3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa classifier/ranker used to retrieve candidate supporting sentences from the large WorldTree corpus for the full-corpus Task3; top 25 retrieved sentences are fed to the generative model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (relevance ranker)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A RoBERTa-trained relevant-sentence classifier used to score every fact in the corpus; its top candidates are reranked (also via TensorFlow-Ranking-BERT) and the top 25 are provided to the T5-based entailment generator for Task3.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Retrieval-augmented reasoning (IR + generative proof construction)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Retrieval system ranks corpus sentences for relevance to the hypothesis (QA) using a RoBERTa classifier; the retrieved set forms the context for the generative entailment model. The retrieval is critical because the full corpus cannot be directly input to T5.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Combines retrieval plus generative reasoning — a hybrid approach but still centered on generation for inference. The retriever is an auxiliary component rather than a different reasoning style inside the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Task3 (full-corpus entailment tree generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Find relevant facts in a large corpus to enable multistep proof generation; then produce entailment tree from the retrieved facts and hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Retrieval retrieved on average 66.1% of the gold leaf sentences for Task3; low retrieval coverage is a major contributor to Task3's poor end-to-end performance (Overall AllCorrect 2.9% with T5-11B).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper contrasts Task2 (gold leaves + distractors) versus Task3 (retrieval + generation) to show that retrieval quality is as critical as the generator; poor retrieval coverage substantially reduces final proof correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval quality is a bottleneck: even a strong generative entailment model fails on full-corpus tasks if the retriever does not surface necessary leaf sentences. Retrieval+generation hybrid is necessary when corpus size exceeds model context window.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with a learned RoBERTa ranker and reranking pipeline, retrieval finds only ~2/3 of required leaves on average, causing sharp drops in downstream proof metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3335.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-context EntailmentWriter (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No-context EntailmentWriter (QA-only full-tree generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant trained to input only a QA pair and generate an entire entailment tree (both leaves and intermediates) without retrieving corpus sentences; evaluated zero-shot on external datasets for OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-based no-context EntailmentWriter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned to generate full explanation trees from just QA pairs (no supplied leaves); intended to synthesize both supporting sentences and intermediate conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Generative synthesis of facts + multistep entailment (zero-context generation)', 'Zero-shot OOD transfer evaluation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model learns to generate leaf facts and intermediate conclusions conditioned only on QA; effectively performs knowledge synthesis and chaining in one pass.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single generative style (synthesis) without retrieval; this encourages generation of diverse supporting facts but still uses the same seq2seq strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Zero-context explanation generation (evaluated on Challenge300 and eQASC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Produce full explanation trees from QA alone, tested zero-shot on out-of-domain datasets to evaluate transfer and generality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Manual evaluation on a random sample from Challenge300: ~35% of generated trees were valid, non-vacuous; ~25% of remainder were valid but largely repeated QA. On eQASC (one-step entailments) Task2 EntailmentWriter (without fine-tuning) achieved Leaves F1 67% and Overall AllCorrect 26% (indication of OOD transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>No-context generation shows that the same generative model can synthesize supporting facts, achieving nontrivial OOD transfer (~35% valid in manual sample), but this approach risks generating unsupported or false facts and lacks the grounding retrieval provides.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generating full proofs from QA alone is possible to some extent and yields useful zero-shot outputs, but reliability is limited; combining generation with verification could improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Generated facts can be false or nonsensical; manual validation is needed. Reliability lower than when gold facts are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3335.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative one-deep EntailmentWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative (shredded) one-step EntailmentWriter for interactive explanation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model variant trained on one-deep (single-step) entailments derived by 'shredding' full trees; intended for interactive iterative proof construction, generating one entailment step at a time for user-driven expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-based one-step EntailmentWriter (iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Trained on shallow one-depth trees where intermediate nodes become new hypotheses; used interactively to expand a selected premise by producing its immediate children; allows stepwise generation and potential validation between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Iterative step-by-step generation', 'Interactive, user-guided drill-down reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model produces a single entailment step (multi-premise -> conclusion) per call; a user or controller selects which newly generated premises to expand, enabling recursive generation and potential validation at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Represents a shift from all-at-once generation to an iterative style; mixes generation with the potential for verification or user guidance, thus increasing methodological diversity compared to the single-pass model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Interactive iterative explanation generation (derived from EntailmentBank)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate one-deep entailments to iteratively build a full proof tree under user control, allowing checks and focused expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>No large-scale numeric evaluation reported for this variant; authors report it as a promising direction and discuss how verification could be applied between steps. They note generative models sometimes produce false facts and suggest fact-verification techniques as mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Proposed as an alternative to all-at-once generation to address specific failure modes (e.g., invalid multi-premise steps, use of facts not present in step). No direct side-by-side quantitative comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative one-step generation may reduce step-level invalid entailments and enable validation between steps, but requires further development and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Authors note generative one-step outputs can still produce false or nonsensical facts; no quantitative evidence provided that iterative approach outperforms all-at-once model in practice yet.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3335.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProofWriter and related deductive-transformer work</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProofWriter, PRover, and transformer-based deductive proof systems (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior systems showing transformers can generate formal or natural-language proofs: ProofWriter (generating implications and proofs), PRover (proof generation over rules), and transformer theorem-proving methods (Polu & Sutskever; Wang & Deng). These works motivated the all-at-once generative approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProofWriter: Generating implications, proofs, and abductive statements over natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProofWriter / PRover / transformer theorem-proving models (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Related work where transformer models were used to generate formal proofs or natural-language proofs from rules; ProofWriter introduced an 'All-at-once' sequence-to-sequence technique for proof generation that inspired EntailmentWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['All-at-once proof generation', 'Rule-based deductive reasoning with transformer generation', 'Formal theorem-proving with generative language models']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>These systems train transformers to generate chains of reasoning or formal proof steps, sometimes in an all-at-once seq2seq manner or by learning to generate intermediate lemmas, demonstrating that transformers can produce reliable formal proofs in constrained settings.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Mentioned works largely explore deductive/formal proof generation (a particular reasoning style) and sometimes compare all-at-once vs iterative approaches; they are referenced as related approaches rather than contrasted comprehensively in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Formal and natural-language proof generation (related benchmarks and tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Produce proofs or chains of logical consequences from a set of rules or premises; prior datasets and tasks are typically formalized or rule-based.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Not reported in this paper beyond citation; these works in the literature report high reliability in formal settings but are not quantitatively compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper cites these works as inspiration and notes similarities (e.g., all-at-once seq2seq). No direct experimental comparison is made within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformer-based proof generation is a viable approach in formal or rule-rich domains and inspired the generative entailment-tree approach used in EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>These prior systems operate in different (often more formal) problem regimes; the paper stresses that open-domain textual entailment trees present additional challenges (diverse reasoning types, free-text premises) not fully addressed by formal-proof transformer methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3335.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRover (Saha et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRover: Proof generation for interpretable reasoning over rules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based system for proof generation over given rules; mentioned in related work as another example of transformer proof generation in constrained rule-based settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PRover: Proof generation for interpretable reasoning over rules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PRover</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PRover is a model trained to generate natural-language proofs over rule sets; cited as related work demonstrating proof generation capabilities of transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Rule-based proof generation', 'Transformer sequence-to-sequence proof synthesis']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generates proof steps that conform to provided rules; focuses on interpretable reasoning over rule sets rather than open-ended textual entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Represents a rule-focused deductive reasoning style, different from EntailmentBank's open-text multi-premise entailments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof generation over rules (related tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Produce interpretable proofs using given rules as premises.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Mentioned to contextualize transformer-based proof generation successes; no experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transforms can learn to produce proofs over rules; motivates exploring similar generative techniques for natural-language entailment trees.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Rule-based settings are narrower than EntailmentBank's open-domain entailments; applicability may be limited without additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3335.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polu & Sutskever (theorem proving)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative language modeling for automated theorem proving (Polu & Sutskever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating that generative language models can be effective at formal theorem proving; cited as prior evidence that transformers can generate formal proofs reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative language modeling for automated theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generative language models for theorem proving (Polu & Sutskever)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies large generative language models to a formal theorem-proving setting, achieving strong results in generating proof steps in formal languages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Formal theorem proving via generative language models']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Train generative models to produce formal proof steps in theorem-proving datasets; typically operates in constrained formal languages with explicit rules.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Focuses on a single formal deductive reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Automated theorem proving (formal tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate formal proofs for theorems in symbolic/formal languages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Referenced as motivating evidence for transformer proof generation; no direct comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformers can generate correct formal proofs with high reliability in constrained settings.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Formal theorem proving differs from open-text multi-premise entailment; success in formal domains does not guarantee success on EntailmentBank's free-text reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3335.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3335.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang & Deng (theorem generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to prove theorems by learning to generate theorems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work referenced for learning to generate and prove theorems; included in related work as an approach demonstrating transformer capabilities on structured reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to prove theorems by learning to generate theorems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer theorem-proving approaches (Wang & Deng)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Explores training models to generate theorem statements and proofs, showing learned generation for structured reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Generative theorem generation and proving']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model learns to propose theorem statements and corresponding proofs in a formalized setting; relevant as background for generative proof methods.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Focused on formal deductive generation, not multiple open-text reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Formal theorem proving/generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate and prove formal theorems; used here as related methodological precedent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Cited for context only; no experimental comparison with EntailmentWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adds to evidence that generative models can be trained for structured reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative results</strong></td>
                            <td>Not directly evaluated in this paper; formal setting differences apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Answers with Entailment Trees', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>PRover: Proof generation for interpretable reasoning over rules <em>(Rating: 2)</em></li>
                <li>Generative language modeling for automated theorem proving <em>(Rating: 2)</em></li>
                <li>Learning to prove theorems by learning to generate theorems <em>(Rating: 2)</em></li>
                <li>PRoBERTa / RoBERTa: A robustly optimized BERT pretraining approach <em>(Rating: 1)</em></li>
                <li>WorldTree V2: A corpus of science-domain structured explanations and inference patterns supporting multi-hop inference <em>(Rating: 1)</em></li>
                <li>Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3335",
    "paper_id": "paper-4a56f72b9c529810ba4ecfe9eac522d87f6db81d",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "EntailmentWriter (T5-11B)",
            "name_full": "EntailmentWriter (T5-based generative entailment-tree model, 11B parameters)",
            "brief_description": "A T5-based generative model fine-tuned to produce multistep entailment trees (linearized proofs) from given inputs; trained in an \"all-at-once\" seq2seq fashion to output leaf identifiers, intermediate conclusions, and step structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (fine-tuned EntailmentWriter)",
            "model_description": "Built on the text-to-text T5 transformer; inputs are labeled sentences (sentN) and hypothesis (hypot) and the model generates a linearized entailment tree (sent* & sent* -&gt; intX: sentence ... -&gt; hypot). For Task3 a retrieval step supplies 25 retrieved sentences as input. Fine-tuned on EntailmentBank.",
            "model_size": "11B",
            "reasoning_methods": [
                "All-at-once generative multistep entailment (seq2seq)",
                "Retrieval-augmented generation (Task 3)",
                "No-context generation (zero-context full-tree generation — variant)",
                "Iterative/one-step generation (shredded one-deep model — variant, interactive use)"
            ],
            "reasoning_methods_description": "All-at-once: the model receives all (or retrieved) context and generates the entire linearized proof in one sequence (identifiers + new intermediate sentences). Retrieval-augmented: for the full-corpus task the system first retrieves 25 sentences from the corpus (RoBERTa-based ranker) and feeds them to the T5 model. No-context variant: model inputs only the QA and generates both leaves and intermediates. One-step/iterative variant: model trained on 'shredded' one-deep trees to generate single-step entailments that can be iteratively expanded by user selection.",
            "diversity_of_methods": "Uses similar generative seq2seq style across settings (all-at-once) but combined with auxiliary methods (retrieval for Task3) and variants (no-context and shredded one-step models). The core approach is a single generative reasoning style rather than a heterogeneous mix of distinct inference algorithms.",
            "reasoning_task_name": "EntailmentBank explanation tasks (Task1 no-distractor, Task2 distractor, Task3 full-corpus)",
            "reasoning_task_description": "Generate a valid multistep entailment tree proving a hypothesis (declarative QA pair) from (Task1) gold leaf sentences, (Task2) gold leaves plus distractors, or (Task3) a large corpus (with retrieval). Trees are multi-premise entailment steps chained to form the proof.",
            "performance_by_method": "Test set (T5-11B EntailmentWriter): Task1 (no-distractor): Leaves F1 99.0, Leaves AllCorrect 89.4, Steps F1 51.5, Steps AllCorrect 38.2, Intermediates F1 (BLEURT) 71.2, Intermediates AllCorrect 52.9, Overall AllCorrect 35.6%. Task2 (distractor): Leaves F1 89.1, Leaves AllCorrect 48.8, Steps F1 41.4, Steps AllCorrect 27.7, Intermediates F1 66.2, Intermediates AllCorrect 53.2, Overall AllCorrect 25.6%. Task3 (full-corpus, retrieval+model): Leaves F1 39.7, Leaves AllCorrect 3.8, Steps F1 7.8, Steps AllCorrect 2.9, Intermediates F1 36.4, Intermediates AllCorrect 13.2, Overall AllCorrect 2.9%. Retrieval retrieved on average 66.1% of gold leaf sentences for Task3.",
            "comparison_of_methods": "Direct comparisons across the three tasks show the same generative method performs well when the gold leaves are provided (Task1) but degrades when distractors are present (Task2) and collapses when retrieval is required (Task3). The paper also compares a no-context zero-shot variant run on an external dataset (Challenge300) where ~35% of generated trees in a manual sample were valid, suggesting some OOD transfer for the no-context generation setting. No explicit ablation switching between fundamentally different reasoning algorithms (e.g., symbolic vs neural) is conducted; comparisons are across input-setting (gold leaves vs distractors vs retrieval).",
            "key_findings": "A single generative seq2seq model (EntailmentWriter) can produce correct multistep entailment trees frequently when provided the necessary raw facts (Task1: ~36% perfect trees; human evaluation suggests performance may be ~20% higher due to valid alternative structures). Performance degrades with distractors and especially with retrieval from a large corpus. The dataset requires diverse forms of reasoning (substitution, rule application, conjunction, class-inference, property inheritance, sequential inference), and a single all-at-once generative approach struggles on larger proofs (performance drops rapidly with number of steps). Iterative/one-step generation is proposed and explored as a promising alternative for future work.",
            "counter_examples_or_negative_results": "Failure modes include high rates of repeated copying of input in intermediates (41% of invalid steps), invalid entailments that rely on facts not present in that step (47%), and mis-evaluation due to valid but differently-structured proofs. For Task3, retrieval failures (only ~66.1% gold leaves retrieved) and the short token input window lead to very low overall success (Overall AllCorrect 2.9%). Scores drop sharply as gold proof length increases (Table A3: Overall AllCorrect approaches 0% for proofs with ≥3-5 steps).",
            "uuid": "e3335.0",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "T5-large baseline",
            "name_full": "T5-large EntailmentWriter baseline (T5-large fine-tuned)",
            "brief_description": "A smaller T5 checkpoint fine-tuned in the same manner as EntailmentWriter to establish a baseline; performs worse than the 11B model but follows the same all-at-once generation approach.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-large (fine-tuned baseline)",
            "model_description": "T5-large (text-to-text transformer) fine-tuned on the same EntailmentBank training data and tasks, using the same linearized tree encoding and training procedure as the main EntailmentWriter.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "All-at-once generative multistep entailment (seq2seq)"
            ],
            "reasoning_methods_description": "Same linearized proof generation approach as the 11B EntailmentWriter but with the T5-large checkpoint and different training schedule/hyperparameters.",
            "diversity_of_methods": "Single generative style (same as T5-11B) — no additional reasoning styles applied.",
            "reasoning_task_name": "EntailmentBank tasks (Task1/Task2/Task3)",
            "reasoning_task_description": "Same multistep entailment tree generation tasks used for the main model.",
            "performance_by_method": "Test set (T5-large): Task1: Leaves F1 98.7, Leaves AllCorrect 86.2, Steps F1 50.5, Steps AllCorrect 37.7, Intermediates F1 67.6, Intermediates AllCorrect 50.3, Overall AllCorrect 34.4%. Task2: Leaves F1 84.3, Leaves AllCorrect 38.5, Steps F1 35.7, Steps AllCorrect 23.5, Intermediates F1 62.6, Intermediates AllCorrect 50.9, Overall AllCorrect 22.4%. Task3: Leaves F1 35.2, Leaves AllCorrect 2.9, Steps F1 6.2, Steps AllCorrect 2.4, Intermediates F1 33.0, Intermediates AllCorrect 13.2, Overall AllCorrect 2.4%. (See Appendix E / Table A4.)",
            "comparison_of_methods": "Compared to the 11B model, T5-large shows lower but broadly similar trends: high leaf selection performance when gold leaves are provided, degraded step/intermediate correctness with distractors, and a collapse in full-corpus task performance due to retrieval and input-size limitations.",
            "key_findings": "Smaller checkpoint obtains qualitatively similar behavior but lower absolute performance; confirms scaling benefits (T5-11B outperforms T5-large across metrics).",
            "counter_examples_or_negative_results": "Same failure modes as larger model; absolute performance notably lower, especially on Steps and Overall AllCorrect metrics.",
            "uuid": "e3335.1",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "RoBERTa-based retriever",
            "name_full": "RoBERTa-based relevant-sentence retriever (used for Task3)",
            "brief_description": "A RoBERTa classifier/ranker used to retrieve candidate supporting sentences from the large WorldTree corpus for the full-corpus Task3; top 25 retrieved sentences are fed to the generative model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa (relevance ranker)",
            "model_description": "A RoBERTa-trained relevant-sentence classifier used to score every fact in the corpus; its top candidates are reranked (also via TensorFlow-Ranking-BERT) and the top 25 are provided to the T5-based entailment generator for Task3.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "Retrieval-augmented reasoning (IR + generative proof construction)"
            ],
            "reasoning_methods_description": "Retrieval system ranks corpus sentences for relevance to the hypothesis (QA) using a RoBERTa classifier; the retrieved set forms the context for the generative entailment model. The retrieval is critical because the full corpus cannot be directly input to T5.",
            "diversity_of_methods": "Combines retrieval plus generative reasoning — a hybrid approach but still centered on generation for inference. The retriever is an auxiliary component rather than a different reasoning style inside the generator.",
            "reasoning_task_name": "Task3 (full-corpus entailment tree generation)",
            "reasoning_task_description": "Find relevant facts in a large corpus to enable multistep proof generation; then produce entailment tree from the retrieved facts and hypothesis.",
            "performance_by_method": "Retrieval retrieved on average 66.1% of the gold leaf sentences for Task3; low retrieval coverage is a major contributor to Task3's poor end-to-end performance (Overall AllCorrect 2.9% with T5-11B).",
            "comparison_of_methods": "The paper contrasts Task2 (gold leaves + distractors) versus Task3 (retrieval + generation) to show that retrieval quality is as critical as the generator; poor retrieval coverage substantially reduces final proof correctness.",
            "key_findings": "Retrieval quality is a bottleneck: even a strong generative entailment model fails on full-corpus tasks if the retriever does not surface necessary leaf sentences. Retrieval+generation hybrid is necessary when corpus size exceeds model context window.",
            "counter_examples_or_negative_results": "Even with a learned RoBERTa ranker and reranking pipeline, retrieval finds only ~2/3 of required leaves on average, causing sharp drops in downstream proof metrics.",
            "uuid": "e3335.2",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "No-context EntailmentWriter (zero-shot)",
            "name_full": "No-context EntailmentWriter (QA-only full-tree generation)",
            "brief_description": "A variant trained to input only a QA pair and generate an entire entailment tree (both leaves and intermediates) without retrieving corpus sentences; evaluated zero-shot on external datasets for OOD generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-based no-context EntailmentWriter",
            "model_description": "Fine-tuned to generate full explanation trees from just QA pairs (no supplied leaves); intended to synthesize both supporting sentences and intermediate conclusions.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "Generative synthesis of facts + multistep entailment (zero-context generation)",
                "Zero-shot OOD transfer evaluation"
            ],
            "reasoning_methods_description": "Model learns to generate leaf facts and intermediate conclusions conditioned only on QA; effectively performs knowledge synthesis and chaining in one pass.",
            "diversity_of_methods": "Single generative style (synthesis) without retrieval; this encourages generation of diverse supporting facts but still uses the same seq2seq strategy.",
            "reasoning_task_name": "Zero-context explanation generation (evaluated on Challenge300 and eQASC)",
            "reasoning_task_description": "Produce full explanation trees from QA alone, tested zero-shot on out-of-domain datasets to evaluate transfer and generality.",
            "performance_by_method": "Manual evaluation on a random sample from Challenge300: ~35% of generated trees were valid, non-vacuous; ~25% of remainder were valid but largely repeated QA. On eQASC (one-step entailments) Task2 EntailmentWriter (without fine-tuning) achieved Leaves F1 67% and Overall AllCorrect 26% (indication of OOD transfer).",
            "comparison_of_methods": "No-context generation shows that the same generative model can synthesize supporting facts, achieving nontrivial OOD transfer (~35% valid in manual sample), but this approach risks generating unsupported or false facts and lacks the grounding retrieval provides.",
            "key_findings": "Generating full proofs from QA alone is possible to some extent and yields useful zero-shot outputs, but reliability is limited; combining generation with verification could improve results.",
            "counter_examples_or_negative_results": "Generated facts can be false or nonsensical; manual validation is needed. Reliability lower than when gold facts are provided.",
            "uuid": "e3335.3",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Iterative one-deep EntailmentWriter",
            "name_full": "Iterative (shredded) one-step EntailmentWriter for interactive explanation",
            "brief_description": "A model variant trained on one-deep (single-step) entailments derived by 'shredding' full trees; intended for interactive iterative proof construction, generating one entailment step at a time for user-driven expansion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-based one-step EntailmentWriter (iterative)",
            "model_description": "Trained on shallow one-depth trees where intermediate nodes become new hypotheses; used interactively to expand a selected premise by producing its immediate children; allows stepwise generation and potential validation between steps.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "Iterative step-by-step generation",
                "Interactive, user-guided drill-down reasoning"
            ],
            "reasoning_methods_description": "Model produces a single entailment step (multi-premise -&gt; conclusion) per call; a user or controller selects which newly generated premises to expand, enabling recursive generation and potential validation at each step.",
            "diversity_of_methods": "Represents a shift from all-at-once generation to an iterative style; mixes generation with the potential for verification or user guidance, thus increasing methodological diversity compared to the single-pass model.",
            "reasoning_task_name": "Interactive iterative explanation generation (derived from EntailmentBank)",
            "reasoning_task_description": "Generate one-deep entailments to iteratively build a full proof tree under user control, allowing checks and focused expansion.",
            "performance_by_method": "No large-scale numeric evaluation reported for this variant; authors report it as a promising direction and discuss how verification could be applied between steps. They note generative models sometimes produce false facts and suggest fact-verification techniques as mitigation.",
            "comparison_of_methods": "Proposed as an alternative to all-at-once generation to address specific failure modes (e.g., invalid multi-premise steps, use of facts not present in step). No direct side-by-side quantitative comparison reported.",
            "key_findings": "Iterative one-step generation may reduce step-level invalid entailments and enable validation between steps, but requires further development and evaluation.",
            "counter_examples_or_negative_results": "Authors note generative one-step outputs can still produce false or nonsensical facts; no quantitative evidence provided that iterative approach outperforms all-at-once model in practice yet.",
            "uuid": "e3335.4",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ProofWriter and related deductive-transformer work",
            "name_full": "ProofWriter, PRover, and transformer-based deductive proof systems (mentioned in related work)",
            "brief_description": "Prior systems showing transformers can generate formal or natural-language proofs: ProofWriter (generating implications and proofs), PRover (proof generation over rules), and transformer theorem-proving methods (Polu & Sutskever; Wang & Deng). These works motivated the all-at-once generative approach.",
            "citation_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "mention_or_use": "mention",
            "model_name": "ProofWriter / PRover / transformer theorem-proving models (mentioned)",
            "model_description": "Related work where transformer models were used to generate formal proofs or natural-language proofs from rules; ProofWriter introduced an 'All-at-once' sequence-to-sequence technique for proof generation that inspired EntailmentWriter.",
            "model_size": "not specified in this paper",
            "reasoning_methods": [
                "All-at-once proof generation",
                "Rule-based deductive reasoning with transformer generation",
                "Formal theorem-proving with generative language models"
            ],
            "reasoning_methods_description": "These systems train transformers to generate chains of reasoning or formal proof steps, sometimes in an all-at-once seq2seq manner or by learning to generate intermediate lemmas, demonstrating that transformers can produce reliable formal proofs in constrained settings.",
            "diversity_of_methods": "Mentioned works largely explore deductive/formal proof generation (a particular reasoning style) and sometimes compare all-at-once vs iterative approaches; they are referenced as related approaches rather than contrasted comprehensively in this paper.",
            "reasoning_task_name": "Formal and natural-language proof generation (related benchmarks and tasks)",
            "reasoning_task_description": "Produce proofs or chains of logical consequences from a set of rules or premises; prior datasets and tasks are typically formalized or rule-based.",
            "performance_by_method": "Not reported in this paper beyond citation; these works in the literature report high reliability in formal settings but are not quantitatively compared here.",
            "comparison_of_methods": "The paper cites these works as inspiration and notes similarities (e.g., all-at-once seq2seq). No direct experimental comparison is made within this paper.",
            "key_findings": "Transformer-based proof generation is a viable approach in formal or rule-rich domains and inspired the generative entailment-tree approach used in EntailmentBank.",
            "counter_examples_or_negative_results": "These prior systems operate in different (often more formal) problem regimes; the paper stresses that open-domain textual entailment trees present additional challenges (diverse reasoning types, free-text premises) not fully addressed by formal-proof transformer methods.",
            "uuid": "e3335.5",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "PRover (Saha et al.)",
            "name_full": "PRover: Proof generation for interpretable reasoning over rules",
            "brief_description": "A transformer-based system for proof generation over given rules; mentioned in related work as another example of transformer proof generation in constrained rule-based settings.",
            "citation_title": "PRover: Proof generation for interpretable reasoning over rules",
            "mention_or_use": "mention",
            "model_name": "PRover",
            "model_description": "PRover is a model trained to generate natural-language proofs over rule sets; cited as related work demonstrating proof generation capabilities of transformers.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "Rule-based proof generation",
                "Transformer sequence-to-sequence proof synthesis"
            ],
            "reasoning_methods_description": "Generates proof steps that conform to provided rules; focuses on interpretable reasoning over rule sets rather than open-ended textual entailment.",
            "diversity_of_methods": "Represents a rule-focused deductive reasoning style, different from EntailmentBank's open-text multi-premise entailments.",
            "reasoning_task_name": "Proof generation over rules (related tasks)",
            "reasoning_task_description": "Produce interpretable proofs using given rules as premises.",
            "performance_by_method": "Not reported in this paper.",
            "comparison_of_methods": "Mentioned to contextualize transformer-based proof generation successes; no experimental comparison in this paper.",
            "key_findings": "Transforms can learn to produce proofs over rules; motivates exploring similar generative techniques for natural-language entailment trees.",
            "counter_examples_or_negative_results": "Rule-based settings are narrower than EntailmentBank's open-domain entailments; applicability may be limited without additional mechanisms.",
            "uuid": "e3335.6",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Polu & Sutskever (theorem proving)",
            "name_full": "Generative language modeling for automated theorem proving (Polu & Sutskever)",
            "brief_description": "Work demonstrating that generative language models can be effective at formal theorem proving; cited as prior evidence that transformers can generate formal proofs reliably.",
            "citation_title": "Generative language modeling for automated theorem proving",
            "mention_or_use": "mention",
            "model_name": "Generative language models for theorem proving (Polu & Sutskever)",
            "model_description": "Applies large generative language models to a formal theorem-proving setting, achieving strong results in generating proof steps in formal languages.",
            "model_size": "not specified in this paper",
            "reasoning_methods": [
                "Formal theorem proving via generative language models"
            ],
            "reasoning_methods_description": "Train generative models to produce formal proof steps in theorem-proving datasets; typically operates in constrained formal languages with explicit rules.",
            "diversity_of_methods": "Focuses on a single formal deductive reasoning style.",
            "reasoning_task_name": "Automated theorem proving (formal tasks)",
            "reasoning_task_description": "Generate formal proofs for theorems in symbolic/formal languages.",
            "performance_by_method": "Not reported in this paper.",
            "comparison_of_methods": "Referenced as motivating evidence for transformer proof generation; no direct comparison in this paper.",
            "key_findings": "Transformers can generate correct formal proofs with high reliability in constrained settings.",
            "counter_examples_or_negative_results": "Formal theorem proving differs from open-text multi-premise entailment; success in formal domains does not guarantee success on EntailmentBank's free-text reasoning.",
            "uuid": "e3335.7",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Wang & Deng (theorem generation)",
            "name_full": "Learning to prove theorems by learning to generate theorems",
            "brief_description": "Work referenced for learning to generate and prove theorems; included in related work as an approach demonstrating transformer capabilities on structured reasoning tasks.",
            "citation_title": "Learning to prove theorems by learning to generate theorems",
            "mention_or_use": "mention",
            "model_name": "Transformer theorem-proving approaches (Wang & Deng)",
            "model_description": "Explores training models to generate theorem statements and proofs, showing learned generation for structured reasoning tasks.",
            "model_size": "not specified in paper",
            "reasoning_methods": [
                "Generative theorem generation and proving"
            ],
            "reasoning_methods_description": "Model learns to propose theorem statements and corresponding proofs in a formalized setting; relevant as background for generative proof methods.",
            "diversity_of_methods": "Focused on formal deductive generation, not multiple open-text reasoning styles.",
            "reasoning_task_name": "Formal theorem proving/generation",
            "reasoning_task_description": "Generate and prove formal theorems; used here as related methodological precedent.",
            "performance_by_method": "Not reported in this paper.",
            "comparison_of_methods": "Cited for context only; no experimental comparison with EntailmentWriter.",
            "key_findings": "Adds to evidence that generative models can be trained for structured reasoning tasks.",
            "counter_examples_or_negative results": "Not directly evaluated in this paper; formal setting differences apply.",
            "uuid": "e3335.8",
            "source_info": {
                "paper_title": "Explaining Answers with Entailment Trees",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "PRover: Proof generation for interpretable reasoning over rules",
            "rating": 2
        },
        {
            "paper_title": "Generative language modeling for automated theorem proving",
            "rating": 2
        },
        {
            "paper_title": "Learning to prove theorems by learning to generate theorems",
            "rating": 2
        },
        {
            "paper_title": "PRoBERTa / RoBERTa: A robustly optimized BERT pretraining approach",
            "rating": 1
        },
        {
            "paper_title": "WorldTree V2: A corpus of science-domain structured explanations and inference patterns supporting multi-hop inference",
            "rating": 1
        },
        {
            "paper_title": "Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering",
            "rating": 1
        }
    ],
    "cost": 0.0193215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Explaining Answers with Entailment Trees</h1>
<p>Bhavana Dalvi ${ }^{<em> 1}$, Peter Jansen ${ }^{</em> 2}$, Oyvind Tafjord ${ }^{1}$, Zhengnan Xie ${ }^{2}$, Hannah Smith ${ }^{2}$, Leighanna Pipatanangkura ${ }^{2}$, Peter Clark ${ }^{1}$<br>${ }^{1}$ Allen Institute for AI, Seattle, WA<br>${ }^{2}$ University of Arizona, Tucson, AZ<br>* equal contribution<br>bhavanad@allenai.org, pajansen@arizona.edu</p>
<h4>Abstract</h4>
<p>Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a "rationale"). If this could be done, new opportunities for understanding and debugging the system's reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created EntailmentBank ${ }^{1}$, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., $35 \%$ of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</p>
<h2>1 Introduction</h2>
<p>Explanation remains a formidable challenge in AI. While today's explanation systems are good at providing a sentence or two of supporting evidence ("rationales") for an answer (DeYoung et al., 2019), they rarely explain the chain of reasoning from what is known to the answer, i.e., how the answer follows, given the evidence - the goal of this work.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given a hypothesis (green, summarizing a question+answer pair), and some partially relevant text (or a corpus), our goal is to generate an entailment tree, including intermediate nodes (blue), showing how the hypothesis follows from the text/corpus.</p>
<p>Without this, it is hard to fully understand a system's response and/or pinpoint the source of errors if its conclusions are wrong. Conversely, if a system could support its answers with a chain of reasoning, new opportunities arise for interactively teaching the machine by debugging its mistakes.</p>
<p>Our approach is to generate explanations in the form of multistep entailment trees, such as shown in Figure 1, made up of individual, multi-premise textual entailment (TE) steps (Dagan et al., 2013; Lai et al., 2017). Although there are many singlestep entailment datasets available (Bentivogli et al., 2011; Bowman et al., 2015) no dataset of multistep entailments exists, and so a significant contribution of this paper is the construction of such a dataset, called EntailmentBank. EntailmentBank contains 1,840 multistep entailment trees for accompanying QA pairs, constructed using expert annotators, and is the first dataset of its kind. We also define three explanation tasks over this dataset, namely: generate a valid entailment tree for a given</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Property $\downarrow$, Dataset $\rightarrow$</th>
<th style="text-align: center;">WorldTree V2 ${ }^{1}$</th>
<th style="text-align: center;">eQASC $^{2}$</th>
<th style="text-align: center;">HotpotQA ${ }^{3}$, R4C $^{4}$</th>
<th style="text-align: center;">StrategyQA ${ }^{5}$</th>
<th style="text-align: center;">ENtAILmentBank</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Semantics of Inference</td>
<td style="text-align: center;">(informal)</td>
<td style="text-align: center;">1-Step Entailment</td>
<td style="text-align: center;">(informal)</td>
<td style="text-align: center;">Deduction</td>
<td style="text-align: center;">Entailment Tree</td>
</tr>
<tr>
<td style="text-align: left;">Average Facts per Inference</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: left;">Average Edges per Inference</td>
<td style="text-align: center;">$9^{2}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2^{2}$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Granularity of Inference</td>
<td style="text-align: center;">Fine</td>
<td style="text-align: center;">Coarse</td>
<td style="text-align: center;">Coarse</td>
<td style="text-align: center;">Coarse</td>
<td style="text-align: center;">Fine</td>
</tr>
<tr>
<td style="text-align: left;">Explicit Ordering of Inference</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">Authoring Method</td>
<td style="text-align: center;">Expert</td>
<td style="text-align: center;">Crowd</td>
<td style="text-align: center;">Crowd</td>
<td style="text-align: center;">Crowd</td>
<td style="text-align: center;">Expert</td>
</tr>
</tbody>
</table>
<p>${ }^{1}$ (Xie et al., 2020) ${ }^{2}$ (Jhamtani and Clark, 2020) ${ }^{3}$ (Yang et al., 2018) ${ }^{4}$ (Inoue et al., 2020) ${ }^{5}$ (Geva et al., 2021)
Table 1: A comparison of EntailmentBank with other similar datasets. In general, EntailmentBank contains larger inference problems, at a finer level of granularity than existing datasets, while being the only dataset to include multi-step entailments that make the reasoning steps explicit. ${ }^{1}$ WT2 and R4C explanations are implied (unannotated) graphs based on overlapping words or entities - values here are inferred by constructing graphs based on lexical overlap.</p>
<p>QA pair given (a) all relevant sentences (the leaves of the gold entailment tree), (b) all relevant and some distractor sentences, or (c) a full corpus.</p>
<p>Our focus here is on generating the derivation (line of reasoning) showing how the evidence leads to the answer, rather than the pragmatics of deciding which parts of that to then show the user. This allows us to separate two (typically confounded) explanation requirements, namely correctness (of the derivation) from utility, allowing us to evaluate derivations with a more objective measure (correctness). This also sets the stage for future work on the pragmatics of what to show users (Miller, 2019).</p>
<p>Finally, we define and train generative models, called EntailmentWriters, for this task, adapting earlier techniques for generating deductive proofs (Tafjord et al., 2021). We find the models partially solve the dataset, with indications of generalization to other domains. Our contributions are thus:</p>
<ul>
<li>A formulation of explanation as multistep, multi-premise textual entailment.</li>
<li>EntailmentBank, the first dataset of multistep entailment trees for QA, to support entailment-based explanation. Each tree contains an average of 6.6 nodes and 2.7 entailment steps, with the full dataset of 1,840 trees including a range of small and large multi-step entailment problems.</li>
<li>Baseline results using a state-of-the-art, generative model, showing that reasonable trees can be generated, in particular when the necessary raw facts are provided as the model input (resulting in $35 \%$ of trees with zero errors). We also present indications that Entailment-BANK-trained models can generalize to other domains.</li>
</ul>
<p>This work is significant as it provides a new avenue for the community to generate richer, more systematic explanations.</p>
<h2>2 Related Work</h2>
<p>In the context of QA, there are multiple notions of explanation/justification, including showing an authoritative, answer-bearing sentence (Perez et al., 2019), an attention map over a passage (Seo et al., 2016), a synthesized phrase connecting question and answer (Rajani et al., 2019), or the syntactic pattern used to locate the answer (Ye et al., 2020; Hancock et al., 2018). These methods are primarily designed for answers to "lookup" questions, to explain where/how an answer was found in a corpus.</p>
<p>For questions requiring inference, the focus of this paper, an explanation is sometimes taken as the chain of steps (typically sentences) leading to an answer. Because crowdsourcing such chains is difficult, existing datasets typically simplify the task, e.g., collecting answer-supporting sentences but not how they combine, and/or largely focusing on one-hop (length 2) chains. Here we generalize to tasks requiring multi-step entailment trees, Table 1 illustrates these comparisons in detail.</p>
<p>Our trees are built from multi-premise entailments (two or more sentences entail a hypothesis), introduced by Lai et al. (2017), in contrast to the majority of prior datasets where typically a single sentence entails $H$ through (typically) paraphrasing (Bentivogli et al., 2011; Bar-Haim et al., 2014; Bowman et al., 2015). We extend multi-sentence entailment in two ways. First, our trees also show the provenance of each entailment, namely which sentences are involved in each entailment (i.e., going beyond a classification task). Second, ours is the first dataset that chains multiple entailments together into a hypothesis-directed tree, rather than containing separate, single-step entailments.</p>
<p>Recent work in deductive reasoning has shown that transformers can generate formal proofs with high reliability, both in a formal setting (Polu and Sutskever, 2020; Wang and Deng, 2020) and with rules expressed in natural language (Saha et al., 2020). Inspired by this, we apply similar ideas to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The web-based authoring tool developed to enable authoring entailment trees. (top) The question and a humanreadable version of the semi-structured explanation are provided to the user. (bottom) The semi-structured explanation, including the entailment tree, as currently authored by the user. Nodes (facts) can be dragged-and-dropped to change their ordering. White nodes represent facts from the corpus, while orange nodes were authored by the user. (right) A shortlist (or pool) of top-ranked relevant facts from the corpus that the user can choose to drag-and-drop into the explanation.
generating entailment trees, in particular leveraging the generative techniques used in the ProofWriter system (Tafjord et al., 2021) (Section 5).</p>
<h2>3 The EntailmentBank Dataset</h2>
<p>EntailmentBank contains two parts: 1,840 entailment trees, each tree showing how a questionanswer pair (QA) is entailed from a small number of relevant sentences (e.g., Figure 1); and a general corpus $C$, containing those and other sentences of domain-specific and general knowledge relevant to the QA domain. We use these two parts shortly to define a simpler task (generate the tree given the leaf sentences, without/with distractors) and a harder task (generate the tree from the corpus).</p>
<p>EntailmentBank uses multiple-choice questions (and the correct answer option) from the ARC dataset of grade-school science questions (Clark et al., 2018), and a corpus of science- and general knowledge derived from WorldTree V2 (Xie et al., 2020; Jansen et al., 2018). WorldTree was created for grade-school level science, making it an ideal source for EntailmentBank's corpus.</p>
<h3>3.1 Guidelines</h3>
<p>Three graduate and undergraduate annotators were trained to construct entailment trees for QA pairs, given a small number of potentially relevant sentences for each QA pair (drawn from WorldTree). Specifically, they were trained to author trees:</p>
<ul>
<li>where each step is an entailment (a conclusion that "a person would typically infer" (Dagan et al., 2013)), i.e., the knowledge expressed in each node reasonably follows from the content of its immediate children.</li>
<li>at a fine-grained granularity, where each step encodes a single inference, e.g., making a single taxonomic inference, conjoining two facts, or applying a single rule in the corpus.</li>
<li>that are explicit, with the informal goal of including all the knowledge that a young child would need to answer the question.</li>
<li>that are compositional, where more complex conclusions can be drawn from simpler facts.</li>
<li>that are relevant, concluding (a declarative version of) the QA pair of interest.</li>
</ul>
<h3>3.2 Tool and Authoring Procedure</h3>
<p>Constructing detailed entailment trees meeting the above desiderata is challenging. To make authoring easier, we designed a web-based graphical drag-and-drop authoring tool ${ }^{2}$ (screenshot in Figure 2) that allows explanation authors to construct and review explanations quickly.</p>
<p>For each question, the tool presents the user with a pool of top-ranked relevant facts from the corpus ${ }^{3}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Two example medium-complexity entailment trees, paired with their questions. The root nodes of each tree (hypotheses) are denoted by $\mathbf{H}$ (green), and intermediate conclusions are blue. The top tree describes the reasoning to determine why an astronaut requires oxygen in spacesuit backpacks, and the bottom to determine the similarity between two concepts (evaporation and condensation).
that might be relevant to building an explanation. To assist in the tree construction process, the user first populates an "explanatory worksheet", labeling facts that they anticipate will be included in the tree with a small number of specific categories (e.g., "core facts", "grounding facts"). From this worksheet, the user then begins constructing the entailment tree - typically starting at the bottommost leaf nodes, authoring intermediate conclusions from them, then progressively working on higher levels of the tree until they author a conclusion that directly answers the question.</p>
<p>If the user requires a fact not present in the pool of provided facts, e.g., a missing science fact or a question-specific statement, the user can quickly add their own facts and use these in the tree. Once completed, the individual entailment steps are then separately reviewed by a different author for quality and suggested edits. In total, this process takes an average of approximately 20 minutes per question. Two example trees authored using this process are shown in Figure 3.</p>
<h3>3.3 Overall Dataset</h3>
<p>Due to the large time investment required to generate detailed entailment trees, we author trees for</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Questions</td>
<td style="text-align: center;">1,313</td>
<td style="text-align: center;">187</td>
<td style="text-align: center;">340</td>
<td style="text-align: center;">1,840</td>
</tr>
<tr>
<td style="text-align: left;">Entailment reasoning steps</td>
<td style="text-align: center;">4,175</td>
<td style="text-align: center;">597</td>
<td style="text-align: center;">1,109</td>
<td style="text-align: center;">5,881</td>
</tr>
</tbody>
</table>
<p>Table 2: Summary statistics for the dataset splits.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Histogram of entailment steps in the training set. The average entailment tree contains 7.6 nodes (facts) across 3.2 entailment steps.</p>
<p>1,840 randomly selected questions (of the 7,787 in ARC), which include a total of 5,881 discrete entailment steps. Overall, approximately 600 (paid) work hours were used to build the dataset.</p>
<p>Summary statistics for the train, development, and test sets are shown in Table 2. On average, each entailment tree includes 7.6 nodes across 3.2 entailment steps, where each entailment step typically involves 3 facts (two leaves, that combine to entail a conclusion). Figure 4 shows a histogram of entailment tree size (measured in terms of number of entailment steps). ENTAILMENTBANK includes a diverse range of problem sizes, with half (50\%) of entailment trees representing short entailment problems with one or two entailment steps (typically composed of 3-5 nodes), while the remaining $50 \%$ of trees contain 3-17 entailment steps.</p>
<h3>3.4 Dataset Analysis</h3>
<p>To understand the entailment challenges in ENTAILMENTBANK, we analyzed 100 randomly sampled entailment steps from trees in the training set. We identified 6 common high-level categories of inference, shown in Table 3. Substitution types refer to entailments that require a model to perform taxonomic, merynomic, or other forms of chaining that substitute one entity for another in one of the input sentences. Inference from Rules entailments require the application of a specific rule, specified as one of the input sentences, to the other input sentence. Our analysis suggests that approximately one-third (33\%) of all entailments require the application of domain-specific rules to complete. Further Specification or Conjunction entailments require a model to combine the details of both input facts into a single output fact. Less frequent types require inferring an object's class</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Inference Type</th>
<th style="text-align: center;">Prop.</th>
<th style="text-align: center;">Example Entailment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Substitution</td>
<td style="text-align: center;">$42 \%$</td>
<td style="text-align: center;">$s_{1}$ when a light wave hits a reflective object, the light wave will be reflected <br> $s_{2}$ a mirror is a kind of reflective object <br> int when a light wave hits a mirror, the light wave will be reflected</td>
</tr>
<tr>
<td style="text-align: center;">Inference from Rule</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">$s_{1}$ if two species have similar characteristics, they may share a common ancestor <br> $s_{2}$ rhinoceroses and horses have similar characteristics <br> int rhinoceroses and horses might share a common ancestor</td>
</tr>
<tr>
<td style="text-align: center;">Further Specification or <br> Conjunction</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$s_{1}$ an animal requires warmth for survival as the season changes to winter <br> $s_{2}$ thick fur can be used for keeping warm <br> int thick fur can be used for keeping warm as the season changes to winter</td>
</tr>
<tr>
<td style="text-align: center;">Infer Class from Properties</td>
<td style="text-align: center;">$4 \%$</td>
<td style="text-align: center;">$s_{1}$ A compound is made of two or more elements chemically combined <br> $s_{2}$ sodium chloride is made of two elements chemically combined <br> int sodium chloride is a kind of compound</td>
</tr>
<tr>
<td style="text-align: center;">Property Inheritance</td>
<td style="text-align: center;">$4 \%$</td>
<td style="text-align: center;">$s_{1}$ an animal's shell is usually hard <br> $s_{2}$ something hard can be used for protection <br> int an animal's shell is usually hard for protection</td>
</tr>
<tr>
<td style="text-align: center;">Sequential Inference</td>
<td style="text-align: center;">$3 \%$</td>
<td style="text-align: center;">$s_{1}$ In molecular biology, translation follows transcription <br> $s_{2}$ transcription is when genetic information flows from DNA to RNA <br> $s_{3}$ translation is when genetic information flows from RNA to proteins <br> int In molecular biology, genetic information flows from DNA to RNA to proteins</td>
</tr>
</tbody>
</table>
<p>Table 3: The prevalence of 6 common reasoning methods required to solve individual entailment tree steps, sampled from 100 random entailment steps in the training corpus. Discrete entailment steps in ENTAILMENTBANK require diverse forms of reasoning to solve, from forms of taxonomic or merynomic chaining (substitution) to application of domain-specific rules. Here, $s_{n}$ denotes input sentences, while int denotes entailed conclusions (intermediate nodes in the trees).
from it's properties, inheriting properties of objects, or determining orders for sequential reasoning. As a whole, this analysis shows diverse forms of reasoning are required to successfully complete the entailment steps in ENTAILMENTBANK.</p>
<h2>4 Task Definitions</h2>
<p>Because producing correct entailment trees from a corpus is challenging, we define three tasks of increasing difficulty that simplify the problems inherent in the task. The inputs to all three are a hypothesis $H$, namely a declarative form of a question + answer $(Q A),{ }^{4}$ and some sentences $S$ expressing (both relevant and irrelevant) knowledge. The desired output is a valid entailment tree $T$ where the leaves are sentences selected from $S$, the intermediate nodes int ${ }<em i="i">{i}$ are intermediate conclusions (new sentences, not part of the input), and the root node (conclusion) is the hypothesis $H . T$ is valid if every node $n</em>$ in the tree is entailed by its children. The 3 tasks vary by the size of $S$, described below.</p>
<p>As an approximation to make automated evaluation feasible, we ensure that $S$ includes all the leaf sentences $S_{\text {gold }}$ that are in the gold entailment tree $T_{\text {gold }}$, and treat $T_{\text {gold }}$ (+ valid reorderings) as</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the only valid entailment tree constructable from that input. This allows us to check validity by comparing the generated tree with $T_{\text {gold }}$. This approximation is reasonable for tasks 1 and 2 below, because their limited input makes it unlikely that an alternative valid tree is constructable from the input. For task 3, though, to avoid alternative valid trees being buildable from the input corpus, we remove the few sentences similar to $S_{\text {gold }}$ from the corpus on a per-question basis. Although these steps are not fool-proof, they do allow tree validity to be reasonably approximated by comparing with $T_{\text {gold }}$, a critical requirement for automatic evaluation.
The three tasks' inputs are thus as follows:
Task 1 (no-distractor): Inputs $=H+Q A+$ leaf sentences $S_{\text {gold }}$
Task 2 (distractor): Inputs $=H+Q A+$ leaf sentences $S_{\text {gold }}+15-20$ distractor sentences
Task 3 (full-corpus): Inputs $=H+Q A+$ a corpus $C$
Task 3 represents the full task where $C$ is large. For our experiments, $C$ is the WorldTree corpus plus all additional science facts created by the annotators (Section 3.2). ${ }^{5}$ The desired output in all cases is a valid entailment tree $T$, approximated as being the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>gold entailment tree $T_{\text {gold }}$ (+ valid reorderings).</p>
<h2>5 Model</h2>
<p>Inspired by the "All-at-once" sequence-to-sequence model in the ProofWriter system (Tafjord et al., 2021), we train three T5-based generative models (one per task), called EntailmentWriters.</p>
<h3>5.1 Entailment Tree Encoding</h3>
<p>We encode entailment trees as a linear structure that can be output by a generative model. To do this, the input sentences $S$ are labeled with identifiers (sent1, sent2, ...), and the hypothesis $H$ is labeled with the special identifier 'hypot' (Figure 1). All nodes in the output tree are then identifiers: sent<em> for leaf nodes, int</em> for internal nodes, and 'hypot' for the conclusion (root node). As the int<em> nodes denote new sentences (not in the input), we include those sentences in the output immediately after their int</em> identifier is first introduced.</p>
<p>When linearizing the tree, we start from leaf facts and work towards proving the root of the tree (hypot). We use the symbol "\&amp;" to denote "and", and "-&gt;" to denote "entails". Thus the depth 2 entailment tree in Figure 1 would be encoded as:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">sent2</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nt">sent5</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="nt">int1</span><span class="o">:</span><span class="w"> </span><span class="nt">Eruptions</span><span class="w"> </span><span class="nt">block</span>
<span class="nt">sunlight</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">sent4</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nt">int1</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="nt">hypot</span>
</code></pre></div>

<p>Note here that the new sentence for intermediate node int1, "Eruptions block sunlight", is explicitly part of the to-be-generated output. The task for the models is to output valid entailment trees encoded in this way, given the input.</p>
<h3>5.2 Model Details</h3>
<p>The EntailmentWriter models are built on top of the text-to-text pretrained T5 transformer (Raffel et al., 2020), where the inputs are as described in Section 4 for Task 1 (no-distractor) and Task 2 (distractor). For Task 3 (full-corpus), the corpus exceeds T5's token limit, so we add a retrieval step of 25 sentences from the corpus $C$ using the hypothesis $H$ as query. The output is the predicted entailment tree, encoded as described earlier.</p>
<p>We fine-tune the models on the training sets using the default hyperparameters (including optimizer) in the T5 library. ${ }^{6}$ We use the largest T511B model, fine-tuned for 40 k steps (batch size 8), selecting the checkpoint with highest dev score.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Additional details about the model can be found in Appendix C.</p>
<h2>6 Experiments</h2>
<p>We train and test three EntailmentWriters, one for each task. The model inputs are those described earlier for the three tasks, with the exception of Task 3 where a retrieval step is inserted (the corpus $C$ is too large to be input directly to T5). For this, we retrieve 25 sentences from $C$ using $Q A$ as the query (using a RoBERTa-trained relevant sentence ranker, details in Appendix A), and input those to the model. The output in all cases is the entailment tree explaining ( $H$, the declarative form of) $Q A$.</p>
<h3>6.1 Evaluation Metrics</h3>
<p>We approach evaluating entailment trees as a two step problem. First, nodes in the predicted tree $T_{\text {pred }}$ are aligned with nodes in gold tree $T_{\text {gold }}$, using the sent* labels and Jaccard similarity for intermediate nodes. Thus, instead of doing exact match against gold tree, we account for semanticpreserving variants (Tree Alignment Algorithm described in Appendix C).</p>
<p>Once aligned, the aligned tree $T_{\text {pred }}^{\prime}$ is scored against gold tree $T_{\text {gold }}$ using the metrics below. The F1/BLEURT metrics score elements of the tree (micro-averaging the results), while "AllCorrect" checks if all the elements are correct ( $1=$ yes, $0=$ no), i.e., the predicted tree is perfect along the dimension being considered. Our four metrics are:</p>
<ul>
<li>Leaf Nodes (F1, AllCorrect): Does the predicted tree use the correct leaf sentences? We compute an F1 score by comparing leaf sentences $S_{\text {pred }}$ to $S_{\text {gold }}$. The "AllCorrect" score is 1 if all nodes are identified correctly $(\mathrm{F} 1=1.0), 0$ otherwise.</li>
<li>Steps (F1, AllCorrect): Are the individual entailment steps in the tree structurally correct? As each intermediate node represents (the conclusion of) a single step, the step is considered structurally correct (score 1) if its input sent<em>/int</em> node labels perfectly match the gold, 0 otherwise. We then measure F1 comparing all steps in the two trees. Then AllCorrect $=1$ if $\mathrm{F} 1=1.0,0$ otherwise.</li>
<li>Intermediates (F1, AllCorrect): Are the synthesized intermediate nodes correct? For comparing gold and generated sentences, we use BLEURT $^{7}$ (Sellam et al., 2020). We define genera-</li>
</ul>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">En</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Entail</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Leaves</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Interme</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCor</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCor</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCor</td>
<td style="text-align: center;">AllCor</td>
</tr>
<tr>
<td style="text-align: center;">Task 1 (no-distractor)</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">35.6</td>
</tr>
<tr>
<td style="text-align: center;">Task 2 (distractor)</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: center;">Task 3 (full-corpus)</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">2.9</td>
</tr>
</tbody>
</table>
<p>Table 4: Baseline scores of the generated entailment trees from EntailmentWriter, along four different dimensions (test set). F1/BLEURT scores measure predicted/gold overlap, while AllCorrect scores 1 when all the predictions are correct for a tree, 0 otherwise. Scores on the Dev set are provided in Appendix Table A2, and results using the T5-large model are presented in Appendix Table A4.
tion correctness as 1 if an aligned pair of $\operatorname{int}<em _gold="{gold" _text="\text">{\text {pred }}$, $\operatorname{int}</em> 0$ otherwise. F1 is computed using the number of aligned, correct intermediates wrt. the number of gold/predicted intermediates. AllCorrect=1 if F1=1, otherwise 0.}}$ gives $B L E U R T&gt;0.28,{ }^{8</p>
<ul>
<li>Overall Proof (AllCorrect): The overall "AllCorrect" score for a generated proof is 1 only if all of the leaves, steps, and intermediates are all correct, i.e., the tree completely matches $T_{\text {gold }}$. Otherwise it scores 0 . This is a strict metric: any error in the generated tree will result in a score of 0 .</li>
</ul>
<h3>6.2 Results</h3>
<p>The results are shown in Table 4. From these, several conclusions can be drawn:</p>
<p>First, in the Task 1 (no-distractor) easiest setting, where only the gold leaves are provided as input, the Task1 model performs reasonably well with over one-third of the trees perfectly matching the gold tree. From a manual analysis of a random sample of low-scoring trees, we find an additional $\approx 20 \%$ are also valid but structured differently (thus incorrectly lowering their score), indicating our evaluation metric is an underestimate. We discuss this in more detail in Section 6.3.2.</p>
<p>Second, Task 2 (distractor) increases the difficulty by adding distractors to the input gold sentences until a total of 30 sentences are supplied as input. Despite this large number of distractors, the model is good at identifying the relevant facts (leaves $\mathrm{F} 1=89 \%$, with nearly half the trees having perfectly selected leaves). The overall tree structure in Task2 is (only) a little worse than for Task1 (F1 of steps $41 \%$, vs. $51 \%$ for Task 1), despite the substantial additional task complexity.</p>
<p>Finally, for Task 3, we reuse our Task 2 model (no additional training) but add an IR component to retrieve context from the entire corpus provided</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>for Task 3 (since our model is not able to ingest the entire corpus), using the RoBERTa-based retriever (Appendix A). Note that the retrieval is a feature of our baseline system, not of the task specification itself.</p>
<p>As shown in Table 4, the Task 3 results are lower, indicating that the full task is difficult. Although most trees are partially correct in places (e.g., leaf $\mathrm{F} 1=39 \%$ ), few perfectly match the gold tree. One additional source of error, not present in the earlier Tasks, is that our IR component may not find all the required sentences $S_{\text {gold }}$ for the tree. In fact, we find it retrieves $66.1 \%$ of them on average (and also the model input does not include any questionspecific scenario facts that may be needed). Thus the lower scores for Task 3 also suggest that the retrieval component is as critical as the tree builder itself (if ingestion of the entire corpus is infeasible); future solutions require either better retrieval or ingestion of the entire corpus. Or, alternatively, a model could generate rather than retrieve some supporting sentences (as illustrated in Figure 4), then use these post-hoc to identify suitable supporting corpus sentences.</p>
<h3>6.3 Error Analysis and Future Work</h3>
<p>To understand why invalid trees are sometimes generated, or valid trees mis-scored, we performed several error analyses that we now describe.</p>
<h3>6.3.1 Individual Entailment Steps</h3>
<p>We first analyze cases where the model is failing at individual entailment reasoning steps. For this we randomly sampled 100 entailment steps from imperfect entailment trees (AllCorrect= 0) in the development set. Manually evaluating these, we found that $30 \%$ were correct entailments (and $13 \%$ were nearly correct), suggesting overall invalid trees still contain good steps within them. In cases where the step was invalid, we identify several failure classes and suggest future directions:</p>
<ul>
<li>Repetition: The entailed conclusion simply</li>
</ul>
<p>repeats one of the input sentences (41\%), likely because, in many training instances, the intermediate conclusions have high word overlap with input sentences. A future direction would be to modify the loss function to encourage the model to add something novel compared with the input sentences.</p>
<ul>
<li>Invalid Entailment: The entailed conclusion does not follow from input sentences (47\%): In these cases, the model is using knowledge unstated in the input for this particular entailment step but present somewhere else in the input context. A future direction would be to explore an interative approach, where the model generates one entailment step at a time (a potentially easier entailment task) and then iterates.</li>
<li>Mis-evaluation and Irrelevance: The entailed conclusion is correct, but either different from gold or irrelevant to prove the hypothesis (12\%). Future directions include improving the evaluation metric, and adding a goal-directed term to the loss function to encourage intermediates that are closer to $H$.</li>
</ul>
<h3>6.3.2 Errors in the Full Entailment Trees</h3>
<p>We analyzed an additional 50 imperfect trees on the dev set, and observed the following errors:</p>
<ul>
<li>Incorrect/missing leaves ( $\approx 50 \%$ ): For example, for the question "Why do mosquitoes move towards carbon dioxide...? A: It helps mosquitoes find food", the predicted tree misses using the critical input fact that "mosquitoes eat animal blood", hence cannot infer "animals are a source of food for mosquitoes", hence cannot infer the importance of moving towards carbon dioxide.</li>
<li>Imperfect evaluation ( $\approx 25 \%$ ): We find that a significant number of trees that were scored as invalid are in fact valid, suggesting that our automated metrics underestimate tree validity. The most common reason was that even with the same input sentences, the tree can be structured in several valid ways. For example, a gold tree with structure:
sent1 \&amp; sent2 \&amp; sent3 $\rightarrow$ hypot
may be predicted as:
sent1 \&amp; sent2 $\rightarrow$ int1; int1 \&amp; sent3 $\rightarrow$ hypot
scoring $\mathrm{F} 1=100 \%$ for leaves but $\mathrm{F} 1=0 \%$ for steps, even though valid. (See Appendix D for an instantiated example). This degree of restructuring is not captured by our metrics.</li>
</ul>
<p>To quantify this further, we randomly sampled and rated 50 trees on Task 1 and found human judgements estimated Overall AllCorrect at 58\% (vs. $35.6 \%$ comparing with the gold tree, Table 4),
suggesting the automated evaluation is underestimating true task performance by $\approx 20 \%$ in this case. Future work on an improved evaluation metric would help reduce such understimates.</p>
<ul>
<li>Correct leaves, but invalid steps ( $\approx 20 \%$ ): For example, for a question asking "Can a person see someone in a dark room? A: No", the model selects the correct leaf sentences but stitches them together in the wrong order, resulting in invalid intermediate conclusions. Here, it incorrectly tries to draw an entailment from "a person is in a dark room" and "a person is looking into the dark room", producing "the person outside can see the person in the dark room", an invalid step and one that directly contradicts the target answer. Future work on more reliable entailment, e.g., using an iterative approach and/or adding an entailment validation module, may help address this.</li>
<li>Disconnected trees ( $\approx 5 \%$ ): We found 2 examples where the generated entailment tree had intermediate conclusions that were not used later towards proving the hypothesis. Future work to avoid this would be to apply structural constraints on the output, enforcing a (single) tree structure.</li>
<li>Correct steps, but incorrect intermediate conclusions ( $&lt;5 \%$ ): For example, for a question with H:"compression waves cause objects to move in the same direction of the wave", the model gets the correct proof structure, but instead of concluding a gold intermediate conclusion "longitudinal waves are also called compression waves" it prematurely predicts the final conclusion H for the intermediate (then re-predicts it in the final step).</li>
</ul>
<p>Finally, we grouped the Task 2 results according to the size (number of steps) in the gold tree. The results are shown in Appendix Table A3, and demonstrate that the scores drop significantly as the number of steps in the gold proof increases.</p>
<h2>7 Generality</h2>
<p>To what extent can EntailmentBank help for other domains? Although a full investigation is out of the scope of this paper, we performed two small out-of-domain (OOD) investigations. First, we used the eQASC dataset (Jhamtani and Clark, 2020). eQASC provides a QA hypothesis $H, 10$ retrieved sentences $C$, and lists valid single step entailments from two sentences in $C$ to $H$ - i.e., one-step (three node) entailment trees. Running our Task 2 EntailmentWriter to generate a tree given $C$ and $H$, we find (comparing with the best-matching</p>
<p>Input: (a QA pair)
Q: I would like to go running, but my ankle is injured. What should I do?
A: ice it
Output:(explanation generated by EntailmentWriter)
Icing an injured body part can help to reduce the pain
An ankle is a kind of body part
$\rightarrow$ Icing an injured ankle can help to reduce the pain
Input: (a QA pair)
Q: If I put some cheese in the fridge, will it melt?
A: no, because it will be too cold
Output:(explanation generated by EntailmentWriter)
If something is put in a cold place then it will remain cold A refrigerator is used for cooling things
$\rightarrow$ If something is put in a refrigerator then it will remain cold.
Cheese is put in the fridge
$\rightarrow$ The cheese will remain cold in the refrigerator Cold objects do not melt in cold temperatures $\rightarrow$ Cheese will not melt in the refrigerator</p>
<p>Input: (a QA pair)
Q: I feel sad. What could I do to cheer myself up?
A: listen to music
Output: (explanation generated by EntailmentWriter)
Music is a kind of entertainment
Entertainment can be used to cheer up a person
$\rightarrow$ Music can be used to cheer up a person
If a person is sad, they may want to cheer themselves up
$\rightarrow$ If a person is sad, they may want to cheer themselves up by listening to music</p>
<p>Figure 5: Three examples of explanations fully generated by EntailmentWriter, zero-shot, out-of-domain. Here, all sentences (leaves and intermediates) are generated by the model.
gold tree) a F1 (leaves) of $67 \%$ and an Overall AllCorrect score of $26 \%$ - a positive indication of transfer OOD. Note that this is without fine-tuning on eQASC, and that eQASC does not list all valid entailments, hence good outputs may be missed.</p>
<p>We also trained a no-context version of EntailmentWriter using EntailmentBank, that inputs just a QA pair and outputs a tree, generating all the tree sentences (both leaves and intermediates). We then ran this on Challenge300, an existing, independently authored dataset of 300 test questions covering multiple domains (Tafjord and Clark, 2021). From a manual evaluation of a random sample of generated trees, $\approx 35 \%$ were valid, non-vacuous trees. ( $\approx 25 \%$ of the remainder were valid but largely repeated the question and answer). Three good examples are shown in Figure 5, again illustrating the potential of EntailmentBank for explanation.</p>
<p>Finally, as an experiment in interactive explanation generation, we re-purposed ENTAILMENTBANK to train a model to generate an explana-
tion one step at a time. To do this, we "shredded" the entailment trees into individual one-deep trees (where the intermediate nodes become new hypotheses to prove), and re-trained a model to generate similar one-deep entailment trees. This model can then be used interactively, generating a one-deep explanation then allowing a user to select which premise(s) to drill down into, based on what he/she wants to know more about, recursively calling the model to explain that premise further. Although such generative models (both generating a full tree or a one-deep tree) can sometimes produce false or nonsensical facts, one could apply fact verification techniques, e.g., (Thorne et al., 2018; Christodoulopoulos et al., 2020), to validate the generated facts, and generate an alternative explanation if validation fails. These are exciting future directions that we are exploring.</p>
<h2>8 Summary and Conclusion</h2>
<p>Our goal is to enable machines to generate richer, more systematic explanations. To this end, we have developed a novel formulation of explanations as multistep entailment trees, and created ENTAILMENTBANK, the first large dataset of such trees.</p>
<p>We have also presented baseline results for automatically generating entailment tree explanations for answers to science questions, trained on ENTAILMENTBANK. These initial results suggest that such generation is possible, in particular when the necessary raw facts are included in the model input. We have also presented indications that models trained on ENTAILMENTBANK can generalize to other domains. This suggests exciting opportunities for future systems that can help users understand and debug a system's answers, and ultimately engage in meaningful dialogs that explore the machine's line of reasoning. ENTAILMENTBANK contributes to this direction, offering a new resource for developing richer, more systematic explanations. ENTAILMENTBANK is available at https://allenai.org/data/entailmentbank.</p>
<h2>Acknowledgements</h2>
<p>We thank Google for providing the TPUs for conducting experiments. We also thank the Allen Institute of Artificial Intelligence and National Science Foundation award #1815948 to Peter Jansen for funding this work.</p>
<h2>References</h2>
<p>Roy Bar-Haim, I. Dagan, and Idan Szpektor. 2014. Benchmarking applied semantic inference: The pascal recognising textual entailment challenges. In Language, Culture, Computation.
L. Bentivogli, Peter Clark, I. Dagan, and Danilo Giampiccolo. 2011. The seventh pascal recognizing textual entailment challenge. Theory and Applications of Categories.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP.</p>
<p>Christos Christodoulopoulos, James Thorne, Andreas Vlachos, Oana Cocarascu, and Arpit Mittal, editors. 2020. Proc. 3rd Workshop on Fact Extraction and Verification. ACL. Https://aclanthology.org/events/fever-2020/.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. ArXiv, abs/1803.05457.</p>
<p>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing Textual Entailment: Models and Applications. Morgan and Claypool.</p>
<p>Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. ArXiv, abs/1809.02922.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HIT.</p>
<p>Jay DeYoung, Sarthak Jain, Nazneen Rajani, E. Lehman, Caiming Xiong, R. Socher, and Byron C. Wallace. 2019. ERASER: A benchmark to evaluate rationalized NLP models. In $A C L$.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, D. Roth, and Jonathan Berant. 2021. Did Aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. ArXiv, abs/2101.02235.</p>
<p>Shuguang Han, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2020. Learning-to-rank with BERT in TF-ranking. ArXiv, abs/2004.08476.</p>
<p>Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher Ré. 2018. Training classifiers with natural language explanations. In $A C L$.
N. Inoue, Pontus Stenetorp, and Kentaro Inui. 2020. R4C: A benchmark for evaluating RC systems to get the right answer for the right reason. In $A C L$.</p>
<p>Peter A. Jansen, Elizabeth Wainwright, Steven Marmorstein, and Clayton T. Morrison. 2018. WorldTree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. In LREC.</p>
<p>Harsh Jhamtani and P. Clark. 2020. Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering. In EMNLP.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UnifiedQA: Crossing format boundaries with a single QA system. In Findings-EMNLP.</p>
<p>Alice Lai, Yonatan Bisk, and J. Hockenmaier. 2017. Natural language inference from multiple premises. In IJCNLP.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. ArXiv, abs/1907.11692.
T. Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell., 267:1-38.</p>
<p>Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. 2019. Finding generalizable evidence by learning to convince Q\&amp;A models. In EMNLP.</p>
<p>Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. ArXiv, abs/2009.03393.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, M. Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Nazneen Rajani, B. McCann, Caiming Xiong, and R. Socher. 2019. Explain yourself! Leveraging language models for commonsense reasoning. In $A C L$.</p>
<p>Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020. PRover: Proof generation for interpretable reasoning over rules. In EMNLP.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. Bleurt: Learning robust metrics for text generation. ArXiv, abs/2004.04696.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. In $I C L R$.</p>
<p>Oyvind Tafjord and Peter Clark. 2021. Generalpurpose question-answering with Macaw. ArXiv, abs/2109.02593.</p>
<p>Oyvind Tafjord, B. D. Mishra, and P. Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. IJCAI.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and verification. In NAACL.</p>
<p>Ming-Zhe Wang and Jun Deng. 2020. Learning to prove theorems by learning to generate theorems. In NeurIPS.</p>
<p>Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, and Peter Jansen. 2020. WorldTree V2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. In LREC.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. ArXiv, abs/1809.09600.</p>
<p>Qinyuan Ye, Xiaozhen Huang, and Xiang Ren. 2020. Teaching machine comprehension with compositional explanations. ArXiv, abs/2005.00806.</p>
<h2>A Relevant Fact Retrieval Algorithm</h2>
<p>When authoring an entailment tree for a question, annotators are shown a pool of potentially relevant facts, selected from WorldTree, to help them get started. To identify those facts, we could simply use standard information retrieval with the QA pair as the query. However, for this dataset, we are able to do better than this: First, we train two "relevant sentence" classifiers (using BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) respectively) using additional WorldTree annotations. ${ }^{9}$ Then, for each question, both models exhaustively score every fact in the corpus, and the top 20 facts from each are retrieved, reranked using Tensorflow-Ranking-BERT (Han et al., 2020), and presented as a ranked list to the entailment tree annotator based on their final scores.</p>
<h2>B Evaluation: Tree Alignment Algorithm</h2>
<p>Predicted entailment trees are evaluated by first aligning them with gold entailment trees, using a variant of the algorithm in (Inoue et al., 2020), as follows:</p>
<ul>
<li>First, for each intermediate conclusion $i n t_{\text {pred }}$ in $T_{\text {pred }}$, and int gold in $T_{\text {gold }}$, we gather their ancestor leaf sentences.</li>
<li>Then, we align each intermediate node int $t_{\text {pred }}$ to the first int gold for which the Jaccard similarity of their respective ancestor sentences is maximum. For any int $t_{\text {pred }}$ with zero Jaccard similarity to all gold nodes int gold, it is aligned to a dummy gold node with a blank conclusion.</li>
</ul>
<h2>C Training and Model Selection</h2>
<p>For Task 1 and Task 2, we trained T5 11B models on the training set using default hyperparameters (except the number of steps) following the procedure of Khashabi et al. (2020). We used batch size of 8 and a block size of 512 tokens on both input and output side. For both training and evaluation we use v3-8 TPUs from Google cloud computing</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure A2: An example question, its gold entailment tree (top), and a model-generated tree (bottom) that has different structure and different intermediate conclusions, but is still valid. The root nodes of each tree (hypotheses) are denoted by $\mathbf{H}$ (green), and intermediate conclusions are blue.
platform. Each model has 11B parameters and takes 22GB space on disk.</p>
<p>During training, we ran the model for different number of steps (up to 40 K steps in the intervals of 4 K ) and picked the model that gives best Overall AllCorrect score on the Dev set. Thus our hyperparameter search involved 10 models each for Task 1 and Task 2. We picked the models after 16 K and 32 K steps for Task 1 and Task 2 respectively. Table A2 shows model scores on the development set.</p>
<p>Each Task required 16 hours of training. Inference on 340 test questions takes 12 minutes. A large fraction of this time is spent in saving the model checkpoints to disk or loading the model from disk.</p>
<h2>D Tree Structure Variation</h2>
<p>As described in Section 6.3.2, although our evaluation metric accounts for different node ordering and intermediates wording between the predicted and gold trees, there are still cases where a valid predicted tree differs from the gold tree in a way which (undesirably) hurts its score. For example, a gold tree with the structure:</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Entailment Tree Scoring</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Leaves</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Intermediates</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">AllCorrect</td>
</tr>
<tr>
<td style="text-align: left;">Task 1 (no-distractor)</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: left;">Task 2 (distractor)</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: left;">Task 3 (full-corpus)</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">3.2</td>
</tr>
</tbody>
</table>
<p>Table A2: Development set results, analogous to test set results for Table 4, showing baseline scores of the generated entailment trees from EntailmentWriter along four different dimensions (dev set).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of <br> steps</th>
<th style="text-align: center;">Number of <br> questions</th>
<th style="text-align: center;">Leaves</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Steps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Intermediates</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall <br> AllCorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">79.3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">17.9</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">7.9</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">$\geq 6$</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">340</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">25.6</td>
</tr>
</tbody>
</table>
<p>Table A3: Results on Task 2 (distractor) broken down by the number of entailment steps in the gold tree, indicating that scores drop rapidly as trees get larger (more steps).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Entailment Tree Scoring</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Leaves</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Intermediates</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">AllCorrect</td>
<td style="text-align: center;">AllCorrect</td>
</tr>
<tr>
<td style="text-align: left;">Task 1 (no-distractor)</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: left;">Task 2 (distractor)</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">22.4</td>
</tr>
<tr>
<td style="text-align: left;">Task 3 (full-corpus)</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">2.4</td>
</tr>
</tbody>
</table>
<p>Table A4: Test set results using T5-large model, analogous to T5-11B results in Table 4.
sent1 \&amp; sent2 \&amp; sent3 $\rightarrow$ hypot
may be predicted as:
sent1 \&amp; sent2 $\rightarrow$ int1; int1 \&amp; sent3 $\rightarrow$ hypot
scoring $\mathrm{F} 1=100 \%$ for leaves but (undesirably)
$\mathrm{F} 1=0 \%$ for steps, even though valid. Figure A2
shows a more complex example, where both the gold and predicted trees have identical leaf nodes (leaf $\mathrm{F} 1=100 \%$ ), but different organization. Although both trees are valid, the predicted tree here (undesirably) scores Step F1 $=0 \%$. Because of cases like this, our predicted scores are an understimate of the true quality of the predictions (by as much as $20 \%$ from a small study, as described in Section 6.3.2).</p>
<h2>E Additional Results: T5-large baseline</h2>
<p>Here, we trained a T5-large model using default hyperparameters following the procedure of Khashabi et al. (2020). We used batch size of 64 and a block size of 512 tokens on both input and output side. During training, we ran the model for different number of steps (up to 80 K steps in the intervals of 8 K ) and picked the model that gives best Overall</p>
<p>AllCorrect score on the Dev set. We picked the models after 48 K and 32 K steps for Task 1 and Task 2 respectively. Table A4 shows model scores on the test set.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ WorldTree includes annotations about which WorldTree table rows are relevant to which questions, i.e., which rows are supporting evidence ("rationales") for which question. Although these rationales do not identify all relevant sentences, they can be used as distant supervision (along with random negative facts drawn from the corpus) to train a "relevant sentence" classifier.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ Using the state-of-the-art BLEURT-Large-512 model. Our analysis based on 300 hand-scored examples suggests its similarity scores correlate well with human ratings (correlation $=0.67$, sensitivity $=0.88$, specificity $=0.80$ )&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2611 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2611</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2611</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-713b604fb9cdd6631074cbd6bf36db029031992e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/713b604fb9cdd6631074cbd6bf36db029031992e" target="_blank">Large Language Models are Zero Shot Hypothesis Proposers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The findings of this paper strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Paper Abstract:</strong> Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2611.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2611.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BHP Multi-Agent System</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Multi-Agent Cooperative Framework for Hypothesis Proposal (BHP Multi-Agent System)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system introduced and evaluated in this paper that assigns Analyst, Engineer, Scientist, and Critic roles to LLM agents (optionally with human-in-the-loop) to generate, refine, and evaluate biomedical hypotheses from literature backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BHP Multi-Agent System (Analyst / Engineer / Scientist / Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A role-based multi-agent framework implemented with LLMs where the Analyst extracts core elements/keywords from background text, the Engineer retrieves and organizes supporting material (via search tools), the Scientist synthesizes findings and the background into hypotheses, and the Critic evaluates and requests refinements; the loop can iterate and optionally include human oversight. The system was implemented with off-the-shelf instructed LLMs, integrated with external search tooling (PubMed) and evaluated both with and without tool use. It is aimed at automating hypothesis generation, critique, and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis Generation System / Multi-Agent AI Scientist Framework</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Biomedical literature-based hypothesis generation (biomedicine: cardiac mechanics, electrophysiology examples used), general scientific hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate novel, relevant, significant, verifiable natural-language scientific hypotheses given a background text (paper abstract/summary); test zero-shot generalization to hypotheses appearing in later literature (temporal split). The task input is a background knowledge description; the output is one or more hypothesis statements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended, high-dimensional hypothesis space (combinatorial and semantic), requires cross-domain reasoning and multi-step inference; problems include uncertain/underspecified causal links and multi-factor biomedical mechanisms. Quantitatively: dataset sizes small (seen training pairs 2700, unseen test 200); evaluation uses multiple soft metrics rather than a single deterministic objective.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Built BHP dataset: training/seen set 2,700 background–hypothesis pairs (2500 train, 200 val), unseen test set 200 pairs from August 2023; data are pre-existing literature summaries distilled via chatGPT-turbo-3.5. High-quality but limited-size paired data for supervised fine-tuning; unseen examples were withheld by publication-date filtering to ensure true zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Experiments used standard large LLM APIs and open-source Llama-family models; fine-tuning described for one model (WizardLM-13B-V1.2) full-parameter for 3 epochs, batch size 8, max sequence length 2048, learning rate 3e-5 (no wall-clock compute hours or dollar costs reported). Running multi-agent loops requires repeated LLM calls (multiple rounds of interaction) and external search calls (PubMed), increasing API call counts but no absolute compute-time numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended and non-deterministic (many plausible hypotheses); discrete textual output space; evaluation lacks a single ground truth (multiple valid hypotheses), so authors used BLEU/ROUGE for overlap plus four human/LLM-scored dimensions (novelty, relevance, significance, verifiability). Domain knowledge is important and available via literature; structured evaluation metrics are soft and partially subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Automatic: BLEU, ROUGE; Human/LLM evaluation: scores (0–3) for novelty, relevance, significance, verifiability, combined as an average. Correlation with human judgments (Pearson/Spearman) also used to validate automatic LLM-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Multi-agent (no tool use) achieved GPT-based evaluation Avg = 2.09 (novelty 1.35, relevance 2.85, significance 2.05, verifiability 2.10) on unseen set; multi-agent + tool use Avg = 2.07 (novelty 1.52, relevance 2.66, significance 2.06, verifiability 2.05). Baseline single-agent gpt-3.5-turbo (0-shot) Avg = 1.92. Thus multi-agent improved average evaluation by ~0.15 points (on 0–3 scale) over the baseline on the unseen biomedical test set. BLEU/ROUGE values were lower for multi-agent variants (BLEU ~11–12) versus baseline (BLEU 15.52).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Iterations: tool use alone provided minimal improvement and in some configurations decreased automatic overlap metrics; the LLMs sometimes failed to extract useful evidence from external searches and struggled with the thought→action→observation loop (authors note ReAct and tool integration limitations). Few-shot/in-context examples often decreased novelty (more conservative outputs). Hallucination risks and dependence on quality of retrieved documents were noted.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Division-of-labor (role decomposition) and iterative critic feedback increased uncertainty/diversity and improved zero-shot generalization; zero-shot prompts (no in-context examples) often produced higher novelty which aided discovery; tight temporal visibility control in dataset (publication cutoff) allowed true zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Zero-shot often outperformed few-shot for large models (e.g., Llama-2-70B, WizardLM-70B) in novelty and generalization; multi-agent (role decomposition) produced higher average LLM-evaluation scores (≈2.09) compared to single-agent baseline (≈1.92). Tool use produced small gains for multi-agent (Avg 2.09 → 2.07, i.e., essentially unchanged), and single-agent tool pipelines had mixed results (see ReAct vs function-calling entries). Fine-tuning a smaller model (WizardLM-13B-V1.2) improved some metrics but could reduce novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No direct human hypothesis-generation baseline reported. Humans only acted as evaluators: three biomedical annotators scored 100 unseen examples; human evaluation averages for top models were ~1.77–2.33 (on 0–3 scale) indicating generated hypotheses were judged moderately good. The paper did not measure humans producing hypotheses from the same backgrounds as a baseline generator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2611.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting) method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that structures LLM behavior as alternating 'thought' and 'action' steps to allow models to reason and call tools/perform actions iteratively; used here to control whether to invoke external search and to process observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct (thought-action-observation loop)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach that instructs LLMs to deliberate (chain-of-thought style) then decide actions (tool calls) and incorporate observations to continue reasoning; used in experiments to allow the Engineer agent or single LLM to call PubMed searches and then incorporate retrieved snippets into hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Tool-use orchestration / Agent action policy</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Biomedical hypothesis generation with external literature retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Enable LLMs to decide when and how to query external search (PubMed) and to interpret the returned evidence in order to form or verify hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended textual decision process; requires deciding whether additional retrieval is needed, formulating queries, interpreting retrieved passages; search space includes many possible queries and many retrieved documents per query.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Used the same BHP dataset; external evidence retrieved from PubMed (filtered to exclude post-Jan-2023 publications to preserve unseen test).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Adds additional LLM API calls and external search API calls per decision/action step; no absolute compute cost reported but increases call count relative to no-tool baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Iterative decision-making with stochastic retrieval results; evaluation uses same BLEU/ROUGE and LLM/human scoring metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>BLEU, ROUGE, and GPT-evaluated novelty/relevance/significance/verifiability (0–3).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In the paper's Table 3 (single-agent + tool use variant labeled 2^a), the ReAct-based variant (tool use) returned BLEU 14.94, ROUGE 24.16, GPT-eval novelty 0.78, relevance 2.42, significance 1.18, verifiability 1.87, Avg = 1.56 on unseen data (lower than multi-agent Avg 2.09).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Authors report LLMs struggled to identify useful information and to correctly follow the thought→action→observation loop; retrieved evidence sometimes not used effectively, leading to decreased novelty and lower overall average in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Structuring decision points and explicitly integrating observations can in principle guide retrieval to support hypothesis verifiability, but success depends on retrieval quality and the model's ability to interpret evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>ReAct-equipped single-agent performed worse than multi-agent approaches; compared to an OpenAI function-calling variant (2^b), numeric table results are close (both lower than multi-agent). The authors narratively suggested ReAct underperformed vs function calling, though exact table numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not applicable (humans were evaluators, not implementing ReAct).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2611.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Function Calling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Function Calling (API feature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>API functionality used to enable LLMs to call external functions (e.g., search functions) in a structured way; used in experiments as an alternative tool-integration mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI Function Calling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An API-level mechanism where the model can return structured function-call requests (e.g., to search PubMed) and receive structured responses; used here to implement tool-backed retrieval (labeled 2^b in Table 3) and compare against ReAct-style tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Tool integration / Agent tool invocation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Biomedical literature retrieval to support hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Allow the LLM to request specific search actions and integrate the returned structured results into hypothesis crafting and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Similar to ReAct: iterative retrieval and interpretation, variable number of function calls per example; external results quality variable.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Same BHP dataset plus filtered PubMed results (post-Jan-2023 filtered out).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Requires extra API calls to both LLM endpoint and search functions; no absolute compute cost reported.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Deterministic structured-call interface but stochastic content in search results; evaluation identical to other variants.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>BLEU/ROUGE and GPT-eval (novelty/relevance/significance/verifiability).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Table 3 (variant 2^b) shows BLEU 15.87, ROUGE 24.94, novelty 0.57, relevance 2.58, significance 0.93, verifiability 1.89, Avg = 1.49 on the unseen test set.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited gains from tool integration; retrieved evidence sometimes not used effectively; function-calling did not yield consistent improvement in hypothesis-generation quality over baseline or multi-agent frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Structured function calls provide clearer semantics for tool use which can simplify integration, but overall improvements depend on retrieval quality and how agents consume evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>OpenAI function-calling single-agent variant produced Avg = 1.49 (Table 3), comparable to ReAct variant but lower than multi-agent Avg = 2.09.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2611.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubMed search tool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubMed (external literature search engine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Biomedical literature search engine used by the Engineer agent to retrieve supporting documents during hypothesis generation (results filtered to exclude publications after Jan 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PubMed search integration</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A retrieval tool integrated into the Engineer role to fetch relevant biomedical literature evidence to support or verify proposed hypotheses; used both via ReAct-style and function-calling tool integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>External literature retrieval / Tool</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Biomedical literature retrieval to support hypothesis formation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Retrieve and present evidence snippets relevant to the background to increase verifiability of generated hypotheses while maintaining unseen-test filtering by publication date.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Large external corpus (PubMed) with many candidate documents per query; quality depends on query formulation and filtering; risk of returning document content present in model pretraining if not carefully date-filtered.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Public biomedical literature; authors explicitly filtered results to exclude documents published after January 2023 to preserve unseen test integrity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Search latency and additional API calls; quantity depends on number of queries per example.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Deterministic retrieval interface but stochastic selection of returned documents; integration complexity arises from aligning retrieved evidence with generation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Impact measured indirectly via final hypothesis evaluation (BLEU/ROUGE and GPT/human scores); no stand-alone retrieval metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Tool use with PubMed yielded minimal net improvement for single-agent pipelines and modest improvements when used within multi-agent loops (multi-agent + tool Avg = 2.07 vs multi-agent no-tool 2.09 — effectively similar).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Models sometimes couldn't identify or correctly use useful evidence from PubMed results; evidence retrieval occasionally reduced novelty and did not reliably increase overall hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>When retrieval is targeted and when agents are specialized (Engineer role), the framework can organize evidence more effectively; strict date filtering ensured no leakage into unseen test set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>PubMed-backed single-agent variants (ReAct/function calling) underperformed compared to multi-agent without tools; tool integration benefits were clearer when combined with role decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2611.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A browser-assisted question-answering system mentioned in related work that integrates web search and human feedback to improve factual QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of LLM tool-augmented systems that use web browsing/search to answer questions with improved factual grounding; mentioned in the paper's related work on tool-augmented LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Mentioned external system (Tool-augmented QA agent)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General question-answering via web retrieval (related literature; not applied in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not applied in this paper; cited as part of the broader landscape of LLMs integrated with search tools.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2611.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method referenced in related work for integrating tool use into LLMs via self-supervised fine-tuning to call external APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in the paper as an example of approaches that fine-tune LLMs to invoke tools (e.g., calculators, search) - cited as background for why and how tool integration can be performed.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Mentioned external approach (tool-use fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General LLM tool integration (mentioned only)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2611.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HuggingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of systems that orchestrate multiple AI tools/agents to solve tasks by delegating sub-tasks to specialized models or APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HuggingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as related work in LLM-driven agent systems that coordinate multiple models and tool APIs to accomplish complex tasks; not directly used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Mentioned external multi-tool orchestration system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General AI task solving via multi-tool orchestration (mentioned only)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2611.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited related work demonstrating augmentation of LLMs with chemistry-specific tools to perform domain tasks, used here as an example of domain-specific tool augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemCrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of tool-augmented LLM systems applied to chemistry tasks; included to motivate integrating domain tools into LLM-driven agents, but not used in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Mentioned external domain-specific tool augmentation system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry (example only; not used)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2611.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2611.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-driven Autonomous Agents (LAAs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-driven Autonomous Agents (LAAs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of systems combining LLMs with planning, tool use, memory and decision-making to autonomously perform complex tasks; discussed in related work and used as conceptual background.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Rise and Potential of Large Language Model Based Agents: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-driven Autonomous Agents (LAAs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A survey-level concept referring to agents that combine LLMs with external tools, memory, and planning modules to autonomously plan and execute multi-step tasks; the paper cites LAAs as the broader category into which their multi-agent hypothesis-proposer fits.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Concept / Surveyed system type</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General autonomous task solving across domains (surveyed; not instantiated by this paper aside from the BHP system)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face <em>(Rating: 1)</em></li>
                <li>ChemCrow: Augmenting large-language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework <em>(Rating: 1)</em></li>
                <li>The Rise and Potential of Large Language Model Based Agents: A Survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2611",
    "paper_id": "paper-713b604fb9cdd6631074cbd6bf36db029031992e",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "BHP Multi-Agent System",
            "name_full": "LLM-based Multi-Agent Cooperative Framework for Hypothesis Proposal (BHP Multi-Agent System)",
            "brief_description": "A multi-agent system introduced and evaluated in this paper that assigns Analyst, Engineer, Scientist, and Critic roles to LLM agents (optionally with human-in-the-loop) to generate, refine, and evaluate biomedical hypotheses from literature backgrounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BHP Multi-Agent System (Analyst / Engineer / Scientist / Critic)",
            "system_description": "A role-based multi-agent framework implemented with LLMs where the Analyst extracts core elements/keywords from background text, the Engineer retrieves and organizes supporting material (via search tools), the Scientist synthesizes findings and the background into hypotheses, and the Critic evaluates and requests refinements; the loop can iterate and optionally include human oversight. The system was implemented with off-the-shelf instructed LLMs, integrated with external search tooling (PubMed) and evaluated both with and without tool use. It is aimed at automating hypothesis generation, critique, and refinement.",
            "system_type": "Hypothesis Generation System / Multi-Agent AI Scientist Framework",
            "problem_domain": "Biomedical literature-based hypothesis generation (biomedicine: cardiac mechanics, electrophysiology examples used), general scientific hypothesis generation",
            "problem_description": "Generate novel, relevant, significant, verifiable natural-language scientific hypotheses given a background text (paper abstract/summary); test zero-shot generalization to hypotheses appearing in later literature (temporal split). The task input is a background knowledge description; the output is one or more hypothesis statements.",
            "problem_complexity": "Open-ended, high-dimensional hypothesis space (combinatorial and semantic), requires cross-domain reasoning and multi-step inference; problems include uncertain/underspecified causal links and multi-factor biomedical mechanisms. Quantitatively: dataset sizes small (seen training pairs 2700, unseen test 200); evaluation uses multiple soft metrics rather than a single deterministic objective.",
            "data_availability": "Built BHP dataset: training/seen set 2,700 background–hypothesis pairs (2500 train, 200 val), unseen test set 200 pairs from August 2023; data are pre-existing literature summaries distilled via chatGPT-turbo-3.5. High-quality but limited-size paired data for supervised fine-tuning; unseen examples were withheld by publication-date filtering to ensure true zero-shot.",
            "computational_requirements": "Experiments used standard large LLM APIs and open-source Llama-family models; fine-tuning described for one model (WizardLM-13B-V1.2) full-parameter for 3 epochs, batch size 8, max sequence length 2048, learning rate 3e-5 (no wall-clock compute hours or dollar costs reported). Running multi-agent loops requires repeated LLM calls (multiple rounds of interaction) and external search calls (PubMed), increasing API call counts but no absolute compute-time numbers provided.",
            "problem_structure": "Open-ended and non-deterministic (many plausible hypotheses); discrete textual output space; evaluation lacks a single ground truth (multiple valid hypotheses), so authors used BLEU/ROUGE for overlap plus four human/LLM-scored dimensions (novelty, relevance, significance, verifiability). Domain knowledge is important and available via literature; structured evaluation metrics are soft and partially subjective.",
            "success_metric": "Automatic: BLEU, ROUGE; Human/LLM evaluation: scores (0–3) for novelty, relevance, significance, verifiability, combined as an average. Correlation with human judgments (Pearson/Spearman) also used to validate automatic LLM-based evaluation.",
            "success_rate": "Multi-agent (no tool use) achieved GPT-based evaluation Avg = 2.09 (novelty 1.35, relevance 2.85, significance 2.05, verifiability 2.10) on unseen set; multi-agent + tool use Avg = 2.07 (novelty 1.52, relevance 2.66, significance 2.06, verifiability 2.05). Baseline single-agent gpt-3.5-turbo (0-shot) Avg = 1.92. Thus multi-agent improved average evaluation by ~0.15 points (on 0–3 scale) over the baseline on the unseen biomedical test set. BLEU/ROUGE values were lower for multi-agent variants (BLEU ~11–12) versus baseline (BLEU 15.52).",
            "failure_modes": "Iterations: tool use alone provided minimal improvement and in some configurations decreased automatic overlap metrics; the LLMs sometimes failed to extract useful evidence from external searches and struggled with the thought→action→observation loop (authors note ReAct and tool integration limitations). Few-shot/in-context examples often decreased novelty (more conservative outputs). Hallucination risks and dependence on quality of retrieved documents were noted.",
            "success_factors": "Division-of-labor (role decomposition) and iterative critic feedback increased uncertainty/diversity and improved zero-shot generalization; zero-shot prompts (no in-context examples) often produced higher novelty which aided discovery; tight temporal visibility control in dataset (publication cutoff) allowed true zero-shot evaluation.",
            "comparative_results": "Zero-shot often outperformed few-shot for large models (e.g., Llama-2-70B, WizardLM-70B) in novelty and generalization; multi-agent (role decomposition) produced higher average LLM-evaluation scores (≈2.09) compared to single-agent baseline (≈1.92). Tool use produced small gains for multi-agent (Avg 2.09 → 2.07, i.e., essentially unchanged), and single-agent tool pipelines had mixed results (see ReAct vs function-calling entries). Fine-tuning a smaller model (WizardLM-13B-V1.2) improved some metrics but could reduce novelty.",
            "human_baseline": "No direct human hypothesis-generation baseline reported. Humans only acted as evaluators: three biomedical annotators scored 100 unseen examples; human evaluation averages for top models were ~1.77–2.33 (on 0–3 scale) indicating generated hypotheses were judged moderately good. The paper did not measure humans producing hypotheses from the same backgrounds as a baseline generator.",
            "uuid": "e2611.0",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting) method",
            "brief_description": "A method that structures LLM behavior as alternating 'thought' and 'action' steps to allow models to reason and call tools/perform actions iteratively; used here to control whether to invoke external search and to process observations.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "mention_or_use": "use",
            "system_name": "ReAct (thought-action-observation loop)",
            "system_description": "An approach that instructs LLMs to deliberate (chain-of-thought style) then decide actions (tool calls) and incorporate observations to continue reasoning; used in experiments to allow the Engineer agent or single LLM to call PubMed searches and then incorporate retrieved snippets into hypothesis generation.",
            "system_type": "Tool-use orchestration / Agent action policy",
            "problem_domain": "Biomedical hypothesis generation with external literature retrieval",
            "problem_description": "Enable LLMs to decide when and how to query external search (PubMed) and to interpret the returned evidence in order to form or verify hypotheses.",
            "problem_complexity": "Open-ended textual decision process; requires deciding whether additional retrieval is needed, formulating queries, interpreting retrieved passages; search space includes many possible queries and many retrieved documents per query.",
            "data_availability": "Used the same BHP dataset; external evidence retrieved from PubMed (filtered to exclude post-Jan-2023 publications to preserve unseen test).",
            "computational_requirements": "Adds additional LLM API calls and external search API calls per decision/action step; no absolute compute cost reported but increases call count relative to no-tool baselines.",
            "problem_structure": "Iterative decision-making with stochastic retrieval results; evaluation uses same BLEU/ROUGE and LLM/human scoring metrics.",
            "success_metric": "BLEU, ROUGE, and GPT-evaluated novelty/relevance/significance/verifiability (0–3).",
            "success_rate": "In the paper's Table 3 (single-agent + tool use variant labeled 2^a), the ReAct-based variant (tool use) returned BLEU 14.94, ROUGE 24.16, GPT-eval novelty 0.78, relevance 2.42, significance 1.18, verifiability 1.87, Avg = 1.56 on unseen data (lower than multi-agent Avg 2.09).",
            "failure_modes": "Authors report LLMs struggled to identify useful information and to correctly follow the thought→action→observation loop; retrieved evidence sometimes not used effectively, leading to decreased novelty and lower overall average in some configurations.",
            "success_factors": "Structuring decision points and explicitly integrating observations can in principle guide retrieval to support hypothesis verifiability, but success depends on retrieval quality and the model's ability to interpret evidence.",
            "comparative_results": "ReAct-equipped single-agent performed worse than multi-agent approaches; compared to an OpenAI function-calling variant (2^b), numeric table results are close (both lower than multi-agent). The authors narratively suggested ReAct underperformed vs function calling, though exact table numbers are provided.",
            "human_baseline": "Not applicable (humans were evaluators, not implementing ReAct).",
            "uuid": "e2611.1",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "OpenAI Function Calling",
            "name_full": "OpenAI Function Calling (API feature)",
            "brief_description": "API functionality used to enable LLMs to call external functions (e.g., search functions) in a structured way; used in experiments as an alternative tool-integration mechanism.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "OpenAI Function Calling",
            "system_description": "An API-level mechanism where the model can return structured function-call requests (e.g., to search PubMed) and receive structured responses; used here to implement tool-backed retrieval (labeled 2^b in Table 3) and compare against ReAct-style tool use.",
            "system_type": "Tool integration / Agent tool invocation",
            "problem_domain": "Biomedical literature retrieval to support hypothesis generation",
            "problem_description": "Allow the LLM to request specific search actions and integrate the returned structured results into hypothesis crafting and verification.",
            "problem_complexity": "Similar to ReAct: iterative retrieval and interpretation, variable number of function calls per example; external results quality variable.",
            "data_availability": "Same BHP dataset plus filtered PubMed results (post-Jan-2023 filtered out).",
            "computational_requirements": "Requires extra API calls to both LLM endpoint and search functions; no absolute compute cost reported.",
            "problem_structure": "Deterministic structured-call interface but stochastic content in search results; evaluation identical to other variants.",
            "success_metric": "BLEU/ROUGE and GPT-eval (novelty/relevance/significance/verifiability).",
            "success_rate": "Table 3 (variant 2^b) shows BLEU 15.87, ROUGE 24.94, novelty 0.57, relevance 2.58, significance 0.93, verifiability 1.89, Avg = 1.49 on the unseen test set.",
            "failure_modes": "Limited gains from tool integration; retrieved evidence sometimes not used effectively; function-calling did not yield consistent improvement in hypothesis-generation quality over baseline or multi-agent frameworks.",
            "success_factors": "Structured function calls provide clearer semantics for tool use which can simplify integration, but overall improvements depend on retrieval quality and how agents consume evidence.",
            "comparative_results": "OpenAI function-calling single-agent variant produced Avg = 1.49 (Table 3), comparable to ReAct variant but lower than multi-agent Avg = 2.09.",
            "human_baseline": "Not applicable.",
            "uuid": "e2611.2",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PubMed search tool",
            "name_full": "PubMed (external literature search engine)",
            "brief_description": "Biomedical literature search engine used by the Engineer agent to retrieve supporting documents during hypothesis generation (results filtered to exclude publications after Jan 2023).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "PubMed search integration",
            "system_description": "A retrieval tool integrated into the Engineer role to fetch relevant biomedical literature evidence to support or verify proposed hypotheses; used both via ReAct-style and function-calling tool integrations.",
            "system_type": "External literature retrieval / Tool",
            "problem_domain": "Biomedical literature retrieval to support hypothesis formation",
            "problem_description": "Retrieve and present evidence snippets relevant to the background to increase verifiability of generated hypotheses while maintaining unseen-test filtering by publication date.",
            "problem_complexity": "Large external corpus (PubMed) with many candidate documents per query; quality depends on query formulation and filtering; risk of returning document content present in model pretraining if not carefully date-filtered.",
            "data_availability": "Public biomedical literature; authors explicitly filtered results to exclude documents published after January 2023 to preserve unseen test integrity.",
            "computational_requirements": "Search latency and additional API calls; quantity depends on number of queries per example.",
            "problem_structure": "Deterministic retrieval interface but stochastic selection of returned documents; integration complexity arises from aligning retrieved evidence with generation steps.",
            "success_metric": "Impact measured indirectly via final hypothesis evaluation (BLEU/ROUGE and GPT/human scores); no stand-alone retrieval metrics reported.",
            "success_rate": "Tool use with PubMed yielded minimal net improvement for single-agent pipelines and modest improvements when used within multi-agent loops (multi-agent + tool Avg = 2.07 vs multi-agent no-tool 2.09 — effectively similar).",
            "failure_modes": "Models sometimes couldn't identify or correctly use useful evidence from PubMed results; evidence retrieval occasionally reduced novelty and did not reliably increase overall hypothesis quality.",
            "success_factors": "When retrieval is targeted and when agents are specialized (Engineer role), the framework can organize evidence more effectively; strict date filtering ensured no leakage into unseen test set.",
            "comparative_results": "PubMed-backed single-agent variants (ReAct/function calling) underperformed compared to multi-agent without tools; tool integration benefits were clearer when combined with role decomposition.",
            "human_baseline": "Not applicable.",
            "uuid": "e2611.3",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT: Browser-assisted question-answering with human feedback",
            "brief_description": "A browser-assisted question-answering system mentioned in related work that integrates web search and human feedback to improve factual QA.",
            "citation_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "mention_or_use": "mention",
            "system_name": "WebGPT",
            "system_description": "Referenced as an example of LLM tool-augmented systems that use web browsing/search to answer questions with improved factual grounding; mentioned in the paper's related work on tool-augmented LLMs.",
            "system_type": "Mentioned external system (Tool-augmented QA agent)",
            "problem_domain": "General question-answering via web retrieval (related literature; not applied in experiments)",
            "problem_description": "Not applied in this paper; cited as part of the broader landscape of LLMs integrated with search tools.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2611.4",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ToolFormer",
            "name_full": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "brief_description": "A method referenced in related work for integrating tool use into LLMs via self-supervised fine-tuning to call external APIs.",
            "citation_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "mention_or_use": "mention",
            "system_name": "ToolFormer",
            "system_description": "Mentioned in the paper as an example of approaches that fine-tune LLMs to invoke tools (e.g., calculators, search) - cited as background for why and how tool integration can be performed.",
            "system_type": "Mentioned external approach (tool-use fine-tuning)",
            "problem_domain": "General LLM tool integration (mentioned only)",
            "problem_description": null,
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2611.5",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "HuggingGPT",
            "name_full": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
            "brief_description": "Mentioned as an example of systems that orchestrate multiple AI tools/agents to solve tasks by delegating sub-tasks to specialized models or APIs.",
            "citation_title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
            "mention_or_use": "mention",
            "system_name": "HuggingGPT",
            "system_description": "Cited as related work in LLM-driven agent systems that coordinate multiple models and tool APIs to accomplish complex tasks; not directly used in experiments.",
            "system_type": "Mentioned external multi-tool orchestration system",
            "problem_domain": "General AI task solving via multi-tool orchestration (mentioned only)",
            "problem_description": null,
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2611.6",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large-language models with chemistry tools",
            "brief_description": "Cited related work demonstrating augmentation of LLMs with chemistry-specific tools to perform domain tasks, used here as an example of domain-specific tool augmentation.",
            "citation_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "mention_or_use": "mention",
            "system_name": "ChemCrow",
            "system_description": "Referenced as an example of tool-augmented LLM systems applied to chemistry tasks; included to motivate integrating domain tools into LLM-driven agents, but not used in the paper's experiments.",
            "system_type": "Mentioned external domain-specific tool augmentation system",
            "problem_domain": "Chemistry (example only; not used)",
            "problem_description": null,
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2611.7",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM-driven Autonomous Agents (LAAs)",
            "name_full": "Large Language Model-driven Autonomous Agents (LAAs)",
            "brief_description": "General class of systems combining LLMs with planning, tool use, memory and decision-making to autonomously perform complex tasks; discussed in related work and used as conceptual background.",
            "citation_title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
            "mention_or_use": "mention",
            "system_name": "LLM-driven Autonomous Agents (LAAs)",
            "system_description": "A survey-level concept referring to agents that combine LLMs with external tools, memory, and planning modules to autonomously plan and execute multi-step tasks; the paper cites LAAs as the broader category into which their multi-agent hypothesis-proposer fits.",
            "system_type": "Concept / Surveyed system type",
            "problem_domain": "General autonomous task solving across domains (surveyed; not instantiated by this paper aside from the BHP system)",
            "problem_description": null,
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2611.8",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
            "rating": 1
        },
        {
            "paper_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "rating": 1
        },
        {
            "paper_title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",
            "rating": 1
        },
        {
            "paper_title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
            "rating": 1
        }
    ],
    "cost": 0.0214665,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Zero Shot Hypothesis Proposers</h1>
<p>Biqing Qi ${ }^{1,2,3 <em>}$<br>qibiqing7@gmail.com<br>Kaiyan Zhang ${ }^{1 </em>}$<br>zhang-ky22@mails.tsinghua.edu.cn<br>Haoxiang Li ${ }^{1}$<br>kai Tian ${ }^{1}$<br>Sihang Zeng ${ }^{4}$<br>hx-li20@mails.tsinghua.edu.cn tk23@mails.tsinghua.edu.cn zengsh@uw.edu<br>Zhang-Ren Chen ${ }^{5}$ Jin-Fang Hu ${ }^{5}$！ Bowen Zhou ${ }^{1,2}$！<br>chenzhangren@ncu.edu.cn hujinfang333@126.com zhoubowen@tsinghua.edu.cn<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Frontis.AI ${ }^{3}$ Harbin Institute of Technology<br>${ }^{4}$ University of Washington ${ }^{5}$ The First Affiliated Hospital of Nanchang University</p>
<h4>Abstract</h4>
<p>Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
<h2>1 Introduction</h2>
<p>"When nothing is sure, everything is possible." - Margaret Drabble</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The pursuit of knowledge discovery stands as a cornerstone of human progress, driving innovation, and shaping our understanding of the world [30, 28]. However, in recent times, the process of knowledge discovery has encountered formidable challenges, characterized by serendipity and sluggishness. As the volume of data and literature continues to expand at an unprecedented rate, the ability to distill high-value insights and gain profound understanding from this wealth of information has become increasingly daunting[28]. Silos of information have erected themselves between disciplines, impeding the crucial cross-pollination of ideas and insights that could propel discovery to new heights. Yet, amidst these challenges, there is a glimmer of hope. The advent of large-scale models (LLMs), possessing the capacity to harness a vast reservoir of world knowledge and span multiple domains, holds promise in revolutionizing the landscape of knowledge discovery. These models present an opportunity to break down the barriers between disciplines, enabling researchers to traverse the expansive sea of information with ease and efficiency. Central to the process of knowledge discovery lies the formulation of sound hypotheses [42, 31, 1, 38]. However, a glaring gap persists in the arsenal of tools available to formally explore and evaluate hypotheses. While literature is replete with discussions on validation, it often overlooks the critical aspect of generating novel hypotheses.</p>
<p>In light of these challenges and opportunities, this paper delves into the current state of knowledge discovery, examining the hurdles posed by information explosion and disciplinary isolation. It explores the potential transformative role of LLMs in bridging these gaps, ultimately emphasizing the pivotal role of hypothesis generation in the knowledge discovery process. Furthermore, it highlights the pressing need for tools and methodologies to facilitate hypothesis generation, thus propelling knowledge discovery into a new era of efficiency and innovation [13]. Currently, both ChatGPT and GPT-4 undergo extensive pre-training on vast datasets and possess the capability of continuous updates. However, ensuring strict traceability of data sources becomes a challenging task, limiting our ability to explore zero-shot hypothesis generation.</p>
<p>The past literatures have explored scenarios of problem discovery, yet rigorous experimental designs to investigate whether LLMs can effectively propose genuine problems under zero-shot conditions remain lacking. To tackle this issue, we assemble a dataset of biomedicine literature spanning from January 2000 to September 2023. This dataset is partitioned into training and testing sets, with the training set exclusively containing literature published before January 2023. We construct an unseen test set using literature from August 2023 and ensure that the evaluated LLMs have been trained on corpora before that date. Additionally, we devise a multi-intelligent collaborative framework that incorporates search tools and role-playing to delve deeper into and uncover the potential for hypothesis generation.</p>
<p>Through experiments and analyses as shown in Figure 1, we draw the following findings: 1) LLMs surprisingly generate hypotheses that are untrained yet validated when tested against literature. 2) Increasing uncertainty levels can benefit by diversifying candidate generation and potentially enhancing zero-shot hypothesis generation capabilities. For instance, introducing heightened uncertainty through collaborative multi-agent approaches significantly improves the model’s ability to generalize in zero-shot scenarios. However, integrating subsequent few-shot enhancements and using additional tools may reduce the model’s proficiency in generating hypotheses. This phenomenon is likely due to the reduction of uncertainty, limiting the model’s space for hypothesis generation. Consequently, it lacks consistent positive effects, underscoring the need for careful consideration of the type of external knowledge employed. The above findings also support the notion: "When nothing is sure, everything is possible." Specifically, our contributions are as follows:</p>
<p>1) To rigorously validate the zero-shot and few-shot hypothesis generation potential of LLMs, we construct temporal biomedical instruction data and devised novel and effective experiments for</p>
<p>comprehensive analysis and evaluation. To the best of our knowledge, this is the first work that formally designs experiments to investigate the zero shot hypothesis generation capacity of LLMs.
2) Through validation across different models and various scenario dimensions, we surprisingly find that LLMs possess rudimentary higher-order knowledge reasoning capabilities and can propose new hypothesis statements. This provides new empirical insights and pathways for knowledge discovery.
3) For a comprehensive review of the generated hypotheses, we design metrics across four dimensions for both ChatGPT-based and human evaluations. The correlation scores between ChatGPT evaluations and manual results indicate that LLMs also play a significant role in hypothesis evaluations.
4) To efficiently explore and further harness the capability of hypothesis generation, we introduce a multi-agent system based on LLMs. Through efficient collaboration among multiple models and tool utilization, we analyze the factors influencing hypothesis generation by LLMs.</p>
<h1>2 Process of Scientific Discovery</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The iterative experimental loop of scientific discovery: observations and data accumulated from past experiments are analyzed and used to generate new hypotheses, and in turn new experiments that will yield new data to continue to cycle. In this paper, we mainly focus on investigating whether LLMs have the zero shot generalization ability to generate new hypotheses.</p>
<p>Scientific discovery involves key components, each crucial for advancing our understanding of the natural world: data analysis, hypothesis formulation, experiment design, execution, and observation and reflection [13] as shown in Figure 2.</p>
<p>1) Data Analysis: Foundational in the scientific process, it entails collecting and examining data to discern patterns and anomalies, extracting insights through statistical techniques and visualization. It initiates scientific inquiry, guiding further exploration. 2) Generating Hypotheses: Among these components, hypothesis formulation is pivotal. It entails crafting informed guesses to explain observed phenomena. Hypotheses serve as guiding frameworks, directing and focusing research by articulating specific relationships and outcomes for experimental exploration. 3) Experiment Design: Once a hypothesis is set, designing experiments becomes essential to rigorously test its validity. This involves defining variables, specifying control groups, and outlining methods and procedures. Well-designed experiments ensure objective hypothesis testing and yield meaningful, informative results. 4) Experiment Execution: Meticulous execution of designed experiments and data collection are critical. Researchers adhere precisely to experimental protocols, recording observations, measurements, and unexpected findings. Integrity in execution ensures reliable, reproducible outcomes. 5) Accumulating Observations: After experiments, scientists engage in observation and reflection. They analyze collected data to determine if results support or refute the initial hypothesis. If unsupported, hypotheses may be revised or new ones formulated based on findings. Observation and reflection permit iterative refinement of scientific understanding.
Hypothesis Pioneers Pathways: Guiding Knowledge Discovery. While all components are essential, hypothesis formulation holds a unique position. It drives the scientific endeavor, guiding research question selection, experiment design, and data analysis. Well-constructed hypotheses not only provide direction but also lay the foundation for meaningful scientific discoveries by posing rigorously testable questions. Hypothesis formulation serves as the intellectual anchor steering scientific investigation and ultimately advancing knowledge.</p>
<p>3 Can LLMs Truly Generate Zero-Shot Hypotheses?</p>
<p>In this section, we outline the methodology employed for a thorough assessment of LLMs’ capacity to generate hypotheses under zero-shot conditions. To accomplish this, we begin by defining the problem of hypothesis generation in zero-shot settings. Next, we elucidate the process of dataset construction within the biomedical domain. Finally, we undertake comprehensive experiments to evaluate various instructed models across multiple dimensions, aiming to explore the factors influencing the ability of LLMs to propose improved hypotheses.</p>
<h3>3.1 Problem Definition</h3>
<p>Following the scientific discovery process outlined in Section 2, hypothesis generation typically occurs after thorough literature analysis and examination of specific phenomena. To enhance evaluation effectiveness, we formalize this process as a text completion task. Given dataset $D$, an instruction $I$, and text pairs $(X_{i},Y_{i})<em i="i">{i=1}^{n}$ containing background knowledge and corresponding hypotheses, extracted from medical papers, our objective is to assess model $M$ by having it generate hypotheses based on the task instruction and background knowledge, i.e., $M(I,X</em>$. The objective function is formulated as:})=Y_{i}$, for each $i$ $\in{1,..,,n</p>
<p>$y^{*}=\underset{y_{1},...,y_{n}}{\arg\max}\prod_{t=1}^{n}{P(y_{t}|y_{1},...,y_{t-1},I,X)}.$</p>
<h3>3.2 Dataset Construction</h3>
<p>In this section, we detail the process of constructing datasets and ensuring the robustness of our evaluation. Prevalent LLMs, like Llama and ChatGPT, face challenges in tracing the origin of their knowledge due to continuous self-updating. To address this, we propose a novel approach to assess LLMs’ hypothesis generation. Recognizing their potential impact on public domain data, we reconstruct a new biomedical literature dataset based on publication dates.</p>
<p>As depicted in Figure 3, we designated the year 2023 as the cut-off point. Our training dataset comprises literature published before January 2023, while the test dataset comprises literature published after January 2023, forming pairs of data with background knowledge and hypothesis proposals. Due to the emergence of more advanced LLMs, our evaluations focus exclusively on the unseen test set, featuring literature published in August 2023. We selected instructed models fine-tuned before August 2023 for both evaluation and fine-tuning testing. In our experimental setup, we implemented stringent measures to ensure the models had no prior exposure to the test data, affirming the validity of our experiments. We strictly follow the standard pipeline as outlined in Self-Instruct [32] for our data generation process, encompassing four key steps: 1) Compose the paper set based on the topic and content of the literature. 2) Utilize chatgpt-turbo-3.5 to summarize the literature knowledge. 3) Generate background knowledge-assume pairs. 4) Filter low-quality data. 5) Split the dataset according to publication time.</p>
<h3>3.3 Datast Analysis</h3>
<p>In this section, we provide a comprehensive overview of the constructed dataset, encompassing details about the data acquisition strategy, dataset size, visibility control measures, distribution by year and month, as well as topic distribution.</p>
<p>We have created two datasets to maintain control over the visibility of hypotheses: 1) Seen dataset This dataset comprises 2700 background and hypothesis pairs sourced from literature published before January 2023. This dataset was partitioned into training (2500) and validation (200) subsets (as well as seen test set). It is consistent with the corpus that the LLMs have been exposed to. 2) Unseen dataset The unseen dataset consists of 200 pairs extracted from papers published in August 2023, which the LLMs have not encountered during training and are used for testing purposes.</p>
<p>We also provide publication date and topic distribution of constructed dataset in Appendix B.1.</p>
<h1>3.4 Experiment Setup</h1>
<p>In this section, we introduce experimental settings for hypothesis generation and evaluation.
Models For a fair comparison, we exclusively evaluate LLMs trained on corpora before March 2023 to ensure the test set remains unseen. We consider three categories of models in total: 1) API-based LLMs: this is mainly ChatGPT. 2) General domain instructed LLMs: These models consist of open-source models that have undergone fine-tuning based on Llama using general domain instructions. We primarily choose the top-tier models based on their performance rankings on the Alpaca Eval Leaderboard ${ }^{3}$. 3) Specific domain instructed LLMs: These include PMC-LLaMA [35], and MedAlpaca [10]. These models are trained on a variety of sources in medicine domain, such as medical books, PMC papers, medical dialogs, and others. We provide detailed meta-information for various models, including their training data sources and publication dates, in Appendix B.2.
Prompts To ensure a consistent output format across different models, we create prompts in two formats: zero-shot and few-shot examples. In our experiments, we adopt a 5-shot format, selecting examples from the training set before January 2023 using both randomly sampled and similarity retrieval methods. We provide illustrations of zero-shot and few-shot prompts in Appendix E.
Finetuning To assess the hypothesis generation capability beyond zero-shot scenarios, we identify the top-performing open-source models through few-shot evaluation. We proceed to fine-tune the full parameters of WizardLM-13B-V1.2 with the background and hypothesis pairs. The fine-tuning process consists of three epochs, employing a batch size of 8 , a maximum sequence length of 2048 tokens, and a learning rate set at $3 \mathrm{e}-5$. We implement early stopping and select the best checkpoints based on their performance on the seen test dataset.
Evaluation Given the disparities between the hypothesis generation task and traditional text generation tasks liking machine translation and summarization, with the former being more challenging and often involving uncertainty that extends beyond established ground truth, we approach our evaluation from two primary perspectives: conducting evaluations with and without golden hypotheses. In evaluations with golden hypotheses, we employ standard text generation metrics, including BLEU and ROUGE in evaluate library ${ }^{4}$, to assess word overlap between the generated outputs and the ground truth. The vastness of the hypothesis space renders it difficult to comprehensively assess the quality of generated hypotheses using word overlap metrics alone. To provide a more comprehensive evaluation of the generated hypotheses from multiple facets, we have thoughtfully devised four metrics: novelty, relevance, significance, and verifiability. Inspired by recent research that highlights ChatGPT as proficient annotators [8, 16], demonstrating a strong correlation with human ratings, we employ ChatGPT for further evaluation. In detail, we request ChatGPT to evaluate both the generated scientific hypotheses and the provided background across these aspects. The scoring scale ranges from 0 to 3 , where a higher score indicates superior results. Additionally, we solicit ChatGPT to furnish a step-by-step explanation to substantiate the assigned score. Moreover, we conduct human evaluation based on the four metrics for the top-tier models identified in the automatic evaluation in Section 3.5, and we provide a detailed description of this process in Section 3.6.</p>
<h3>3.5 Experiment Results</h3>
<p>This section presents the results of hypothesis generation across various models in both zero-shot and few-shot settings. We primarily analyze the results from two perspectives: the impact of the zero-shot setting and the influence of introducing external knowledge on hypothesis generation.</p>
<h3>3.5.1 Impact of zero-shot settings</h3>
<p>The results presented in Table 1 demonstrate the significant impact of zero-shot settings in improving hypothesis generation, particularly in terms of fostering high novelty. We analyze these results from two key perspectives as following.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results of various LLMs: We assess instructed models using zero-shot and few-shot format prompts to generate constrained outputs. To provide a comprehensive assessment, we calculate the average scores for novelty, relevance, significance, and verifiability, denoted as Avg. Results marked with an asterisk (*) indicate that the few-shot prompts are constructed by retrieving samples from the training set that are similar to the background of inputs. To facilitate better comparison, we highlight the highest and sub-high score with both bold and underline formatting under each category.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>Seen</th>
<th></th>
<th></th>
<th></th>
<th>Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>BLEU</td>
<td>ROUGE</td>
<td>BLEU</td>
<td>ROUGE</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbot(8-shot)</td>
<td>13.93</td>
<td>25.32</td>
<td>15.52</td>
<td>26.48</td>
<td>1.42</td>
<td>2.63</td>
<td>1.58</td>
<td>1.97</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)</td>
<td>16.47</td>
<td>27.07</td>
<td>16.49</td>
<td>26.96</td>
<td>1.22</td>
<td>2.57</td>
<td>1.84</td>
<td>2.03</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)*</td>
<td>17.33</td>
<td>27.28</td>
<td>17.71</td>
<td>27.53</td>
<td>1.02</td>
<td>2.61</td>
<td>1.85</td>
<td>2.36</td>
</tr>
<tr>
<td>General</td>
<td>Vicuna-33b-v1.3(0-shot)</td>
<td>13.97</td>
<td>24.43</td>
<td>13.66</td>
<td>23.43</td>
<td>1.67</td>
<td>2.55</td>
<td>2.04</td>
<td>1.84</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)</td>
<td>11.23</td>
<td>22.54</td>
<td>11.49</td>
<td>22.68</td>
<td>1.60</td>
<td>2.40</td>
<td>1.67</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)*</td>
<td>12.78</td>
<td>24.11</td>
<td>13.12</td>
<td>23.66</td>
<td>1.19</td>
<td>2.71</td>
<td>2.00</td>
<td>2.17</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>10.95</td>
<td>21.56</td>
<td>11.44</td>
<td>22.04</td>
<td>1.86</td>
<td>2.41</td>
<td>1.91</td>
<td>1.98</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>8.17</td>
<td>21.09</td>
<td>7.63</td>
<td>20.70</td>
<td>1.95</td>
<td>2.38</td>
<td>2.06</td>
<td>2.22</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>8.40</td>
<td>21.65</td>
<td>9.66</td>
<td>22.43</td>
<td>1.43</td>
<td>2.50</td>
<td>1.94</td>
<td>2.15</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(0-shot)</td>
<td>11.91</td>
<td>23.35</td>
<td>12.03</td>
<td>23.55</td>
<td>1.62</td>
<td>2.55</td>
<td>1.90</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)</td>
<td>14.00</td>
<td>24.30</td>
<td>13.82</td>
<td>24.38</td>
<td>1.33</td>
<td>2.54</td>
<td>1.81</td>
<td>2.23</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)*</td>
<td>14.96</td>
<td>25.66</td>
<td>15.26</td>
<td>25.78</td>
<td>1.06</td>
<td>2.64</td>
<td>1.73</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>13.45</td>
<td>24.12</td>
<td>14.25</td>
<td>25.05</td>
<td>1.57</td>
<td>2.45</td>
<td>1.74</td>
<td>1.89</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>14.04</td>
<td>24.59</td>
<td>13.78</td>
<td>24.28</td>
<td>1.17</td>
<td>2.61</td>
<td>2.12</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)*</td>
<td>14.46</td>
<td>24.78</td>
<td>15.26</td>
<td>25.56</td>
<td>0.97</td>
<td>2.67</td>
<td>1.85</td>
<td>1.99</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(0-shot)</td>
<td>8.79</td>
<td>22.71</td>
<td>8.38</td>
<td>21.48</td>
<td>1.58</td>
<td>2.51</td>
<td>1.70</td>
<td>2.05</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)</td>
<td>12.46</td>
<td>23.60</td>
<td>12.58</td>
<td>24.21</td>
<td>1.06</td>
<td>2.64</td>
<td>2.09</td>
<td>2.20</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)*</td>
<td>12.37</td>
<td>23.93</td>
<td>12.88</td>
<td>24.78</td>
<td>1.16</td>
<td>2.76</td>
<td>2.10</td>
<td>2.23</td>
</tr>
<tr>
<td>Medicine</td>
<td>MedAlpaca-13B(0-shot)</td>
<td>6.10</td>
<td>22.07</td>
<td>5.82</td>
<td>20.49</td>
<td>0.55</td>
<td>1.17</td>
<td>1.17</td>
<td>1.06</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)</td>
<td>0.99</td>
<td>3.84</td>
<td>1.08</td>
<td>3.84</td>
<td>0.98</td>
<td>1.32</td>
<td>1.32</td>
<td>1.49</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)*</td>
<td>4.60</td>
<td>9.36</td>
<td>4.50</td>
<td>9.07</td>
<td>1.09</td>
<td>1.40</td>
<td>1.20</td>
<td>1.53</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>22.89</td>
<td>40.36</td>
<td>22.37</td>
<td>40.45</td>
<td>0.76</td>
<td>1.94</td>
<td>1.42</td>
<td>1.52</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)</td>
<td>1.36</td>
<td>4.83</td>
<td>1.41</td>
<td>4.78</td>
<td>1.13</td>
<td>1.45</td>
<td>1.26</td>
<td>0.88</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>6.21</td>
<td>12.39</td>
<td>6.16</td>
<td>12.13</td>
<td>1.73</td>
<td>2.17</td>
<td>1.88</td>
<td>2.09</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13B-V1.2</td>
<td>19.13</td>
<td>27.35</td>
<td>19.73</td>
<td>27.58</td>
<td>0.97</td>
<td>2.55</td>
<td>1.38</td>
<td>2.26</td>
</tr>
</tbody>
</table>
<p>Zero-shot Outperforms Few-shot. Our findings indicate that, for extra large models like Llama-2-70b-chat and WizardLM-70B-V1.0, zero-shot performance surpasses that of the fewshot setting, where few-shot examples are obtained by randomly sampling. This suggests that the capacity of hypothesis generation is limited by the inclusion of few-shot examples, and models exhibit stronger abilities in a zero-shot setting.
Outperforming the Unseen Compared to the Seen Test Set. Despite the visibility of literature published before 2022 in the pre-training corpus of most LLMs, we have categorized the test set into "seen" and "unseen." Typically, LLMs may excel in the "seen" test set due to the potential memorization of hypotheses present in the pre-training corpus, resulting in higher performance compared to the "unseen" test set. However, our results indicate that LLMs tend to perform better on the "unseen" test set. We speculate that this is because the complexity of hypothesis generation may hinder LLMs from effectively leveraging the dark knowledge in their parameters.</p>
<h1>3.5.2 Influence of external knowledge</h1>
<p>Based on the results, we observe that the introduction of external knowledge, such as few-shot examples, domain adaptation, and instruction fine-tuning, does not consistently enhance the ability of hypothesis proposing.
Few-Shot Examples Enhance Verifiability but Decrease Novelty. In comparison to zero-shot settings, models using few-shot prompts benefit from the provided examples, resulting in very high matching rates. Regarding word overlap metrics, including BLEU and ROUGE, most models, especially WizardLM series models, and Openchat-v3.2-super, show improved performance when provided with in-context examples, with retrieved examples being particularly beneficial for their generations. However, it's important to note that these few-shot prompts significantly increase verifiability while simultaneously leading to lower levels of novelty compared to zero-shot results.
Randomly Sampled Few-Shot Examples vs. Similarity Retrieval. Given that randomly sampled in-context examples often differ from the provided background in terms of topics or domains, this can potentially confuse LLMs and lead to decreased performance. In our pursuit of further exploration into the hypothesis generation capabilities of LLMs, we retrieve examples from the training dataset based on their similarity to the given background. The results indicate that similarity retrieval can further enhance performance.</p>
<p>Instruction Tuning Enhances LLM Performance. Following fine-tuning on a dataset comprising background and hypothesis pairs, WizardLM-13B-V1.2 attains superior performance, surpassing even gpt-3.5-turbo and WizardLM-70B-V1.0. This finding underscores that domain adaptation remains a valuable approach to enhance the hypothesis generation capabilities of LLMs. It not only leads to greater resource efficiency but also supports the promotion of privacy in a localized context.
Impact of Domain Adaptation on Hypothesis Generation. We have also conducted an analysis of the influence of fine-tuning for domain adaptation on hypothesis generation. In this comparison, we utilize instructed models adapted to the field of medicine. The results obtained from MedAplaca and PMC-LLaMA indicate that domain adaptation can significantly improve word overlap performance. However, the metrics derived from ChatGPT suggest that domain adaptation has only a minimal effect on hypothesis generation. This discrepancy between word overlap metrics and ChatGPT's evaluation highlights the need for more comprehensive and unified metrics in the context of hypothesis generation tasks.</p>
<h1>3.6 Human Evaluation and Case Study</h1>
<p>In this section, we conduct a human evaluation to assess the generated hypotheses and calculate coherence scores to compare them with ChatGPT evaluation scores, guiding further evaluation efforts.</p>
<h3>3.6.1 Evaluation Settings</h3>
<p>Evaluation Metrics To comprehensively evaluate the generations manually and simultaneously assess the quality of ChatGPT's evaluations, we continue to utilize the four metrics outlined in Section 3.4, which encompass novelty, relevance, significance, and verifiability. The range of each metric remains from 0 to 3 , with higher values indicating better performance. Additionally, we calculate coherence scores between human evaluations and ChatGPT evaluations.
Selection of Models Given the constraints associated with the cost of human evaluation, our primary objective is to assess whether LLMs can produce valuable hypotheses, rather than striving for state-of-the-art performance. As a result, we exclusively perform human evaluation on the outputs generated by the LLM that ranks highest in performance based on automatic metrics and ChatGPT evaluation. Furthermore, we aim to encompass a variety of prompts and models in our evaluation. The final models selected for human evaluation are detailed in Table 2.
Evaluation Details We randomly selected 100 examples from the unseen test set and had three evaluators with a biomedical background assign scores for each metric to each model.</p>
<h3>3.6.2 Evaluation Results</h3>
<p>As depicted in Table 2, the human evaluations exhibit a strong correlation with ChatGPT's evaluations, with Pearson and Spearman coefficients exceeding 0.7 for all models. These results strongly support our earlier findings regarding the influence of zero-shot learning and external knowledge. This reinforces our assertion that large language models can effectively propose hypotheses and significantly contribute to scientific discovery. For additional insights, we present correlation coefficients between word overlap scores and manual scores in the Appendix C, revealing lower coherence and highlighting the need for more advanced evaluation metrics.
We also conduct a case study that showcases the hypotheses generated by various models and includes examples of step-by-step evaluations by ChatGPT. Details can be found in Appendix D.</p>
<h2>4 Can agent collaboration enhance LLMs' zero-shot generalization?</h2>
<p>In this section, we will strive to enhance the ability of LLMs in hypothesis generation through multi-agent collaboration and the use of tools. Our objective is to improve hypothesis efficiency by employing multi-agent collaboration, simulating real-world research scenarios. To begin, we introduce the conceptual system of multi-agent collaboration for hypothesis generation, drawing inspiration from scientific research. Subsequently, we present the role design and the tools use in this context. Finally, we present preliminary validated results of the multi-agent system using our proposed BHP dataset.</p>
<p>Table 2: This table presents the results of human evaluation. The Avg Coefficient are used to assess the correlation between the average scores obtained from ChatGPT and those from human evaluation.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>ChatGPT</th>
<th>Human Eval</th>
<th></th>
<th></th>
<th></th>
<th>Avg Coefficient</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Eval.Avg</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
<td>Avg</td>
<td>Pearson</td>
<td>Spearman</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbo(0-shot)</td>
<td>1.90</td>
<td>1.54</td>
<td>2.69</td>
<td>1.77</td>
<td>2.08</td>
<td>2.02</td>
<td>0.87</td>
<td>0.78</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbo(5-shot)*</td>
<td>1.96</td>
<td>1.31</td>
<td>2.62</td>
<td>2.08</td>
<td>2.62</td>
<td>2.15</td>
<td>0.80</td>
<td>0.78</td>
</tr>
<tr>
<td>General</td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>2.04</td>
<td>1.77</td>
<td>2.23</td>
<td>1.92</td>
<td>1.92</td>
<td>1.96</td>
<td>0.89</td>
<td>0.84</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>2.20</td>
<td>2.15</td>
<td>2.77</td>
<td>2.08</td>
<td>2.31</td>
<td>2.33</td>
<td>0.96</td>
<td>0.90</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>2.01</td>
<td>1.38</td>
<td>2.62</td>
<td>2.31</td>
<td>2.00</td>
<td>2.08</td>
<td>0.97</td>
<td>0.94</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>1.91</td>
<td>1.38</td>
<td>2.31</td>
<td>1.54</td>
<td>2.00</td>
<td>1.81</td>
<td>0.90</td>
<td>0.75</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>2.01</td>
<td>1.15</td>
<td>2.69</td>
<td>2.46</td>
<td>1.77</td>
<td>2.02</td>
<td>0.85</td>
<td>0.89</td>
</tr>
<tr>
<td>Medicine</td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>1.41</td>
<td>1.00</td>
<td>2.62</td>
<td>1.92</td>
<td>2.00</td>
<td>1.88</td>
<td>0.73</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>1.97</td>
<td>1.85</td>
<td>2.23</td>
<td>1.92</td>
<td>1.69</td>
<td>1.92</td>
<td>0.95</td>
<td>0.94</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13BV1.2</td>
<td>1.79</td>
<td>0.85</td>
<td>2.77</td>
<td>1.23</td>
<td>2.23</td>
<td>1.77</td>
<td>0.83</td>
<td>0.85</td>
</tr>
</tbody>
</table>
<h1>4.1 Multi-agent Framework</h1>
<p>Inspired by the structured methodology detailed in Section 2, we introduce a comprehensive framework tailored for hypothesis formulation. This framework encapsulates a multi-agent system where each agent assumes a distinct role, mirroring the collaborative nature of scientific endeavors. Through a symbiotic and iterative process, these agents collaborate to craft hypotheses that are not only grounded in existing knowledge but also pave the way for novel insights. By emulating the essence of scientific discovery, our framework strives to produce hypotheses that are both innovative and scientifically robust. As depicted in Figure 4, we have partitioned the framework into five components, encompassing four automated agents and the option for human involvement within the loop.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: The conceptual system of multi-agent collaboration for hypothesis generation. The overall prototyping process is illustrated below, allowing users to choose optional involvement. We offer core role descriptions of multi-agents and the fully automated system above.</p>
<p>Role Design In our proposed multi-agent framework, each component plays a distinct and pivotal role. The Analyst serves as the foundation, meticulously extracting and defining core elements from the research background. Its primary objective is to interpret the literature, distilling it into keywords or topics that subsequently guide the Engineer's search efforts. The Engineer, leveraging these keywords, embarks on a mission to retrieve and organize pertinent information. They meticulously plan and execute detailed searches, ensuring that the findings are compiled in a structured manner. This organized materials then lands in the domain of the Scientist, whose objective is to weave together the Engineer's findings with the original research background. Through careful interpretation, the Scientist crafts a hypothesis that is both grounded in existing knowledge and offers a fresh perspective. However, before this hypothesis is finalized, it undergoes scrutiny by the Critic. The Critic's role</p>
<p>is paramount in ensuring the hypothesis's robustness, coherence, and novelty. They evaluate the hypothesis against the backdrop of the research background, ensuring it stands up to academic rigor. Feedback from the Critic, if necessary, loops back to refine the hypothesis or prompts the Analyst for further insights, creating a cyclical and iterative process of refinement.</p>
<p>Tool Use To explore external knowledge beyond the inherent dark knowledge within LLMs, we integrate the Engineer agent with search engines, mainly PubMed ${ }^{5}$. Similarly, to control the visibility of the unseen test dataset, we filter and exclude literature published after January 2023 from the search results. We carry out tool use experiments using ReAct [40] and OpenAI function calling. ReAct is a method that extends the concept of Chain of Thought (CoT) [34], involving thinking before taking action and subsequently making observations based on feedback from the environment. In our experiments, we instruct the LLMs to initially contemplate the provided background information and then make a decision regarding whether to utilize tools. Upon receiving feedback from the tools, the LLMs are expected to identify supporting evidence in the results or potentially make further tool requests. The LLMs are responsible for concluding the hypothesis generation process and summarizing the hypotheses independently. In the case of OpenAI function calling, we directly specify tools for publication searching and transmit them to OpenAI APIs. This process is roughly implemented through fine-tuning, as described in ToolFormer [24].</p>
<h1>4.2 Experiment Results</h1>
<p>Our primary focus is to investigate the impact of tool use and multi-agent collaboration on hypothesis generation. We present the experimental results in Table 3. Based on the results, we summarize our findings from two perspectives: tool use and role-playing.</p>
<p>Influence of Tool Use Based on our results, we observe that tool use has minimal impact on improving the hypothesis generation ability of LLMs. This observation aligns with the findings presented in the previous sections regarding the analysis of external knowledge. Notably, the ReAct-based method performs worse than OpenAI function calling. It is also evident that LLMs struggle to identify useful information and exhibit weaknesses in the thought-action-observation process, even when utilizing the official interface from OpenAI. Hypothesis generation is indeed a challenging task that necessitates iterative discussions and the exchange of ideas among various individuals.</p>
<p>Multi-Agent Collaboration In addition to tool use, our findings suggest that the division of labor and interaction among multi-agents can significantly enhance the model's capability to propose hypotheses by introducing uncertainty. This mirrors the dynamics of real-world scientific research, where hypotheses are formulated through iterative discussions and refutations. Additionally, it is worth noting that tool use can further enhance the performance of the multi-agent framework.</p>
<p>Table 3: Results of individual agents and multi-agent systems, both with and without the use of tools, on the unseen test dataset. The results demonstrate that both multi-agent systems and the utilization of tools enhance the ability of LLMs in hypothesis generation. Among the various types of models, both $2^{a}$ and $2^{b}$ are evaluated with tool use. The difference between them lies in their implementations: ReAct [40] and OpenAI function calling ${ }^{6}$, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Influence Factor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Automatic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4 Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-agent</td>
<td style="text-align: center;">Tool use</td>
<td style="text-align: center;">BLUE</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">Novelty</td>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">Significance</td>
<td style="text-align: center;">Verifiability</td>
<td style="text-align: center;">Avg</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.52</td>
<td style="text-align: center;">26.48</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">1.92</td>
</tr>
<tr>
<td style="text-align: center;">$2^{a}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">14.94</td>
<td style="text-align: center;">24.16</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: center;">$2^{b}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">15.87</td>
<td style="text-align: center;">24.94</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">1.49</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.71</td>
<td style="text-align: center;">22.11</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.10</td>
<td style="text-align: center;">2.09</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">22.04</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.07</td>
</tr>
</tbody>
</table>
<h2>5 Conclusion</h2>
<p>From the hypothesis-proposer perspective, we investigated LLMs' zero-shot generalisation ability in scientific research. Specifically, we first build a comprehensive corpus based on biomedical literature,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>split by publication date, including background knowledge and hypothesis pairs. This corpus is then used as a basis for fine-tuning LLMs, leading to the generation of the LLM. To further analysis and enhance the capabilities of the hypothesis proposer, we introduce a LLM-based multi-agent collaboration system. Experimental results show that fine-tuned LLMs of various sizes can propose new hypotheses that did not appear in the training data but can be confirmed by the test literature, with performance comparable to ChatGPT and in some cases even better. Notably, our study revealed that introducing uncertainty into processes and operations enhances zero-shot generalization capabilities. These findings confirm the potential of LLMs to propose new hypotheses and offers hope for future unlocked scientific discovery. In future work, we will focus on optimizing models and generating hypotheses guided by effective uncertainty assessment metrics.</p>
<h1>Acknowledgements</h1>
<p>We extend our gratitude to the anonymous reviewers for their insightful feedback.</p>
<h2>References</h2>
<p>[1] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.
[2] Andres M. Bran, Sam Cox, Andrew D. White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools, June 2023. arXiv:2304.05376.
[3] Boxi Cao, Hongyu Lin, Xianpei Han, and Le Sun. The Life Cycle of Knowledge in Big Language Models: A Survey, March 2023. arXiv:2303.07616 [cs].
[4] Zhuo Chang, Jing Zhang, Yilun Liu, Huajian Gao, and Guang-Kui Xu. New Mechanical Markers for Tracking the Progression of Myocardial Infarction. Nano Letters, 23(16):7350-7357, August 2023.
[5] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking Before Action: Gather Information in Embodied Decision Making with Language Models, May 2023. arXiv:2305.15695 [cs].
[6] Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Empowering Private Tutoring by Chaining Large Language Models, September 2023. arXiv:2309.08112 [cs] version: 1.
[7] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback, May 2023. arXiv:2305.10142 [cs].
[8] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, July 2023. arXiv:2303.15056 [cs].
[9] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. MindAgent: Emergent Gaming Interaction, September 2023. arXiv:2309.09971 [cs] version: 1.
[10] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno K Bressem. Medalpaca-an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023.
[11] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework, August 2023. arXiv:2308.00352 [cs].
[12] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory, June 2023. arXiv:2306.03901 [cs].</p>
<p>[13] Moksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. Digital Discovery, 2(3):557-577, 2023.
[14] Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, and Jie Fu. Think Before You Act: Decision Transformers with Internal Working Memory, May 2023. 0 citations (Semantic Scholar/arXiv) [2023-05-30] arXiv:2305.16338 [cs].
[15] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society, March 2023. arXiv:2303.17760 [cs].
[16] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, May 2023. arXiv:2303.16634 [cs].
[17] Philipp Maas, Frank Carey, Chris Wheeler, Edward Saatchi, Pete Billington, and Jessica Yaffa Shamash. SHOW-1 and Showrunner Agents in Multi-Agent Simulations. arXiv preprint, 2023.
[18] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, June 2022. arXiv:2112.09332.
[19] Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: Tool Augmented Language Models, May 2022. arXiv:2205.12255 [cs].
[20] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative Agents: Interactive Simulacra of Human Behavior, August 2023. arXiv:2304.03442 [cs].
[21] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large Language Model Connected with Massive APIs, May 2023. arXiv:2305.15334 [cs].
[22] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative Agents for Software Development, August 2023. arXiv:2307.07924 [cs].
[23] Vipula Rawte, Amit Sheth, and Amitava Das. A Survey of Hallucination in Large Foundation Models, September 2023. arXiv:2309.05922 [cs].
[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools, February 2023. arXiv:2302.04761.
[25] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, May 2023. arXiv:2303.17580 [cs].
[26] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, June 2023. arXiv:2303.11366 [cs].
[27] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs, June 2023. arXiv:2306.06624 [cs].
[28] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[29] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the Planning Abilities of Large Language Models - A Critical Investigation, May 2023. arXiv:2305.15771 [cs].</p>
<p>[30] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, 2023.
[31] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Learning to generate novel scientific directions with contextualized literature-based discovery. arXiv preprint arXiv:2305.14259, 2023.
[32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January 2023. arXiv:2201.11903 [cs].
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
[35] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmcllama: Towards building open-source language models for medicine, 2023.
[36] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The Rise and Potential of Large Language Model Based Agents: A Survey, September 2023. arXiv:2309.07864 [cs].
[37] Hui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions, June 2023. arXiv:2306.02224.
[38] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023.
[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, May 2023. arXiv:2305.10601 [cs].
[40] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models, March 2023. arXiv:2210.03629.
[41] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models, September 2023. arXiv:2309.01219 [cs].
[42] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. arXiv preprint arXiv:2302.14233, 2023.
[43] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory, June 2023. arXiv:2305.17144 [cs].
[44] Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. Pad: Program-aided distillation specializes large models in reasoning. arXiv preprint arXiv:2305.13888, 2023.</p>
<h1>A Related Works</h1>
<h2>A. 1 Data-Driven Scientific Discovery</h2>
<p>Data-driven knowledge discovery research within LLM is relatively limited, with the current focus primarily on dataset construction and task-driven design. In this context, [42] proposed a dataset for investigating the transition from goals to discoveries. However, it should be noted that accurate discoveries within this dataset are not recent. [31] introduced a method for automatically collecting and constructing publication data, along with a proposal for a hypothesis generation approach in the natural language processing (NLP) domain. However, this method requires prior human knowledge, explicit context, and is not an automated process. It's worth noting that their data was constructed from literature before 2021 from the ACL collection, implying that the information may already exist in open-source models like chatGPT and LLAMA. Furthermore, [31] focused on integrating computational tools in the field of chemistry, primarily analyzing the capabilities of LLMs in using integrated tools but neglecting the ability for zero-shot generalization in chemistry reactions. [1] delved more into the abilities of LLMs regarding planning and conducting experiments but did not consider proposing new hypotheses. [38] introduced a new task for open-domain hypothesis induction and created a dataset comprising 50 articles from social science journals. Additionally, they developed a multi-module system for exploring feedback mechanisms. However, all of the above-mentioned literature lacks strict guarantees on the visibility of test data to models, thereby limiting our exploration of the zero-shot generalization capability of LLMs through learning from existing knowledge to propose new hypothesis. Unlike existing works, we have designed datasets based on publication dates, which can easily ensure a strict independence between test data and LLMs.</p>
<h2>A. 2 LLM-driven Autonomous Agents</h2>
<p>Large language models demonstrate exceptional capabilities in tasks such as question answering, program coding, and instruction following. However, they still confront significant challenges related to factual hallucination [41, 23], knowledge outdated [3], and interactions with real-world. To address these challenges, recent research has explored enhancing LLMs by incorporating tools such as search engines [18, 19], calculators [24], code interpreter [44], RESTful APIs [27, 21] and others. The integration of LLMs with tool use, also known as LLM-driven autonomous agents (LAAs), has attracted substantial public attention. These agents are equipped with reasoning [33, 39], planning [25, 29], decision-making [37, 14, 5], and long-term memory capabilities [43, 12], and they are constructed upon the foundation of LLMs. LAAs can autonomously plan sub-goals for complex tasks, execute actions, obtain feedback from the environment, and adjust their behaviors to adapt [40, 36, 26]. LAAs have demonstrated significant potential in addressing complex real-world tasks, including software development [22, 11], drama creation [17], course design [6], chemistry experiments [2] and more. Furthermore, multi-agent collaboration plays a significant role in LAA applications, allowing agents to collaborate and interact to solve problems through various roleplaying scenarios [20, 7, 9, 15]. To the best of our knowledge, there is still a dearth of exploration regarding the use of agents, particularly multi-agents, for scientific discovery. In this paper, our objective is to undertake a preliminary effort to enhance the hypothesis proposing capability of LLMs by harnessing tools and multiple agents, along with conducting an analysis of influencing factors.</p>
<h2>B Implementation Details</h2>
<p>In this section, we delve into further implementation details of our experiments, including information about the constructed dataset and open-source models.</p>
<h2>B. 1 Details of Dataset</h2>
<p>Distribution of Training and Test Sets. We present the publication dates and topic distributions of the various datasets for comparison, as illustrated in Figure 5, where we utilize Nomic Atlas ${ }^{7}$ to visualize the topic distribution of abstracts in both the training and test datasets.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Distribution of the background and hypothesis pairs (BHP) dataset: In the left panel, we present the publication distribution by year for the training and seen test datasets, indicating a steady increase year by year until January 2023. In the center panel, we depict the publication distribution by month for the unseen test dataset, which was sampled from August 2023 and emphasizes the latter part of the month. The right panel displays the distribution of keywords in abstracts from the training, seen test, and unseen test datasets, represented by blue, yellow, and red, respectively.</p>
<h1>B. 2 Details of Models</h1>
<p>We present the meta-information of the open-source models used in our experiments, as shown in Table 4. We have gathered data regarding their pre-training, supervised learning corpus, and release dates to ensure the non-visibility of the unseen test data.</p>
<p>Table 4: To further ensure the non-visibility of the test data, we provide an overview of the related literature corpus within the training set of various LLMs, accompanied by their respective publication dates. The data marked with $\left(^{*}\right)$ is the data generated by people talking to ChatGPT. Our date marking is consistent with ChatGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">SFT Data (Y/M)</th>
<th style="text-align: center;">Released</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo (0-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo (5-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-4*</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Vicuna-33b-v1.3</td>
<td style="text-align: center;">Llama-1</td>
<td style="text-align: center;">ShareGPT (Unknown)</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-7b-chat</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-13b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">openchat-v3.2-super</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Sharegpt4 Dataset (2023/06)</td>
<td style="text-align: center;">2023/09</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">MedAlpaca-13B</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/03)</td>
<td style="text-align: center;">2023/03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatDoctor*</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B</td>
<td style="text-align: center;">Llama-2*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/08*</td>
</tr>
</tbody>
</table>
<h2>C Additional Results</h2>
<p>We have included additional results from human evaluations in Table 5, primarily focusing on correlation scores between word overlap metrics and manual evaluations. Note that we continue to use the same samples used in human evaluation to compute BLEU and ROUGE-L for a fair comparison. We calculate the Pearson and Spearman coefficients between each automatic metric and the average human score. These results reveal that word overlap metrics, such as BLEU and ROUGE-L, exhibit notably lower correlation with manual scores. While BLEU and ROUGE-L may have a high correlation with relevance metrics, they are weak in providing a comprehensive evaluation of the generations. Conversely, evaluations conducted by ChatGPT demonstrate higher correlation with human evaluations, as illustrated in Table 2. However, there is still a significant need to explore advanced metrics, particularly automated ones, in the context of scientific discovery.</p>
<p>Table 5: The table illustrates the correlations between automatic metrics and human evaluations. We annotate the Pearson and Spearman scores after each correlation score, denoting them as $r$ and $\rho$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Word Overlap</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU $(r / \rho)$</td>
<td style="text-align: center;">ROUGE-L $(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
</tr>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo(0-shot)</td>
<td style="text-align: center;">16.59(0.03/0.01)</td>
<td style="text-align: center;">29.87(-0.04/-0.05)</td>
<td style="text-align: center;">1.90(0.87/0.78)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo(5-shot)*</td>
<td style="text-align: center;">14.99(-0.09/0.12)</td>
<td style="text-align: center;">27.51(-0.33/-0.35)</td>
<td style="text-align: center;">1.96(0.80/0.78)</td>
<td style="text-align: center;">2.15(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Llama-2-70b-chat(0-shot)</td>
<td style="text-align: center;">9.64(-0.21/-0.20)</td>
<td style="text-align: center;">22.17(-0.31/-0.28)</td>
<td style="text-align: center;">2.04(0.89/0.84)</td>
<td style="text-align: center;">1.96(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)</td>
<td style="text-align: center;">9.42(-0.58/-0.65)</td>
<td style="text-align: center;">20.59(-0.47/-0.42)</td>
<td style="text-align: center;">2.20(0.96/0.90)</td>
<td style="text-align: center;">2.33(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)*</td>
<td style="text-align: center;">9.60(-0.16/-0.10)</td>
<td style="text-align: center;">19.99(-0.15/-0.17)</td>
<td style="text-align: center;">2.01(0.97/0.94)</td>
<td style="text-align: center;">2.08(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(0-shot)</td>
<td style="text-align: center;">11.42(0.21/0.36)</td>
<td style="text-align: center;">24.11(0.29/0.49)</td>
<td style="text-align: center;">1.91(0.90/0.75)</td>
<td style="text-align: center;">1.81(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(5-shot)</td>
<td style="text-align: center;">9.86(-0.28/-0.37)</td>
<td style="text-align: center;">23.52(-0.17/-0.24)</td>
<td style="text-align: center;">2.01(0.85/0.89)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">PMC-LLaMA-13B(0-shot)</td>
<td style="text-align: center;">8.19(0.32/0.39)</td>
<td style="text-align: center;">21.85(0.18/0.27)</td>
<td style="text-align: center;">1.41(0.73/0.73)</td>
<td style="text-align: center;">1.88(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B(5-shot)*</td>
<td style="text-align: center;">5.52(0.06/-0.01)</td>
<td style="text-align: center;">13.64(0.26/0.23)</td>
<td style="text-align: center;">1.97(0.95/0.94)</td>
<td style="text-align: center;">1.92(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">21.48(-0.00/0.00)</td>
<td style="text-align: center;">27.83(0.17/0.27)</td>
<td style="text-align: center;">1.79(0.83/0.85)</td>
<td style="text-align: center;">1.77(1.00/1.00)</td>
</tr>
</tbody>
</table>
<h1>D Case Study</h1>
<p>In this section, we present several generated hypotheses from various models and provide examples of the evaluation process, step by step, using ChatGPT.</p>
<h2>D. 1 Generated Hypothesis</h2>
<p>We compare the generated hypotheses of different LLMs selected in human evaluation. The selected medicine literature was published in August 2023 [4], which proposed the power law index as an early marker of myocardial infarction. As shown in Table 6, some responses like gpt-3.5-turbo (0-shot) propose this hypothesis from zero, based only on the background. This indicates that LLMs have great potential in proposing highly novel and verifiable hypotheses and could play a significant role in scientific discovery. Furthermore, we showcase the proposed hypotheses in a multi-agent collaboration involving three roles: Analyst, Scientist, and Critic, working together to generate hypotheses across 6 rounds, as shown in Table 7.</p>
<p>Table 6: This table illustrates the hypothesis proposals from different LLMs.</p>
<h2>InSTRUCTED LLMS CASE</h2>
<h2>Background:</h2>
<p>(1) In the early stage of MI, $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples with an AUC $&gt;0.7$.
(2) In the mid-stage and late-stage of MI, E2, E3, and Etotal exhibit excellent discriminating performance with an AUC $&gt;0.6$.
(3) All cardiac tissues display a universal two-stage power-law rheology at different time scales, independent of their disease status.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The sequential order of MI-related changes in cardiac tissues starts from the cytoplasmic level and progresses to the cellular and tissue levels.
(2) Pathological changes at subcellular, cellular, and tissue levels are accompanied by mechanical alterations.
(3) The proposed mechanical markers have diagnostic potential for assessing the progression of MI and studying other cardiovascular diseases.</p>
<h2>gpt-3.5-turbo (0-shot):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long can be used as a reliable biomarker for early detection and differentiation of myocardial infarction (MI) from healthy controls.
(2) E2, E3, and Etotal can serve as effective biomarkers for distinguishing between mid-stage and late-stage MI patients with high accuracy.
(3) The universal two-stage power-law rheology observed in cardiac tissues may play a crucial role in maintaining tissue integrity and function, regardless of the presence or absence of disease.</p>
<h1>gpt-3.5-turbo (5-shot):</h1>
<p>(1) The levels of $\alpha$ short and $\alpha$ long may serve as potential biomarkers for early detection and diagnosis of myocardial infarction (MI).
(2) E2, E3, and Etotal may have utility as biomarkers for assessing the progression and severity of MI in the mid and late stages.
(3) The universal two-stage power-law rheology observed in cardiac tissues suggests a fundamental biomechanical principle underlying tissue behavior, regardless of disease status.</p>
<h2>Llama-2-70b-chat (0-shot):</h2>
<p>(1) E2, E3, and Etotal can also track and distinguish MI samples from healthy control samples in the early stage of MI, with an AUC $&gt;0.7$, similar to $\alpha$ short and $\alpha$ long.
(2) The universal two-stage power-law rheology of cardiac tissues is related to the alteration of the extracellular matrix and the degradation of collagen in the early stage of MI, which can be detected by E2, E3, and Etotal.
(3) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI is affected by the changes in the levels of collagen and proteoglycans in the extracellular matrix, which can be evaluated by measuring the mechanical properties of cardiac tissues using a uniaxial tensiometer.</p>
<h2>Llama-2-70b-chat (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI is attributed to their ability to capture the subtle changes in cardiac tissue mechanics caused by the onset of fibrosis.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the mid-stage and late-stage of MI due to their sensitivity to changes in collagen content and cross-linking, which are hallmarks of fibrosis progression.
(3) The universal two-stage power-law rheology observed in all cardiac tissues, independent of disease status, suggests a common mechanism of mechanical regulation that can be exploited for early disease detection and monitoring.</p>
<h2>Llama-2-70b-chat (5-shot)*:</h2>
<p>(1) The $\alpha$ short and $\alpha$ long parameters can detect changes in myocardial stiffness in the early stages of myocardial infarction (MI).
(2) E2, E3, and Etotal can differentiate between healthy controls and MI samples in the mid and late stages.
(3) The universal two-stage power-law rheology in cardiac tissues may be useful in developing novel therapeutic approaches for MI.</p>
<h2>WizardLM-70B-V1.0 (0-shot):</h2>
<p>(1) $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples in all stages of MI with an AUC $&gt;0.7$.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the early stage of MI with an AUC $&gt;0.6$.
(3) The two-stage power-law rheology observed in all cardiac tissues is related to the progression of MI.</p>
<h2>WizardLM-70B-V1.0 (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI may be attributed to their sensitivity to early molecular changes in the myocardium.
(2) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI may be due to their ability to reflect the progression of myocardial damage and remodeling.</p>
<p>(3) The universal two-stage power-law rheology observed in all cardiac tissues may be a fundamental property of cardiac tissue biomechanics, potentially useful for developing diagnostic and therapeutic strategies for MI.</p>
<h1>PMC-LLaMA-13B (0-shot):</h1>
<p>(1) $\alpha$ short and $\alpha$ long are effective biomarkers for distinguishing MI samples from healthy control samples both in the early stage and the late stage of MI.
(2) E2 and E3 (Etotal) are likely to serve as effective biomarkers for discriminating mid-stage or late-stage MI samples from healthy control samples.
(3) Cardiac tissues display a universal two-stage power-law rheology that is independent of their disease status.</p>
<h2>PMC-LLaMA-13B (5-shot):</h2>
<p>(1) Further comprehensive validation of the proposed framework is needed to elucidate the pathological changes in MI patients.
(2) Consideration of multiple markers is important.
(3) The influence of various external factors should be incorporated for accurate interpretation of the mechanical markers.</p>
<h2>WizardLM-13B-V1.2 (SFT):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long features can provide a more robust identification of MI samples.
(2) E2, E3, and Etotal features can serve as independent predictors of MI.
(3) The universal two-stage power-law rheology observed in all cardiac tissues may have diagnostic implications.</p>
<p>Table 7: This table illustrates a case from multi-agent collaboration on hypothesis proposing.</p>
<h2>Multi-Agent Case</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>Dialogue process of multi-agent collaboration.</h1>
<p>(Round 1) Analyst: Based on the provided research background, I have identified the following keywords and topics:</p>
<p>Keywords:</p>
<ul>
<li>Digital electrophysiology</li>
<li>Telemonitoring</li>
<li>Cardiac implantable electronic devices (CIEDs)</li>
<li>Wearable devices</li>
<li>Cardiac rhythm monitoring</li>
<li>Photoplethysmography</li>
</ul>
<p>Topics:</p>
<ol>
<li>Advancements in digital electrophysiology and telemonitoring of CIEDs</li>
<li>Benefits of telemonitoring in improving patient outcomes</li>
<li>Accuracy and reliability of wearable devices for cardiac rhythm monitoring using photoplethysmography</li>
</ol>
<p>Preliminary insights:</p>
<ul>
<li>The field of digital electrophysiology has seen significant development in the past 25 years, particularly in the areas of telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring.</li>
<li>Telemonitoring of CIEDs has been shown to have positive impacts, such as reducing the time to detect clinically actionable events and improving patient outcomes.</li>
<li>Wearable devices, including smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</li>
</ul>
<p>Based on these insights, it can be hypothesized that the advancements in telemonitoring and wearable devices have led to improved cardiac rhythm monitoring and patient outcomes. However, further analysis and collaboration with the Scientist and Critic are needed to refine and validate this hypothesis.
(Round 2) Scientist: Based on the insights provided by the Analyst and the research background, I propose the following hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes and more accurate detection of clinically actionable events.</p>
<p>This hypothesis suggests that the development and implementation of telemonitoring technologies for CIEDs, along with the increasing accuracy and reliability of wearable devices using photoplethysmography, have positively impacted patient outcomes by reducing the time to detect clinically actionable events. By continuously monitoring cardiac rhythm, these advancements have the potential to provide timely interventions and improve overall patient care.</p>
<p>The hypothesis is grounded in the existing literature, which highlights the benefits of telemonitoring in improving patient outcomes and the increasing accuracy of wearable devices for cardiac rhythm monitoring. However, further collaboration with the Critic is necessary to evaluate the novelty, coherence, and scientific validity of the hypothesis.</p>
<p>(Round 3) Critic: The hypothesis proposed by the Scientist is well-aligned with the research background and the insights provided by the Analyst. It effectively captures the essence of the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring. The hypothesis also highlights the potential positive impact on patient outcomes and the detection of clinically actionable events.</p>
<p>However, there are a few areas that could be further refined or expanded upon in the hypothesis:</p>
<ol>
<li><strong>Specific outcomes</strong>: The hypothesis mentions ""improved patient outcomes," but it would be beneficial to specify the specific outcomes that are expected to improve. For example, are we referring to reduced hospitalizations, improved quality of life, or decreased mortality rates?</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The hypothesis briefly mentions the potential for timely interventions and improved patient care, but it would be helpful to elaborate on the specific mechanisms through which the advancements in telemonitoring and wearable devices are expected to achieve these outcomes. Are there specific interventions or treatments that can be administered based on the data collected from these devices?</li>
<li>${ }^{<em> </em>}$ Scientific validity**: While the hypothesis is grounded in the existing literature, it would be valuable to provide some references or studies that support the claims made in the hypothesis. This would enhance the scientific validity of the hypothesis and demonstrate that it is not just speculative but based on empirical evidence.</li>
</ol>
<p>To refine the hypothesis, I would recommend the Scientist to consider incorporating these suggestions and providing more specific details and references to support the claims made. This will strengthen the coherence and scientific validity of the hypothesis.</p>
<p>Analyst, based on the feedback provided, please re-evaluate the research background and provide any additional insights or interpretations that could further refine the hypothesis.
(Round 4) Analyst: Based on the feedback from the Critic, I have re-evaluated the research background and identified additional insights:</p>
<ol>
<li><strong>Specific outcomes</strong>: The research background mentions that telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes. These outcomes could include early detection of arrhythmias, reduced hospitalizations due to cardiac events, improved quality of life, and decreased mortality rates. It would be beneficial to specify one or more of these specific outcomes in the hypothesis.</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The advancements in telemonitoring and wearable devices can potentially lead to improved patient outcomes through several mechanisms. For example, continuous monitoring of cardiac rhythm can enable early detection of abnormal rhythms, allowing for timely interventions such as medication adjustments or electrical cardioversion. Additionally, the data collected from these devices can provide valuable insights for personalized treatment plans and remote patient management. Including these specific mechanisms in the hypothesis would enhance its coherence.</li>
<li>${ }^{<em> </em>}$ Scientific validity**: To strengthen the scientific validity of the hypothesis, it would be valuable to reference specific studies or research that support the claims made. For example, citing studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events or improving patient outcomes would provide empirical evidence to support the hypothesis.</li>
</ol>
<p>Incorporating these additional insights and suggestions, the refined hypothesis could be:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis takes into account the specific outcomes, mechanisms, and scientific validity, as suggested by the Critic. It provides a more comprehensive and well-supported statement that captures the essence of the research background and the collaborative insights.
(Round 5) Scientist: Based on the additional insights and suggestions provided by the Analyst and the Critic, I propose the following refined hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis incorporates the specific outcomes mentioned in the research background, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. It also expands on the mechanisms through which these advancements are expected to achieve these outcomes, including early detection of abnormal rhythms, timely interventions, and personalized treatment plans. Additionally, the hypothesis acknowledges the importance of scientific validity by referencing studies that support the claims made.</p>
<p>The refined hypothesis is both innovative and scientifically grounded, as it builds upon the existing literature and incorporates the collaborative insights provided by the Analyst and the Critic. It provides a logical and well-supported statement that can be further evaluated and tested.
(Round 6) Critic: The refined hypothesis proposed by the Scientist is well-structured and addresses the feedback provided by the Critic. It incorporates specific outcomes, mechanisms, and scientific validity, making it a comprehensive and scientifically robust statement.</p>
<p>Final Answer:
(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>D. 2 Evaluation Examples</h1>
<p>We provide evaluation results conducted by ChatGPT in Table 8. ChatGPT offers a step-by-step explanation for each metric, ultimately providing independent scores in integer format.</p>
<p>Table 8: This table illustrates a case for hypothesis evaluation using ChatGPT.</p>
<h2>CASE OF ChatGPT as ANNOTATOR</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h2>Evaluation Results</h2>
<p>Novelty:
Explanation: The generated scientific hypothesis builds upon the background information by stating that the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes. It further suggests that these advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis goes beyond the background by proposing specific mechanisms through which the advancements in telemonitoring and wearable devices contribute to improved patient outcomes. It also mentions the supporting evidence from studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. Overall, the hypothesis provides new insights and expands upon the existing knowledge in the field.
Score: 2</p>
<h2>Relevance:</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/nomic-ai/nomic&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
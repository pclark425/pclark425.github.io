<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5186 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5186</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5186</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-263909251</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.08118v1.pdf" target="_blank">Can Large Language Models Really Improve by Self-critiquing Their Own Plans?</a></p>
                <p><strong>Paper Abstract:</strong> There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5186.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5186.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+LLM (self-critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM generator + LLM verifier backprompting (self-critiquing) using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative generate-then-reflect planning pipeline where the same LLM (GPT-4) acts as the plan generator and as a verifier that critiques candidate plans and provides feedback; iteration continues until verifier accepts or a max iteration threshold is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 used as both generator and verifier in experiments (temperature set to 0); paper does not report model size or architecture details beyond naming GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-critique / backprompting (LLM+LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator GPT-4 produces a candidate plan from a PDDL domain+instance prompt (one-shot). A verifier GPT-4 (zero-shot prompt) inspects the plan, outputs a binary validity judgment and optionally feedback describing errors; the generator is re-prompted with that feedback to produce a new plan. Iteration stops when verifier accepts a plan or iteration count exceeds a threshold. Feedback format is unconstrained.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Blocksworld (classical planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Goal-directed deterministic planning problems in PDDL (Blocksworld): produce a sequence of actions that, when applied from the initial state, satisfies the goal conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>55/100 (55% accuracy) for LLM+LLM backprompting; observed average number of iterations = 3.48</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Generator-only (no backprompting) GPT-4: 40/100 (40%); LLM+VAL (GPT-4 generator + external sound verifier VAL) with backprompting: 88/100 (88%)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: LLM+LLM backprompting attained 55% vs generator-only 40% (a modest absolute gain of 15 percentage points), showing some improvement over a single-shot generator; however, it performed substantially worse than using an external sound verifier (LLM+VAL: 88%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The improvement is marginal and overshadowed by severe verifier reliability issues: the LLM verifier produces many false positives (see verifier metrics) which undermines trust; self-critiquing underperforms compared to using a sound external verifier. Paper also reports that the nature of feedback (binary vs detailed) had minimal impact on improving generation when the verifier is an LLM.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5186.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5186.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 verifier (binary verification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as a zero-shot plan verifier (binary validity judgments and feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used as the verifier component in the LLM+LLM setup, given a domain, instance and candidate plan and asked to decide plan validity and optionally provide feedback describing errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 used with temperature 0 as the verifier LLM; no model size reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>binary verification / feedback generation (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Verifier GPT-4 receives the domain, instance and candidate plan and returns a binary validity decision; when invalid it may provide error feedback (format unconstrained). This binary judgment is compared to a ground-truth external verifier (VAL).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Plan verification (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Determine whether a proposed action sequence is a correct plan for the given PDDL Blocksworld instance (binary valid/invalid), and optionally enumerate reasons for invalidity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Overall classification accuracy 61/100 (61%); true positive rate: 54/55 (98.2% of truly valid plans labeled valid); true negative count: 7/45 (15.6% of truly invalid plans labeled invalid).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A (verification role only); comparison baseline is VAL (ground truth).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No evidence that GPT-4 as verifier improves plan quality reliably — although it rarely misses valid plans (high TP rate), it labels a large fraction of invalid plans as valid, enabling incorrect plans to be accepted and thus harming end-to-end system reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High false positive rate: 38/45 invalid plans were judged valid by GPT-4 (false positive rate ≈84.4% on invalid instances), leading to many incorrect plans being accepted; this is cited as a primary reason LLM+LLM underperforms versus LLM+VAL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5186.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5186.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+VAL (backprompting with sound verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 generator + external sound verifier (VAL) backprompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative backprompting where GPT-4 generates plans and the external, sound verifier VAL checks plans and returns structured feedback (binary + error details) used to reprompt the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 + VAL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generator: GPT-4 (temperature 0). Verifier: VAL, a classical planning plan validator (external sound verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>backprompting with external sound verifier (LLM+VAL)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator GPT-4 produces a plan. VAL (sound external verifier) checks plan correctness and provides binary verdict and structured feedback (first inexecutable action and unmet preconditions, or unmet goals; or open-conditions info). The generator is re-prompted with VAL feedback iteratively until VAL accepts or iteration threshold reached.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Blocksworld (classical planning) — plan generation with varied feedback levels</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same Blocksworld PDDL planning instances; experiments additionally vary how much feedback from VAL is provided (none; binary only; binary + first error; binary + all errors/open conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Overall LLM+VAL with backprompting: 88/100 (88% accuracy, Table 1). Per feedback-level experiments (Table 3, smaller sample sizes reported): No feedback 40/100 (40%, avg steps 1.00); Only binary feedback 37/50 (74%, avg steps 5.38); Binary + first-error (VAL) 43/50 (86%, avg steps 4.18); Binary + all-errors 43/50 (86%, avg steps 4.42).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Generator-only (no backprompting): 40/100 (40%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Strong quantitative improvement: LLM+VAL backprompting yields 88% vs generator-only 40%, demonstrating that iterative generation with a sound verifier substantially improves plan correctness. Feedback-level experiments show that accurate binary feedback + iterative opportunities already improves performance substantially; detailed feedback (first-error or all-errors) produced modest additional gains in the reported samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports that beyond having an accurate binary signal and allowing multiple attempts, adding more detailed feedback yielded limited extra gains in these experiments; results also depend on having a sound verifier (VAL) — the gains disappear when the verifier itself is unreliable (as with LLM+LLM).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Large language models are reasoners with self-verification <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Lm vs lm: Detecting factual errors via cross examination <em>(Rating: 1)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5186",
    "paper_id": "paper-263909251",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "LLM+LLM (self-critiquing)",
            "name_full": "LLM generator + LLM verifier backprompting (self-critiquing) using GPT-4",
            "brief_description": "Iterative generate-then-reflect planning pipeline where the same LLM (GPT-4) acts as the plan generator and as a verifier that critiques candidate plans and provides feedback; iteration continues until verifier accepts or a max iteration threshold is reached.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 used as both generator and verifier in experiments (temperature set to 0); paper does not report model size or architecture details beyond naming GPT-4.",
            "reflection_method_name": "self-critique / backprompting (LLM+LLM)",
            "reflection_method_description": "Generator GPT-4 produces a candidate plan from a PDDL domain+instance prompt (one-shot). A verifier GPT-4 (zero-shot prompt) inspects the plan, outputs a binary validity judgment and optionally feedback describing errors; the generator is re-prompted with that feedback to produce a new plan. Iteration stops when verifier accepts a plan or iteration count exceeds a threshold. Feedback format is unconstrained.",
            "num_iterations": 15,
            "task_name": "Blocksworld (classical planning)",
            "task_description": "Goal-directed deterministic planning problems in PDDL (Blocksworld): produce a sequence of actions that, when applied from the initial state, satisfies the goal conditions.",
            "performance_with_reflection": "55/100 (55% accuracy) for LLM+LLM backprompting; observed average number of iterations = 3.48",
            "performance_without_reflection": "Generator-only (no backprompting) GPT-4: 40/100 (40%); LLM+VAL (GPT-4 generator + external sound verifier VAL) with backprompting: 88/100 (88%)",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: LLM+LLM backprompting attained 55% vs generator-only 40% (a modest absolute gain of 15 percentage points), showing some improvement over a single-shot generator; however, it performed substantially worse than using an external sound verifier (LLM+VAL: 88%).",
            "limitations_or_failure_cases": "The improvement is marginal and overshadowed by severe verifier reliability issues: the LLM verifier produces many false positives (see verifier metrics) which undermines trust; self-critiquing underperforms compared to using a sound external verifier. Paper also reports that the nature of feedback (binary vs detailed) had minimal impact on improving generation when the verifier is an LLM.",
            "uuid": "e5186.0"
        },
        {
            "name_short": "GPT-4 verifier (binary verification)",
            "name_full": "GPT-4 used as a zero-shot plan verifier (binary validity judgments and feedback)",
            "brief_description": "GPT-4 used as the verifier component in the LLM+LLM setup, given a domain, instance and candidate plan and asked to decide plan validity and optionally provide feedback describing errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 used with temperature 0 as the verifier LLM; no model size reported in the paper.",
            "reflection_method_name": "binary verification / feedback generation (zero-shot)",
            "reflection_method_description": "Verifier GPT-4 receives the domain, instance and candidate plan and returns a binary validity decision; when invalid it may provide error feedback (format unconstrained). This binary judgment is compared to a ground-truth external verifier (VAL).",
            "num_iterations": null,
            "task_name": "Plan verification (Blocksworld)",
            "task_description": "Determine whether a proposed action sequence is a correct plan for the given PDDL Blocksworld instance (binary valid/invalid), and optionally enumerate reasons for invalidity.",
            "performance_with_reflection": "Overall classification accuracy 61/100 (61%); true positive rate: 54/55 (98.2% of truly valid plans labeled valid); true negative count: 7/45 (15.6% of truly invalid plans labeled invalid).",
            "performance_without_reflection": "N/A (verification role only); comparison baseline is VAL (ground truth).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No evidence that GPT-4 as verifier improves plan quality reliably — although it rarely misses valid plans (high TP rate), it labels a large fraction of invalid plans as valid, enabling incorrect plans to be accepted and thus harming end-to-end system reliability.",
            "limitations_or_failure_cases": "High false positive rate: 38/45 invalid plans were judged valid by GPT-4 (false positive rate ≈84.4% on invalid instances), leading to many incorrect plans being accepted; this is cited as a primary reason LLM+LLM underperforms versus LLM+VAL.",
            "uuid": "e5186.1"
        },
        {
            "name_short": "LLM+VAL (backprompting with sound verifier)",
            "name_full": "GPT-4 generator + external sound verifier (VAL) backprompting",
            "brief_description": "Iterative backprompting where GPT-4 generates plans and the external, sound verifier VAL checks plans and returns structured feedback (binary + error details) used to reprompt the generator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 + VAL",
            "model_description": "Generator: GPT-4 (temperature 0). Verifier: VAL, a classical planning plan validator (external sound verifier).",
            "reflection_method_name": "backprompting with external sound verifier (LLM+VAL)",
            "reflection_method_description": "Generator GPT-4 produces a plan. VAL (sound external verifier) checks plan correctness and provides binary verdict and structured feedback (first inexecutable action and unmet preconditions, or unmet goals; or open-conditions info). The generator is re-prompted with VAL feedback iteratively until VAL accepts or iteration threshold reached.",
            "num_iterations": 15,
            "task_name": "Blocksworld (classical planning) — plan generation with varied feedback levels",
            "task_description": "Same Blocksworld PDDL planning instances; experiments additionally vary how much feedback from VAL is provided (none; binary only; binary + first error; binary + all errors/open conditions).",
            "performance_with_reflection": "Overall LLM+VAL with backprompting: 88/100 (88% accuracy, Table 1). Per feedback-level experiments (Table 3, smaller sample sizes reported): No feedback 40/100 (40%, avg steps 1.00); Only binary feedback 37/50 (74%, avg steps 5.38); Binary + first-error (VAL) 43/50 (86%, avg steps 4.18); Binary + all-errors 43/50 (86%, avg steps 4.42).",
            "performance_without_reflection": "Generator-only (no backprompting): 40/100 (40%).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Strong quantitative improvement: LLM+VAL backprompting yields 88% vs generator-only 40%, demonstrating that iterative generation with a sound verifier substantially improves plan correctness. Feedback-level experiments show that accurate binary feedback + iterative opportunities already improves performance substantially; detailed feedback (first-error or all-errors) produced modest additional gains in the reported samples.",
            "limitations_or_failure_cases": "Paper reports that beyond having an accurate binary signal and allowing multiple attempts, adding more detailed feedback yielded limited extra gains in these experiments; results also depend on having a sound verifier (VAL) — the gains disappear when the verifier itself is unreliable (as with LLM+LLM).",
            "uuid": "e5186.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Large language models are reasoners with self-verification",
            "rating": 2,
            "sanitized_title": "large_language_models_are_reasoners_with_selfverification"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Lm vs lm: Detecting factual errors via cross examination",
            "rating": 1,
            "sanitized_title": "lm_vs_lm_detecting_factual_errors_via_cross_examination"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 1,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        }
    ],
    "cost": 0.01147875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Really Improve by Self-critiquing Their Own Plans?
12 Oct 2023</p>
<p>Karthik Valmeekam kvalmeek@asu.edu 
Equal Contribution Preprint
Under Review</p>
<p>Matthew Marquez mmarqu22@asu.edu 
Equal Contribution Preprint
Under Review</p>
<p>Subbarao Kambhampati </p>
<p>School of Computing &amp; AI
Arizona State University Tempe</p>
<p>School of Computing &amp; AI
Arizona State University
Tempe</p>
<p>School of Computing &amp; AI
Arizona State University
Tempe</p>
<p>Can Large Language Models Really Improve by Self-critiquing Their Own Plans?
12 Oct 20230D2A42F7FC2C2F57F9FE679C62CC224BarXiv:2310.08118v1[cs.AI]
There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode.Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning.We evaluate a planning system that employs LLMs for both plan generation and verification.We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance.Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability.Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation.Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.</p>
<p>Introduction</p>
<p>Large Language Models have rapidly captured the attention of the AI research community with their exceptional natural language completion capabilities.Trained on web-scale language corpora, these models have demonstrated the ability to generate seemingly valuable completions across a wide range of topics.This led to a surge of interest in determining whether such models were able to perform well on reasoning tasks.Even though initial anecdotal results showed promise, systematic studies revealed their incompetency in reasoning -be it planning [12] or in simple arithmetic or logic [3].These results questioning the robustness of their reasoning abilities led to researchers exploring ways to improve these systems.Of particular interest to us is the emerging research on self-critiquing, where the LLMs are used to critique their own candidate generations and iterate.The current works [15,10,14] exhibit considerable optimism about using LLMs to critique their own candidate generations, especially in an iterative setting where they keep refining their candidate generations.Additionally, the notion that verifying correctness is computationally simpler than generation for reasoning adds to the optimism.However, there are grounds to be skeptical about it as Intrigued by the prevailing optimism, in this paper, we set out to systematically investigate the effectiveness of using LLMs to critique their own generations in the context of planning.We look at the simplest class of planning problems, the goal-directed deterministic planning problems colloquially referred to as classical planning problems.Our methodology employs a planning system that utilizes the same LLM for both generation and verification, which we term the LLM+LLM system in an iterative setting.Within this setting, the generator LLM continuously produces candidate plans, drawing upon feedback from the verifier LLM, until the verifier LLM either approves a candidate plan as correct or the number of iterations surpasses a predefined threshold.We present an empirical evaluation of (i) the effect of self-critiquing on the plan generation performance of the overall LLM+LLM system (ii) the performance of the verifier LLM in comparison to the ground-truth verification and finally (iii) the influence of varying feedback levels while critiquing the LLM's generation on the overall system performance.For our study, we use GPT-4 [9] as both the generator and verifier.</p>
<p>Our findings suggest that self-critiquing degrades the plan generation performance compared to when an external, sound verifier is utilized.This decline in performance can be directly attributed to the verifier LLM's subpar results.The verifier LLM yields a significant number of false positives, which can severely undermine the system's reliability.Furthermore, we explored whether the nature of feedback on invalid plans influences plan generation performance.Our results indicate that the type of feedback-whether it's merely binary verification or combined with detailed feedback on the errors of the generated plan-doesn't significantly impact plan generation performance.Thus, our systematic investigation offers compelling preliminary evidence to question the efficacy of LLMs as verifiers for planning tasks within an iterative, self-critiquing framework.In the rest of the paper, we first present the related work, then the required background before delving into the methodology and the evaluation.</p>
<p>Related Work</p>
<p>There has been significant interest in investigating the reasoning capabilities of LLMs, spanning from planning [12] to logic and arithmetic [3], and even puzzles [15].As the initial excitement from triumphant anecdotes about LLMs' reasoning capabilities began to wane with systematic studies [12,11,3], researchers proposed that allowing LLMs to verify their own candidate solutions and iterate over this process could enhance their reasoning abilities [10,7,6,14].Our work systematically investigates the effect of iterative self-critiquing in the context of planning.</p>
<p>There have also been studies that utilize multiple LLMs to generate and verify candidate solutions, either in the form of a debate [2] or through cross-examination [1].However, these studies still rely solely on the verification/self-critiquing abilities of the LLMs, an aspect our work critically examines in the context of planning.Our results provide compelling reasons to question the use of LLMs for self-critiquing in planning.</p>
<p>Background</p>
<p>We specifically are interested in classical planning problems that are represented within the PDDL (Planning Domain and Definition Language) framework [8].These problem classes consist of a domain, initial state and a goal state.The domain consists of a set of predicates and a set of actions.The state-space of the planning problem is represented with some truth-assignment on the predicates.Every action in domain have a set of preconditions which determine when an action can be applied and a set of effects which determine the modifications to the state after the action is applied.A plan here is a sequence of actions which are present in the domain that when executed in the initial state, satisfy the goal conditions.The LLM+LLM planning system (as shown in Figure 1) consists of a generator LLM and a verifier LLM.For a given instance, the generator LLM produces a candidate plan, while the verifier LLM determines its correctness.If the plan is found to be incorrect, the verifier provides feedback detailing the reasons for its failure.This feedback is then relayed to the generator LLM, prompting the generation of a new candidate plan.It's worth noting that there are no constraints on the type or format of feedback the verifier LLM produces.The system ceases generation either when the verifier LLM approves the candidate plan as valid or when the number of prompting iterations exceeds a set threshold (for our experiments, this threshold is set at 15 iterations).This method is similar to the backprompting technique described in [12].However, the main distinction lies in the type of verifier employed.In our system, both the verifier and generator are LLMs, whereas the referenced approach utilizes an external sound verifier, VAL [4].For all our experiments, GPT-4 serves as the default LLM.</p>
<p>Prompt generation</p>
<p>For the LLM+LLM Planning system described above, we utilize distinct prompts for the generator and verifier LLMs.The prompt generator (as shown in Figure 1) utilizes the PDDL domain and instance files to generate the required prompts in natural language.Our prompts are structured similarly to the natural language prompts found in [12].For plan generation, our prompts are one-shot: we begin by presenting the domain description, followed by an example instance (along with its corresponding plan).We then present the query instance.These example instances are randomly selected from our set of instances, and this forms the input for the generator LLM.For the verifier LLM, we adopt a zero-shot approach.Here, we present the domain description, followed by the query instance and its corresponding plan.The verifier LLM is then tasked with verifying the query plan and providing feedback if necessary.As mentioned earlier, we do not restrict the type or format of the feedback for the verifier LLM.Detailed examples of the prompts given to both the generator and verifier LLMs can be found in the Appendix.</p>
<p>Evaluation and Analysis</p>
<p>We evaluate our planning system on Blocksworld, a widely recognized common-sense planning domain in AI planning literature [5].We generate 100 random instances for evaluation across various methods.To provide a ground-truth assessment of the final LLM plan's correctness, we employ an external sound verifier, VAL [4].For all experiments, GPT-4 [9] serves as the chosen LLM and was run with a temperature of 0, thereby making it deterministic.</p>
<p>Effect of self-critiquing on plan generation</p>
<p>We assessed the impact of self-critiquing on plan generation by comparing the LLM+LLM backprompting system with two other baselines.The first baseline is the LLM+VAL backprompting system, which mirrors the backprompting method described in [12].In this method, the plan produced by the LLM is validated by an external sound verifier, VAL.If the plan is found lacking, the generator-LLM is reprompted using feedback from VAL.The second baseline involves a generator-LLM without backprompting.Here, the generator LLM receives a single prompt, and the resulting plan is considered final.</p>
<p>As illustrated in Table 1, the LLM+LLM backprompting approach slightly outperforms the nonbackprompting method in terms of accuracy.However, it falls short when compared to the LLM+VAL system.It's worth noting that the marginal improvement over the generator-LLM-only method might not solely be attributed to the LLM verifier.The backprompting itself, which offers the generator LLM multiple opportunities to produce a plan, could be a contributing factor.The subpar performance of the LLM+LLM system, especially when compared to LLM+VAL, can likely be traced back to the substantial number of type-1 errors produced by the LLM verifier.It's evident that incorporating a sound verifier in the backprompting process can significantly enhance overall performance.</p>
<p>Plan Generation</p>
<p>Analysis on the self-critique verifier</p>
<p>We base our evaluation of the verifier LLM on its binary verification (i.e., determining whether the plan is valid or not) of the final plan produced by the LLM+LLM system.It's important to note that the system halts either when the verifier LLM considers the plan valid or when the number of iterations surpasses 15.We compare the LLM verifier's output with ground truth classifications made using VAL [4], a sound verifier.To make the ground truth determination available for each input plan, we separately evaluate that plan using VAL as well.</p>
<p>As illustrated in Table 2, out of the 100 instances, the verifier accurately identifies 61 (or 61%).However, a deeper examination of the verifier's errors reveals a concerning number of false positives.In this context, a false positive refers to the verifier LLM deeming a generated plan valid when, in fact, it is not.Out of the 100 instances, the verifier LLM produces 54 true positives and 38 false positives (type-1 errors).This indicates that the verifier deemed 38 plans, which were actually invalid, to be valid which can be catastrophic if such a system is deployed in scenarios where correctness is paramount.</p>
<p>Accuracy</p>
<p>Effect of the levels of feedback on plan generation</p>
<p>While the use of a sound verifier appears to enhance overall performance, we sought to further investigate the impact of varied levels of feedback on plan generation performance.We assessed the system's performance across four distinct feedback levels:</p>
<p>1.No Feedback: At this level, the initial plan generated by the LLM is considered to be final and no feedback is provided to the LLM. 2. Binary Feedback: This level simply indicates whether the generated plan is valid or not.3. Inexecutable Action Feedback: If the plan is invalid and inexecutable, this feedback highlights the first inexecutable action and the unmet preconditions causing the inexecutability.If the plan is executable but fails to meet all goal conditions, the unmet goal conditions are presented.This feedback mirrors what VAL provides.4. Open Conditions Feedback: This level treats the plan as a partial-order plan [13] and presents all the actions for which there exists atleast one unmet pre-condition and the corresponding unmet preconditions.Further it also presents the unmet goal conditions.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we conducted a systematic investigation into the ability of Large Language Models (LLMs) to critique their own outputs, specifically within the context of classical planning problems.While recent research has been optimistic about LLMs' potential in self-critiquing, especially in iterative settings, our findings present a different perspective.</p>
<p>Our empirical evaluations on Blocksworld, a simple common-sense domain, highlighted the ineffectiveness of self-critiquing in LLMs in the context of planning.We showed that the verifier LLM generates a significant number of false positives which be detrimental to the overall system's reliability, particularly in domains where the correctness of plans is paramount.Interestingly, the nature of feedback, whether binary or detailed, did not have a pronounced impact on plan generation performance, suggesting that the core issue lies in the LLM's binary verification capabilities rather than the granularity of feedback.</p>
<p>In the future, we plan to conduct more extensive experiments with respect to the number of instances, the number of domains and prompting methods (such as chain-of-thought).</p>
<p>Figure 1 :
1
Figure 1: Overall evaluation architecture</p>
<p>Table 1 :
1
Comparison between various plan generation methods on the Blocksworld domain.
MethodAccuracyAvg. Number of iterationsLLM+LLM w/ Backprompting (BP) 55/100 (55%)3.48LLM+VAL w/ BP88/100 (88%)4.18Generator LLM only w/o BP40/100 (40%)1.00</p>
<p>Table 2 :
2
Breakdown of Plan Verification results on Blocksworld domain.The denominators (in aspects other than Accuracy) are ground-truth values based on VAL.
True PositiveFalse PositiveTrue NegativeFalse NegativeRateRateRateRateVerifier61/100 (61%) 54/55 (98.2%) 38/45 (84.45%) 7/45 (15.55%)1/55 (1.8%)LLM</p>
<p>Table 3
3
showcases the LLM's performance when subjected to various levels of feedback (including one with no feedback).Interestingly, the amount of feedback provided to the LLM seems to have minimal influence on its performance improvement.As long as the binary feedback is accurate and the LLM is given ample opportunities to generate a plan, the detailed feedback on invalid plans doesn't appear to significantly enhance the LLM's performance.We have provided examples for each feedback level in the Appendix.
Levels of feedbackAccuracyAvg. no ofstepsNo feedback40/100 (40%)1.00Only binary feedback37/50 (74%)5.38Binary + First error feedback (by VAL) 43/50 (86%)4.18Binary + All errors feedback43/50 (86%)4.42</p>
<p>Table 3 :
3
Performance of LLM+VAL system on plan generation with varied levels of feedback.</p>
<p>Lm vs lm: Detecting factual errors via cross examination. Roi Cohen, May Hamri, Mor Geva, Amir Globerson, arXiv:2305.132812023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jian, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Hwang, arXiv:2305.186542023arXiv preprint</p>
<p>VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. Richard Howey, Derek Long, Maria Fox, 16th IEEE International Conference on Tools with Artificial Intelligence. IEEE2004</p>
<p>. IPC. International planning competition. 1998</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Pddl-the planning domain definition language. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 1998</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>PDDL planning with pretrained large language models. Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tomás Lozano-Pérez, Leslie Pack, Kaelbling , NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>On the planning abilities of large language models-a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.157712023arXiv preprint</p>
<p>An introduction to least commitment planning. Daniel S Weld, AI magazine. 1541994</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
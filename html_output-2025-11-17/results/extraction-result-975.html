<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-975 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-975</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-975</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-dc135dabef805c7271f53ec4b212bdf8996cfd9d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dc135dabef805c7271f53ec4b212bdf8996cfd9d" target="_blank">AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> This work performs few-shot translation from natural language task descriptions to an intermediate task representation that can be consumed by a TAMP algorithm to jointly solve the task and motion plan and shows that this approach outperforms several methods using LLMs as planners in complex task domains.</p>
                <p><strong>Paper Abstract:</strong> For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website§ for prompts, videos, and code.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e975.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e975.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning Domain Definition Language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic formalism for expressing planning domains and problems (including actions, predicates, objects, and goals), widely used in classical and temporal planning research and robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pddl2. 1: An extension to pddl for expressing temporal planning domains</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDDL (classic symbolic planner representation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PDDL is a domain/problem specification language for symbolic planners. It expresses objects, predicates, action schemas with preconditions and effects, and (in extensions) temporal and numeric constraints. In this paper PDDL is discussed as an example of a dedicated planning representation that classical TAMP and task planners rely on, but it is not used in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Represents world state as grounded and/or lifted symbolic predicates over objects; actions are parameterized schemas with deterministic preconditions and effects in classical PDDL, with PDDL extensions (e.g., PDDL2.1) providing temporal/numeric constructs. The paper references PDDL as a symbolic (typically deterministic) representation; probabilistic extensions (e.g., PPDDL) are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>classical symbolic planners (general reference; not executed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>PDDL is referenced as a general symbolic planning representation for robotics/TAMP domains; the paper does not tie PDDL usage to any text-based benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper cites PDDL as a canonical symbolic task-specification language; it is discussed as an alternative to using natural language directly, but the experiments instead translate NL to STL. No experiments here use PDDL or its probabilistic variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e975.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e975.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief-space TAMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integrated Task and Motion Planning in Belief Space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach to integrate task-and-motion planning where planning operates over belief (probability) distributions over continuous states, thus explicitly modelling state uncertainty during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Integrated task and motion planning in belief space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Belief-space integrated TAMP (Kaelbling & Lozano-Pérez)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A class of methods that integrate discrete task planning and continuous motion planning by planning directly over belief states (distributions over the robot/environment continuous state), enabling decisions that account for uncertainty in sensing and dynamics. The AutoTAMP paper cites this work in related work to note approaches that explicitly reason in belief space for integrated planning.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>belief states (probabilistic)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>World state is represented as a probability distribution (belief) over continuous variables (e.g., robot pose, object poses). Actions have stochastic outcomes and measurement models; planning reasons about belief updates (Bayesian filtering) and selects actions to achieve task objectives under state uncertainty. The AutoTAMP paper references this approach but does not implement or evaluate it.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>state uncertainty (belief over continuous states), epistemic/aleatoric depending on model</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>belief-state representation (probability distributions), Bayesian filtering / belief propagation (as per belief-space planning literature); specific methods not detailed in AutoTAMP</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>belief-space planning methods (integrated TAMP in belief space)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Referenced as a general TAMP approach that models uncertainty; not applied to text-based environments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AutoTAMP cites belief-space integrated TAMP as existing work that explicitly models uncertainty in planning; the authors position their NL-to-STL translation + formal STL planner as an alternative that does not itself perform belief-space planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e975.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e975.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-constructed world models (Guan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced recent effort that uses pre-trained LLMs to construct and use world models for model-based task planning (as cited in AutoTAMP's related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-constructed world models for model-based planning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited work that (per its title) leverages pre-trained LLMs to build and use world models for model-based task planning. In AutoTAMP this is mentioned in the context of related work exploring how pretrained LLMs can be used to construct symbolic/world models for downstream planning, but AutoTAMP does not implement or evaluate that system.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>not specified in AutoTAMP (paper reference implies symbolic/world models constructed by LLMs; could include PDDL-like or other symbolic forms)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>AutoTAMP only cites this paper title in related work and does not describe how states/actions/transitions are represented; the referenced work likely describes techniques for extracting symbolic/world-model elements from LLM outputs, but details are not present in AutoTAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>world model construction and utilization for planning (as implied by the cited title)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>pre-trained large language models (unspecified in AutoTAMP)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>model-based task planning (specific planner not detailed in AutoTAMP)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Mentioned in related work as an approach to use LLMs to construct world models for planning; AutoTAMP does not report experiments in text-based benchmarks for this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AutoTAMP acknowledges this line of work that uses LLMs to build or augment symbolic/world models for planning, but provides no experimental details; thus integration of LLM uncertainty into probabilistic symbolic models is not described in AutoTAMP itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e975.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e975.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDDL + LLM (Silver et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PDDL planning with pretrained large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work exploring the use of pretrained LLMs in conjunction with PDDL-based planning (cited in AutoTAMP's related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PDDL planning with pretrained large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDDL planning with pretrained LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited work that combines pretrained LLMs with PDDL planning; AutoTAMP references it as related research investigating LLMs for symbolic planning domains (the AutoTAMP experiments instead use STL + an STL planner). The paper does not provide implementation details of this referenced system.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (symbolic planning representation)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>As referenced, the approach ties pretrained LLMs into PDDL-based planning; AutoTAMP does not specify whether the referenced work models probabilities or uses deterministic PDDL. The referenced title suggests using LLMs to assist PDDL planning rather than introducing probabilistic PDDL variants.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>assist in planning / generate planning content (as per citation context)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>pretrained large language models (unspecified in AutoTAMP)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>PDDL-based symbolic planning (details in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Mentioned as related work combining LLMs with symbolic PDDL planning; AutoTAMP does not evaluate this approach in text-based environment benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AutoTAMP cites this paper to situate LLMs being used with symbolic planners; AutoTAMP's experiments do not explore probabilistic symbolic models extracted from LLMs or their integration into text-based environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e975.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e975.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL2TL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NL2TL: Transforming natural languages to temporal logics using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular translation pipeline (fine-tuned T5-large) that maps natural language instructions to temporal logic specifications; referenced and used as an ablation/comparison in AutoTAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nl2tl: Transforming natural languages to temporal logics using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NL2TL (modular NL → temporal logic translator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NL2TL is a modular translation pipeline that uses a fine-tuned LLM (T5-large) trained on a corpus of NL paired with temporal logic expressions to translate language into temporal logic specifications (e.g., STL/LTL). AutoTAMP experiments include NL2TL as an alternative translator into task specifications; AutoTAMP still uses the STL planner and the same syntactic/semantic re-prompting verification steps on top of NL2TL outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>temporal logic (STL/LTL) task specifications (symbolic temporal logic)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Represents tasks as temporal logic formulae over predicates (e.g., in(room)), rather than an explicit probabilistic world model; NL2TL produces logical task specifications consumed by a formal STL planner. The representation is symbolic/deterministic at the specification level; the planner reasons about continuous trajectories to satisfy the logic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>natural language → temporal-logic translator; used to construct task specifications</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>T5-large (fine-tuned) as reported for NL2TL; AutoTAMP uses GPT-4 for re-prompting/error correction</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>STL trajectory optimization planner (multi-agent STL planner cited [23])</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Used for robot-navigation TAMP domains in AutoTAMP experiments (2D/3D navigation and multi-agent scenarios); not a text-only environment benchmark like TextWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate (percentage of test cases where produced trajectory satisfies instruction/specification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>In AutoTAMP experiments, NL2TL + syntax + semantics correction: e.g., HouseWorld2 ~83.5% success (reported in Table I), Rover ~79.7% (reported in Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4 few-shot translation + syntax/semantic corrections (AutoTAMP): NL2TL shows similar performance; explicit comparisons are reported in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>AutoTAMP ablation compares using GPT-4 few-shot translation vs NL2TL (fine-tuned); results show finetuned NL2TL performs similarly to GPT-4 few-shot when combined with syntax+semantic correction—no explicit ablation on probabilistic uncertainty modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NL2TL provides a competitive modular translator from NL to temporal logic; AutoTAMP finds that in-context learning with GPT-4 plus syntax/semantic re-prompting is competitive with fine-tuned NL2TL, but neither approach models probabilistic world models or LLM uncertainty explicitly in the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Integrated task and motion planning in belief space <em>(Rating: 2)</em></li>
                <li>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning <em>(Rating: 2)</em></li>
                <li>PDDL planning with pretrained large language models <em>(Rating: 2)</em></li>
                <li>Llms p: Empowering large language models with optimal planning proficiency <em>(Rating: 2)</em></li>
                <li>Translating natural language to planning goals with large-language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-975",
    "paper_id": "paper-dc135dabef805c7271f53ec4b212bdf8996cfd9d",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "PDDL",
            "name_full": "Planning Domain Definition Language",
            "brief_description": "A symbolic formalism for expressing planning domains and problems (including actions, predicates, objects, and goals), widely used in classical and temporal planning research and robotics.",
            "citation_title": "Pddl2. 1: An extension to pddl for expressing temporal planning domains",
            "mention_or_use": "mention",
            "system_name": "PDDL (classic symbolic planner representation)",
            "system_description": "PDDL is a domain/problem specification language for symbolic planners. It expresses objects, predicates, action schemas with preconditions and effects, and (in extensions) temporal and numeric constraints. In this paper PDDL is discussed as an example of a dedicated planning representation that classical TAMP and task planners rely on, but it is not used in the experiments.",
            "world_model_type": "PDDL",
            "world_model_description": "Represents world state as grounded and/or lifted symbolic predicates over objects; actions are parameterized schemas with deterministic preconditions and effects in classical PDDL, with PDDL extensions (e.g., PDDL2.1) providing temporal/numeric constructs. The paper references PDDL as a symbolic (typically deterministic) representation; probabilistic extensions (e.g., PPDDL) are not detailed here.",
            "uses_llm": null,
            "llm_role": "",
            "llm_model_name": "",
            "uncertainty_modeling": null,
            "uncertainty_type": "",
            "uncertainty_method": "",
            "planning_algorithm": "classical symbolic planners (general reference; not executed in this paper)",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": "PDDL is referenced as a general symbolic planning representation for robotics/TAMP domains; the paper does not tie PDDL usage to any text-based benchmark.",
            "performance_metric": "",
            "performance_value": "",
            "baseline_comparison": "",
            "has_ablation_uncertainty": null,
            "ablation_results": "",
            "key_findings": "The paper cites PDDL as a canonical symbolic task-specification language; it is discussed as an alternative to using natural language directly, but the experiments instead translate NL to STL. No experiments here use PDDL or its probabilistic variants.",
            "uuid": "e975.0",
            "source_info": {
                "paper_title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Belief-space TAMP",
            "name_full": "Integrated Task and Motion Planning in Belief Space",
            "brief_description": "An approach to integrate task-and-motion planning where planning operates over belief (probability) distributions over continuous states, thus explicitly modelling state uncertainty during planning.",
            "citation_title": "Integrated task and motion planning in belief space",
            "mention_or_use": "mention",
            "system_name": "Belief-space integrated TAMP (Kaelbling & Lozano-Pérez)",
            "system_description": "A class of methods that integrate discrete task planning and continuous motion planning by planning directly over belief states (distributions over the robot/environment continuous state), enabling decisions that account for uncertainty in sensing and dynamics. The AutoTAMP paper cites this work in related work to note approaches that explicitly reason in belief space for integrated planning.",
            "world_model_type": "belief states (probabilistic)",
            "world_model_description": "World state is represented as a probability distribution (belief) over continuous variables (e.g., robot pose, object poses). Actions have stochastic outcomes and measurement models; planning reasons about belief updates (Bayesian filtering) and selects actions to achieve task objectives under state uncertainty. The AutoTAMP paper references this approach but does not implement or evaluate it.",
            "uses_llm": false,
            "llm_role": "",
            "llm_model_name": "",
            "uncertainty_modeling": true,
            "uncertainty_type": "state uncertainty (belief over continuous states), epistemic/aleatoric depending on model",
            "uncertainty_method": "belief-state representation (probability distributions), Bayesian filtering / belief propagation (as per belief-space planning literature); specific methods not detailed in AutoTAMP",
            "planning_algorithm": "belief-space planning methods (integrated TAMP in belief space)",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": "Referenced as a general TAMP approach that models uncertainty; not applied to text-based environments in this paper.",
            "performance_metric": "",
            "performance_value": "",
            "baseline_comparison": "",
            "has_ablation_uncertainty": null,
            "ablation_results": "",
            "key_findings": "AutoTAMP cites belief-space integrated TAMP as existing work that explicitly models uncertainty in planning; the authors position their NL-to-STL translation + formal STL planner as an alternative that does not itself perform belief-space planning.",
            "uuid": "e975.1",
            "source_info": {
                "paper_title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLM-constructed world models (Guan et al.)",
            "name_full": "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning",
            "brief_description": "A referenced recent effort that uses pre-trained LLMs to construct and use world models for model-based task planning (as cited in AutoTAMP's related work).",
            "citation_title": "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning",
            "mention_or_use": "mention",
            "system_name": "LLM-constructed world models for model-based planning",
            "system_description": "Cited work that (per its title) leverages pre-trained LLMs to build and use world models for model-based task planning. In AutoTAMP this is mentioned in the context of related work exploring how pretrained LLMs can be used to construct symbolic/world models for downstream planning, but AutoTAMP does not implement or evaluate that system.",
            "world_model_type": "not specified in AutoTAMP (paper reference implies symbolic/world models constructed by LLMs; could include PDDL-like or other symbolic forms)",
            "world_model_description": "AutoTAMP only cites this paper title in related work and does not describe how states/actions/transitions are represented; the referenced work likely describes techniques for extracting symbolic/world-model elements from LLM outputs, but details are not present in AutoTAMP.",
            "uses_llm": true,
            "llm_role": "world model construction and utilization for planning (as implied by the cited title)",
            "llm_model_name": "pre-trained large language models (unspecified in AutoTAMP)",
            "uncertainty_modeling": null,
            "uncertainty_type": "",
            "uncertainty_method": "",
            "planning_algorithm": "model-based task planning (specific planner not detailed in AutoTAMP)",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": "Mentioned in related work as an approach to use LLMs to construct world models for planning; AutoTAMP does not report experiments in text-based benchmarks for this work.",
            "performance_metric": "",
            "performance_value": "",
            "baseline_comparison": "",
            "has_ablation_uncertainty": null,
            "ablation_results": "",
            "key_findings": "AutoTAMP acknowledges this line of work that uses LLMs to build or augment symbolic/world models for planning, but provides no experimental details; thus integration of LLM uncertainty into probabilistic symbolic models is not described in AutoTAMP itself.",
            "uuid": "e975.2",
            "source_info": {
                "paper_title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PDDL + LLM (Silver et al.)",
            "name_full": "PDDL planning with pretrained large language models",
            "brief_description": "A referenced work exploring the use of pretrained LLMs in conjunction with PDDL-based planning (cited in AutoTAMP's related work).",
            "citation_title": "PDDL planning with pretrained large language models",
            "mention_or_use": "mention",
            "system_name": "PDDL planning with pretrained LLMs",
            "system_description": "Cited work that combines pretrained LLMs with PDDL planning; AutoTAMP references it as related research investigating LLMs for symbolic planning domains (the AutoTAMP experiments instead use STL + an STL planner). The paper does not provide implementation details of this referenced system.",
            "world_model_type": "PDDL (symbolic planning representation)",
            "world_model_description": "As referenced, the approach ties pretrained LLMs into PDDL-based planning; AutoTAMP does not specify whether the referenced work models probabilities or uses deterministic PDDL. The referenced title suggests using LLMs to assist PDDL planning rather than introducing probabilistic PDDL variants.",
            "uses_llm": true,
            "llm_role": "assist in planning / generate planning content (as per citation context)",
            "llm_model_name": "pretrained large language models (unspecified in AutoTAMP)",
            "uncertainty_modeling": null,
            "uncertainty_type": "",
            "uncertainty_method": "",
            "planning_algorithm": "PDDL-based symbolic planning (details in cited work)",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": "Mentioned as related work combining LLMs with symbolic PDDL planning; AutoTAMP does not evaluate this approach in text-based environment benchmarks.",
            "performance_metric": "",
            "performance_value": "",
            "baseline_comparison": "",
            "has_ablation_uncertainty": null,
            "ablation_results": "",
            "key_findings": "AutoTAMP cites this paper to situate LLMs being used with symbolic planners; AutoTAMP's experiments do not explore probabilistic symbolic models extracted from LLMs or their integration into text-based environments.",
            "uuid": "e975.3",
            "source_info": {
                "paper_title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "NL2TL",
            "name_full": "NL2TL: Transforming natural languages to temporal logics using large language models",
            "brief_description": "A modular translation pipeline (fine-tuned T5-large) that maps natural language instructions to temporal logic specifications; referenced and used as an ablation/comparison in AutoTAMP.",
            "citation_title": "Nl2tl: Transforming natural languages to temporal logics using large language models",
            "mention_or_use": "use",
            "system_name": "NL2TL (modular NL → temporal logic translator)",
            "system_description": "NL2TL is a modular translation pipeline that uses a fine-tuned LLM (T5-large) trained on a corpus of NL paired with temporal logic expressions to translate language into temporal logic specifications (e.g., STL/LTL). AutoTAMP experiments include NL2TL as an alternative translator into task specifications; AutoTAMP still uses the STL planner and the same syntactic/semantic re-prompting verification steps on top of NL2TL outputs.",
            "world_model_type": "temporal logic (STL/LTL) task specifications (symbolic temporal logic)",
            "world_model_description": "Represents tasks as temporal logic formulae over predicates (e.g., in(room)), rather than an explicit probabilistic world model; NL2TL produces logical task specifications consumed by a formal STL planner. The representation is symbolic/deterministic at the specification level; the planner reasons about continuous trajectories to satisfy the logic constraints.",
            "uses_llm": true,
            "llm_role": "natural language → temporal-logic translator; used to construct task specifications",
            "llm_model_name": "T5-large (fine-tuned) as reported for NL2TL; AutoTAMP uses GPT-4 for re-prompting/error correction",
            "uncertainty_modeling": false,
            "uncertainty_type": "",
            "uncertainty_method": "",
            "planning_algorithm": "STL trajectory optimization planner (multi-agent STL planner cited [23])",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": "Used for robot-navigation TAMP domains in AutoTAMP experiments (2D/3D navigation and multi-agent scenarios); not a text-only environment benchmark like TextWorld.",
            "performance_metric": "task success rate (percentage of test cases where produced trajectory satisfies instruction/specification)",
            "performance_value": "In AutoTAMP experiments, NL2TL + syntax + semantics correction: e.g., HouseWorld2 ~83.5% success (reported in Table I), Rover ~79.7% (reported in Table II).",
            "baseline_comparison": "Compared to GPT-4 few-shot translation + syntax/semantic corrections (AutoTAMP): NL2TL shows similar performance; explicit comparisons are reported in the paper's tables.",
            "has_ablation_uncertainty": false,
            "ablation_results": "AutoTAMP ablation compares using GPT-4 few-shot translation vs NL2TL (fine-tuned); results show finetuned NL2TL performs similarly to GPT-4 few-shot when combined with syntax+semantic correction—no explicit ablation on probabilistic uncertainty modeling.",
            "key_findings": "NL2TL provides a competitive modular translator from NL to temporal logic; AutoTAMP finds that in-context learning with GPT-4 plus syntax/semantic re-prompting is competitive with fine-tuned NL2TL, but neither approach models probabilistic world models or LLM uncertainty explicitly in the planner.",
            "uuid": "e975.4",
            "source_info": {
                "paper_title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Integrated task and motion planning in belief space",
            "rating": 2
        },
        {
            "paper_title": "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning",
            "rating": 2
        },
        {
            "paper_title": "PDDL planning with pretrained large language models",
            "rating": 2
        },
        {
            "paper_title": "Llms p: Empowering large language models with optimal planning proficiency",
            "rating": 2
        },
        {
            "paper_title": "Translating natural language to planning goals with large-language models",
            "rating": 2
        }
    ],
    "cost": 0.014824749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers</h1>
<p>Yongchao Chen ${ }^{1,2}$, Jacob Arkin ${ }^{1}$, Charles Dawson ${ }^{1}$, Yang Zhang ${ }^{3}$, Nicholas Roy ${ }^{1}$, and Chuchu Fan ${ }^{1}$</p>
<h4>Abstract</h4>
<p>For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website ${ }^{1}$ for prompts, videos, and code.</p>
<h2>I. INTRODUCTION</h2>
<p>Providing agents with the ability to find and execute optimal plans for complex tasks is a long-standing goal in robotics. Robots need to not only reason about the task in the environment and find a satisfying sequence of actions but also verify the feasibility of executing those actions given the robot's motion capabilities. This problem is referred to as task and motion planning (TAMP), and there has been considerable research on efficient algorithms [1]. Classic solutions rely on specifying tasks in a dedicated planning representation, such as PDDL [2] or Temporal logics [3], that is both sufficiently expressive to specify task complexities (e.g. constraints on task execution) and amenable to such algorithms [2], [3], [4], [5].</p>
<p>While this approach to task specification has been quite successful, directly using these representations requires training and experience, making them poor interfaces for nonexpert users. As an alternative, natural language (NL) provides an intuitive and flexible way to describe tasks. Pretrained large language models (LLMs) have demonstrated surprisingly good performance on many language-related tasks [6], and there has been an associated burst of research</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>applying them to task execution [7], task planning [8], [9], [10], [11] and TAMP [12], [13].</p>
<p>Promising early efforts used LLMs as direct task planners [8] generating a sequence of sub-tasks based on a set of natural language instructions, but these approaches were limited by a lack of feedback and inability to verify whether subtask sequences are executable. Further research addressed executability by connecting sub-tasks to control policy affordance functions [9], providing environmental feedback of robot actions [11], and interleaving action feasibility checking with LLM action proposals [12]; this last work also addressed long-horizon action dependencies. However, these approaches struggle with complex tasks involving temporally-dependent multi-step actions, action sequence optimization [9], [11], and constraints on task execution [12]. Furthermore, these frameworks factor the planning problem and use LLMs to infer a task plan separately from the motion plan. In many situations, the task and motion plan must be optimized together to fulfill the task. For instance, when the task is 'reach all locations via the shortest path', the order of places to be visited (task planning) depends on the geometry of the environment and the related motion optimization. Unfortunately, we find that LLMs do not seem capable of directly generating trajectories, possibly due to limitations in complex spatial and numerical reasoning [14], [15].</p>
<p>To benefit from both the user-friendliness of NL and the capabilities of existing TAMP algorithms, we approach the problem by using LLMs to translate from high-level task descriptions to formal task specifications. We are not the first to use LLMs in this way [16], [17], but our work addresses some limitations of prior approaches. Previous work translated natural language to Linear Temporal Logics (LTL) [18], which only considered the problem of task planning, and PDDL problem descriptions [16] or PDDL goals [17]. Here we utilize Signal Temporal Logic (STL) as the intermediary representation, allowing for more expressive constraints than LTL and facilitating integrated task and motion planning as with PDDL [19].</p>
<p>The LLM translation process can produce malformed (syntax errors) and semantically misaligned (semantic errors) formal task specifications. To address syntax errors, we adopt an existing iterative re-prompting technique that relies on an external syntax verifier to prompt the LLM with the specific syntactic error for correction [20]. Unfortunately, the lack of an external verifier makes this technique inapplicable for a semantic misalignment between the original natural language instruction and the translated specification. To address this problem, we contribute a novel autoregressive re-prompting</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Illustration of different approaches applying LLMs for task and motion planning; our work contributes the LLM-As-Translator &amp; Checker approach. Each approach accepts a natural language instruction and environment state as input and outputs a robot trajectory.</p>
<p>technique that uses an LLM to evaluate whether the generated plan is semantically consistent with the original instruction. We re-prompt the model to check the alignment between the original instruction and the generated plan by providing the context of the instruction, the generated STL, and the output of the planner. We conduct comprehensive experiments in challenging 2D task domains, including several multiagent tasks, and find that our approach outperforms direct LLM planning for tasks with hard geometric and temporal constraints. We show that, when combined with automatic syntactic correction, our technique significantly improves task success rates. We conduct an ablation study over the translation step by integrating a fine-tuned NL-to-STL model [21] with the AutoTAMP framework and show that GPT-4 few-shot learning is competitive with fine-tuning. In addition to our code, we publish a dataset of 1400 test cases consisting of the language instructions, environments, generated STL, and planner trajectory outputs. We conclude that in-context learning with pre-trained LLMs is well suited for language-to-task-specification translation for solving TAMP problems.</p>
<h2>II. PROBLEM DESCRIPTION</h2>
<p>As shown in Figure 1, we aim to convert a natural language instruction, including spatial and temporal constraints, into a motion plan for a robot encoded as a set of timed waypoints, e.g., $(x_i, y_i, t_i)$. The environment state is encoded as set of named obstacles described as polygons and is provided as additional context. Our task is to generate a constraintsatisfying trajectory based on the given instruction and the environment state. The robot must not surpass its maximum velocity, and the total operation time should not exceed the task time limit. We assume that the full trajectory is a linear interpolation between the timed waypoints; complex trajectories can be specified by dense waypoint sequences.</p>
<h2>III. METHODS</h2>
<p>Figure 1 illustrates three of the approaches we compare in our work, each using LLMs in some capacity. Each takes as input (1) a text-based representation of the global environment state, (2) in-context examples for few-shot learning, and (3) a natural language instruction. The LLM-As-Translator &amp; Checker approach is the contribution of this paper. Details and examples of context for prompting and re-prompting can be found in our code repository<sup>§</sup>.</p>
<h3>A. LLM End-to-end Motion Planning</h3>
<p>One natural idea is to use an LLM for both task and motion planning by directly generating a trajectory for a given language instruction; we refer to this as LLM End-to-end Motion Planning. In cases where the generated trajectory violates constraints, we re-prompt the model with the constraint violation to produce another trajectory, allowing up to five such re-prompts. Figure 2 shows this pipeline, including a specific failure case with two constraint-violating trajectories. The LLM End-to-end Motion Planning violates the constraints multiple times even with direct correction re-prompts. This results from the poor spatial and numerical reasoning abilities of LLMs.</p>
<h3>B. LLM Task Planning</h3>
<p>A more common approach is to use an LLM to handle the task planning by directly generating a sequence of subtasks from a given language instruction; we refer to this as LLM Task Planning. To generate a final trajectory, the subtasks are handled by an independent motion planner. In this work, these subtasks are limited to navigation actions, and the motion planning is handled by the STL planner used by our proposed approach; this permits fair comparison of results across methods. Each subtask is converted to STL to be consumed by the planner. We evaluate and compare against three methods that each use LLMs for task planning: (1) Naive Task Planning, (2) SayCan, and (3) LLM Task Planning + Feedback.</p>
<p><strong>Naive Task Planning</strong> As proposed by [8], we evaluate using LLMs to generate the entire subtask sequence without checking for executability.</p>
<p><strong>SayCan</strong> Alternatively, an LLM can be iteratively prompted to generate each subsequent subtask conditioned on the previous subtasks in the sequence. The next subtask can be selected from the top K candidates by combining the language model likelihood with a feasibility likelihood of the candidate action and choosing the most-likely next subtask, as proposed by [9]. We set K to 5 in our evaluations.</p>
<p><strong>LLM Task Planning + Feedback</strong> A third task planning method combines full sequence generation with feasibility checking to both find subtask sequences that satisfy the full task and verify their feasibility before execution. For any infeasible subtasks, the LLM can be re-prompted with feedback about the infeasible actions to generate a new subtask sequence. This is similar to the hierarchical method proposed by [12] but with feedback for re-prompting.</p>
<h3>C. Autoregressive LLM Specification Translation&amp;Checking + Formal Planner</h3>
<p>Unlike LLM Task Planning, our approach translates NL to STL with an LLM and then plans the trajectory with an</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. GPT-4 failure case for direct end-to-end trajectory planning. The orange line shows the correct path obeying the instruction. The purple and gray dashed lines show the trajectories from GPT-4 after first and second prompts, respectively. GPT-4 generates a list of (x, y) locations with associated timestamps. The initial prompt describes the language modeling task, environment state, and instruction. Each object is a rectangle described by (x, y) boundaries.</p>
<p>STL planner, as shown in Figure 1. We include two reprompting techniques to improve translation performance: one for syntactic errors and another for semantic errors. By "semantic error", we mean a misalignment between the intended task described in natural language and the STL expression to which it is translated. Figure 3 shows the structure of the context for re-prompting the model for semantic error correction; we include a full prompt example in our code repository‡.</p>
<p><strong>Signal Temporal Logic Syntax</strong> In this work, we use STL [22] as a formal task specification that supports continuous real-time constraints suitable for time-critical missions. An STL formula is defined recursively according to the following syntax:</p>
<p>$$
\phi ::= \pi^\mu | \neg \phi | \phi \wedge \varphi | \phi \vee \varphi | \mathbf{F}<em _a_b_="[a,b]">{[a,b]} \phi | \mathbf{G}</em>
$$} \phi | \phi \mathbf{U}_{[a,b]} \varphi \tag{1</p>
<p>where $\phi$ and $\varphi$ are STL formulas, and $\pi^\mu$ is an atomic predicate. $\neg$ (negation), $\wedge$ (and), $\vee$ (or), $\Rightarrow$ (imply), and $\Leftrightarrow$ (equal) are logical operators. $\mathbf{F}<em _a_b_="[a,b]">{[a,b]}$ (eventually/finally), $\mathbf{G}</em>$ (until) are temporal operators with real-time constraints $t \in [a,b]$. The action primitives in this work are 'enter(room_name)' and 'not_enter(room_name)'.}$ (always/globally), and $\mathbf{U}_{[a,b]</p>
<p><strong>STL Trajectory Planner</strong> We use a state-of-the-art multi-agent STL planner [23] that uses piece-wise linear reference paths defined by timed waypoints to recursively encode the constraints expressed in the provided STL expression. It defines the validity of an STL formula with respect to a trajectory and then optimizes the trajectory to maximize the validity. The planner not only searches for a sub-task sequence but also optimizes the time efficiency under dynamical constraints of robot maximum velocity. Here we assume that the locations and shapes of all the objects/rooms in the whole environment are known, which serves as the environment information to the STL planner.</p>
<p><strong>Syntactic Checking &amp; Semantic Checking</strong> Open-loop translation can suffer from syntactic and semantic errors. We use two re-prompting techniques to automatically correct such errors. Like [20], we use a verifier to check for syntax errors (we use a simple rules-based STL syntax checker); any errors are provided as feedback when re-prompting the LLM to generate corrected STL. We repeat until no errors are found (up to five iterations). For semantic errors, we propose a novel autoregressive re-prompting technique; we provide the STL planner's generated state sequence (i.e., [[in(road), 0], [in(red kitchen), 0.5], [in(blue restroom2), 1.2],...]) as context alongside the original instruction and ask the LLM to check whether the plan aligns with the instruction's semantics. If it does not, the LLM is prompted to modify the STL, which repeats the syntactic and semantic re-prompting. This process terminates in the case of no detected errors or no change in STL (up to three iterations). The structure of the semantic error prompt is shown in Figure 3; full example prompts can be found in our code repository‡.</p>
<h1>IV. EXPERIMENTAL DESIGN</h1>
<p>Each task scenario is set in a 2D environment and entails navigation of one or more robots; the robots have extent in the environment and are initialized with varying start positions. Each environment consists of regions with shapes, locations, and properties (e.g., color, name, function). For each method, the LLM is initially prompted with a description of the language task (e.g., task planning or translation) and five in-context examples for that task. To mitigate variance across prompts, we initially tested six different sets of examples for each method and chose the one that performed best. Through this testing, we found that the variance over prompts was insignificant relative to overall performance.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. High-level structure of the prompt used for AutoTAMP. The arrow on the right indicates re-prompting for syntax error correction. The arrow on the left indicates re-prompting in cases of semantic errors.</p>
<p>We evaluated the different methods described in Section III across six different task scenarios (three single-agent and three multi-agent) with different combinations of geometric and temporal constraints. For each scenario description below, we indicate the presence of these constraints below with G and T, respectively. For each method, we evaluate performance with both GPT-3 and GPT-4 as the LLM. Note that in multi-agent scenarios, we do not test SayCan or LLM Task Planning + Feedback because these methods are not straight-forwardly adaptable for multiple agents. For multiagent tasks, the agents are assigned a subtask and a time for completion at each time step; since the time for completion is often different, it is not obvious how/when to check and provide feedback. We also terminate and report failure for test cases that take more than 90 minutes. We automatically check resulting trajectories via hard-coded checkers. The full set of experiments took two weeks using four 16-core CPUs; the cost of LLM API calls for evaluating all of the approaches was ~1500 USD.</p>
<p><strong>HouseWorld1 (single-agent)</strong> As shown in Figure 4(a), this is a house environment from [24]. We first manually constructed 10 different instructions of varying complexity before prompting GPT-4 to paraphrase each into 9 differently worded instructions with the same meaning, resulting in 100 total instructions for this environment. For each instruction, we randomly initialize between two start-end position pairs for 200 total test cases. For this scenario, we do not impose a hard time constraint for the planned trajectory.</p>
<p><strong>HouseWorld2 (T, single-agent)</strong> This scenario is identical to HouseWorld1, but each planned trajectory is subjected to a hard time constraint. This time limit is pre-determined by</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. HouseWorld and Chip's Challenge are single-agent scenarios. Overcooked, Rover, and Wall are multi-agent scenarios. The black square in Overcooked is inadmissible. The lines indicate the correct trajectories following the instructions. For the HouseWorld and Chip's Challenge environments, the black round dot and pentagonal dot indicate the start and end positions, respectively.</p>
<p>completing the correct trajectory with 0.8 maximum velocity.</p>
<p>The remaining task scenarios were designed with specific rules and goals for the agent(s) to follow. For each scenario, GPT-4 was used to paraphrase the original description into 20 unique variants with the same meaning, which are further checked by humans. We instantiate three different instances of the environment for each scenario and randomize five different start/end location pairs for a total of 300 test cases.</p>
<p><strong>Chip's Challenge (G, single-agent)</strong> Figure 4(b) shows a scenario inspired by Chip's Challenge, a classic puzzle solving game with strict geometric and logical constraints. The robot must reach all goal regions (blue) but must acquire a unique key to pass through the corresponding door.</p>
<p><strong>Overcooked (G &amp; T, multi-agent)</strong> Figure 4(c) shows a scenario inspired by Overcooked, a popular cooking simulation game with strict time constraints. The agents must cooperatively gather ingredients and return to CookingRoom in a limited time. The multi-agent motion planning is challenged by limited space for agents to maneuver.</p>
<p><strong>Rover (G &amp; T, multi-agent)</strong> Figure 4(d) is a scenario used by [23]. Multiple agents must reach each observation region (blue) before transmitting their observations from a red region, all while subjected to time and energy constraints.</p>
<p><strong>Wall (G &amp; T, multi-agent)</strong> Figure 4(e) is also from [23]. Multiple agents must occupy each goal region (blue) while subject to a time constraint and a maneuver bottleneck.</p>
<h2>V. RESULTS</h2>
<p>We report the task success rates for the single-agent and multi-agent scenarios in Table I and Table II, respectively. For HouseWorld1 (Figure 4(a)) with no hard time constraint, we find that all methods using LLMs as task planners outperform our approach; whereas our approach can fail due to translation errors, this environment permits direct trajectories between any two positions and thus lacks geometric challenges that direct task planning methods will struggle with.</p>
<p>TABLE I
TAsk SUCESS RATES FOR SINGLE-AGENT SCENARIOS. EACH SCENARIO'S CONSTRAINTS ARE LISTED IN THE TABLE.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>HouseWorld1</td>
<td>HouseWorld2</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Soft Time Constraint</td>
<td>Hard Time Constraint</td>
<td>Hard Geometric Constraints</td>
</tr>
<tr>
<td>$\begin{aligned} &amp; \text { OPT-1 } \ &amp; \text { OPT- } \end{aligned}$</td>
<td>LLMs as Motion Planners</td>
<td>End-to-end Motion Planning</td>
<td>$0.0 \%$</td>
<td>$0.0 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Task Planners</td>
<td>Task Planning (naive)</td>
<td>$74.0 \%$</td>
<td>$36.0 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SayCan</td>
<td>$75.5 \%$</td>
<td>$36.0 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Task Planning (feedback)</td>
<td>79.0\%</td>
<td>$40.0 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Translators</td>
<td>No Corrections</td>
<td>$28.0 \%$</td>
<td>$27.0 \%$</td>
<td>$29.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax</td>
<td>$49.0 \%$</td>
<td>$47.0 \%$</td>
<td>$66.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax + Semantics (AutoTAMP)</td>
<td>$62.0 \%$</td>
<td>$62.0 \%$</td>
<td>74.3\%</td>
</tr>
<tr>
<td></td>
<td>LLMs as Motion Planners</td>
<td>End-to-end Motion Planning</td>
<td>$9.5 \%$</td>
<td>$9.5 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Task Planners</td>
<td>Task Planning (naive)</td>
<td>$90.0 \%$</td>
<td>$45.0 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Saycan</td>
<td>$90.0 \%$</td>
<td>$47.5 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Task Planning (feedback)</td>
<td>92.0\%</td>
<td>$49.0 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Translators</td>
<td>No Corrections</td>
<td>$43.5 \%$</td>
<td>$42.0 \%$</td>
<td>$42.7 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax</td>
<td>$59.5 \%$</td>
<td>$59.0 \%$</td>
<td>$70.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax + Semantics (AutoTAMP)</td>
<td>$82.5 \%$</td>
<td>$82.0 \%$</td>
<td>87.7\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>NL2TL + Syntax + Semantics</td>
<td>-</td>
<td>83.5\%</td>
<td>$86.0 \%$</td>
</tr>
</tbody>
</table>
<p>TABLE II TASK SUCCESS RATES FOR MULTI-AGENT SCENARIOS. EACH SITUATION HAS HARD CONSTRAINTS ON TIME AND GEOMETRY.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Overcooked</td>
<td>Rover</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Hard Time \&amp; Geometric Constraints</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LLMs as Motion Planners</td>
<td>End-to-end Motion Planning</td>
<td>$0.0 \%$</td>
<td>$0.0 \%$</td>
<td>$0.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Task Planners</td>
<td>Task Planning (naive)</td>
<td>$13.3 \%$</td>
<td>$0.0 \%$</td>
<td>$7.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Translators</td>
<td>No Corrections</td>
<td>$25.0 \%$</td>
<td>$22.0 \%$</td>
<td>$74.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax Corrections</td>
<td>$70.0 \%$</td>
<td>$35.0 \%$</td>
<td>$85.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax + Semantic Corrections (AutoTAMP)</td>
<td>89.0\%</td>
<td>60.7\%</td>
<td>89.7\%</td>
</tr>
<tr>
<td></td>
<td>LLMs as Motion Planners</td>
<td>End-to-end Motion Planning</td>
<td>$5.0 \%$</td>
<td>$0.0 \%$</td>
<td>$6.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Task Planners</td>
<td>Task Planning (naive)</td>
<td>$17.0 \%$</td>
<td>$0.0 \%$</td>
<td>$47.0 \%$</td>
</tr>
<tr>
<td></td>
<td>LLMs as Translators</td>
<td>No Corrections</td>
<td>$85.0 \%$</td>
<td>$46.0 \%$</td>
<td>$95.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax Corrections</td>
<td>$94.0 \%$</td>
<td>$67.0 \%$</td>
<td>$95.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Syntax + Semantic Corrections (AutoTAMP)</td>
<td>100.0\%</td>
<td>$79.0 \%$</td>
<td>100.0\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>NL2TL + Syntax + Semantic Corrections</td>
<td>100.0\%</td>
<td>79.7\%</td>
<td>100.0\%</td>
</tr>
</tbody>
</table>
<p>When adding a strict time constraint (HouseWorld2), we see that such methods perform much worse while AutoTAMP's success rate persists. For the other tasks that include geometric constraints, LLM End-to-end Motion Planning and Naive Task Planning both perform quite poorly. Unsurprisingly, we observe a general trend that GPT-4 outperforms GPT-3.</p>
<p>We find that most failures for LLM Task Planning methods result from task execution time violation and sequencing of actions for long-horizon tasks. For example, Chip's Challenge requires the robot to efficiently collect keys for future doors. Also, the Naive Task Planning method fails to avoid collisions in the multi-agent scenarios. Failures for methods that translate to STL primarily are due to incorrect translation; while our re-prompting techniques help address this issue, there remain cases of poor translation.</p>
<p>Ablation Studies In Table I and Table II, we evaluate the impact of syntactic and semantic error correction on using LLMs to translate to STL. The results show that translation with no error correction has modest success across task scenarios, but both syntactic and semantic error correction significantly improve performance; this trend is present across all scenarios. We also evaluate replacing a pretrained LLM for translation with a state-of-the-art modular translation pipeline, NL2TL, that uses a smaller LLM (T5large) fine-tuned on a multi-domain corpus of 30 K examples of instructions paired with their corresponding temporal logic expressions [21]; the error correction steps were still performed by GPT-4. Integrating NL2TL performs similarly to using a pre-trained LLM for translation, providing a modest improvement in HouseWorld2 and Rover. We note that incorporating the two re-prompting techniques for error correction is competitive with fine-tuning since we do not rely on additional data or training.</p>
<p>3D Simulation In supplemental videos, we demonstrate plans generated via AutoTAMP in two 3D simulated environments: a drone navigation scenario that requires reasoning about height, and a tabletop color sorting manipulation scenario. We did not incorporate the semantic check for these demos. The STL planner is directly applicable to the drone scenario using timed waypoints, as done in the 2D experiments. For manipulation tasks, we integrated a simple discrete planner to handle the dynamics mode transitions. We discuss this more in Section VII.</p>
<p>Physical Demonstrations We demonstrate AutoTAMP on physical differential-drive robots via the remotely-accessible Robotarium platform [25] for the Overcooked, Rover, Wall, and Chip's Challenge scenarios. We track the planned trajectories using a low-level controller that also includes a control barrier function to prevent collisions between robots. This controller and the underlying nonlinear dynamics in-</p>
<p>duce a tracking error; we account for this by padding obstacles at planning time. Obstacles are displayed in the robot workspace using an overhead projector. These physical demos provide evidence that our method can be applied to real-world navigation task and motion planning. They are included as part of supplemental videos.</p>
<h2>VI. RELATED WORK</h2>
<p>Task and Motion Planning Planning for robotics involves both high-level, discrete planning of tasks [5] and lowlevel continuous planning of motions [26]; solving these simultaneously is referred to as task and motion planning [1]. Modern approaches either attempt to satisfy the motion constraints prior to action sequencing [27], [28], [29], find action sequences then satisfy the motion constraints [30], [31], [32], [19], or interleave these steps [33], [34], [35]. For tasks specified in temporal logic, existing methods either use multi-layer planning [36], like the aforementioned approaches, or direct optimization via a mixed-integer linear program [37], [23] or a non-linear program [38]. Our work focuses on translating natural language to STL, relying on [23] as a TAMP solver, but can be integrated with other STL-based planners.</p>
<p>LLMs for TAMP Recent claims about the impressive reasoning capabilities of LLMs [6], [39] have led to interest in such models for task and motion planning. One approach is to directly use LLMs as planners [8], [9], [12], [11], [7], [10], [13]. Initial work showed that zero-shot generation of an action sequence from a high-level task description had relatively poor executability, but few-shot in-context learning, constraining output to admissible actions, and iterative action generation significantly improved performance [8]. Subsequent efforts grounded the primitive actions to motion control policies, using affordance functions to guide LLM-based task planning [9] and TAMP [12], also adding feedback [11]. Other work focused on how prompting can inform task execution[7], [13]. Despite these successes, however, there is evidence that LLMs perform poorly on more realistic tasks [15], [40], motivating different approaches. While we are interested in LLMs for TAMP, our work does not directly use LLMs as planners.</p>
<p>Translating Language to Task Representations A natural alternative is to rely on dedicated planners by mapping from natural language to a planning representation. There is a rich history of parsing natural language into formal semantic representations [41], [42], [43], of which we only provide a relatively small sampling. The robotics community adopted parsing and other techniques to map language to such representations as lambda calculus [44], [45], motion planning constraints [46], linear temporal logic [47], [48], [49], [50], and signal temporal logic [51], [52], among others [53]. We refer readers to [54] for a more thorough review. To address challenges of data availability, task generalization, linguistic complexity, common sense reasoning, and more, recent work has applied LLMs to this translation problem. Modular approaches have used LLMs to extract referring expressions with corresponding logic propositions to then construct a full temporal logic specification [55], [21]. Relying on LLMs for direct translation, other work has mapped from language to PDDL goals [17] or full PDDL problems [56], [16]. Our work similarly translates to a task specification, but we can represent complex constraints (e.g. temporal), and we introduce a novel mechanism for automatic detection and correction of semantic errors. An interesting alternative maps language to code [57], which is highly expressive but does not easily optimize or provide behavior guarantees for long-horizon tasks.</p>
<p>Re-prompting of LLMs The quality of LLM output is greatly improved with useful context, such as few-shot incontext learning for novel tasks [6]. LLMs for TAMP are typically also provided task-relevant information, such as environment state or admissible actions [10]. Re-prompting with additional context based on LLM output has been shown to be extremely beneficial, such as with iterative action generation [8], environmental feedback [11], inadmissible actions [8], [9], [12], unmet action preconditions [58], [56], code execution errors [59], and syntactic errors in structured output [20]. Our work uses the same syntactic correction reprompting technique as [20], but we also introduce automatic detection and correction of semantic errors via re-prompting.</p>
<h2>VII. CONCLUSION</h2>
<p>This paper presented AutoTAMP, a framework for using pre-trained LLMs as both (1) translators from language task descriptions to formal task specifications (e.g. STL) via fewshot in-context learning and (2) checkers of syntactic and semantic errors via corrective re-prompting, in which we contributed a novel autoregressive re-prompting technique for semantic errors. Our experimental results show using LLMs to translate to task specifications that can be solved via a formal planner outperforms approaches that use LLMs directly as planners when handling tasks with complex geometric and temporal constraints.</p>
<p>We note a few limitations. First, though our results rely on using the best prompt out of several candidates, alternatives may elicit better performance. However, we expect the trends between methods to persist even with better prompts, supporting the conclusion that LLMs are not well suited for directly solving complex TAMP. Second, the cost of planning time is high, especially when there are multiple iterations of re-prompting. Further work is needed to address the runtime of formal planners and LLM inference. Third, the STL planner used in this work is not immediately applicable to manipulation tasks due to the optimization methods used in the planner; however, our approach does not depend on this specific planner, and we believe it can be integrated with STL planners more suitable for such TAMP domains.</p>
<h2>VIII. ACKNOWLEDGEMENTS</h2>
<p>This work was supported by ONR under Award N00014-22-1-2478, Army Research Laboratory under DCIST CRA W911NF-17-2-0181, and MIT-IBM Watson AI Lab. This article solely reflects the conclusions of its authors.</p>
<h2>REFERENCES</h2>
<p>[1] C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-Pérez, "Integrated task and motion planning," Annual review of control, robotics, and autonomous systems, vol. 4, pp. 265-293, 2021.
[2] M. Fox and D. Long, "Pddl2. 1: An extension to pddl for expressing temporal planning domains," Journal of artificial intelligence research, vol. 20, pp. 61-124, 2003.
[3] E. A. Emerson, "Temporal and modal logic," in Formal Models and Semantics. Elsevier, 1990, pp. 995-1072.
[4] K. He, M. Lahijanian, L. E. Kavraki, and M. Y. Vardi, "Towards manipulation planning with temporal logic specifications," in 2015 IEEE International Conference on Robotics and Automation (ICRA), 2015, pp. 346-352.
[5] R. E. Fikes and N. J. Nilsson, "Strips: A new approach to the application of theorem proving to problem solving," Artificial Intelligence, vol. 2, no. 3, pp. 189-208, 1971. [Online]. Available: https://www.sciencedirect.com/science/article/pii/0004370271900105
[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[7] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser, "Tidybot: Personalized robot assistance with large language models," arXiv preprint arXiv:2305.05658, 2023.
[8] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," in International Conference on Machine Learning. PMLR, 2022, pp. 9118-9147.
[9] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al., "Do as i can, not as i say: Grounding language in robotic affordances," arXiv preprint arXiv:2204.01691, 2022.
[10] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, "ProgPrompt: Generating situated robot task plans using large language models," in International Conference on Robotics and Automation (ICRA), 2023. [Online]. Available: https://arxiv.org/abs/2209.11302
[11] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al., "Inner monologue: Embodied reasoning through planning with language models," arXiv preprint arXiv:2207.05608, 2022.
[12] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg, "Text2motion: From natural language instructions to feasible plans," arXiv preprint arXiv:2303.12153, 2023.
[13] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, "Task and motion planning with large language models for object rearrangement," arXiv preprint arXiv:2303.06247, 2023.
[14] N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi, "Chatgpt empowered long-step robot control in various environments: A case application," arXiv preprint arXiv:2304.03893, 2023.
[15] K. Valmeekam, A. Ohno, S. Sreedharan, and S. Kambhampati, "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)," arXiv preprint arXiv:2206.10498, 2022.
[16] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, "Llms p: Empowering large language models with optimal planning proficiency," arXiv preprint arXiv:2304.11477, 2023.
[17] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, "Translating natural language to planning goals with large-language models," arXiv preprint arXiv:2302.05128, 2023.
[18] J. Pan, G. Chou, and D. Berenson, "Data-efficient learning of natural language to linear temporal logic translators for robot task specification," arXiv preprint arXiv:2303.08006, 2023.
[19] C. R. Garrett, T. Lozano-Pérez, and L. P. Kaelbling, "Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning," in Proceedings of the International Conference on Automated Planning and Scheduling, vol. 30, 2020, pp. 440-448.
[20] M. Skreta, N. Yoshikawa, S. Arellano-Rubach, Z. Ji, L. B. Kristensen, K. Darvish, A. Aspuru-Guzik, F. Shkurti, and A. Garg, "Errors are useful prompts: Instruction guided task programming with verifierassisted iterative prompting," arXiv preprint arXiv:2303.14100, 2023.
[21] Y. Chen, R. Gandhi, Y. Zhang, and C. Fan, "Nl2tl: Transforming natural languages to temporal logics using large language models," arXiv preprint arXiv:2305.07766, 2023.
[22] O. Maler and D. Nickovic, "Monitoring temporal properties of continuous signals," in Formal Techniques, Modelling and Analysis of Timed and Fault-Tolerant Systems: Joint International Conferences on Formal Modeling and Analysis of Timed Systems, FORMATS 2004, and Formal Techniques in Real-Time and Fault-Tolerant Systems, FTRTFT 2004, Grenoble, France, September 22-24, 2004. Proceedings. Springer, 2004, pp. 152-166.
[23] D. Sun, J. Chen, S. Mitra, and C. Fan, "Multi-agent motion planning from signal temporal logic specifications," IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 3451-3458, 2022.
[24] C. Finucane, G. Jing, and H. Kress-Gazit, "Ltlmop: Experimenting with language, temporal logic and robot control," in 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2010, pp. 1988-1993.
[25] S. Wilson, P. Glotfelter, L. Wang, S. Mayya, G. Notomista, M. Mote, and M. Egerstedt, "The robotarium: Globally impactful opportunities, challenges, and lessons learned in remote-access, distributed control of multirobot systems," IEEE Control Systems Magazine, vol. 40, no. 1, pp. 26-44, 2020.
[26] S. M. LaValle, Planning algorithms. Cambridge university press, 2006.
[27] J. Ferrer-Mestres, G. Frances, and H. Geffner, "Combined task and motion planning as classical ai planning," arXiv preprint arXiv:1706.06927, 2017.
[28] C. R. Garrett, T. Lozano-Perez, and L. P. Kaelbling, "Ffrob: Leveraging symbolic planning for efficient task and motion planning," The International Journal of Robotics Research, vol. 37, no. 1, pp. 104136, 2018.
[29] A. Akbari, J. Rosell, et al., "Task planning using physics-based heuristics on manipulation actions," in 2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA). IEEE, 2016, pp. 1-8.
[30] F. Lagriffoul and B. Andres, "Combining task and motion planning: A culprit detection problem," The International Journal of Robotics Research, vol. 35, no. 8, pp. 890-927, 2016.
[31] J. Wolfe, B. Marthi, and S. Russell, "Combined task and motion planning for mobile manipulation," in Proceedings of the International Conference on Automated Planning and Scheduling, vol. 20, 2010, pp. $254-257$.
[32] S. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel, "Combined task and motion planning through an extensible plannerindependent interface layer," in 2014 IEEE international conference on robotics and automation (ICRA). IEEE, 2014, pp. 639-646.
[33] M. Colledanchise, D. Almeida, and P. Ögren, "Towards blended reactive planning and acting using behavior trees," in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 8839-8845.
[34] L. P. Kaelbling and T. Lozano-Pérez, "Integrated task and motion planning in belief space," The International Journal of Robotics Research, vol. 32, no. 9-10, pp. 1194-1227, 2013.
[35] E. Fernandez-Gonzalez, B. Williams, and E. Karpas, "Scottyactivity: Mixed discrete-continuous planning with convex optimization," Journal of Artificial Intelligence Research, vol. 62, pp. 579-664, 2018.
[36] K. He, M. Lahijanian, L. E. Kavraki, and M. Y. Vardi, "Towards manipulation planning with temporal logic specifications," in 2015 IEEE international conference on robotics and automation (ICRA). IEEE, 2015, pp. 346-352.
[37] M. Katayama, S. Tokuda, M. Yamakita, and H. Oyama, "Fast ltlbased flexible planning for dual-arm manipulation," in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 6605-6612.
[38] R. Takano, H. Oyama, and M. Yamakita, "Continuous optimizationbased task and motion planning with signal temporal logic specifications for sequential manipulation," in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 8409-8415.
[39] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," in ICML 2022 Workshop on Knowledge Retrieval and Language Models, 2022. [Online]. Available: https://openreview.net/forum?id=6p3AuaHAFiN
[40] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. LozanoPérez, and L. P. Kaelbling, "PDDL planning with pretrained large language models," in NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. [Online]. Available: https: //openreview.net/forum?id=1QMMUB4zfl</p>
<p>[41] L. S. Zettlemoyer and M. Collins, "Learning to map sentences to logical form: structured classification with probabilistic categorial grammars," in Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, 2005, pp. 658-666.
[42] L. Zettlemoyer and M. Collins, "Online learning of relaxed ccg grammars for parsing to logical form," in Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007, pp. 678-687.
[43] Y. W. Wong and R. J. Mooney, "Learning for semantic parsing with statistical machine translation," in Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. Association for Computational Linguistics, 2006, pp. 439-446.
[44] J. Dzifcak, M. Scheutz, C. Baral, and P. Schermerhorn, "What to do and how to do it: Translating natural language directives into temporal and dynamic logic representation for goal management and action execution," in 2009 IEEE International Conference on Robotics and Automation. IEEE, 2009, pp. 4163-4168.
[45] Y. Artzi and L. Zettlemoyer, "Weakly supervised learning of semantic parsers for mapping instructions to actions," Transactions of the Association for Computational Linguistics, vol. 1, pp. 49-62, 2013.
[46] T. M. Howard, S. Tellex, and N. Roy, "A natural language planner interface for mobile manipulators," in 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 66526659.
[47] A. Boteanu, J. Arkin, T. Howard, and H. Kress-Gazit, "A model for verifiable grounding and execution of complex language instructions," in Proceedings of the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems, Oct. 2016.
[48] N. Gopalan, D. Arumugam, L. L. Wong, and S. Tellex, "Sequence-tosequence language grounding of non-markovian task specifications." in Robotics: Science and Systems, vol. 2018, 2018.
[49] R. Patel, E. Pavlick, and S. Tellex, "Grounding language to nonmarkovian tasks with no supervision of task specifications." in Robotics: Science and Systems, vol. 2020, 2020.
[50] H. Kress-Gazit, G. E. Fainekos, and G. J. Pappas, "Translating structured english to robot controllers," Advanced Robotics, vol. 22, no. 12, pp. 1343-1359, 2008.
[51] J. He, E. Bartocci, D. Ničković, H. Isakovic, and R. Grosu, "Deepstl: from english requirements to signal temporal logic," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 610-622.
[52] S. Mohammadinejad, J. Thomason, and J. V. Deshmukh, "Interactive learning from natural language and demonstrations using signal temporal logic," arXiv preprint arXiv:2207.00627, 2022.
[53] C. N. Bonial, L. Donatelli, J. Ervin, and C. R. Voss, "Abstract meaning representation for human-robot dialogue," Proceedings of the Society for Computation in Linguistics, vol. 2, no. 1, pp. 236-246, 2019.
[54] S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek, "Robots that use language," Annual Review of Control, Robotics, and Autonomous Systems, vol. 3, pp. 25-55, 2020.
[55] J. X. Liu, Z. Yang, B. Schornstein, S. Liang, I. Idrees, S. Tellex, and A. Shah, "Lang2LTL: Translating natural language commands to temporal specification with large language models," in Workshop on Language and Robotics at CoRL 2022, 2022. [Online]. Available: https://openreview.net/forum?id=VxfjGZzrdn
[56] L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati, "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning," arXiv preprint arXiv:2305.14909, 2023.
[57] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," arXiv preprint arXiv:2209.07753, 2022.
[58] S. S. Raman, V. Cohen, E. Rosen, I. Idrees, D. Paulius, and S. Tellex, "Planning with large language models via corrective re-prompting," in NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. [Online]. Available: https://openreview.net/ forum?id=cMDMRBe1TKs
[59] T. Silver, S. Dan, K. Srinivas, J. B. Tenenbaum, L. P. Kaelbling, and M. Katz, "Generalized planning in pddl domains with pretrained large language models," arXiv preprint arXiv:2305.11014, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Massachusetts Institute of Technology. jarkin@mit.edu, cbd@mit.edu, nickroy@csail.mit.edu, chuchu@mit.edu
${ }^{2}$ Harvard University. yongchaochen@fas.harvard.edu
${ }^{3}$ MIT-IBM Watson AI Lab. yang.zhang2@ibm.com
${ }^{4}$ https://yongchao98.github.io/MIT-REALM-AutoTAMP/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
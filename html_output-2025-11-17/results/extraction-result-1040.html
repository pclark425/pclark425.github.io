<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1040 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1040</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1040</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-230770183</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2101.01774v1.pdf" target="_blank">An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Training robots to navigate diverse environments is a challenging problem as it involves the confluence of several different perception tasks such as mapping and localization, followed by optimal path-planning and control. Recently released photo-realistic simulators such as Habitat allow for the training of networks that output control actions directly from perception: agents use Deep Reinforcement Learning (DRL) to regress directly from the camera image to a control output in an end-to-end fashion. This is data-inefficient and can take several days to train on a GPU. Our paper tries to overcome this problem by separating the training of the perception and control neural nets and increasing the path complexity gradually using a curriculum approach. Specifically, a pre-trained twin Variational AutoEncoder (VAE) is used to compress RGBD (RGB&depth) sensing from an environment into a latent embedding, which is then used to train a DRL-based control policy. A*, a traditional path-planner is used as a guide for the policy and the distance between start and target locations is incrementally increased along the A* route, as training progresses. We demonstrate the efficacy of the proposed approach, both in terms of increased performance and decreased training times for the PointNav task in the Habitat simulation environment. This strategy of improving the training of direct-perception based DRL navigation policies is expected to hasten the deployment of robots of particular interest to industry such as co-bots on the factory floor and last-mile delivery robots.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1040.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1040.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point-to-Point Navigation (PointNav) Baseline Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DRL agent trained end-to-end (using a pretrained VAE for perception) to navigate from a random start to a goal location in Habitat using RGBD, point-goal and heading sensors; actions are discrete (forward 0.25 m, turn ±10°).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PointNav agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied simulated navigation agent trained with Proximal Policy Optimization (PPO) using a pretrained twin-VAE visual embedding; exploration during training (stochastic policy) and greedy actions at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat PointGoal Navigation (Quantico scene, Gibson dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photo-realistic indoor scene (Quantico) with rooms, furniture, doorways and obstacles; episodic tasks with random start and target locations, no map provided; RGBD and heading/point-goal sensors available.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Path complexity measured primarily via start-to-goal path length (A* shortest path length l) and obstacle/room topology (multi-room traversal with doorways); also measured implicitly by number of intermediate waypoints (WP count).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (full start-to-target PointNav episodes with no intermediate waypoints; long-range navigation across rooms)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Within-scene variation: randomization of start and target locations per episode; across-scene variation tested via transfer to a different scene (Quantico -> Pleasant).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (random start/goal positions within a single scene); across-scene variation (Quantico vs Pleasant) treated separately via transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success-weighted Path Length (SPL) and Mean Success Rate (S)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Test mean SPL = 0.73; mean success rate = 0.852 (evaluated over 500 test episodes, greedy policy at test time).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper shows that full-complexity PointNav (no intermediate waypoints) is harder to learn and slower to converge than curricula that reduce instantaneous task complexity; variation (random start/goal and scene-to-scene differences) requires separate transfer/fine-tuning but was not explored as a primary axis in trade-off experiments. No explicit formal trade-off between environment complexity and environment variation is derived, only empirical observations.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>mean SPL = 0.73; success = 0.852 (Quantico scene, original PointNav)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training with PPO; no curriculum for baseline PointNav (end-to-end DRL on the PointNav task using pretrained VAE perception).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Policy itself was not directly transferred without retraining; transfer experiments used curriculum-trained weights (SWP) as initialization and required fine-tuning on the new scene to reach good performance (see TL-SWP).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>PointNav baseline was trained up to ~170k episodes in experiments; learning was slower compared to curriculum agents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Full PointNav (highest complexity at training time) achieves lower final SPL and success and requires more training episodes compared with curriculum-assisted agents; it is more prone to early failures near the start and to overshooting near the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1040.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1040.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WP-N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Way-point-N agents (WP-N)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents trained to navigate between a sequence of N equidistant A* waypoints that partition the optimal path; reducing instantaneous sub-task length simplifies learning and improves training speed and performance, but requires A* at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WP-N agents (WP-10, WP-8, WP-6, WP-4, WP-3, WP-2, WP-1)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied simulated agents trained with PPO using twin-VAE embeddings; at training and test time the goal is the next waypoint on an A* path; agent learns to reach successive short subgoals (0.2 m success threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat PointGoal Navigation (Quantico scene) with A*-generated waypoints</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same indoor environment as PointNav but task simplified by subdividing the A* path into N equidistant waypoints; each sub-episode is a short-range navigation between successive WPs, reducing path length and local complexity per subgoal.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity explicitly controlled by number of waypoints N (higher N → shorter subpaths → lower instantaneous complexity); path length per subgoal and total number of subgoals are the operative measures.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low to medium (depends on N; e.g., WP-10 implies low per-subgoal complexity; WP-1 = PointNav = high complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Same-scene variation via random starts/targets but reduced per-episode complexity; no across-scene variation reported for WP-N test-time since these agents rely on A* at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium (operating within a single scene with random starts/goals; WP-N focuses on lowered complexity rather than increased environmental variation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SPL and Success Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Test success rate > 0.98 and SPL > 0.85 for WP-N agents (reported averaged over 500 test episodes for WP variants; specific per-N values not fully enumerated but all WP-N are substantially better than PointNav).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Direct empirical relationship: reducing instantaneous complexity (increasing N) improves learning speed and final performance; WP-N demonstrates that small number of intermediate waypoints (even WP-2 or WP-3) suffices to dramatically improve performance. However, WP-N requires access to A* at test time, so this is a complexity-vs-deployment trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>WP-N (low instantaneous complexity within same scene): success > 0.98; SPL > 0.85 (500 episode averages).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Simplification via task decomposition (A* waypoints); trained with PPO on WP subgoals. WP-N used as a component to bootstrap curricula (SWP and FWP).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>WP-N agents learn much faster than PointNav; fewer episodes required to reach high SPL (exact episode counts vary with N but substantially less than full PointNav which required >100k episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shorter sub-paths generated by A* dramatically speed learning and increase final performance; only a handful of intermediate waypoints (2–3) suffice to bootstrap fast learning. Trade-off: WP-N requires A* at test time and thus is not suitable for deployment without a map.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1040.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1040.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWP-10</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Waypoint Curriculum Agent (SWP-10)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-trained agent that starts training with many waypoints (WP-10) and progressively reduces the number of intermediate waypoints (increasing per-episode complexity) episodically until reaching full PointNav (WP-1).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SWP-10 (Sequential WP curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied simulated agent trained with PPO on twin-VAE embeddings using a staged curriculum: WP-10 for initial episodes, then WP-8, WP-6, WP-4, WP-3, WP-2 and finally WP-1 (PointNav), with each stage lasting specified episode ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat PointGoal Navigation (Quantico scene) under sequential waypoint curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same indoor scene with episodic randomization of start/goal; training complexity is increased discretely by decreasing N every fixed number of episodes (e.g., 10k per stage early, then larger stages).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Curriculum stages defined by number of waypoints N (10→1); complexity per stage measured by subpath length (inversely proportional to N) and overall path length remains same but per-episode immediate difficulty increases over curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies during training from low (early WP-10) to high (final WP-1 equal to PointNav); at test time complexity = high (PointNav).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Within-scene episodic variation (random starts/targets); curriculum affects only complexity, not scene variation. Transfer to new scenes tested separately (TL experiments used SWP-10 weights as initialization).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (same-scene randomization); cross-scene variation addressed via transfer learning experiments using SWP initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SPL and Success Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Test (500 episodes) SWP-10: average SPL = 0.91 (best performing agent reported) and success rate > 0.9 (exact reported success > 0.9).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>SWP-10 shows that a discrete curriculum of gradually increasing complexity yields faster learning and higher final performance than training directly on the high-complexity task; however, SWP-10 exhibited drops in performance when the curriculum reaches more difficult stages, indicating sensitivity to abrupt increases in complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>At test time (high complexity, same scene): SPL = 0.91; success > 0.9.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (discrete staged reduction in waypoint count: WP-10 → WP-8 → WP-6 → WP-4 → WP-3 → WP-2 → WP-1), using PPO and pretrained VAE perception.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>SWP-10 weights were used as initialization for transfer learning (TL-SWP) to a different scene (Pleasant) and fine-tuned; after ~60k episodes of retraining on Pleasant, TL-SWP achieved mean SPL = 0.87 and success rate = 0.95, indicating good transfer with limited retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Learns significantly faster than baseline; curriculum stages described with episode counts (e.g., WP-10 for <10k episodes then staged reductions up to >80k); reached strong performance earlier than PointNav which required far more episodes (~170k).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Discrete sequential curriculum (SWP-10) trains faster and produces higher final SPL than baseline PointNav; however, performance can drop when curriculum difficulty increases abruptly, indicating that curriculum pacing matters. SWP-10 transfers well with limited fine-tuning to a new scene.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1040.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1040.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FWP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Farther Waypoint Curriculum Agent (FWP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum agent where a single waypoint progressively moves farther along the A* path from start to goal in continuous increments, starting at 20% path length and reaching 100% (PointNav) over training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FWP (Farther Waypoint) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied simulated agent trained with PPO using twin-VAE embeddings; training goal is a single waypoint whose percentiles along the A* path increase linearly from 20% to 100% over ~64k episodes, progressively increasing per-episode difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat PointGoal Navigation (Quantico scene) with continuously moving waypoint curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor scene; curriculum reduces instantaneous simplification gradually by moving the single waypoint farther along the true path, thus linearly increasing path-length-to-goal during training.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Waypoint percentile along A* path (starts at 20% and increments linearly to 100% over training); complexity measured as instantaneous remaining path length to the waypoint and ultimately to the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>increases continuously during training from low (20% path) to high (100% = PointNav) over ~64k episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Within-scene episodic variation (random starts/targets); no explicit across-scene curriculum variation for FWP in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (random start/goal within a single scene)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SPL and Success Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>At convergence/test (500 episodes) FWP: success rate > 0.9; SPL higher than PointNav and comparable to SWP; during training FWP achieved and maintained SPL ≈ 0.8 (and final test SPL higher than baseline, numerical table shows FWP also improved vs PointNav but SWP-10 was best).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Continuous curriculum that increases path complexity smoothly leads to stable training (FWP maintained SPL ~0.8 through training) and faster learning than baseline; compared to SWP-10 which had high early performance but drops as curriculum difficulty increases, FWP appears more stable under a smoothly increasing complexity schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (continuous increase of waypoint percentile from 20% to 100% along A* path over ~64k episodes), PPO, pretrained twin-VAE perception.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Learns faster than baseline PointNav; reaches good SPL values earlier in training and maintains them; completion to full PointNav occurs after ~64k episodes in their schedule (compared to PointNav baseline run to ~170k).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A continuous curriculum (FWP) produces stable learning and good final performance, maintaining SPL ≈ 0.8 during training and achieving higher test metrics than the baseline; compared to discrete SWP, FWP is less prone to sudden drops when difficulty increases because difficulty increments are smoother.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1040.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1040.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TL-SWP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer-Learned SWP (TL-SWP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SWP-10 trained policy and fine-tuned VAE transferred from Quantico to a different scene (Pleasant) and fine-tuned with further training; used to evaluate generalization across scene variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TL-SWP (SWP-10 weights fine-tuned on Pleasant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied simulated agent initialized from SWP-10 policy and VAE weights (Quantico), then fine-tuned on a new scene (Pleasant) using PPO; evaluates transferability of curriculum-trained policies and representations.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pleasant scene (Gibson dataset) after transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor home-like scene distinct from Quantico but similar in category (rooms, furniture); represents cross-scene variation / distribution shift relative to training environment.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Full PointNav complexity at test (after fine-tuning); scene-level variation measured as distinct environment instance (Quantico → Pleasant).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (PointNav episodes after fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Across-scene variation (different scene geometry/furniture/layout); transfer involved fine-tuning on the new scene for ~60k episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (scene-to-scene variation requiring retraining/fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SPL and Success Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>After ~60k episodes of retraining on Pleasant, TL-SWP achieved mean SPL = 0.87 and mean success rate = 0.95 (500 test episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Empirical result: curriculum-trained policies (SWP) transfer effectively to different but related scenes with substantially fewer retraining episodes than training from scratch; thus curriculum can improve robustness to variation when combined with fine-tuning, but performance after transfer depends on extent of retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>After transfer and fine-tuning (high complexity PointNav in a different scene): mean SPL = 0.87; success rate = 0.95 (60k fine-tuning episodes reported).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Transfer learning (initialize from SWP-10 trained on Quantico, fine-tune twin-VAE and policy on Pleasant for ~60k episodes), PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>TL-SWP achieved strong performance (SPL = 0.87, success = 0.95) after ~60k episodes of fine-tuning, indicating good generalization with modest retraining effort; authors note that using more scenes during initial training likely improves transfer further (citing prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Fine-tuning on Pleasant required ~60k episodes to reach reported performance; this is framed as fewer episodes than would be needed to train from scratch on Pleasant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum-trained policies (SWP) provide a good initialization for transfer to new scenes; after modest fine-tuning the transferred policy attains high SPL and success, demonstrating that curricula can improve cross-environment generalization when combined with retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Splitnet: Sim2sim and task2task transfer for embodied visual navigation <em>(Rating: 2)</em></li>
                <li>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames <em>(Rating: 2)</em></li>
                <li>PRM-RL: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning <em>(Rating: 2)</em></li>
                <li>Target-driven visual navigation in indoor scenes using deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>On evaluation of embodied navigation agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1040",
    "paper_id": "paper-230770183",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PointNav",
            "name_full": "Point-to-Point Navigation (PointNav) Baseline Agent",
            "brief_description": "A DRL agent trained end-to-end (using a pretrained VAE for perception) to navigate from a random start to a goal location in Habitat using RGBD, point-goal and heading sensors; actions are discrete (forward 0.25 m, turn ±10°).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PointNav agent (baseline)",
            "agent_description": "Embodied simulated navigation agent trained with Proximal Policy Optimization (PPO) using a pretrained twin-VAE visual embedding; exploration during training (stochastic policy) and greedy actions at test time.",
            "agent_type": "simulated agent (Habitat)",
            "environment_name": "Habitat PointGoal Navigation (Quantico scene, Gibson dataset)",
            "environment_description": "Photo-realistic indoor scene (Quantico) with rooms, furniture, doorways and obstacles; episodic tasks with random start and target locations, no map provided; RGBD and heading/point-goal sensors available.",
            "complexity_measure": "Path complexity measured primarily via start-to-goal path length (A* shortest path length l) and obstacle/room topology (multi-room traversal with doorways); also measured implicitly by number of intermediate waypoints (WP count).",
            "complexity_level": "high (full start-to-target PointNav episodes with no intermediate waypoints; long-range navigation across rooms)",
            "variation_measure": "Within-scene variation: randomization of start and target locations per episode; across-scene variation tested via transfer to a different scene (Quantico -&gt; Pleasant).",
            "variation_level": "medium (random start/goal positions within a single scene); across-scene variation (Quantico vs Pleasant) treated separately via transfer learning",
            "performance_metric": "Success-weighted Path Length (SPL) and Mean Success Rate (S)",
            "performance_value": "Test mean SPL = 0.73; mean success rate = 0.852 (evaluated over 500 test episodes, greedy policy at test time).",
            "complexity_variation_relationship": "The paper shows that full-complexity PointNav (no intermediate waypoints) is harder to learn and slower to converge than curricula that reduce instantaneous task complexity; variation (random start/goal and scene-to-scene differences) requires separate transfer/fine-tuning but was not explored as a primary axis in trade-off experiments. No explicit formal trade-off between environment complexity and environment variation is derived, only empirical observations.",
            "high_complexity_low_variation_performance": "mean SPL = 0.73; success = 0.852 (Quantico scene, original PointNav)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment training with PPO; no curriculum for baseline PointNav (end-to-end DRL on the PointNav task using pretrained VAE perception).",
            "generalization_tested": true,
            "generalization_results": "Policy itself was not directly transferred without retraining; transfer experiments used curriculum-trained weights (SWP) as initialization and required fine-tuning on the new scene to reach good performance (see TL-SWP).",
            "sample_efficiency": "PointNav baseline was trained up to ~170k episodes in experiments; learning was slower compared to curriculum agents.",
            "key_findings": "Full PointNav (highest complexity at training time) achieves lower final SPL and success and requires more training episodes compared with curriculum-assisted agents; it is more prone to early failures near the start and to overshooting near the goal.",
            "uuid": "e1040.0",
            "source_info": {
                "paper_title": "An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "WP-N",
            "name_full": "Way-point-N agents (WP-N)",
            "brief_description": "Agents trained to navigate between a sequence of N equidistant A* waypoints that partition the optimal path; reducing instantaneous sub-task length simplifies learning and improves training speed and performance, but requires A* at test time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "WP-N agents (WP-10, WP-8, WP-6, WP-4, WP-3, WP-2, WP-1)",
            "agent_description": "Embodied simulated agents trained with PPO using twin-VAE embeddings; at training and test time the goal is the next waypoint on an A* path; agent learns to reach successive short subgoals (0.2 m success threshold).",
            "agent_type": "simulated agent (Habitat)",
            "environment_name": "Habitat PointGoal Navigation (Quantico scene) with A*-generated waypoints",
            "environment_description": "Same indoor environment as PointNav but task simplified by subdividing the A* path into N equidistant waypoints; each sub-episode is a short-range navigation between successive WPs, reducing path length and local complexity per subgoal.",
            "complexity_measure": "Complexity explicitly controlled by number of waypoints N (higher N → shorter subpaths → lower instantaneous complexity); path length per subgoal and total number of subgoals are the operative measures.",
            "complexity_level": "low to medium (depends on N; e.g., WP-10 implies low per-subgoal complexity; WP-1 = PointNav = high complexity).",
            "variation_measure": "Same-scene variation via random starts/targets but reduced per-episode complexity; no across-scene variation reported for WP-N test-time since these agents rely on A* at test time.",
            "variation_level": "low-to-medium (operating within a single scene with random starts/goals; WP-N focuses on lowered complexity rather than increased environmental variation).",
            "performance_metric": "SPL and Success Rate",
            "performance_value": "Test success rate &gt; 0.98 and SPL &gt; 0.85 for WP-N agents (reported averaged over 500 test episodes for WP variants; specific per-N values not fully enumerated but all WP-N are substantially better than PointNav).",
            "complexity_variation_relationship": "Direct empirical relationship: reducing instantaneous complexity (increasing N) improves learning speed and final performance; WP-N demonstrates that small number of intermediate waypoints (even WP-2 or WP-3) suffices to dramatically improve performance. However, WP-N requires access to A* at test time, so this is a complexity-vs-deployment trade-off.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "WP-N (low instantaneous complexity within same scene): success &gt; 0.98; SPL &gt; 0.85 (500 episode averages).",
            "training_strategy": "Simplification via task decomposition (A* waypoints); trained with PPO on WP subgoals. WP-N used as a component to bootstrap curricula (SWP and FWP).",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "WP-N agents learn much faster than PointNav; fewer episodes required to reach high SPL (exact episode counts vary with N but substantially less than full PointNav which required &gt;100k episodes).",
            "key_findings": "Shorter sub-paths generated by A* dramatically speed learning and increase final performance; only a handful of intermediate waypoints (2–3) suffice to bootstrap fast learning. Trade-off: WP-N requires A* at test time and thus is not suitable for deployment without a map.",
            "uuid": "e1040.1",
            "source_info": {
                "paper_title": "An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "SWP-10",
            "name_full": "Sequential Waypoint Curriculum Agent (SWP-10)",
            "brief_description": "A curriculum-trained agent that starts training with many waypoints (WP-10) and progressively reduces the number of intermediate waypoints (increasing per-episode complexity) episodically until reaching full PointNav (WP-1).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SWP-10 (Sequential WP curriculum)",
            "agent_description": "Embodied simulated agent trained with PPO on twin-VAE embeddings using a staged curriculum: WP-10 for initial episodes, then WP-8, WP-6, WP-4, WP-3, WP-2 and finally WP-1 (PointNav), with each stage lasting specified episode ranges.",
            "agent_type": "simulated agent (Habitat)",
            "environment_name": "Habitat PointGoal Navigation (Quantico scene) under sequential waypoint curriculum",
            "environment_description": "Same indoor scene with episodic randomization of start/goal; training complexity is increased discretely by decreasing N every fixed number of episodes (e.g., 10k per stage early, then larger stages).",
            "complexity_measure": "Curriculum stages defined by number of waypoints N (10→1); complexity per stage measured by subpath length (inversely proportional to N) and overall path length remains same but per-episode immediate difficulty increases over curriculum.",
            "complexity_level": "varies during training from low (early WP-10) to high (final WP-1 equal to PointNav); at test time complexity = high (PointNav).",
            "variation_measure": "Within-scene episodic variation (random starts/targets); curriculum affects only complexity, not scene variation. Transfer to new scenes tested separately (TL experiments used SWP-10 weights as initialization).",
            "variation_level": "medium (same-scene randomization); cross-scene variation addressed via transfer learning experiments using SWP initialization.",
            "performance_metric": "SPL and Success Rate",
            "performance_value": "Test (500 episodes) SWP-10: average SPL = 0.91 (best performing agent reported) and success rate &gt; 0.9 (exact reported success &gt; 0.9).",
            "complexity_variation_relationship": "SWP-10 shows that a discrete curriculum of gradually increasing complexity yields faster learning and higher final performance than training directly on the high-complexity task; however, SWP-10 exhibited drops in performance when the curriculum reaches more difficult stages, indicating sensitivity to abrupt increases in complexity.",
            "high_complexity_low_variation_performance": "At test time (high complexity, same scene): SPL = 0.91; success &gt; 0.9.",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (discrete staged reduction in waypoint count: WP-10 → WP-8 → WP-6 → WP-4 → WP-3 → WP-2 → WP-1), using PPO and pretrained VAE perception.",
            "generalization_tested": true,
            "generalization_results": "SWP-10 weights were used as initialization for transfer learning (TL-SWP) to a different scene (Pleasant) and fine-tuned; after ~60k episodes of retraining on Pleasant, TL-SWP achieved mean SPL = 0.87 and success rate = 0.95, indicating good transfer with limited retraining.",
            "sample_efficiency": "Learns significantly faster than baseline; curriculum stages described with episode counts (e.g., WP-10 for &lt;10k episodes then staged reductions up to &gt;80k); reached strong performance earlier than PointNav which required far more episodes (~170k).",
            "key_findings": "Discrete sequential curriculum (SWP-10) trains faster and produces higher final SPL than baseline PointNav; however, performance can drop when curriculum difficulty increases abruptly, indicating that curriculum pacing matters. SWP-10 transfers well with limited fine-tuning to a new scene.",
            "uuid": "e1040.2",
            "source_info": {
                "paper_title": "An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "FWP",
            "name_full": "Farther Waypoint Curriculum Agent (FWP)",
            "brief_description": "A curriculum agent where a single waypoint progressively moves farther along the A* path from start to goal in continuous increments, starting at 20% path length and reaching 100% (PointNav) over training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "FWP (Farther Waypoint) agent",
            "agent_description": "Embodied simulated agent trained with PPO using twin-VAE embeddings; training goal is a single waypoint whose percentiles along the A* path increase linearly from 20% to 100% over ~64k episodes, progressively increasing per-episode difficulty.",
            "agent_type": "simulated agent (Habitat)",
            "environment_name": "Habitat PointGoal Navigation (Quantico scene) with continuously moving waypoint curriculum",
            "environment_description": "Indoor scene; curriculum reduces instantaneous simplification gradually by moving the single waypoint farther along the true path, thus linearly increasing path-length-to-goal during training.",
            "complexity_measure": "Waypoint percentile along A* path (starts at 20% and increments linearly to 100% over training); complexity measured as instantaneous remaining path length to the waypoint and ultimately to the goal.",
            "complexity_level": "increases continuously during training from low (20% path) to high (100% = PointNav) over ~64k episodes.",
            "variation_measure": "Within-scene episodic variation (random starts/targets); no explicit across-scene curriculum variation for FWP in main experiments.",
            "variation_level": "medium (random start/goal within a single scene)",
            "performance_metric": "SPL and Success Rate",
            "performance_value": "At convergence/test (500 episodes) FWP: success rate &gt; 0.9; SPL higher than PointNav and comparable to SWP; during training FWP achieved and maintained SPL ≈ 0.8 (and final test SPL higher than baseline, numerical table shows FWP also improved vs PointNav but SWP-10 was best).",
            "complexity_variation_relationship": "Continuous curriculum that increases path complexity smoothly leads to stable training (FWP maintained SPL ~0.8 through training) and faster learning than baseline; compared to SWP-10 which had high early performance but drops as curriculum difficulty increases, FWP appears more stable under a smoothly increasing complexity schedule.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (continuous increase of waypoint percentile from 20% to 100% along A* path over ~64k episodes), PPO, pretrained twin-VAE perception.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Learns faster than baseline PointNav; reaches good SPL values earlier in training and maintains them; completion to full PointNav occurs after ~64k episodes in their schedule (compared to PointNav baseline run to ~170k).",
            "key_findings": "A continuous curriculum (FWP) produces stable learning and good final performance, maintaining SPL ≈ 0.8 during training and achieving higher test metrics than the baseline; compared to discrete SWP, FWP is less prone to sudden drops when difficulty increases because difficulty increments are smoother.",
            "uuid": "e1040.3",
            "source_info": {
                "paper_title": "An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "TL-SWP",
            "name_full": "Transfer-Learned SWP (TL-SWP)",
            "brief_description": "An SWP-10 trained policy and fine-tuned VAE transferred from Quantico to a different scene (Pleasant) and fine-tuned with further training; used to evaluate generalization across scene variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TL-SWP (SWP-10 weights fine-tuned on Pleasant)",
            "agent_description": "Embodied simulated agent initialized from SWP-10 policy and VAE weights (Quantico), then fine-tuned on a new scene (Pleasant) using PPO; evaluates transferability of curriculum-trained policies and representations.",
            "agent_type": "simulated agent (Habitat)",
            "environment_name": "Pleasant scene (Gibson dataset) after transfer learning",
            "environment_description": "Indoor home-like scene distinct from Quantico but similar in category (rooms, furniture); represents cross-scene variation / distribution shift relative to training environment.",
            "complexity_measure": "Full PointNav complexity at test (after fine-tuning); scene-level variation measured as distinct environment instance (Quantico → Pleasant).",
            "complexity_level": "high (PointNav episodes after fine-tuning)",
            "variation_measure": "Across-scene variation (different scene geometry/furniture/layout); transfer involved fine-tuning on the new scene for ~60k episodes.",
            "variation_level": "high (scene-to-scene variation requiring retraining/fine-tuning)",
            "performance_metric": "SPL and Success Rate",
            "performance_value": "After ~60k episodes of retraining on Pleasant, TL-SWP achieved mean SPL = 0.87 and mean success rate = 0.95 (500 test episodes).",
            "complexity_variation_relationship": "Empirical result: curriculum-trained policies (SWP) transfer effectively to different but related scenes with substantially fewer retraining episodes than training from scratch; thus curriculum can improve robustness to variation when combined with fine-tuning, but performance after transfer depends on extent of retraining.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "After transfer and fine-tuning (high complexity PointNav in a different scene): mean SPL = 0.87; success rate = 0.95 (60k fine-tuning episodes reported).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Transfer learning (initialize from SWP-10 trained on Quantico, fine-tune twin-VAE and policy on Pleasant for ~60k episodes), PPO.",
            "generalization_tested": true,
            "generalization_results": "TL-SWP achieved strong performance (SPL = 0.87, success = 0.95) after ~60k episodes of fine-tuning, indicating good generalization with modest retraining effort; authors note that using more scenes during initial training likely improves transfer further (citing prior work).",
            "sample_efficiency": "Fine-tuning on Pleasant required ~60k episodes to reach reported performance; this is framed as fewer episodes than would be needed to train from scratch on Pleasant.",
            "key_findings": "Curriculum-trained policies (SWP) provide a good initialization for transfer to new scenes; after modest fine-tuning the transferred policy attains high SPL and success, demonstrating that curricula can improve cross-environment generalization when combined with retraining.",
            "uuid": "e1040.4",
            "source_info": {
                "paper_title": "An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation",
                "publication_date_yy_mm": "2021-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Splitnet: Sim2sim and task2task transfer for embodied visual navigation",
            "rating": 2,
            "sanitized_title": "splitnet_sim2sim_and_task2task_transfer_for_embodied_visual_navigation"
        },
        {
            "paper_title": "DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "rating": 2,
            "sanitized_title": "ddppo_learning_nearperfect_pointgoal_navigators_from_25_billion_frames"
        },
        {
            "paper_title": "PRM-RL: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning",
            "rating": 2,
            "sanitized_title": "prmrl_longrange_robotic_navigation_tasks_by_combining_reinforcement_learning_and_samplingbased_planning"
        },
        {
            "paper_title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "targetdriven_visual_navigation_in_indoor_scenes_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "On evaluation of embodied navigation agents",
            "rating": 1,
            "sanitized_title": "on_evaluation_of_embodied_navigation_agents"
        }
    ],
    "cost": 0.014119749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation</p>
<p>Kaushik Balakrishnan 
Punarjay Chakravarty 
Shubham Shrivastava 
An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation</p>
<p>Training robots to navigate diverse environments is a challenging problem as it involves the confluence of several different perception tasks such as mapping and localization, followed by optimal path-planning and control. Recently released photo-realistic simulators such as Habitat [1] allow for the training of networks that output control actions directly from perception: agents use Deep Reinforcement Learning (DRL) to regress directly from the camera image to a control output in an end-to-end fashion. This is data-inefficient and can take several days to train on a GPU. Our paper tries to overcome this problem by separating the training of the perception and control neural nets and increasing the path complexity gradually using a curriculum approach. Specifically, a pre-trained twin Variational AutoEncoder (VAE) [2] is used to compress RGBD (RGB &amp; depth) sensing from an environment into a latent embedding, which is then used to train a DRLbased control policy. A<em>, a traditional path-planner is used as a guide for the policy and the distance between start and target locations is incrementally increased along the A</em> route, as training progresses. We demonstrate the efficacy of the proposed approach, both in terms of increased performance and decreased training times for the PointNav task in the Habitat simulation environment. This strategy of improving the training of direct-perception based DRL navigation policies is expected to hasten the deployment of robots of particular interest to industry such as co-bots on the factory floor and last-mile delivery robots. 2</p>
<p>I. INTRODUCTION</p>
<p>To go from point A to B in an indoor environment is challenging for a mobile robot. In the absence of GPS and using the visual/RGBD sensor available on the robot, one has to map an environment &amp; localize in it (SLAM) and then path-plan an obstacle-free route to get from a start to target location. This was the traditional approach to mobile robotics. Recently, Deep Reinforcement Learning (DRL) has shown to provide more robust navigation policies compared to SLAM, if the robot (agent) is trained in simulation and exposed to an order of magnitude more experience [1].</p>
<p>This involves training navigation policies that regress directly from the camera image to a control output. However, splitting this task into two: learning a compact state representation, termed "representation learning" and then using this representation to learn a robust control policy [3], [4], [5], [6], [7], [8], [9] has the following advantages: (1) errors in policy learning will not affect perception as the latter is decoupled from the former, but the vice versa is not true; Fig. 1: Top: waypoints generated between desired start and target locations by the A* algorithm. This work looks at assisting the training of a DRL-based robot navigation policy, by incrementally increasing the difficulty of the navigation task in a curriculum. Bottom: 3 curriculum training approaches with 9 and 4 discrete waypoints and a continuously moving waypoint: WP-9, WP-4 &amp; FWP.</p>
<p>(2) once perception is learned, it can be reused to learn multiple policies for different tasks [6], which is not feasible in complete end-to-end training as the perception needs to be re-learned every time a new task is learned. These advantages have the potential to speed-up the overall learning of the task at hand.</p>
<p>The recently released Gibson [10] and Habitat simulators [1] have generated excitement in the field of RGBD visionbased robot navigation in indoor environments. In Split-Net [6], the authors investigated three tasks in the Habitat environment: (1) Point-to-Point Navigation (PointNav); (2) Scene Exploration; (3) Run Away from Location (Flee). They decoupled the perception and policy, and used an Encoder-Decoder architecture, where the visual/perception encoder is trained using auxiliary visual and motion-based tasks, and the policy Decoder is trained on embodied tasks using the visual Encoder. They demonstrated robust learning of both perception and policy on all three tasks, including transfer to new visual environments as well as to new embodied tasks. In [11], the authors undertook Imitation Learning to train a robot to navigate in the Gibson simulator using the Dijkstra algorithm and obtained high success rates. In another study, the authors used DRL for target-driven robot navigation in an indoor scenes simulator called AI2THOR [12], where only RGB images of the state and the target are used to train the navigation policy network. All these studies demonstrated robustness of DRL for robot navigation using large amounts of vision data, however techniques to speedup DRL for vision-based navigation is warranted as most of these techniques are GPU-compute intensive and their speedup can help in faster learning and deployment of robots in the real world.</p>
<p>In this paper, we build on these past investigations and train DRL agents for the problem of indoor robot navigation in the Habitat environment by separating perception (i.e., representation learning) and control (i.e., navigation policy). We use a VAE to encode RGB and Depth images, and use these latent encodings as well as a reading and heading angle for the target (from the PointGoal sensor), to learn navigation policies.</p>
<p>Additionally, we use a traditional path-planner, A<em> to assist the DRL agent during training. A</em> guides the agent by giving it shorter-distance goal locations (waypoints) between the original start and target locations. We experiment with two different curriculum-based training of the DRL agents, one by decreasing the number of intermediate waypoints used (termed the SWP-N agent) or by moving the episodic goal farther away from the start position (termed the FWP agent). We describe the problem and our method in Section III, implementation details in Section IV and an experimental analysis of these methods in Section V.</p>
<p>In summary, our contributions are as follows: (1) a principled approach to compare different navigation-agnostic VAE-based perception embeddings for their usefulness to a DRL in learning a subsequent navigation policy; (2) Using a traditional A* path-planning algorithm in a curriculum fashion to assist in the training process of this navigation policy.</p>
<p>II. RELATED WORK A. Visual perception</p>
<p>Training end-to-end Vision-based DRL navigation policies can be very time consuming as the CNNs used to learn vision-based features involve several matrix operations, and this can particularly require several million images/experiences, accompanied by several days of compute hours, for training the DRL policy [1], [12]. End-to-end learning of DRL policies from RGB images has been very successful for Atari games [13], but for larger images such as the Habitat environment, it is very challenging [1], [14]. For learning robust navigation policies using RGBD images, it is critical to obtain good perception representation that is feature rich and compact enough.</p>
<p>We use the Variational Auto-Encoder (VAE) [2] for perception representation; VAE is a generative version of the vanilla Auto-encoder, that constrains the latent space z to a Gaussian distribution with zero mean and unit variance p(z) = N (0, I). The encoder is used to produce the latent space encoding and the decoder takes in z to reconstruct the input image. The VAE is trained using a combination of the reconstruction loss (typically, L2) and the KL divergence loss for the embedding to conform it to a unit Gaussian.</p>
<p>Using VAEs for representation learning, followed by a DRL control policy is not new, see for instance [5], [9]. The VAE is generally pre-trained, and so the perception is learned independent of control. In [6], the authors used a vanilla Autoencoder (AE) to compress the image, without the KL divergence loss. One difference between the two approaches is the that when VAE is used for perception, the resulting embedding is stochastic, i.e., the same input image fed to the encoder multiple times will result in different embeddings as they are sampled from the Gaussian; whereas in the vanilla AE this is not the case. Other approaches such as using shared latent spaces [15], [16] can also be considered in future studies.</p>
<p>Another challenge in vision-based robot navigation is on transfer learning to new targets and/or new scenes. To this end, [12] trained RGB-based DRL navigation policies for one or more scenes and used this to transfer learn (or fine-tune) to newer scenes. They showed that the DRL learns faster and the overall trajectory length is shorter if more scenes are used in the training. This improves the overall data-efficiency of the DRL training on newer scenes/targets. We will also briefly address transfer learning in the experiments conducted in this paper (section V).</p>
<p>B. Robot navigation with A* and vision</p>
<p>Robot navigation using vision-based sensors is gaining renewed interest in the literature with the advent of stateof-the-art simulators [1], [10], [12], robust datasets [10], [17], and efficient deep learning algorithms. A goal-driven DRL framework for visual robotic navigation was provided by [18]. Robot navigation using a PointGoal, i.e., the position of the goal with respect to the current location of the robot/agent, was used in [19], [1]; we undertake the PointGoal navigation task in this study, but with a curriculum that gradually increases the difficulty during training. A hierarchical method for navigation combining a sampling based path planning with DRL, called PRM-RL, was proposed in [20]. Their DRL agents were trained for short-range, point-to-point navigation capturing robot dynamics and task constraints without knowledge of the large-scale topology, while Probabilistic Roadmaps (PRMs) as sampling-based planners were used to provide roadmaps which connect robot configurations. Our hybrid path-planning/DRL is similar in spirit to theirs, but we use A* as our path-planner and use a curriculum-based training of the DRL agents.</p>
<p>Specifically, we undertake an investigation of using a small number of waypoints between the start and target locations for the DRL agent to successfully learn to navigate, as well as aiming for longer start-to-target distances as the learning progresses (more details on this in Section IV). Note that we are sequentially increasing the complexity of the navigation problem on the policy learned by the DRL agents, sticking to a pre-determined curriculum. Our approach is not imitation learning as the actions have to be learned by the DRL agents by exploration. Furthermore, despite the use of a two-level hierarchy for the policy, i.e., A<em> for waypoint determination (in training only) and a DRL policy for the navigation action, we are not undertaking Hierarchical Reinforcement Learning (HRL) for navigation in the spirit of [21]. HRL involves learning at multiple levels, whereas in our case the higherlevel A</em> is a graphical search algorithm, and only the lowerlevel DRL policy involves learning from data.</p>
<p>III. PROBLEM SETUP</p>
<p>The PointNav task We use the Habitat simulator [1] to train our DRL to learn policies for the point-goal navigation task in the Gibson environment [10]. The robot/agent is equipped with an RGBD camera, a point-goal sensor and a heading sensor. The point-goal sensor is like an indoor GPS: it provides the agent with its current position and the relative position of the target location. The heading sensor provides the current global heading angle of the agent. In the pointgoal navigation task, the agent is asked to navigate from the initial starting position to the required end position using only its RGBD, heading and point-goal sensors and without a map. These start and target locations are randomly initialized at the beginning of each episode, for which no straight line path is possible. The agent needs to learn navigation strategies that avoid obstacles and negotiate doorways since the start and target locations can be in different rooms.</p>
<p>Twin-VAE We pre-train perception in the environment by using a twin-VAE setup as shown in Figure 2. RGBD cameras are initialized randomly in the environment, and at each location, RGB and depth images are collected at angular increments of 10 o for a full 360 o sweep. These images are used to train the RGB and depth encoder-decoder branches (blue and purple in the figure) with the standard VAE reconstruction and KL divergence losses [2]. Once the VAE is pre-trained, only its encoders are used for training the DRL. RGB and depth images are encoded to their respective embeddings, which are concatenated to provide the final visual embedding from the camera. This embedding is used for training the DRL policy.</p>
<p>A<em> Curriculum Learning The task of learning the DRL policy is assisted by incrementally increasing the difficulty of the PointNav task. We do this during training by using A</em> to determine an optimal path between start and target locations in the bird's eye view (BeV) map of the environment. A new sub-goal, a point to navigate to, that is on this A<em> path, is provided to the DRL. This sub-goal is close to the starting location to begin with, and then as training progresses, gets farther and farther away from it. We test the following variants of curriculum learning based on discrete and continuous subdivisions of the path: 1) WP-N: In Way-point-N or WP-N, the A</em> path is divided into N equidistant waypoints (WPs) including the target location. At the beginning of the training episode, the agent is asked to navigate to the first WP.</p>
<p>When it reaches within 0.2m of this WP, the goal is revised to the next one and so on till the final target location. We investigate the number of intermediate waypoints required for successful navigation by experimenting with WP-10, WP-8, WP-6. WP-4, WP-3 and WP-2. WP-1 involves no subdivisions of the path and is the same as the original PointNav task. 2) SWP-N: Sequential WP-N or SWP-N involves keeping the number of WPs constant for a fixed number (few thousand) episodes. This is the same as WP-N, where the agent is asked to navigate from the 1st to the Nth waypoint within the same episode. However, N decreases episodically. Once the agent has mastered a higher N, requiring a smaller length sub-path traversal, the agent is subjected to a lower N, requiring a larger length sub-path traversal. For instance, the start-totarget path is divided into N waypoints for every 10k episodes, and N is decreased following the set: (10, 8, 6, 4, 3, 2, 1). Note that WP-1 is the same as PointNav. 3) FWP: Farther Waypoint involves only one WP that moves farther and farther away from the start in continuous, linear increments, as training progresses. The training is commenced with the WP at 20% distance along the A* path from the start. Over the course of training, this WP is moved farther and farther along the path until it is at 100% of the distance from the start to target after several tens of thousands of episodes, at which point the FWP problem is the same as the PointNav problem. . These are concatenated with the other two sensor readings to obtain a compact representation of the state at time t as:
s t = (z RGB t , z Depth t , P G t , H t ).
Once the sensory readings are concatenated into a compact s t , we use Deep Reinforcement Learning (DRL) to learn a policy π θ that outputs action a t at time t:
a t = π θ (s t ),(1)
where the actions are one of three: (1) move forward by 0.25 m;</p>
<p>(2) turn left by 10 o ; (3) turn right by 10 o . A fouth action called "Done" is executed whenever the agent is within 0.2 m from the goal position. Note that in [1], they trained an agent to also learn this trivial task, but this was not the case in [11], [6]; we take the latter approach. A schematic of the overall setup is shown in Fig. 2.</p>
<p>IV. IMPLEMENTATION DETAILS</p>
<p>A. Training the VAE</p>
<p>The RGB and Depth maps obtained from Habitat are 256×256 and are resized to 192×192 before being fed into the VAE for training. For the Encoder of the VAE, we use 10 layers of 4×4 filters with the number of feature maps per layer varying from 64 to 512. This is followed by fullyconnected layers of size N z each to obtain µ z and log σ 2 z . The reparameterization trick is used to obtain z [2], the Encoder's embedding vector. For the Decoder, we use a mirror image of the Encoder to scale back to 192×192 size. For the latent code, we will consider a size of N z = 128 and 256 in this study. All layers use the Relu activation function, except the final output layer of the Decoder which uses a Sigmoid; the intermediate latent layer uses no activation functions to determine µ z and log σ 2 z for the Gaussian latent code. We use a batch size of 64 and 50,000 iterations to train each of the VAEs, and use the Adam optimizer [22].</p>
<p>B. Training the DRL Policy</p>
<p>The Proximal Policy Optimization (PPO) algorithm [23] is used to train the policy network. The input to the DRL algorithm comprises of the visual embedding plus the point-goal and heading data as detailed in the previous section (This state at time t is given by s t = (z RGB t , z Depth t , P G t , H t ). The policy network consists of two fully connected layers with 512 and 256 units and tanh activation function, followed by a LSTM layer [24] with 256 units. The output of the LSTM branches out into the policy and value streams, each going through a fully connected layer with 256 units and the tanh activation function. This is followed by a Softmax layer with 3 probabilities corresponding to the actions described earlier For training the DRL agent, the reward function at time t, r t , is same as [1] and is given by:
r t = S + d t−1 − d t + λ goal reached d t−1 − d t + λ otherwise (2)
where d t is the distance to the goal from the agent's current location at time t, S = 10 is a bonus for reaching the goal, and λ = -0.01 to penalize a stationary agent. The Adam optimizer [22] is used to train the policy network as well. Other PPOrelated hyper-parameters are selected as follows: discount factor γ = 0.95, PPO clip value = 0.1, and Generalized Advantage Estimation [25] parameter λ = 1.0.</p>
<p>C. Evaluation metrics</p>
<p>Similar to other Habitat-based navigation work, [1], [6], [11], we use the Success-weighted Path Length (SPL) for evaluating the policy learnt at the end of each episode:
SP L = S l max(l, p)(3)
where S is a binary indicator of success, l is the shortest path length from start to goal position and p is the path length traveled by the agent in the episode. We also evaluate the Mean Success Rate, which is the mean of S over a fixed number of episodes.</p>
<p>V. EXPERIMENTAL RESULTS</p>
<p>Our primary interest in this study is to undertake indoor robot navigation in the Habitat environment [1]. The Quantico indoor scene in the Gibson [10] dataset is used for the experiments, except for the Transfer Learning (TL) experiments, where we TL from Quantico to the Pleasant environment. Our experiments relate to the VAE latent encoding parameters, followed by quantitative and qualitative evaluations of the baseline PointNav agent compared to the agents (WP-N, SWP and FWP) trained by A* curriculum learning.</p>
<p>A. Choice of latent encoding</p>
<p>As aforementioned, for representation learning, we use separate VAEs to encode the RGB and Depth images, concatenating the two encodings for subsequent use to train the DRL agents. For this we can use either the latent code z sampled from the Gaussian z ∼ p(z), or we can directly use the mean of the Gaussian µ z . For instance, [5] uses the VAE's z for the RL, whereas [9] uses µ z . Likewise, the dimension of the latent code can also have an effect on the training of the DRL agents. To better understand the choice of the VAE encoding that would result in a robust encoding of the visual inputs for efficient training of the DRL agents, we consider three cases: (1) N z = 256; µ z , (2) N z = 256; z, and (3) N z = 128; z. Separate DRL agents are trained on the PointNav problem using these three different choices and the exponentially-averaged (with degree of weighting α = 0.001) SPL during the training of the agents are presented in Fig. 3. As evident, N z = 128 performs better than 256, which we believe is due to over-fitting when the latent code dimension is large. Likewise, the training is better when z is used in lieu of µ z , as the stochasticity involved in z acts as a regularizer and can be a good source of exploration noise. For the rest of this paper, we will use N z = 128 and z ∼ p(z) as the encoding for training the DRL agents. </p>
<p>B. Navigation results</p>
<p>We will now summarize results from the different cases of interest as identified earlier. The best performing PointNav agent is used as the baseline (this is the agent corresponding to the red curve in Fig. 3). Note that we will only compare this PointNav baseline performance with SWP and FWP agents as these two agents are essentially PointNav at the end of the training. Performance of the WP-N agents are presented only for demonstration that shorter paths lead to improvement in learning; note that WP-N agents are not desired for deployment as they still require the help of A<em> at test time. Thus, the test time performance of WP-N agents are not presented or compared with other agents as we are only interested in agents that use A</em> in training but not at test time.</p>
<p>1) PointNav: For the PointNav problem, the best performing agent is the N z = 128 and z sampled from the Gaussian z ∼ p(z), i.e., the agent corresponding to the red curve in Fig. 3. We test the agent's performance over 500 random test episodes. The only difference between training and test mode is that in training the action is sampled from the policy's softmax output, but in test mode, the greedy action is chosen. The best performing PointNav agent (red curve in Fig. 3) has a test time performance of mean SPL = 0.73 and mean success rate = 0.852. We will use these values as the baseline for comparing the other agents with.</p>
<p>2) WP-N: We present the training curves for the different WP-N cases considered, along with the best performing PointNav agent in Fig. 4. For WP-N, the SPL is computed as a piecewise average over the individual sub-goals. The availability of the intermediate waypoints has significantly improved the performance for the WP-N cases vis-á-vis the PointNav case (refer to Fig. 4), as the overall problem of navigation from start to goal has been simplified. With fewer   The averages of the SPL and the success rate of the agents for 500 test episodes are summarized in Table I. The success rate is over 0.98 and the SPL over 0.85 for all the WP-N agents but not the PointNav agent. This investigation proves that only a small number of intermediate waypoints suffices, and a DRL policy can make efficient use of it to learn to navigate much faster than the PointNav cases. This has important implications for robot navigation applications, as a small handful number of intermediate goal points to visit between the start and target can be generated even with a low resolution map, and one can then combine it with a DRLbased navigation policy and still obtain high accuracy. From  Fig. 4, we observe that even 1-2 intermediate waypoints (WP-2 and WP-3) between the start and target positions suffices. However, as aforementioned, these WP-N agents are not preferred for deployment/test time as they still require the use of A<em>. We will now consider the hybrid training practice of using WP-N agents in training and following a curriculum (i.e., SWP and FWP agents) to gradually transition them to the PointNav agents so that they can be deployed without any further use of A</em> at test time.</p>
<p>3) SWP-N and FWP:</p>
<p>Having established that a few intermediate waypoints between the start and target locations suf-   II: Test time SPL and success rate for SWP-10 and  FWP agents fice, we will now investigate the SWP-N and FWP problems, both of which involve curriculum-based training. Note that WP-N agents require waypoints at both training test time, whereas the SWP-N and FWP agents require intermediate waypoints only during training, and are thus preferred. For SWP-10, we follow the curriculum: WP-10(&lt;10k), WP-8(10-20k), WP-6(20-30k), WP-4(30-40k), WP-3(40-60k), WP-2(60-80k) and WP-1(&gt;80k), where the number in parenthesis denotes the episode number range. (Note: WP-1 is identical to PointNav). For the FWP agent, we start the training with the revised goal set as the 20-th percentile of the start-to-goal A<em> path, and gradually increase it linearly over the course of 64k episodes to the 100-th percentile of the A</em> path. The FWP problem also reduces to the PointNav problem at the end of the training as the waypoint the agent is shooting for coincides with the target location (for episode number &gt; 64k). The training curves are presented in Fig. 5 for the SWP-10, FWP and the baseline PointNav agents. We observe that SWP-10 agent achieves high SPL values in the early stages of the curriculum, but drops in performance as the curriculum is more difficult at the later stages of the training. On the other hand, the FWP agent has learned to achieve SPL values of ∼ 0.8 and maintains the same level of performance. Both the SWP-10 and FWP agents (both of which are essentially PointNav at the end of the training) maintain superior performance over the PointNav agent. Furthermore, both SWP-10 and FWP agents also learn much faster than the PointNav agent.</p>
<p>The averages of SPL and success rates over 500 test time episodes for the SWP-10 and FWP agents are summarized in Table II. Note that at test time, the greedy action from the policy probabilities is used, instead of sampling. Both SWP-10 and FWP agents maintain a success rate of over 0.9 and the SPL is also higher for these agents compared with the PointNav agent. The SWP-10 agent performs the best among all the agents and achieves an average SPL of 0.91. The SWP and FWP agents also learn much faster than the PointNav agent (see Fig. 5). Thus, learning with a curriculum assists in learning a better policy and also faster (measured in terms of number of training episodes).</p>
<p>C. Comparison of different agents</p>
<p>We now compare the test time performance of the Point-Nav, SWP-10 and FWP agents for a few test episodes. The paths traced by the agents, including the start and target positions, for 8 episodes at test time are shown in Fig. 6. The observations reported here are based on performance over several more test episodes, but we will discuss only these 8 chosen test episodes for brevity; we will use the term "episode-a" to refer to Fig. 6 (a) and so on for the other episodes as well. In episodes-a and b, the PointNav (shown in red) fails to reach the target position. We noticed many episodes where the PointNav agent failed closer to the start position than otherwise; this agent has starting trouble in such episodes. On the other hand, agents SWP and FWP successfully navigated to the goal for episodes-a and b, with the SWP agent being smoother for episode-a and FWP for episode-b.</p>
<p>In many episodes, we also observed the PointNav agent to reach close to the target, only to overshoot it and get confused, but the agent was still able to reach the target, albeit with an unnecessarily longer path. This is reflective in episodes-c, d, e, and f (notice in the vicinity of the target, shown by the blue square). The SWP and FWP agents perform better in these episodes and successfully reach the target in shorter paths than the PointNav agent.</p>
<p>The FWP agent is coasting more than the SWP, as evident from episodes-a, e, and f, where we observe the FWP agent to stay closer to walls and make near 90 o turns. In these episodes, the SWP agent has relatively smoother paths, which we desire for real word deployment. However, in episodes-b and g, the reverse is observed-the SWP agent (shown in yellow) is coasting more than the FWP agent. This is an interesting behavior in that one agent is not "better" than the other agent in all test episodes, and the location of the start and target positions has an influence on the agents' performance. This behavior is observed in other episodes as well (not presented in the Figure), and the choice of the curriculum used in the training has a significant influence in the final test time performance. This has implications to real world robots that are trained using a DRL-based navigation policy following a curriculum.</p>
<p>In episode-h, the PointNav agent actually performs better than the other two agents, which is a very rare outcome. For this episode, the FWP agent (in purple) actually fails to reach the target and is stuck in a corner. Another interesting observation is that episodes b and h have start locations somewhat close to each other, as well as target locations, but the FWP agent (shown in purple) is able to successfully navigate episode-b but not episode-h. This, however, is quite rare an occurrence from our observation of the other test episodes as well (not presented here). Thus, agents can fail for even a slight increase in the level of difficulty at times.
(a) (b) (c) (d) (e) (f) (g) (h)</p>
<p>D. Transfer Learning (TL) Experiments</p>
<p>One of the test of robustness of a learned policy on one task is its ability to generalize to another, but related, task. To this end, we undertake TL of VAE representations and DRL policies from Quantico to the Pleasant scene in the Gibson dataset [10]. Both scenes correspond to indoor homes with living rooms, bedrooms, etc., and have similar, albeit not same, furniture/appliances. Similar tests were also undertaken by [12] for evaluating TL for robot navigation. The VAE is fine-tuned for Pleasant using the pre-trained VAE weights from Quantico. The SWP-10 trained for Quantico is used as starting policy network weights and fine-tuned on Pleasant following a PointNav approach (i.e., without any A* curriculum), and this agent after transfer learning (TL) on the new scene is referred to as TL-SWP. After re-training for ∼ 60k episodes on the new scene, on 500 test episodes, TL-SWP has a mean SPL of 0.87 and a mean success rate of 0.95. Thus, with fewer episodes of re-training on the new scene, TL achieves reasonable performance. In future work, we expect to conduct a more systematic TL investigation on Habitat scenes similar to [12], where the number of scenes used in the first training was shown to have an impact on the transfer learning performance.</p>
<p>VI. CONCLUSIONS</p>
<p>We look at the task of point-to-point navigation (PointNav) in the Habitat environment [1]. An agent gets photo-realistic RGBD images from this environment and learns an optimal navigation policy using Deep Reinforcement Learning (DRL). We introduce two ways of improving, in terms of speed and final metrics, the DRL training process: (1) We separate the perception and control tasks by pre-training a VAE to learn an embedding for the RGBD data. (2) We use A<em>, a traditional robotic path planning algorithm like training wheels on a bike that eventually come off, and train the DRL algorithm in an incremental curriculum. This involves increasing the path length required to successfully complete the PointNav task. We experiment with two curricula: one that increases the path length in a continuous fashion, and another that does this discretely. At test time, we do not need the help of A</em> any more.</p>
<p>Our agents learn faster compared to the PointNav baseline as well as achieve higher final SPL scores and success rates. The perception embedding is trained once for an environment, and can then be used for a variety of agent policies. The training of a number of sub-policies allows these policies to be used independently for shorter-distance navigation, or to be used to build up more sophisticated long range navigation policies. The ability to scan a real-world environment and set up its photo-realistic virtual twin for training direct perception-to-navigation policies is expected to accelerate the deployment of mobile robots around the homes and factories of our future. Our work is a step improvement in this direction.</p>
<p>In future work, we plan on using conventional local path planning approaches to augment the reward function for DRL and try to imbibe the network with a neural map [26].</p>
<p>Fig. 2 :
2A twin (RGB-depth) VAE learns an embedded representation of the environment, which is then used to train a navigation policy using DRL. Information flow during the VAE and DRL training are shown in red and orange respectively.</p>
<p>Fig. 3 :
3Effect of the latent encoding on the training of DRL agents.</p>
<p>Fig. 4 :
4Training performance for the WP-N agents. The PointNav agent (red curve) was run till ∼ 170k episodes (see the red curve fromFig. 3) but is presented here only till 75k episodes for better comparison with the other agents presented in the figure.</p>
<p>Fig. 5 :
5Training performance for the SWP-10 and FWP agents.</p>
<p>Fig. 6 :
6Test time paths traced by the PointNav, SWP-10 and FWP agents for different episodes. The start and target positions are represented by the green and blue squares respectively. The paths traced are shown in red for PointNav, yellow for SWP and purple for the FWP agents. The SPL values for the three agents for the respective episode are also shown in each sub-figure. We recommend that this figure be viewed in digital format and zoomed in for better clarity.</p>
<p>Note that SWP-N and FWP become PointNav agents by the end of their training. However WP-N is an agent that has ) pointgoal sensor reading, P G t ; and (4) heading angle, H t . The pre-trained twin-VAE is used to encode s RGBonly mastered a shorter navigation distance compared to the 
original PointNav agent. We use WP-N to set up the problem 
and demonstrate the efficacy of navigating smaller paths and 
using this to boot-strap the training of longer path (SWP-N 
and FWP) agents. Hence, SWP-N and FWP do not require 
A<em> at test time, whereas WP-N agents require A</em> at test time </p>
<p>to obtain intermediate goal locations. In this paper, we will 
focus more on SWP-N and FWP for this reason. 
At any time instant t, let the sensory readings be denoted 
as follows: (1) RGB image, s RGB </p>
<p>t </p>
<p>; (2) Depth map, s Depth </p>
<p>t </p>
<p>; 
(3t </p>
<p>to 
z RGB </p>
<p>t </p>
<p>and s Depth </p>
<p>t </p>
<p>to z Depth </p>
<p>t </p>
<p>TABLE I :
ISPL and success rate for WP-N agentsnumber of intermediate waypoints, the agents take longer to 
achieve an average SPL of over 0.8, which is as expected. </p>
<p>TABLE</p>
<p>The authors are with Ford Greenfield Labs, Palo Alto, CA, USA.kbalak18@ford.com, pchakra5@ford.com, sshriva5@ford.com 2 More information and videos of robot navigation using our approach can be found at: https://www.towardsautonomy.com/ Robot-Navigation-Using-Vision-Embedding</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra, "Habitat: A platform for embodied ai research," Arxiv: https://arxiv.org/abs/1904.01201, 2019.</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, arXiv:1312.6114arXiv preprintD. P. Kingma and M. Welling, "Auto-encoding variational bayes," arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Neural representations for sensory-motor control, ii: Learning a head-centered visuomotor representation of 3-d target position. S Grossberg, F Guenther, D Bullock, D Greve, Neural Networks. 61S. Grossberg, F. Guenther, D. Bullock, and D. Greve, "Neural rep- resentations for sensory-motor control, ii: Learning a head-centered visuomotor representation of 3-d target position," Neural Networks, vol. 6, no. 1, pp. 43-67, 1993.</p>
<p>State representation learning for control: An overview. T Lesort, N Diaz-Rodriguez, J.-F Goudou, D Filliat, Neural Networks. 108T. Lesort, N. Diaz-Rodriguez, J.-F. Goudou, and D. Filliat, "State representation learning for control: An overview," Neural Networks, vol. 108, pp. 379-392, 2018.</p>
<p>World models. D Ha, J Schmidhuber, D. Ha and J. Schmidhuber, "World models," Arxiv: https://arxiv.org/abs/1803.10122, 2018.</p>
<p>Splitnet: Sim2sim and task2task transfer for embodied visual navigation. D Gordon, A Kadian, D Parikh, J Hoffman, D Batra, D. Gordon, A. Kadian, D. Parikh, J. Hoffman, and D. Batra, "Splitnet: Sim2sim and task2task transfer for embodied visual navigation," Arxiv: https://arxiv.org/abs/1905.07512, 2019.</p>
<p>Near-optimal representation learning for hierarchical reinforcement learning. O Nachum, S Gu, H Lee, S Levine, O. Nachum, S. Gu, H. Lee, and S. Levine, "Near-optimal repre- sentation learning for hierarchical reinforcement learning," Arxiv: https://arxiv.org/abs/1810.01257, 2019.</p>
<p>Recent advances in autoencoder-based representation learning. M Tschannen, O Bachem, M Lucic, M. Tschannen, O. Bachem, and M. Lucic, "Recent advances in autoencoder-based representation learning," Arxiv: https://arxiv.org/abs/1812.05069, 2018.</p>
<p>A Nair, V Pong, M Dalal, S Bahl, S Lin, S Levine, Visual reinforcement learning with imagined goals. A. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine, "Visual reinforcement learning with imagined goals," Arxiv: https://arxiv.org/abs/1807.04742, 2018.</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A Zamir, H Zhi-Yang, A Sax, J Malik, S Savarese, CVPR. F. Xia, A. Zamir, H. Zhi-Yang, A. Sax, J. Malik, and S. Savarese, "Gibson env: Real-world perception for embodied agents," CVPR, Arxiv: https://arxiv.org/abs/1808.10654, 2018.</p>
<p>Learning your way without map or compass: Panoramic target driven visual navigation. D Watkins-Valls, J Xu, N Waytowich, P Allen, D. Watkins-Valls, J. Xu, N. Waytowich, and P. Allen, "Learning your way without map or compass: Panoramic target driven visual navigation," Arxiv: https://arxiv.org/abs/1909.09295, 2019.</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, E. Kolve, R. Mottaghi, W. Han, E. Vanderbilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, "Ai2-thor: An interactive 3d environment for visual ai," Arxiv: https://arxiv.org/abs/1712.05474, 2017.</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, "Playing atari with deep reinforcement learning," Arxiv: https://arxiv.org/abs/1312.5602, 2013.</p>
<p>Dd-ppo: Learning nearperfect pointgoal navigators from 2.5 billion frames. E Wijmans, A Kadian, A Morcos, I Lee, D Essa, M Parikh, D Savva, Batra, E. wijmans, A. Kadian, A. Morcos, s. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, "Dd-ppo: Learning near- perfect pointgoal navigators from 2.5 billion frames," ICLR: https://openreview.net/forum?id=H1gX8C4YPr, 2020.</p>
<p>Unsupervised image-to-image translation networks. M.-Y Liu, T Breuel, J Kautz, Advances in Neural Information Processing Systems. M.-Y. Liu, T. Breuel, and J. Kautz, "Unsupervised image-to-image translation networks," in Advances in Neural Information Processing Systems, 2017, pp. 700-708.</p>
<p>Gen-slam: Generative modeling for monocular simultaneous localization and mapping. P Chakravarthy, P Narayanan, T Roussel, P. Chakravarthy, P. Narayanan, and T. Roussel, "Gen-slam: Generative modeling for monocular simultaneous localization and mapping," Arxiv: https://arxiv.org/abs/1902.02086, 2019.</p>
<p>Matterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, International Conference on 3D Vision (3DV). A. Chang, A. Dai, T. funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3d: Learning from rgb-d data in indoor environments," International Conference on 3D Vision (3DV), 2017.</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, "Target-driven visual navigation in indoor scenes using deep reinforcement learning," Arxiv: https://arxiv.org/abs/1609.05143, 2016.</p>
<p>On evaluation of embodied navigation agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, A R Zamir, P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir, "On evaluation of embodied navigation agents," Arxiv: https://arxiv.org/abs/1807.06757, 2018.</p>
<p>Prm-rl: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning. A F Faust, O Ramirez, M Fiser, K Oslund, A Francis, J Davidson, L Tapia, A. F. Faust, O. Ramirez, M. Fiser, K. Oslund, A. Francis, J. Davidson, and L. Tapia, "Prm-rl: Long-range robotic navigation tasks by com- bining reinforcement learning and sampling-based planning," Arxiv: https://arxiv.org/abs/1710.03937, 2018.</p>
<p>Learning multi-level hierarchies with hindsight. A Levy, G Konidaris, R Platt, K Saenko, A. Levy, G. Konidaris, R. Platt, and K. Saenko, "Learning multi-level hierarchies with hindsight," Arxiv: https://arxiv.org/pdf/1712.00948.pdf, 2019.</p>
<p>Adam: A method for stochastic optimization. D Kingma, J Ba, D. Kingma and J. Ba, "Adam: A method for stochastic optimization," Arxiv: https://arxiv.org/abs/1412.6980, 2014.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, Proximal policy optimization algorithms. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," Arxiv: https://arxiv.org/abs/1707.06347, 2017.</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Computation, vol. 9(8), pp. 1735-1780, 1997.</p>
<p>Highdimensional continuous control using generalized advantage estimation. J Schulman, P Moritz, S Levine, M Jordan, P Abbeel, J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, "High- dimensional continuous control using generalized advantage estima- tion," Arxiv: https://arxiv.org/abs/1506.02438, 2016.</p>
<p>Learning to explore using active neural mapping. D S Chaplot, S Gupta, D Gandhi, A Gupta, R Salakhutdinov, ICLRD. S. Chaplot, S. Gupta, D. Gandhi, A. Gupta, and R. Salakhutdinov, "Learning to explore using active neural mapping," ICLR, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
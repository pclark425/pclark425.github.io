<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1029 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1029</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1029</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-253098789</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.12448v1.pdf" target="_blank">Probing Transfer in Deep Reinforcement Learning without Task Engineering</a></p>
                <p><strong>Paper Abstract:</strong> We evaluate the use of original game curricula supported by the Atari 2600 console as a heterogeneous transfer benchmark for deep reinforcement learning agents. Game designers created curricula using combinations of several discrete modifications to the basic versions of games such as Space Invaders, Breakout and Freeway, making them progressively more challenging for human players. By formally organising these modifications into several factors of variation, we are able to show that Analyses of Variance (ANOVA) are a potent tool for studying the effects of human-relevant domain changes on the learning and transfer performance of a deep reinforcement learning agent. Since no manual task engineering is needed on our part, leveraging the original multi-factorial design avoids the pitfalls of unintentionally biasing the experimental setup. We find that game design factors have a large and statistically significant impact on an agent's ability to learn, and so do their combinatorial interactions. Furthermore, we show that zero-shot transfer from the basic games to their respective variations is possible, but the variance in performance is also largely explained by interactions between factors. As such, we argue that Atari game curricula offer a challenging benchmark for transfer learning in RL, that can help the community better understand the generalisation capabilities of RL agents along dimensions which meaningfully impact human generalisation performance. As a start, we report that value-function finetuning of regularly trained agents achieves positive transfer in a majority of cases, but significant headroom for algorithmic innovation remains. We conclude with the observation that selective transfer from multiple variants could further improve performance.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1029.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1029.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rainbow-IQN (SpaceInv)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rainbow-IQN agent evaluated on Space Invaders curricula</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free deep reinforcement learning agent (Rainbow improvements + Implicit Quantile Networks) trained and evaluated on the 32 Space Invaders variants in the Atari ALE curricula; experiments measure learning from scratch, zero-shot policy transfer and value-function finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rainbow-IQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free deep reinforcement learning agent combining Rainbow components (double Q-learning, prioritized replay, multi-step learning, etc.) with Implicit Quantile Networks (IQN) to approximate return distributions; trained with clipped-SGD and tuned hyperparameters per variant.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Atari 2600 - Space Invaders variants (ALE)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>32 discrete variants built from combinatorial binary factors (difficulty switch, moving shields, zigzagging bombs, fast bombs, invisible invaders) that change observation properties and dynamics relative to the default Space Invaders entry game; variants reuse the same sprites but modify visibility, projectile dynamics, shield movement and player size.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterised qualitatively by game-design factors and by empirical learning difficulty defined as final average returns under a fixed budget; experiment-specific quantitative measures: number of factor combinations (32 variants), training budget (200 million environment steps for experts), finetuning budget (10 million steps), and multi-factor ANOVA F-statistics (e.g. Interaction F=1341.37 for expert scores).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (variants produce substantially different learning dynamics; many variants induce low expert returns under fixed budget)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct environment instances (32 variants); combinatorial activation of 5 binary design factors; analysis of variation via multi-factor ANOVA testing main and interaction effects; off-diagonal transfer matrix (all-to-all zero-shot evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (32 combinatorial variants with multiple interacting factors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Game return (score), reported as raw scores and as normalized scores (percentage of respective variant-expert final performance); zero-shot and finetuned results compared to variant-expert baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Expert training: 200 million environment steps per variant; finetuning: 10 million steps. Empirical findings: large spread of raw expert returns across Space Invaders variants (see Table 8); ANOVA interaction for expert scores: F=1341.37, p=5.33e-78. Zero-shot and finetuning normalized percentages vary widely per variant (examples in Table 8); zero-shot from default sometimes low (single-digit %s) or moderate (tens of %), finetuning from default often gives positive transfer but highly heterogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly reports that discrete changes (factors) and their combinatorial interactions have large, statistically significant impacts on both learning-from-scratch performance and zero-shot transfer; interaction terms in ANOVA explain a large portion of variance, implying trade-offs where added variation (factor combinations) can increase task complexity and reduce both learnability and transferability under fixed data budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-variant expert training from scratch (200M steps), zero-shot evaluation of default-game experts, and value-function finetuning of default experts on target variants (10M steps); also all-to-all expert transfer matrices and source-selection strategies ('random','default','top3','best').</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot generalisation from default (0 00) resulted in non-negligible positive transfer in many Space Invaders variants but with large variance; multi-factor ANOVA shows difficulty, invisibility, bomb speed and moving shields significantly affect zero-shot performance. Finetuning default experts for 10M steps produced positive transfer in many cases but results were heterogeneous across variants.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experts trained with 200 million environment interactions; finetuning used 10 million interactions (20× less), with results showing that finetuning can recover a substantial fraction of variant-expert performance within the smaller budget for many variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Space Invaders design factors significantly change learnability and transfer (ANOVA interaction highly significant). 2) Zero-shot transfer from default experts is sometimes substantial but highly variable and often far from optimal. 3) Value-function finetuning (10M steps) yields positive transfer in a majority of variants, but heterogeneity remains. 4) Combinatorial interactions between factors largely explain variance in learning and transfer performance, indicating a trade-off between environment variation (factor combinations) and ease of learning under fixed data budgets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1029.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1029.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rainbow-IQN (Breakout)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rainbow-IQN agent evaluated on Breakout curricula</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rainbow-IQN evaluated across 24 Breakout variants generated by three factors (difficulty switch reducing paddle size, 'Rules' with 3 modes, and 'Extras' with 4 modes) measuring training-from-scratch performance, zero-shot transfer from default, and finetuning benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rainbow-IQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free deep RL agent (Rainbow + IQN) used to learn per-variant policies, produce default-game experts and be finetuned on target variants; learning via Q-learning updates with prioritized replay and IQN quantile approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Atari 2600 - Breakout variants (ALE)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>24 variants produced by combinatorial factors: binary difficulty switch (paddle width), 'Rules' with values {Breakout, Timed Breakout, Breakthru} that change scoring and dynamics, and 'Extras' with options {Steerable ball, Catch, Invisible wall, None}. Variants change both transition dynamics and observation formatting (e.g. invisible walls), creating diverse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Measured via empirical final returns under fixed data budgets (200M steps for variant-experts), normalized scores relative to variant-expert, and statistical ANOVA (Rules factor: F=1368.66 for expert scores). Number of variants = 24.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (different rule-sets and observation/dynamics changes create substantially different learning dynamics and difficulty levels under fixed budgets)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>24 discrete environment instances; multi-level factor 'Rules' (3 values) and 'Extras' (4 values) leading to combinatorial variation; evaluated through all-to-all transfer matrices and selection strategies ('top3','best').</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (24 combinatorial variants spanning dynamics and observation changes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Raw game score and normalized score (% of variant-expert); zero-shot and finetuned scores reported in tables and figures; ANOVA F-statistics used to quantify factor effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Experts trained 200M steps; ANOVA indicates strong main effects and interactions (expert-scores: Rules F=1368.66, Extras F=179.84; interaction F=23.04). Zero-shot from default sometimes outperformed variant-experts for some 'Timed Breakout' variants; finetuning (10M steps) generally produced positive transfer in a majority of cases, with heterogeneous magnitudes (see Table 9 for per-variant normalized values).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper finds that rule changes and extras substantially modulate both learning difficulty and zero-shot transfer; interactions between factors cause non-additive effects, so increasing variation via combinations of factors often increases difficulty and reduces uniform transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Per-variant expert training from scratch (200M steps), zero-shot evaluation of default experts, finetuning default experts for 10M steps, and source-selection experiments (all-to-all matrices, 'top3', 'best' single-source).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Breakout showed some of the highest zero-shot transfer from default experts (occasionally exceeding variant-expert performance in 'Timed Breakout'), but overall transferred policies were generally suboptimal; finetuning improved performance beyond zero-shot in most cases but with large heterogeneity depending on Rule and Extras factors.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experts: 200M steps; finetuning: 10M steps (20× less). Some variants were learnable from scratch within 10M (e.g., Freeway-like), but Breakout often required larger budgets; hyperparameter grids tuned per variant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Breakout 'Rules' and 'Extras' are dominant factors affecting learning and transfer (very large ANOVA F-values). 2) Some variants (e.g., Timed Breakout) allow default experts to zero-shot outperform variant-experts, demonstrating non-monotonic relations between variation and difficulty. 3) Finetuning from default experts generally yields positive transfer but heterogeneously across variants, indicating significant headroom for improved transfer strategies and source-selection.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1029.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1029.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rainbow-IQN (Freeway)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rainbow-IQN agent evaluated on Freeway curricula</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rainbow-IQN trained and evaluated across 16 Freeway variants (combinations of difficulty switch, traffic thickness (4 levels), and vehicle speed randomness) to study the impact of variation on learning and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rainbow-IQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free deep RL agent (Rainbow + IQN) trained as per-variant experts and used for zero-shot transfer and finetuning experiments; learning uses replay with sticky actions and standard Atari preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Atari 2600 - Freeway variants (ALE)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>16 variants composed from 3 factors: binary difficulty switch (penalty on hit knockback distance), traffic thickness (4 discrete density levels), and vehicle speed (constant or randomized). Tasks require repeatedly crossing multi-lane traffic for score within a fixed time.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterised empirically via final returns under 200M step expert training, normalized relative scores, and ANOVA (Traffic F=453.84 for expert scores; speeds F=91.38). Variation count = 16.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (per paper, Freeway variants produced more similar learning performance than the other games — agents achieved comparable performance across many variants under fixed budgets)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>16 distinct variants; traffic thickness has 4 levels; vehicle speed factor has 2 levels; variation analysed with ANOVA and transfer matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (fewer and less disruptive factor combinations; empirical results show less variance in agent performance across variants)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Raw game score and normalized score (% of variant-expert); zero-shot and finetuned results compared; ANOVA F-statistics used to evaluate factor effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Experts trained 200M steps; finetuning 10M steps. Empirically, Freeway shows more uniform expert performance across variants (Table 10) and stronger zero-shot transfer between policies differing only by difficulty switch. ANOVA interaction significant (expert scores interaction F=274.76, p=4.42e-28). 'Top3' source-selection strategy yielded >40% median normalized performance across curricula (paper-level summary).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — although Freeway variants are related and show less heterogeneous expert performance, ANOVA still finds significant main and interaction effects; the paper notes combinations of factors induce different learning dynamics, but Freeway exhibited comparatively low sensitivity to the difficulty switch.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Per-variant expert training (200M steps), zero-shot evaluation, finetuning default experts for 10M steps, all-to-all transfer matrices and source-selection ('top3','best').</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot transfer between certain Freeway variants (especially those differing only by difficulty switch) was strong; finetuning often improved performance but in some Freeway variants agents learned competitively even from scratch in 10M steps. Overall Freeway exhibited the most uniform transfer/learning of the three games.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experts: 200M steps; finetuning: 10M steps. In some Freeway variants substantial performance could be attained with the 10M finetuning budget from scratch as well (ablation showed agents can learn to significant performance with 10M steps for Freeway).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Freeway variants show less variance in expert performance compared to Space Invaders or Breakout. 2) Traffic thickness and speed factors significantly affect transfer and learning, but difficulty switch has smaller effect on expert learning for Freeway. 3) Source selection (top3) provides a robust zero-shot strategy producing >40% median normalized performance across curricula, demonstrating that multi-source reuse helps generalisation in varied environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is deep reinforcement learning really superhuman on atari? <em>(Rating: 2)</em></li>
                <li>Progressive neural networks <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 1)</em></li>
                <li>Meta- world: A benchmark and evaluation for multi-task and meta reinforcement learning <em>(Rating: 1)</em></li>
                <li>Transfer learning in deep reinforcement learning: A survey <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1029",
    "paper_id": "paper-253098789",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Rainbow-IQN (SpaceInv)",
            "name_full": "Rainbow-IQN agent evaluated on Space Invaders curricula",
            "brief_description": "A model-free deep reinforcement learning agent (Rainbow improvements + Implicit Quantile Networks) trained and evaluated on the 32 Space Invaders variants in the Atari ALE curricula; experiments measure learning from scratch, zero-shot policy transfer and value-function finetuning.",
            "citation_title": "PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING",
            "mention_or_use": "use",
            "agent_name": "Rainbow-IQN",
            "agent_description": "Model-free deep reinforcement learning agent combining Rainbow components (double Q-learning, prioritized replay, multi-step learning, etc.) with Implicit Quantile Networks (IQN) to approximate return distributions; trained with clipped-SGD and tuned hyperparameters per variant.",
            "agent_type": "simulated agent",
            "environment_name": "Atari 2600 - Space Invaders variants (ALE)",
            "environment_description": "32 discrete variants built from combinatorial binary factors (difficulty switch, moving shields, zigzagging bombs, fast bombs, invisible invaders) that change observation properties and dynamics relative to the default Space Invaders entry game; variants reuse the same sprites but modify visibility, projectile dynamics, shield movement and player size.",
            "complexity_measure": "Characterised qualitatively by game-design factors and by empirical learning difficulty defined as final average returns under a fixed budget; experiment-specific quantitative measures: number of factor combinations (32 variants), training budget (200 million environment steps for experts), finetuning budget (10 million steps), and multi-factor ANOVA F-statistics (e.g. Interaction F=1341.37 for expert scores).",
            "complexity_level": "high (variants produce substantially different learning dynamics; many variants induce low expert returns under fixed budget)",
            "variation_measure": "Number of distinct environment instances (32 variants); combinatorial activation of 5 binary design factors; analysis of variation via multi-factor ANOVA testing main and interaction effects; off-diagonal transfer matrix (all-to-all zero-shot evaluations).",
            "variation_level": "high (32 combinatorial variants with multiple interacting factors)",
            "performance_metric": "Game return (score), reported as raw scores and as normalized scores (percentage of respective variant-expert final performance); zero-shot and finetuned results compared to variant-expert baseline.",
            "performance_value": "Expert training: 200 million environment steps per variant; finetuning: 10 million steps. Empirical findings: large spread of raw expert returns across Space Invaders variants (see Table 8); ANOVA interaction for expert scores: F=1341.37, p=5.33e-78. Zero-shot and finetuning normalized percentages vary widely per variant (examples in Table 8); zero-shot from default sometimes low (single-digit %s) or moderate (tens of %), finetuning from default often gives positive transfer but highly heterogeneous.",
            "complexity_variation_relationship": "Yes — the paper explicitly reports that discrete changes (factors) and their combinatorial interactions have large, statistically significant impacts on both learning-from-scratch performance and zero-shot transfer; interaction terms in ANOVA explain a large portion of variance, implying trade-offs where added variation (factor combinations) can increase task complexity and reduce both learnability and transferability under fixed data budgets.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-variant expert training from scratch (200M steps), zero-shot evaluation of default-game experts, and value-function finetuning of default experts on target variants (10M steps); also all-to-all expert transfer matrices and source-selection strategies ('random','default','top3','best').",
            "generalization_tested": true,
            "generalization_results": "Zero-shot generalisation from default (0 00) resulted in non-negligible positive transfer in many Space Invaders variants but with large variance; multi-factor ANOVA shows difficulty, invisibility, bomb speed and moving shields significantly affect zero-shot performance. Finetuning default experts for 10M steps produced positive transfer in many cases but results were heterogeneous across variants.",
            "sample_efficiency": "Experts trained with 200 million environment interactions; finetuning used 10 million interactions (20× less), with results showing that finetuning can recover a substantial fraction of variant-expert performance within the smaller budget for many variants.",
            "key_findings": "1) Space Invaders design factors significantly change learnability and transfer (ANOVA interaction highly significant). 2) Zero-shot transfer from default experts is sometimes substantial but highly variable and often far from optimal. 3) Value-function finetuning (10M steps) yields positive transfer in a majority of variants, but heterogeneity remains. 4) Combinatorial interactions between factors largely explain variance in learning and transfer performance, indicating a trade-off between environment variation (factor combinations) and ease of learning under fixed data budgets.",
            "uuid": "e1029.0"
        },
        {
            "name_short": "Rainbow-IQN (Breakout)",
            "name_full": "Rainbow-IQN agent evaluated on Breakout curricula",
            "brief_description": "Rainbow-IQN evaluated across 24 Breakout variants generated by three factors (difficulty switch reducing paddle size, 'Rules' with 3 modes, and 'Extras' with 4 modes) measuring training-from-scratch performance, zero-shot transfer from default, and finetuning benefits.",
            "citation_title": "PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING",
            "mention_or_use": "use",
            "agent_name": "Rainbow-IQN",
            "agent_description": "Model-free deep RL agent (Rainbow + IQN) used to learn per-variant policies, produce default-game experts and be finetuned on target variants; learning via Q-learning updates with prioritized replay and IQN quantile approximations.",
            "agent_type": "simulated agent",
            "environment_name": "Atari 2600 - Breakout variants (ALE)",
            "environment_description": "24 variants produced by combinatorial factors: binary difficulty switch (paddle width), 'Rules' with values {Breakout, Timed Breakout, Breakthru} that change scoring and dynamics, and 'Extras' with options {Steerable ball, Catch, Invisible wall, None}. Variants change both transition dynamics and observation formatting (e.g. invisible walls), creating diverse tasks.",
            "complexity_measure": "Measured via empirical final returns under fixed data budgets (200M steps for variant-experts), normalized scores relative to variant-expert, and statistical ANOVA (Rules factor: F=1368.66 for expert scores). Number of variants = 24.",
            "complexity_level": "high (different rule-sets and observation/dynamics changes create substantially different learning dynamics and difficulty levels under fixed budgets)",
            "variation_measure": "24 discrete environment instances; multi-level factor 'Rules' (3 values) and 'Extras' (4 values) leading to combinatorial variation; evaluated through all-to-all transfer matrices and selection strategies ('top3','best').",
            "variation_level": "high (24 combinatorial variants spanning dynamics and observation changes)",
            "performance_metric": "Raw game score and normalized score (% of variant-expert); zero-shot and finetuned scores reported in tables and figures; ANOVA F-statistics used to quantify factor effects.",
            "performance_value": "Experts trained 200M steps; ANOVA indicates strong main effects and interactions (expert-scores: Rules F=1368.66, Extras F=179.84; interaction F=23.04). Zero-shot from default sometimes outperformed variant-experts for some 'Timed Breakout' variants; finetuning (10M steps) generally produced positive transfer in a majority of cases, with heterogeneous magnitudes (see Table 9 for per-variant normalized values).",
            "complexity_variation_relationship": "Yes — the paper finds that rule changes and extras substantially modulate both learning difficulty and zero-shot transfer; interactions between factors cause non-additive effects, so increasing variation via combinations of factors often increases difficulty and reduces uniform transferability.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Per-variant expert training from scratch (200M steps), zero-shot evaluation of default experts, finetuning default experts for 10M steps, and source-selection experiments (all-to-all matrices, 'top3', 'best' single-source).",
            "generalization_tested": true,
            "generalization_results": "Breakout showed some of the highest zero-shot transfer from default experts (occasionally exceeding variant-expert performance in 'Timed Breakout'), but overall transferred policies were generally suboptimal; finetuning improved performance beyond zero-shot in most cases but with large heterogeneity depending on Rule and Extras factors.",
            "sample_efficiency": "Experts: 200M steps; finetuning: 10M steps (20× less). Some variants were learnable from scratch within 10M (e.g., Freeway-like), but Breakout often required larger budgets; hyperparameter grids tuned per variant.",
            "key_findings": "1) Breakout 'Rules' and 'Extras' are dominant factors affecting learning and transfer (very large ANOVA F-values). 2) Some variants (e.g., Timed Breakout) allow default experts to zero-shot outperform variant-experts, demonstrating non-monotonic relations between variation and difficulty. 3) Finetuning from default experts generally yields positive transfer but heterogeneously across variants, indicating significant headroom for improved transfer strategies and source-selection.",
            "uuid": "e1029.1"
        },
        {
            "name_short": "Rainbow-IQN (Freeway)",
            "name_full": "Rainbow-IQN agent evaluated on Freeway curricula",
            "brief_description": "Rainbow-IQN trained and evaluated across 16 Freeway variants (combinations of difficulty switch, traffic thickness (4 levels), and vehicle speed randomness) to study the impact of variation on learning and transfer.",
            "citation_title": "PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING",
            "mention_or_use": "use",
            "agent_name": "Rainbow-IQN",
            "agent_description": "Model-free deep RL agent (Rainbow + IQN) trained as per-variant experts and used for zero-shot transfer and finetuning experiments; learning uses replay with sticky actions and standard Atari preprocessing.",
            "agent_type": "simulated agent",
            "environment_name": "Atari 2600 - Freeway variants (ALE)",
            "environment_description": "16 variants composed from 3 factors: binary difficulty switch (penalty on hit knockback distance), traffic thickness (4 discrete density levels), and vehicle speed (constant or randomized). Tasks require repeatedly crossing multi-lane traffic for score within a fixed time.",
            "complexity_measure": "Characterised empirically via final returns under 200M step expert training, normalized relative scores, and ANOVA (Traffic F=453.84 for expert scores; speeds F=91.38). Variation count = 16.",
            "complexity_level": "medium (per paper, Freeway variants produced more similar learning performance than the other games — agents achieved comparable performance across many variants under fixed budgets)",
            "variation_measure": "16 distinct variants; traffic thickness has 4 levels; vehicle speed factor has 2 levels; variation analysed with ANOVA and transfer matrices.",
            "variation_level": "medium (fewer and less disruptive factor combinations; empirical results show less variance in agent performance across variants)",
            "performance_metric": "Raw game score and normalized score (% of variant-expert); zero-shot and finetuned results compared; ANOVA F-statistics used to evaluate factor effects.",
            "performance_value": "Experts trained 200M steps; finetuning 10M steps. Empirically, Freeway shows more uniform expert performance across variants (Table 10) and stronger zero-shot transfer between policies differing only by difficulty switch. ANOVA interaction significant (expert scores interaction F=274.76, p=4.42e-28). 'Top3' source-selection strategy yielded &gt;40% median normalized performance across curricula (paper-level summary).",
            "complexity_variation_relationship": "Yes — although Freeway variants are related and show less heterogeneous expert performance, ANOVA still finds significant main and interaction effects; the paper notes combinations of factors induce different learning dynamics, but Freeway exhibited comparatively low sensitivity to the difficulty switch.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Per-variant expert training (200M steps), zero-shot evaluation, finetuning default experts for 10M steps, all-to-all transfer matrices and source-selection ('top3','best').",
            "generalization_tested": true,
            "generalization_results": "Zero-shot transfer between certain Freeway variants (especially those differing only by difficulty switch) was strong; finetuning often improved performance but in some Freeway variants agents learned competitively even from scratch in 10M steps. Overall Freeway exhibited the most uniform transfer/learning of the three games.",
            "sample_efficiency": "Experts: 200M steps; finetuning: 10M steps. In some Freeway variants substantial performance could be attained with the 10M finetuning budget from scratch as well (ablation showed agents can learn to significant performance with 10M steps for Freeway).",
            "key_findings": "1) Freeway variants show less variance in expert performance compared to Space Invaders or Breakout. 2) Traffic thickness and speed factors significantly affect transfer and learning, but difficulty switch has smaller effect on expert learning for Freeway. 3) Source selection (top3) provides a robust zero-shot strategy producing &gt;40% median normalized performance across curricula, demonstrating that multi-source reuse helps generalisation in varied environments.",
            "uuid": "e1029.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is deep reinforcement learning really superhuman on atari?",
            "rating": 2,
            "sanitized_title": "is_deep_reinforcement_learning_really_superhuman_on_atari"
        },
        {
            "paper_title": "Progressive neural networks",
            "rating": 2,
            "sanitized_title": "progressive_neural_networks"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 1,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Meta- world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "rating": 1,
            "sanitized_title": "meta_world_a_benchmark_and_evaluation_for_multitask_and_meta_reinforcement_learning"
        },
        {
            "paper_title": "Transfer learning in deep reinforcement learning: A survey",
            "rating": 2,
            "sanitized_title": "transfer_learning_in_deep_reinforcement_learning_a_survey"
        }
    ],
    "cost": 0.0181385,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING</p>
<p>Andrei A Rusu andrei@deepmind.com 
Sebastian Flennerhag flennerhag@deepmind.com 
Dushyant Rao dushyantr@deepmind.com 
Razvan Pascanu 
UKRaia Hadsell Deepmind 
PROBING TRANSFER IN DEEP REINFORCEMENT LEARNING WITHOUT TASK ENGINEERING</p>
<p>We evaluate the use of original game curricula supported by the Atari 2600 console as a heterogeneous transfer benchmark for deep reinforcement learning agents. Game designers created curricula using combinations of several discrete modifications to the basic versions of games such as Space Invaders, Breakout and Freeway, making them progressively more challenging for human players. By formally organising these modifications into several factors of variation, we are able to show that Analyses of Variance (ANOVA) are a potent tool for studying the effects of human-relevant domain changes on the learning and transfer performance of a deep reinforcement learning agent. Since no manual task engineering is needed on our part, leveraging the original multi-factorial design avoids the pitfalls of unintentionally biasing the experimental setup. We find that game design factors have a large and statistically significant impact on an agent's ability to learn, and so do their combinatorial interactions. Furthermore, we show that zero-shot transfer from the basic games to their respective variations is possible, but the variance in performance is also largely explained by interactions between factors. As such, we argue that Atari game curricula offer a challenging benchmark for transfer learning in RL, that can help the community better understand the generalisation capabilities of RL agents along dimensions which meaningfully impact human generalisation performance. As a start, we report that value-function finetuning of regularly trained agents achieves positive transfer in a majority of cases, but significant headroom for algorithmic innovation remains. We conclude with the observation that selective transfer from multiple variants could further improve performance.Published at 1st Conference on Lifelong Learning Agents, 2022 effects between factors, not just the isolated effects of the modifications they introduce. This reinforces the case for agents leveraging these systematic curricula, originally designed for human players, through transfer learning.Contributions:(1) We show that discrete changes to Atari game environments, challenging for human players, also modulate the performance of a popular model-free deep reinforcement learning algorithm starting from scratch. (2) Zero-shot transfer of policies trained on one game variation and tested on others can be significant, but performance is far from uniform across respective environments. (3) Interestingly, zero-shot transfer variance from default game experts is also well explained by game design factors, especially their interactions. (4) We empirically evaluate the performance of value-function finetuning, a general transfer learning technique compatible with model-free deep RL, and confirm that it can lead to positive transfer from basic game experts. (5) We point out that more complex challenges of transfer with deep RL are captured by Atari game variations, e.g. appropriate source task selection, fast policy transfer and evaluation using experts, as well as data efficient behaviour adaptation more generally.BACKGROUNDReinforcement Learning. A Markov Decision Processes (MDP) (Puterman, 1994) is the classic abstraction used to characterise the sequential interaction loop between an action taking agent and its environment, which responds with observations and rewards (Sutton &amp; Barto, 2018). Formally, a finite MDP is a tuple M = X , A, T , R, γ , where X is the state space, A is the action space, both finite sets, T : X × A × X → [0, 1] is the stochastic transition function which maps each state and action to a probability distribution over possible future states T (x, a, x ) = P (x |x, a), R : X × A × X → R is the reward distribution function, with r : X × A → R the expected immediate reward r(x, a) = E T [R(x, a, x )], and γ ∈ [0, 1] is the discount factor (Bellman, 1957). A stochastic policy function is the action selection strategy π : X ×A → [0, 1] which maps states to probability distributions over actions π(x) = P (a|x). The discounted sum of future rewards is the random variable Z π M (x, a) = ∞ t=0 γ t r(x t , a t ), where x 0 = x, a 0 = a, x t ∼ T (·|x t−1 , a t−1 ) and a t ∼ π(·|x t ). Given an MDP M and a policy π, the value function is the expectation over the discounted sum of future rewards, also called the expected return: V π</p>
<p>INTRODUCTION</p>
<p>A key open challenge in artificial intelligence is training reinforcement learning (RL) agents which generally achieve high returns when faced with critical changes to their environments (Schaul et al., 2018), motivated by impressive flexibility of animal and human learning. One way to approach the problem is through the prism of generalisation across related but distinct environments, also called transfer learning (Pan &amp; Yang, 2009;Taylor &amp; Stone, 2009) and comprehensively reviewed in the RL setting by Zhu et al. (2020). Many purpose-built benchmarks serve investigations into more specific research questions, e.g. transfer learning in particular cases where additional assumptions hold. While useful for progress, the challenge of transfer learning in the more general case remains.</p>
<p>Motivated by visual observation similarity, Machado et al. (2018) suggest using the newest iteration of the Atari Learning Environment (ALE) (Bellemare et al., 2013) to study the transfer learning between single-player game variants, or "flavours", found in the curricula of many Atari game titles. We will use the terms default or basic game interchangeably to refer to the environment recommended by respective manuals as the entry point. We call all other distinct games variations or variants of their respective default game. Hence, each Atari game title we consider provides a curriculum consisting of a default and its variants, all designed to teach and challenge human players in novel ways.</p>
<p>Studying transfer within curricula ensures that environments are related, and that meaningful knowledge reuse should be possible and beneficial. Interestingly, differences in game variant dynamics, subtle changes in observations which are crucial for optimal behaviour, novel environment states, as well as new player abilities challenge unified approaches to transfer learning across variations. Farebrother et al. (2018) argue that Deep Q-Network (DQN) agents (Mnih et al., 2015), trained with appropriate regularisation, can be effective in zero-shot and finetuning transfer scenarios. In this work we study the learning and transfer performance of an updated version of the DQN agent, called Rainbow-IQN (Toromanoff et al., 2019). We use ANOVA to quantitatively confirm the suspected link between game design factors and agent performance. While current approaches occasionally achieve meaningful transfer from default games, they have limited success for variations with several modifications. Our analyses reveal this is due to strong interaction Note that value functions depend critically on all aspects of the MDP. For any policy π, in general Q π M = Q π M for MDPs M and M defined over the same state and action sets, with T = T or R = R . Even if differences in dynamics or rewards are isolated to a subset of X × A, changes may be induced across the support of Q π M , since value functions are expectations over sums of discounted future rewards, issued according to R , along sequences of states decided entirely by the new environment dynamics T when following a fixed behaviour policy π. Nevertheless, many particular cases of interest exist, which we discuss below.</p>
<p>Transfer learning. Described and motivated in its general form by Caruana (1997); Thrun &amp; Pratt (1998);Bengio (2012), the goal of transfer learning is to use knowledge acquired from one or more source tasks to improve the learning process, or its outcomes, for one or more target tasks, e.g. by using fewer resources compared to learning from scratch. When this is achieved, we call it positive transfer. We often further qualify transfer learning by the metric used to measure specific effects. Several transfer learning metrics have been defined for the RL setting (Taylor &amp; Stone, 2009), but none capture all aspects of interest on their own. A first metric we use is "jumpstart" or "zero-shot" transfer, which is performance on the target task before gaining access to its data. Another highly relevant metric is "performance with fixed training epochs" (Zhu et al., 2020), defined as returns achieved in the target task after using fixed computation and data budgets under transfer learning conditions. One way to classify such approaches in RL is by the format of knowledge being transferred (Zhu et al., 2020), commonly: datasets, predictions, and/or parameters of neural networks encoding representations of observations, policies, value-functions or approximate state-transition "world models" acquired using source tasks. Another way to classify transfer learning approaches in RL is by their respective sets of assumptions about source and target tasks, also well illustrated by their associated benchmark domains: (1) Differences are limited to observations, but the underlying MDP is the same, e.g. domain adaptation and randomisation (Tobin et al., 2017), mastered through the generalisation abilities of large models (Cobbe et al., 2019;. (2) MDP states and dynamics are the same, but reward functions are different: successor features and representations (Barreto et al., 2017). (3) Overlapping state spaces with similar dynamics, e.g. multitask training and policy transfer (Rusu et al., 2016a;Schmitt et al., 2018), contextual parametric approaches, e.g. UVFAs (Schaul et al., 2015) and collections of skills/goals/options (Barreto et al., 2019). (4) Sus-pected but unqualified overlaps between tasks (Parisotto et al., 2016;Rusu et al., 2016b;2017). (5) Large, curated collections of similar environments designed to facilitate complex transfer and fast adaptation through meta-learning (Yu et al., 2020;Hospedales et al., 2020). All these works capture important sub-problems of interest, and clever design of specialised benchmarks has greatly aided progress. Atari game variations (Machado et al., 2018) offer the exciting prospect of direct comparisons between generalisations of transfer learning methods originally developed under different sets of assumptions, by measuring their performance along dimensions of variation which are meaningful and challenging for human players, one of many interesting criteria cutting across specialised paradigms.</p>
<p>[_____]-0_00</p>
<p>[  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I   Freeway (bottom right). All factors of variation are categorical and highlighted only when taking different values from default games. Binary factors are abbreviated by their initial, e.g. "Moving Shields" is plotted as 'M'. Non-binary factors are: Breakout "Rules" with additional values: 'T' for "Timed Breakout", or 'U' for "Breakthru"; Breakout "Extras" with additional values: 'S' for "Steerable", 'C' for "Catch", or 'I' for "Invisible"; Freeway "Traffic" with levels: 'K' for "Thick", 'R' for "Thicker", or 'T' for "Thickest". Colours indicate the "game mode", and hatching denotes the "difficulty" switch being activated. Colours and horizontal labels correspond to those used in subsequent figures. Labels include factor abbreviations to supplement the ALE naming conventions.</p>
<p>[<strong><em>]-0</em>00 [</strong>S]-0_04 [<strong>C]-0_08 [__I]-0_12 [<em>T</em>]-0_16 [<em>TS]-0_20 [_TC]-0_24 [_TI]-0_28 [_U</em>]-0_32 [_US]-0_36 [_UC]-0_40 [_UI]-0_44 [D</strong>]-1_00 [D_S]-1_04 [D_C]-1_08 [D_I]-1_12 [DT_]-1_16 [DTS]-1_20 [DTC]-1_24 [DTI]-1_28 [DU_]-1_32 [DUS]-1_36 [DUC]-1_40 [DUI]-1_44</p>
<p>Breakout</p>
<p>ALE ATARI AS A TRANSFER LEARNING BENCHMARK</p>
<p>With the first version of the Arcade Learning Environment (ALE), Bellemare et al. (2013) gave access to over 50 different Atari 2600 game titles through a unified observation and action interface, modelled after the standard reinforcement learning loop. The ALE proved an excellent development test-bed for building general deep RL agents, in no small part due to its diversity and being devoid of experimenter bias, since games were not modified by researchers.</p>
<p>The second and latest version of the ALE (Machado et al., 2018) opens up a wealth of game variations for transfer learning research. This is achieved by emulating functions of "difficulty" and "game select" switches for single-player games. The original cartridges of many game titles came with variations which could be selected and played using these switches. We assign unique identifiers of game variants using the X YZ notation, with X ∈ {0, 1} denoting the position of the "difficulty" switch, and YZ ∈ {00, 01, 02, . . . } indicating the selected "game mode". Value ranges are game specific and identical to those used by the ALE code-base (Machado et al., 2018). For example, the entry game version of each title is denoted as 0 00, which we call the default or basic game. All game variations are different environments that still feature the main concepts of the default. Furthermore, the variants were designed to serve as curricula for human players, hence positive transfer should be possible. Most importantly for our purposes, the ALE remains largely free of experimenter bias. Farebrother et al. (2018) selected a few representative variants from four game titles for experiments. We aim to analyse entire curricula, thus we consider the top three most popular titles from their list, according to sales, and use all their variations, for a total of 72 distinct environments.</p>
<p>The original designers created game variations using combinations of discrete modifications to a default "entry" game, qualitatively described in accompanying game manuals. We organise these discrete modification into formal factors of variation, following closely the original game design matrices sometimes explicitly plotted in manuals. In Figure 1 we formalise variant naming conventions relative to default games. We briefly explain the meanings of all design factors below, to help build an intuitive understanding of this heterogeneous collection of transfer learning scenarios.</p>
<p>Space Invaders. The player controls an Earth-based laser cannon which can move horizontally along the bottom of the screen. There is a grid of approaching aliens above the player, shooting laser bombs, and the objective of the game is to eliminate all the aliens before they reach the Earth, especially their Command Ships. Three destructible shields above the cannon offer some protection. The game ends once any alien reaches the Earth or the player's cannon is hit a third time with laser bombs. Game variants are combinations of five binary factors which, when activated, modify the default: (1) The difficulty switch widens the player's laser cannon, making it an easier target for enemy laser bombs.</p>
<p>(2) Shields move, and thus are harder to use.</p>
<p>(3) Enemy laser bombs zigzag, which makes them harder to predict.</p>
<p>(4) Enemy laser bombs drop faster. (5) Invaders are invisible, and only hitting one with the laser briefly illuminates the others. This creates a total of 32 variants of Space Invader.</p>
<p>Breakout. In order to achieve the best score in any variant, the players need to completely break walls with six layers of coloured bricks by knocking off said bricks with a ball, served and bounced towards the wall with a controllable paddle, which they can move horizontally along the bottom of the screen. The ball will also bounce off of screen edges, except for the bottom edge, where balls either come in contact with the paddle or are lost. Players have a total of five balls at their disposal, and the game ends when all balls are lost. Points are scored when bricks are knocked off the wall, and the ball accelerates on contact with bricks in the top three rows or after twelve consecutive hits. Game variants are created by all combinations of three factors: (1) The binary difficulty switch reduces the paddle's width by a quarter, making it easier to miss the ball.</p>
<p>(2) The precise rules which determine how points accumulate: (2-a) with standard "Breakout" rules, e.g. in the default game (0 00), the player must completely break down two walls of bricks, one after the other, while loosing the fewest balls; (2-b) under "Timed Breakout" rules, the player must completely break a single wall as fast as possible, no matter how many of the five balls are used; (2-c) with "Breakthru" rules, the player needs to break two walls consecutively, but the ball does not bounce off bricks, unlike in previous variants; the ball keeps going through the wall, quickly picking up speed and accumulating points.</p>
<p>(3) How the ball is aimed at the wall and "extras": (3-a) the ball simply bounces off the paddle at a position dependent angle; (3-b) the player can also steer the ball in flight; (3-c) the player is also able catch the ball and release it at a slower speed; (3-d) the wall is invisible and is only briefly illuminated when a brick if knocked off, so the player needs to remember its configuration in order to aim effectively. All factors together define 24 variants of Breakout.</p>
<p>Freeway. In all variants, the goal of the player is to safely get a chicken across ten lanes of freeway traffic as many times as possible in 2 minutes and 16 seconds. Variants are constructed as combinations of three design factors:</p>
<p>(1) The difficulty switch controls whether the chicken is knocked back one lane, or all the way to the kerb, when hit by incoming vehicles.</p>
<p>(2) Traffic "thickness", defined as four levels of traffic density.</p>
<p>(3) Vehicle speeds across lanes, either constant or randomised. Hence, there are 16 variants of Freeway.</p>
<p>METHODOLOGY</p>
<p>Experimental setup. Following the latest ALE benchmark recommendations (Machado et al., 2018), information about player "lives" is not disclosed to agents, and stochasticity is introduced using randomised action replay with probability 25%, also known as "sticky actions". Irrespective of redundancies, agents act using the full set of 18 actions. Environment observations (Atari screen frames) were pre-processed according to standard practice (Mnih et al., 2015;Hessel et al., 2018). We report agent returns at the end of training, averaged over the last 2 million steps.  (Wang et al., 2016) or noisy networks (Fortunato et al., 2018). We replaced the distributional RL approach C51 (Bellemare et al., 2017) with its more general form (IQN), since it has been shown to be superior (Dabney et al., 2018;Toromanoff et al., 2019).</p>
<p>We used the standard limit on environment interactions of 200 million steps (Mnih et al., 2015;Dabney et al., 2018;Machado et al., 2018;Toromanoff et al., 2019). Our study is the first to report Rainbow-IQN results for Atari game variants, hence we use independent hyper-parameter selection for expert training and finetuning experiments.</p>
<p>Expert Agent Finetuning. The experimental setup for finetuning follows closely that for agent training, except for two important differences: (1) We used 10 million environment steps to adapt to new variants, following Farebrother et al. (2018), which is 20× less data than what variant-experts are trained with. The aim of transfer learning is to reduce resources needed for acquiring new knowledge and behaviours, hence we are interested in improving sample complexity using transfer.</p>
<p>(2) The hyper-parameter grid was slightly adapted in order to improve chances of fast learning in this reduced data regime. Further details, including parameter grids, are listed in Appendix B.</p>
<p>Statistical Analyses. A multi-factor Analyses of Variance (ANOVA) (Girden, 1992) is a statistical procedure which investigates the influence that two or more independent variables, or factors, have on a dependent variable. The factors can take two or more categorical values, and experimental designs are often balanced: they use equal numbers of independent observations for all combinations of factor values, no less than three. Other assumptions are normality of deviations from group means, and equal or similar variances, see the discussion in Appendix C. ANOVA can be used to reveal statistically significant evidence against the null-hypothesis that group means are all the same. We study the influence of game design factors, discrete modification to default games, on average returns of learned policies.</p>
<p>Study Limitations. The relationship between hyper-parameters and quality of policies learned with deep RL is complex and poorly understood. Mnih et al. (2015) used task-agnostic DQN hyper-parameters, tuned across a few basic games. Later works introduce several interacting modification, including changes to sets of hyper-parameters (Schaul et al., 2016;Dabney et al., 2018;Toromanoff et al., 2019). With the risk of maximisation bias, we used grid searches per game variant in order to mitigate the greater risks of incorrect inferences about Atari curricula due to poor hyperparameter settings or divergence. Future works may perform fine-grained sensitivity analyses, using more than two random seeds. Due to computational limitations, we instead report the returns of the top-three agents from our grids.</p>
<p>EXPERIMENTS</p>
<p>We would like to characterise the performance of general transfer learning strategies across heterogeneous scenarios, designed to progressively challenge human players. While the shapes of learning curves are expected to be different between game variants, agent performance is thought to be bounded only by its inductive biases, the learning algorithm and practical limitation on resources, such as computation or interactions (Silver et al., 2021). We say that some game variant is "harder" for a given agent if the average returns of learned policies are lower given equal resource budgets. We aim to explain this in terms of the factors of variation which combinatorially define environments within curricula, knowing that they also impact human player performance. Comparing raw scores across the benchmark is difficult if the underlying tasks are "harder" to learn from scratch, or if variants have different scoring scales, as is sometimes the case here. Hence, we must first establish the performance of our agent learning game variants in isolation. On this basis, we then introduce a relative scoring scheme which enables conclusions to be drawn at the benchmark level.</p>
<p>TRAINING VARIANT-EXPERTS</p>
<p>We aim to answer the following questions: (1) Does our agent achieve similar levels of performance when learning variants of the same game from scratch? (2) If not, what explains differences in performance across variants? In Figure 2 we report means and standard deviations of top three variant-experts trained from scratch for 200 million environment steps. Our scores on default games are largely in line with those reported by other implementations of basic IQN agents, e.g. Castro et al. (2018). On remaining variants, we find that Rainbow-IQN performance varies widely for some game titles, less so on Freeway, even with independent hyper-parameter tuning. Overall, our agent's scores are below variant performance ceilings, suggesting that some game variations may be harder to learn. While this may be at times due to the agent's inductive biases-in particular, invisible objects-it is unlikely to fully explain observed variation, because such changes to basic games do not always have a detrimental impact on their own. For example, experts achieve significantly different levels of performance on identical variants of Space Invaders, except for visible vs invisible aliens (0 00 vs.0 08). However, experts have virtually equal performance with the same change to environment observations across Breakout variants, visible vs invisible walls of bricks (e.g. 0 00 vs. 0 12). Rather, we hypothesise that some variants are inherently harder to learn for the chosen agent, and thus a fixed training budget leads to performance differences across variations. This imposes a challenging bottleneck for transfer learning, which places high priority on limiting the resources expended for acquiring behaviours which maximise cumulative returns.</p>
<p>Statistical Analyses. We verify that variants have meaningful impact on learning by rejecting the null hypothesis that expert performance is the same across all combinations of the design factors which define game variants. We perform multi-factor Analysis of Variance (ANOVA) tests separately for each game title, and report results in Table 1. Interaction effects are statistically significant in all cases, supporting the hypothesis that-although conceptually similardesign factors introduce significant changes to agent learning dynamics, even within the same game. In the post-hoc   analyses of differences between groups we found that all factors have a statistically significant effect on agent learning from scratch apart from the "difficulty" factor in Freeway. Further details can be found in Appendix C.</p>
<p>Discussion. Some differences in agent performance across variants were expected due to the qualitative impact of some factors on human play, e.g. changes in the dynamics of state transitions between "Breakout" and "Breakthru" variants, changes in observations such as visible vs. invisible invaders, or the effect of the difficulty switch. It is somewhat surprising that agents learn Freeway variants to similar performance, irrespective of difficulty level. Generally, we find that combinations of factors seem to induce substantially different learning dynamics, each having a significant effect on the final performance of the agent, at least within the standard training budget considered here. Hence, Rainbow-IQN demonstrates different levels of data efficiency across variants, despite their many similarities. A good amount of the variation can be explained by game design factors, which further emphasises the value of this benchmark for systematic and unbiased investigations of transfer with reinforcement learning algorithms.</p>
<p>Considering these empirical findings, as well as the possibility of different asymptotic performance ceilings for different variants, the remainder of the paper reports results relative to variant-expert performance, in order to meaningfully compare transfer learning across variations and curricula on a unified scale. We stress that variant-expert agents are generally not near-optimal. The most surprising results were the comparatively low agent score on "Timed Breakout" variants (orange bars in the bottom left plot of Figure 2), which have identical transition dynamics to regular "Breakout", but observations differ in that time in seconds is displayed instead of current score. Overall, these findings imply that scores normalised by variant-expert performance can be higher than 100%. Nevertheless, a relative scoring scheme accounts for the main effects of factors of variation on our agent's data efficiency while learning particular game variants. This is essential for meaningful comparisons of transfer learning within and across game curricula.</p>
<p>[_____]-0_00</p>
<p>[ normalised score (%)</p>
<p>[<strong><em>]-0</em>00 [</strong>S]-0_04 [<strong>C]-0_08 [__I]-0_12 [<em>T</em>]-0_16 [<em>TS]-0_20 [_TC]-0_24 [_TI]-0_28 [_U</em>]-0_32 [_US]-0_36 [_UC]-0_40 [_UI]-0_44 [D</strong>]-1_00 [D_S]-1_04 [D_C]-1_08 [D_I]-1_12 [DT_]-1_16 [DTS]-1_20 [DTC]-1_24 [DTI]-1_28 [DU_]-1_32 [DUS]-1_36 [DUC]-1_40 [DUI]-1_44</p>
<p>Breakout: evaluation flavour 0 50 100 150 200 normalised score (%)</p>
<p>[<strong><em>]-0</em>00 [</strong>K]-0_01 [<strong>R]-0_02 [__T]-0_03 [<em>S</em>]-0_04 [_SK]-0_05 [_SR]-0_06 [_ST]-0_07 [D</strong>]-1_00 [D_K]-1_01 [D_R]-1_02 [D_T]-1_03 [DS_]-1_04 [DSK]-1_05 [DSR]-1_06 [DST]-1_07</p>
<p>Freeway  </p>
<p>ZERO-SHOT TRANSFER USING DEFAULT GAME EXPERTS</p>
<p>Game variants go beyond defaults in ways which are interesting to humans, providing additional challenges by design. Players will often become proficient in the default game, and then attempt to master others; indeed, several game manuals explicitly recommend this approach. Since deep neural networks are known to generalise to similar inputs, and since variants reuse the same visual elements (game sprites), we would like to answer the following questions:</p>
<p>(1) How much transfer can be achieved purely through the generalisation power of deep neural networks trained on default games (0 00)? (2) Do factors of variation explain the success of zero-shot transfer (Taylor &amp; Stone, 2009)?</p>
<p>Default game experts are evaluated, without further training, on all remaining variants of their respective games, and normalised scores are reported in Figure 3. Means and standard deviations of zero-shot transfer performance are plotted as percentages of the scores achieved by respective variant-experts, which were trained from scratch using 200 million interaction steps. Rainbow-IQN policies are derived using -greedy action selection according to value function approximations output by deep neural networks. In the zero-shot transfer case, errors of such predictions are unbounded, but depend on the differences in reward functions and transition dynamics between the source and transfer target MDPs, as well as on the specific function approximation method used. In the case of deep neural networks we have no formal guarantees for the quality of prediction on environment states never encountered in the source task, especially for those which do not exist at all in the source. All things considered, a surprising amount of positive zero-shot transfer is observed in all game domains, due to the sheer generalisation power of deep neural networks. In some cases, e.g. "Timed Breakout" variants, zero-shot transfer results in superior policies to what variant-experts have learned, although data and computational budgets were identical. However, these are exceptions. In the majority of cases, learning on default games results in lower return policies on other variants. That said, the observed zero-shot transfer is not negligible, and it does not appear uniform across game variations, which we investigate next.</p>
<p>Statistical Analyses.</p>
<p>In order to answer the second question we check whether zero-shot performance is statistically indistinguishable across game variants, our null hypothesis of interest. We proceed with a multi-factor ANOVA of the dependent variable, here raw zero-shot transfer score, as a function of factors of variation in each game domain. Results reported in Table 2 indicate a statistically significant interactions in all games, meaning that factors play a role in the level of zero-shot transfer observed. In post-hoc analyses we must correct for multiple comparisons, and hence significance at level α = 0.05 in Space Invaders is achieved for a corrected threshold of α c = 0.0015 or lower. We find statistically significant effects of all factors except the introduction of "Zigzagging Bombs" in Space Invaders.</p>
<p>Discussion. Zero-shot transfer results provide evidence that the benchmark offers ample challenges for transfer learning approaches. We find the highest levels of zero-shot policy transfer from default Breakout agents to variants, occasionally with higher performance than variant-experts. However, transferred policies are generally far from optimal, which naturally motivates investigations into fast adaptation through transfer learning. Overall, we observe substantial variance in the performance of fixed expert policies trained on defaults, then directly evaluated on all other variations. Although game variants within curricula are related, the statistically significant interaction effects between factors suggest that they are indeed distinct tasks with substantial and diverse "transfer gaps", necessitating additional learning on top of default game policies. The most significant factors impacting zero-shot transfer were the "difficulty" switch in Space Invaders, the point accumulation "Rules" factor in Breakout, and "Traffic" thickness in Freeway, which we interpret as introducing the broadest differences. It is important to note that such modifications to default games do not easily fit within the more particular sets of assumptions often studied in transfer learning research. It is hard to argue that environment dynamics are precisely the same, or that state spaces are identical in distinct variants. Furthermore, even when observations change in subtle ways, e.g. the "difficulty" switch increases the size of the player's cannon in Space Invaders, this change has a measurable impact on agent transfer performance, and it is not immediately clear how domain adaptation approaches would mitigate the issue. Hence, we go on to investigate how these diverse "transfer gaps" influence the performance of value-function finetuning, a more general transfer learning algorithm.</p>
<p>FINETUNING OF DEFAULT GAME EXPERTS</p>
<p>Zero-shot policy transfer can be substantial, and as such, constitutes an appropriate starting point for fine-tuning on other variants. However, it is not immediately obvious that current RL agents can capitalise on this zero-shot policy transfer to meaningfully speed up learning relative to variant-experts. In order to improve performance on new variants it important that agent representations and predictions are finetuned on data from transfer target tasks. We report results of default game expert finetuning experiments in Figure 4, together with the natural baseline of zero-shot policy transfer from the same experts, and an ablation: learning from scratch for the same amount of environment interaction as used for finetuning. In a majority of cases we find that default expert finetuning outperforms baseline and ablation approaches. Hence, a simple and general algorithm such as Rainbow-IQN finetuning can leverage knowledge acquired in default games to improve performance beyond the zero-shot generalisation offered by its function approximator alone. Note that these gains are significant on the normalised scales induced by variant-expert performance, which account for the different learning dynamics caused by particular game variation, as shown in previous sections.</p>
<p>It is important not to overlook the heterogeneity of these finetuning results within and across curricula. Using a fixed, somewhat generous amount of data to finetune (10 million steps) does allow a substantial amount of performance to be acquired in some cases. Rainbow-IQN agents can also learn to a significant level of performance starting from scratch in this restricted training regime, which we would not have observed without the ablation experiment, e.g. on Freeway variants, but to some extent also on Breakout. Conversely, direct evaluation without training on the transfer target task can confer a competitive level of zero-shot performance, which we necessarily investigated in the previous section.  . Please note that 100% indicates comparable performance with variant-experts, which were trained from scratch with 20× more data than finetuning agents. Black bars show an ablation of finetuning training, starting from scratch rather than from default agent parameters. The delta to is due to transfer learning via agent network parameter initialisation. White bars indicate zero-shot transfer performance of default game agents. The delta to finetuning results represents the net benefit of learning in the transfer target variant. Hatching denotes the "difficulty" switch being on, and colours indicate the "game mode", see Figure 1 for details.</p>
<p>Taken together, we interpret these heterogeneous finetuning results as strong motivation for research on improved general transfer learning algorithms, applicable across many transfer scenarios. But what can help transfer in this very general setting? Intuitively, learning several variants within curricula could offer a solution in the form of transfer using multiple sources, perhaps at the cost of additional complexity.</p>
<p>ZERO-SHOT TRANSFER SOURCE GAME SELECTION</p>
<p>In this section, we focus on knowledge in the form of policies learned by experts on other game variants, and we perform an all-to-all zero-shot transfer evaluation, reported as transfer matrices in Figure 5. We observe substantial off-diagonal structure in all games. The default game is not the best single source task in many cases, especially for variants with many factors introducing changes to the basic game. Naturally, we would like to know how well a superior source task selection strategy could perform on average. More precisely, we would like to empirically characterise the average upper bound on zero-shot transfer possible by appropriate selection of a single source policy for any game variant from all the remaining variant-expert policies. We plot the expectation for success of several transfer source task selection strategies in Figure 6, as well as the "best" case selection strategy, which provides an upper bound for what can be achieved by selecting a single variant-expert's policy. For Freeway we observe strong transfer between policies learned with and without the "difficulty" switch being turned on. Matching the "best" strategy may be costly, e.g. in terms of expert evaluations, so we also report a "top3" selection strategy: zero-shot transfer performance of acting in every episode with one of the "top3" variant-experts at random. This strategy results in over 40% median normalised performance for all curricula. This level of transfer could be achieved at the expense of some target task interactions for ascertaining the quality of all other variant-expert policies on the current game variation. Interestingly, for all game titles, the "top3" strategy is superior to zero-shot transfer from the default games. This cannot be said for the average or "random" choice strategy, which is inferior to zero-shot transfer from default   Figure 6: Box-plots of zero-shot normalised score distributions for source task selection strategies on Space Invaders (left), Breakout (middle) and Freeway variants (right): "default" indicates using the default game expert (0 00) policy to act in all variants; the following selection strategies denote ways of choosing policies exclusively from all other variants in their respective domains: "random" means using a randomly chosen expert policy in every evaluation episode; "top3" and "best" strategies require access to some approximation of the transfer matrix to select the top three or best single source policy for each variant, from all other experts. experts in Breakout. These observations motivate further research into automatic selection of variant-expert policies for transfer. The rich off-diagonal structure also recommends the use of Atari game variations as systematic curricula in a continual learning setting, if only for leveraging the potential for policy reuse highlighted by our evaluations.</p>
<p>CONCLUSIONS</p>
<p>Using Atari curricula, we have demonstrated that general transfer learning approaches, such as zero-shot policy generalisation and single expert model finetuning, lead to significant performance improvements in a large number of diverse transfer learning scenarios that are non-trivial for human players. Our analyses indicate that introducing modifications to basic games in a combinatorial fashion can substantially impact the empirical learning efficiency of a deep RL agent. Furthermore, zero-shot transfer performance from default games alone is also hindered by interactions between modifications. These results highlight the value of learning several variant-experts and reusing their knowledge to quickly find effective policies for new variations. Such strategies have the potential to efficiently match expert performance on transfer target tasks, but require the ability to both accumulate knowledge across game variants, and to appropriately transfer from several source tasks at once. Our study serves as a proof-of-concept that Atari curricula offer an invaluable benchmark of suitable complexity for systematically probing transfer without task engineering. A APPENDIX: EXTENDED BACKGROUND Q-learning. Watkins (1989) propose an RL algorithm which iteratively improves a parametric estimatê Q θ =Q M (·, ·, θ) of the optimal action-value function Q * M using the Bellman optimality operator (Bellman, 1957):
Q θ (x, a) ← E T [R(x, a, x )] + γE T max a ∈AQ θ (x , a ) .
In model-free RL we do not assume explicit knowledge of the transition function T , but the above optimisation can be implemented using samples procured by acting greedily with respect to the estimateQ θ with probability 1 − , and choosing actions uniformly at random otherwise. Watkins &amp; Dayan (1992) show that, under certain condition, this procedure converges to Q * M , and an optimal policy can be derived as π * M (x) = arg max It is important to note that such results are not yet achievable without access to super-computing infrastructure, e.g. thousands of GPUs or TPUs, and complex software stacks for distributed training, with estimated costs in single digit millions of dollars (OpenAI, 2018). Even when such resources are available, agent training can still take many months, so detailed ablations or hyper-parameter tuning become prohibitively expensive (Berner et al., 2019). Hence, improving data and computational efficiency through principled algorithmic innovations is essential for state-of-the-art deep RL.</p>
<p>B APPENDIX: AGENT TRAINING DETAILS</p>
<p>We tuned relevant hyper-parameters for each game variant in order to avoid sub-optimal performance due to poor settings. Please note that we are the first to report Rainbow-IQN results on Atari variants, and hence we had only the DQN experiments of Farebrother et al. (2018) as a guide, but no results were reported for the standard 200 million steps training regime, and the two algorithms have many differences which can impact learning dynamics. Farebrother et al. (2018) also used weight decay and dropout to reduce parameter norms and improve the generalisation of trained agents, but final performance was negatively impacted. We share the intuition that models which are closer to a random initialisation have an increased chance to generalise to related tasks, and may even be easier to adapt to new variants during finetuning.  Please not that the aforementioned optimisers can provide superior performance, but tend to optimistically increase learning rates when parameter updates are correlated. We used hyper-parameter grid search to choose appropriate learning rates, and we purposefully also including smaller than typical values in the grid; we also allowed larger batch sizes and less frequent parameter updates besides standard values, see Table 3 for details. To further avoid parameter norms increasing due to correlated updates at the start of training, we sampled 500 000 transitions in the replay memory before starting learning, using decay from fully random behaviour to an -greedy policy with = 0.01. The intuition behind these choices is that sampling data using a diverse set of behaviours policies will reduce the chances of correlated updates and learning divergence early on in training.</p>
<p>We used a slightly modified hyper-parameter grid for expert finetuning, primarily aimed at finding appropriate learning rates and the optimal levels of gradient clipping, as shown in Table 4. Expert finetuning is performed using a total of 10 million environment steps, which is 20× less experience and parameter updates compared to expert training from scratch. Hence, we allowed for more lenient gradient clipping, as well as disabling it altogether.</p>
<p>Our choice to use hyper-parameter grid searches for both expert training and finetuning has greatly increased the computational costs of this study. We have somewhat offset these costs by reducing the numbers of filters in the convolutional trunk of the agent network, to the same sizes used by Mnih et al.  Optimiser Clipped-SGD Re-scaling gradients with norms larger than C.  We used generalised linear models, fit with robust covariance estimates ('HC3'), to perform multi-factor Analyses of Variance (ANOVA). After rejecting null hypotheses of the omnibus tests, we used Bonferroni corrections for multiple comparisons to perform post-hoc analyses of differences between group means. The desired significance level was α = 0.05, which we corrected to α c = 0.0015 for Space Invaders, α c = 0.0020, for Breakout, α c = 0.0031 for Freeway, since the numbers of groups are 32, 24 and 16 respectively.  1. The observations in each group are independent of the observations in every other group. We trained agents independently of each other in all cases, and we selected hyper-parameters for each group (i.e. game variation) independently of others.</p>
<p>Gradient Clipping Norm</p>
<ol>
<li>
<p>Normality of measurement errors. For a give game, if we were to re-run the hyper-parameter grid search many times, choose the top 3 agents and average their performance, we assume that such measurements would be normally distributed. We have no reasons to believe that the distribution of the top 3 would be affected by divergence of online learning, one of the likely failure cases. We investigated empirical distribution of residuals from our analyses in Figure 7. We observe some deviation from normality, hence it is best to only draw conclusions based on the strongest significant effects; note that deviations from normality are relevant when effect sizes are very small or when the experimental design is not balanced (Glass &amp; Hopkins, 1996); in our case we use 3 samples per combination of factors (game variation), which is a balanced design.</p>
</li>
<li>
<p>Similar group variances of measurements. We used heteroskedasticity robust variance calculations of type 'HC3' (Davidson et al., 1993) to account for observed differences in variance, which we believe are primarily due to sample size.</p>
</li>
</ol>
<p>D APPENDIX: COMPLETE RESULTS TABLES</p>
<p>In Table 8, Table 9 and Table 10 we provide raw and normalised scores for experts. In Table 11, Table 12, Table 13 we list raw and normalised scores of variant-expert zero-shot transfer.          </p>
<p>[Figure 1 :
1<strong><em>]-0</em>00 [</strong>K]-0_01 [<strong>R]-0_02 [__T]-0_03 [<em>S</em>]-0_04 [_SK]-0_05 [_SR]-0_06 [_ST]-0_07 [D</strong>]-1_00 [D_K]-1_01 [D_R]-1_02 [D_T]-1_03 [DS_]-1_04 [DSK]-1_05 [DSR]-1_06 [DST]Variant naming convention and factorial design matrices for Space Invaders (top), Breakout (bottom left) and</p>
<p>Figure 2 :
2Variant-Expert final raw score distributions, means and standard deviations across the top three experts in their respective hyper-parameter grids, reported for Space Invaders (top), Breakout (bottom left) and Freeway variants (bottom right). In all plots, hatching denotes the "difficulty" switch being activated, and colours indicate the "game mode". See factorial design matrices for meanings of colour groups(Figure 1).</p>
<p>Figure 3 :
3Zero-shot transfer performance of default game experts (0 00) on all other variants, plotted as normalised scores (percentages) of the final performance of respective variant-experts, reported for Space Invaders (top), Breakout (bottom left) and Freeway variants (bottom right). Hatching denotes the "difficulty" switch being activated, and colours indicate the "game mode". Colour groups are meaningful with respect to the factorial design matrices of particular game variants(Figure 1).</p>
<p>Figure 4 :
4Finetuning performance starting from default game expert (0 00) on all other variants, plotted as normalised scores (percentages) of the final performance of respective variant-experts, reported for Space Invaders (top), Breakout (bottom left) and Freeway variants (bottom right)</p>
<p>Figure 5 :
5Transfer matrices depicting zero-shot evaluation of Space Invaders (left), Breakout (middle) and Freeway variant-experts (right), plotted as columns, and evaluated on all variants (rows). Values reported as normalised scores (percentages) of respective variant-expert scores.</p>
<p>Deep Q-Learning.Mnih et al. (2015)  use a convolutional neural network to parameteriseQ θ , and train the resulting Deep Q-Network (DQN) by iterative minimisation of the squared temporal difference (TD) error:Samples (x t , a t , r t , x t+1 ) are drawn randomly from a moving-window replay memory, where experience is collected online, and θ − denotes a slow moving copy of online parameters θ. DQN agents are efficiently trained using RMSProp(Tieleman &amp; Hinton, 2012), a variant of mini-batch stochastic gradient descent (SGD). This approach, with fixed hyper-parameters, was the first to master many classic Atari 2600 games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013) up to a casual human level of performance, using screen pixels as inputs. Several improvements are brought together into the Rainbow agent byHessel et al. (2018).Dabney et al. (2018)  introduce Implicit Quantile Networks (IQN), a principled generalisation of DQN such that the full return distribution Z π M is modelled implicitly through its quantile function, leading to superior performance. Rainbow-IQN was shown to be state-of-the-art at the time for the standard (200 million steps) data regime byToromanoff et al. (2019). This family of algorithms remains the most widely implemented of deep RL approaches.Deep Reinforcement Learning. Online model-free RL with deep neural networks has been known to suffer from learning instability due to correlations between parameter updates. Stabilisation heuristics proved successful early on,such as off-line, full-batch training (Riedmiller, 2005; Schulman et al., 2015), using a replay memory to decorrelate parameter updates (Mnih et al., 2013; Schaul et al., 2016), and correcting over-estimation errors (Van Hasselt et al., 2016). Mnih et al. (2016) introduced A3C, an online deep RL algorithm which leveraged parallel data generation for stable learning, further improved with auxiliary losses (Jaderberg et al., 2016), and a replay memory(Wang et al., 2016) for better sample efficiency. Deep RL has also been successfully applied to continuous control domains(Lillicrap et al.,  2015; Schulman et al., 2015;2017; Haarnoja et al., 2018). Model-based RL algorithms have proven successful in full information environments which require complex strategy, such as the game ofGo (Silver et al., 2016;2017), several board-games(Schrittwieser et al., 2020)  and domains with limited data, such as robotic control(Levine &amp; Koltun,  2013; Levine et al., 2016). Recently, model-based RL approaches with learned models have proven competitive with model-free algorithms on domains with high-dimensional observations, e.g. DreamerV2(Hafner et al., 2020), and  MuZero (Schrittwieser et al., 2020). More complex training strategies have also been successful, such as combining deep RL with supervised learning from human games(Silver et al., 2016), and using self-play, or multi-agent training, leading to human Grandmaster level performance in Starcraft II(Vinyals et al., 2019), and winning against world champions at an e-sports game(Berner et al., 2019).</p>
<p>{10 − 6 ,
610 −5 , 10 −4 , 10 −3 } Hessel et al. (2018). Target Network Update Period {1000, 10 000} Agent steps (Castro et al., 2018). Number of τ Samples N = 64 Dabney et al. (2018). Number of τ Samples N = 64 Dabney et al. (2018). Number ofτ Samples (Policy) K = 64 Dabney et al. (2018). We replaced RMSProp (Hinton et al., 2006), used by DQN (Mnih et al., 2015), and Adam (Kingma &amp; Ba, 2015), used by Rainbow (Hessel et al., 2018), with Stochastic Gradient Descent (SGD) and gradient norm clipping (Pascanu et al., 2012).</p>
<p>(2013), but we kept the number of convolutional layers at 3 (Mnih et al., 2015; Dabney et al., 2018). Other network parameters were set according to Dabney et al. (2018), see architecture details inTable 5. These modifications did not significantly impact agent performance, which is largely comparable to results reported by Castro et al. (2018) on default games.</p>
<p>C ∈ {0, 10, 40} Largest admissible gradient norm. Setting C = 0 disables gradient clipping. Agent Steps per Update 4 Agent steps (Mnih et al., 2015). Learning Rate {10 −7 , 10 −6 , 5 × 10 −6 , 10 −5 , 5 × 10 −5 , 10 −4 } Hessel et al. (2018). Target Network Update Period 10 000 Agent steps (Castro et al., 2018). Number of τ Samples N = 64 Dabney et al. (2018). Number of τ Samples N = 64 Dabney et al. (2018). Number ofτ Samples (Policy) K = 64 Dabney et al. (2018).</p>
<p>Figure 7 :
7Empirical distribution plots of residuals from Multi-Factor ANOVA of Expert Scores vs. design factors. C.1 ASSUMPTIONS:</p>
<p>Expert Agent Training. We use the Rainbow-IQN model-free deep reinforcement learning algorithm(Hessel et al.,  2018; Dabney et al., 2018;Toromanoff et al., 2019) since it is available to the community in several implementations, e.g.Castro et al. (2018), and is effective with widely available commodity hardware and open-source software.Rainbow (Hessel et al., 2018) collects a number of improvements to DQN (Mnih et al., 2013; 2015), of which we used 
Double Q-Learning (Van Hasselt et al., 2016), Prioritised Replay (Schaul et al., 2016) and multi-step learning (Sutton 
&amp; Barto, 2018). Following Castro et al. (2018), we did not use the dueling network architecture </p>
<p>Table 1 :
1Multi-Factor ANOVA (type 3) of Expert Scores vs. design factors. statistically significant after appropriate Bonferroni correction. b statistically significant interaction effect at level: α = 0.05.Space Invaders 
df 
F 
P R(&gt; F ) 
Difficulty 
1.0 
115.90 5.32e − 16 a 
Invisible Invaders 
1.0 1000.59 8.67e − 41 a 
Fast Bombs 
1.0 
314.09 2.24e − 26 a 
Zigzagging Bombs 1.0 
12.08 
9.22e − 04 a 
Moving Shields 
1.0 
125.51 9.93e − 17 a 
Interaction 
26.0 1341.37 5.33e − 78 b </p>
<p>Breakout 
df 
F 
P R(&gt; F ) 
Difficulty 
1 
56.28 
1.26e − 09 a 
Rules 
2 1368.66 4.71e − 43 a 
Extras 
3 
179.84 4.22e − 26 a 
Interaction 17 
23.04 
1.91e − 17 b </p>
<p>Freeway 
df 
F 
P R(&gt; F ) 
Difficulty 
1 
1.39 
2.46e − 01 
Speeds 
1 
91.38 6.75e − 11 a 
Traffic 
3 453.84 2.73e − 26 a 
Interaction 10 274.76 4.42e − 28 b </p>
<p>a </p>
<p>Table 2 :
2Multi-Factor ANOVA (type 3) of default game expert zero-shot transfer vs. design factors. statistically significant after appropriate Bonferroni correction. b statistically significant interaction effect at level: α = 0.05.Space Invaders 
df 
F 
P R(&gt; F ) 
Difficulty 
1 319.42 1.43e − 26 a 
Invisible Invaders 
1 208.13 8.68e − 22 a 
Fast Bombs 
1 250.95 7.92e − 24 a 
Zigzagging Bombs 1 
6.51 
1.32e − 02 
Moving Shields 
1 116.63 4.67e − 16 a 
Interaction 
26 117.61 1.57e − 44 b </p>
<p>Breakout 
df 
F 
P R(&gt; F ) 
Difficulty 
1 182.22 5.83e − 18 a 
Rules 
2 476.93 2.14e − 32 a 
Extras 
3 177.63 5.53e − 26 a 
Interaction 17 31.38 2.87e − 20 b </p>
<p>Freeway 
df 
F 
P R(&gt; F ) 
Difficulty 
1 
15.87 
3.66e − 04 a 
Speeds 
1 
211.61 1.17e − 15 a 
Traffic 
3 1524.22 1.36e − 34 a 
Interaction 10 100.01 3.14e − 21 b </p>
<p>a </p>
<p>Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016b. URL http: //arxiv.org/abs/1606.04671. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1889-1897, Lille, France, 07-09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/schulman15.html. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning. COURSERA Neural Networks Mach. Learn, 2012.Andrei A. Rusu, Matej Večerík, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real 
robot learning from pixels with progressive nets. In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg (eds.), 
Proceedings of the 1st Annual Conference on Robot Learning, volume 78 of Proceedings of Machine Learn-
ing Research, pp. 262-270. PMLR, 13-15 Nov 2017. URL https://proceedings.mlr.press/v78/ 
rusu17a.html. </p>
<p>Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In Francis 
Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of 
Proceedings of Machine Learning Research, pp. 1312-1320, Lille, France, 07-09 Jul 2015. PMLR. URL https: 
//proceedings.mlr.press/v37/schaul15.html. </p>
<p>Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In ICLR (Poster), 2016. </p>
<p>Tom Schaul, Hado van Hasselt, Joseph Modayil, Martha White, Adam White, Pierre-Luc Bacon, Jean Harb, Shibl 
Mourad, Marc G. Bellemare, and Doina Precup. The barbados 2018 list of open issues in continual learning. CoRR, 
abs/1811.07004, 2018. URL http://arxiv.org/abs/1811.07004. </p>
<p>Simon Schmitt, Jonathan J Hudson, Augustin Zidek, Simon Osindero, Carl Doersch, Wojciech M Czarnecki, Joel Z 
Leibo, Heinrich Kuttler, Andrew Zisserman, Karen Simonyan, et al. Kickstarting deep reinforcement learning. 
arXiv preprint arXiv:1803.03835, 2018. </p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization 
algorithms. arXiv preprint arXiv:1707.06347, 2017. </p>
<p>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrit-
twieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural 
networks and tree search. nature, 529(7587):484-489, 2016. </p>
<p>nature, 550 
(7676):354-359, 2017. </p>
<p>David Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artificial Intelligence, 
299:103535, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2021.103535. URL https://www. 
sciencedirect.com/science/article/pii/S0004370221000862. </p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. </p>
<p>Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of 
Machine Learning Research, 10(7), 2009. </p>
<p>Sebastian Thrun and Lorien Pratt. Learning to Learn: Introduction and Overview, pp. 3-17. Springer US, Boston, 
MA, 1998. ISBN 978-1-4615-5529-2. doi: 10.1007/978-1-4615-5529-2 1. URL https://doi.org/10. 
1007/978-1-4615-5529-2_1. </p>
<p>Joshua Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization 
for transferring deep neural networks from simulation to the real world. CoRR, abs/1703.06907, 2017. URL 
http://arxiv.org/abs/1703.06907. </p>
<p>Table 3 :
3Variant-Expert Rainbow-IQN hyper-parameters. When multiple values are specified, the final hyperparameter was chosen via grid search. Note that each configuration was trained using two different random seeds.Name 
Value(s) 
Units/Comments/Attributions </p>
<p>Reward Clipping Bound 
1 
Possible values {−1, 0, 1} (Mnih et al., 
2015). 
Replay Capacity 
1 000 000 
Transitions. (Mnih et al., 2015). 
Replay Initial Size 
500 000 
Transitions. (Hessel et al., 2018). </p>
<p>Initial Policy 
= 1 
Replay memory initialised using random 
policy. </p>
<p>Initial Policy Annealing Steps 
100 000 
Linearly annealed to = 0.01 (Mnih 
et al., 2015). 
Behaviour Policy ( -greedy) 
= 0.01 
The same during training and evaluation. 
Action Repeats 
4 
Environment steps. (Mnih et al., 2015). 
History Length 
4 
Agent steps. (Mnih et al., 2015). 
Discount Factor 
γ = 0.99 
Mnih et al. (2015). 
Multi-step Bootstrap 
n = 3 
Hessel et al. (2018). 
Double Q-Learning 
True 
Van Hasselt et al. (2016). 
Prioritised Replay 
True 
Schaul et al. (2016); Hessel et al. (2018). 
Prioritisation Exponent 
0.6 
Horgan et al. (2018). 
Prioritisation Type 
proportional 
Hessel et al. (2018). 
Prioritisation Importance 
Sampling 
0.4 
Horgan et al. (2018). </p>
<p>Parallel Environments 
1 
Toromanoff et al. (2019). 
Batch size 
{32, 64} 
Mnih et al. (2015) </p>
<p>Optimiser 
Clipped-SGD 
Re-scaling gradients with norms larger 
than C. 
Gradient Clipping Norm 
C = 10 
Largest admissible gradient norm. 
Agent Steps per Update 
{4, 8} 
Agent steps (Mnih et al., 2015). 
Learning Rate </p>
<p>Table 4 :
4Rainbow-IQN expert finetuning hyper-parameters. When multiple values are specified, the final hyperparameter was chosen via grid search. Note that each configuration was trained using two different random seeds.Name 
Value(s) 
Units/Comments/Attributions </p>
<p>Reward Clipping Bound 
1 
Possible values {−1, 0, 1} (Mnih et al., 
2015). 
Replay Capacity 
1 000 000 
Transitions. (Mnih et al., 2015). 
Replay Initial Size 
500 000 
Transitions. (Hessel et al., 2018). </p>
<p>Initial Policy 
= 1 
Replay memory initialised using random 
policy. </p>
<p>Initial Policy Annealing 
100 000 
Linearly annealed to = 0.01 (Mnih 
et al., 2015). 
Behaviour Policy ( -greedy) 
= 0.01 
The same during training and evaluation. 
Action Repeats 
4 
Environment steps. (Mnih et al., 2015). 
History Length 
4 
Agent steps. (Mnih et al., 2015). 
Discount Factor 
γ = 0.99 
Mnih et al. (2015). 
Multi-step Bootstrap 
n = 3 
Hessel et al. (2018). 
Double Q-Learning 
True 
Van Hasselt et al. (2016). 
Prioritised Replay 
True 
Schaul et al. (2016); Hessel et al. (2018). 
Prioritisation Exponent 
0.6 
Horgan et al. (2018). 
Prioritisation Type 
proportional 
Hessel et al. (2018). 
Prioritisation Importance 
Sampling 
0.4 
Horgan et al. (2018). </p>
<p>Parallel Environments 
1 
Toromanoff et al. (2019). 
Batch size 
32 
Mnih et al. (2015) </p>
<p>Table 5 :
5Rainbow-IQN Network Architecture.Name 
Value(s) 
Convolution Channels 
16, 32, 32 
Convolution Filter Sizes 8 × 8, 4 × 4, 3 × 3 
Convolution Filter Stride 4 × 4, 2 × 2, 1 × 1 
Convolution Activations ReLU, ReLU, Identity 
Embedding Hiddens 
1568 
Embedding Activations 
ReLU 
Latent Dimension 
64 
Combined Hiddens 
256, 18 
Combined Activations 
ReLU, Identity </p>
<p>C APPENDIX: STATISTICAL ANALYSES </p>
<p>Table 6 :
6Multi-Factor ANOVA (type 3) of Expert Scores vs. design factors, extended tables.Space Invaders 
sum sq 
df 
F 
P R(&gt; F ) 
Intercept 
8.18e + 07 1 
10862.43 3.67e − 73 
Difficulty 
8.73e + 05 1 
115.90 
5.32e − 16 
Invisible Invaders 
7.54e + 06 1 
1000.59 
8.67e − 41 
Fast Bombs 
2.37e + 06 1 
314.09 
2.24e − 26 
Zigzagging Bombs 9.10e + 04 1 
12.08 
9.22e − 04 
Moving Shields 
9.46e + 05 1 
125.51 
9.93e − 17 
Interaction 
2.63e + 08 26 
1341.37 
5.33e − 78 
Residual 
4.82e + 05 64 </p>
<p>Breakout 
sum sq 
df 
F 
P R(&gt; F ) 
Intercept 
4.06e + 06 1 
7623.13 
1.49e − 54 
Difficulty 
2.99e + 04 1 
56.28 
1.26e − 09 
Rules 
1.46e + 06 2 
1368.66 
4.71e − 43 
Extras 
2.87e + 05 3 
179.84 
4.22e − 26 
Interaction 
2.08e + 05 17 
23.04 
1.91e − 17 
Residual 
2.55e + 04 48 </p>
<p>Freeway 
sum sq 
df 
F 
P R(&gt; F ) 
Intercept 
27174.26 
1 134697.26 1.44e − 59 
Difficulty 
0.28 
1 
1.39 
2.46e − 01 
Traffic 
274.67 
3 
453.84 
2.73e − 26 
Speeds 
18.44 
1 
91.38 
6.75e − 11 
Interaction 
554.30 
10 
274.76 
4.42e − 28 
Residual 
6.46 
32 </p>
<p>Table 7 :
7Multi-Factor ANOVA (type 3) of Default game expert zero-shot transfer vs. design factors, extended tables.Space Invaders 
sum sq 
df 
F 
P R(&gt; F ) 
Intercept 
3.33e + 06 1 
638.21 5.35e − 35 
Difficulty 
1.66e + 06 1 
319.42 1.43e − 26 
Invisible Invaders 
1.08e + 06 1 
208.13 8.68e − 22 
Fast Bombs 
1.31e + 06 1 
250.95 7.92e − 24 
Zigzagging Bombs 3.39e + 04 1 
6.51 
1.32e − 02 
Moving Shields 
6.08e + 05 1 
116.63 4.67e − 16 
Interaction 
1.59e + 07 26 117.61 1.57e − 44 
Residual 
3.33e + 05 64 </p>
<p>Breakout 
sum sq 
df 
F 
P R(&gt; F ) 
Intercept 
1.36e + 06 1 3436.57 2.51e − 46 
Difficulty 
7.23e + 04 1 
182.22 5.83e − 18 
Rules 
3.78e + 05 2 
476.93 2.14e − 32 
Extras 
2.11e + 05 3 
177.63 5.53e − 26 
Interaction 
2.12e + 05 17 
31.38 
2.87e − 20 
Residual 
1.90e + 04 48 </p>
<p>Freeway 
sum sq 
df 
F 
P R(&gt; F ) 
Intercept 
1385.27 
1 2775.16 1.14e − 32 
Difficulty 
7.92 
1 
15.87 
3.66e − 04 
Traffic 
2282.53 
3 1524.22 1.36e − 34 
Speeds 
105.63 
1 
211.61 1.17e − 15 
Interaction 
499.23 
10 100.01 3.14e − 21 
Residual 
15.97 
32 </p>
<p>Table 8 :
8Space Invaders raw and variant-expert normalisation scores in various transfer learning settings and their 
ablations. 
Raw Scores 
Variant-Expert Normalised Scores </p>
<p>Space Invaders 
(Difficulty &amp; 
Game Mod) </p>
<p>Variant-
Expert 
(100%) </p>
<p>Finetuning 
from 
Scratch </p>
<p>Zero-shot 
Expert 
(0 00) </p>
<p>Finetuned 
Expert 
(0 00) </p>
<p>Finetuning 
from 
Scratch 
(%) </p>
<p>Zero-shot 
Expert 
(0 00) 
(%) </p>
<p>Finetuned 
Expert 
(0 00) 
(%) 
0 00 
2167.74 
326.68 
2167.78 
33090.00 
15.07 
100.00 
1526.48 
0 01 
1656.22 
244.79 
427.31 
8949.71 
14.78 
25.80 
540.37 
0 02 
2250.00 
275.85 
1243.42 
2820.79 
12.26 
55.26 
125.37 
0 03 
2000.00 
235.40 
412.60 
2465.37 
11.77 
20.63 
123.27 
0 04 
1783.01 
157.97 
260.19 
1581.55 
8.86 
14.59 
88.70 
0 05 
1170.38 
121.72 
89.57 
634.09 
10.40 
7.65 
54.18 
0 06 
1354.09 
110.09 
280.22 
1017.78 
8.13 
20.69 
75.16 
0 07 
1025.23 
128.67 
136.02 
593.75 
12.55 
13.27 
57.91 
0 08 
838.10 
350.32 
278.12 
669.79 
41.80 
33.18 
79.92 
0 09 
875.00 
274.31 
147.58 
597.76 
31.35 
16.87 
68.31 
0 10 
851.50 
307.05 
237.02 
601.44 
36.06 
27.84 
70.63 
0 11 
821.58 
198.41 
178.30 
540.59 
24.15 
21.70 
65.80 
0 12 
921.15 
175.94 
125.16 
559.70 
19.10 
13.59 
60.76 
0 13 
830.55 
179.48 
61.88 
443.98 
21.61 
7.45 
53.46 
0 14 
719.11 
144.40 
117.15 
512.50 
20.08 
16.29 
71.27 
0 15 
697.57 
118.31 
64.77 
357.77 
16.96 
9.28 
51.29 
1 00 
1938.98 
315.28 
105.91 
1499.32 
16.26 
5.46 
77.33 
1 01 
1207.91 
226.72 
131.86 
623.19 
18.77 
10.92 
51.59 
1 02 
1893.35 
229.66 
174.02 
1244.93 
12.13 
9.19 
65.75 
1 03 
1250.00 
183.12 
164.62 
604.48 
14.65 
13.17 
48.36 
1 04 
1250.00 
145.50 
17.18 
576.14 
11.64 
1.37 
46.09 
1 05 
929.55 
68.88 
40.73 
385.40 
7.41 
4.38 
41.46 
1 06 
993.20 
88.99 
20.10 
524.81 
8.96 
2.02 
52.84 
1 07 
713.80 
135.76 
58.13 
342.39 
19.02 
8.14 
47.97 
1 08 
857.34 
352.02 
65.88 
648.87 
41.06 
7.68 
75.68 
1 09 
886.21 
186.99 
46.95 
530.19 
21.10 
5.30 
59.83 
1 10 
845.13 
304.50 
64.59 
591.04 
36.03 
7.64 
69.93 
1 11 
829.79 
283.46 
87.92 
485.00 
34.16 
10.60 
58.45 
1 12 
729.67 
82.16 
24.88 
476.72 
11.26 
3.41 
65.33 
1 13 
653.80 
141.09 
24.23 
203.37 
21.58 
3.71 
31.11 
1 14 
644.76 
77.95 
24.39 
413.87 
12.09 
3.78 
64.19 
1 15 
600.00 
60.90 
15.48 
204.83 
10.15 
2.58 
34.14 </p>
<p>Table 9 :
9Breakout raw and variant-expert normalisation scores in various transfer learning settings and their ablations.Raw Scores 
Variant-Expert Normalised Scores 
Breakout 
(Difficulty 
&amp; Game 
Mode) </p>
<p>Variant-
Expert 
(100%) </p>
<p>Finetuning 
from 
Scratch </p>
<p>Zero-shot 
Expert 
(0 00) </p>
<p>Finetuned 
Expert 
(0 00) </p>
<p>Finetuning 
from 
Scratch 
(%) </p>
<p>Zero-shot 
Expert 
(0 00) 
(%) </p>
<p>Finetuned 
Expert 
(0 00) 
(%) 
0 00 
264.60 
6.69 
264.60 
342.21 
2.53 
100.00 
129.33 
0 01 
420.10 
5.25 
212.41 
412.43 
1.25 
50.56 
98.17 
0 02 
298.60 
6.51 
201.41 
325.49 
2.18 
67.45 
109.01 
0 03 
247.00 
6.05 
22.02 
69.99 
2.45 
8.92 
28.33 
0 04 
71.90 
82.43 
116.85 
111.95 
114.65 
162.51 
155.70 
0 05 
55.50 
70.97 
106.79 
101.32 
127.87 
192.42 
182.56 
0 06 
255.40 
72.79 
253.50 
320.40 
28.50 
99.26 
125.45 
0 07 
82.40 
75.70 
50.19 
90.10 
91.87 
60.91 
109.35 
0 08 
386.30 
202.85 
316.54 
385.42 
52.51 
81.94 
99.77 
0 09 
670.10 
251.56 
387.64 
635.95 
37.54 
57.85 
94.90 
0 10 
650.20 
170.03 
357.95 
591.00 
26.15 
55.05 
90.90 
0 11 
385.60 
203.56 
197.61 
356.12 
52.79 
51.25 
92.35 
1 00 
155.80 
14.41 
86.12 
198.12 
9.25 
55.28 
127.16 
1 01 
398.00 
6.65 
50.89 
364.01 
1.67 
12.79 
91.46 
1 02 
335.60 
9.46 
59.29 
223.65 
2.82 
17.67 
66.64 
1 03 
166.30 
14.62 
15.19 
40.97 
8.79 
9.14 
24.64 
1 04 
55.50 
75.97 
62.92 
81.70 
136.89 
113.36 
147.20 
1 05 
51.40 
69.62 
63.44 
80.50 
135.44 
123.43 
156.61 
1 06 
188.10 
92.15 
110.40 
434.00 
48.99 
58.69 
230.73 
1 07 
63.20 
74.81 
35.28 
67.71 
118.37 
55.82 
107.13 
1 08 
353.20 
133.44 
260.52 
362.86 
37.78 
73.76 
102.74 
1 09 
542.40 
217.99 
299.91 
450.62 
40.19 
55.29 
83.08 
1 10 
516.90 
182.10 
293.05 
455.30 
35.23 
56.69 
88.08 
1 11 
361.90 
126.34 
219.33 
343.84 
34.91 
60.61 
95.01 </p>
<p>Table 10 :
10Freeway raw and variant-expert normalisation scores in various transfer learning settings and their ablations.Raw Scores 
Variant-Expert Normalised Scores 
Freeway 
(Difficulty 
&amp; Game 
Mode) </p>
<p>Variant-
Expert 
(100%) </p>
<p>Finetuning 
from 
Scratch </p>
<p>Zero-shot 
Expert 
(0 00) </p>
<p>Finetuned 
Expert 
(0 00) </p>
<p>Finetuning 
from 
Scratch 
(%) </p>
<p>Zero-shot 
Expert 
(0 00) 
(%) </p>
<p>Finetuned 
Expert 
(0 00) 
(%) 
0 00 
33.10 
23.31 
33.14 
33.43 
70.43 
100.12 
100.99 
0 01 
25.83 
5.94 
0.37 
23.55 
22.99 
1.43 
91.18 
0 02 
27.13 
4.98 
0.10 
11.14 
18.35 
0.38 
41.07 
0 03 
32.97 
11.06 
0.24 
24.90 
33.55 
0.73 
75.52 
0 04 
33.30 
21.86 
18.30 
24.55 
65.64 
54.94 
73.73 
0 05 
25.65 
7.96 
1.74 
9.94 
31.03 
6.80 
38.74 
0 06 
25.66 
7.98 
1.53 
10.10 
31.10 
5.98 
39.38 
0 07 
28.74 
13.23 
1.19 
16.49 
46.03 
4.13 
57.38 
1 00 
33.08 
16.94 
33.06 
33.31 
51.20 
99.93 
100.68 
1 01 
26.87 
5.31 
0.23 
23.86 
19.75 
0.86 
88.79 
1 02 
27.34 
0.74 
0.00 
0.00 
2.69 
0.01 
0.00 
1 03 
32.88 
8.16 
0.01 
23.86 
24.83 
0.02 
72.56 
1 04 
33.39 
17.98 
13.16 
23.37 
53.85 
39.41 
69.98 
1 05 
25.85 
4.92 
1.01 
7.94 
19.03 
3.90 
30.71 
1 06 
25.66 
4.67 
0.74 
7.06 
18.21 
2.89 
27.52 
1 07 
28.81 
7.67 
0.42 
14.24 
26.63 
1.48 
49.44 </p>
<p>Table 11 :
11Space Invaders zero-shot transfer matrix between variant-experts trained from scratch. Top: raw scores, Bottom: Variant-Expert normalised scores (percentages).</p>
<p>Table 12 :
12Breakout zero-shot transfer matrix between variant-experts trained from scratch. Top: raw scores, Bottom: Variant-Expert normalised scores (percentages).0 00 </p>
<p>0 04 </p>
<p>0 08 </p>
<p>0 12 </p>
<p>0 16 </p>
<p>0 20 </p>
<p>0 24 </p>
<p>0 28 </p>
<p>0 32 </p>
<p>0 36 </p>
<p>0 40 </p>
<p>0 44 </p>
<p>1 00 </p>
<p>1 04 </p>
<p>1 08 </p>
<p>1 12 </p>
<p>1 16 </p>
<p>1 20 </p>
<p>1 24 </p>
<p>1 28 </p>
<p>1 32 </p>
<p>1 36 </p>
<p>1 40 </p>
<p>1 44 </p>
<p>0 00 264.60 </p>
<p>49.83 </p>
<p>197.20 </p>
<p>3.44 </p>
<p>19.86 </p>
<p>10.99 </p>
<p>9.35 </p>
<p>2.05 </p>
<p>6.76 </p>
<p>12.96 </p>
<p>11.34 </p>
<p>3.50 </p>
<p>84.07 </p>
<p>15.98 </p>
<p>28.55 </p>
<p>2.57 </p>
<p>15.94 </p>
<p>18.76 </p>
<p>9.80 </p>
<p>2.60 </p>
<p>7.24 </p>
<p>13.97 </p>
<p>9.04 </p>
<p>3.35 </p>
<p>0 04 212.41 420.14 149.08 </p>
<p>3.63 </p>
<p>17.27 </p>
<p>15.32 </p>
<p>9.43 </p>
<p>1.95 </p>
<p>6.60 </p>
<p>13.98 </p>
<p>10.87 </p>
<p>3.55 </p>
<p>60.81 </p>
<p>336.39 </p>
<p>17.65 </p>
<p>2.68 </p>
<p>16.39 </p>
<p>22.88 </p>
<p>8.37 </p>
<p>2.62 </p>
<p>7.07 </p>
<p>15.64 </p>
<p>9.15 </p>
<p>3.11 </p>
<p>0 08 201.41 </p>
<p>51.59 </p>
<p>298.62 </p>
<p>4.14 </p>
<p>12.31 </p>
<p>10.61 </p>
<p>59.04 </p>
<p>1.87 </p>
<p>6.88 </p>
<p>13.97 </p>
<p>11.50 </p>
<p>3.94 </p>
<p>64.53 </p>
<p>20.51 </p>
<p>68.71 </p>
<p>2.61 </p>
<p>14.97 </p>
<p>14.14 </p>
<p>13.98 </p>
<p>2.46 </p>
<p>8.18 </p>
<p>14.64 </p>
<p>8.31 </p>
<p>3.54 </p>
<p>0 12 </p>
<p>22.03 </p>
<p>21.62 </p>
<p>29.89 </p>
<p>247.02 </p>
<p>9.86 </p>
<p>5.08 </p>
<p>8.11 </p>
<p>27.23 </p>
<p>5.36 </p>
<p>23.52 </p>
<p>10.99 </p>
<p>4.92 </p>
<p>16.91 </p>
<p>10.94 </p>
<p>14.44 </p>
<p>83.12 </p>
<p>8.24 </p>
<p>10.70 </p>
<p>4.47 </p>
<p>12.90 </p>
<p>5.96 </p>
<p>12.95 </p>
<p>11.85 </p>
<p>7.21 </p>
<p>0 16 116.85 </p>
<p>63.87 </p>
<p>100.90 </p>
<p>18.93 </p>
<p>71.87 </p>
<p>51.25 </p>
<p>54.56 </p>
<p>12.34 </p>
<p>22.30 </p>
<p>32.53 </p>
<p>30.54 </p>
<p>19.06 </p>
<p>70.59 </p>
<p>35.67 </p>
<p>41.34 </p>
<p>39.90 </p>
<p>54.43 </p>
<p>68.25 </p>
<p>55.42 </p>
<p>13.13 </p>
<p>23.40 </p>
<p>34.65 </p>
<p>26.62 </p>
<p>15.49 </p>
<p>0 20 106.79 </p>
<p>97.89 </p>
<p>94.39 </p>
<p>20.60 </p>
<p>65.87 </p>
<p>55.50 </p>
<p>54.50 </p>
<p>11.99 </p>
<p>21.72 </p>
<p>33.91 </p>
<p>29.87 </p>
<p>20.13 </p>
<p>66.49 </p>
<p>73.66 </p>
<p>38.78 </p>
<p>39.65 </p>
<p>62.52 </p>
<p>75.85 </p>
<p>52.40 </p>
<p>13.09 </p>
<p>23.34 </p>
<p>37.34 </p>
<p>26.97 </p>
<p>17.51 </p>
<p>0 24 253.50 </p>
<p>79.24 </p>
<p>111.92 </p>
<p>37.66 </p>
<p>70.13 </p>
<p>77.14 </p>
<p>255.37 </p>
<p>33.86 </p>
<p>44.71 </p>
<p>71.89 </p>
<p>30.83 </p>
<p>34.38 </p>
<p>73.10 </p>
<p>48.87 </p>
<p>49.04 </p>
<p>42.40 </p>
<p>62.41 </p>
<p>85.33 </p>
<p>185.14 </p>
<p>26.27 </p>
<p>60.77 </p>
<p>59.98 </p>
<p>26.15 </p>
<p>23.80 </p>
<p>0 28 </p>
<p>50.19 </p>
<p>46.86 </p>
<p>54.81 </p>
<p>107.75 </p>
<p>44.53 </p>
<p>34.87 </p>
<p>29.16 </p>
<p>82.43 </p>
<p>18.52 </p>
<p>44.44 </p>
<p>29.00 </p>
<p>18.32 </p>
<p>38.62 </p>
<p>29.75 </p>
<p>35.03 </p>
<p>67.74 </p>
<p>44.32 </p>
<p>36.40 </p>
<p>28.84 </p>
<p>60.91 </p>
<p>25.04 </p>
<p>32.79 </p>
<p>30.68 </p>
<p>21.68 </p>
<p>0 32 316.54 </p>
<p>233.13 </p>
<p>278.13 </p>
<p>91.53 </p>
<p>97.33 </p>
<p>96.82 </p>
<p>147.38 </p>
<p>61.07 
386.28 334.58 </p>
<p>317.23 </p>
<p>155.98 </p>
<p>239.04 </p>
<p>197.45 </p>
<p>162.91 </p>
<p>62.72 
106.22 116.31 
101.84 114.53 238.13 </p>
<p>279.19 </p>
<p>261.74 </p>
<p>112.23 </p>
<p>0 36 387.64 </p>
<p>266.13 </p>
<p>375.25 
107.09 107.66 107.15 </p>
<p>160.42 </p>
<p>66.77 
288.18 670.12 389.60 </p>
<p>150.43 </p>
<p>321.64 </p>
<p>236.49 </p>
<p>249.94 </p>
<p>93.53 
106.38 107.58 
135.97 113.85 272.37 </p>
<p>590.80 </p>
<p>335.90 </p>
<p>127.88 </p>
<p>0 40 357.96 </p>
<p>274.98 </p>
<p>327.24 </p>
<p>101.61 </p>
<p>85.79 </p>
<p>94.19 </p>
<p>135.09 </p>
<p>64.42 </p>
<p>308.50 
391.45 650.17 139.49 </p>
<p>298.53 </p>
<p>224.96 </p>
<p>188.86 </p>
<p>70.51 </p>
<p>100.03 </p>
<p>98.77 </p>
<p>121.47 </p>
<p>91.92 </p>
<p>254.19 </p>
<p>343.97 </p>
<p>364.47 </p>
<p>114.69 </p>
<p>0 44 197.61 </p>
<p>205.65 </p>
<p>221.48 
286.32 103.41 124.85 </p>
<p>158.16 </p>
<p>122.18 </p>
<p>190.58 </p>
<p>227.56 
222.25 385.60 </p>
<p>215.49 </p>
<p>169.27 </p>
<p>194.93 </p>
<p>242.73 </p>
<p>98.39 </p>
<p>105.59 
113.53 150.57 200.63 </p>
<p>201.28 </p>
<p>204.19 </p>
<p>262.03 </p>
<p>1 00 </p>
<p>86.12 </p>
<p>33.57 </p>
<p>69.71 </p>
<p>2.29 </p>
<p>8.91 </p>
<p>12.99 </p>
<p>5.43 </p>
<p>1.69 </p>
<p>8.95 </p>
<p>17.70 </p>
<p>15.76 </p>
<p>3.72 </p>
<p>155.83 </p>
<p>28.70 </p>
<p>59.28 </p>
<p>3.76 </p>
<p>18.06 </p>
<p>9.61 </p>
<p>9.37 </p>
<p>2.25 </p>
<p>6.95 </p>
<p>18.75 </p>
<p>11.67 </p>
<p>3.45 </p>
<p>1 04 </p>
<p>50.89 </p>
<p>270.63 </p>
<p>49.13 </p>
<p>2.28 </p>
<p>8.15 </p>
<p>15.92 </p>
<p>5.20 </p>
<p>1.66 </p>
<p>7.64 </p>
<p>14.19 </p>
<p>11.04 </p>
<p>3.62 </p>
<p>46.19 </p>
<p>398.02 </p>
<p>16.10 </p>
<p>3.32 </p>
<p>14.67 </p>
<p>9.63 </p>
<p>8.87 </p>
<p>2.18 </p>
<p>6.97 </p>
<p>16.85 </p>
<p>9.93 </p>
<p>3.47 </p>
<p>1 08 </p>
<p>59.29 </p>
<p>24.59 </p>
<p>110.26 </p>
<p>3.30 </p>
<p>7.67 </p>
<p>7.90 </p>
<p>15.25 </p>
<p>2.26 </p>
<p>7.63 </p>
<p>16.75 </p>
<p>12.24 </p>
<p>4.31 </p>
<p>66.53 </p>
<p>32.42 </p>
<p>335.56 </p>
<p>3.57 </p>
<p>11.98 </p>
<p>7.75 </p>
<p>13.40 </p>
<p>2.00 </p>
<p>7.15 </p>
<p>16.47 </p>
<p>9.17 </p>
<p>3.50 </p>
<p>1 12 </p>
<p>15.19 </p>
<p>15.13 </p>
<p>19.13 </p>
<p>34.37 </p>
<p>3.74 </p>
<p>5.40 </p>
<p>5.96 </p>
<p>9.37 </p>
<p>6.33 </p>
<p>21.02 </p>
<p>10.34 </p>
<p>6.62 </p>
<p>28.06 </p>
<p>12.01 </p>
<p>17.92 </p>
<p>166.31 </p>
<p>8.31 </p>
<p>6.50 </p>
<p>5.06 </p>
<p>18.26 </p>
<p>7.24 </p>
<p>13.37 </p>
<p>17.95 </p>
<p>10.16 </p>
<p>1 16 </p>
<p>62.92 </p>
<p>44.24 </p>
<p>65.13 </p>
<p>11.50 </p>
<p>44.48 </p>
<p>53.30 </p>
<p>43.68 </p>
<p>10.58 </p>
<p>23.84 </p>
<p>31.69 </p>
<p>30.56 </p>
<p>21.49 </p>
<p>70.34 </p>
<p>36.79 </p>
<p>46.12 </p>
<p>43.93 </p>
<p>55.46 </p>
<p>50.18 </p>
<p>49.15 </p>
<p>11.13 </p>
<p>19.92 </p>
<p>32.76 </p>
<p>26.77 </p>
<p>14.77 </p>
<p>1 20 </p>
<p>63.44 </p>
<p>65.52 </p>
<p>68.37 </p>
<p>11.51 </p>
<p>44.23 </p>
<p>60.38 </p>
<p>44.32 </p>
<p>10.51 </p>
<p>23.92 </p>
<p>30.84 </p>
<p>27.80 </p>
<p>22.16 </p>
<p>59.26 </p>
<p>77.07 </p>
<p>35.00 </p>
<p>43.45 </p>
<p>49.37 </p>
<p>51.45 </p>
<p>47.86 </p>
<p>11.36 </p>
<p>20.48 </p>
<p>34.74 </p>
<p>25.70 </p>
<p>15.56 </p>
<p>1 24 110.40 </p>
<p>56.46 </p>
<p>67.38 </p>
<p>34.26 </p>
<p>52.80 </p>
<p>68.76 </p>
<p>147.61 </p>
<p>29.07 </p>
<p>49.45 </p>
<p>79.30 </p>
<p>28.27 </p>
<p>29.20 </p>
<p>63.06 </p>
<p>84.63 </p>
<p>82.14 </p>
<p>46.06 </p>
<p>57.29 </p>
<p>63.25 
188.06 20.26 </p>
<p>51.87 </p>
<p>53.75 </p>
<p>25.58 </p>
<p>19.02 </p>
<p>1 28 </p>
<p>35.28 </p>
<p>35.38 </p>
<p>40.50 </p>
<p>47.83 </p>
<p>34.67 </p>
<p>29.15 </p>
<p>22.79 </p>
<p>64.74 </p>
<p>17.86 </p>
<p>35.91 </p>
<p>24.77 </p>
<p>18.69 </p>
<p>40.63 </p>
<p>28.17 </p>
<p>34.15 </p>
<p>71.67 </p>
<p>45.04 </p>
<p>32.69 </p>
<p>19.92 </p>
<p>63.16 </p>
<p>24.47 </p>
<p>28.89 </p>
<p>33.77 </p>
<p>24.34 </p>
<p>1 32 260.52 </p>
<p>223.76 </p>
<p>261.03 </p>
<p>85.64 </p>
<p>77.54 </p>
<p>83.63 </p>
<p>115.08 </p>
<p>64.84 </p>
<p>217.10 </p>
<p>265.24 </p>
<p>267.66 </p>
<p>115.87 </p>
<p>237.96 </p>
<p>198.38 </p>
<p>193.30 </p>
<p>94.40 </p>
<p>112.71 </p>
<p>86.83 </p>
<p>104.63 
96.77 353.22 302.25 </p>
<p>303.72 </p>
<p>137.83 </p>
<p>1 36 299.91 </p>
<p>248.41 </p>
<p>301.40 </p>
<p>90.66 </p>
<p>71.85 </p>
<p>87.63 </p>
<p>111.49 </p>
<p>58.70 </p>
<p>233.00 </p>
<p>398.90 </p>
<p>303.78 </p>
<p>133.61 </p>
<p>267.63 </p>
<p>210.01 </p>
<p>219.81 </p>
<p>88.54 </p>
<p>84.77 </p>
<p>91.64 </p>
<p>125.32 </p>
<p>93.06 
261.45 542.43 338.24 </p>
<p>125.56 </p>
<p>1 40 293.05 </p>
<p>241.07 </p>
<p>285.05 </p>
<p>77.30 </p>
<p>72.59 </p>
<p>83.71 </p>
<p>108.18 </p>
<p>58.49 </p>
<p>220.03 </p>
<p>325.41 </p>
<p>347.72 </p>
<p>110.83 </p>
<p>264.17 </p>
<p>197.67 </p>
<p>205.66 </p>
<p>88.08 </p>
<p>91.39 </p>
<p>84.67 </p>
<p>124.35 </p>
<p>85.72 </p>
<p>284.48 
351.44 516.93 128.91 </p>
<p>1 44 219.33 </p>
<p>192.96 </p>
<p>227.90 </p>
<p>220.58 </p>
<p>84.22 </p>
<p>117.52 </p>
<p>132.58 </p>
<p>125.78 </p>
<p>159.99 </p>
<p>209.12 </p>
<p>186.04 </p>
<p>248.64 </p>
<p>238.91 </p>
<p>164.82 </p>
<p>208.75 
258.05 101.89 115.20 </p>
<p>101.94 149.35 209.33 </p>
<p>193.54 
206.80 361.90 </p>
<p>0 00 </p>
<p>0 04 </p>
<p>0 08 </p>
<p>0 12 </p>
<p>0 16 </p>
<p>0 20 </p>
<p>0 24 </p>
<p>0 28 </p>
<p>0 32 </p>
<p>0 36 </p>
<p>0 40 </p>
<p>0 44 </p>
<p>1 00 </p>
<p>1 04 </p>
<p>1 08 </p>
<p>1 12 </p>
<p>1 16 </p>
<p>1 20 </p>
<p>1 24 </p>
<p>1 28 </p>
<p>1 32 </p>
<p>1 36 </p>
<p>1 40 </p>
<p>1 44 </p>
<p>0 00 100.00 </p>
<p>18.83 </p>
<p>74.53 </p>
<p>1.30 </p>
<p>7.50 </p>
<p>4.15 </p>
<p>3.53 </p>
<p>0.77 </p>
<p>2.55 </p>
<p>4.90 </p>
<p>4.28 </p>
<p>1.32 </p>
<p>31.77 </p>
<p>6.04 </p>
<p>10.79 </p>
<p>0.97 </p>
<p>6.02 </p>
<p>7.09 </p>
<p>3.70 </p>
<p>0.98 </p>
<p>2.74 </p>
<p>5.28 </p>
<p>3.42 </p>
<p>1.27 </p>
<p>0 04 </p>
<p>50.56 </p>
<p>100.01 </p>
<p>35.49 </p>
<p>0.86 </p>
<p>4.11 </p>
<p>3.65 </p>
<p>2.24 </p>
<p>0.46 </p>
<p>1.57 </p>
<p>3.33 </p>
<p>2.59 </p>
<p>0.84 </p>
<p>14.47 </p>
<p>80.07 </p>
<p>4.20 </p>
<p>0.64 </p>
<p>3.90 </p>
<p>5.45 </p>
<p>1.99 </p>
<p>0.62 </p>
<p>1.68 </p>
<p>3.72 </p>
<p>2.18 </p>
<p>0.74 </p>
<p>0 08 </p>
<p>67.45 </p>
<p>17.28 </p>
<p>100.01 </p>
<p>1.39 </p>
<p>4.12 </p>
<p>3.55 </p>
<p>19.77 </p>
<p>0.63 </p>
<p>2.30 </p>
<p>4.68 </p>
<p>3.85 </p>
<p>1.32 </p>
<p>21.61 </p>
<p>6.87 </p>
<p>23.01 </p>
<p>0.87 </p>
<p>5.01 </p>
<p>4.74 </p>
<p>4.68 </p>
<p>0.82 </p>
<p>2.74 </p>
<p>4.90 </p>
<p>2.78 </p>
<p>1.19 </p>
<p>0 12 </p>
<p>8.92 </p>
<p>8.75 </p>
<p>12.10 </p>
<p>100.01 </p>
<p>3.99 </p>
<p>2.06 </p>
<p>3.28 </p>
<p>11.02 </p>
<p>2.17 </p>
<p>9.52 </p>
<p>4.45 </p>
<p>1.99 </p>
<p>6.85 </p>
<p>4.43 </p>
<p>5.85 </p>
<p>33.65 </p>
<p>3.34 </p>
<p>4.33 </p>
<p>1.81 </p>
<p>5.22 </p>
<p>2.41 </p>
<p>5.24 </p>
<p>4.80 </p>
<p>2.92 </p>
<p>0 16 162.51 </p>
<p>88.83 </p>
<p>140.34 </p>
<p>26.32 </p>
<p>99.96 </p>
<p>71.27 </p>
<p>75.88 </p>
<p>17.16 </p>
<p>31.01 </p>
<p>45.24 </p>
<p>42.47 </p>
<p>26.51 </p>
<p>98.18 </p>
<p>49.61 </p>
<p>57.50 </p>
<p>55.50 </p>
<p>75.70 </p>
<p>94.93 </p>
<p>77.08 </p>
<p>18.26 </p>
<p>32.55 </p>
<p>48.19 </p>
<p>37.02 </p>
<p>21.54 </p>
<p>0 20 192.42 </p>
<p>176.39 </p>
<p>170.07 </p>
<p>37.11 
118.68 100.00 </p>
<p>98.19 </p>
<p>21.60 </p>
<p>39.13 </p>
<p>61.09 </p>
<p>53.82 </p>
<p>36.26 </p>
<p>119.80 </p>
<p>132.72 </p>
<p>69.88 </p>
<p>71.44 
112.64 136.66 </p>
<p>94.42 </p>
<p>23.58 </p>
<p>42.05 </p>
<p>67.28 </p>
<p>48.59 </p>
<p>31.55 </p>
<p>0 24 </p>
<p>99.25 </p>
<p>31.02 </p>
<p>43.82 </p>
<p>14.75 </p>
<p>27.46 </p>
<p>30.20 </p>
<p>99.99 </p>
<p>13.26 </p>
<p>17.51 </p>
<p>28.15 </p>
<p>12.07 </p>
<p>13.46 </p>
<p>28.62 </p>
<p>19.14 </p>
<p>19.20 </p>
<p>16.60 </p>
<p>24.44 </p>
<p>33.41 </p>
<p>72.49 </p>
<p>10.29 </p>
<p>23.79 </p>
<p>23.48 </p>
<p>10.24 </p>
<p>9.32 </p>
<p>0 28 </p>
<p>60.91 </p>
<p>56.87 </p>
<p>66.51 </p>
<p>130.77 </p>
<p>54.04 </p>
<p>42.32 </p>
<p>35.39 </p>
<p>100.03 </p>
<p>22.48 </p>
<p>53.93 </p>
<p>35.19 </p>
<p>22.24 </p>
<p>46.87 </p>
<p>36.11 </p>
<p>42.51 </p>
<p>82.21 </p>
<p>53.79 </p>
<p>44.18 </p>
<p>35.00 </p>
<p>73.92 </p>
<p>30.39 </p>
<p>39.80 </p>
<p>37.23 </p>
<p>26.31 </p>
<p>0 32 </p>
<p>81.94 </p>
<p>60.35 </p>
<p>72.00 </p>
<p>23.69 </p>
<p>25.20 </p>
<p>25.06 </p>
<p>38.15 </p>
<p>15.81 </p>
<p>99.99 </p>
<p>86.61 </p>
<p>82.12 </p>
<p>40.38 </p>
<p>61.88 </p>
<p>51.11 </p>
<p>42.17 </p>
<p>16.24 </p>
<p>27.50 </p>
<p>30.11 </p>
<p>26.36 </p>
<p>29.65 </p>
<p>61.64 </p>
<p>72.27 </p>
<p>67.76 </p>
<p>29.05 </p>
<p>0 36 </p>
<p>57.85 </p>
<p>39.71 </p>
<p>56.00 </p>
<p>15.98 </p>
<p>16.07 </p>
<p>15.99 </p>
<p>23.94 </p>
<p>9.96 </p>
<p>43.01 </p>
<p>100.00 </p>
<p>58.14 </p>
<p>22.45 </p>
<p>48.00 </p>
<p>35.29 </p>
<p>37.30 </p>
<p>13.96 </p>
<p>15.88 </p>
<p>16.05 </p>
<p>20.29 </p>
<p>16.99 </p>
<p>40.65 </p>
<p>88.17 </p>
<p>50.13 </p>
<p>19.08 </p>
<p>0 40 </p>
<p>55.05 </p>
<p>42.29 </p>
<p>50.33 </p>
<p>15.63 </p>
<p>13.19 </p>
<p>14.49 </p>
<p>20.78 </p>
<p>9.91 </p>
<p>47.45 </p>
<p>60.20 </p>
<p>100.00 </p>
<p>21.45 </p>
<p>45.91 </p>
<p>34.60 </p>
<p>29.05 </p>
<p>10.84 </p>
<p>15.38 </p>
<p>15.19 </p>
<p>18.68 </p>
<p>14.14 </p>
<p>39.09 </p>
<p>52.90 </p>
<p>56.05 </p>
<p>17.64 </p>
<p>0 44 </p>
<p>51.25 </p>
<p>53.33 </p>
<p>57.44 </p>
<p>74.25 </p>
<p>26.82 </p>
<p>32.38 </p>
<p>41.02 </p>
<p>31.69 </p>
<p>49.43 </p>
<p>59.02 </p>
<p>57.64 </p>
<p>100.00 </p>
<p>55.88 </p>
<p>43.90 </p>
<p>50.55 </p>
<p>62.95 </p>
<p>25.52 </p>
<p>27.38 </p>
<p>29.44 </p>
<p>39.05 </p>
<p>52.03 </p>
<p>52.20 </p>
<p>52.95 </p>
<p>67.96 </p>
<p>1 00 </p>
<p>55.28 </p>
<p>21.54 </p>
<p>44.74 </p>
<p>1.47 </p>
<p>5.72 </p>
<p>8.34 </p>
<p>3.49 </p>
<p>1.08 </p>
<p>5.74 </p>
<p>11.36 </p>
<p>10.12 </p>
<p>2.38 </p>
<p>100.02 </p>
<p>18.42 </p>
<p>38.05 </p>
<p>2.41 </p>
<p>11.59 </p>
<p>6.17 </p>
<p>6.01 </p>
<p>1.45 </p>
<p>4.46 </p>
<p>12.03 </p>
<p>7.49 </p>
<p>2.22 </p>
<p>1 04 </p>
<p>12.79 </p>
<p>68.00 </p>
<p>12.34 </p>
<p>0.57 </p>
<p>2.05 </p>
<p>4.00 </p>
<p>1.31 </p>
<p>0.42 </p>
<p>1.92 </p>
<p>3.56 </p>
<p>2.77 </p>
<p>0.91 </p>
<p>11.61 </p>
<p>100.00 </p>
<p>4.05 </p>
<p>0.83 </p>
<p>3.69 </p>
<p>2.42 </p>
<p>2.23 </p>
<p>0.55 </p>
<p>1.75 </p>
<p>4.23 </p>
<p>2.50 </p>
<p>0.87 </p>
<p>1 08 </p>
<p>17.67 </p>
<p>7.33 </p>
<p>32.86 </p>
<p>0.98 </p>
<p>2.28 </p>
<p>2.35 </p>
<p>4.54 </p>
<p>0.67 </p>
<p>2.27 </p>
<p>4.99 </p>
<p>3.65 </p>
<p>1.28 </p>
<p>19.82 </p>
<p>9.66 </p>
<p>99.99 </p>
<p>1.06 </p>
<p>3.57 </p>
<p>2.31 </p>
<p>3.99 </p>
<p>0.60 </p>
<p>2.13 </p>
<p>4.91 </p>
<p>2.73 </p>
<p>1.04 </p>
<p>1 12 </p>
<p>9.14 </p>
<p>9.10 </p>
<p>11.50 </p>
<p>20.67 </p>
<p>2.25 </p>
<p>3.25 </p>
<p>3.59 </p>
<p>5.63 </p>
<p>3.80 </p>
<p>12.64 </p>
<p>6.22 </p>
<p>3.98 </p>
<p>16.87 </p>
<p>7.22 </p>
<p>10.78 </p>
<p>100.01 </p>
<p>5.00 </p>
<p>3.91 </p>
<p>3.05 </p>
<p>10.98 </p>
<p>4.35 </p>
<p>8.04 </p>
<p>10.79 </p>
<p>6.11 </p>
<p>1 16 113.36 </p>
<p>79.71 </p>
<p>117.36 </p>
<p>20.72 </p>
<p>80.15 </p>
<p>96.04 </p>
<p>78.71 </p>
<p>19.06 </p>
<p>42.95 </p>
<p>57.10 </p>
<p>55.07 </p>
<p>38.72 </p>
<p>126.75 </p>
<p>66.30 </p>
<p>83.10 </p>
<p>79.16 </p>
<p>99.93 </p>
<p>90.42 </p>
<p>88.56 </p>
<p>20.06 </p>
<p>35.89 </p>
<p>59.04 </p>
<p>48.23 </p>
<p>26.60 </p>
<p>1 20 123.43 </p>
<p>127.47 </p>
<p>133.01 </p>
<p>22.39 </p>
<p>86.05 </p>
<p>117.48 </p>
<p>86.23 </p>
<p>20.45 </p>
<p>46.53 </p>
<p>60.00 </p>
<p>54.09 </p>
<p>43.12 </p>
<p>115.30 </p>
<p>149.94 </p>
<p>68.09 </p>
<p>84.54 
96.04 100.09 </p>
<p>93.12 </p>
<p>22.10 </p>
<p>39.84 </p>
<p>67.59 </p>
<p>50.00 </p>
<p>30.28 </p>
<p>1 24 </p>
<p>58.69 </p>
<p>30.01 </p>
<p>35.82 </p>
<p>18.21 </p>
<p>28.07 </p>
<p>36.56 </p>
<p>78.47 </p>
<p>15.45 </p>
<p>26.29 </p>
<p>42.16 </p>
<p>15.03 </p>
<p>15.52 </p>
<p>33.53 </p>
<p>44.99 </p>
<p>43.67 </p>
<p>24.49 </p>
<p>30.46 </p>
<p>33.63 </p>
<p>99.98 </p>
<p>10.77 </p>
<p>27.57 </p>
<p>28.57 </p>
<p>13.60 </p>
<p>10.11 </p>
<p>1 28 </p>
<p>55.82 </p>
<p>55.98 </p>
<p>64.08 </p>
<p>75.68 </p>
<p>54.86 </p>
<p>46.13 </p>
<p>36.05 </p>
<p>102.44 </p>
<p>28.26 </p>
<p>56.82 </p>
<p>39.19 </p>
<p>29.58 </p>
<p>64.29 </p>
<p>44.58 </p>
<p>54.03 </p>
<p>113.40 </p>
<p>71.27 </p>
<p>51.72 </p>
<p>31.51 </p>
<p>99.93 </p>
<p>38.72 </p>
<p>45.72 </p>
<p>53.43 </p>
<p>38.51 </p>
<p>1 32 </p>
<p>73.76 </p>
<p>63.35 </p>
<p>73.90 </p>
<p>24.25 </p>
<p>21.95 </p>
<p>23.68 </p>
<p>32.58 </p>
<p>18.36 </p>
<p>61.47 </p>
<p>75.10 </p>
<p>75.78 </p>
<p>32.81 </p>
<p>67.37 </p>
<p>56.17 </p>
<p>54.73 </p>
<p>26.73 </p>
<p>31.91 </p>
<p>24.58 </p>
<p>29.62 
27.40 100.01 </p>
<p>85.57 </p>
<p>85.99 </p>
<p>39.02 </p>
<p>1 36 </p>
<p>55.29 </p>
<p>45.80 </p>
<p>55.57 </p>
<p>16.71 </p>
<p>13.25 </p>
<p>16.16 </p>
<p>20.56 </p>
<p>10.82 </p>
<p>42.96 </p>
<p>73.54 </p>
<p>56.01 </p>
<p>24.63 </p>
<p>49.34 </p>
<p>38.72 </p>
<p>40.52 </p>
<p>16.32 </p>
<p>15.63 </p>
<p>16.89 </p>
<p>23.11 </p>
<p>17.16 </p>
<p>48.20 </p>
<p>100.01 </p>
<p>62.36 </p>
<p>23.15 </p>
<p>1 40 </p>
<p>56.69 </p>
<p>46.64 </p>
<p>55.15 </p>
<p>14.95 </p>
<p>14.04 </p>
<p>16.19 </p>
<p>20.93 </p>
<p>11.32 </p>
<p>42.57 </p>
<p>62.95 </p>
<p>67.27 </p>
<p>21.44 </p>
<p>51.11 </p>
<p>38.24 </p>
<p>39.79 </p>
<p>17.04 </p>
<p>17.68 </p>
<p>16.38 </p>
<p>24.06 </p>
<p>16.58 </p>
<p>55.04 </p>
<p>67.99 </p>
<p>100.01 </p>
<p>24.94 </p>
<p>1 44 </p>
<p>60.61 </p>
<p>53.32 </p>
<p>62.97 </p>
<p>60.95 </p>
<p>23.27 </p>
<p>32.47 </p>
<p>36.63 </p>
<p>34.75 </p>
<p>44.21 </p>
<p>57.78 </p>
<p>51.41 </p>
<p>68.70 </p>
<p>66.02 </p>
<p>45.54 </p>
<p>57.68 </p>
<p>71.30 </p>
<p>28.16 </p>
<p>31.83 </p>
<p>28.17 </p>
<p>41.27 </p>
<p>57.84 </p>
<p>53.48 </p>
<p>57.14 </p>
<p>100.00 </p>
<p>Table 13 :
13Freeway zero-shot transfer matrix between variant-experts trained from scratch. Top: raw scores, Bottom: Variant-Expert normalised scores (percentages).0 00 
0 01 
0 02 
0 03 
0 04 
0 05 
0 06 
0 07 
1 00 
1 01 
1 02 
1 03 
1 04 
1 05 
1 06 
1 07 
0 00 
33.10 
16.24 
14.70 
n/a 
20.03 
18.25 
15.86 
n/a 
33.14 
11.71 
8.44 
17.48 
19.60 
19.06 
17.14 
19.80 
0 01 
n/a 
25.83 
1.95 
n/a 
1.25 
4.49 
3.23 
n/a 
0.37 
26.68 
2.63 
1.89 
0.56 
4.86 
1.53 
2.15 
0 02 
n/a 
2.66 
27.13 
n/a 
1.32 
1.47 
3.41 
n/a 
0.10 
1.87 
27.41 
2.65 
0.71 
1.50 
2.31 
3.94 
0 03 
n/a 
2.99 
0.91 
32.97 
2.28 
14.01 
0.73 
n/a 
0.24 
3.34 
4.36 
32.97 
3.25 
7.72 
0.07 
1.43 
0 04 
n/a 
13.07 
11.38 
n/a 
33.30 
17.76 
15.65 
n/a 
18.30 
10.37 
8.20 
15.01 
33.47 
16.98 
17.25 
18.92 
0 05 
n/a 
3.31 
3.60 
n/a 
3.03 
25.65 
5.07 
n/a 
1.75 
1.95 
2.53 
4.22 
2.32 
25.82 
3.95 
8.26 
0 06 
n/a 
3.52 
4.27 
n/a 
3.51 
5.82 
25.66 
n/a 
1.54 
1.95 
3.50 
3.13 
2.40 
5.47 
25.34 
7.13 
0 07 
n/a 
3.90 
4.41 
n/a 
1.15 
4.53 
0.61 
28.74 
1.19 
2.55 
4.01 
8.34 
1.05 
2.96 
0.58 
28.74 
1 00 
n/a 
6.90 
6.54 
n/a 
16.35 
11.84 
9.87 
n/a 
33.08 
5.63 
2.87 
7.51 
14.04 
12.37 
10.61 
13.04 
1 01 
n/a 
25.50 
0.79 
n/a 
1.21 
2.73 
2.44 
n/a 
0.23 
26.87 
2.13 
0.32 
0.47 
3.18 
0.78 
0.67 
1 02 
n/a 
0.26 
27.00 
n/a 
0.06 
0.07 
0.59 
n/a 
n/a 
0.75 
27.34 
0.20 
0.31 
0.10 
0.48 
0.13 
1 03 
n/a 
1.98 
0.78 
n/a 
0.10 
6.19 
0.86 
n/a 
0.01 
1.01 
1.28 
32.88 
0.23 
7.99 
0.04 
0.12 
1 04 
n/a 
8.46 
6.47 
n/a 
32.97 
13.40 
11.94 
n/a 
13.16 
5.86 
5.28 
10.39 33.39 
12.73 
12.56 
13.73 
1 05 
n/a 
1.83 
1.76 
n/a 
1.87 
25.27 
4.01 
n/a 
1.01 
0.90 
1.04 
2.13 
1.60 
25.85 
3.37 
4.65 
1 06 
n/a 
2.04 
2.62 
n/a 
2.45 
2.94 
25.39 
n/a 
0.74 
1.01 
2.42 
1.45 
1.70 
3.41 
25.66 
4.44 
1 07 
n/a 
1.73 
1.84 
n/a 
0.75 
3.25 
0.35 
n/a 
0.42 
1.18 
2.17 
4.87 
0.45 
2.14 
0.37 
28.81 
0 00 
0 01 
0 02 
0 03 
0 04 
0 05 
0 06 
0 07 
1 00 
1 01 
1 02 
1 03 
1 04 
1 05 
1 06 
1 07 
0 00 100.00 
49.06 
44.42 
n/a 
60.52 
55.14 
47.91 
n/a 
100.13 35.38 
25.49 
52.80 
59.22 
57.57 
51.78 
59.81 
0 01 
n/a 
100.12 
7.54 
n/a 
4.86 
17.39 
12.50 
n/a 
1.43 
103.40 
10.18 
7.34 
2.16 
18.82 
5.93 
8.34 
0 02 
n/a 
9.82 
100.10 
n/a 
4.89 
5.41 
12.57 
n/a 
0.38 
6.92 
101.14 
9.76 
2.63 
5.55 
8.51 
14.56 
0 03 
n/a 
9.05 
2.77 
99.91 
6.90 
42.44 
2.20 
n/a 
0.73 
10.13 
13.22 
99.92 
9.86 
23.39 
0.21 
4.33 
0 04 
n/a 
39.25 
34.19 
n/a 
100.00 53.33 
47.01 
n/a 
54.94 
31.14 
24.63 
45.08 100.51 
51.01 
51.80 
56.83 
0 05 
n/a 
12.89 
14.01 
n/a 
11.79 
99.81 19.72 
n/a 
6.80 
7.59 
9.84 
16.43 
9.02 
100.48 
15.37 
32.13 
0 06 
n/a 
13.68 
16.60 
n/a 
13.64 
22.64 99.84 
n/a 
5.97 
7.58 
13.60 
12.16 
9.32 
21.29 
98.59 
27.76 
0 07 
n/a 
13.60 
15.37 
n/a 
4.00 
15.80 
2.12 
100.14 
4.13 
8.89 
13.99 
29.07 
3.65 
10.31 
2.01 
100.14 
1 00 
n/a 
20.84 
19.77 
n/a 
49.40 
35.77 
29.83 
n/a 
99.93 
17.01 
8.67 
22.70 
42.40 
37.37 
32.05 
39.41 
1 01 
n/a 
94.79 
2.94 
n/a 
4.51 
10.15 
9.06 
n/a 
0.86 
99.90 
7.91 
1.21 
1.73 
11.82 
2.89 
2.51 
1 02 
n/a 
0.95 
98.90 
n/a 
0.23 
0.26 
2.15 
n/a 
0.01 
2.73 
100.16 
0.72 
1.15 
0.38 
1.77 
0.47 
1 03 
n/a 
6.01 
2.36 
n/a 
0.32 
18.80 
2.63 
n/a 
0.02 
3.07 
3.88 
99.92 
0.69 
24.29 
0.12 
0.35 
1 04 
n/a 
25.34 
19.36 
n/a 
98.70 
40.12 
35.75 
n/a 
39.41 
17.54 
15.82 
31.11 99.96 
38.12 
37.62 
41.11 
1 05 
n/a 
7.11 
6.82 
n/a 
7.25 
97.96 
15.55 
n/a 
3.91 
3.49 
4.04 
8.25 
6.21 
100.19 13.05 
18.01 
1 06 
n/a 
7.93 
10.20 
n/a 
9.53 
11.45 
98.81 
n/a 
2.89 
3.94 
9.40 
5.65 
6.63 
13.27 
99.85 
17.29 
1 07 
n/a 
5.99 
6.40 
n/a 
2.62 
11.28 
1.23 
n/a 
1.48 
4.08 
7.54 
16.92 
1.57 
7.42 
1.30 
100.03 </p>
<p>Successor features for transfer in reinforcement learning. Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, David Hado P Van Hasselt, Silver, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol- ume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 350db081a661525235354dd3e19b8c05-Paper.pdf.</p>
<p>Is deep reinforcement learning really superhuman on atari? CoRR, abs/1908.04683. Marin Toromanoff, Émilie Wirbel, Fabien Moutarde, Marin Toromanoff,Émilie Wirbel, and Fabien Moutarde. Is deep reinforcement learning really superhuman on atari? CoRR, abs/1908.04683, 2019. URL http://arxiv.org/abs/1908.04683.</p>
<p>Deep reinforcement learning with double q-learning. Arthur Hado Van Hasselt, David Guez, Silver, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence30Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, H David, Richard Choi, Timo Powell, Petko Ewalds, Georgiev, Nature. 5757782Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.</p>
<p>Sample efficient actor-critic with experience replay. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando De Freitas, Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. 2016.</p>
<p>Q-learning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 83Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279-292, 1992.</p>
<p>Learning from delayed rewards. Christopher John Cornish Hellaby Watkins, Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.</p>
<p>Metaworld: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on Robot Learning. PMLRTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta- world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pp. 1094-1100. PMLR, 2020.</p>
<p>Transfer learning in deep reinforcement learning: A survey. CoRR, abs. Zhuangdi Zhu, Kaixiang Lin, Jiayu Zhou, Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A survey. CoRR, abs/2009.07888, 2020. URL https://arxiv.org/abs/2009.07888.</p>            </div>
        </div>

    </div>
</body>
</html>
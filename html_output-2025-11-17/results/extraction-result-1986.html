<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1986 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1986</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1986</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-45.html">extraction-schema-45</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <p><strong>Paper ID:</strong> paper-277104853</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.14217v1.pdf" target="_blank">Decision Tree Induction Through LLMs via Semantically-Aware Evolution</a></p>
                <p><strong>Paper Abstract:</strong> Decision trees are a crucial class of models offering robust predictive performance and inherent interpretability across various domains, including healthcare, finance, and logistics. However, current tree induction methods often face limitations such as suboptimal solutions from greedy methods or prohibitive computational costs and limited applicability of exact optimization approaches. To address these challenges, we propose an evolutionary optimization method for decision tree induction based on genetic programming (GP). Our key innovation is the integration of semantic priors and domain-specific knowledge about the search space into the optimization algorithm. To this end, we introduce $\texttt{LLEGO}$, a framework that incorporates semantic priors into genetic search operators through the use of Large Language Models (LLMs), thereby enhancing search efficiency and targeting regions of the search space that yield decision trees with superior generalization performance. This is operationalized through novel genetic operators that work with structured natural language prompts, effectively utilizing LLMs as conditional generative models and sources of semantic knowledge. Specifically, we introduce $\textit{fitness-guided}$ crossover to exploit high-performing regions, and $\textit{diversity-guided}$ mutation for efficient global exploration of the search space. These operators are controlled by corresponding hyperparameters that enable a more nuanced balance between exploration and exploitation across the search space. Empirically, we demonstrate across various benchmarks that $\texttt{LLEGO}$ evolves superior-performing trees compared to existing tree induction methods, and exhibits significantly more efficient search performance compared to conventional GP approaches.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1986.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1986.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLEGO_XO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLEGO fitness-guided crossover (LLM-XO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantically-aware, LLM-conditioned crossover operator that generates offspring by prompting an LLM with multiple parent trees and a target fitness f* computed from parent fitnesses; controlled by hyperparameter α which steers extrapolation toward higher fitness regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>fitness-guided crossover (LLEGO_XO)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>crossover</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Samples ν parents from the population using fitness-proportionate (roulette-wheel) sampling, computes a target fitness f* = f_max + α(f_max - f_min) where α is a user-controlled hyperparameter, constructs a few-shot natural-language prompt containing serialized parent trees, their fitnesses, and task context C, then queries an LLM to generate offspring conditioned on f*. The operator naturally supports arity ν > 2 via in-context examples. α controls the degree of extrapolation (α > 0 encourages offspring that exceed best parent fitness; negative α is conservative).</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td>In-context learning via LLM generation (few-shot conditioning) combined with fitness-proportionate parent sampling and a parameter α that adaptively shifts the target fitness; no gradient-based fitting of the operator itself.</td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>Conditional generative distribution induced by an LLM prompt (probability distribution over natural-language serialized trees), parameterized by α and by prompt contents (parents + context).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>combinatorial (decision trees represented in natural language / text)</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Balanced accuracy (classification), MSE (regression) for final models; population fitness (normalized balanced accuracy), population diversity (median L1 functional-distance), offspring fitness and diversity as functions of α.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Evaluated end-to-end within LLEGO: across classification tasks (depth=4) LLEGO (with fitness-guided crossover + mutation) achieved e.g. Breast 0.951, Compas 0.663, Credit 0.684, Diabetes 0.721, Heart 0.751, Liver 0.676, Vehicle 0.929 (balanced accuracy, mean over runs; Table 1). The paper shows the crossover α ablation: best offspring fitness at α = 0.1 (no single-offspring numeric reported for XO alone).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td>GP baseline (GATree) using conventional structural subtree crossover: depth=4 results (balanced accuracy) e.g. Breast 0.941, Compas 0.650, Credit 0.658, Diabetes 0.675, Heart 0.676, Liver 0.633, Vehicle 0.895 (Table 1 / Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td>Absolute improvements in final balanced accuracy (end-to-end GP with LLEGO XO+MUT) versus GATree ranged up to ~0.075 (7.5 percentage points) on some datasets (e.g., Heart depth=4: 0.751 vs 0.676); typical per-dataset absolute gains were in the 0.01–0.05 range.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td>Population diversity measured as median L1 distance between functional signatures φ(t) = [t(x1),...,t(xn)]; offspring/diversity dynamics reported versus α (diversity decreases with increasing α).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Operator requires LLM calls per crossover; overall LLEGO incurs higher inference cost. Paper reports wall-clock comparisons in Appendix D.6; e.g., aggregate runtimes reported (Table 12/13) and discussion that LLEGO has a larger computational footprint due to LLM inference but can require fewer functional evaluations in some comparisons. (Qualitative: higher per-operator compute vs conventional structural crossover.)</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>25 (default used in main experiments); experiments also report comparisons with larger GATree runs (N=100, G=200).</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td>Yes — fitness-guided crossover specializes via α to focus sampling on higher-fitness regions; ablations show α controls exploitation-exploration tradeoff and that α = 0.1 provided best offspring fitness in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conditioning crossover on a target fitness via LLM in a few-shot prompt (fitness-guided crossover) yields higher-quality offspring and accelerates search convergence compared to structural crossover; increasing α increases offspring fitness up to a point but reduces population diversity and overly large α harms performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Excessive extrapolation (large α) leads to worse offspring (unstable extrapolation); operator incurs notable LLM inference cost; it reduces population diversity if overused, possibly risking premature convergence.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1986.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1986.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLEGO_MUT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLEGO diversity-guided mutation (LLM-MUT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mutation operator that uses an LLM to propose multiple candidate mutations and selects offspring by weighting candidates with a softmax over negative log-probabilities, controlled by temperature τ to prioritize low-likelihood (diverse) candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>diversity-guided mutation (LLEGO_MUT)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>mutation</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Randomly samples ν parents uniformly from the population; constructs few-shot prompts with serialized parent trees (no fitness in prompt), queries the LLM to generate λ' candidate offspring {(õ_j)} and records their negative log-probabilities s(õ_j) = -log p(õ_j | S). Sampling weights are computed as θ_j = exp(s(õ_j)/τ) / Σ_i exp(s(õ_i)/τ), where τ is the temperature hyperparameter; lower τ emphasizes low-prob (diverse) candidates. The operator then samples λ offspring from Cat(θ). The log-probabilities are justified as correlating with structural and functional distance (TED correlation -0.85 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td>In-context generation by an LLM with selection guided by candidate log-probabilities and temperature τ (adaptive, non-gradient parameterization).</td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>Sampling-and-selection over LLM-induced candidate distribution; operator represented implicitly as a parameterized sampling distribution (via LLM prompt) with selection weights derived from log-probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>combinatorial (decision trees represented as natural-language nested dictionaries / text)</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Population diversity (median L1 functional-distance), parent-offspring distance (TED), final task performance (balanced accuracy / MSE), and correlation between log-prob and structural distance (TED).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>As part of full LLEGO algorithm: final balanced accuracy examples (depth=4) listed in Table 1 (e.g., Credit 0.684, Diabetes 0.721). Ablation over τ ∈ {5,10,25,50} showed lower τ increases population diversity and parent-offspring distance; no single numeric isolated for mutation-only performance beyond these dynamics plots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td>Baseline mutation in GATree: random structural mutation (e.g., subtree replacement); GATree end-to-end balanced accuracy results (depth=4) e.g. Credit 0.658, Diabetes 0.675 (Table 1/10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td>Mutation guided by log-probabilities improves population diversity and contributes to higher final performance when combined with fitness-guided crossover — ablations show the tandem of XO+MUT outperforms using only one operator. Exact numeric operator-only improvement not isolated; end-to-end improvements versus GATree up to ~0.075 absolute in balanced accuracy on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td>Population diversity measured as median L1 functional-distance; structural novelness correlated with candidate log-probabilities (reported Pearson correlation coefficient = -0.85 between log-prob and Tree Edit Distance (TED)).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires λ' LLM generations per mutation operation (multiple candidate proposals) and evaluation of log-probabilities; increased compute relative to single random mutation. Paper notes higher LLM inference costs overall and suggests inference-acceleration and quantization as mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>25 (default in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td>Yes — by adjusting τ the mutation operator specializes between proposing more likely (conservative) vs low-probability (diverse) offspring; empirical results show τ modulates diversity and complements crossover.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using LLM log-probabilities to prioritize low-likelihood offspring provides an effective, controllable mechanism to increase structural/functional diversity; low τ yields higher diversity and helps escape local optima when combined with fitness-guided crossover.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Higher computational cost due to multiple candidate generations; choice of τ requires tuning (too high → uniform sampling reduces diversity benefit; too low may produce very unlikely/unhelpful candidates); operator effectiveness depends on LLM quality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1986.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1986.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLEGO (LLM-based operators)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLEGO: LLM-Enhanced Genetic Operators (fitness-guided crossover + diversity-guided mutation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end GP framework that replaces conventional structural variation operators with semantically-aware operators realized via LLM prompting: fitness-guided crossover (α) and diversity-guided mutation (τ), with trees represented in natural language and operators controlled by hyperparameters to trade off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>LLM-based variation operators (combined LLEGO_XO and LLEGO_MUT)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>variation (crossover + mutation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Represent decision trees as nested natural-language dictionaries and frame genetic operations as LLM prompts; crossover is conditioned on a target fitness f* computed from sampled parents and α, while mutation generates multiple candidate offspring and selects via a softmax over negative log-probabilities with temperature τ. Both operator types use in-context examples (parents + task context) to exploit LLM semantic priors and allow higher arity operations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td>In-context learning + parameterized sampling via hyperparameters α (fitness target) and τ (diversity temperature); adaptation occurs online through conditioning on the population and sampling distributions, not via gradient-based training of operator parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>Prompt-conditioned probability distributions induced by an LLM; selection mechanism for mutation uses log-prob-derived weights; parent sampling uses roulette-wheel (fitness-proportionate) for crossover and uniform for mutation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>combinatorial decision-tree induction (trees serialized to natural language / text prompts); experiments on tabular classification & regression.</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Balanced accuracy (classification), mean squared error (regression), normalized population fitness, population diversity (median L1 functional-distance), tree edit distance (TED) and correlation with log-probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>End-to-end LLEGO (α=0.1, τ=10, ν=4, N=25, G=25) outperforms baselines on benchmarks: Table 1 (depth=4) shows LLEGO achieves e.g. Breast 0.951, Compas 0.663, Credit 0.684, Diabetes 0.721, Heart 0.751, Liver 0.676, Vehicle 0.929 (balanced accuracy). LLEGO also yields lower generalization gap (Table 6: average gap 0.043 for LLEGO vs higher for many baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td>GATree (GP with conventional structural operators) results (depth=4) e.g. Breast 0.941, Compas 0.650, Credit 0.658, Diabetes 0.675, Heart 0.676, Liver 0.633, Vehicle 0.895 (Table 1/10). Comparisons with a heavily budgeted GATree (N=100, G=200) still favor LLEGO (Table 10/11).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td>End-to-end LLEGO produced absolute balanced-accuracy improvements up to ~0.075 (7.5 percentage points) on some datasets compared to GATree; LLEGO achieved top average rank (1.0) across classification benchmarks at depth=4 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td>Population diversity = median L1 distance of functional signatures; correlation between offspring log-prob and TED reported (Pearson r = -0.85) indicating low-prob candidates are structurally diverse.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Paper reports increased computational footprint due to LLM inference; in reported runtime tables LLEGO wall-clock is higher than some lightweight baselines (Appendix D.6), but LLEGO often requires fewer functional evaluations and achieves better solutions for the same or comparable budget in many settings. Example runtime entries (Appendix): depth=4 aggregate runtimes quoted in Table 12/13 (see paper) and discussion recommending inference acceleration and quantization.</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>Primarily N=25 in main experiments; ablations and comparisons include GATree with N=25 and N=100 (larger budgets).</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td>Operators adapt via α and τ to focus either on higher-fitness exploitation (crossover) or on diverse exploration (mutation); ablations show both operators are necessary for best performance and that restricting arity (ν=2) is suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating LLM semantic priors into GP variation operators substantially improves search efficiency and generalization of evolved decision trees; fitness conditioning (α) steers exploitation while log-probability-based selection (τ) enables controllable diversity, and combining both yields best results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Higher LLM inference costs; reduced population diversity when crossover is overly exploitative (high α); extrapolation beyond reliable regions for large α causing poorer offspring; potential concerns about LLM memorization and bias (investigated via no-semantics ablations and fairness-regularized objectives).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1986.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1986.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATree (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATree (conventional GP for decision tree induction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional genetic programming implementation for decision tree induction using structural operators (subtree crossover, subtree mutation) and standard selection schemes (tournament etc.), used in the paper as the primary GP baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>structural subtree crossover & subtree mutation (GATree default)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>crossover and mutation</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Classic GP operators operating on tree structure: ν=2 subtree crossover swaps randomly chosen subtrees between parent trees; mutation performs structural node insertions/replacements sampled uniformly over allowed structural operations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>Hand-designed structural rule set (random structural transformations), effectively inducing a uniform prior over permitted structural edits.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>combinatorial (decision trees)</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Balanced accuracy (classification), MSE (regression), population fitness and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td>Reported end-to-end results: e.g. GATree (depth=4) balanced accuracy per Table 1: Breast 0.941, Compas 0.650, Credit 0.658, Diabetes 0.675, Heart 0.676, Liver 0.633, Vehicle 0.895; GATree (large budget N=100,G=200) reported in Table 10 produced slightly improved numbers but still often worse than LLEGO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td>Population diversity reported (GATree tends to maintain higher diversity than LLEGO in experiments, due to unguided structural perturbations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Lower per-operator cost (no LLM calls). Reported runtimes in Appendix D.6 show GATree runtime lower than LLEGO in some aggregated tables (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>Experiments with N=25; larger-budget comparisons used N=100, G=200.</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conventional structural operators are semantically agnostic; while they maintain diversity, they are less efficient at finding high-quality trees within limited evaluation budgets compared to LLEGO.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Semantically unaware structural changes can be disruptive (rough genotype-phenotype mapping), leading to less efficient search and lower-quality final models for the same budget.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language model crossover: Variation through few-shot prompting <em>(Rating: 2)</em></li>
                <li>Evolution through large models <em>(Rating: 2)</em></li>
                <li>Geometric semantic genetic programming <em>(Rating: 2)</em></li>
                <li>Approximating geometric crossover by semantic backpropagation <em>(Rating: 1)</em></li>
                <li>LLMatic: Neural architecture search via large language models and quality diversity optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1986",
    "paper_id": "paper-277104853",
    "extraction_schema_id": "extraction-schema-45",
    "extracted_data": [
        {
            "name_short": "LLEGO_XO",
            "name_full": "LLEGO fitness-guided crossover (LLM-XO)",
            "brief_description": "A semantically-aware, LLM-conditioned crossover operator that generates offspring by prompting an LLM with multiple parent trees and a target fitness f* computed from parent fitnesses; controlled by hyperparameter α which steers extrapolation toward higher fitness regions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "operator_name": "fitness-guided crossover (LLEGO_XO)",
            "operator_type": "crossover",
            "operator_description": "Samples ν parents from the population using fitness-proportionate (roulette-wheel) sampling, computes a target fitness f* = f_max + α(f_max - f_min) where α is a user-controlled hyperparameter, constructs a few-shot natural-language prompt containing serialized parent trees, their fitnesses, and task context C, then queries an LLM to generate offspring conditioned on f*. The operator naturally supports arity ν &gt; 2 via in-context examples. α controls the degree of extrapolation (α &gt; 0 encourages offspring that exceed best parent fitness; negative α is conservative).",
            "is_learned_or_adaptive": true,
            "learning_mechanism": "In-context learning via LLM generation (few-shot conditioning) combined with fitness-proportionate parent sampling and a parameter α that adaptively shifts the target fitness; no gradient-based fitting of the operator itself.",
            "operator_representation": "Conditional generative distribution induced by an LLM prompt (probability distribution over natural-language serialized trees), parameterized by α and by prompt contents (parents + context).",
            "domain_type": "combinatorial (decision trees represented in natural language / text)",
            "context_dependent": true,
            "modality_specific": false,
            "compositional": true,
            "performance_metric": "Balanced accuracy (classification), MSE (regression) for final models; population fitness (normalized balanced accuracy), population diversity (median L1 functional-distance), offspring fitness and diversity as functions of α.",
            "performance_learned_operator": "Evaluated end-to-end within LLEGO: across classification tasks (depth=4) LLEGO (with fitness-guided crossover + mutation) achieved e.g. Breast 0.951, Compas 0.663, Credit 0.684, Diabetes 0.721, Heart 0.751, Liver 0.676, Vehicle 0.929 (balanced accuracy, mean over runs; Table 1). The paper shows the crossover α ablation: best offspring fitness at α = 0.1 (no single-offspring numeric reported for XO alone).",
            "performance_fixed_operator": "GP baseline (GATree) using conventional structural subtree crossover: depth=4 results (balanced accuracy) e.g. Breast 0.941, Compas 0.650, Credit 0.658, Diabetes 0.675, Heart 0.676, Liver 0.633, Vehicle 0.895 (Table 1 / Table 10).",
            "performance_improvement": "Absolute improvements in final balanced accuracy (end-to-end GP with LLEGO XO+MUT) versus GATree ranged up to ~0.075 (7.5 percentage points) on some datasets (e.g., Heart depth=4: 0.751 vs 0.676); typical per-dataset absolute gains were in the 0.01–0.05 range.",
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": "Population diversity measured as median L1 distance between functional signatures φ(t) = [t(x1),...,t(xn)]; offspring/diversity dynamics reported versus α (diversity decreases with increasing α).",
            "transfer_learning": false,
            "transfer_results": null,
            "computational_cost": "Operator requires LLM calls per crossover; overall LLEGO incurs higher inference cost. Paper reports wall-clock comparisons in Appendix D.6; e.g., aggregate runtimes reported (Table 12/13) and discussion that LLEGO has a larger computational footprint due to LLM inference but can require fewer functional evaluations in some comparisons. (Qualitative: higher per-operator compute vs conventional structural crossover.)",
            "population_size": "25 (default used in main experiments); experiments also report comparisons with larger GATree runs (N=100, G=200).",
            "cold_start_addressed": true,
            "operator_specialization": "Yes — fitness-guided crossover specializes via α to focus sampling on higher-fitness regions; ablations show α controls exploitation-exploration tradeoff and that α = 0.1 provided best offspring fitness in experiments.",
            "key_findings": "Conditioning crossover on a target fitness via LLM in a few-shot prompt (fitness-guided crossover) yields higher-quality offspring and accelerates search convergence compared to structural crossover; increasing α increases offspring fitness up to a point but reduces population diversity and overly large α harms performance.",
            "limitations_or_failures": "Excessive extrapolation (large α) leads to worse offspring (unstable extrapolation); operator incurs notable LLM inference cost; it reduces population diversity if overused, possibly risking premature convergence.",
            "uuid": "e1986.0"
        },
        {
            "name_short": "LLEGO_MUT",
            "name_full": "LLEGO diversity-guided mutation (LLM-MUT)",
            "brief_description": "A mutation operator that uses an LLM to propose multiple candidate mutations and selects offspring by weighting candidates with a softmax over negative log-probabilities, controlled by temperature τ to prioritize low-likelihood (diverse) candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "operator_name": "diversity-guided mutation (LLEGO_MUT)",
            "operator_type": "mutation",
            "operator_description": "Randomly samples ν parents uniformly from the population; constructs few-shot prompts with serialized parent trees (no fitness in prompt), queries the LLM to generate λ' candidate offspring {(õ_j)} and records their negative log-probabilities s(õ_j) = -log p(õ_j | S). Sampling weights are computed as θ_j = exp(s(õ_j)/τ) / Σ_i exp(s(õ_i)/τ), where τ is the temperature hyperparameter; lower τ emphasizes low-prob (diverse) candidates. The operator then samples λ offspring from Cat(θ). The log-probabilities are justified as correlating with structural and functional distance (TED correlation -0.85 reported).",
            "is_learned_or_adaptive": true,
            "learning_mechanism": "In-context generation by an LLM with selection guided by candidate log-probabilities and temperature τ (adaptive, non-gradient parameterization).",
            "operator_representation": "Sampling-and-selection over LLM-induced candidate distribution; operator represented implicitly as a parameterized sampling distribution (via LLM prompt) with selection weights derived from log-probabilities.",
            "domain_type": "combinatorial (decision trees represented as natural-language nested dictionaries / text)",
            "context_dependent": true,
            "modality_specific": false,
            "compositional": false,
            "performance_metric": "Population diversity (median L1 functional-distance), parent-offspring distance (TED), final task performance (balanced accuracy / MSE), and correlation between log-prob and structural distance (TED).",
            "performance_learned_operator": "As part of full LLEGO algorithm: final balanced accuracy examples (depth=4) listed in Table 1 (e.g., Credit 0.684, Diabetes 0.721). Ablation over τ ∈ {5,10,25,50} showed lower τ increases population diversity and parent-offspring distance; no single numeric isolated for mutation-only performance beyond these dynamics plots.",
            "performance_fixed_operator": "Baseline mutation in GATree: random structural mutation (e.g., subtree replacement); GATree end-to-end balanced accuracy results (depth=4) e.g. Credit 0.658, Diabetes 0.675 (Table 1/10).",
            "performance_improvement": "Mutation guided by log-probabilities improves population diversity and contributes to higher final performance when combined with fitness-guided crossover — ablations show the tandem of XO+MUT outperforms using only one operator. Exact numeric operator-only improvement not isolated; end-to-end improvements versus GATree up to ~0.075 absolute in balanced accuracy on some datasets.",
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": "Population diversity measured as median L1 functional-distance; structural novelness correlated with candidate log-probabilities (reported Pearson correlation coefficient = -0.85 between log-prob and Tree Edit Distance (TED)).",
            "transfer_learning": false,
            "transfer_results": null,
            "computational_cost": "Requires λ' LLM generations per mutation operation (multiple candidate proposals) and evaluation of log-probabilities; increased compute relative to single random mutation. Paper notes higher LLM inference costs overall and suggests inference-acceleration and quantization as mitigation.",
            "population_size": "25 (default in main experiments).",
            "cold_start_addressed": true,
            "operator_specialization": "Yes — by adjusting τ the mutation operator specializes between proposing more likely (conservative) vs low-probability (diverse) offspring; empirical results show τ modulates diversity and complements crossover.",
            "key_findings": "Using LLM log-probabilities to prioritize low-likelihood offspring provides an effective, controllable mechanism to increase structural/functional diversity; low τ yields higher diversity and helps escape local optima when combined with fitness-guided crossover.",
            "limitations_or_failures": "Higher computational cost due to multiple candidate generations; choice of τ requires tuning (too high → uniform sampling reduces diversity benefit; too low may produce very unlikely/unhelpful candidates); operator effectiveness depends on LLM quality.",
            "uuid": "e1986.1"
        },
        {
            "name_short": "LLEGO (LLM-based operators)",
            "name_full": "LLEGO: LLM-Enhanced Genetic Operators (fitness-guided crossover + diversity-guided mutation)",
            "brief_description": "An end-to-end GP framework that replaces conventional structural variation operators with semantically-aware operators realized via LLM prompting: fitness-guided crossover (α) and diversity-guided mutation (τ), with trees represented in natural language and operators controlled by hyperparameters to trade off exploration and exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "operator_name": "LLM-based variation operators (combined LLEGO_XO and LLEGO_MUT)",
            "operator_type": "variation (crossover + mutation)",
            "operator_description": "Represent decision trees as nested natural-language dictionaries and frame genetic operations as LLM prompts; crossover is conditioned on a target fitness f* computed from sampled parents and α, while mutation generates multiple candidate offspring and selects via a softmax over negative log-probabilities with temperature τ. Both operator types use in-context examples (parents + task context) to exploit LLM semantic priors and allow higher arity operations.",
            "is_learned_or_adaptive": true,
            "learning_mechanism": "In-context learning + parameterized sampling via hyperparameters α (fitness target) and τ (diversity temperature); adaptation occurs online through conditioning on the population and sampling distributions, not via gradient-based training of operator parameters.",
            "operator_representation": "Prompt-conditioned probability distributions induced by an LLM; selection mechanism for mutation uses log-prob-derived weights; parent sampling uses roulette-wheel (fitness-proportionate) for crossover and uniform for mutation.",
            "domain_type": "combinatorial decision-tree induction (trees serialized to natural language / text prompts); experiments on tabular classification & regression.",
            "context_dependent": true,
            "modality_specific": false,
            "compositional": true,
            "performance_metric": "Balanced accuracy (classification), mean squared error (regression), normalized population fitness, population diversity (median L1 functional-distance), tree edit distance (TED) and correlation with log-probabilities.",
            "performance_learned_operator": "End-to-end LLEGO (α=0.1, τ=10, ν=4, N=25, G=25) outperforms baselines on benchmarks: Table 1 (depth=4) shows LLEGO achieves e.g. Breast 0.951, Compas 0.663, Credit 0.684, Diabetes 0.721, Heart 0.751, Liver 0.676, Vehicle 0.929 (balanced accuracy). LLEGO also yields lower generalization gap (Table 6: average gap 0.043 for LLEGO vs higher for many baselines).",
            "performance_fixed_operator": "GATree (GP with conventional structural operators) results (depth=4) e.g. Breast 0.941, Compas 0.650, Credit 0.658, Diabetes 0.675, Heart 0.676, Liver 0.633, Vehicle 0.895 (Table 1/10). Comparisons with a heavily budgeted GATree (N=100, G=200) still favor LLEGO (Table 10/11).",
            "performance_improvement": "End-to-end LLEGO produced absolute balanced-accuracy improvements up to ~0.075 (7.5 percentage points) on some datasets compared to GATree; LLEGO achieved top average rank (1.0) across classification benchmarks at depth=4 (Table 1).",
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": "Population diversity = median L1 distance of functional signatures; correlation between offspring log-prob and TED reported (Pearson r = -0.85) indicating low-prob candidates are structurally diverse.",
            "transfer_learning": false,
            "transfer_results": null,
            "computational_cost": "Paper reports increased computational footprint due to LLM inference; in reported runtime tables LLEGO wall-clock is higher than some lightweight baselines (Appendix D.6), but LLEGO often requires fewer functional evaluations and achieves better solutions for the same or comparable budget in many settings. Example runtime entries (Appendix): depth=4 aggregate runtimes quoted in Table 12/13 (see paper) and discussion recommending inference acceleration and quantization.",
            "population_size": "Primarily N=25 in main experiments; ablations and comparisons include GATree with N=25 and N=100 (larger budgets).",
            "cold_start_addressed": true,
            "operator_specialization": "Operators adapt via α and τ to focus either on higher-fitness exploitation (crossover) or on diverse exploration (mutation); ablations show both operators are necessary for best performance and that restricting arity (ν=2) is suboptimal.",
            "key_findings": "Integrating LLM semantic priors into GP variation operators substantially improves search efficiency and generalization of evolved decision trees; fitness conditioning (α) steers exploitation while log-probability-based selection (τ) enables controllable diversity, and combining both yields best results.",
            "limitations_or_failures": "Higher LLM inference costs; reduced population diversity when crossover is overly exploitative (high α); extrapolation beyond reliable regions for large α causing poorer offspring; potential concerns about LLM memorization and bias (investigated via no-semantics ablations and fairness-regularized objectives).",
            "uuid": "e1986.2"
        },
        {
            "name_short": "GATree (baseline)",
            "name_full": "GATree (conventional GP for decision tree induction)",
            "brief_description": "A conventional genetic programming implementation for decision tree induction using structural operators (subtree crossover, subtree mutation) and standard selection schemes (tournament etc.), used in the paper as the primary GP baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "operator_name": "structural subtree crossover & subtree mutation (GATree default)",
            "operator_type": "crossover and mutation",
            "operator_description": "Classic GP operators operating on tree structure: ν=2 subtree crossover swaps randomly chosen subtrees between parent trees; mutation performs structural node insertions/replacements sampled uniformly over allowed structural operations.",
            "is_learned_or_adaptive": false,
            "learning_mechanism": null,
            "operator_representation": "Hand-designed structural rule set (random structural transformations), effectively inducing a uniform prior over permitted structural edits.",
            "domain_type": "combinatorial (decision trees)",
            "context_dependent": false,
            "modality_specific": false,
            "compositional": false,
            "performance_metric": "Balanced accuracy (classification), MSE (regression), population fitness and diversity.",
            "performance_learned_operator": null,
            "performance_fixed_operator": "Reported end-to-end results: e.g. GATree (depth=4) balanced accuracy per Table 1: Breast 0.941, Compas 0.650, Credit 0.658, Diabetes 0.675, Heart 0.676, Liver 0.633, Vehicle 0.895; GATree (large budget N=100,G=200) reported in Table 10 produced slightly improved numbers but still often worse than LLEGO.",
            "performance_improvement": null,
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": "Population diversity reported (GATree tends to maintain higher diversity than LLEGO in experiments, due to unguided structural perturbations).",
            "transfer_learning": false,
            "transfer_results": null,
            "computational_cost": "Lower per-operator cost (no LLM calls). Reported runtimes in Appendix D.6 show GATree runtime lower than LLEGO in some aggregated tables (see paper).",
            "population_size": "Experiments with N=25; larger-budget comparisons used N=100, G=200.",
            "cold_start_addressed": false,
            "operator_specialization": null,
            "key_findings": "Conventional structural operators are semantically agnostic; while they maintain diversity, they are less efficient at finding high-quality trees within limited evaluation budgets compared to LLEGO.",
            "limitations_or_failures": "Semantically unaware structural changes can be disruptive (rough genotype-phenotype mapping), leading to less efficient search and lower-quality final models for the same budget.",
            "uuid": "e1986.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language model crossover: Variation through few-shot prompting",
            "rating": 2
        },
        {
            "paper_title": "Evolution through large models",
            "rating": 2
        },
        {
            "paper_title": "Geometric semantic genetic programming",
            "rating": 2
        },
        {
            "paper_title": "Approximating geometric crossover by semantic backpropagation",
            "rating": 1
        },
        {
            "paper_title": "LLMatic: Neural architecture search via large language models and quality diversity optimization",
            "rating": 1
        }
    ],
    "cost": 0.026563749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DECISION TREE INDUCTION THROUGH LLMS VIA SEMANTICALLY-AWARE EVOLUTION
18 Mar 2025</p>
<p>Tennison Liu 
Nicolas Huynh 
Mihaela Van Der Schaar </p>
<p>DAMTP
University of Cambridge Cambridge
UK</p>
<p>1 2 3 1 2 3</p>
<p>DECISION TREE INDUCTION THROUGH LLMS VIA SEMANTICALLY-AWARE EVOLUTION
18 Mar 2025F0E2C1CCCE750023901AC9951D621FF1arXiv:2503.14217v1[cs.LG]
Decision trees are a crucial class of models offering robust predictive performance and inherent interpretability across various domains, including healthcare, finance, and logistics.However, current tree induction methods often face limitations such as suboptimal solutions from greedy methods or prohibitive computational costs and limited applicability of exact optimization approaches.To address these challenges, we propose an evolutionary optimization method for decision tree induction based on genetic programming (GP).Our key innovation is the integration of semantic priors and domain-specific knowledge about the search space into the optimization algorithm.To this end, we introduce LLEGO, a framework that incorporates semantic priors into genetic search operators through the use of Large Language Models (LLMs), thereby enhancing search efficiency and targeting regions of the search space that yield decision trees with superior generalization performance.This is operationalized through novel genetic operators that work with structured natural language prompts, effectively utilizing LLMs as conditional generative models and sources of semantic knowledge.Specifically, we introduce fitness-guided crossover to exploit high-performing regions, and diversity-guided mutation for efficient global exploration of the search space.These operators are controlled by corresponding hyperparameters that enable a more nuanced balance between exploration and exploitation across the search space.Empirically, we demonstrate across various benchmarks that LLEGO evolves superior-performing trees compared to existing tree induction methods, and exhibits significantly more efficient search performance compared to conventional GP approaches.</p>
<p>INTRODUCTION</p>
<p>Decision trees are fundamental models, which are widely utilized across various domains, including finance, healthcare, and bioinformatics (Morgan &amp; Sonquist, 1963;Che et al., 2011;Soleimanian et al., 2012).These hierarchical models recursively partition the feature space, creating a tree-like structure where internal nodes represent decision rules based on feature values, and leaf nodes correspond to class labels or predicted values.Decision trees are particularly appealing as they offer both predictive accuracy and interpretability, which have stood the test of time against recently developed black-box predictive models (Borisov et al., 2022;Grinsztajn et al., 2022).</p>
<p>However, decision tree induction represents a challenging optimization problem.Finding the optimal tree given a training dataset is NP-complete (Laurent &amp; Rivest, 1976), often necessitating the use of heuristic algorithms (Quinlan, 1986).While computationally efficient, these heuristics yield approximate, locally greedy solutions that sacrifice some degree of performance and global optimality (Rokach &amp; Maimon, 2005).Exact optimization methods have been developed to address these limitations, but they face their own constraints.Their computational complexity typically scales exponentially with problem size, limiting their applicability to restricted search spaces, and specific problem types (e.g., binary classification) (Verwer &amp; Zhang, 2019;Lin et al., 2020).</p>
<p>Genetic programming (GP) is a class of evolutionary algorithms which offers a promising middle ground for decision tree induction, balancing computational efficiency with global optimization Selection &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 i C q 9 7 N 8 8 W X g W V 0 9 Q r T X w j x + W E E = " &gt; A A A C n X i c f Z F t a x N B E M f 3 z m p r f G i q 7 / R F F 4 O Q o o S 7 U t q + E Y I S 0 J K 2 E Z s m k I t h b z O X L t 1 7 Y H e u N S y X T + U n 8 Z 3 f x r 1 r w J o W B x b + / P 6 z s 7 M z Y S a F R s / 7 7 b g P 1 h 4 + W t 9 4 X H v y 9 N n z z f r W i 3 O d 5 o p D n 6 c y V c O Q a Z A i g T 4 K l D D M F L A 4 l D A I L z + V / u A K l B Z p c o b z D M Y x m y U i E p y h R Z P 6 z w C F n I I J Y o Y X n E n T K 4 r v p j l 7 5 + 8 U 9 A O 9 h U t q W c D z 7 C 8 + L S Y B w g 8 0 w 9 N 7 L H P c P 7 M 4 q K 2 U q Y o H E i J k S q X X t C q B a L 5 1 u k X z P w 2 9 p y c 7 d L F Y T O o N r + V V Q e 8 K f y k a Z B m 9 S f 1 X M E 1 5 H k O C X D K t R 7 6 X 4 d g w h Y J L K G p B r i F j / J L N Y G R l w m L Q Y 1 N N t 6 B v L Z n S K F X 2 J E g r e v u G Y b H W 8 z i 0 m W X P e t U r 4 X 3 e K M f o c G x E k u U I C b 9 5 K M o l x Z S W q 6 J T o Y C j n F v B u B K 2 V 8 o v m G I c 7 U J r d g j + 6 p f v i v P d l r / f 2 v u 6 1 2 h / X I 5 j g 7 w m b 0 i T + O S A t M l n 0 i N 9 w p 1 X T t v 5 4 h y 5 2 2 7 H 7 b o n N 6 m u s 7 z z k v w T 7 u A P A E T N P g = = &lt; / l a t e x i t &gt; P(g+1) = P (g) [ OXO [ OMUT P (g+1)  SEL( P(g+1) , N)</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j Y O K f l s V y 5 T / + b g T G R E c I 2 g 1 z U Y = " &gt; A A A C K H i c b V B N S w M x E M 3 6 b f 2 q e v Q S L I K n s i u i 4 s W i H j w q 9 A u 6 p c y m a Q 1 m s 0 s y K 5 Z l / T d e / C t e R B T p 1 V 9 i t u 1 B q w 8 C j / d m J j M v i K U w 6 L p D Z 2 Z 2 b n 5 h c W m 5 s L K 6 t r 5 R 3 N y q m y j R j N d Y J C P d D M B w K R S v o U D J m 7 H m E A a S N 4 K 7 i 9 x v 3 H N t R K S q O I h 5 O 4 S + E j 3 B A K 3 U K Z 7 5 y B 8 w r W o Q S q h + 9 m h B f b 8 w l r u A d j a e 0 s w P A W 8 Z y P Q y 6 4 w 9 z F u y T r H k l t 0 R 6 F / i T U i J T H D d K b 7 5 3 Y g l I V f I J B j T 8 t w Y 2 y l o F E z y r O A n h s f A 7 q D P W 5 Y q C L l p p 6 N D M 7 p n l S 7 t R d o + h X S k / u x I I T R m E A a 2 M t / X T H u 5 + J / X S r B 3 0 k 6 F i h P k i o 0 / 6 i W S Y k T z 1 G h X a M 5 Q D i w B p o X d l b J b 0 M D Q Z l u w I X j T J / 8 l 9 Y O y d 1 Q + v D k s V c 4 n c S y R H b J L 9 o l H j k m F X J F r U i O M P J E X 8 k 4 + n G f n 1 f l 0 h u P S G W f S s 0 1 + w f n 6 B o U n q L o = &lt; / l a t e x i t &gt;</p>
<p>Training dataset: Dtrain &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 z / 4 A 1 v T a n k r r d V X V x W b 7 + q o Q j o = " &gt; A A A C Q H i c b V B N S x x B E O 3 x O x s 1 q z l 6 a V w C O Y R l R h Y j n k Q v H g 2 4 r r C z L D W 9 N d p s d 8 / Q X S M u w / j P v P g T v O W c i 4 e E k K u n 9 H 6 A n w U F j / e q q F c v y Z V 0 F I Y / g 7 n 5 h c W l 5 Z U P t Y + r a + u f 6 h u b Z y 4 r r M C 2 y F R m z x N w q K T B N k l S e J 5 b B J 0 o 7 C T D o 7 H e u U L r Z G Z O a Z R j T 8 O F k a k U Q J 7 q 1 z s x 4 T W V p + C G 3 K E G Q 1 I 4 X s U a 6 F K A K o + q f R 7 H t e l U i k C F R W 5 A o / t W 3 d w 8 S b n N / E n N B + h E 0 y v 9 e i N s h p P i b 0 E 0 A w 0 2 q 5 N + / T 4 e Z K L Q a E g o c K 4 b h T n 1 S r D e k M K q F h c O c x B D u M C u h x M L v X I S Q M W / e G b A 0 8 z 6 N s Q n 7 P O N E r R z I 5 3 4 y f F j 7 r U 2 J t / T u g W l e 7 1 S m r w g N G J 6 K C 0 U p 4 y P 0 + Q D a V G Q G n k A w k r v l Y t L s C D I Z 1 7 z I U S v X 3 4 L z n a a 0 W 6 z 9 a P V O D i c x b H C t t g 2 + 8 o i 9 p 0 d s G N 2 w t p M s F v 2 i / 1 m f 4 K 7 4 C H 4 G / y b j s 4 F s 5 3 P 7 E U F j / 8 B k M y x k A = = &lt; / l a t e x i t &gt;</p>
<p>Task semantics C : feature names, problem desc.</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 / 4 Y i s K q / b n c c a E Y 7 E k d Q l p A 6 E g = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x K U I 9 B L 5 4 k A f O A Z A m z k 9 5 k z O z s M j M r h J A v 8 O J B E a 9 + k j f / x k m y B 0 0 s a C i q u u n u C h L B t X H d b y e 3 t r 6 x u Z X f L u z s 7 u 0 f F A + P m j p O F c M G i 0 W s 2 g H V K L j E h u F G Y D t R S K N A Y C s Y 3 c 7 8 1 h M q z W P 5 Y M Y J + h E d S B 5 y R o 2 V 6 v e 9 Y s k t u 3 O Q V e J l p A Q Z a r 3 i V 7 c f s z R C a Z i g W n c 8 N z H + h C r D m c B p o Z t q T C g b 0 Q F 2 L J U 0 Q u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 9 u J q Y W Q f C j j X U N O v x h x V d / v 0 b U = " &gt; A A A C J 3 i c b V D L S s N A F J 3 4 r P V V d e l m s A i 6 K Y m I u h L R j c s K t g p N L Z P p T R 2 c Z M L M j V h C + j V u / B U 3 g o r o 0 j 9 x 0 n b h 6 8 D A 4 Z x z m X t P k E h h 0 H U / n I n J q e m Z 2 d J c e X 5 h c W m 5 s r L a N C r V H B p c S a U v A 2 Z A i h g a K F D C Z a K B R Y G E i + D m p P A v b k E b o e J z 7 C f Q j l g v F q H g D K 3 U q R z 6 C H e Y 1 V V S G 6 i Q d o G L I p t T 3 y + P L N Q A h l o h Y n j N m c z q + V W 2 1 d v O 6 W A w 6 F S q b s 0 d g v 4 l 3 p h U y R j 1 T u X Z 7 y q e R h A j l 8 y Y l u c m 2 M 6 Y R s E l 5 G U / N Z A w f s N 6 0 L I 0 Z h G Y d j a 8 M 6 e b V u n S U G n 7 Y q R D 9 f t E x i J j + l F g k 8 W y 5 r d X i P 9 5 r R T D g 3 Y m 4 i R F i P n o o z C V F B U t S q N d o Y G j 7 F v C u B Z 2 V 8 q v m W Y c b b V l W 4 L 3 + + S / p L l T 8 / Z q u 2 e 7 1 a P j c R 0 l s k 4 2 y B b x y D 4 5 I q e k T h q E k 3 v y S F 7 I q / P g P D l v z v s o O u G M Z 9 b I D z i f X 1 w X p t s = &lt; / l a t e x i t &gt;</p>
<p>Pop. of decision trees P (g)   Section 3.2 Section 3.3 Section 3.4</p>
<p>Figure 1: LLEGO Overview.In each generation g ∈ [G], a population of trees P (g) is evolved through crossover O MUT = LLEGO XO (P (g) , C; α) and mutation O MUT = LLEGO MUT (P (g) , C; τ ).Subsequently, the offsprings O XO ∪ O MUT are evaluated for fitness on D train ; and selection preserves the top-N trees, P (g+1) ← SEL( P(g+1) , N ), where P(g) = P (g) ∪ O XO ∪ O MUT .</p>
<p>of the tree structure (Koza, 1994a;b).Inspired by principles of evolution, GP algorithms evolve a population of candidate solutions through iterative application of genetic operators such as crossover and mutation.They are particularly well-suited for optimizing combinatorial problems with discrete, variable-length search spaces, as is the case in decision tree induction (Koza, 1990;Tanigawa &amp; Zhao, 2000;Kuo et al., 2007;Lahovnik, 2024).While much research in GP has focused on designing genetic operators to enhance search efficiency, these approaches face inherent limitations that constrain their exploratory effectiveness.Key challenges include the difficulty of incorporating semantic information into genetic operations-resulting in primarily structural, unguided mechanisms-and the narrow operational contexts that limit global exploration.</p>
<p>Key considerations.</p>
<p>The key insight of this work is to employ large language models (LLMs) to design semantically-aware genetic operators for decision tree induction.LLMs are powerful generative models capable of learning distributions over discrete and variable-length sequences given only few-shot examples (Radford et al., 2019;Brown et al., 2020).We utilize LLMs as the foundation for crossover and mutation operators, leveraging their encoded semantic priors to create meaningful distributions over potential offspring.Building on LLMs, our approach introduces fitness-guided crossover and diversity-guided mutation operators within a GP framework, enabling efficient search space exploration that contrasts with the structural focus and unguided nature of conventional genetic operators.By representing decision trees in natural language, we also enable higher-arity genetic operations, capable of operating over multiple trees simultaneously.</p>
<p>Contributions.Conceptually, we propose a novel GP algorithm that leverages semantic priors contained in LLMs to enhance search efficiency and performance on challenging decision tree induction problems.Technically, we introduce LLEGO (LLM-Enhanced Genetic Operators), which uses LLMs to define two key search operators: fitness-guided crossover that steers the search towards promising regions using a target fitness; and diversity-guided mutation that employs log-probabilities to evolve solutions in under-explored search regions.Empirically, on a wide range of classification and regression tabular benchmarks, we demonstrate that LLEGO significantly improves search efficiency and consistently evolves trees with superior generalization performance.</p>
<p>PRELIMINARIES</p>
<p>DECISION TREE INDUCTION</p>
<p>Decision tree induction is the problem of learning a decision tree t ∈ T from a training dataset
D train = {(x i , y i )} n i=1
, where x i ∈ X ⊆ R d denotes a d-dimensional input and y i ∈ Y denotes the output.Decision trees recursively partition the input space X into hierarchical, disjoint regions.In this work, we focus on binary decision trees, where splits partition regions in two subregions.These regions define a set of leaf nodes R = {R 1 , R 2 , ..., R L }, where each leaf R l is assigned a constant c l (Hastie et al., 2009).This in turn yields a predictor t : X → Y which is defined by t(x) = training dataset is a challenging optimization problem, since full tree optimization has been proven to be an NP-complete problem (Laurent &amp; Rivest, 1976).Greedy algorithms like CART (Breiman et al., 1984) build trees top-down, offering computational efficiency but sacrifices performance by only finding locally optimal trees.By comparison, exact optimization methods (Lin et al., 2020) provide theoretical guarantees of global optimality, but scale exponentially with problem size, and apply only to classification tasks and objective functions of specific forms.</p>
<p>GENETIC PROGRAMMING</p>
<p>Genetic Programming (GP) is a class of evolutionary algorithms for searching combinatorial spaces and offers a flexible middle ground between greedy and exact optimization methods.The fundamental objective of GP is to evolve trees t ∈ T to maximize a fitness function f : T → R, where T is the combinatorial search space (Koza, 1994a).In GP, each individual is described by the tuple (t, f (t)) containing a tree and its fitness.We denote this population of
N individuals P = {(t 1 , f (t 1 )), (t 2 , f (t 2 )), . . . , (t N , f (t N ))}, with N ∈ N.
The algorithm evolves the population across G ∈ N generations.In each generation g ∈ [G], the population P (g) undergoes three key genetic operations: selection and two variation operators (crossover and mutation).</p>
<p>Selection.The selection mechanism preserves performant trees across generations, placing selection pressure on sufficient exploitation and ensures convergence (Goldberg, 1989).The N -ary selection operator is defined as SEL : T N × R N → ∆(T N ), where ∆(T N ) represents the probability simplex over T N .Often, selection operators implicitly create this probability distribution over P, wherein trees with higher fitness are more likely to be preserved.</p>
<p>Crossover.The crossover operator combines the genetic material of multiple candidate trees to generate performant offspring (Langdon &amp; Poli, 2013).Crossover is an ν-ary operator, denoted XO : T ν → ∆(T ), taking in ν parents to generate an offspring o ∈ T , sampled as, o ∼ p XO (• | S), where S is usually a pair of parents (ν = 2) sampled uniformly from the population.XO induces the offspring distribution p XO , which can be interpreted as a uniform distribution over all trees producible by the crossover operator.For example, a popular version of XO is subtree crossover, where randomly selected subtrees from two parent trees are swapped (Koza, 1994a).</p>
<p>Mutation.The mutation operator promotes global exploration, thus mitigating premature convergence to local optima (Goldberg, 1989).An ν-ary mutation operator MUT : T ν → ∆(T ) performs random modifications to parents to generate an offspring o ∼ p MUT (• | S).Traditionally, mutation operates on a single parent tree (ν = 1) and p MUT is uniform over the set of trees that can be generated through a mutation operator (e.g., random insertion or replacement of nodes).</p>
<p>DESIDERATA</p>
<p>We can conceptualize a variation operator, v, as implicitly defining a sampling distribution p v (o | S) that depends on the parent trees S and its rules.More formally,
p v (o | S) = p v (o | S, g)p v (g | S) dg, where p v (g | S)
represents the prior distribution of applying a specific genetic operation (e.g., pairs of nodes when v corresponds to subtree crossover), and p v (o | S, g) is the likelihood of an offspring given parents and genetic operation (generally, 1 if producible, and 0 otherwise).As such, genetic operators can be viewed as sampling from a posterior distribution over offspring, where the prior is encoded through the operator's design.While these variation mechanisms are core to GP, they present notable limitations that negatively impact search performance, leading to the following desiderata:</p>
<p>• Semantic priors: Conventional variation operators encode inductive biases on structural properties, crucially lacking any considerations for tree semantics, i.e., p v (g|S) is independent of the semantics.This can be problematic, as small changes to the structure can lead to disruptive changes to the functional behavior (an issue known as rough genotype-phenotype mapping Rothlauf et al. (2011)).This can be improved through integration of problem semantics into the prior p v (g | S, C), where C represents the semantics, leading to the sampling distribution p v (o | S, C). • Guided variations: Generally, conventional variation mechanisms place an uninformative distribution over any genetic operations.For example, they might consider any structural operations as equally likely, i.e., p(g | S) is uniform.This lack of search guidance can lead to inefficient exploration and slower convergence, which can be improved with more informative priors that prioritize offsprings that are more semantically meaningful, or likely to improve fitness and cover unexplored regions of the search space.• Broader context: The designs of existing operators often constrain the arity of allowed operations (e.g., it is difficult to define valid operators on &gt; 2 trees), restricting offsprings to local exploration.In contrast, including more parents in genetic operations can improve global exploration.</p>
<p>A line of work has aimed to address some of these desiderata.Notably, previous works in semantic GP have attempted to address the first two limitations with variation operators, which consider the semantics of solutions (Krawiec &amp; Lichocki, 2009;Moraglio et al., 2012;Krawiec &amp; Pawlak, 2013).</p>
<p>In the semantic GP literature, a solution's semantics typically refers to the functional output of a solution, i.e., h(t) = [t(x 1 ), t(x 2 ), . . ., t(x n )] ∈ R n .In contrast, our work uses the term to describe domain knowledge about the solution space encoded in the LLM.Additionally, semantic GP is limited to application-specific definitions of semantics that restrict its broader applicability.Crucially, no comparable semantically-aware method has been developed for decision tree induction.</p>
<p>3 LLEGO: GENETIC OPERATORS WITH SEMANTIC PRIORS Designing genetic operators that satisfy the aforementioned desiderata using conventional methods has proven difficult.In this work, we build on the insight that LLMs are powerful generative models that can be employed as semantically aware variation operators.Indeed, LLMs possess several properties that make them appealing (Meyerson et al., 2023;Lehman et al., 2023).Firstly, we utilize LLMs as a source of semantic prior p LLM (g | S, C), as they contain rich semantic knowledge of the problem and tree solutions, forming the basis of variation operators (Xie et al., 2021).Secondly, they are able to reason over and learn from in-context examples to identify high-potential patterns in candidate trees and produce guided variations towards desired regions.Lastly, their relatively large context window facilitates utilization of broader context, increasing arity of feasible genetic operations.Building on this semantic prior, we design genetic operators through mechanisms that incorporate fitness information and log probabilities of generated offspring, to further improve exploration and exploitation abilities.</p>
<p>Method overview.At a high level, LLEGO represents trees and frames genetic operations in natural language.Specifically, each genetic operation is realized through a distinct prompt which contains parent trees, semantics, and auxiliary information.We introduce fitness-guided crossover LLEGO XO that performs in-context learning over solutions and their fitness, and generates offspring conditioned on a desired fitness f * , to steer evolution towards high-performing regions.Additionally, we propose diversity-guided mutation LLEGO MUT , which uses the log probabilities of candidate offspring to construct a weighted sampling distribution, prioritizing efficient exploration that cover unexplored search regions.We note that the level of fitness-or diversity-guidance is intentionally controllable through two hyperparameters, α and τ that correspond to different degrees of exploitation vs. exploration.An overview of our method is visualized in Figure 1.</p>
<p>LLEGO PROMPT DESIGN</p>
<p>The genetic operations are performed through natural language queries to the LLM.While the specific structure of each query differs, they are constructed from three essential components.For an extended description of prompts and examples, please refer to Appendix B.</p>
<ol>
<li>Task context.Denoted as C and encapsulates the semantic description of the problem, including semantic and statistical descriptions of the input space features X , output label Y, and characteristics of the overall dataset, e.g., number of samples or features.2. Parent solutions.This contains the solution representation and fitness of each parent, which are serialized into natural language and provided as few-shot examples in each genetic operation.3. Task-specific instructions.For each genetic operator, we include task-specific instructions on offspring generation and guidelines on the format of the response.</li>
</ol>
<p>Fitness-GUIDED CROSSOVER OPERATOR</p>
<p>Traditional crossover operators are not semantically aware, as they randomly select subtrees from parents to be recombined into an offspring.This ignores patterns in the parents, introducing the possibility for performant substructures to be destroyed through random perturbations.Additionally, they do not make use of parent fitness explicitly to guide offspring generation (i.e., no fitness guidance), foregoing any informative signals on correlations between fitness and solution structure.We seek to address these factors in our fitness-guided crossover operator.More specifically, the crossover operation includes three steps: (1) sampling a subset of parents, weighted by parent fitness, (2) compute desired fitness f * based on parent fitness, (3) constructing the prompt and querying the LLM to generate offsprings, conditioning on f * (see Figure 2).</p>
<p>Parent sampling.Each crossover operation is conditioned on parents, which are sampled from the current population.We utilize a roulette-wheel mechanism (Blickle &amp; Thiele, 1996) to favour existing solutions with high fitness.Specifically, we aim to sample a set of ν ∈ N parents for each crossover operation, where the sampling weights θ = (θ 1 , . . ., θ N ) are proportional to the solutions' fitness.These weights define a categorical distribution Cat N (θ), based on which we sample parents without replacement.Intuitively, solutions with higher fitness are more likely to be involved in genetic operations.We use S k to represent the set of parents sampled for operation k ∈ [κ], where κ ∈ N is the number of crossover operations performed.
𝑜 Population of trees &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v X O O b U o F E S y 3 G / s Z D F i 6 3 r X g H c g = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W I S 6 s C R S 1 G W x i C 4 K r W A f 0 J Y y m U 7 b o Z N M m L k R S 4 g / 4 c Z f c e N C E b f i z r 9 x m n a h r Q c G D u e c y 5 1 7 3 I A z B b b 9 b S w s L i 2 v r K b W 0 u s b m 1 v b 5 s 5 u T Y l Q E l o l g g v Z c L G i n P m 0 C g w 4 b Q S S Y s / l t O 4 O i 2 O / f k e l Y s K / h V F A 2 x 7 u + 6 z H C A Y t d c z j F t B 7 i I p S K C V 0 M H 7 I J g p A V C p d X p X j T j R J N M p x f N Q x M 3 b O T m D N E 2 d K M m i K S s f 8 a n U F C T 3 q A + F Y q a Z j B 9 C O s A R G O I 3 T r V D R A J M h 7 t O m p j 7 2 q G p H y V m x d a i V r t U T U j 8 f r E T 9 P R F h T 6 m R 5 + q k h 2 G g Z r 2 x + J / X D K F 3 3 o 6 Y H 4 R A f T J Z 1 A u 5 B c I a d 2 R 1 m a Q E + E g T T C T T f 7 X I A E t M Q D e Z 1 i U 4 s y f P k 9 p J z j n N 5 W / y m c L F t I 4 U 2 k c H K I s c d I Y K 6 B p V U B U R 9 I i e 0 S t 6 M 5 6 M F + P d + J h E F 4 z p z B 7 6 A + P z B 0 Z Y o K 8 = &lt; / l a t e x i t &gt; Crossover (LLEGOXO) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c E 6 P F Z j Q Q D h E X j N A 7 h N o F 7 K w x n o = " &gt; A A A C C H i c b V D J S g N B E O 1 x j X G L e v R g Y x D i J c x I U I / B X D x J B L N A J g w 9 n U r S p G e h u 0 Y M Q 4 5 e / B U v H h T x 6 i d 4 8 2 / s L A d N f F D w e K + K q n p + L I V G 2 / 6 2 l p Z X V t f W M x v Z z a 3 t n d 3 c 3 n 5 d R 4 n i U O O R j F T T Z x q k C K G G A i U 0 Y w U s 8 C U 0 / E F l 7 D f u Q W k R h X c 4 j K E d s F 4 o u o I z N J K X O 0 J P U F e L g L o I D 4 i Y V h i O v J u C i 3 1 A d u r l 8 n b R n o A u E m d G 8 m S G q p f 7 c j s R T w I I k U u m d c u x Y 2 y n T K H g E k Z Z N 9 E Q M z 5 g P W g Z G r I A d D u d P D K i J 0 b p 0 G 6 k T I V I J + r v i Z Q F W g 8 D 3 3 Q G D P t 6 3 h u L / 3 m t B L u X 7 V S E c Y I Q 8 u m i b i I p R n S c C u 0 I B R z l 0 B D G l T C 3 U t 5 n i n E 0 2 W V N C M 7 8 y 4 u k f l Z 0 z o u l 2 1 K + f D W L I 0 M O y T E p E I d c k D K 5 J l V S I 5 w 8 k m f y S t 6 s J + v F e r c + p q 1 L 1 m z m g P y B 9 f k D Y n G Z l A = = &lt; / l a t e x i t &gt; ti ⇠ Ca tN (✓ ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g O D Z k 7 v f 3 h c T E Z l 9 / w y Z + 0 x Y V 5 o = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 r L o x m U F + 4 A 2 l M l 0 0 g 6 d T M L M j V B C P 8 O N C 0 X c + j X u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T u 9 z v P H F t R K w e c Z p w P 6 I j J U L B K F q p 1 4 8 o j h m V W X M 2 q N b c u j s H W S V e Q W p Q o D m o f v W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n l X 5 q e E L Z h I 5 4 z 1 J F I 2 7 8 b B 5 5 R s 6 s M i R h r O 1 T S O b q 7 4 2 M R s Z M o 8 B O 5 h H N s p e L / 3 m 9 F M M b P x M q S Z E r t v g o T C X B m O T 3 k 6 H Q n K G c W k K Z F j Y r Y W O q K U P b U s W W 4 C 2 f v E r a F 3 X v q n 7 5 c F l r 3 B Z 1 l O E E T u E c P L i G B t x D E 1 r A I I Z n e I U 3 B 5 0 X 5 9 3 5 W I y W n G L n G P 7 A + f w B i g e R b w = = &lt; / l a t e x i t &gt; P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W g U X M H b U M G n 8 m P d R 1 M 7 P 4 D 9 w f Z 0 = " &gt; A A A C J X i c d V D L S g M x F M 3 4 t r 6 q L t 0 E i + D G M t X i A 1 y I g r i s Y F X o l J J J 7 7 T B J D M k d 9 Q y 9 G f c + C t u X C g i u P J X T B + C z w O B w z n 3 k n N P m E h h 0 f f f v J H R s f G J y a n p 3 M z s 3 P x C f n H p 3 M a p 4 V D l s Y z N Z c g s S K G h i g I l X C Y G m A o l X I R X R z 3 / 4 h q M F b E + w 0 4 C d c V a W k S C M 3 R S I 7 8 f I N x i d i x Q g 7 U b N y B a b Y R m l w Z B b m B Z p l w Q 3 a K R i R V 1 h m L Y 5 k x m l W 4 j X / C L 5 b 1 d f 2 u P / i a l o t 9 H g Q x R a e S f g 2 b M U w U a u W T W 1 k p + g v W M G R R c Q j c X p B Y S x q 9 Y C 2 q O a q b A 1 r P + l V 2 6 5 p Q m j W L j n k b a V 7 9 u Z E x Z 2 1 G h m + x F t D + 9 n v i X V 0 s x 2 q 1 n Q i c p g u a D j 6 J U U o x p r z L a F A Y 4 y o 4 j j B v h s l L e Z o Z x d M X m X A m f l 9 L / y f l m s b R d L J + W C w e H w z q m y A p Z J e u k R H b I A T k h F V I l n N y R B / J E nI o V u z y n g i a j I t 9 C K q 2 F 6 w N c a I = " &gt; A A A C C H i c d V C 7 S g N B F J 3 1 G e N r 1 d L C w S B Y S N h N g i a F E L S x j G g e k A 3 L 7 G S S D J l 9 M H N X C E t K G 3 / F x k I R W z / B z r 9 x N o n g 8 8 A M h 3 P u 5 d 5 7 v E h w B Z b 1 b s z N L y w u L W d W s q t r 6 x u b 5 t Z 2 Q 4 W x p K x O Q x H K l k c U E z x g d e A g W C u S j P i e Y E 1 v e J 7 6 z R s m F Q + D a x h F r O O T f s B 7 n B L Q k m v u O T 6 B A S U i u R r j U + w k 4 N p H G N x C + h W d s W v m r H y p U r a K F f y b 2 H l r g h y a o e a a b 0 4 3 p L H P A q C C K N W 2 r Q g 6 C Z H A q W D j r B M r F h E 6 J H 3 W 1 j Q g P l O d Z H L I G B 9 o p Y t 7 o d Q v A D x R v 3 Y k x F d q 5 H u 6 M l 1 b / f R S 8 S + v H U O v 3 E l 4 E M X A A j o d 1 I s F h h C n q e A u l 4 y C G G l C q O R 6 V 0 w H R B I K O r u s D u H z U v w / a R T y 9 n G + d F n K V c 9 m c W T Q L t p H h 8 h G J 6 i K L l A N 1 R F F t + g e P a I n 4 8 5 4 M J 6 N l 2 n p n D H r 2 U H f Y L x + A O C i m K I = &lt; / l a t e x i t &gt; S = {t1, t2, t3} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R o W r S L d Q F m Q t O i x x 1 O m 1 e K I Q a W Y = " &gt; A A A B + H i c d V D L S s N A F J 3 U V 6 2 P R l 2 6 G S y C q 5 B o 0 X Z X d O O y g n 1 A G 8 p k O m m H T i Z h 5 k a s o V / i x o U i b v 0 U d / 6 N 0 4 f g 8 8 C F w z n 3 c u 8 9 Q S K 4 B t d 9 t 3 J L y y u r a / n 1 w s b m 1 n b R 3 t l t 6 j h V l D V o L G L V D o h m g k v W A A 6 C t R P F S B Q I 1 g p G F 1 O / d c O U 5 r G 8 h n H C / I g M J A 8 5 J W C k n l 3 s A r u F T J P I b J O D S c 8 u u U 6 5 W n F P q v g 3 8 R x 3 h h J a o N 6 z 3 7 r 9 m K Y R k 0 A F 0 b r j u Q n 4 G V H A q W C T Q j f V L C F 0 R A a s Y 6 g k E d N + N j t 8 g g + N 0 s d h r E x J w D P 1 6 0 R G I q 3 H U W A 6 I w J D / d O b i n 9 5 n R T C i p 9 x m a T A J J 0 v C l O B I c b T F H C f K 0 Z B j A 0 h V H F z K 6 Z D o g g F k 1 X B h P D 5 K f 6 f N I 8 d 7 9 Q p X 5 V L t f N F H H m 0 j w 7 Q E f L Q G a q h S 1 R H D U R R i u 7 R I 3 q y 7 q w H 6 9 l 6 m b f m r M X M H v o G 6 / U D N N y U I Q = = &lt; / l a t e x i t &gt;
sampling &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 g U K M 8 x h 6 w W E J D e 6 v 1 g I y w w 8
g m o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l B + x 7 / X L F r b p z k F X i 5 a Q C O R r 9 8 l d v E L M 0 4 g q Z p M Z 0 P T d B P 6 M a B Z N 8 W u q l h i e U j e m Q d y 1 V N O L G z + a n T s m Z V Q Y k j L U t h W S u / p 7 I a G T M J A p s Z 0 R x Z J a 9 m f i f 1 0 0 x v P Y z o Z I U u W K L R W E q C c Z k 9 j c Z C M 0 Z y o k l l G l h b y V s R D V l a N M p 2 R C 8 5 Z d X S e u i 6 l 1 W a / e 1 S v 0 m j 6 M I J 3 A K 5 + D B F d T h D h r Q B A Z D e I Z X e H O k 8 + K 8 O x + L 1 o K T z x z D H z i f P w i G j a U = &lt; / l a t e x i t &gt; t1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V f Q l b j z 1 4 o E v J + w N q J l C E m c z K x A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K U Y 9 F L x 4 r 2 l p o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U N n G q G W + x W M a 6 E 1 D D p V C 8 h Q I l 7 y S a 0 y i Q / D E Y 3 8 z 8 x y e u j Y j V A 0 4 S 7 k d 0 q E Q o G E U r 3 W O / 1 i 9 X 3 K o 7 B 1 k l X k 4 q k K P Z L 3 / 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J p 6 V e a n h C 2 Z g O e d d S R S N u / G x + 6 p S c W W V A w l j b U k j m 6 u + J j E b G T K L A d k Y U R 2 b Z m 4 n / e d 0 U w y s / E y p J k S u 2 W B S m k m B M Z n + T g d C c o Z x Y Q p k W 9 l b C R l R T h j a d k g 3 B W 3 5 5 l b R r V e + i W r + r V x r X e R x F O I F T O A c P L q E B t 9 C E F j A Y w j O 8 w p s j n R f n 3 f l Y t B a c f O Y Y / s D 5 / A E K C o 2 m &lt; / l a t e x i t &gt; t2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g H 6 o 9 C e D 6 K K p O T N n 4 A x U g h U 0 f x A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 q M e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B q w 8 G H u / N M D M v S K Q w 6 L p f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x + 0 T J x q x p s s l r H u B N R w K R R v o k D J O 4 n m N A o k b w f j m 5 n f f u T a i F g 9 4 C T h f k S H S o S C U b T S P f b P + + W K W 3 X n I H + J l 5 M K 5 G j 0 y 5 + 9 Q c z S i C t k k h r T 9 d w E / Y x q F E z y a a m X G p 5 Q N q Z D 3 r V U 0 Y g b P 5 u f O i U n V h m Q M N a 2 F J K 5 + n M i o 5 E x k y i w n R H F k V n 2 Z u J / X j f F 8 M r P h E p S 5 I o t F o W p J B i T 2 d 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 n Z E P w l l / + S 1 p n V e + i W r u r V e r X e R x F O I J j O A U P L q E O t 9 C A J j A Y w h O 8 w K s j n W f n z X l f t B a c f O Y Q f s H 5 + A Y L j o 2 n &lt; / l a t e x i t &gt; t3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a + f O / q q 7 P R Q v n g F A S c n V a y L b s F w = " &gt; A A A C K X i c b V D L S g M x F M 3 4 r P V V d e k m W I Q q U m Z E V F w V u 3 G h q G h r o V N L J s 3 U Y O Z B c k c s 4 / y O G 3 / F j Y K i b v 0 R 0 3 E Q b T 0 Q O O f c e 8 m 9 x w k F V 2 C a 7 8 b I 6 N j 4 x G R u K j 8 9 M z s 3 X 1 h Y r K s g k p T V a C A C 2 X C I Y o L 7 r A Y c B G u E k h H P E e z C u a 7 2 6 x c 3 T C o e + O f Q C 1 n L I 1 2 f u 5 w S 0 F a 7 U L G B 3 Q J A f H h 4 l L R T E T e O k 5 J N O w F g e + / O 3 r M 9 A l e U i P g s 2 c A / o q q F e 7 m + 1 i 4 U z b K Z A g 8 T K y N F l O G k X X i 2 O w G N P O Y D F U S p p m W G 0 I q J B E 4 F S / J 2 p F h I 6 D X p s q a m P v G Y a s X p p Q l e 1 U 4 H u 4 H U z w e c u r 8 n Y u I p 1 f M c 3 d l f V A 3 W + u Z / t W Y E 7 m 4 r 5 n 4 Y A f P p 9 0 d u J D A E u B 8 b 7 n D J K I i e J o R K r n f F 9 I p I Q k G H m 9 c h W I M n D 5 P 6 Z t n a L m + d b h U r + 1 k c O b S M V l A J W W g H V d A B O k E 1 R N E 9 e k Q v 6 N V 4 M J 6 M N + P j u 3 X E y G a W 0 B 8 Y n 1 + q 3 6 b Q &lt; / l a t e x i t &gt; LLMXO(• | S, C, f ⇤ ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K T H U 3 5 Q + M l o M X / o S K H v y + O 6 V w P k = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K q M e g F 4 8 R z Q O S J c x O Z p M x 8 1 h m Z o W w 5 B O 8 e F D E q 1 / k z b 9 x k u x B E w s a i q p u u r u i h D N j f f / b K 6 y s r q 1 v F D d L W 9 s 7 u 3 v l / Y O m U a k m t E E U V 7 o d Y U M 5 k 7 R h m e W 0 n W i K R c R p K x r d T P 3 W E 9 W G K f l g x w k N B R 5 I F j O C r Z P u V e + x V 6 7 4 V X 8 G t E y C n F Q g R 7 1 X / u r 2 F U k F l Z Z w b E w n 8 B M b Z l h b R j i d l L q p o Q k m I z y g H U c l F t S E 2 e z U C T p x S h / F S r u S F s 3 U 3 x M Z F s a M R e Q 6 B b Z D s + h N x f + 8 T m r j q z B j M k k t l W S + K E 4 5 s g p N / 0 Z 9 p i m x f O w I J p q 5 W x E Z Y o 2 J d e m U X A j B 4 s v L p H l W D S 6 q 5 3 f n l d p 1 H k c R j u A Y T i G A S 6 j B L d S h A Q Q G 8 A y v 8 O Z x 7 8 V 7 9 z 7 m r Q U v n z m E P / A + f w B X T I 3 Z &lt; / l a t e x i t &gt; oj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C C T j Q B r z w w k B V 9 L J n / O M 6 5 N R k S M = " &gt; A A A C C H i c b V C 7 S g N B F J 2 N r x h f U U s L B 4 N g F X Y l q G X Q x i J i B P O A 7 L L M T i b J m J n d Z e a u G J a U N v 6 K j Y U i t n 6 C n X / j 5 F F o 4 o E L Z 8 6 5 l 7 n 3 B L H g G m z 7 2 8 o s L C 4 t r 2 R X c 2 v r G 5 t b + e 2 d u o 4 S R V m N R i J S z Y B o J n j I a s B B s G a s G J G B Y I 2 g f z H y G / d M a R 6 F t z C I m S d J N + Q d T g k Y y c / v R / 4 d d j W X 2 A X 2 A A B p p X I 1 9 M e P t H k 9 9 P M F u 2 i P g e e J M y U F N E X V z 3 + 5 7 Y g m k o V A B d G 6 5 d g x e C l R w K l g w 5 y b a B Y T 2 i d d 1 j I 0 J J J p L x 0 f M s S H R m n j T q R M h Y D H 6 u + J l E i t B z I w n Z J A T 8 9 6 I / E / r 5 V A 5 8 x L e R g n w E I 6 + a i T C A w R H q W C 2 1 w x C m J g C K G K m 1 0 x 7 R F F K J j s c i Y E Z / b k e V I / L j o n x d J N q V A + n 8 a R R X v o A B 0 h B 5 2 i M r p E V V R D F D 2 i Z / S K 3 q w n 6 8 V 6 t z 4 m r R l r O r O L / s D 6 / A E Y q J o M &lt; / l a t e x i t &gt; oj ⇠ LLMXO &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j l 2 A 0 M 4 p 8 9 9 Y x m 8 5 F b 2 n A i Y M u T o = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 G S y C q 5 J I U Z d F N y 4 q V r A P a E O Y T C f t 0 M m D m R u x x C z 8 F T c u F H H r b 7 j z b 5 y m X W j r g Q t n z r m X u f d 4 s e A K L O v b W F h c W l 5 Z L a w V 1 z c 2 t 7 b N n d 2 m i h J J W Y N G I p J t j y g m e M g a w E G w d i w Z C T z B W t 7 w c u y 3 7 p l U P A r v Y B Q z J y D 9 k P u c E t C S a + 5 3 g T 0 A Q F q r X W d u / k j b N 5 l r l q y y l Q P P E 3 t K S m i K u m t + d X s R T Q I W A h V E q Y 5 t x e C k R A K n g m X F b q J Y T O i Q 9 F l H 0 5 A E T D l p v n + G j 7 T S w 3 4 k d Y W A c / X 3 R E o C p U a B p z s D A g M 1 6 4 3 F / 7 x O A v 6 5 k / I w T o C F d P K R n w g M E R 6 H g X t c M g p i p A m h k u t d M R 0 Q S S j o y I o 6 B H v 2 5 H n S P C n b p + X K b a V U v Z j G U U A H 6 B A d I x u d o S q 6 Q n X U Q B Q 9 o m f 0 i t 6 M J + P F e D c + J q 0 L x n R m D / 2 B 8 f k D v 7 u W l Q = = &lt; / l a t e x i t &gt; LLMXO &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n O f x r S t 7 r R I K S U y L E P J a b L 9 1 n o I = " &gt; A A A C I X i c d V D L S g M x F M 3 4 r P V V d e k m W A Q f W K Z a t C 4 E 0 Y 3 L C l a F t p Y 7 a a Y N z W S G 5 I 5 Y h v 6 K G 3 / F j Q t F 3 I k / Y / o Q 3 w c C h 3 P O 5 e Y e L 5 L C o O u + O i O j Y + M T k 6 m p 9 P T M 7 N x 8 Z m H x 3 I S x Z r z M Q h n q S w 8 M l 0 L x M g q U / D L S H A J P 8 g u v f d z z L 6 6 5 N i J U Z 9 i J e C 2 A p h K + Y I B W q m e K / t U G P a B + v Y r 8 B p M A b r p 0 k 1 Z B R i 1 Y + 6 Z u f Y a E 6 q 7 X M 1 k 3 V 9 g v u j v 7 9 D f J 5 9 w + s m S I U j 3 z U m 2 E L A 6 4 Q i b B m E r e j b C W g E b B J O + m q 7 H h E b A 2 N H n F U g U B N 7 W k f 2 G X r l q l Q f 1 Q 2 6 e Q 9 t W v E w k E x n Q C z y Y D w J b 5 6 f X E v 7 x K j H 6 x l g g V x c g V G y z y Y 0 k x p L 2 6 a E N o z l B 2 L A G m h f 0 r Z S 3 Q w N C W m r Y l f F x K / y f n 2 7 n 8 b q 5 w W s g e H g 3 r S J F l s k L W S J 7 s k U N y Q k q k T B i 5 J f f k k T w 5 d 8 6 D 8 + y 8 D K I j z n B m i X y D 8 / Y O N 9 + j p w = = &lt; / l a t e x i t &gt; f ⇤ = fmax + ↵(fmax fmin) Figure 2: LLEGO XO .
In each operation, the crossover operator (1) samples a set of parents S weighted by their fitness, (2) computes the desired fitness f * using S and α, and (3) samples offspring via the LLM.</p>
<p>Crossover through fitness guidance.To perform crossover, we utilize both the tree structures t i and the fitness metric f (t i ) to create few-shot prompts.For each of the sampled parents in S, we serialize the tree into natural language as a nested dictionary, which we denote as t nl i , where each intermediary key represents the splitting condition (feature name and splitting value) and the leaf item represents the value of the leaf node.Please refer to Figure 10 for more description of this representation.Each example is then constructed as "fitness: f (t i ), tree: t nl i " and we use S nl to represent the serialized few-shot prompt.We further condition the generation by specifying a desired fitness f * in the prompt to steer the generation towards high-fitness regions.This steering is controlled by a hyperparameter α, where f * = f max + α(f max − f min ), with f max and f min the best and worst fitness in S respectively.Intuitively, f * is defined relative to the best parent fitness, with the improvement proportional to the observed variability.A positive α defines f * to improve over the best fitness in the parent set, whereas −1 ≤ α &lt; 0 results in a more conservative target fitness that is within the observed fitness range.</p>
<p>We generate offsprings as o j ∼ LLM XO (• | S nl , C, f * ), by sampling from an LLM conditioned on the parents S nl , the task context C, and target fitness f * controlled by α.We write the complete crossover operation as O XO,k = LLEGO XO (P (g) , C; α), where O XO,k is the set of offspring generated from the operation k ∈ [κ].α controls the level of extrapolation, and we systematically investigate its effect in Section 5.2.By framing crossover using natural language, our crossover operator naturally allows for an arity ν strictly than 2, by including additional parents as in-context examples through S nl .</p>
<p>Diversity-GUIDED MUTATION OPERATOR</p>
<p>On the other side of the coin is the mutation operator, where the objective is to efficiently traverse under-explored areas in the search space to improve diversity and escape local minima.Traditional mutation operators can be viewed as inducing a uniform distribution over the space of solutions one random mutation away from the parent.However, this does not consider whether such mutations are semantically meaningful.To contextualize this, imagine the space one mutation away from a decision tree; many of these mutations are highly unlikely to be interesting or optimal given some degree of domain knowledge, resulting in inefficient exploration.In this setting, our mutation operator uses its semantic prior to effectively guide exploration, enabling more efficient diversity-driven exploration.</p>
<p>Parent sampling.As before, each mutation operation is conditioned on a set S of ν parents.However, whereas for crossover, parents are sampled based on their fitness, for mutation, parents are randomly sampled from the population to increase the diversity of S. Specifically, S = {(t j , f (t j )) | j ∈ [ν], t j ∼ Uniform N (P (g) )}, where each solution has uniform probability of being selected as a parent.Future works should consider more advanced sampling schemes to improve parent diversity.
𝜃 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / / 1 h n h m K s s 3 t R I e P U Q v + y g D 4 M C w = " &gt; A A A C C 3 i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B a h X k o i R T 0 W v X i s Y D + g K W G z 3 b R L N 5 u w O x F L 6 N 2 L f 8 W L B 0 W 8 + g e 8 + W / c t D 1 o 6 4 O B x 3 s z z M w L E s E 1 O M 6 3 V V h Z X V v f K G 6 W t r Z 3 d v f s / Y O W j l N F W Z P G I l a d g G g m u G R N 4 C B Y J 1 G M R I F g 7 W B 0 n f v t e 6 Y 0 j + U d j B P W i 8 h A 8 p B T A k b y 7 W P w O f Y 0 j 7 A H 7 A E A s q b k k 4 o X E R h S I r L G 5 N S 3 y 0 7 V m Q I v E 3 d O y m i O h m 9 / e f 2 Y p h G T Q A X R u u s 6 C f Q y o o B T w S Y l L 9 U s I X R E B q x r q C Q R 0 7 1 s + s s E n x i l j 8 N Y m Z K A p + r v i Y x E W o + j w H T m N + p F L x f / 8 7 o p h J e 9 j M s k B S b p b F G Y C g w x z o P B f a 4 Y B T E 2 h F D F z a 2 Y D o k i F E x 8 J R O C u / j y M m m d V d 3 z a u 2 2 V q 5 f z e M o o i N 0 j C r I R R e o j m 5 Q A z U R R Y / o G b 2 i N + v J e r
H e r Y 9 Z a 8 G a z x y i P 7 A + f w A r r Z s l &lt; / l a t e x i t &gt; ti ⇠ Un i( P ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y P 1 6 9 j R F 0
Z D P d g v M M z 7 + c Y k r z V U = " &gt; A A A B 8 n i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I U Z d F N y 4 r 9 A X T o W T S T B u a S Y Y k I 5 S h n + H G h S J u / R p 3 / o 2 Z d h b a e i B w O O d e c u 4 J E 8 6 0 c d 1 v p 7 S x u b W 9 U 9 6 t 7 O 0 f H B 5 V j 0 + 6 W q a K 0 A 6 R X K p + i D X l T N C O Y Y b T f q I o j k N O e + H 0 P v d 7 T 1 R p J k X b z B I a x H g s W M Q I N l b y B z E 2 E 4 J 5 1 p 4 P q z W 3 7 i 6 A 1 o l X k B o U a A 2 r X 4 O R J G l M h S E c a + 1 7 b m K C D C v D C K f z y i D V N M F k i s f U t 1 T g m O o g W 0 S e o w u r j F A k l X 3 C o I X 6 e y P D s d a z O L S T e U S 9 6 u X i f 5 6 f m u g 2 y J h I U k M F W X 4 U p R w Z i f L 7 0 Y g p S g y f W Y K J Y j Y r I h O s M D G 2 p Y o t w V s 9 e Z 1 0 r + r e d b 3 x 2 K g 1 7 4 o 6 y n A G 5 3 A J H t x A E x 6 g B R 0 g I O E Z X u H N M c 6 L 8 + 5 8 L E d L T r F z C n / g f P 4 A k B u R c w = = &lt; / l a t e x i t &gt; T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P T G Y P U 0 m m V Q z g e U / L P X 5 4 b 9 / 6 + g = " &gt; A A A C R 3 i c b V B N S y N B E O 2 J 6 1 f 8 y r r H v T Q b h A Q k z I j o k p P o x Y O C y x o V 0 n H o 6 a l o m 5 7 p o b t m 2 T D O v / O y V 2 / + h b 1 4 U J Y 9 b i e G x a 8 H D e + 9 q q K q X 5 Q p a d H 3 7 7 z K 1 I f p m d m 5 + e r C 4 t L y S u 3 j 6 o n V u R H Q E V p p c x Z x C 0 q m 0 E G J C s 4 y A z y J F J x G g 7 1 R / f Q H G C t 1 e o z D D H o J v 0 h l X w q O z g p r 5 w 2 G U s V Q 6 D K 8 W q f 2 u W w 2 K b M y o Q z h J y I W B w e H Z T g W x W H n u G w w E W t k 7 W v W Z g n H S 8 F V 8 b 1 c p / / F X t k M a 3 W / 5 Y 9 B 3 5 J g Q u p k g q O w d s t i L f I E U h S K W 9 s N / A x 7 B T c o h Y K y y n I L G R c D f g F d R 1 O e g O 0 V 4 x x K u u a c m P a 1 c S 9 F O n a f T x Q 8 s X a Y R K 5 z d K N 9 X R u Z 7 9 W 6 O f a / 9 g q Z Z j l C K p 4 W 9 X N F U d N R q D S W B g S q o S N c G O l u p e K S G y 7 Q R V 9 1 I Q S v v / y W n G y 0 g q 3 W 5 r f N + s 7 u J I 4 5 8 p l 8 I Q 0 S k G 2 y Q / b J E e k Q Q W 7 I b / J A H r 1 f 3 r 3 3 x / v 7 1 F r x J j O f y A t U v H / D r r J 0 &lt; / l a t e x i t &gt; (õj, s(õj)) ⇠ LLMMUT(• | S, C) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w / + Y s z L l E r P B B 1 t g e 9 b L 4 j f j Z g = " &gt; A A A C W X i c b V F N b x M x E P U u t I T w F e i x F 4 s I K b 2 E 3 a q C X p A q e u H W I p G 2 U h y t Z p 3 Z x q 3 X u 7 J n K y J r / y Q H p I q / w g E n X Q Q 0 j G T p z X s z 4 / F z X m v l K E l u o / j B w 6 3 t R 7 3 H / S d P n z 1 / M X j 5 6 s x V j Z U 4 k Z W u 7 E U O D r U y O C F F G i 9 q i 1 D m G s / z 6 + O V f n 6 D 1 q n K f K F l j b M S L o 0 q l A Q K V D a o B S 2 Q I L v i H 7 g o L E g v 8 G s 9 c i N B S s / R V 2 1 2 t f d W E D R 7 r R e u K T P / R 1 F c K M O 7 X J R A C w n a n 7 R t u z F E / R 6 S D Y b J O F k H 3 w R p B 4 a s i 9 N s 8 E 3 M K 9 m U a E h q c G 6 a J j X N P F h S U m P b F 4 3 D G u Q 1 X O I 0 Q A M l u p l f O 9 P y N 4 G Z 8 6 K y 4 R j i a / b v D g + l c 8 s y D 5 W r 9 d 1 9 b U X + T 5 s 2 V B z O v D J 1 Q 2 j k 3 U V F o z l V f G U z n y u L k v Q y A J B W h V 2 5 X E D w l 8 J n 9 I M J 6 f 0 n b 4 K z / X H 6 b n z w + W B 4 9 L G z o 8 d 2 2 W s 2 Y i l 7 z 4 7 Y J 3 b K J k y y 7 + x n t B V t R z / i K O 7 F / b v S O O p 6 d t g / E e / 8 A p I 0 t h Y = &lt; / l a t e x i t &gt; ✓j = exp(s(õj)/⌧ ) P õi2 Õ exp(s(õi)/⌧ ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q n v E 3 I G D X Q E Z g O m R s W Q 8 w Q 3 E + h Q = " &gt; A A A C J X i c d V D L S g M x F M 3 U V 6 2 v U Z d u g k V w V a Z a t A U X R T c u K 9 g H d E r J p G k b m m S G 5 I 5 Y h v o x b v w V N y 4 s I r j y V 0 w f g s 8 D g Z N z 7 u X e e 4 J I c A O e 9 + a k F h a X l l f S q 5 m 1 9 Y 3 N L X d 7 p 2 b C W F N W p a E I d S M g h g m u W B U 4 C N a I N C M y E K w e D C 4 m f v 2 G a c N D d Q 3 D i L U k 6 S n e 5 Z S A l d r u m Q / s F p K q 1 U I t s S H S T l W 9 E f b 9 z M z q 6 l B i + 5 c E + p S I p D L C d 3 O 0 3 a y X K 5 S K 3 n E J / y b 5 n D d F F s 1 R a b t j v x P S W D I F V B B j m n k v g l Z C N H A q 2 C j j x 4 Z F h A 5 I j z U t V U Q y 0 0 q m V 4 7 w g V U 6 2 K 5 p n w I 8 V b 9 2 J E Q a M 5 S B r Z z s a n 5 6 E / E v r x l D t 9 h K u I p i Y I r O B n V j g S H E k 8 h w h 2 t G Q Q w t I V R z u y u m f a I J B R t s x o b w e S n + n 9 S O c v m T X O G q k C 2 f z + N I o z 2 0 j w 5 R H p 2 i M r p E F V R F F N 2 j R / S M x s 6 D 8 + S 8 O K + z 0 p Q z 7 9 l F 3 + C 8 f w D R C a d g &lt; / l a t e x i t &gt; U ni fo rm sa m pl in g fro m P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y V N l 2 Y d O j a s h I P f C 2 + m G o m 3 9 d d U = " &gt; A A A C F H i c b Z D L S g M x F I Y z 9 V b r r e r S T b A I F a H M S F G X R R F d V K z Q G 7 S l Z N K 0 D c 1 c S M 6 I Z Z i H c O O r u H G h i F s X 7 n w b 0 + k s t P V A 4 O f 7 z + H k / L Y v u A L T / D Z S C 4 t L y y v p 1 c z a + s b m V n Z 7 p 6 6 8 Q F J W o 5 7 w Z N M m i g n u s h p w E K z p S 0 Y c W 7 C G P b q Y + I 1 7 J h X 3 3 C q M f d Z x y M D l f U 4 J a N T N H r W B P U B 4 E 0 A M I p y P A U B Y L l 9 e 3 U b d x K 9 V o 8 N u N m c W z L j w v L A S k U N J V b r Z r 3 b P o 4 H D X K C C K N W y T B 8 6 I Z H A q W B R p h 0 o 5 h M 6 I g P W 0 t I l D l O d M D 4 q w g e a 9 H D f k / q 5 g G P 6 e y I k j l J j x 9 a d D o G h m v U m 8 D + v F U D / r B N y 1 w + A u X S 6 q B 8 I D B 6 e J I R 7 X D I K Y q w F o Z L r v 2 I 6 J J J Q 0 D l m d A j W 7 M n z o n 5 c s E 4 K x b t i r n S e x J F G e 2 g f 5 Z G F T l E J X a M K q i G K H t E z e k V v x p P x Y r w b H 9 P W l J H M 7 K I / Z X z + A I t 8 n x 8 = &lt; / l a t e x i t &gt; Mutation(LLEGOMUT) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R o W r S L d Q F m Q t O i x x 1 O m 1 e K I Q a W Y = " &gt; A A A B + H i c d V D L S s N A F J 3 U V 6 2 P R l 2 6 G S y C q 5 B o 0 X Z X d O O y g n 1 A G 8 p k O m m H T i Z h 5 k a s o V / i x o U i b v 0 U d / 6 N 0 4 f g 8 8 C F w z n 3 c u 8 9 Q S K 4 B t d 9 t 3 J L y y u r a / n 1 w s b m 1 n b R 3 t l t 6 j h V l D V o L G L V D o h m g k v W A A 6 C t R P F S B Q I 1 g p G F 1 O / d c O U 5 r G 8 h n H C / I g M J A 8 5 J W C k n l 3 s A r u F T J P I b J O D S c 8 u u U 6 5 W n F P q v g 3 8 R x 3 h h J a o N 6 z 3 7 r 9 m K Y R k 0 A F 0 b r j u Q n 4 G V H A q W C T Q j f V L C F 0 R A a s Y 6 g k E d N + N j t 8 g g + N 0 s d h r E x J w D P 1 6 0 R G I q 3 H U W A 6 I w J D / d O b i n 9 5 n R T C i p 9 x m a T A J J 0 v C l O B I c b T F H C f K 0 Z B j A 0 h V H F z K 6 Z D o g g F k 1 X B h P D 5 K f 6 f N I 8 d 7 9 Q p X 5 V L t f N F H H m 0 j w 7 Q E f L Q G a q h S 1 R H D U R R i u 7 R I 3 q y 7 q w H 6 9 l 6 m b f m r M X M H v o G 6 / U D N N y U I Q = = &lt; / l a t e x i t &gt; sampling Space of trees &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z O I o V u z y n g i a j I t 9 C K q 2 F 6 w N c a I = " &gt; A A A C C H i c d V C 7 S g N B F J 3 1 G e N r 1 d L C w S B Y S N h N g i a F E L S x j G g e k A 3 L 7 G S S D J l 9 M H N X C E t K G 3 / F x k I R W z / B z r 9 x N o n g 8 8 A M h 3 P u 5 d 5 7 v E h w B Z b 1 b s z N L y w u L W d W s q t r 6 x u b 5 t Z 2 Q 4 W x p K x O Q x H K l k c U E z x g d e A g W C u S j P i e Y E 1 v e J 7 6 z R s m F Q + D a x h F r O O T f s B 7 n B L Q k m v u O T 6 B A S U i u R r j U + w k 4 N p H G N x C + h W d s W v m r H y p U r a K F f y b 2 H l r g h y a o e a a b 0 4 3 p L H P A q C C K N W 2 r Q g 6 C Z H A q W D j r B M r F h E 6 J H 3 W 1 j Q g P l O d Z H L I G B 9 o p Y t 7 o d Q v A D x R v 3 Y k x F d q 5 H u 6 M l 1 b / f R S 8 S + v H U O v 3 E l 4 E M X A A j o d 1 I s F h h C n q e A u l 4 y C G G l C q O R 6 V 0 w H R B I K O r u s D u H z U v w / a R T y 9 n G + d F n K V c 9 m c W T Q L t p H h 8 h G J 6 i K L l A N 1 R F F t + g e P a I n 4 8 5 4 M J 6 N l 2 n p n D H r 2 U H f Y L x + A O C i m K I = &lt; / l a t e x i t &gt; S = {t1, t2, t3} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K T H U 3 5 Q + M l o M X / o S K H v y + O 6 V w P k = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K q M e g F 4 8 R z Q O S J c x O Z p M x 8 1 h m Z o W w 5 B O 8 e F D E q 1 / k z b 9 x k u x B E w s a i q p u u r u i h D N j f f / b K 6 y s r q 1 v F D d L W 9 s 7 u 3 v l / Y O m U a k m t E E U V 7 o d Y U M 5 k 7 R h m e W 0 n W i K R c R p K x r d T P 3 W E 9 W G K f l g x w k N B R 5 I F j O C r Z P u V e + x V 6 7 4 V X 8 G t E y C n F Q g R 7 1 X / u r 2 F U k F l Z Z w b E w n 8 B M b Z l h b R j i d l L q p o Q k m I z y g H U c l F t S E 2 e z U C T p x S h / F S r u S F s 3 U 3 x M Z F s a M R e Q 6 B b Z D s + h N x f + 8 T m r j q z B j M k k t l W S + K E 4 5 s g p N / 0 Z 9 p i m x f O w I J p q 5 W x E Z Y o 2 J d e m U X A j B 4 s v L p H l W D S 6 q 5 3 f n l d p 1 H k c R j u A Y T i G A S 6 j B L d S h A Q Q G 8 A y v 8 O Z x 7 8 V 7 9 z 7 m r Q U v n z m E P / A + f w B X T I 3 Z &lt; / l a t e x i t &gt; oj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 g U K M 8 x h 6 w W E J D e 6 v 1 g I y w w 8 g m o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l B + x 7 / X L F r b p z k F X i 5 a Q C O R r 9 8 l d v E L M 0 4 g q Z p M Z 0 P T d B P 6 M a B Z N 8 W u q l h i e U j e m Q d y 1 V N O L G z + a n T s m Z V Q Y k j L U t h W S u / p 7 I a G T M J A p s Z 0 R x Z J a 9 m f i f 1 0 0 x v P Y z o Z I U u W K L R W E q C c Z k 9 j c Z C M 0 Z y o k l l G l h b y V s R D V l a N M p 2 R C 8 5 Z d X S e u i 6 l 1 W a / e 1 S v 0 m j 6 M I J 3 A K 5 + D B F d T h D h r Q B A Z D e I Z X e H O k 8 + K 8 O x + L 1 o K T z x z D H z i f P w i G j a U = &lt; / l a t e x i t &gt; t1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h o 7 L S 2 3 v g c Z 1 V 3 v v P R I k V G s O 1 0 M = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 m 0 q M e i F 4 8 V 7 A e k o W w 2 m 3 b p Z j f s T o Q S + j O 8 e F D E q 7 / G m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B X c g O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U M S r T l L W p E k r 3 Q m K Y 4 J K 1 g Y N g v V Q z k o S C d c P x 3 c z v P j F t u J K P M E l Z k J C h 5 D G n B K z k 9 4 G L i O V q O r g c V G t u 3 Z 0 D r x K v I D V U o D W o f v U j R b O E S a C C G O N 7 b g p B T j R w K t i 0 0 s 8 M S w k d k y H z L Z U k Y S b I 5 y d P 8 Z l V I h w r b U s C n q u / J 3 K S G D N J Q t u Z E B i Z Z W 8 m / u f 5 G c Q 3 Q c 5 l m g G T d L E o z g Q G h W f / 4 4 h r R k F M L C F U c 3 s r p i O i C Q W b U s W G 4 C 2 / v E o 6 F 3 X v q t 5 4 a N S a t 0 U c Z X S C T t E 5 8 t A 1 a q J 7 1 E J t R J F C z + g V v T n g v D j v z s e i t e Q U M 8 f o D 5 z P H 2 f o k V g = &lt; / l a t e x i t &gt; õ3 𝑜
&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d j t M H T e t c r P 0 T n 1 y T u X w
d + c N A F w = " &gt; A A A C a n i c d V F d a 9 R A F J 2 k a u t q d a 2 g S F 8 u r m I L s m T r U t u C U P T F N y u 6 b W E n L D e T m + 2 0 k 0 m Y m Y h L z I N / 0 T d / g S / + C C e 7 W 6 h f F w b O n H M P d + 6 Z p F T S u i j 6 H o Q r 1 6 7 f W F 2 7 2 b l 1 e / 3 O 3 e 6 9 j W N b V E b Q S B S q M K c J W l J S 0 8 h J p + i 0 N I R 5 o u g k u X j T 6 i e f y F h Z 6 I 9 u V l K c 4 1 T L T A p 0 n p p 0 v 3 J H n 1 3 9 A f N S E T T A l f e m + A w W P A j U q U z R E R R Z Z k s j 9 b T t c l K l V P M c 3 Z l A V b 9 r G n g F v N 5 a C k U z O X 8 O 9 u p 1 e 5 s f f O E H 5 8 C l h v H l m J g 3 k 2 4 v 6 g / 3 9 6 I X + / A 3 G P S j e f X Y s o 4 m 3 W 8 8 L U S V k 3 Z C o b X j Q V S 6 u E b j p F D U d H h l q U R x g V M a e 6 g x J x v X 8 6 g a e O q Z F L L C + K M d z N m r j h p z a 2 d 5 4 j v b 5 e y f W k v + S x t X L t u L a 6 n L y p E W i 0 F Z p c A V 0 O Y O q T Q k n J p 5 g M J I / 1 Y Q Z 2 h Q O P 8 7 H R / C 5 a b w f 3 C 8 0 x / s 9 o f v h 7 3 D 1 8 s 4 1 t g m e 8 y 2 2 I C 9 Z I f s L T t i I y b Y j 2 A 9 e B A 8 D H 6 G G + G j c H P R G g Z L z 3 3 2 W 4 V P f g F D E b q 5 &lt; / l a t e x i t &gt; Sample 0 candidate o↵spring Õ = {(õj, s(õj)) | j 2 [ 0 ]} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z U + d s g V I E J R v y i r Y r o Z t W I q 5 x q o = " &gt; A A A C Q X i c d V B N a x s x E N W m a Z u 6 X 2 5 7 7 E X E l K Y Q z G 5 q 2 u Q Q C M m l x w T i x O A 1 Z l Y 7 X o v o Y 5 F m k 5 j F f y 2 X / o P e e s + l h 4 b Q a y + V P w r 9 f C D 0 9 G a G N 3 p Z q a S n O P 4 c r d x Z v X v v / t q D x s N H j 5 8 8 b T 5 7 f u J t 5 Q R 2 h V X W 9 T L w q K T B L k l S 2 C s d g s 4 U n m Z n B 7 P 6 6 T k 6 L 6 0 5 p k m J A w 2 F k S M p g I I 0 b P Z S w k u q D 6 w u K 0 L u Q Q d X U / A L l M W Y P D + X w J U t S m c z z 6 c 8 p T E S 7 G 4 s 7 m G y m a r c k t 9 c v u t U B e s c X k / f D J u t u N 3 Z 2 Y 7 f 7 v C / S d K O 5 2 i x J Q 6 H z U 9 p b k W l 0 Z B Q 4 H 0 / i U s a 1 O B I C o X T R l p 5 L E G c Q Y H 9 Q A 1 o 9 I N 6 n s C U v w p K z k f W h W O I z 9 V f J 2 r Q 3 k 9 0 F j o 1 0 N j / W Z u J / 6 r 1 K x p t D 2 p p Z u k Y s T A a V Y q T 5 b M 4 e S 4 d C l K T Q E A 4 G X b l Y g w O B I X Q G y G E n z / l / y c n W + 3 k X b t z 1 G n t 7 S / j W G M v 2 T r b Y A l 7 z / b Y B 3 b I u k y w K 3 b N v r K b 6 G P 0 J b q N v i 1 a V 6 L l z A v 2 G 6 L v P w A R I L H E &lt; / l a t e x i t &gt;
Compute sampling weights via logprobs ✓ = (✓1, . . ., ✓ 0 ) Mutation with diversity guidance.To perform mutation, we only include the tree structure t j to create few-shot prompts: each parent is serialized as "tree: t nl j ", to create S nl .Subsequently, we generate λ ′ (where λ ′ ≥ λ) candidate offsprings õj , and track the negative log probabilities of the candidates obtained from the LLM, represented s(õ j ) = −p(õ j | S).Intuitively, s(õ j ) reflects the likelihood of the candidate offspring given the set of parents, with smaller values indicating that the candidate offspring has low probability under the current population distribution and hence that its inclusion can introduce more diversity at the population level.We provide further justification for the use of log probabilities in Section 5.2 and Appendix D.4, demonstrating their correlation with functional and structural distances between parent and offspring trees.
&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D T h X H Z V d D t 4 K D Q C n A B p q V L M E D T 4 = " &gt; A A A C E X i c b V D J S g N B E O 1 x j X G L e v T S G M R 4 C T M S 1 G P Q i 0 c F o 4 F M G G o 6 F d P a s 9 B d I 4 Y h v + D F X / H i Q R G v 3 r z 5 N 3 a W g 9 u D g s d 7 V d 1 V L 0 y V N O S 6 n 8 7 U 9 M z s 3 H x h o b i 4 t L y y W l p b v z B J p g U 2 R K I S 3 Q z B o J I x N k i S w m a q E a J Q 4 W V 4 c z z 0 L 2 9 R G 5 n E 5 9 R P s R 3 B V S y 7 U g B Z K S h V k u C a + 0 Z G 3 C e 8 I 6 L 8 G G g Q 5 L 6 y b 3 R g Z 1 D x q Y c E u 0 G p 7 F b d E f h f 4 k 1 I m U 1 w G p Q + / E 4 i s g h j E g q M a X l u S u 0 c N E m h c F D 0 M 4 M p i B u 4 w p a l M U R o 2 v n o o g H f t k q H d x N t K y Y + U
As such, the candidate sampling step is represented as (õ j , s(õ j )) ∼ LLM MUT (•|S nl , C).Given this set of λ ′ candidates, we select λ offspring based on their log probabilities, i.e.,
O MUT = {(o j , f (o j ) | j ∈ [λ], o j ∼ Cat λ ′ (θ)}, where θ = (θ 1 , . . . , θ λ ′ ) and θ j = exp(s(õj )/τ ) λ ′ i=1 exp(s(õj )/τ )
. Here, τ is the sampling temperature, where larger values of τ &gt; 1 results in a more uniform distribution over the candidate offspring, and lower values of 0 &lt; τ ≤ 1 would put more weight on candidates with lower likelihood.</p>
<p>As such, we use τ and the log probabilities to guide the sampling of offspring with controllable levels of diversity.In Section 5.2, we empirically investigate the effect of τ on offspring diversity.In summary, we define the k-th mutation operation as O MUT,k = LLEGO MUT (P (g) , C; τ ).</p>
<p>END-TO-END ALGORITHM</p>
<p>Having detailed our LLM-based genetic operators, we now put together the end-to-end GP algorithm.</p>
<p>The search is initialized with a set of N solutions, P (0) .In each generation, we sample N crossover offspring, represented as O MUT .This is performed through κ genetic operations, with each operation involving ν parents, and generating λ offsprings.The fitness of each solution is then calculated against the training set D train .For selection, we consider the set of candidates as the union of the solutions from the previous generation and the newly generated offsprings, i.e.P(g+1
) = P (g) ∪ O (g) XO ∪ O (g) MUT .
We use the elitism selection to select the top-N unique solutions from the candidate population to preserve the highest quality solutions across generations (Goldberg, 1989).Here, top-N is selected based on training set fitness.More formally, we denote this process as P (g+1) ← SEL( P(g+1) ; N ).After G generations of evolution, we select the solution with the highest validation fitness as the final solution.</p>
<p>RELATED WORKS</p>
<p>Our work relates to multiple strands of research, which we summarize in brief below.We provide an extended literature survey in Appendix A.1.</p>
<p>Tree induction algorithms.Existing algorithms for decision tree induction can be broadly categorized into three main classes: greedy, globally optimal, and GP algorithms.Greedy algorithms recursively construct a tree in a top-down approach, heuristically making locally optimal splits at each node (Breiman et al., 1984;Quinlan, 1986;1993).While computationally efficient, these methods do not pursue global optimality.Recent works have proposed exact combinatorial methods to construct optimal decision trees (Verwer &amp; Zhang, 2019;Hu et al., 2019;Lin et al., 2020;Aglin et al., 2020).These methods face two key limitations: exponential complexity scaling with tree depth and number of splits, and restricted applicability to specific objectives (primarily classification problems).</p>
<p>Genetic programming.GP approaches present a middle ground between search performance and computational efficiency (Koza, 1990;Tanigawa &amp; Zhao, 2000;Lahovnik, 2024).GP builds on genetic operators that operate over structure (genotype) but can have disruptive effects because of the complex genotype-phenotype mapping (Rothlauf et al., 2011).This observation has motivated works on semantic GP (Krawiec &amp; Lichocki, 2009;Moraglio et al., 2012;Krawiec &amp; Pawlak, 2013), aiming to produce offspring that inherit semantics (phenotype) from parents.However, these approaches are highly domain-specific, and have not extended to tree induction, which is the focus of our work.</p>
<p>LLMs and optimization.</p>
<p>Recent studies have explored LLMs for optimization tasks, with some works employing LLMs as variation operators (Meyerson et al., 2023).Examples of applications include code evolution (Lehman et al., 2023;Nasir et al., 2024;Brownlee et al., 2023), Bayesian Optimization (Liu et al., 2024), and prompt optimization (Fernando et al., 2024;Guo et al., 2024), where unguided variations are sampled from LLMs.In contrast, LLEGO generates guided variations by considering search dynamics at the population level, modulating fitness and diversity through hyperparameters α and τ , while exploiting in-context learning with parent solutions.</p>
<p>EXPERIMENTS</p>
<p>Benchmark datasets.We empirically evaluate LLEGO's ability to find performant decision trees for 12 open-source tabular datasets from OpenML curated benchmarks (Vanschoren et al., 2014), including 7 classification and 5 regression datasets.These datasets were selected based on the number of features, samples and the presence of semantically meaningful feature names and descriptions.We provide further details on this selection of datasets and preprocessing in Appendix C.1.</p>
<p>Baselines.We compare LLEGO against a comprehensive set of competitive decision tree induction methods across major categories: greedy induction (CART (Breiman, 2017) and C4.5 (Quinlan, 1993)), sparse optimal tree induction (GOSDT (Lin et al., 2020) and DL8.5 (Aglin et al., 2020)), and a GP approach using conventional genetic operators (GATree (Lahovnik, 2024), which is an implementation of GP for decision tree induction in Python).More details on these baselines, their implementation, hyperparameters, and experimental details are given in Appendices C.2 and C.3.For GP-based methods (LLEGO, GATree), we initialize the population with CART models bootstrapped on 25% of the training data.We report results using G = 25, N = 25, and we use α = 0.1, τ = 10 and ν = 4 as the hyperparameters for the variation operators of LLEGO.</p>
<p>Evaluation.For classification tasks, we use balanced accuracy (ACC), and for regression tasks, mean squared error (MSE), computed on a held-out test dataset D test .Each metric is averaged over 5 runs with different random seeds, due to different dataset splits, and we present these averages with their standard deviations.For LLEGO, we use gpt-3.5-turboversion 0301 as the underlying LLM.For a fair comparison, each method is allowed 10 minutes of wall clock time per seeded run, which includes time spent on hyperparameter tuning.</p>
<p>LLEGO-EVOLVED TREES ACHIEVE SUPERIOR GENERALIZATION PERFORMANCE</p>
<p>We first compare the performance of the complete LLEGO algorithm against baselines for decision tree induction.We report in Table 1 and Table 2 the test performance on classification and regression datasets, respectively, for maximum tree depths of 3 and 4. For regression, we report the results for CART and GATree since other baselines cannot optimize regression objectives.The results demonstrate that LLEGO outperforms baselines comprehensively.We observe that this performance advantage becomes more pronounced in the space of trees with depth 4, which is intuitive since  it represents a substantially larger search space compared to the set of trees with depth 3.In the more constrained space of trees with depth 3, sparse optimal induction methods such as DL85 and GOSDT demonstrate increased competitiveness.This suggests that LLEGO's efficiency gains are particularly evident when navigating more complex and expansive search spaces.We also compare in Appendix D.1 the generalization gap across all methods.Notably, LLEGO consistently achieves the lowest generalization gap, with this advantage becoming especially pronounced at depth 4.</p>
<p>The results also underscore the impact of incorporating semantic priors.Indeed, LLEGO consistently outperforms the GP baseline GATree, which cannot take into account contextual information.Further analysis in Appendix D.5 demonstrates that LLEGO produces superior trees even when compared to a GATree configuration utilizing substantially larger search budgets.Notably, LLEGO achieves this superior performance while requiring fewer evaluations, highlighting its efficiency and effectiveness.</p>
<p>Takeaway: LLEGO optimizes decision trees that are superior against a diverse benchmark of methods, while being more applicable to a wider range of optimization objectives (e.g., regression).</p>
<p>Search efficiency.Having shown the superior generalization performance of decision trees evolved by LLEGO, we now compare search efficiency between LLEGO and the GP baseline GATree.We evaluate population dynamics via normalized population fitness and diversity between the two methods across all classification datasets, when optimizing trees with depth 3. Fitness values (i.e., balanced accuracy) were normalized to enable comparison across different seeds and datasets (refer to Appendix C.4 for details).Figure 4 (Left) shows the median population fitness, where LLEGO demonstrates superior search efficiency, finding fitter individuals more efficiently.</p>
<p>Figure 4 (Right) shows that the populations evolved by LLEGO exhibit decreasing diversity as the search progresses, whereas GATree maintains roughly the same level of diversity in its population.This is expected, as LLEGO uses its semantic priors to focus the search on semantically meaningful regions, which naturally reduces diversity.A similar effect has been observed when employing semantically aware GP in other domains (Krawiec &amp; Pawlak, 2013).In comparison, GATree, which is semantically unaware, performs random structural perturbations that maintain a certain level of diversity in the population.In Appendix D.8, we investigate search efficiency on problems with depth 4 and show search dynamics on individual tasks, observing the same effects at play.Takeaway: LLEGO leverages its semantic priors for more efficient search convergence, although this can sacrifice population diversity, requiring this trade-off to be carefully balanced by its operators.Results.</p>
<p>UNDERSTANDING THE SOURCES OF GAIN</p>
<p>(1) Crossover: We examine the effect of α ∈ {−0.25, −0.1, 0.1, 0.25} on offspring generation, where α determines the target fitness f * that conditions the offspring generation.In Figure 5, we visualize the median population fitness and diversity as a function of α.Offspring fitness improves as α increases from −0.25 to 0.1, but regresses beyond this point as the target fitness f * leads to extrapolation in less reliable regions.Interestingly, the best offspring fitness emerges at α = 0.1, suggesting LLEGO XO 's ability to perform a reasonable degree of extrapolation.Corresponding, diversity decreases with increasing α, reflecting sampling from progressively smaller search regions.Hence, compared to GATree, LLEGO produces higher quality offspring but with lower diversity, which is consistent with our findings in Section 5.1.(2) Mutation: We investigate the role of LLEGO MUT in maintaining diversity by considering a range of τ ∈ {5, 10, 25, 50}.</p>
<p>In Figure 6, we observe that lower values of τ increases population diversity, as they prioritize offspring that have low likelihood given parents.As such, the offspring introduce greater diversity at the population level, which complements the dynamics of the crossover operator mentioned above, crucial in balancing exploitation and exploration during search.Results for individual datasets can be found in Appendix D.8.Having demonstrated the superior performance of LLEGO against existing baselines, we finally scrutinize the contribution of each algorithmic component to its optimization performance.Specifically, we aim to investigate the effects of (1) leveraging the LLM's semantic prior to evolve solutions, (2) the fitnessguided crossover and diversity-guided mutation, and (3) the higher arity of genetic operations.Now, we systematically ablate each component: LLEGO no_semantics removes any semantic information from the prompts (see Appendix B.1 for detailed description), which is equivalent to removing C from p(o|S, C); LLEGO no_xo removes the fitness-guided crossover, using only the mutation operator during search; LLEGO no_mut removes diversity-guided mutation, using only crossover during search; and LLEGO ν=2 restricts the context to 2 parents, akin to traditional genetic operators.We evaluate search efficiency in Figure 7, observing that best performance is obtained when both operators are used in tandem, likely as they balance exploration of higher fitness regions (guided by f * ) and exploration of less visited regions (guided by τ ).The semantic information leveraged by the operator also improves performance, although we note that even without it, LLEGO no_semantics performs very competitively, highlighting the strong few-shot learning capabilities of LLMs.Finally, using binary operators in LLEGO ν=2 is suboptimal, underlining the often overlooked importance of using a wider context in genetic operations.We provide more fine-grained ablation results in Appendix D.7.</p>
<p>ABLATION STUDY: ALL COMPONENTS CONTRIBUTE TO ENHANCED SEARCH EFFICIENCY</p>
<p>Takeaway: Our ablation experiment demonstrates that all algorithmic components contribute to the enhanced optimization performance of LLEGO.</p>
<p>ADDITIONAL RESULTS.</p>
<p>In the interest of space, we relegated additional investigations to Appendix D. Specifically, we investigated memorization concerns by evaluating generalization performance on datasets with removed identifying information and context, as well as testing LLEGO on unseen proprietary datasets (detailed in Appendix D.2).In Appendix D.3, we investigated LLEGO's ability to mitigate negative bias by optimizing fairness-regularized objectives.We also provide additional analyses into LLEGO's performance and its individual components.</p>
<p>DISCUSSION</p>
<p>In summary, we introduced LLEGO, a novel GP method for decision tree induction that integrates semantic priors over the search space by using LLMs as variation operators.Our approach leverages the semantic understanding and domain knowledge of LLMs to evolve decision trees through crossover and mutation operators, while incorporating fitness and diversity guidance and flexible operation arity.Empirical results across diverse datasets demonstrate LLEGO's superior optimization efficiency, yielding high-performing decision trees compared to existing baselines.</p>
<p>Limitations and future works.However, our work is not without its limitations.Performing inference through LLMs incurs a larger computational footprint than conventional GP algorithms.Our findings indicate that LLEGO trades off computational requirements for improved search efficiency and generalization performance, making it particularly appealing in performance-sensitive domains or problems where evaluation costs exceed search costs.Future works should prioritize reducing computational requirements while retaining performance, such as through inference acceleration (Leviathan et al., 2023) and memory-efficient model architectures (Han et al., 2015).Additionally, while LLEGO can operate effectively without semantic priors, its performance can be further improved when such knowledge is available.Future works could explore finetuning strategies and prompt augmentation strategies to incorporate semantic knowledge in specialized domains.We also recognize that using black-box LLMs could potentially lead to the propagation of negative biases into the solutions returned by LLEGO-to this end, we presented initial steps to mitigate bias via the design of objective functions.Outlook.In the long run, we believe this work illuminates the promise of employing LLM capabilities for enhancing efficiency and performance in complex combinatorial optimization problems beyond decision tree induction.</p>
<p>A ADDITIONAL DISCUSSIONS</p>
<p>A.1 EXTENDED RELATED WORKS Tree induction algorithms.Greedy algorithms sequentially grow trees by optimizing a given objective myopically.Popular methods in this class of algorithms are CART (Breiman et al., 1984), ID3 (Quinlan, 1986) and C4.5 (Quinlan, 1993).These algorithms differ in the predictive tasks in which they can be applied.These algorithms mainly differ in the criterion used to split the nodes at each local node, including Gini impurity (Breiman et al., 1984) or information gain (Quinlan, 1993).</p>
<p>Owing to their greedy nature, they are computationally efficient in searching the combinatorial space.</p>
<p>In contrast, a branch of work employs exact combinatorial optimization techniques to search for sparse, optimal trees, e.g.branch and bound (Lin et al., 2020) and dynamic programming (Aglin et al., 2020).Notable works include BinOCT (Verwer &amp; Zhang, 2019), DL85 (Aglin et al., 2020), OSDT (Hu et al., 2019), and GOSDT (Lin et al., 2020).These approaches are fundamentally limited by the N P -hardness of the tree induction problem, and struggle to scale to larger size problems.Additionally, they have exclusively focused on the classification setting, and are limited in the types of feature (e.g.binary or continuous features) and objective functions that can be optimized.We compare LLEGO with representative tree induction methods in Table 3.
O(d!) Monotonic functions ✗ ✓ ✗ ✓ ✗ LLEGO GP O(GN ) Any ✓ ✓ ✓ ✓ ✓
Genetic programming.GP is an evolutionary optimization framework, particularly effective for a variety of combinatorial optimization problems, since it only requires the provision of a fitness function to evaluate and evolve a population of solutions to find optimal solutions (Koza, 1994a).As such, GP has been used in diverse tasks including tree induction (Tanigawa &amp; Zhao, 2000;Kuo et al., 2007;Zhao, 2007;Koza, 1990), discovery symbolic mathematical expressions (Augusto &amp; Barbosa, 2000;Qian et al., 2022), scheduling problems (Guillaume et al., 2007), neural architecture search (Broni-Bediako et al., 2020), and policy design (Hein et al., 2018).While the design of genetic operators differ significantly across domains, genetic operators share several limitations, being agnostic to the solution semantics, relying on stochastic perturbations without any search directionality, and narrow contexts.Several works in semantic genetic programming have considered the first two limitations and proposed variation operators (Krawiec &amp; Pawlak, 2013;Moraglio et al., 2012) or rejection sampling mechanisms (Krawiec &amp; Lichocki, 2009) to obtain semantic consistency between the offspring and their parents.However, these methods are domain-specific: for example, (Krawiec &amp; Pawlak, 2013) considers convex combinations in the particular case of symbolic expressions.This limits their generalizability, and we note that no semantic operator has been designed for the tree induction setting which is the focus of our work.</p>
<p>LLM and optimization.</p>
<p>Recent studies have explored LLMs for optimization tasks, exploiting their domain priors to enhance optimization efficiency (Song et al., 2024).Notable applications include prompt (Yang et al., 2024), reward-function (Ma et al., 2024), and code optimization (Liventsev et al., 2023).Particularly relevant is research employing LLMs as variation operators.(Lehman et al., 2023;Nasir et al., 2024;Brownlee et al., 2023)   Expression: ## {'credit_history': {'&lt;= 1.5000': {'property_magnitude': {'&lt;= 0.5000': {'employment': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 0}}}, '&gt; 0.5000': {'value': 0}}}, '&gt; 1.5000': {'savings_status': {'&lt;= 3.5000': {'property_magnitude': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 1}}}, '&gt; 3.5000': {'employment': {'&lt;= 2.5000': {'value': 1}, '&gt; 2.5000': {'value': 1}}}}}}} ## Expression: ## {'other_payment_plans': {'&lt;= 1.5000':</p>
<p>{'property_magnitude': {'&lt;= 1.5000': {'own_telephone': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 0}}}, '&gt; 1.5000': {'num_dependents': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 0}}}}}, '&gt; 1.5000': {'purpose': {'&lt;= 6.5000': {'residence_since': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 1}}}, '&gt; 6.5000': {'housing': {'&lt;= 0.5000': {'value': 1}, '&gt; 0.5000': {'value': 1}}}}}}} ## Expression: ## {'credit_history': {'&lt;= 3.5000': {'duration': {'&lt;= 34.5000': {'checking_status': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 1}}}, '&gt; 34.5000': {'credit_amount': {'&lt;= 10552.5000':{'value': 1}, '&gt; 10552.5000':{'value': 0}}}}}, '&gt; 3.5000': {'credit_amount': {'&lt;= 9597.5000':{'employment': {'&lt;= And its corresponding natural language representation as a nested dictionary.</p>
<p>B.1 ABLATION PROMPTS</p>
<p>In our ablation study, we removed all semantic information from the prompts, with examples illustrated in Listing 3 and 4. Here, we remove the semantic description of the task, and replace its features names with X i .</p>
<p>The task is to generate interpretable and high-performing decision trees given a set of attributes.The dataset contains 360 samples and 20 features, of which 7 are numerical and 13 are categorical.The target variable is y, it is binary, the label distribution is fitness: 0.5882, Expression: ## {'X_3': {'&lt;= 5.5000': {'X_14': {'&lt;= 0.5000': {'X_10': {'&lt;= 2.5000': {'value': 0}, '&gt; 2.5000': {'value': 1}}}, '&gt; 0.5000': {'X_16': {'&lt;= 1.5000': {'value': 0}, '&gt; 1.5000': {'value': 1}}}}}, '&gt; 5.5000': {'X_1': {'&lt;= 25.5000': {'X_2': {'&lt;= 3.5000': {'value': 1}, '&gt; 3.5000': {'value': 1}}}, '&gt; 25.5000': {'X_10': {'&lt;= 3.5000': {'value': 1}, '&gt; 3.5000': {'value': 0}}}}}}} ## fitness: 0.5930, Expression: ## {'X_5': {'&lt;= 2.5000': {'X_4': {'&lt;= 9597.5000':{'X_2': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 1}}}, '&gt; 9597.5000':{'value': 0}}}, '&gt; 2.5000': {'X_0': {'&lt;= 0.5000': {'X_11': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 1}}}, '&gt; 0.5000': {'X_10': {'&lt;= 2.5000': {'value': 1}, '&gt; 2.5000': {'value': 1}}}}}}} ## fitness: 0.6162, Expression: ## {'X_11': {'&lt;= 0.5000': {'X_1': {'&lt;= 33.0000': {'X_14': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 0}}}, '&gt; 33.0000': {'X_6': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 0}}}}}, '&gt; 0.5000': {'X_6': {'&lt;= 0.5000': {'X_4': {'&lt;= 3359.5000':{'value': 0}, '&gt; 3359.5000':{'value': 1}}}, '&gt; 0.5000': {'X_3': {'&lt;= 5.5000': {'value': 1}, '&gt; 5.5000': {'value': 1}}}}}}} ## fitness: 0.6815, Expression: ## {'X_0': {'&lt;= 1.5000': {'X_11': {'&lt;= 1.5000': {'X_9': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 0}}}, '&gt; 1.5000': {'X_1': {'&lt;= 20.5000': {'value': 1}, '&gt; 20.5000': {'value': 1}}}}}, '&gt; 1.5000': {'X_2': {'&lt;= 2.5000': {'X_17': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 0}}}, '&gt; 2.5000': {'X_13': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 1}}}}}}} ## fitness: 0.6908, Expression:</p>
<p>Listing 4: Example crossover prompt with semantics removed.On credit dataset.</p>
<p>C DETAILS OF EXPERIMENTAL PROCEDURES</p>
<p>In this section, we outline the benchmark datasets employed in our evaluations, as well as implementation details of our method and considered baselines.</p>
<p>C.1 DATASET DETAILS</p>
<p>We employ a total of 12 datasets for our evaluation, of which 7 are classification tasks, and 5 are regression tasks.Additionally, we consider 2 propriety datasets in Appendix D.2, for which the LLM would not have seen during pretraining, and thus used to check for any memorization concerns.</p>
<p>Open-source datasets.The 12 open-source tabular datasets are sourced from OpenML (Vanschoren et al., 2014).The classification datasets were selected from the curated suite OpenML-CC18 (Bischl et al., 2019) with the following criteria: ≤ 20 features, ≤ 10000 samples, binary labels and no missing data.This stems from the fact that optimal tree induction methods scale exponentially with the number of features and samples, and some baselines only support binary classification.Additionally, we excluded datasets lacking semantically meaningful feature names and descriptions, required by LLEGO.Regression datasets were selected from OpenML-CTR23 (Fischer et al., 2023) with identical criteria.We detail dataset characteristics, including OpenML ID, number of attributes, number of samples and label distribution in Table 4.These datasets can be loaded by querying their OpenML IDs.The datasets describe:</p>
<p>• credit (Kelly et al.):This dataset classifies people as good or bad credit risks.</p>
<p>• diabetes (Smith et al., 1988): This dataset classifies patients based on WHO definition of diabetes.</p>
<p>• compas (Inc., 2016): Contains criminal history, jail and prison time, demographics, and is used to predict two year recidivism.• heart (hea): Prediction of heart disease in patients.</p>
<p>• liver (Kelly et al.):This data set contains 416 liver patient records and 167 non liver patient records.The data set was collected from north east of Andhra Pradesh, India.The class label divides the patients into 2 groups (liver patient or not).This data set contains 441 male patient records and 142 female patient records.• heart (Street et al., 1993): Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.They describe characteristics of the cell nuclei present in the image.The target feature records the prognosis (malignant or benign).• vehicle (Pete &amp; Shepherd): The dataset classifies a given silhouette as one of four types of vehicle, using a set of features extracted from the silhouette.The target label is re-relabelled, where the majority class as positive ('P') and all others as negative ('N').• cholesterol (Janosi et al., 1988): The dataset predicts the cholesterol level among patients diagnosed with heart disease.• wine (Cortez et al., 2009): The task is to predict quality of white and red wine.</p>
<p>• wage (Berndt, 1991): The task is to predict individual wages using the Current Population Survey (CPS), used to supplement census information between census years.• abalone (Nash et al., 1995): Predicting the age of abalone from physical measurements.The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -a boring and time-consuming task.• cars (Bohanec, 1997): Dataset of the suggested retail prices (column Price) and various characteristics of each car.elitism selection to preserve the top 25 trees after merging the offspring of the crossover and the mutation.To compute the desired fitness, we use α = 0.1, based on observations in Section 5.2 as the value that balanced diversity and fitness.We use τ = 10 for diversity guidance.For each genetic operation, we use λ = 4 parent trees.For our experiments, we use gpt-35-turbo, version 0301 with default hyperparameters temperature = 0.7 and top_p = 0.95.</p>
<p>Function and terminal set.For both LLEGO and GATree, the function set is {&lt;, &gt;, ≤, ≥} and the terminal set includes numerical constants based on target feature values.</p>
<p>In Section 5.2, we perform 3 steps of crossover starting from the initial population, for both LLEGO and GATree to obtain Figure 5.We similarly perform 3 steps of mutation starting from the initial population to obtain Figure 6.</p>
<p>C.4 EVALUATION METRICS</p>
<p>MSE.</p>
<p>For regression dataset, we report MSE (sklearn.metrics.mean_squared_error):
MSE(D, f ) = 1 N N n=1 ||f (x n ) − y n || 2
Balanced accuracy.For classification datasets, we report balanced accuracy, which is equivalent to accuracy with class-balanced sample weights (sklearn.metrics.balanced_accuracy_score).</p>
<p>This has the effect of giving equal importance to both the positive and negative classes, thereby mitigating the impact of class imbalance and providing a more reliable assessment of the classifier's performance across all classes: We utilize the implementation aif360.sklearn.metrics.equal_opportunity_differenceprovided in https://aif360.readthedocs.io/(Bellamy et al., 2019;Roemer &amp; Trannoy, 2015).</p>
<p>Population Fitness.In order to assess the fitness of the populations evolved by the GP-based algorithms, we compute for a given population P:
Fitness = Median({f ′ (t) | t ∈ P})
where f ′ (t) denotes the normalized accuracy, calculated as t) , where f (t) here denotes the accuracy.P ′ is the union of all individuals produced by all methods for a particular seeded run on a particular dataset.In other words, the best accuracy obtained by any method on a particular seed for a particular dataset will have f ′ (t) = 1 and the worst will have f ′ (t) = 0.This normalization allows accuracy results from different datasets, seeds, and methods to be compared.
f (t)−min t∈P f (t) max t∈P ′ f (t)−min t∈P ′ f (
Population diversity.In order to assess the diversity of the populations evolved by the GP-based algorithms, we compute for a given population P:
Diversity = Median({||φ(t) − φ(t ′ )|| 1 | (t, t ′ ) ∈ P 2 })
where φ(t) denotes the functional signature of t, i.e. the vector (t(x 1 ), ..., t(x n )).</p>
<p>D ADDITIONAL RESULTS</p>
<p>In this section of the appendix, we provide additional empirical results.Specifically:</p>
<p>1.In Appendix D.1, we analyze the train-test generalization gap. 2. In Appendix D.2, we report generalization performance on tasks with all semantics removed.The objectives of this experiment are to (1) check for memorization and (2) evaluate the contribution of semantic priors to search efficiency.We also evaluate LLEGO on proprietary datasets.3.In Appendix D.3, we investigate the potential for bias and the flexibility of LLEGO in optimizing for fairness-regularized objectives.4. In Appendix D.4, we demonstrate that the log-probabilities of candidate trees is directly correlated with the structural distances.5.In Appendix D.5, we compare LLEGO against a version of GATree running with larger population sizes and more generations than LLEGO.6.In Appendix D.6, we report the runtimes of LLEGO and the baselines.7.In Appendix D.7, we perform additional ablation studies on key variables, including prompting strategies, choice of underlying LLM, genetic operation arity, and different parent sampling mechanisms, and population initialization strategies.8.In Appendix D.8, we provide fine-grained results of the aggregate analysis presented in the main paper.</p>
<p>D.1 GENERALIZATION GAP</p>
<p>In Table 6, we report the generalization gap (defined as BAcctrain−BAcctest</p>
<p>BAcctrain</p>
<p>) averaged across the classification datasets for each method.The results show that LLEGO consistently achieve a lower generalization gap compared to the baselines.In particular, the difference between LLEGO and the baselines is more noticeable at depth 4, where the baselines are more susceptible to overfitting, such as the optimal induction method DL85.This aligns with empirical observations in recent works (Zharmagambetov et al., 2021;Marton et al., 2023;Sullivan et al., 2024).As with any LLM application, there is a concern about LLM memorization.Although it is highly unlikely that the LLM has encountered the optimal trees for the considered datasets-especially given that high-performing solutions can vary significantly across different training splits, seeds, and preprocessing steps-we empirically investigate this concern.This is done by removing any dataset-specific metadata or semantic information that could identify the underlying data.For prompts with semantics removed, please refer to Appendix B.1.We refer to this setting as LLEGO no_semantics and compare its performance against LLEGO with semantics included in Table 7.We observe that LLEGO no_semantics achieves similar performance, even outperforming LLEGO on two of the tasks.(CUTRACT, 2019)).We report the results against CART and GATree for depth = 4 in Table 8, showing that LLEGO achieves superior performance on these private datasets, further demonstrating that it relies on generalized semantic priors rather than dataset-specific memorization.Setup.In this experiment, we focus in particular on bias.</p>
<p>More precisely, we assess group fairness (Verma &amp; Rubin, 2018) by computing the Difference in Equality of Opportunity (DEO) metric, defined as the difference in recall between unprivileged and privileged groups (cf.Appendix C.4 for an exact definition).We show an illustrative example on the dataset COMPAS, which is known to be biased on the sensitive attribute race African American (Angwin et al., 2016).Our objective is to mitigate bias with a DEO-based regularization, by defining LLEGO's new fitness function, i.e. f ′ (t) = f (t) + βDEO(t) for any t ∈ T .</p>
<p>Results.As can be seen in Table 9, LLEGO does not natively return fair decision trees when the fitness functions are based purely on accuracy.However, the DEO regularization permits LLEGO to find decision trees with less bias compared to the other baselines.This highlights the flexibility of LLEGO, which can handle composite search objectives unlike the other baselines.LLEGO also returns a population of individuals, which makes it possible to trade-off predictive performance with fairness metrics.We show this in Figure 11, where one can choose individuals returned by LLEGO with acceptable tradeoffs.</p>
<p>D.4 CORRELATION BETWEEN LOG-PROBABILITIES AND TREE EDIT DISTANCE</p>
<p>We show in this experiment that the log-probabilities of the offspring trees (utilized in LLEGO's mutation operator) are negatively correlated with the structural distances between parent and offspring solutions.</p>
<p>Experimental setting.We generate 1000 offspring trees using the LLEGO's mutation operator with a single parent tree.For each offspring individual, we assess its structural distance to the parent tree by computing the Tree Edit Distance (TED) (Bille, 2005) between this individual and the parent.</p>
<p>Observations.As shown in Figure 12, we observe a strong negative correlation (correlation coefficient = −0.85) between log-probabilities of the offspring and TED values.This relationship indicates that offspring with lower log-probabilities tend to exhibit greater structural differences from their parent, as measured by the TED.This demonstrates that LLEGO's log-probability-based selection mechanism inherently promotes diversity in the population by favoring mutations which introduce varied structural changes.We extend our comparisons against GATree by increasing the population size to N = 100 and the number of generations to G = 200, while keeping LLEGO's default hyperparameters.We report the results for the classification and regression tasks in Table 10 and Table 11.Despite GATree's larger number of evaluations, LLEGO evolved superior trees.This underscores the importance of LLEGO's integration of semantic priors, search guidance, and broader context to enhance search efficiency.This superior search efficiency is especially important in settings where evaluation costs significantly exceed search costs (e.g.complex simulations, hardware optimizations, robotics control).</p>
<p>D.6 RUN-TIME COMPARISONS</p>
<p>We provide the total runtimes for the different methods in Table 12, averaged across the 7 classification datasets used in Section 5.1.We also report in Table 13 the detailed timings for LLEGO and GATREE with varying population sizes (P ∈ {25, 100}) and generations (G ∈ {25, 100, 200}), and also report the number of functional evaluations.These results, along with the ones presented in Section 5.1, highlight that LLEGO evolves superior trees compared to GATREE while necessitating less functional evaluations and wall-clock time.Nevertheless, we acknowledge that there is room for improvement for the runtime of LLEGO.Potential solutions include (1) reducing runtime through inference acceleration techniques such as speculative decoding and vLLM serving (Leviathan et al., 2023) and (2) reducing memory requirements through specialized fine-tuned models or quantization (Han et al., 2015).</p>
<p>D.7.1 PROMPTING STRATEGIES</p>
<p>Experimental setting.We compare LLEGO to LLEGO naive , a variant which removes the crossover operator and changes the mutation prompt to an "improve the solution"-type of prompt.</p>
<p>Results.We report the results in Table 14, where we see that LLEGO consistently outperforms LLEGO naive .This demonstrates the importance of explicit fitness-guidance via the hyperparameter α in order to steer the search towards high-fitness regions.</p>
<p>D.7.2 DIFFERENT LLMS</p>
<p>A key property of LLEGO's design is that it is LLM-agnostic.To demonstrate the advantage of this flexibility, we evaluate LLEGO's performance using gpt-4, comparing it to gpt-3.5.We report the results in Table 15 for all the classification datasets, for depth 4 problems, across 3 seeds.We see that the gpt-4 variant of LLEGO achieves superior performance than its gpt-3.5 counterpart.These results have two significant implications, as they indicate that (1) LLEGO's effectiveness is robust across LLM architectures, and importantly that (2) its performance can scale with advances in capabilities of the underlying LLMs.</p>
<p>D.7.3 ARITY OF GENETIC OPERATIONS</p>
<p>In Figure 5, we compared the crossover dynamics between LLEGO XO with ν = 4 parents and roulette wheel selection, and GATree XO with ν = 2 parents and uniform parent sampling.In Figure 13 (Left), we compare LLEGO XO with ν = 2 parents and uniform parent sampling against GATree XO with ν = 2 parent and uniform parent sampling.In Figure 13 (Right), we compare LLEGO XO with ν = 4 parents and uniform parent sampling against GATree XO with ν = 2 parent and uniform parent sampling.</p>
<p>We observe similar dynamics as in Figure 5, where varying α enables to control the population fitness and diversity.Additionally, ν = 4 leads to significantly improved offspring fitness at the cost of a lower diversity, highlighting the nuanced impact of higher arity on search efficiency (corroborating the ablation results in Figure 7).Experimental setting.Specifically, we replace the roulette wheel selection (fitness-proportionate) mechanism with a tournament selection mechanism (Miller et al., 1995) with varying tournament   Experimental setting.Specifically, we compare two variants of LLEGO.The baseline variant, LLEGO Init. 1 corresponds to the instanciation of LLEGO described in Section 5, which initializes the population with CART models trained on bootstrap samples comprising 25% of the training data.In contrast, LLEGO Init. 2 initializes trees using CART models trained on minimal random subsets of just two training samples, resulting in weaker initial decision trees.</p>
<p>Observations. Figure 16 illustrates the convergence of the mean population fitness across generations, aggregated and normalized over all classification datasets for one random seed.The results demonstrate that LLEGO Init. 2 exhibits slower convergence compared to LLEGO Init. 1 , which shows the role of effective population initialization in improving search efficiency and faster discovery of high-quality solutions.Nevertheless, we remark that LLEGO Init. 2 still achieves good performance in the later stages of the search (after G = 20 generations), showing the effectiveness of LLEGO's variation operators in steering the search towards promising regions, independent of the initialization scheme.</p>
<p>D.8 FINE-GRAINED RESULTS</p>
<p>In this subsection, we present a detailed analysis of the results from the main paper.Our examination includes: (1) mutation dynamics observed in individual tasks; (2) convergence analysis across varying depths, including ablation studies; and (3) convergence trajectories for specific tasks.</p>
<p>Mutation dynamics.We provide the mutation dynamics for each individual dataset in Figure 17, showing that τ meaningfully controls the diversity in the population for 5 of the 7 classification datasets, where the diversity metrics are computed between parents and offspring (Top) and among the offspring (Bottom).Convergence analysis.We provide separate convergence plots in this subsection, obtained when optimizing trees of depths 3 and 4, under the experimental setup described in Section 5.1.The results are reported in Figure 18a and Figure 18b.In these two settings, LLEGO leads to a more efficient search compared to GATree.This improved efficiency also comes with a reduced diversity, showing that LLEGO concentrates its populations in the later generations in high-fitness regions.Ablation study.We report the ablation study results for depth 3 and 4 in Figure 19 and Figure 20.These results align with the observations made in Section 5.3, highlighting the importance of using crossover and mutation in tandem, the importance of incorporating more than 2 parents for the operators and using semantic information.With a higher maximum depth, the space of possible trees becomes more complex, and accentuates the need for both exploration and exploitation, which explains why the mutation only (LLEGO no_xo ) and crossover only (LLEGO no_mut ) baselines perform worse than LLEGO.</p>
<p>1 P 5 o d O y Z l V + i S M l S 1 p y F z 9 P T G h k d b j K L C d E T V D v e z N x P + 8 T m r C a 3 / C Z Z I a l G yx K E w F M T G Z f U 3 6 X C E z Y m w J Z Y r b W w k b U k W Z s d k U b A j e 8 s u r p H l R 9 i 7 L l X q l V L 3 J 4 s j D C Z z C O X h w B V W 4 g x o 0 g A H C M 7 z C m / P o v D j v z s e i N e d k M 8 f w B 8 7 n D 6 l / j N s = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y G Q S F E B t 7 T y a O x 8 d z 5 w N r k t 1 C W c = " &gt; A A A C O X i c b V B N S x x B E O 0 x 8 W v 9 y J o c v T R Z B A V Z Z k S i L A i i F 4 8 b z K q w P Q w 1 v T 3 a b k / P 0 F 0 j L u P 6 s 7 z 4 L 3 I T c s k h I X j 1 D 6 R n d w 9 + P W j 6 8 V 4 V V f X i X E m L v v / g T X 3 4 O D 0 z O z d f W 1 h c W v 5 U X / l 8 Y r P C c N H h m c r M W Q x W K K l F B y U q c Z Y b A W m s x G n c P 6 z 8 0 y t h r M z 0 D x z k I k z h X M t E c k A n R f U 2 Q 3 G N 5 T G k u R L D W 6 a L 2 7 G S g x E a b c t p K e A F B 1 U e D 6 M + 3 a O s X M f o c p M m 1 b f B W j e s d c m k 7 r r e k A 2 j e s N v + i P Q t y S Y k A a Z o B 3 V f 7 J e x o v U T e M K r O 0 G f o 5 h C Q Y l d y v V W G F F D r w P 5 6 L r q I Z U 2 L A c X T 6 k a 0 7 p 0 S Q z 7 m m k I / V 5 R w m p t Y M 0 d p X V F f a 1 V 4 n v e d 0 C k 9 2 w l D o v U G g + H p Q U i m J G q x h p T x r B U Q 0 c A W 6 k 2 5 X y C z D A 0 Y V d cy E E r 0 9 + S 0 6 2 m s G 3 5 v b 3 7 c b + w S S O O b J K v p J 1 E p A d s k + O S J t 0 C C d 3 5 B f 5 Q / 5 6 9 9 5 v 7 5 / 3 O C 6 d 8 i Y 9 X 8 g L e E / / A f E s r r E = &lt; / l a t e x i t &gt; Sample ⌫ parents: Sk = {(tj, f(tj) | j 2 [⌫]}</p>
<p>Figure 3 :
3
Figure 3: LLEGO MUT .In each operation, the mutation operator: (1) samples a set of λ ′ candidate offsprings Õ, (2) computes sampling weights, θ, inversely proportional to logprobs, with temperature τ controlling diversity, and (3) sample offspring via this weighted distribution Cat λ ′ (θ).</p>
<p>Figure 4 :
4
Figure 4: Search efficiency.Median fitness and diversity across 25 generations.</p>
<p>Figure 5 :
5
Figure 5: XO dynamics.</p>
<p>Figure 6 :
6
Figure 6: MUT dynamics.</p>
<p>Figure 7 :
7
Figure 7: Ablation study.Comparing search efficiency of ablations.</p>
<p>Figure 9 :
9
Figure 9: Prompt structure for crossover operation.</p>
<p>Figure 10 :
10
Figure 10: Example decision tree.And its corresponding natural language representation as a nested dictionary.</p>
<p>[0: 29.17%, 1: 70.83%].The features and their ranges are: [X_0 (int) [0, 3], X_1 (float) [5.00, 60.00], X_2 (int) [0, 4], X_3 (int) [0, 9], X_4 (float) [276.00,15672.00],X_5 (int) [0, 4], X_6 (int) [0, 4], X_7 (float) [1.00, 4.00], X_8 (int) [0, 3], X_9 (int) [0, 2], X_10 (float) [1.00, 4.00], X_11 (int) [0, 3], X_12 (float) [19.00, 74.00], X_13(int) [0, 2], X_14 (int) [0, 2], X_15 (float) [1.00, 4.00], X_16 (int) [0, 3], X_17 (float) [1.00, 2.00], X_18 (int) [0, 1], X_19 (int) [0, 1]].You should generate a diverse decision tree that is more interpretable.Please generate decision trees in the desired JSON format, you can use any of the features, but are only allowed to use operators [&lt;, &gt;, &lt;=, &gt;=].Return only the JSON in the format ## tree ##.Expression: ## {'X_16': {'&lt;= 1.5000': {'X_12': {'&lt;= 38.5000': {'X_4': {'&lt;= 2443.0000':{'value': 1}, '&gt; 2443.0000':{'value': 0}}}, '&gt; 38.5000': {'X_1': {'&lt;= 21.0000': {'value': 0}, '&gt; 21.0000': {'value': 1}}}}}, '&gt; 1.5000': {'X_0': {'&lt;= 1.5000': {'X_3': {'&lt;= 5.5000': {'value': 0}, '&gt; 5.5000': {'value': 1}}}, '&gt; 1.5000': {'X_1': {'&lt;= 19.0000': {'value': 1}, '&gt; 19.0000': {'value': 1}}}}}}} ## Expression: ## {'X_0': {'&lt;= 0.5000': {'X_4': {'&lt;= 976.5000': {'X_3': {'&lt;= 3.5000': {'value': 0}, '&gt; 3.5000': {'value': 0}}}, '&gt; 976.5000': {'X_5': {'&lt;= 1.5000': {'value': 0}, '&gt; 1.5000': {'value': 1}}}}}, '&gt; 0.5000': {'X_4': {'&lt;= 13765.5000':{'X_12': {'&lt;= 22.5000': {'value': 0}, '&gt; 22.5000': {'value': 1}}}, '&gt; 13765.5000':{'value': 0}}}}} ## Expression: ## {'X_5': {'&lt;= 2.5000': {'X_4': {'&lt;= 9597.5000':{'X_2': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 1}}}, '&gt; 9597.5000':{'value': 0}}}, '&gt; 2.5000': {'X_0': {'&lt;= 0.5000': {'X_11': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 1}}}, '&gt; 0.5000': {'X_10': {'&lt;= 2.5000': {'value': 1}, '&gt; 2.5000': {'value': 1}}}}}}} ## Expression: ## {'X_2': {'&lt;= 0.5000': {'X_12': {'&lt;= 23.5000': {'value': 1}, '&gt; 23.5000': {'value': 0}}}, '&gt; 0.5000': {'X_5': {'&lt;= 3.5000': {'X_12': {'&lt;= 25.5000': {'value': 0}, '&gt; 25.5000': {'value': 1}}}, '&gt; 3.5000': {'X_4': {'&lt;= 1034.5000':{'value': 0}, '&gt; 1034.5000':{'value': 1}}}}}}} ## Expression: ## Listing 3: Example mutation prompt with semantics removed.On credit dataset.The task is to generate interpretable and high-performing decision trees given a set of attributes.The dataset contains 360 samples and 20 features, of which 7 are numerical and 13 are categorical.The target variable is y, it is binary, the label distribution is [0: 29.17%, 1: 70.83%].The features and their ranges are: [X_0 (int) [0, 3], X_1 (float) [5.00, 60.00], X_2 (int) [0, 4], X_3 (int) [0, 9], X_4 (float) [276.00,15672.00],X_5 (int) [0, 4], X_6 (int) [0, 4], X_7 (float) [1.00, 4.00], X_8 (int) [0, 3], X_9 (int) [0, 2], X_10 (float) [1.00, 4.00], X_11 (int) [0, 3], X_12 (float) [19.00, 74.00], X_13 (int) [0, 2], X_14 (int) [0, 2], X_15 (float) [1.00, 4.00], X_16 (int) [0, 3], X_17 (float) [1.00, 2.00], X_18 (int) [0, 1], X_19 (int) [0, 1]].Generate a different, interpretable decision tree which should have the improved fitness.Please generate decision trees in the desired JSON format, you can use any of the features, but are only allowed to use operators [&lt;, &gt;, &lt;=, &gt;=].Return only the JSON in the format ## tree ##.</p>
<p>Difference in equal opportunity.When evaluating fairness, we consider difference in equal opportunity (DEO).This score measures the difference in recall between unprivileged and privileged groups, where a value of DEO = 0 indicates equality of opportunity.DEO = |p(ŷ = 1 | group = 1, y = 1) − p(ŷ = 1 | group = 0, y = 1)|</p>
<p>Figure 11 :
11
Figure 11: Accuracy-fairness tradeoff.On compas dataset.</p>
<p>Figure 12 :
12
Figure12: between log-probabilities and stuctural distances.There is a strong negative correlation between log-probabilities of the offspring and their TED values with respect to the parent tree.</p>
<p>Per-generation runtime Total run-time # Functional evaluations GATREE (N = 25, G = 25) additional investigations into several key variables affecting LLEGO performance.We investigate the impact of diverse prompting strategies (Appendix D.7.1), the selection of different LLMs as genetic operators (Appendix D.7.2), and the impact of arity of genetic operations(Appendix D.7.3).Additionally, we analyze how various parent sampling mechanisms for crossover and mutation influence outcomes(Appendices D.7.4 and D.7.5), alongside an evaluation of different population initialization strategies (Appendix D.7.6).</p>
<p>Figure 13 :
13
Figure 13: XO dynamics.Effect of fitness guidance (α) on population and diversity using uniformly sampled parents.(Left) ν = 2 parents, (Right) ν = 4 parents</p>
<p>Figure 15 :
15
Figure 15: Mutation dynamics with Quality-Diversity selection.Offspring diversity increases with the number of niches employed in CVT-MAP-Elites for parent selection.</p>
<p>Figure 16 :
16
Figure 16: Ablation on the population initialization.LLEGO Init. 1 initializes populations using CART models trained on 25% bootstrap samples, while LLEGO Init. 2 uses minimal training subsets of size 2. Results are aggregated across all classification datasets for one seed.</p>
<p>Figure 17 :
17
Figure 17: MUT dynamics.Effect of diversity guidance (τ ) on (Top) median parent-offspring distance and (Bottom) median offspring distance.</p>
<p>Figure 18 :
18
Figure 18: Convergence dynamics.Comparing LLEGO with GATREE.</p>
<p>Figure 19 :
19
Figure 19: Additional ablation results.Depth = 3.</p>
<p>Max population fitness.</p>
<p>Figure 20 :Figure 21 :
2021
Figure 20: Additional ablation results.Depth = 4.</p>
<p>Figure 22 :
22
Figure 22: Convergence plots.Mean population fitness (↑) of LLEGO and GATREE on individual tasks across 25 generations (depth=4).</p>
<p>Table 1 :
1
Performance on classification tasks.Balanced accuracy (↑) on 7 datasets, reporting mean (std) and emboldening best results.We also report average rank (↓) for comparing baselines.
MethodBreastCompasCreditDiabetesHeartLiverVehicleRank (↓)depth = 3CART0.941 (0.009)0.655 (0.012)0.668 (0.021)0.710 (0.029)0.734 (0.068)0.646 (0.025)0.903 (0.021)2.9 (0.83)C4.50.938 (0.012)0.650 (0.009)0.579 (0.030)0.687 (0.045)0.704 (0.019)0.569 (0.047)0.857 (0.039)4.9 (1.07)DL850.947 (0.008) 0.665 (0.005)0.590 (0.045)0.703 (0.027)0.688 (0.024)0.598 (0.034)0.932 (0.009)3.0 (1.73)GOSDT0.935 (0.005)0.641 (0.004)0.681 (0.000)0.698 (0.012)0.651 (0.086)0.656 (0.018)0.852 (0.047)4.4 (2.15)GATREE 0.942 (0.009)0.647 (0.005)0.648 (0.045)0.681 (0.027)0.669 (0.031)0.626 (0.037)0.922 (0.022)4.3 (1.11)LLEGO0.946 (0.011)0.652 (0.004)0.679 (0.007)0.713 (0.015) 0.736 (0.024) 0.672 (0.019) 0.937 (0.017)1.6 (0.79)depth = 4CART0.945 (0.010)0.660 (0.011)0.675 (0.019)0.704 (0.026)0.713 (0.059)0.632 (0.063)0.925 (0.020)2.9 (0.90)C4.50.942 (0.013)0.660 (0.006)0.622 (0.043)0.699 (0.024)0.714 (0.032)0.585 (0.046)0.921 (0.011)3.6 (1.13)DL850.939 (0.012)0.661 (0.004)0.576 (0.017)0.671 (0.020)0.706 (0.058)0.561 (0.019)0.898 (0.065)4.7 (1.50)GOSDT0.938 (0.007)0.641 (0.004)0.680 (0.002)0.701 (0.011)0.677 (0.028)0.660 (0.016)0.885 (0.019)4.3 (1.89)GATREE 0.941 (0.008)0.650 (0.007)0.658 (0.013)0.675 (0.038)0.676 (0.025)0.633 (0.047)0.895 (0.033)4.6 (0.98)LLEGO0.951 (0.007) 0.663 (0.005) 0.684 (0.011) 0.721 (0.017) 0.751 (0.042) 0.676 (0.021) 0.929 (0.015)1.0 (0.00)</p>
<p>Table 2 :
2
Performance on regression tasks.MSE (↓) across 5 regression datasets, best results emboldened.
1.0Median Pop Fitness0.50Median Pop DiversityLLEGOLLEGOGATREE0.40GATREE0.90.30MethodAbaloneCarsCholesterolWageWinedepth = 30.80.20CART GATREE 0.595 (0.044) 0.591 (0.027)0.250 (0.028) 0.198 (0.039)1.500 (0.244) 1.427 (0.187)1.036 (0.146) 0.811 (0.009) 1.150 (0.149) 0.825 (0.016)0.10LLEGO0.584 (0.030)0.200 (0.037)1.324 (0.139)1.045 (0.149)0.814 (0.010)0.75101520250.00510152025CART0.561 (0.018)depth = 4 0.269 (0.041) 1.552 (0.230)1.185 (0.193)0.807 (0.004)Generations (G)GATREE 0.586 (0.036)0.100 (0.020)1.343 (0.158)1.188 (0.168)0.847 (0.017)LLEGO0.577 (0.029)0.099 (0.020) 1.322 (0.145) 1.067 (0.203)0.828 (0.026)</p>
<p>Table 3 :
3
Comparison with the related works.LLEGO provides a general framework for global optimization of decision trees, contrasting with prior works along several dimensions: computational complexity, support for different objective and regularization functions, task types, and incorporation of structural and semantic priors.
MethodAlgorithmWorst-case complexityObjective functionArbitrary regularizationTask Classification Regression Structural Semantic PriorsCART (BreimanGreedyO(2 h )Gini impurity/MSE✗✓✓✓✗etal.,1984)C4.5 (Quinlan,GreedyO(2 h )Information gain✗✓✗✓✗1993)DL8.5 (AglinDPO(d!)Additive functions✗✓✗✓✗etal.,2020)GOSDTDP(Lin et al.,2020)</p>
<p>use LLMs as mutation operators for code {task_description}.The dataset contains {n_samples} samples and {n_attributes} features, of which {n_numerical} are numerical and {n_categorical} are categorical.The target variable is {target_name}, it is {target_type}, {label_information}.The features and their ranges are: {feature_semantics}.You should generate a diverse decision tree that is more interpretable.Please generate decision trees in the desired JSON format, you can use any of the features, but are only allowed to use operators [&lt;, &gt;, &lt;=, &gt;=].Return only the JSON in the format ## tree ##.Figure 8: Prompt structure for mutation operation.{task_description}.The dataset contains {n_samples} samples and {n_attributes} features, of which {n_numerical} are numerical and {n_categorical} are categorical.The target variable is {target_name}, it is {target_type}, {label_information}.The features and their ranges are: {feature_semantics}.Generate a different, interpretable decision tree which should have the improved fitness.Please generate decision trees in the desired JSON format, you can use any of the features, but are only allowed to use operators [&lt;, &gt;, &lt;=, &gt;=].Return only the JSON in the format ## tree ##.</p>
<p>Table 4 :
4
(Vanschoren et al., 2014)ils of open-source datasets from OpenML(Vanschoren et al., 2014).# Cat: number of categorical attributes, # Num: number of numerical attributes, Label dist: label distribution.
DatasetID# Samples # Attributes # Num # CatLabelLabel distrcredit31100020713binary0: 29.17%, 1: 70.83%diabetes37768880binary0: 66.30%, 1: 33.70%compas4219252781358binary0: 52.50%, 1: 47.50%heart532701358binary0: 52.58%, 1: 47.42%liver14805831091binary0: 67.94%, 1: 32.06%breast15699990binary0: 65.34%, 1: 34.66%vehicle99484618180binary0: 73.03%, 1: 26.97%cholesterol 2043031367continuous-wine287649711110continuous-wage5345341037continuous-abalone449564177871continuous-cars4499480417116continuous-</p>
<p>Table 5 :
5
Hyperparameter search ranges.Hyperparameter search ranges for all baselines.
min_samples_split [int, 2, 16]min_samples_split [int, 1, 16]CARTmax_depthfixedsplitterbestcriterion['squared_error' (reg), 'gini' (clas)]min_samples_split [int, 2, 16]C4.5min_samples_split [int, 1, 16]max_depthfixedDL8.5min_sup max_depth[int, 1, 10] fixedGOSDTregularization max_depth[float, 0.001, 1] fixedpopulation_size[int, 10, 50]mutation_prob[float, 0.1, 0.5]GATreecrossover_prob[float, 0.1, 0.95]max_iterations100tournament size2max_depthfixed</p>
<p>Table 6 :
6
Relative generalization gap.Averaged over the classification datasets.
MethodDepth=3 Depth=4C450.0730.086CART0.0780.101DL850.1310.161GATREE0.0860.092GOSDT0.0440.052LLEGO0.0430.043</p>
<p>Table 7 :
7
Performance on classification tasks.Comparing LLEGO with LLEGO no_semantics (i.e.all semantic information removed).Best results are emboldened.
MethodCompasCreditDiabetesHeartLiverdepth = 3LLEGO no_semantics 0.654 (0.010) 0.683 (0.012)0.700 (0.033)0.726 (0.030)0.643 (0.033)LLEGO0.652 (0.004)0.679 (0.007)0.713 (0.015) 0.736 (0.024) 0.672 (0.019)depth = 4LLEGO no_semantics0.659 (0.011)0.667 (0.024)0.701 (0.013)0.716 (0.038)0.651 (0.025)LLEGO0.663 (0.005) 0.684 (0.011) 0.721 (0.017) 0.751 (0.042) 0.676 (0.021)
(Wong et al., 2014)hat LLEGO's superior performance does not rely on memorization, we evaluate it on two proprietary datasets (requiring authorized access, and hence extremely unlikely to be in the LLM training corpus): MAGGIC (heart failure,(Wong et al., 2014)) and CUTRACT (prostate cancer,</p>
<p>Table 8 :
8
Performance on proprietary datasets.Comparing LLEGO with CART and GATree, with depth = 4. Best results are emboldened.As illustrated in the previous experiments, the genetic operators in LLEGO benefit from the properties of LLMs (i.e.semantic priors and wide context).It is then natural to wonder if, conversely, negative artifacts of LLMs may propagate to the decision trees found by LLEGO.
MethodMAGGICCUTRACTCART0.610 (0.014)0.694 (0.038)GATree 0.619 (0.015)0.706 (0.024)LLEGO0.623 (0.007) 0.710 (0.009)D.3 ADDRESSING BIAS VIA REGULARIZATION</p>
<p>Table 9 :
9
Fairness aware objective.
Method FA?Compas (race) ACC (↑) DEO (↓)CART✗0.651 (0.012) 0.255 (0.016)C4.5✗0.650 (0.008) 0.258 (0.014)DL85✗0.666 (0.006) 0.264 (0.008)GOSDT✗0.641 (0.003) 0.187 (0.019)LLEGO✗0.652 (0.004) 0.308 (0.070)LLEGO✓0.651 (0.002) 0.161 (0.071)</p>
<p>Table 10 :
10
Comparison against GATree.Balanced accuracy (↑) on classification tasks (depth d = 4).
MethodBreastCompasCreditDiabetesHeartLiverVehicleGATREE (N = 100, G = 200) 0.948 (0.011)0.658 (0.003)0.667 (0.009)0.684 (0.013)0.738 (0.028)0.635 (0.019)0.939 (0.017)LLEGO (N = 25, G = 25)0.951 (0.007) 0.663 (0.005) 0.684 (0.011) 0.721 (0.017) 0.751 (0.042) 0.676 (0.021)0.929 (0.015)</p>
<p>Table 11 :
11
Comparison against GATree.MSE (↓) on regression tasks (depth d = 4).
MethodAbaloneCarsCholesterolWageWineGATREE (N = 100, G = 200) 0.566 (0.022) 0.099 (0.012)1.395 (0.202)1.143 (0.147)0.829 (0.027)LLEGO (N = 25, G = 25)0.577 (0.029)0.099 (0.025) 1.322 (0.145) 1.067 (0.203) 0.828 (0.026)</p>
<p>Table 12 :
12
Runtime comparisons (all methods).Total runtime (in seconds), averaged across 7 classification datasets.
CART C4.5 DL85 GOSDT GATREE LLEGOTotal run time (depth d = 3) 0.00220.0822.10261.1415.50407.66Total run time (depth d = 4) 0.00230.13172.70 234.4415.77430.32</p>
<p>Table 13 :
13
Runtime comparisons (GP methods).Per-generation, total runtime (in seconds), and total number of fitness evaluations (depth d = 4, averaged across 7 classification datasets).</p>
<p>Table 14 :
14
Performance of naive prompting.Test balanced accuracy (↑) on classification tasks (depth d = 4, 3 seeds), reporting mean (std) .
MethodBreastCompasCreditDiabetesHeartLiverVehicleLLEGO naive0.942 (0.006)0.660 (0.011)0.670 (0.003)0.708 (0.019)0.714 (0.051)0.629 (0.033)0.943 (0.015)LLEGO0.951 (0.007) 0.663 (0.005) 0.684 (0.011) 0.721 (0.017) 0.751 (0.042) 0.676 (0.021)0.929 (0.015)</p>
<p>Table 15 :
15
Performance of different LLMs.Test balanced accuracy (↑) on classification tasks (depth d = 4, 3 seeds), reporting mean (std) .LLEGO (gpt-4) 0.957 (0.005) 0.671 (0.011) 0.684 (0.008) 0.741 (0.023) 0.751 (0.017) 0.640 (0.025) 0.951 (0.015)
MethodBreastCompasCreditDiabetesHeartLiverVehicleLLEGO (gpt-35) 0.951 (0.007)0.663 (0.005)0.684 (0.011)0.721 (0.017)0.751 (0.042) 0.676 (0.021)0.929 (0.015)
Also available at the wider lab repository https://github.com/vanderschaarlab/LLEGO.
ACKNOWLEDGMENTSWe thank the anonymous ICLR reviewers, members of the van der Schaar lab, and Andrew Rashbass for many insightful comments and suggestions.TL would like to thank AstraZeneca for their sponsorship and support.NH thanks Illumina for their support.This work was supported by Microsoft's Accelerate Foundation Models Academic Research initiative.D F i 6 3 r X g H c g = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W I S 6 s C R S 1 G W x i C 4 K r W A f 0 J Y y m U 7 b o Z N M m L k R S 4 g / 4 c Z f c e N C E b f i z r 9 x m n a h r Q c G D u e c y 5 1 7 3 I A z B b b 9 b S w s L i 2 v r K b W 0 u s b m 1 v b 5 s 5 u T Y l Q E l o l g g v Z c L G i n P m 0 C g w 4 b Q S S Y s / l t O 4 O i 2 O / f k e l Y s K / h V F A 2 x 7 u + 6 z H C A Y t d c z j F t B 7 i I p S K Cq w 3 6 9 P 6 m o 1 m r P n O I f o D a / w D H s u h E g = = &lt; / l a t e x i t &gt; OMUT o↵spring &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d o 5 M k U c I a r S b 6 59 a T 9 W K 9 W x + z 0 Z w 1 3 z l E f 2 B 9 / g B x j q C K C 1 U J T h k z 6 j U Y z y Z B 8 s 2 w Z x n 9 x Y f H i H + L F g 5 p p e / D X g 8 D j v e 9 L v r w k z p R 0 G A Q P t f r M 7 L e 5 7 / M L j R + L P 5 e W m y u r F 8 7 k V k B X G G V s L + Y O l N T Q R Y k K e p k F n s Y K L u P b w 8 q / / A P W S a P P c Z T B I O V X W i Z S c P R S 1 J Q M 4 S 8 W h y b N c g S a S N T g X K e k y Z a J b n 5 R x h q s w x J j u V K s Q 7 1 G m d S U p R y v B V f F S R l N d u i d l J S J P P v C O u 6 e l 3 d 3 U b M V t I M x 6 G c S T k m L T H E a N f + z o R F 5 C h q F 4 s 7 1 w y D D Q c E t S q G g b L D c Q c b F L bETHICS AND REPRODUCIBILITY STATEMENTSEthics.In this work, we evaluate both public benchmarks and private datasets.The private datasets are de-identified and used following the guidance of the respective data providers.We follow recommendations to use the Azure OpenAI service when using GPT models, where via the agreement we ensure the medical data is not sent for human review or stored, hence respecting the guidelines given by the dataset providers.Reproducibility.We provide all the details on the datasets, the implementation of baselines and the LLM in Appendix C. Furthermore, we detail the prompts used by the crossover and the mutation operators in Appendix B. We provide the code to reproduce our results at https://github.com/nicolashuynh/LLEGO, and https://github.com/tennisonliu/LLEGO. 1 evolution, sampling mutation instructions from predefined sets.LLMs also have been utilized as variation operators for prompt optimization(Fernando et al., 2024), where task prompts contain explicit directives for generating variations.These approaches generate unguided variations, primarily utilizing LLMs'instruction-following capabilities.For example, in(Guo et al., 2024), crossover is performed using the prompt template: "Cross over the following prompts and generate a new prompt".Recent works have also considered the integration of LLMs with advanced evolutionary frameworks, namely quality-diversity algorithms(Pugh et al., 2016), to evolve both neural architectures and variation prompts(Nasir et al., 2024).In contrast, LLEGO generates guided variations, utilizing incontext learning of patterns in parent solutions to generate intelligent variations.Specifically, LLEGO steers offspring towards high-fitness regions by conditioning on desired fitness, while LLEGO controls diversity and exploration with the hyperparameter to define the offspring sampling distribution.Finally, recent work(Ye et al., 2024)has proposed using LLM for meta-heuristic optimization.It differs from LLEGO as it focuses on finding general meta-heuristics for a set of optimization tasks rather than tailoring the search with dataset-specific characteristics and relevant domain knowledge as LLEGO does.A.2 NO FREE LUNCHIn accordance with the principle of No Free Lunch(Wolpert &amp; Macready, 1997), we expect LLEGO to excel in domains with the following characteristics:1. Natural language representation: Problems where solutions are expressible in natural language, enabling LLEGO to employ the LLM's semantic and contextual understanding for effective variations.2. Complex genotype-phenotype mapping: Tasks with low locality, where LLEGO's semantic prior enhances variation efficacy.3. Contextual knowledge: Domains benefiting from broader knowledge, where contextual knowledge (e.g.clinical guidelines for risk scoring) can be flexibly incorporated via prompt design (C).This integration remains non-trivial for traditional evolutionary algorithms.4. Challenging operator design: Areas where conventional semantic operators are difficult to craft (e.g.preserving semantics in program synthesis).LLEGO offers broadly applicable and flexibly customizable semantic variation operators.These characteristics are prevalent in many applications, including decision trees, mathematical equations, and symbolic programs.B COMPLETE PROMPTSPrompt design.In this section, we describe the details of the prompts.To recap, each of the genetic operations is realized through natural language queries to the LLM.Each prompt is constructed of three essential elements:1. Task context.This includes information about the input space X , the output space Y, and the characteristics of the dataset D, e.g.number of samples, categorical features, continuous features.It also includes feature summary characteristics that are computed on the training set.2. Parent trees.This contains the tree structure of each parent and possibly the fitness metric (in the case of crossover).These are translated to natural language and provided as few-shot examples to perform ICL in each genetic operation.3. Task-specific instructions.For each genetic operator, we include task-specific instructions on offspring generations and guidelines on the format of the response.The structured prompt for mutation is described in Figure8. Descriptions enclosed in {}, such as {task_description} represent placeholder values that are populated dynamically at run-time.For a concrete example of this, the mutation prompt on credit dataset is shown in full in Listing 1.Similarly, the structured prompt for crossover is described in Figure9with a concrete example shown in Listing 2.Listing 1: Example mutation prompt.On credit dataset.The task is to classify people described by a set of attributes as good or bad credit risks.Listing 2: Example crossover prompt.On credit dataset.Tree representation.We represent trees in natural language as a nested dictionary.This dictionary represents a decision tree where each key is a feature and the subsequent nested dictionaries correspond to decision rules and their outcomes.An example is illustrated in Figure10on the iris dataset.In this example, if 'petal width (cm)' is less than or equal to 0.80, the classification is 0; otherwise, further splits are made on 'petal width (cm)' at 1.75, leading to classifications of 1 or 2 depending on the condition.{ "petal width (cm)": { "&lt;= 0.80": {"value": 0}, "&gt; 0.80": { "petal width (cm)": { "&lt;= 1.75": {"value": 1}, "&gt; 1.75": {"value": 2} } } } } Dataset preprocessing.We preprocess the dataset using a train-validation-test split ratio of [0.2, 0.4, 0.4].The low training split is used to accentuate the difference in performance as given sufficient training data, all methods perform comparably.For each run, we only vary the seed used for data splitting, such that for seed 0, we use train_test_split(seed=0).For any algorithms that have inherent randomness (i.e.CART and GATree), we seed them with seed=42.As such, the randomness reported is induced only by different datasets.We do not apply any additional preprocessing to continuous features.For categorical features, we follow the recommendations provided in §9.2.4 of(Hastie et al., 2009), where we rank each category of the predictor by calculating the proportion of observations that fall into the outcome class 1(Hastie et al., 2009).This results in a ranking of the categories based on these proportions.No additional preprocessing is applied to categorical or continuous labels.C.2 IMPLEMENTATION DETAILSBaselines.To assess the performance of LLEGO, we compare it against a comprehensive set of state-of-the-art algorithms, covering representative methods from main categories of tree induction.Specifically, CART and C4.5 are greedy tree induction methods, GOSDT and DL8.5 are optimal tree induction methods, and GATree is a genetic programming based approach:• CART (Classification and Regression Trees)(Breiman et al., 1984): CART is a decision tree algorithm that splits data into subsets based on feature values, creating a binary tree for classification or regression tasks using measures like Gini impurity or mean squared error.We use the implementation provided in sklearn.tree,https://scikit-learn.org/stable/modules/ generated/sklearn.tree.DecisionTreeClassifier.html.• C4.5(Quinlan, 1993): C4.5 is an extension of the ID3 algorithm that generates decision trees by handling both categorical and continuous data, and uses information gain ratio to choose splits.We use the implementation provided in the PyPI package c45-decision-tree, https: //pypi.org/project/c45-decision-tree/.• GOSDT(Lin et al., 2020): GOSDT constructs decision trees by optimizing a trade-off between accuracy and complexity, ensuring sparsity and interpretability through global optimization techniques.We use the implementation provided by the original authors https: //github.com/ubc-systopia/gosdt-guesses.• DL8.5 (Aglin et al., 2020): DL8.5 is a decision tree learning algorithm that focuses on constructing optimal decision trees given specific constraints, using dynamic programming to find the best tree structure.We use the implemented provided in the PyPI package dl8.5, https://github.com/ubc-systopia/gosdt-guesses.• GATree(Lahovnik, 2024): GATree is a Python library designed for implementing evolutionary decision trees using a genetic algorithm approach.We use the official implementation https://gatree.readthedocs.io/en/latest/and keep the defaults settings of the implementation (i.e.tournament selection, subtree crossover and subtree mutation).Hyperparameter search ranges.Next, we detail the hyperparameters of each method, and their respective search ranges.Across experiments, we keep max_depth fixed to enable fair comparison, the details of tunable hyperparameters are detailed in Table5.Hyperparameter tuning.We use Optuna(Akiba et al., 2019)and the default Tree-Parzen Estimator for hyperparameter tuning (HPT)(Watanabe, 2023).For all baselines, we permit wall-clock time to a maximum of 10 minutes.This allows 50 iterations of HPT for CART and C4.5, and 10 iterations for the computationally more intensive DL8.5, GOSDT, and GATree.In each iteration of HPT, we evaluate the objective on the validation set, selecting the best configuration to evaluate on the test set.Computer resources.We run all experiments on an AMD EPYC 7V13 64-Core Processor.C.3 LLEGO IMPLEMENTATION DETAILSFor our instantiation of LLEGO in Section 5, we use N = 25 and G = 25.We seed the algorithm with a population of trees generated by CART, where each tree is fitted on 25% of the D train .We use the same population to initialize GATree.In each iteration, we generate 25 crossover offspring and 25 mutation offspring, using a rejection mechanism where invalid solutions are discarded (in Section 5, ∼ 86% of crossover and ∼ 88% of mutation offspring are syntactically valid).We use sizes k ∈ {1, 2, 3, 5}.We then compute the median offspring fitness and diversity as a function of k, following the experimental setup described in Section 5.2.Observations.The results, shown in Figure14, demonstrate a clear trade-off between fitness and diversity which is modulated by the tournament size.As shown in Figure14a, larger tournament sizes consistently yield higher population fitness, while Figure14bshows a corresponding decrease in population diversity.Indeed, larger values of k intensify selection pressure by increasing the probability that highly fit individuals win multiple tournaments, thereby reducing population diversity.Conversely, smaller values of k lead to an increased population diversity.For example, when k = 1, tournament selection corresponds to random sampling, which maximizes diversity at the cost of fitness performance.In comparison to tournament selection, the roulette wheel selection mechanism employed in LLEGO achieves a good middle-ground by striking a balance between fitness and diversity.In this experiment, we investigate an alternative choice for the selection mechanism in LLEGO's mutation operator.Experimental setting.We replace the random parent selection in the mutation operator with the quality-diversity algorithm CVT-MAP-Elites(Vassiliades et al., 2017), which requires defining a behavioral space.Given n training samples, we define the behavioral space for classification tasks as H = [0, 1] n , encompassing the trees' functional signatures.The CVT-MAP-Elites algorithm then partitions H into M niches using uniformly distributed centroids found with k-means clustering.We then select parents for the mutation operator by uniformly sampling ν niches and selecting the best individual in the sampled niches.Finally, we compute the offspring diversity, similarly as in Section 5.2.Observations.We report the results in Figure15, averaged across the classification datasets.We see that the total number of niches M serves as a control parameter for offspring diversity, with an increasing relationship between diversity and the number of niches M .When M = 1, the process reduces to repeatedly sampling the population's best individual, resulting in minimal diversity for the generated offspring.Conversely, when solutions are spread into distinct niches, the sampling process becomes equivalent to uniform sampling without replacement from the population, yielding high diversity.Furthermore, we see in Figure15that the random selection of parents employed in LLEGO comparatively yields high diversity, justifying its use in the diversity-guided mutation operator.D.7.6 POPULATION INITIALIZATION STRATEGIESIn this experiment, we investigate the impact of a different population initialization on the search performance of LLEGO.
Statlog, 10.24432/C57303UCI Machine Learning Repository. </p>
<p>Learning optimal decision trees using caching branch-and-bound search. Siegfried Gaël Aglin, Pierre Nijssen, Schaus, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Optuna: A next-generation hyperparameter optimization framework. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, Masanori Koyama, Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining2019</p>
<p>Machine bias. Julia Angwin, Jeff Larson, Lauren Kirchner, Surya Mattu, May 2016</p>
<p>Symbolic regression via genetic programming. Douglas Adriano, Augusto Helio, J C Barbosa, Sixth Brazilian symposium on neural networks. IEEE20001Proceedings</p>
<p>Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. Rachel Ke Bellamy, Kuntal Dey, Michael Hind, Stephanie Samuel C Hoffman, Kalapriya Houde, Pranay Kannan, Jacquelyn Lohia, Sameep Martino, Aleksandra Mehta, Mojsilović, IBM Journal of Research and Development. 634/52019</p>
<p>Determinants of wages from the 1985 current population survey. The practice of econometrics: classic and contemporary. E R Berndt, 1991</p>
<p>A survey on tree edit distance and related problems. Philip Bille, Theoretical computer science. 3371-32005</p>
<p>Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G Mantovani, Jan N Van Rijn, Joaquin Vanschoren, arXiv:1708.03731v2stat.ML]Openml benchmarking suites. 2019</p>
<p>A comparison of selection schemes used in evolutionary algorithms. Tobias Blickle, Lothar Thiele, Evolutionary Computation. 441996</p>
<p>Marko Bohanec, 10.24432/C5JP48Car Evaluation. UCI Machine Learning Repository. 1997</p>
<p>Deep neural networks and tabular data: A survey. Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, Gjergji Kasneci, IEEE Transactions on Neural Networks and Learning Systems. 2022</p>
<p>Classification and regression trees. Leo Breiman, 2017Routledge</p>
<p>Classification and Regression Trees. Leo Breiman, Jerome Friedman, Charles J Stone, Olshen, 1984CRC Press</p>
<p>Evolutionary nas with gene expression programming of cellular encoding. Cliford Broni-Bediako, Yuki Murata, Masayasu Luiz Hb Mormille, Atsumi, 2020 IEEE symposium series on computational intelligence (SSCI). IEEE2020</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Enhancing genetic improvement mutations using large language models. James Alexander Ei Brownlee, Karine Callan, Alina Even-Mendoza, Carol Geiger, Justyna Hanna, Federica Petke, Dominik Sarro, Sobania, International Symposium on Search Based Software Engineering. Springer2023</p>
<p>Khaled Rasheed, and Xiuping Tao. Decision tree and ensemble learning algorithms with their applications in bioinformatics. Software tools and algorithms for biological systems. Dongsheng Che, Qi Liu, 2011</p>
<p>Paulo Cortez, A Cerdeira, F Almeida, T Matos, J Reis, 10.24432/C56S3TWine Quality. UCI Machine Learning Repository. 2009</p>
<p>. Cutract, Cutract, 2019</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. Chrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, Tim Rocktäschel, Forty-first International Conference on Machine Learning. 2024</p>
<p>Openml-ctr23-a curated tabular regression benchmarking suite. Sebastian Felix Fischer, Matthias Feurer, Bernd Bischl, AutoML Conference 2023 (Workshop). 2023</p>
<p>Genetic algorithms in search, optimization, and machine learning. De Goldberg, 1989</p>
<p>Why do tree-based models still outperform deep learning on typical tabular data? Advances in neural information processing systems. Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux, 202235</p>
<p>Deep space network scheduling using evolutionary computational methods. Alexandre Guillaume, Seugnwon Lee, Yeou-Fang Wang, Hua Zheng, Robert Hovden, Savio Chau, Yu-Wen Tung, Richard J Terrile, 2007 IEEE Aerospace Conference. IEEE2007</p>
<p>Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Song Han, Huizi Mao, William J Dally, arXiv:1510.00149Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. 2015arXiv preprint</p>
<p>The elements of statistical learning: data mining, inference, and prediction. Trevor Hastie, Robert Tibshirani, Jerome H Friedman, Jerome H Friedman, 2009Springer2</p>
<p>Interpretable policies for reinforcement learning by genetic programming. Daniel Hein, Steffen Udluft, Thomas A Runkler, Engineering Applications of Artificial Intelligence. 762018</p>
<p>Optimal sparse decision trees. Xiyang Hu, Cynthia Rudin, Margo Seltzer, Advances in Neural Information Processing Systems. 201932</p>
<p>compas-recidivism-risk-score-data-and-analysis. 2016Northpointe Inc. Compas risk scales</p>
<p>Heart Disease. UCI Machine Learning Repository. Andras Janosi, William Steinbrunn, Matthias Pfisterer, Detrano Robert, 10.24432/C52P4X1988</p>
<p>The uci machine learning repository. Markelle Kelly, Rachel Longjohn, Kolby Nottingham, </p>
<p>Concept formation and decision tree induction using the genetic programming paradigm. John R Koza, International Conference on Parallel Problem Solving from Nature. Springer1990</p>
<p>Genetic programming as a means for programming computers by natural selection. John R Koza, Statistics and computing. 41994a</p>
<p>Krzysztof Krawiec and Tomasz Pawlak. Approximating geometric crossover by semantic backpropagation. John R Koza, Proceedings of the 11th Annual conference on Genetic and evolutionary computation. the 11th Annual conference on Genetic and evolutionary computationMIT press1994b. 2009. 2013Proceedings of the 15th annual conference on Genetic and evolutionary computation</p>
<p>Applying genetic programming technique in classification trees. Chan-Sheng Kuo, Tzung-Pei Hong, Chuen-Lung Chen, Soft Computing. 112007</p>
<p>Tadej Lahovnik, Gatree -gatree 0.1.4 documentation. 05/19/2024</p>
<p>Foundations of genetic programming. B William, Riccardo Langdon, Poli, 2013Springer Science &amp; Business Media</p>
<p>Constructing optimal binary decision trees is np-complete. Hyafil Laurent, Ronald L Rivest, Information processing letters. 511976</p>
<p>Evolution through large models. Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, Kenneth O Stanley, Handbook of Evolutionary Machine Learning. Springer2023</p>
<p>Fast inference from transformers via speculative decoding. Yaniv Leviathan, Matan Kalman, Yossi Matias, International Conference on Machine Learning. PMLR2023</p>
<p>Generalized and scalable optimal sparse decision trees. Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, Margo Seltzer, International Conference on Machine Learning. PMLR2020</p>
<p>Nabeel Seedat, and Mihaela van der Schaar. Large language models to enhance bayesian optimization. Tennison Liu, Nicolás Astorga, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Fully autonomous programming with large language models. Vadim Liventsev, Anastasiia Grishina, Aki Härmä, Leon Moonen, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2023</p>
<p>Eureka: Human-level reward design via coding large language models. Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Linxi Zhu, Anima Fan, Anandkumar, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Gradtree: Learning axis-aligned decision trees with gradient descent. Sascha Marton, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt, arXiv:2305.035152023arXiv preprint</p>
<p>Language model crossover: Variation through few-shot prompting. Elliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover, Joel Lehman, arXiv:2302.121702023arXiv preprint</p>
<p>Genetic algorithms, tournament selection, and the effects of noise. Brad L Miller, David E Goldberg, Complex systems. 931995</p>
<p>Geometric semantic genetic programming. Alberto Moraglio, Krzysztof Krawiec, Colin G Johnson, Parallel Problem Solving from Nature-PPSN XII: 12th International Conference. Taormina, ItalySpringerSeptember 1-5, 2012. 2012Proceedings, Part I 12</p>
<p>Problems in the analysis of survey data, and a proposal. N James, John A Morgan, Sonquist, Journal of the American statistical association. 583021963</p>
<p>Warwick Nash, Tracy Sellers, Simon Talbot, Andrew Cawthorn, Wes Ford, 10.24432/C55C7WAbalone. UCI Machine Learning Repository. 1995</p>
<p>Llmatic: Neural architecture search via large language models and quality diversity optimization. Sam Muhammad Umair Nasir, Julian Earle, Steven Togelius, Christopher James, Cleghorn, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2024</p>
<p>Statlog (Vehicle Silhouettes). Mowforth Pete, Barry Shepherd, 10.24432/C5HG6NUCI Machine Learning Repository. </p>
<p>Quality diversity: A new frontier for evolutionary computation. Justin K Pugh, Lisa B Soros, Kenneth O Stanley, Frontiers in Robotics and AI. 32028452016</p>
<p>D-code: Discovering closed-form odes from observed trajectories. Krzysztof Zhaozhi Qian, Mihaela Kacprzyk, Van Der Schaar, International Conference on Learning Representations. 2022</p>
<p>Induction of decision trees. J Ross Quinlan, Machine learning. 11986</p>
<p>Quinlan Ross, C4. 5: Programs for Machine Learning. Morgan Kaufmann1993</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Equality of opportunity. E John, Alain Roemer, Trannoy, Handbook of income distribution. Elsevier20152</p>
<p>Top-down induction of decision trees classifiers-a survey. Lior Rokach, Oded Maimon, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 3542005</p>
<p>Design of modern heuristics: principles and application. Franz Rothlauf, 2011Springer8</p>
<p>Using the adap learning algorithm to forecast the onset of diabetes mellitus. Jack W Smith, James E Everhart, William C Dickson, Robert Knowler, Johannes Scott, Proceedings of the annual symposium on computer application in medical care. the annual symposium on computer application in medical care1988261</p>
<p>Application of decision tree algorithm for data mining in healthcare operations: a case study. Farhad Soleimanian, Peyman Mohammadi, Parvin Hakimi, Int J Comput Appl. 5262012</p>
<p>Position: Leverage foundational models for black-box optimization. Xingyou Song, Yingtao Tian, Robert Tjarko Lange, Chansoo Lee, Yujin Tang, Yutian Chen, Forty-first International Conference on Machine Learning. 2024</p>
<p>Nuclear feature extraction for breast tumor diagnosis. Nick Street, William H Wolberg, Olvi L Mangasarian, Biomedical image processing and biomedical visualization. SPIE19931905</p>
<p>Maptree: Beating "optimal" decision trees with bayesian decision trees. Colin Sullivan, Mo Tiwari, Sebastian Thrun, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>A study on efficient generation of decision trees using genetic programming. Toru Tanigawa, Qiangfu Zhao, Proceedings of the 2nd Annual Conference on Genetic and Evolutionary Computation. the 2nd Annual Conference on Genetic and Evolutionary Computation2000</p>
<p>Openml: networked science in machine learning. Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, Luis Torgo, ACM SIGKDD Explorations Newsletter. 1522014</p>
<p>Using centroidal voronoi tessellations to scale up the multidimensional archive of phenotypic elites algorithm. Vassilis Vassiliades, Konstantinos Chatzilygeroudis, Jean-Baptiste Mouret, IEEE Transactions on Evolutionary Computation. 2242017</p>
<p>Fairness definitions explained. Sahil Verma, Julia Rubin, Proceedings of the international workshop on software fairness. the international workshop on software fairness2018</p>
<p>Learning optimal classification trees using a binary linear program formulation. Sicco Verwer, Yingqian Zhang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Tree-structured parzen estimator: Understanding its algorithm components and their roles for better empirical performance. Shuhei Watanabe, arXiv:2304.111272023arXiv preprint</p>
<p>No free lunch theorems for optimization. H David, William G Wolpert, Macready, IEEE transactions on evolutionary computation. 111997</p>
<p>Heart failure in younger patients: the meta-analysis global group in chronic heart failure (maggic). Nathaniel M Chih M Wong, Mark C Hawkins, Pardeep S Petrie, Roy S Jhund, Cono A Gardner, Katrina K Ariti, Nikki Poppe, Gillian A Earle, Iain B Whalley, Squire, European heart journal. 35392014</p>
<p>Haoran Ye, Jiarui Wang, Zhiguang Cao, and Guojie Song. Reevo: Large language models as hyper-heuristics with reflective evolution. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma ; Chengrun, Xuezhi Yang, Yifeng Wang, Hanxiao Lu, Quoc V Liu, Denny Le, Xinyun Zhou, Chen, ArXiv, abs/2402.01145International Conference on Learning Representations. 2021. 2024. 2024An explanation of in-context learning as implicit bayesian inference</p>
<p>A multi-objective genetic programming approach to developing pareto optimal decision trees. Huimin Zhao, Decision Support Systems. 4332007</p>
<p>Non-greedy algorithms for decision tree optimization: An experimental comparison. Arman Zharmagambetov, Suryabhan Singh Hada, Magzhan Gabidolla, Miguel A Carreira-Perpinán, IEEE, 2021. 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 1}}}, '&gt; 9597.5000': {'value': 0}}}}} ## Expression: ## {'property_magnitude': {'&lt;= 0.5000': {'duration': {'&lt;= 33.0000': {'housing': {'&lt;= 1.5000': {'value': 1}, '&gt; 1.5000': {'value': 0}}}. 2021 International Joint Conference on Neural Networks (IJCNN). 33.0000': {'employment': {'&lt;= 0.5000': {'value': 0}, '&gt; 0.5000': {'value': 0}}}}}, '&gt; 0.5000': {'employment': {'&lt;= 0.5000': {'credit_amount': {'&lt;= 3359.5000': {'value': 0}, '&gt; 3359.5000': {'value': 1}}}, '&gt; 0.5000': {'purpose': {'&lt;= 5.5000': {'value': 1}, '&gt; 5.5000': {'value': 1}}}}}}} ## Expression: ##</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4483 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4483</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4483</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-7d506280c8cc30569cfe0dbe921fbf18814d254c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7d506280c8cc30569cfe0dbe921fbf18814d254c" target="_blank">Machine-Learning Rationalization and Prediction of Solid-State Synthesis Conditions</a></p>
                <p><strong>Paper Venue:</strong> Chemistry of Materials</p>
                <p><strong>Paper TL;DR:</strong> A machine-learning approach is demonstrated that predicts synthesis conditions using large solid-state synthesis data sets text-mined from scientific journal articles, finding that optimal heating temperatures have strong correlations with the stability of precursor materials quantified using melting points and formation energies.</p>
                <p><strong>Paper Abstract:</strong> There currently exist no quantitative methods to determine the appropriate conditions for solid-state synthesis. This not only hinders the experimental realization of novel materials but also complicates the interpretation and understanding of solid-state reaction mechanisms. Here, we demonstrate a machine-learning approach that predicts synthesis conditions using large solid-state synthesis data sets text-mined from scientific journal articles. Using feature importance ranking analysis, we discovered that optimal heating temperatures have strong correlations with the stability of precursor materials quantified using melting points and formation energies (ΔGf, ΔHf). In contrast, features derived from the thermodynamics of synthesis-related reactions did not directly correlate to the chosen heating temperatures. This correlation between optimal solid-state heating temperature and precursor stability extends Tamman’s rule from intermetallics to oxide systems, suggesting the importance of reaction kinetics in determining synthesis conditions. Heating times are shown to be strongly correlated with the chosen experimental procedures and instrument setups, which may be indicative of human bias in the data set. Using these predictive features, we constructed machine-learning models with good performance and general applicability to predict the conditions required to synthesize diverse chemical systems.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4483.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4483.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TMR text-mining pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-mined Recipes (TMR) dataset and NLP/text-mining pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-scale natural language processing and information-retrieval pipeline used to extract >30,000 solid-state synthesis 'recipes' (reaction entries) from the scientific literature, producing the TMR dataset used as training data for downstream ML analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TMR NLP/text-mining pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The authors used an NLP and information-retrieval pipeline (references cited in the paper) to parse journal articles and extract structured synthesis records (precursors, targets, heating temperature, heating time, procedural keywords, etc.) into the TMR dataset. Extracted fields were then featurized (133 engineered features spanning precursor properties, composition indicators, reaction thermodynamics, and experiment-adjacent flags) for ML modeling. The pipeline details (parsing rules, entity extraction, relation linking) are referenced but not specified in architecture-level detail in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / solid-state chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset comprises over 30,000 text-mined solid-state synthesis reactions (TMR dataset) extracted from the literature; exact number of source papers not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Enabling dataset for empirical relationships and correlations (empirical correlations between synthesis conditions and precursor properties discovered downstream). The pipeline itself extracts quantitative experimental data (temperatures, times) and contextual metadata from text.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>The pipeline provides the data used to discover correlations such as the relation between precursor melting points and optimal heating temperature (see downstream ML models). The pipeline does not itself output equations; it produces structured numerical/textual data used by regressors that produced explicit relationships (see ML predictor entry).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Natural language processing and information retrieval applied to full-text journal articles to extract numeric synthesis conditions and procedural descriptors; produced structured records (precursors, products, temperature, time, keywords) that were featurized into 133 features for ML.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Data quality validated implicitly via downstream predictive-model performance and by removing overlaps with test dataset (PCD) for OOS testing; paper discusses dataset imbalances and limitations. No independent annotation-based precision/recall metrics for the NLP extraction are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports downstream ML predictive performance (e.g., R^2 for temperature/time models) as an indirect measure of dataset utility; no standalone extraction accuracy metrics (precision/recall/F1) for the text-mining pipeline are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported for the text-mining pipeline itself (no numerical precision/recall/F1 provided).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Authors note dataset biases (imbalanced distributions, e.g., <5% non-air atmospheres), extraction incompleteness (many PCD entries lack heating times), and human-reporting bias in literature; these limit the diversity and fidelity of extracted labels and downstream model generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No direct baseline NLP system comparisons reported in this paper; the pipeline is used as the primary data source and its quality is indirectly evaluated via ML cross-validation and OOS tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine-Learning Rationalization and Prediction of Solid-State Synthesis Conditions', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4483.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4483.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML condition predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning synthesis-condition predictor (dominance-importance feature ranking, linear regressors, and XGBoost)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined statistical ML pipeline that (1) ranks feature importance using dominance importance analysis, and (2) trains interpretable linear regressors and non-linear gradient-boosted tree models (XGBoost) on text-mined synthesis data to learn quantitative relationships between precursor/experimental features and synthesis conditions (temperature and time), including a data-driven formalization/extension of Tamman's rule.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML synthesis-condition discovery pipeline (DI analysis + linear models + XGBoost)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system featurizes each extracted synthesis record into 133 engineered features (precursor melting points, ΔHf, ΔGf, element-presence indicators, thermodynamic driving-force features, and experiment-adjacent flags). Dominance importance (DI) analysis (IDI, IADI, APDI) ranks features by predictive power. Linear models (weighted least squares) are trained with BIC-based feature selection to yield interpretable coefficients (used to quantify element/feature additive effects on predicted temperature/time). A non-linear model (gradient-boosted regression trees implemented with XGBoost) is trained and hyperparameter-tuned to capture non-linear relations and to improve generalizability. Models are cross-validated via leave-one-out CV and tested out-of-sample on the independently extracted PCD dataset. The pipeline is used to (a) identify which features most strongly correlate with synthesis conditions and (b) fit explicit empirical linear relationships (e.g., extensions of Tamman's rule).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XGBoost (gradient-boosted regression trees) and linear weighted least-squares regressors</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / solid-state chemistry (solid-state synthesis conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained on the TMR dataset assembled from literature (>30,000 text-mined synthesis reactions); tested OOS on the PCD literature-derived dataset (size not specified for entries used).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical/empirical-correlation laws and quantitative relationships (linear regression relationships, proportionality rules, and predictive regression models correlating precursor properties to optimal synthesis temperature and experiment-adjacent features to heating time).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Explicit quantitative relationships discovered/extracted by the ML system include: 1) A proportional relation between optimal synthesis temperature and average precursor melting point, fit as T_Tamman = α*(avg T_melt) + β + ε, with fitted slope α ≈ 1/3 (paper reports slope ~1/3) and β a fitted intercept; 2) Tamman's original-style fit using minimum precursor melting point: T_Tamman = α*(min T_melt) + ε with α = 1.2 (carbonate reactions) and α = 0.8 (non-carbonate reactions) in the authors' fits; 3) Feature-importance derived empirical rules: average precursor melting point, ΔG_f and ΔH_f of precursors strongly correlate with selected heating temperature, while experiment-adjacent categorical features (e.g., 'sintering', 'ball-milling', 'polycrystal', 'phosphor') strongly correlate with chosen heating times. (Equations and numerical α values taken directly from the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Quantitative laws were extracted by (1) assembling a large structured dataset via text-mining, (2) computing engineered numeric/textual features for each synthesis record, (3) using dominance-importance analysis to identify high-impact features, and (4) fitting linear regressions (for interpretable coefficients and explicit equations) and XGBoost regressors (to capture non-linearities). The fitted linear coefficients provide explicit quantitative laws/empirical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Leave-one-out cross-validation (LOOCV) used to assess predictive performance; out-of-sample evaluation on an independent literature-derived dataset (PCD) to assess generalizability; model selection used Bayesian information criterion (BIC) for linear models; statistical significance of coefficients assessed with p-values (5% level); comparison of linear vs non-linear model LOOCV and OOS performance reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported predictive metrics: For heating temperature prediction: LOOCV/general cross-validated R^2 ≈ 0.5–0.6 and mean absolute error (MAE) ≈ 134–147 °C. For heating time prediction (transformed variable log10(1/t)): R^2 ≈ 0.3 and MAE ≈ 0.30–0.32 log10(h^-1) (interpreted as predicted time within factor ≈ [0.5 t, 2 t]). Tamman's-rule linear fit (using avg or min melting point) achieved lower R^2 (~0.2–0.3) but is statistically significant (high F-statistic, small p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No explicit 'success rate' fraction reported for extracted laws; performance reported via R^2 and MAE as above (these quantify how well learned relationships predict experimental conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Authors report limitations including: dataset biases and human selection bias in reported syntheses (affecting heating times); thermodynamic driving-force features provided little predictive power for temperatures in this literature-derived positive-results dataset; dataset imbalance (e.g., atmosphere labels rare); dataset shift/generalizability (performance drops when moving from TMR to PCD, R^2 decrease ~0.1), possible overfitting for non-linear models if not regularized; assumptions made (e.g., one-shot syntheses, good synthesizability) that may not hold broadly; limited interpretability of some non-linear models compared to linear coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons included: 1) Tamman's-rule baseline (fitting T as proportional to min or avg precursor melting point) — ML models outperform simple Tamman fits in predictive power; 2) Linear regressors vs XGBoost — XGBoost achieves higher training R^2 and often higher LOOCV on PCD (better generalizability), though linear models are more interpretable; 3) OOS evaluation on PCD shows degradation relative to LOOCV but XGBoost is more robust to dataset shift than linear models when many features are used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine-Learning Rationalization and Prediction of Solid-State Synthesis Conditions', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4483",
    "paper_id": "paper-7d506280c8cc30569cfe0dbe921fbf18814d254c",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "TMR text-mining pipeline",
            "name_full": "Text-mined Recipes (TMR) dataset and NLP/text-mining pipeline",
            "brief_description": "A literature-scale natural language processing and information-retrieval pipeline used to extract &gt;30,000 solid-state synthesis 'recipes' (reaction entries) from the scientific literature, producing the TMR dataset used as training data for downstream ML analyses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TMR NLP/text-mining pipeline",
            "system_description": "The authors used an NLP and information-retrieval pipeline (references cited in the paper) to parse journal articles and extract structured synthesis records (precursors, targets, heating temperature, heating time, procedural keywords, etc.) into the TMR dataset. Extracted fields were then featurized (133 engineered features spanning precursor properties, composition indicators, reaction thermodynamics, and experiment-adjacent flags) for ML modeling. The pipeline details (parsing rules, entity extraction, relation linking) are referenced but not specified in architecture-level detail in the paper.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science / solid-state chemistry",
            "number_of_papers": "Dataset comprises over 30,000 text-mined solid-state synthesis reactions (TMR dataset) extracted from the literature; exact number of source papers not specified.",
            "law_type": "Enabling dataset for empirical relationships and correlations (empirical correlations between synthesis conditions and precursor properties discovered downstream). The pipeline itself extracts quantitative experimental data (temperatures, times) and contextual metadata from text.",
            "law_examples": "The pipeline provides the data used to discover correlations such as the relation between precursor melting points and optimal heating temperature (see downstream ML models). The pipeline does not itself output equations; it produces structured numerical/textual data used by regressors that produced explicit relationships (see ML predictor entry).",
            "extraction_method": "Natural language processing and information retrieval applied to full-text journal articles to extract numeric synthesis conditions and procedural descriptors; produced structured records (precursors, products, temperature, time, keywords) that were featurized into 133 features for ML.",
            "validation_approach": "Data quality validated implicitly via downstream predictive-model performance and by removing overlaps with test dataset (PCD) for OOS testing; paper discusses dataset imbalances and limitations. No independent annotation-based precision/recall metrics for the NLP extraction are reported in this paper.",
            "performance_metrics": "The paper reports downstream ML predictive performance (e.g., R^2 for temperature/time models) as an indirect measure of dataset utility; no standalone extraction accuracy metrics (precision/recall/F1) for the text-mining pipeline are reported here.",
            "success_rate": "Not reported for the text-mining pipeline itself (no numerical precision/recall/F1 provided).",
            "challenges_limitations": "Authors note dataset biases (imbalanced distributions, e.g., &lt;5% non-air atmospheres), extraction incompleteness (many PCD entries lack heating times), and human-reporting bias in literature; these limit the diversity and fidelity of extracted labels and downstream model generalizability.",
            "comparison_baseline": "No direct baseline NLP system comparisons reported in this paper; the pipeline is used as the primary data source and its quality is indirectly evaluated via ML cross-validation and OOS tests.",
            "uuid": "e4483.0",
            "source_info": {
                "paper_title": "Machine-Learning Rationalization and Prediction of Solid-State Synthesis Conditions",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "ML condition predictor",
            "name_full": "Machine-learning synthesis-condition predictor (dominance-importance feature ranking, linear regressors, and XGBoost)",
            "brief_description": "A combined statistical ML pipeline that (1) ranks feature importance using dominance importance analysis, and (2) trains interpretable linear regressors and non-linear gradient-boosted tree models (XGBoost) on text-mined synthesis data to learn quantitative relationships between precursor/experimental features and synthesis conditions (temperature and time), including a data-driven formalization/extension of Tamman's rule.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ML synthesis-condition discovery pipeline (DI analysis + linear models + XGBoost)",
            "system_description": "The system featurizes each extracted synthesis record into 133 engineered features (precursor melting points, ΔHf, ΔGf, element-presence indicators, thermodynamic driving-force features, and experiment-adjacent flags). Dominance importance (DI) analysis (IDI, IADI, APDI) ranks features by predictive power. Linear models (weighted least squares) are trained with BIC-based feature selection to yield interpretable coefficients (used to quantify element/feature additive effects on predicted temperature/time). A non-linear model (gradient-boosted regression trees implemented with XGBoost) is trained and hyperparameter-tuned to capture non-linear relations and to improve generalizability. Models are cross-validated via leave-one-out CV and tested out-of-sample on the independently extracted PCD dataset. The pipeline is used to (a) identify which features most strongly correlate with synthesis conditions and (b) fit explicit empirical linear relationships (e.g., extensions of Tamman's rule).",
            "model_name": "XGBoost (gradient-boosted regression trees) and linear weighted least-squares regressors",
            "model_size": null,
            "scientific_domain": "Materials science / solid-state chemistry (solid-state synthesis conditions)",
            "number_of_papers": "Trained on the TMR dataset assembled from literature (&gt;30,000 text-mined synthesis reactions); tested OOS on the PCD literature-derived dataset (size not specified for entries used).",
            "law_type": "Empirical/empirical-correlation laws and quantitative relationships (linear regression relationships, proportionality rules, and predictive regression models correlating precursor properties to optimal synthesis temperature and experiment-adjacent features to heating time).",
            "law_examples": "Explicit quantitative relationships discovered/extracted by the ML system include: 1) A proportional relation between optimal synthesis temperature and average precursor melting point, fit as T_Tamman = α*(avg T_melt) + β + ε, with fitted slope α ≈ 1/3 (paper reports slope ~1/3) and β a fitted intercept; 2) Tamman's original-style fit using minimum precursor melting point: T_Tamman = α*(min T_melt) + ε with α = 1.2 (carbonate reactions) and α = 0.8 (non-carbonate reactions) in the authors' fits; 3) Feature-importance derived empirical rules: average precursor melting point, ΔG_f and ΔH_f of precursors strongly correlate with selected heating temperature, while experiment-adjacent categorical features (e.g., 'sintering', 'ball-milling', 'polycrystal', 'phosphor') strongly correlate with chosen heating times. (Equations and numerical α values taken directly from the paper.)",
            "extraction_method": "Quantitative laws were extracted by (1) assembling a large structured dataset via text-mining, (2) computing engineered numeric/textual features for each synthesis record, (3) using dominance-importance analysis to identify high-impact features, and (4) fitting linear regressions (for interpretable coefficients and explicit equations) and XGBoost regressors (to capture non-linearities). The fitted linear coefficients provide explicit quantitative laws/empirical rules.",
            "validation_approach": "Leave-one-out cross-validation (LOOCV) used to assess predictive performance; out-of-sample evaluation on an independent literature-derived dataset (PCD) to assess generalizability; model selection used Bayesian information criterion (BIC) for linear models; statistical significance of coefficients assessed with p-values (5% level); comparison of linear vs non-linear model LOOCV and OOS performance reported.",
            "performance_metrics": "Reported predictive metrics: For heating temperature prediction: LOOCV/general cross-validated R^2 ≈ 0.5–0.6 and mean absolute error (MAE) ≈ 134–147 °C. For heating time prediction (transformed variable log10(1/t)): R^2 ≈ 0.3 and MAE ≈ 0.30–0.32 log10(h^-1) (interpreted as predicted time within factor ≈ [0.5 t, 2 t]). Tamman's-rule linear fit (using avg or min melting point) achieved lower R^2 (~0.2–0.3) but is statistically significant (high F-statistic, small p-values).",
            "success_rate": "No explicit 'success rate' fraction reported for extracted laws; performance reported via R^2 and MAE as above (these quantify how well learned relationships predict experimental conditions).",
            "challenges_limitations": "Authors report limitations including: dataset biases and human selection bias in reported syntheses (affecting heating times); thermodynamic driving-force features provided little predictive power for temperatures in this literature-derived positive-results dataset; dataset imbalance (e.g., atmosphere labels rare); dataset shift/generalizability (performance drops when moving from TMR to PCD, R^2 decrease ~0.1), possible overfitting for non-linear models if not regularized; assumptions made (e.g., one-shot syntheses, good synthesizability) that may not hold broadly; limited interpretability of some non-linear models compared to linear coefficients.",
            "comparison_baseline": "Comparisons included: 1) Tamman's-rule baseline (fitting T as proportional to min or avg precursor melting point) — ML models outperform simple Tamman fits in predictive power; 2) Linear regressors vs XGBoost — XGBoost achieves higher training R^2 and often higher LOOCV on PCD (better generalizability), though linear models are more interpretable; 3) OOS evaluation on PCD shows degradation relative to LOOCV but XGBoost is more robust to dataset shift than linear models when many features are used.",
            "uuid": "e4483.1",
            "source_info": {
                "paper_title": "Machine-Learning Rationalization and Prediction of Solid-State Synthesis Conditions",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01106925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Machine-learning rationalization and prediction of solid-state synthesis conditions</h1>
<p>Haoyan Huo, ${ }^{\dagger, \ddagger}$ Christopher J. Bartel, ${ }^{\dagger, \ddagger}$ Tanjin He, ${ }^{\dagger, \ddagger}$ Amalie Trewartha, ${ }^{\ddagger, \boldsymbol{<em>}}$<br>Alexander Dunn, ${ }^{\dagger, \S}$ Bin Ouyang, ${ }^{\dagger, \ddagger}$ Anubhav Jain, ${ }^{\S}$ and Gerbrand Ceder ${ }^{</em>}, \dagger, \ddagger$<br>$\dagger$ Department of Materials Science and Engineering, University of California, Berkeley, 210<br>Hearst Memorial Mining Building, Berkeley, CA, 94720, USA<br>$\ddagger$ Materials Sciences Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA, 94720, USA<br>$\llbracket$ Present address: Toyota Research Institute, 4440 El Camino Real, Los Altos, CA, 94022, USA<br>§Energy Technologies Area, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA, 94720, USA<br>E-mail: gceder@berkeley.edu</p>
<h4>Abstract</h4>
<p>There currently exist no quantitative methods to determine the appropriate conditions for solid-state synthesis. This not only hinders the experimental realization of novel materials but also complicates the interpretation and understanding of solidstate reaction mechanisms. Here, we demonstrate a machine-learning approach that predicts synthesis conditions using large solid-state synthesis datasets text-mined from scientific journal articles. Using feature importance ranking analysis, we discovered that optimal heating temperatures have strong correlations with the stability of precursor materials quantified using melting points and formation energies $\left(\Delta G_{f}, \Delta H_{f}\right)$.</p>
<p>In contrast, features derived from the thermodynamics of synthesis-related reactions did not directly correlate to the chosen heating temperatures. This correlation between optimal solid-state heating temperature and precursor stability extends Tamman's rule from intermetallics to oxide systems, suggesting the importance of reaction kinetics in determining synthesis conditions. Heating times are shown to be strongly correlated with the chosen experimental procedures and instrument setups, which may be indicative of human bias in the dataset. Using these predictive features, we constructed machine-learning models with good performance and general applicability to predict the conditions required to synthesize diverse chemical systems.</p>
<h1>Introduction</h1>
<p>While solid-state synthesis is the prevailing approach for making inorganic solids, the determination of synthesis conditions for new solids is mostly based on heuristics and humanacquired experiences, with no analytical predictive approaches. ${ }^{1,2}$ Recent work has focused on rationalizing solid-state reaction pathways observed in in-situ experiments, ${ }^{3-7}$ by decomposing them into a sequence of phase evolution steps ${ }^{1}$ that can be modeled using thermodynamic calculations. ${ }^{8-11}$ To design synthesis routes for new materials, it is essential to understand why certain conditions are preferred and develop models for predicting these conditions for synthesis (e.g., temperature, time). While thermodynamic calculations have been used to rationalize synthesis conditions in specific chemical systems, ${ }^{8,12}$ a synthesis condition predictor with broad applicability for general inorganic compounds is still elusive.</p>
<p>Here, we use statistical machine-learning (ML) methods to systematically learn and quantitatively evaluate synthesis condition predictors from a large set of experimental data. Such ML approaches require large, high-quality synthesis datasets covering many chemistries, which have only recently become available through the application of natural language processing (NLP) and information retrieval techniques on the large body of scientific literature. ${ }^{13-19}$ In this work, using the dataset of over 30,000 text-mined solid-state synthesis</p>
<p>reactions (denoted as the text-mined "recipes" or the TMR dataset in this paper), ${ }^{16}$ we demonstrate an inductive ML approach that learns synthesis conditions from the knowledge parsed from the past literature.</p>
<p>The overall pipeline of our ML approach is shown in Fig. 1. Datasets of synthesis conditions compiled from NLP/text-mined datasets are used to train ML models. Each synthesis reaction was represented using a set of human-designed features, which will be discussed in more detail in subsequent sections. Interpretable ML models were trained on this basis of features to predict two key solid-state synthesis conditions that must be specified for any reaction: heating temperature and heating time.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic of the ML methods developed in this work for predicting solid-state synthesis conditions.</p>
<p>Throughout this paper, the prediction of solid-state synthesis conditions is defined as regression (point estimations) of the two experimental condition variables - temperature and time. Several important assumptions have been made: a) Good synthesizability is assumed, ${ }^{20-23}$ i.e., when a publication reports the synthesis of some material at a specified set of conditions, we assume that this reaction was successful. b) Synthesis experiments are performed in a one-shot fashion, i.e., reactants react and form the target compound in a single heating step, such that a simple synthesis route of "mix and heat" would be sufficient. c) The ML models predict the "optimal" synthesis conditions as implicitly defined by the</p>
<p>consensus of training data.
Note that the above assumptions oversimplify the synthesis condition prediction problem. These assumptions are often violated in many cases of practical solid-state syntheses. For example, a simple one-shot reaction route can thermodynamically favor an impurity phase which can only be avoided by using a multi-step synthesis with specific intermediate compounds; ${ }^{11,24}$ solid-state syntheses are often performed with many more degrees of freedom, such as special heating schedules, ${ }^{8,24}$ special mixing devices, ${ }^{25}$ different sintering aids, ${ }^{26}$ etc. Moreover, heating atmosphere strongly affects target material formation by changing the chemical potentials of gas species. ${ }^{27} \mathrm{ML}$ models require sufficient and consistent data to draw statistically significant conclusions, ${ }^{28,29}$ while the dataset used in this work has too imbalanced distributions for these additional labels. For example, only $&lt;5 \%$ of the reactions in the TMR dataset have non-air synthesis atmospheres. Therefore, the aforementioned conditions, although are present in the TMR dataset, are not predicted by the ML models in this work. Modeling of these factors may become possible as text-mined datasets become abundant in the future. ${ }^{30}$</p>
<p>In this work, we considered 133 synthesis features describing four aspects of solid-state syntheses: 1) precursor properties, 2) composition of the target material, 3) reaction thermodynamics, and 4) experimental procedure setup. We ranked these features according to their predictive power using dominance importance (DI) analysis. ${ }^{31}$ The features were used to train linear and non-linear (tree-based) regressors for synthesis heating temperature and time. For all models, we split the dataset into reactions with carbonate precursors and reactions without carbonate reactions. This splitting is necessary because the release of CO 2 gas in carbonate precursor materials systematically shifts the reaction driving forces for this subset and, consequently, the coefficients of the related features in linear models. Grouping the dataset into carbonate and non-carbonate reactions thus fits two sets of coefficients that accounts for this shift and improves the overall performance. We performed leave-oneout cross-validation (LOOCV) to diagnose model performance. We also used out-of-sample</p>
<p>(OOS) evaluation on Pearson's Crystal Data ${ }^{32}$ (another synthesis dataset independently extracted from the literature, denoted as the PCD dataset in this paper) to test model generalizability on unseen datasets. The detailed data pre-processing and model construction can be found in the Methods section.</p>
<p>Our ML results achieve a goodness-of-fit measured by $R^{2} \sim 0.5-0.6$ and mean absolute error (MAE) $\sim 140^{\circ} \mathrm{C}$ for heating temperature prediction. To compare with, typical heating temperatures used in solid-state synthesis range from $\sim 500^{\circ} \mathrm{C}$ to $\sim 1500^{\circ} \mathrm{C}$. For heating time prediction, the time variable is transformed into a new prediction variable representing reaction speed: $t \rightarrow \log <em 10="10">{10}(1 / t)$. The goodness-of-fit for this new time variable is $R^{2} \sim 0.3$ and MAE is $\sim 0.3 \log </em> \cdot t\right]$, or $[0.5 t, 2 t])$. Analysis of the model predictive power reveals that heating temperature prediction is dominated by precursor properties, which we hypothesize to be linked to reaction kinetics. Heating time prediction is dominated by experimental operations, which may be indicative of human selection bias. The ML methods developed and applied in this work provide a statistically rigorous approach towards learning robust synthesis predictors from large datasets mined from the scientific literature.}\left(h^{-1}\right)$ (e.g., if the predicted time is $t$, the MAE estimates a range of $\left[10^{-0.3} \cdot t, 10^{0.3</p>
<h1>Results</h1>
<h2>Synthesis feature selection using dominance analysis</h2>
<p>In total, we created 133 features in four categories: 1) precursor properties - 12 features calculated from melting points, standard enthalpy of formation $\Delta H_{f}^{300 K}$, and standard Gibbs free energy of formation $\Delta G_{f}^{300 K}$ of precursors; 2) composition of the target material - 74 indicator variables representing the presence (1) or absence (0) of different chemical elements in the target compound; 3) reaction thermodynamics - 33 descriptive features of the driving forces for synthesis-relevant reactions constructed by decomposing synthesis into multi-step phase evolution paths using previously developed principles; ${ }^{7,8} 4$ ) experiment-adjacent features - 14</p>
<p>indicator variables representing whether certain devices, procedures, and/or additives were used in the synthesis procedure. See Methods for a more detailed description of how each of these classes of features were computed.</p>
<p>We first use DI analysis ${ }^{31}$ to rank the predictive power of these features. In DI analysis, one constructs many linear models that predict outcomes using subsets of features, called submodels. DI analysis then calculates the incremental effect of a feature $f_{i}$ on submodels that do not use $f_{i}$ in three different ways. The average partial dominance importance (APDI) value for $f_{i}$ is computed as the average increase of model performance, measured by $R^{2}$, when $f_{i}$ is added to any submodel that does not include $f_{i}$. In other words, APDI measures the averaged gain of predictive power by including a feature. Individual dominance importance (IDI) values are the $R^{2}$ of models trained using only one feature and quantify the predictive power of the features by themselves. Interactional dominance importance (IADI) values are the decrease of model $R^{2}$ when a feature is removed from the whole model that uses all features, therefore measuring the gain of predictive power by a feature over all other features. All three DI values are computed for both heating temperature and time prediction models and are shown in Fig. 2. We split the dataset into carbonate reactions (reactions with at least one carbonate precursor) and non-carbonate reactions (reactions with no carbonate precursors). This is necessary because these two subsets have dissimilar distributions of reaction thermodynamic driving forces, which must be separated to be modeled in linear regression. ${ }^{33,34}$</p>
<p>We first evaluate the predictive powers of the features by themselves, as demonstrated by the IDI values in Fig. 2. For heating temperature prediction, Fig. 2 (a) and (b) show that the IDI values of the average precursor melting points are significantly higher than those of other features. Average precursor melting points alone achieve $R^{2} \sim 0.2-0.3$ for heating temperature prediction. Other features, such as experimental Gibbs free energy of formation at standard conditions $\Delta G_{f}^{300 K}$ and experimental enthalpy of formation at standard conditions $\Delta H_{f}^{300 K}$ of precursors, are also highly predictive features as measured</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: DI values and rankings of top 15 synthesis features for heating temperature models (a and b) and heating time models (c and d). The dataset is split into carbonate reactions (reactions with at least one carbonate precursor) (a and c) and non-carbonate reactions (reactions with no carbonate precursors) (b and d). Interactional dominance DI (IADI): decrease of model $R^{2}$ when a feature is removed from the whole model that uses all features. Individual dominance DI (IDI): $R^{2}$ of models trained using only one feature. Average partial dominance DI (APDI): average $R^{2}$ increase when a feature is added to a submodel. Features are ordered according to the sum of all three DI values.</p>
<p>by IDI. Note that precursor melting points, $\Delta G_{f}^{300 K}$, and $\Delta H_{f}^{300 K}$ are likely to be good proxy variables for precursor reactivity. The next set of predictive features as ranked by IDI are compositional indicator variables (e.g., indicating the presence/absence of $\mathrm{Li}, \mathrm{Mo}, \mathrm{Bi}$, etc.). These features can be understood as chemistry-specific corrections to heating temperatures. Note that ML models aim to reduce prediction errors for the whole training dataset, which is dominated by the elements that are characteristic of large application fields, such as Li (Liion batteries) and Ba (perovskite oxides). It is thus not surprising that these most frequently synthesized chemical systems appear at the top of the list in Fig. 2 (a) and (b).</p>
<p>For heating time prediction, Fig. 2 (c) and (d) show that the IDI of experiment-adjacent features (e.g., indicators of polycrystal synthesis, phosphors, and usage of ball-milling devices) completely outweigh precursor property features. This suggests that heating time is largely controlled by the desired applications (e.g., the need for dense pellets, small particles, single crystals, etc.) and experimental setups rather than reaction mechanisms. Meanwhile, compositional indicator variables still rank second after the experiment-adjacent features, again acting as chemistry-specific corrections.</p>
<p>The blue bars in Fig. 2 are IADI values. IADI values measure the gain of predictive power by a feature over all other features. For heating temperature prediction, Fig. 2 (a) and (b) show that IADI values are very small for most features. A low IADI value is usually due to high correlation among features, e.g., average precursor melting points and maximal precursor melting points. These high correlations suggest it is necessary to use feature selection to choose the strongest feature among highly correlated features, as will be discussed in the next section. Nevertheless, a few features have relatively higher IADI values, a sign that they bring unique extra information over all other features. For example, describing syntheses using the word "sintering" may suggest the experimenters actively chose higher heating temperatures. As a consequence, the experiment-adjacent feature of "sintering" has the highest IADI value for temperature prediction models.</p>
<p>The green bars in Fig. 2 are APDI values. APDI values are the average $R^{2}$ increase</p>
<p>of a feature to all submodels. Thus, APDI estimates the general usefulness of a feature. APDI and IDI values are therefore two important factors in ranking feature importance. For example, in Fig. 2 (a), even though average precursor melting point and $\Delta G_{f}^{300 K}$ both have high IDI values, $\Delta G_{f}^{300 K}$ has smaller APDI values and is less important due to correlation with alternative features. By ranking all features according to the summation of DI values, we are able to consistently select the most uniquely predictive features.</p>
<p>While in general, synthesis temperature and time together determine the overall reaction kinetics, they are not ranked as top predictive features in Fig. 2 when included as features to predict each other (also see Table S1). This seems contrary to the expectation that they would be strongly correlated because elevated temperatures can lead to faster reactions by promoting atomic diffusion. We hypothesize that the low correlation between time and temperature may be due to a variety of reasons: 1) As opposed to sampling many synthesis conditions for a specific chemical system, the TMR dataset spans diverse chemistries. There are usually less than 5 reported syntheses for a majority ( $&gt;60 \%$ ) of the chemical systems which is not enough to reveal a stronger correlation, and 2) The TMR dataset is text-mined from journal articles in which synthesis conditions, especially synthesis time, are generally not optimized but are determined by other external factors, such as the desired applications or the researcher's convenience. These external factors make the time variable more noisy and less correlated to temperature than it might be in a variationally constrained set of data (e.g., the collection of shortest times for each temperature)</p>
<p>To summarize, the overall rankings in Fig. 2 suggest each prediction variable is dominated by two types of features. For heating temperature prediction, precursor material properties have the most feature importance, while compositional features act as secondary corrections. For heating time prediction, experiment-adjacent features dominate the prediction, while compositional features also provide secondary corrections. Contrary to the common application of decomposing synthesis reactions into multi-step phase evolution paths using thermodynamic principles, ${ }^{8,10-12}$ Fig. 2 shows the phase evolution thermodynamic driving</p>
<p>force features, developed using similar principles in this work, provide little predictive power for heating temperature and time. We hypothesize that this is due to the fact that the TMR dataset contains only positive experimental results for which researchers actively optimize for reasonable reaction kinetics. Therefore, reaction driving forces are less useful as these features are more likely to indicate whether something is synthesizable (e.g., if reactions to form a target are thermodynamically spontaneous) rather than indicate at what conditions reactions may occur quickly. We will revisit this finding in more detail in the Discussion section.</p>
<h1>Building and interpreting linear regression models</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Regression result of linear models. The scatter plots show reported conditions v.s. predicted conditions for temperature prediction (a) and time prediction (b). Opacity of the markers indicates the weights of data points. Histograms of prediction errors are also shown.</p>
<p>To build regression models, we start with linear regressors as baseline models since their good interpretability allows one to focus on feature engineering and decipher the relations between features and synthesis conditions. To balance between high predictive power and possible overfitting, we add features in the order of DI rankings and drop any feature that increases model Bayesian information criterion (BIC) values. ${ }^{29}$ In total, four linear models (heating temperature and time prediction models for carbonate and non-carbonate reac-</p>
<p>tions) were trained using weighted least squares (WLS). ${ }^{29}$ The scatter plots of the predicted synthesis conditions versus the reported conditions are shown in Fig. 3 (a) and (b). For heating temperature prediction, the $R^{2}$ values of the models are 0.55 on carbonate reactions and 0.56 on non-carbonate reactions, while the MAE are $134^{\circ} \mathrm{C}$ and $147^{\circ} \mathrm{C}$, respectively. For heating time prediction, the $R^{2}$ values of the models are 0.31 on carbonate reactions and 0.33 on non-carbonate reactions, while the MAE are $0.30 \log <em 10="10">{10}\left(h^{-1}\right)$ and $0.32 \log </em> \cdot t\right]$, or $[0.5 t, 2 t]$ (e.g., for a 2 -hour experiment, the expected prediction range is $0.5-4$ hours). Note that these metrics are evaluated on training data. Thus, they may not reflect the model performance when applied on unseen data. We will perform cross validation and discuss the results in later sections.}\left(h^{-1}\right)$, respectively. Since we predict the transformed time variable $\log _{10}(1 / t)$, such MAE estimates the time prediction is within range $\left[10^{-0.3} \cdot t, 10^{0.3</p>
<p>In a linear regressor $\hat{y}=\sum_{i} \beta_{i} x_{i}$, the feature coefficients $\beta_{i}$ quantify how the regression target variable responds to unit changes of $x_{i}$. As a special case, when $x_{i} \in{0,1}$ are indicator variables (e.g., compositional and experimental-adjacent features), $\beta_{i}$ can be interpreted as additive effects on the prediction target variable when features $x_{i}=1$. For all compositional features, the effects are shown in Fig. 4 (a) and (b). Note that these values are relative to the "average" according to the training dataset and must be interpreted in relative values. For example, if Li is present in the target compound, Fig. 4 (a) suggests the heating temperature will decrease by $360^{\circ} \mathrm{C}$ on average for non-carbonate reactions. On the other hand, the presence of N will increase the heating temperature by $260^{\circ} \mathrm{C}$ on average. Therefore, Fig. 4 (a) and (b) are maps that associate different chemistries with their effect on optimal synthesis conditions. Such maps can be used as empirical "synthesis rules" that are helpful for designing synthesis routes to new materials.</p>
<p>The learned coefficients in Fig. 4 (a) and (b) are sparse because some elements appear only a few times or are even missing in the training dataset, precluding a confident estimate of their effect (assessed by the p-values of the coefficients with a $5 \%$ significance level ${ }^{35}$ ). In Fig. 4 , we observe more consistent compositional effects across similar element periods and groups</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The average effect of each chemical element to predicted heating temperatures (a) and times (b) in trained linear models. The values are coefficients of the corresponding features in the linear models, quantifying how much the predicted value changes relatively if a new chemical element is added to (or removed from) the synthesis.</p>
<p>for temperature predictions than for heating time predictions. The lack of correlation with compositional effects for time prediction matches the DI analysis result in Fig. 2 (c) and (d), which suggests compositional features are less helpful for predicting heating time. Moreover, the compositional effects are less consistent between carbonate reactions and non-carbonate reactions for heating time prediction. These observations suggests the compositional effects are generally less reliable for heating time prediction and must be used with more caution.</p>
<h1>Training and cross-validating non-linear models</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Model performance versus number of training features for both linear and nonlinear (gradient boosting tree regressor) models. The x-axis shows the number of features used. The features are added in the order of DI value rankings. The first row shows performances of temperature prediction models trained on carbonate reactions (a) and noncarbonate reactions (b). The second row shows performances of time prediction models trained on reactions with (c) and without (d) carbonate precursors.</p>
<p>Having used DI analysis and linear models to probe the synthesis prediction features,</p>
<p>we next aim to systematically cross-validate ML models to understand their generalizability or propensity for overfitting. Fig. 5 shows the model performances versus the number of features, which characterize training $R^{2}$ and the LOOCV Pseudo- $R^{2}$ (a metric comparable to $R^{2}$, see Methods ) scores of the linear models as more features are included in training. In Fig. 5, features are added into the models in the order of DI value rankings. Fig. 5 shows that both training and LOOCV scores increase quickly when the number of features is less than 10. This result is consistent with the DI values in Fig. 2 as the first few features have the highest feature importance. The model performance continues to improve as we include all other features, although the marginal improvement decreases rapidly. The training and LOOCV curves for linear models exhibit very similar performance, suggesting that these linear models have little risk of overfitting.</p>
<p>The linear model may be incapable of capturing non-linear correlations among features and synthesis conditions. We next use advanced ML models that are capable of modeling non-linear relations on the same set of features as for the linear models. Among many ML models we attempted during preliminary experiments, gradient boosted regression trees (GBRT), implemented in the XGBoost package, ${ }^{36}$ demonstrated the best LOOCV scores after proper hyperparameter tuning. XGBoost models use a large number of weak tree learners to build a strong ensemble regressor and are able to learn non-linear effects. Indeed, we observe in Fig. 5 that XGBoost training Pseudo- $R^{2}$ (red dashed curves) are significantly higher than linear models. However, as shown by the teal crosses in Fig. 5, compared to the LOOCV scores of linear models (green stars), the LOOCV Pseudo- $R^{2}$ scores of XGBoost models do not improve as much when compared to the LOOCV performance of the linear models, suggesting an increased level of overfitting by XGBoost models. One advantage of XGBoost over linear models is improved utilization of a small number of features, as shown by the steeper curves when the number of features is less than 10 in Fig. 5 (a) and (b), although the advantage diminishes once sufficiently many features are used. Finally, to help better understand the uncertainties of the models, we visualize the error distributions</p>
<p>of synthesis conditions in Fig. 6 using violin plots, where we mark the interquartile range (IQR) representing $50 \%$ of the errors, and 1.5 x IQR, representing the range of prediction errors beyond which are considered outliers.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: LOOCV prediction error distributions of synthesis temperature and time. Plotted are prediction median values (shown with white dots), interquartile ranges (IQR, or the spread of errors between $25 \%$ and $75 \%$ percentiles, shown with thick lines), and 1.5x IQR (shown with thin lines). Shaded areas are probabilistic density estimations of the errors. Our models are expected to make prediction errors within the IQR half of the times and within the 1.5 x IQR most of the times.</p>
<h1>Testing model generalizability using the PCD dataset</h1>
<p>When applied to unseen datasets, ML model predictions tend to have larger errors due to dataset shift, i.e., unseen datasets have a different distribution than the training datasets. ${ }^{37}$ In particular, the relations between features and outcomes may change for unseen data, leading to concept drift, degrading model generalizability and limiting model applicability.</p>
<p>The TMR dataset mostly contains syntheses for inorganic oxide materials and is dominated by target materials containing $\mathrm{Ti}, \mathrm{Sr}, \mathrm{Li}, \mathrm{Ba}, \mathrm{La}, \mathrm{Nb}, \mathrm{Fe}$, etc., reflecting popular materials in the inorganic materials research community such as perovskite oxides and battery materials. The TMR dataset also contains a large fraction of solid solutions or doped materials. To estimate and understand how the ML model trained on the TMR dataset generalizes to unseen datasets, we utilized the PCD dataset as an additional test. The original</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Performance of the models versus the number of features evaluated on the PCD dataset. X-axes show the number of features used in each model. Features are added in the order of DI value rankings as in Fig. 2. The left panels (a) and (c) show models trained on carbonate reactions and the right panels (b) and (d) show models trained on non-carbonate reactions. Top panels (a) and (b) show performance of models trained and evaluated on the PCD dataset, which represent the upper bounds of OOS scores (c) and (d), which show performance of the models trained on the TMR dataset. A higher OOS score indicate better model generalizability.</p>
<p>PCD collection contains inorganic materials syntheses that were manually extracted from the literature in a semi-structured natural language form. 32 We processed the PCD (Pearson's Crystal Data) collection using the same text-mining pipeline and only kept oxide syntheses such that the final PCD dataset has a similar chemistry distribution as the TMR dataset. To ensure there are no duplicate syntheses, we removed any entry in the PCD dataset whose digital object identifier (DOI) is present in the TMR dataset (i.e., syntheses in same papers are not allowed, but the same compositions from different papers are allowed). Compared to the TMR dataset, the PCD dataset shares a similar distribution of chemical systems and synthesis conditions, as indicated by similar sets of popular chemical elements (i.e. Ti, Fe, Sr, Ba, Si, etc.) and average synthesis temperatures around 1200°C, see Fig. S3. The PCD dataset thus represents a reasonable benchmark dataset for our ML models. However, since</p>
<p>many reactions in the PCD dataset do not have heating times extracted, we only predicted heating temperatures for the PCD dataset.</p>
<p>To establish an upper bound of the model performance, we performed the same training/validation procedure using the PCD dataset as was used on the TMR dataset. Fig. 7 shows the performance of the ML models versus the number of features. The green stars and teal crosses in Fig. 7 are the LOOCV scores of linear and XGBoost models, respectively. XGBoost models achieve $0.5 \sim 0.6$ LOOCV Pseudo- $R^{2}$ which is considerably better than linear models $(0.4 \sim 0.5)$. Moreover, XGBoost shows steeper performance increase when few synthesis features are used. Compared to Fig. 5, the advantage of the non-linear models are much more substantial for the PCD dataset than for the TMR dataset. This clear advantage of XGBoost models indicates they are more robust than linear models against possible dataset shift effects.</p>
<p>Next, we performed tests to understand how well ML models trained on the TMR dataset are generalizable to the PCD dataset. The purple diamonds and yellow-brown triangles in Fig. 7 show the OOS performances of the linear and XGBoost models trained using the TMR dataset but evaluated on the PCD dataset. It is interesting to note that XGBoost and linear models have very similar OOS scores for carbonate reactions, but XGBoost clearly outperforms linear models for non-carbonate reactions when more $(&gt;30)$ features are used. Upon further investigation, the features #30 to #40 used on non-carbonate reactions are mostly related to thermodynamic properties of the reactions. The performance drop after features #30 suggests that relations between thermodynamic features and heating temperatures learned on the TMR dataset by linear models do not transfer well to the PCD dataset. On the other hand, XGBoost models seem to be able to consistently maintain good performance regardless of the number of features used.</p>
<p>In Fig. 7, the difference between LOOCV scores and OOS scores confirms the ML models have degraded prediction performance ( $R^{2}$ drops by 0.1 ) when applied to a different dataset. The performance degradation caused by dataset shift is often inevitable and requires reg-</p>
<p>ularly retraining the ML models in order to adapt to the new datasets. However, Fig. 7 suggests XGBoost models are more robust against dataset shift and have a better generalizability. We hypothesize this is due to the strong regularization and therefore recommend ML synthesis condition predictors to be built with XGBoost or similarly regularized models.</p>
<h1>Discussion</h1>
<p>ML predictions must be statistically evaluated using large datasets, so this work has focused heavily on reducing the expected prediction errors and improving the coefficient of determination $R^{2}$. We do not optimize models for any particular reaction but aim at predicting the synthesis conditions over a dataset of several thousand synthesis reactions. As demonstrated by the cross-validation and OOS evaluations in Fig. 5 and Fig. 7, our models achieve $R^{2} \sim 0.5-0.6\left(\mathrm{MAE} \sim 140^{\circ} \mathrm{C}\right)$ for heating temperature predictions and $R^{2} \sim 0.3$ $\left(\mathrm{MAE} \sim 0.3 \log _{10}\left(h^{-1}\right)\right)$ for heating time predictions. When evaluating these $R^{2}$ values, it is important to consider that heating temperature and time do not have a single value for a synthesis reaction, as compounds can often be synthesized over a broad range of time and temperature. As such, our models may be more successful at predicting reaction conditions that successfully created the target, as surmised from the $R^{2}$ scores.</p>
<p>Based on the ranking of DI values in Fig. 2, the deciding factors for the synthesis conditions can be organized into a two-level hierarchy. Synthesis temperature prediction is dominated by precursor properties, which we speculate are proxies for reactivity stemming from the mobility of ions, with additional corrections learned for different chemistries. Synthesis time prediction is dominated by experiment-adjacent features that are linked to experimental setups/intentions, also with corrections according to chemistry. The features used in this work to account for reaction thermodynamics were inspired by recent efforts to understand phase evolution during synthesis. ${ }^{7-9,12,38}$ These features involve decomposing overall synthesis reactions into a sequence of phase evolution reactions between pairs</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The curves are the distribution of heating temperatures for each group of reactions in the training dataset. The dashed/dotted lines show temperature distributions for the reaction $\mathrm{TiO}<em 3="3">{2}+\mathrm{BaCO}</em>} \rightarrow \mathrm{BaTiO<em 2="2">{3}+\mathrm{CO}</em>$ (red dashed line for single-heating reactions and blue dotted line for multiple-heating reactions). Green solid line shows the temperature distribution for the entire dataset.
of compounds and quantifying the grand potential thermodynamic driving force for these phase evolution reactions. This approach has been proved especially useful for understanding phase evolution pathways observed in in-situ experiments. However, in this work, they are shown to provide little predictive power of synthesis conditions and even cause the models to generalize poorly on OOS datasets (as demonstrated in Fig. 7). This discrepancy will be discussed in more detail in the subsequent sections.</p>
<h1>Synthesis adjacent information</h1>
<p>We use the particular synthesis of $\mathrm{BaTiO}<em 3="3">{3}$ from $\mathrm{BaCO}</em>}$ and $\mathrm{TiO<em 3="3">{2}$ precursors to demonstrate how ML models combine synthesis adjacent information with the other regressors. $\mathrm{BaTiO}</em>}$ is a popular compound with many applications in materials science and appears more than 100 times as the synthesis target in the TMR dataset. A variety of synthesis temperatures have been reported for $\mathrm{BaTiO<em 3="3">{3}$ in the literature. For example, $\mathrm{BaTiO}</em>$. Fig. 8 shows the distribution of}$ has been synthesized at $1000^{\circ} \mathrm{C},{ }^{39} 1100^{\circ} \mathrm{C},{ }^{40} 1200^{\circ} \mathrm{C},{ }^{41} 1300^{\circ} \mathrm{C},{ }^{42}$ and $1400^{\circ} \mathrm{C} .{ }^{43}$ Here we focus on the effect of how many heating steps are used in the synthesis of $\mathrm{BaTiO}_{3</p>
<p>heating temperatures for all the reactions, $\mathrm{BaTiO}<em 3="3">{3}$ with a single heating step, and $\mathrm{BaTiO}</em>$ (for example, see ref. 43 ).}$ with multiple heating steps in the training dataset. It is clear that the reported heating temperatures with a single heating step have a lower center around $1100^{\circ} \mathrm{C}$ (for example, see ref. 40), while the entries with multiple heating steps have a higher center around $1300-$ $1400^{\circ} \mathrm{C</p>
<p>As a result, adding the target composition and experiment-adjacent features allows ML models to identify different groups of data as in Fig. 8 and optimize the predicted heating temperature within each group. For example, if 0 means single heating and 1 means multiple heating, then the ML model should have a coefficient for the feature of "is multiple heating" of about $250^{\circ} \mathrm{C}$, roughly equal to the difference between the centers of the two temperatures distributions in Fig. 8.</p>
<h1>Connection to Tamman's rule</h1>
<p>Our finding that the average precursor melting points are the most predictive feature for heating temperatures is reminiscent of Tamman's rule. ${ }^{44,45}$ Tamman's rule can be formulated as predicting that the synthesis temperature of metal alloys should be more than $\frac{1}{3}$ (for example, $\frac{1}{2}-\frac{2}{3}$ ) of precursor melting points. This rule is derived from the observation that atomic diffusion quickly ceases below $\frac{1}{3}$ of melting temperatures. ${ }^{46}$ Tamman's empirical rule was never formally defined. It is also questionable whether the rule is applicable to the synthesis of ionic compounds in addition to intermetallics. Nevertheless, variants of Tamman's rule are still used to help determine solid-state synthesis conditions. For example, Becker and Dronskowski used $\frac{2}{3}$ of the most "volatile" compound ${ }^{47}$; other values, such as $\frac{1}{2}$, have also been used. ${ }^{45}$</p>
<p>Our ML framework allows us to formally model and test Tamman's rule within a statistical approach. We start with Tamman's original formulation and fit a linear model without an intercept term:</p>
<p>$$
T_{\text {Tamman }}=\alpha\left(\min T_{\text {melt }}\right)+\varepsilon
$$</p>
<p>where $T_{\text {Tamman }}$ is the predicted heating temperature, $\left(\min T_{\text {melt }}\right)$ is the minimum of precursor melting points, $\alpha$ is a parameter to be learned, and $\varepsilon$ is an error term. Both the prediction and the melting points are presented in degrees Kelvin. The fit linear model finds $\alpha=1.2$ when trained on carbonate reactions and $\alpha=0.8$ when trained on non-carbonate reactions. These $\alpha$ values are larger than the commonly used values for Tamman's rule, such as $1 / 2$ and $2 / 3$, suggesting the required temperatures for atoms to diffuse significantly in ionic compounds are higher than in intermetallics, or that for ionic compounds Tamman's rule is a surrogate for another property than diffusion.</p>
<p>The above linear model is not the model with highest predictive power ( $R^{2}$ values). As shown in Fig. 2, using average precursors melting points (instead of minimum precursor melting points) yields the highest prediction performance. Therefore, we update Tamman's rule to give the optimal synthesis temperature $T_{\text {Tamman }}$ as proportional to the average of precursor melting points $\left(\operatorname{avg} T_{\text {melt }}\right)$ plus a constant. Mathematically, the predictor is defined as:</p>
<p>$$
T_{\text {Tamman }}=\alpha\left(\operatorname{avg} T_{\text {melt }}\right)+\beta+\varepsilon
$$</p>
<p>where $\alpha, \beta$ are parameters to be learned and $\varepsilon$ is an error term.
As demonstrated in Fig. 9, fitting a linear model reveals a slope of $\sim 1 / 3$. Since we used the average of precursor melting points, the predicted heating temperatures should be generally larger than $\frac{1}{3}$ of the minimal precursor melting point, agreeing with Tamman's original observation. ${ }^{44}$ The predicted versus reported heating temperatures and the histogram of prediction errors are shown in Fig. 9 (a). The parameters of the fitted linear model are shown in Fig. 9 (b). The large F-statistic values and very small p-values show strong statistical significance of the model although this is contrasted by the low coefficient of determination $\left(R^{2} \sim 0.2-0.3\right)$. Tamman's rule is not a perfect predictor and has larger prediction errors at low temperatures. However, it contributes more than $\frac{1}{3}$ of the maximal predictive power developed in this work.</p>            </div>
        </div>

    </div>
</body>
</html>
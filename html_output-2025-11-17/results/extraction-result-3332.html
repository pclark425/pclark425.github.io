<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3332 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3332</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3332</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-ea8c46e193d5121e440daf96edfd15a47151c293</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ea8c46e193d5121e440daf96edfd15a47151c293" target="_blank">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a></p>
                <p><strong>Paper Venue:</strong> Conference of the European Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Interestingly, it is observed that the performance of this method significantly improves when increasing the number of retrieved passages, evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.</p>
                <p><strong>Paper Abstract:</strong> Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3332.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3332.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fusion-in-Decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented sequence-to-sequence architecture that encodes each retrieved passage independently and performs evidence fusion in the decoder by attending to concatenated encoder outputs, enabling scaling to large numbers of passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fusion-in-Decoder (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence-to-sequence (encoder-decoder) generative model pipeline: retrieve up to 100 passages (BM25 or DPR), encode each passage+question independently (per-passage encoder runs), then the decoder attends over concatenated encoder outputs to generate the answer. Implemented by fine-tuning pretrained T5 checkpoints (base and large).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented generation', 'evidence aggregation / fusion in decoder', 'generative decoding (seq2seq answer generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Retrieval-augmented generation: external passages are retrieved (BM25 or DPR) and provided as input to a generative seq2seq model. Evidence aggregation: each passage is encoded independently but the decoder attends to the concatenation of all encoder outputs, allowing the decoder to combine evidence across many passages when producing the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single coherent method (retrieval-augmented generative) applied across experiments; contrasts are drawn with extractive (span-prediction) models and closed-book generative models, but FiD itself uses a single style of reasoning (seq2seq evidence fusion) rather than multiple distinct prompting or chain-of-thought styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain Question Answering (NaturalQuestions, TriviaQA, SQuAD Open)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Answer general-domain questions where supporting evidence is not provided as input; tasks evaluated on NaturalQuestions (open-domain), TriviaQA (open-domain, unfiltered) and SQuAD v1.1 (open-domain variant). Evaluation uses Exact Match (EM) and F1 where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported Exact Match (EM) / F1 from Table 1 (table columns: NaturalQuestions EM, TriviaQA EM (open), TriviaQA EM (hidden), SQuAD Open EM, SQuAD F1): FiD (base) — NQ: 48.2 EM; TriviaQA open: 65.0 EM; TriviaQA hidden: 77.1 EM; SQuAD Open: 53.4 EM; SQuAD F1: 60.6. FiD (large) — NQ: 51.4 EM; TriviaQA open: 67.6 EM; TriviaQA hidden: 80.1 EM; SQuAD Open: 56.7 EM; SQuAD F1: 63.2. Additionally, paper reports that increasing retrieved passages from 10→100 yielded ~+6% EM on TriviaQA and ~+3.5% EM on NaturalQuestions for FiD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>FiD (generative evidence fusion) outperforms prior extractive and generative baselines on NQ and TriviaQA as reported in Table 1. The paper shows FiD's performance continues to improve when scaling retrieved passages up to 100, whereas extractive models were observed (citing prior work) to peak around 10–20 passages. The paper also contrasts FiD to closed-book T5 (Roberts et al., 2020): closed-book T5 (11B) achieves 36.6% on NQ while the retrieval-augmented generative approach achieves substantially higher accuracy with far fewer parameters plus Wikipedia retrieval, demonstrating retrieval+generation is more parameter-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative seq2seq models that are retrieval-augmented and that perform evidence fusion in the decoder (FiD) combine information from many passages better than extractive models, and their accuracy improves monotonically as the number of retrieved passages increases (up to 100). Retrieval augmentation yields large gains over closed-book generative models for open-domain QA, allowing smaller models to match or exceed larger closed-book models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Training with fewer passages reduces performance when testing with many passages; models trained with 5 or 10 passages and tested with 100 show lower EM (Table 2), though finetuning with 100 passages for a short number of steps reduces the gap. The paper also notes that many existing extractive systems peak at ~10–20 passages, implying cases where extractive approaches do not benefit from more contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3332.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3332.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Text-to-Text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained text-to-text Transformer (encoder-decoder) model used as the base generative backbone for FiD experiments; T5 checkpoints (base and large) are fine-tuned for retrieval-augmented QA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (pretrained checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained sequence-to-sequence Transformer model used as initialization for FiD; experiments use HuggingFace T5 checkpoints and fine-tune on QA tasks. Encoders process question+passage context; decoder generates answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>220M (base) and 770M (large) as used in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['closed-book generation (as referenced)', 'retrieval-augmented generation when used within FiD']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Closed-book generation: model must store knowledge in parameters and answer without external retrieval (referenced via prior work). Retrieval-augmented generation: the same T5 architecture is fine-tuned to consume retrieved passages and generate answers, with encoder-per-passage and decoder fusion (FiD).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>used in two modes across the literature: closed-book (no retrieval) and retrieval-augmented (with external passages). In this paper T5 is used primarily in the retrieval-augmented FiD configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain Question Answering (NaturalQuestions, TriviaQA, SQuAD Open)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same QA tasks as FiD experiments. The paper also references closed-book T5 results from prior work on NaturalQuestions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>T5 (closed-book) reported from Roberts et al. (2020): 36.6% accuracy on NaturalQuestions (closed-book T5 with 11B parameters, cited in text). T5 (as backbone in FiD) yields FiD base/large numbers (see FiD entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper contrasts closed-book T5 (large 11B reported in prior work) with T5 used in FiD: retrieval augmentation with external Wikipedia allows smaller T5 checkpoints (770M) to outperform or match much larger closed-book models, indicating retrieval-augmented generative reasoning is more parameter-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When combined with retrieval, T5-based generative models (FiD) gain large improvements and can surpass closed-book T5 with far fewer parameters by using explicit text memories (Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Closed-book T5 requires much larger parameter counts to achieve competitive open-domain QA performance; no positive closed-book scaling result in this paper that beats retrieval-augmented FiD at comparable memory usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3332.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3332.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (few-shot in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive language model cited as a few-shot baseline for open-domain QA; included in the comparative table but not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited as a few-shot in-context learning language model baseline (Brown et al., 2020); paper only reports published results from that work in the comparison table, not experiments run by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot in-context learning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Few-shot in-context learning: the model is prompted with a small number of input-output exemplars (no gradient updates) and then asked to generate answers for new questions. This is contrasted with finetuned generative models and retrieval-augmented methods in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>mentioned as a different style (few-shot prompting) relative to the paper's finetuned retrieval-augmented seq2seq approach; not experimented with further in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain Question Answering (reported comparators in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Reported in table (comparative baseline): GPT-3 few-shot numbers are given as external results for open-domain QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported in Table 1 (as given by the referenced work): NaturalQuestions: 29.9 EM (table entry); other columns show '-' or 71.2 for another column in the table—paper includes GPT-3 entries for comparison only and does not run these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper uses GPT-3 results only in a comparative table to show FiD and other retrieval-augmented/finetuned methods outperform few-shot GPT-3 on the reported open-domain QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Few-shot prompting with large autoregressive LMs like GPT-3 is a competitive baseline but is outperformed by retrieval-augmented generative methods (FiD) on the reported open-domain QA benchmarks per the comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No in-paper experiments; only a reported baseline. No internal negative results are reported because GPT-3 was not experimentally evaluated by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3332.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3332.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior retrieval-augmented generative approach that conditions a generative model on retrieved passages; cited and compared to FiD, but FiD differs in how retrieved passages are processed (encoder/decoder fusion design).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieval-augmented generative model that integrates retrieval into a seq2seq generation pipeline; in prior work the passage processing differs from FiD (paper notes FiD differs by independent encoding and decoder-level fusion enabling scaling to many contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>RAG retrieves passages and conditions a generative model on them to generate answers; FiD is contrasted as differing in how contexts are encoded and fused (FiD encodes passages independently and fuses in the decoder to scale to many passages).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>RAG represents a retrieval-augmented generative family; the paper treats it as a related single-style approach and contrasts FiD's architectural differences rather than treating it as a distinct reasoning style ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>RAG is a retrieval-augmented approach evaluated on knowledge-intensive tasks; here it is used as a comparative baseline in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported in Table 1 (from Lewis et al., 2020b): NaturalQuestions: 44.5 EM; TriviaQA: 56.1 EM; TriviaQA hidden/test (?) column: 68.0 (table shows these numbers as reported by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>FiD outperforms RAG in this paper's table, and the authors attribute part of FiD's gains to the ability to scale to more retrieved passages by encoding passages independently and fusing in the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Architectural differences in how retrieved contexts are processed (independent encoding + decoder fusion in FiD) can yield empirical gains and better scaling with more passages compared to other retrieval-augmented generative approaches like RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No ablation showing RAG outperforming FiD; only comparison in reported metrics where FiD is shown better on the chosen benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3332.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3332.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dense Passage Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense vector retrieval method that encodes queries and passages with learned BERT encoders and retrieves passages via approximate nearest neighbor search (used as the primary retriever for NQ and TriviaQA in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dense passage retrieval for open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DPR (dense retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dense retrieval approach using two BERT networks to produce dense query and passage embeddings; ranking by dot product and retrieval via FAISS approximate nearest neighbor search. Used to retrieve passages for FiD on NQ and TriviaQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['dense retrieval (learned dense embeddings)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DPR produces dense vector representations for passages and queries using BERT-based encoders; retrieval is a nearest-neighbor lookup that supplies candidate evidence passages to the generative model (FiD).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>A retrieval component (dense) rather than a reasoning style; DPR is used in conjunction with the generative FiD model. The paper contrasts DPR with BM25 (sparse) retrieval choices.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Passage retrieval for Open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Retrieve supporting passages from Wikipedia to provide evidence to the generative reader (FiD); DPR is used for NQ and TriviaQA as per prior recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>DPR is evaluated implicitly as part of the pipeline: paper follows Karpukhin et al. (2020) and uses DPR for NQ and TriviaQA retrieval; direct retrieval-only metrics are not reported, but downstream FiD results (using DPR for retrieval on these tasks) are reported (see FiD entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper compares BM25 vs DPR for retrieval choice and uses DPR for NQ and TriviaQA (BM25 for SQuAD). The paper does not present a detailed DPR vs BM25 ablation beyond stating which retriever was used for which dataset following prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Choice of retrieval method matters; DPR (dense retrieval) is used for the large open-domain benchmarks (NQ, TriviaQA) and enables FiD to access relevant passages that the decoder fuses to improve answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No explicit numeric ablation comparing DPR vs BM25 retrieval effectiveness in this paper beyond the dataset-specific choices reported (they follow Karpukhin et al., 2020 recommendations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3332.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3332.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 (Okapi/BM25 sparse retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical sparse retrieval (bag-of-words ranking) method based on term frequency and inverse document frequency; used to retrieve passages for SQuAD experiments and as a baseline retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Okapi at TREC-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BM25</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sparse lexical retrieval approach implemented via Apache Lucene in the paper; passages and queries are tokenized and ranked by BM25 scoring. Used for SQuAD retrieval in FiD experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['sparse lexical retrieval']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>BM25 represents passages and queries as bag-of-words and ranks passages by a function of term frequency and inverse document frequency to provide evidence passages to the reader.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>BM25 is a single retrieval algorithm contrasted with DPR; not a reasoning style per se but a component influencing evidence supplied to the reasoning model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Passage retrieval for Open-domain QA (SQuAD Open)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to retrieve candidate passages from Wikipedia for FiD's SQuAD experiments; the retrieved passages are then encoded and fused by the generative reader.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>BM25 itself is not reported with standalone downstream metrics in the paper; FiD using BM25 for SQuAD achieves the reported FiD scores (see FiD entry). The paper also notes BM25 implementation details (Apache Lucene, SpaCy tokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>BM25 is used for SQuAD (where it performed well in prior work) while DPR is used for NQ and TriviaQA; the paper follows prior results in choosing the retrieval method per dataset but does not present an extensive BM25 vs DPR quantitative comparison in the main results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sparse retrieval (BM25) remains a valid retrieval approach for some datasets and is used in the FiD pipeline where appropriate; retrieval choice was dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No direct negative results reported in the paper showing BM25 beating DPR across the board; choice is dataset dependent and authors follow prior recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3332.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3332.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Passage BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Passage BERT (globally normalized BERT for multi-paragraph QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extractive, span-prediction model that aggregates evidence across multiple passages using global normalization; cited as a representative extractive baseline that FiD outperforms on some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-passage BERT: A globally normalized BERT model for open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-Passage BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An extractive QA model based on BERT that processes multiple passages and applies a global normalization across candidate spans to improve multi-paragraph extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['extractive span prediction (BERT-based)', 'global normalization for multi-passage aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Extractive span prediction: predict answer spans inside retrieved passages using a BERT-based reader. Global normalization aggregates likelihoods across passages to select the best span across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Represents an extractive style distinct from generative seq2seq reasoning; paper contrasts this with generative FiD, arguing extractive models have difficulty aggregating evidence across many passages and often plateau at fewer passages.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Extractive QA on open-domain benchmarks where the reader must select an answer span from retrieved passages; metrics evaluated using Exact Match (EM) and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported in Table 1 (as a baseline): Multi-Passage BERT — SQuAD Open: 53.0 EM and 60.9 F1 (table entry). Other dataset columns for this model are '-' in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Authors report FiD outperforms extractive models on NaturalQuestions and TriviaQA, and argue extractive models empirically peak around 10–20 retrieved passages while FiD benefits from many more passages.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Extractive models can be effective but are less able to scale in benefit with the number of retrieved passages compared to generative FiD; FiD achieves higher EM when aggregating evidence from many passages.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Extractive models may outperform or be competitive when only a small number of highly relevant passages are available; paper cites prior work showing extractive performance peaks around 10–20 passages, implying diminishing returns with more contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>How much knowledge can you pack into the parameters of a language model? <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>REALM: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3332",
    "paper_id": "paper-ea8c46e193d5121e440daf96edfd15a47151c293",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "FiD",
            "name_full": "Fusion-in-Decoder",
            "brief_description": "A retrieval-augmented sequence-to-sequence architecture that encodes each retrieved passage independently and performs evidence fusion in the decoder by attending to concatenated encoder outputs, enabling scaling to large numbers of passages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fusion-in-Decoder (FiD)",
            "model_description": "Sequence-to-sequence (encoder-decoder) generative model pipeline: retrieve up to 100 passages (BM25 or DPR), encode each passage+question independently (per-passage encoder runs), then the decoder attends over concatenated encoder outputs to generate the answer. Implemented by fine-tuning pretrained T5 checkpoints (base and large).",
            "model_size": null,
            "reasoning_methods": [
                "retrieval-augmented generation",
                "evidence aggregation / fusion in decoder",
                "generative decoding (seq2seq answer generation)"
            ],
            "reasoning_methods_description": "Retrieval-augmented generation: external passages are retrieved (BM25 or DPR) and provided as input to a generative seq2seq model. Evidence aggregation: each passage is encoded independently but the decoder attends to the concatenation of all encoder outputs, allowing the decoder to combine evidence across many passages when producing the answer.",
            "diversity_of_methods": "single coherent method (retrieval-augmented generative) applied across experiments; contrasts are drawn with extractive (span-prediction) models and closed-book generative models, but FiD itself uses a single style of reasoning (seq2seq evidence fusion) rather than multiple distinct prompting or chain-of-thought styles.",
            "reasoning_task_name": "Open-domain Question Answering (NaturalQuestions, TriviaQA, SQuAD Open)",
            "reasoning_task_description": "Answer general-domain questions where supporting evidence is not provided as input; tasks evaluated on NaturalQuestions (open-domain), TriviaQA (open-domain, unfiltered) and SQuAD v1.1 (open-domain variant). Evaluation uses Exact Match (EM) and F1 where applicable.",
            "performance_by_method": "Reported Exact Match (EM) / F1 from Table 1 (table columns: NaturalQuestions EM, TriviaQA EM (open), TriviaQA EM (hidden), SQuAD Open EM, SQuAD F1): FiD (base) — NQ: 48.2 EM; TriviaQA open: 65.0 EM; TriviaQA hidden: 77.1 EM; SQuAD Open: 53.4 EM; SQuAD F1: 60.6. FiD (large) — NQ: 51.4 EM; TriviaQA open: 67.6 EM; TriviaQA hidden: 80.1 EM; SQuAD Open: 56.7 EM; SQuAD F1: 63.2. Additionally, paper reports that increasing retrieved passages from 10→100 yielded ~+6% EM on TriviaQA and ~+3.5% EM on NaturalQuestions for FiD.",
            "comparison_of_methods": "FiD (generative evidence fusion) outperforms prior extractive and generative baselines on NQ and TriviaQA as reported in Table 1. The paper shows FiD's performance continues to improve when scaling retrieved passages up to 100, whereas extractive models were observed (citing prior work) to peak around 10–20 passages. The paper also contrasts FiD to closed-book T5 (Roberts et al., 2020): closed-book T5 (11B) achieves 36.6% on NQ while the retrieval-augmented generative approach achieves substantially higher accuracy with far fewer parameters plus Wikipedia retrieval, demonstrating retrieval+generation is more parameter-efficient.",
            "key_findings": "Generative seq2seq models that are retrieval-augmented and that perform evidence fusion in the decoder (FiD) combine information from many passages better than extractive models, and their accuracy improves monotonically as the number of retrieved passages increases (up to 100). Retrieval augmentation yields large gains over closed-book generative models for open-domain QA, allowing smaller models to match or exceed larger closed-book models.",
            "counter_examples_or_negative_results": "Training with fewer passages reduces performance when testing with many passages; models trained with 5 or 10 passages and tested with 100 show lower EM (Table 2), though finetuning with 100 passages for a short number of steps reduces the gap. The paper also notes that many existing extractive systems peak at ~10–20 passages, implying cases where extractive approaches do not benefit from more contexts.",
            "uuid": "e3332.0",
            "source_info": {
                "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "T5",
            "name_full": "T5 (Text-to-Text Transfer Transformer)",
            "brief_description": "A pretrained text-to-text Transformer (encoder-decoder) model used as the base generative backbone for FiD experiments; T5 checkpoints (base and large) are fine-tuned for retrieval-augmented QA in this paper.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5 (pretrained checkpoint)",
            "model_description": "Pretrained sequence-to-sequence Transformer model used as initialization for FiD; experiments use HuggingFace T5 checkpoints and fine-tune on QA tasks. Encoders process question+passage context; decoder generates answers.",
            "model_size": "220M (base) and 770M (large) as used in this paper",
            "reasoning_methods": [
                "closed-book generation (as referenced)",
                "retrieval-augmented generation when used within FiD"
            ],
            "reasoning_methods_description": "Closed-book generation: model must store knowledge in parameters and answer without external retrieval (referenced via prior work). Retrieval-augmented generation: the same T5 architecture is fine-tuned to consume retrieved passages and generate answers, with encoder-per-passage and decoder fusion (FiD).",
            "diversity_of_methods": "used in two modes across the literature: closed-book (no retrieval) and retrieval-augmented (with external passages). In this paper T5 is used primarily in the retrieval-augmented FiD configuration.",
            "reasoning_task_name": "Open-domain Question Answering (NaturalQuestions, TriviaQA, SQuAD Open)",
            "reasoning_task_description": "Same QA tasks as FiD experiments. The paper also references closed-book T5 results from prior work on NaturalQuestions.",
            "performance_by_method": "T5 (closed-book) reported from Roberts et al. (2020): 36.6% accuracy on NaturalQuestions (closed-book T5 with 11B parameters, cited in text). T5 (as backbone in FiD) yields FiD base/large numbers (see FiD entry).",
            "comparison_of_methods": "Paper contrasts closed-book T5 (large 11B reported in prior work) with T5 used in FiD: retrieval augmentation with external Wikipedia allows smaller T5 checkpoints (770M) to outperform or match much larger closed-book models, indicating retrieval-augmented generative reasoning is more parameter-efficient.",
            "key_findings": "When combined with retrieval, T5-based generative models (FiD) gain large improvements and can surpass closed-book T5 with far fewer parameters by using explicit text memories (Wikipedia).",
            "counter_examples_or_negative_results": "Closed-book T5 requires much larger parameter counts to achieve competitive open-domain QA performance; no positive closed-book scaling result in this paper that beats retrieval-augmented FiD at comparable memory usage.",
            "uuid": "e3332.1",
            "source_info": {
                "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "GPT-3 (few-shot)",
            "name_full": "GPT-3 (few-shot in-context learning)",
            "brief_description": "A large autoregressive language model cited as a few-shot baseline for open-domain QA; included in the comparative table but not used in experiments in this paper.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (few-shot)",
            "model_description": "Cited as a few-shot in-context learning language model baseline (Brown et al., 2020); paper only reports published results from that work in the comparison table, not experiments run by the authors.",
            "model_size": null,
            "reasoning_methods": [
                "few-shot in-context learning"
            ],
            "reasoning_methods_description": "Few-shot in-context learning: the model is prompted with a small number of input-output exemplars (no gradient updates) and then asked to generate answers for new questions. This is contrasted with finetuned generative models and retrieval-augmented methods in the paper's comparisons.",
            "diversity_of_methods": "mentioned as a different style (few-shot prompting) relative to the paper's finetuned retrieval-augmented seq2seq approach; not experimented with further in this paper.",
            "reasoning_task_name": "Open-domain Question Answering (reported comparators in Table 1)",
            "reasoning_task_description": "Reported in table (comparative baseline): GPT-3 few-shot numbers are given as external results for open-domain QA benchmarks.",
            "performance_by_method": "Reported in Table 1 (as given by the referenced work): NaturalQuestions: 29.9 EM (table entry); other columns show '-' or 71.2 for another column in the table—paper includes GPT-3 entries for comparison only and does not run these experiments.",
            "comparison_of_methods": "Paper uses GPT-3 results only in a comparative table to show FiD and other retrieval-augmented/finetuned methods outperform few-shot GPT-3 on the reported open-domain QA benchmarks.",
            "key_findings": "Few-shot prompting with large autoregressive LMs like GPT-3 is a competitive baseline but is outperformed by retrieval-augmented generative methods (FiD) on the reported open-domain QA benchmarks per the comparison table.",
            "counter_examples_or_negative_results": "No in-paper experiments; only a reported baseline. No internal negative results are reported because GPT-3 was not experimentally evaluated by the authors.",
            "uuid": "e3332.2",
            "source_info": {
                "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A prior retrieval-augmented generative approach that conditions a generative model on retrieved passages; cited and compared to FiD, but FiD differs in how retrieved passages are processed (encoder/decoder fusion design).",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "mention",
            "model_name": "RAG",
            "model_description": "Retrieval-augmented generative model that integrates retrieval into a seq2seq generation pipeline; in prior work the passage processing differs from FiD (paper notes FiD differs by independent encoding and decoder-level fusion enabling scaling to many contexts).",
            "model_size": null,
            "reasoning_methods": [
                "retrieval-augmented generation"
            ],
            "reasoning_methods_description": "RAG retrieves passages and conditions a generative model on them to generate answers; FiD is contrasted as differing in how contexts are encoded and fused (FiD encodes passages independently and fuses in the decoder to scale to many passages).",
            "diversity_of_methods": "RAG represents a retrieval-augmented generative family; the paper treats it as a related single-style approach and contrasts FiD's architectural differences rather than treating it as a distinct reasoning style ensemble.",
            "reasoning_task_name": "Open-domain Question Answering",
            "reasoning_task_description": "RAG is a retrieval-augmented approach evaluated on knowledge-intensive tasks; here it is used as a comparative baseline in the table.",
            "performance_by_method": "Reported in Table 1 (from Lewis et al., 2020b): NaturalQuestions: 44.5 EM; TriviaQA: 56.1 EM; TriviaQA hidden/test (?) column: 68.0 (table shows these numbers as reported by the authors).",
            "comparison_of_methods": "FiD outperforms RAG in this paper's table, and the authors attribute part of FiD's gains to the ability to scale to more retrieved passages by encoding passages independently and fusing in the decoder.",
            "key_findings": "Architectural differences in how retrieved contexts are processed (independent encoding + decoder fusion in FiD) can yield empirical gains and better scaling with more passages compared to other retrieval-augmented generative approaches like RAG.",
            "counter_examples_or_negative_results": "No ablation showing RAG outperforming FiD; only comparison in reported metrics where FiD is shown better on the chosen benchmarks.",
            "uuid": "e3332.3",
            "source_info": {
                "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "DPR",
            "name_full": "Dense Passage Retrieval",
            "brief_description": "A dense vector retrieval method that encodes queries and passages with learned BERT encoders and retrieves passages via approximate nearest neighbor search (used as the primary retriever for NQ and TriviaQA in the paper).",
            "citation_title": "Dense passage retrieval for open-domain question answering",
            "mention_or_use": "use",
            "model_name": "DPR (dense retrieval)",
            "model_description": "Dense retrieval approach using two BERT networks to produce dense query and passage embeddings; ranking by dot product and retrieval via FAISS approximate nearest neighbor search. Used to retrieve passages for FiD on NQ and TriviaQA.",
            "model_size": null,
            "reasoning_methods": [
                "dense retrieval (learned dense embeddings)"
            ],
            "reasoning_methods_description": "DPR produces dense vector representations for passages and queries using BERT-based encoders; retrieval is a nearest-neighbor lookup that supplies candidate evidence passages to the generative model (FiD).",
            "diversity_of_methods": "A retrieval component (dense) rather than a reasoning style; DPR is used in conjunction with the generative FiD model. The paper contrasts DPR with BM25 (sparse) retrieval choices.",
            "reasoning_task_name": "Passage retrieval for Open-domain QA",
            "reasoning_task_description": "Retrieve supporting passages from Wikipedia to provide evidence to the generative reader (FiD); DPR is used for NQ and TriviaQA as per prior recommendations.",
            "performance_by_method": "DPR is evaluated implicitly as part of the pipeline: paper follows Karpukhin et al. (2020) and uses DPR for NQ and TriviaQA retrieval; direct retrieval-only metrics are not reported, but downstream FiD results (using DPR for retrieval on these tasks) are reported (see FiD entry).",
            "comparison_of_methods": "Paper compares BM25 vs DPR for retrieval choice and uses DPR for NQ and TriviaQA (BM25 for SQuAD). The paper does not present a detailed DPR vs BM25 ablation beyond stating which retriever was used for which dataset following prior work.",
            "key_findings": "Choice of retrieval method matters; DPR (dense retrieval) is used for the large open-domain benchmarks (NQ, TriviaQA) and enables FiD to access relevant passages that the decoder fuses to improve answer generation.",
            "counter_examples_or_negative_results": "No explicit numeric ablation comparing DPR vs BM25 retrieval effectiveness in this paper beyond the dataset-specific choices reported (they follow Karpukhin et al., 2020 recommendations).",
            "uuid": "e3332.4",
            "source_info": {
                "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "BM25",
            "name_full": "BM25 (Okapi/BM25 sparse retrieval)",
            "brief_description": "A classical sparse retrieval (bag-of-words ranking) method based on term frequency and inverse document frequency; used to retrieve passages for SQuAD experiments and as a baseline retriever.",
            "citation_title": "Okapi at TREC-3",
            "mention_or_use": "use",
            "model_name": "BM25",
            "model_description": "Sparse lexical retrieval approach implemented via Apache Lucene in the paper; passages and queries are tokenized and ranked by BM25 scoring. Used for SQuAD retrieval in FiD experiments.",
            "model_size": null,
            "reasoning_methods": [
                "sparse lexical retrieval"
            ],
            "reasoning_methods_description": "BM25 represents passages and queries as bag-of-words and ranks passages by a function of term frequency and inverse document frequency to provide evidence passages to the reader.",
            "diversity_of_methods": "BM25 is a single retrieval algorithm contrasted with DPR; not a reasoning style per se but a component influencing evidence supplied to the reasoning model.",
            "reasoning_task_name": "Passage retrieval for Open-domain QA (SQuAD Open)",
            "reasoning_task_description": "Used to retrieve candidate passages from Wikipedia for FiD's SQuAD experiments; the retrieved passages are then encoded and fused by the generative reader.",
            "performance_by_method": "BM25 itself is not reported with standalone downstream metrics in the paper; FiD using BM25 for SQuAD achieves the reported FiD scores (see FiD entry). The paper also notes BM25 implementation details (Apache Lucene, SpaCy tokenization).",
            "comparison_of_methods": "BM25 is used for SQuAD (where it performed well in prior work) while DPR is used for NQ and TriviaQA; the paper follows prior results in choosing the retrieval method per dataset but does not present an extensive BM25 vs DPR quantitative comparison in the main results.",
            "key_findings": "Sparse retrieval (BM25) remains a valid retrieval approach for some datasets and is used in the FiD pipeline where appropriate; retrieval choice was dataset-dependent.",
            "counter_examples_or_negative_results": "No direct negative results reported in the paper showing BM25 beating DPR across the board; choice is dataset dependent and authors follow prior recommendations.",
            "uuid": "e3332.5",
            "source_info": {
                "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Multi-Passage BERT",
            "name_full": "Multi-Passage BERT (globally normalized BERT for multi-paragraph QA)",
            "brief_description": "An extractive, span-prediction model that aggregates evidence across multiple passages using global normalization; cited as a representative extractive baseline that FiD outperforms on some benchmarks.",
            "citation_title": "Multi-passage BERT: A globally normalized BERT model for open-domain question answering",
            "mention_or_use": "mention",
            "model_name": "Multi-Passage BERT",
            "model_description": "An extractive QA model based on BERT that processes multiple passages and applies a global normalization across candidate spans to improve multi-paragraph extraction.",
            "model_size": null,
            "reasoning_methods": [
                "extractive span prediction (BERT-based)",
                "global normalization for multi-passage aggregation"
            ],
            "reasoning_methods_description": "Extractive span prediction: predict answer spans inside retrieved passages using a BERT-based reader. Global normalization aggregates likelihoods across passages to select the best span across contexts.",
            "diversity_of_methods": "Represents an extractive style distinct from generative seq2seq reasoning; paper contrasts this with generative FiD, arguing extractive models have difficulty aggregating evidence across many passages and often plateau at fewer passages.",
            "reasoning_task_name": "Open-domain Question Answering",
            "reasoning_task_description": "Extractive QA on open-domain benchmarks where the reader must select an answer span from retrieved passages; metrics evaluated using Exact Match (EM) and F1.",
            "performance_by_method": "Reported in Table 1 (as a baseline): Multi-Passage BERT — SQuAD Open: 53.0 EM and 60.9 F1 (table entry). Other dataset columns for this model are '-' in the table.",
            "comparison_of_methods": "Authors report FiD outperforms extractive models on NaturalQuestions and TriviaQA, and argue extractive models empirically peak around 10–20 retrieved passages while FiD benefits from many more passages.",
            "key_findings": "Extractive models can be effective but are less able to scale in benefit with the number of retrieved passages compared to generative FiD; FiD achieves higher EM when aggregating evidence from many passages.",
            "counter_examples_or_negative_results": "Extractive models may outperform or be competitive when only a small number of highly relevant passages are available; paper cites prior work showing extractive performance peaks around 10–20 passages, implying diminishing returns with more contexts.",
            "uuid": "e3332.6",
            "source_info": {
                "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "How much knowledge can you pack into the parameters of a language model?",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "REALM: Retrieval-augmented language model pre-training",
            "rating": 2
        }
    ],
    "cost": 0.016084,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</h1>
<p>Gautier Izacard ${ }^{1,2,3}$ Edouard Grave ${ }^{1}$<br>${ }^{1}$ Facebook AI Research, Paris<br>${ }^{2}$ ENS, PSL University, Paris<br>${ }^{3}$ Inria, Paris<br>gizacard|egrave@fb.com</p>
<h4>Abstract</h4>
<p>Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-theart results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.</p>
<h2>1 Introduction</h2>
<p>Recently, several works have shown that factual information can be extracted from large scale language models trained on vast quantities of data (Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019). Building on that observation and the advances in pretraining of natural language processing models, Roberts et al. (2020) introduced a generative model for open domain question answering. Without relying on external knowledge, this method obtained competitive results on several benchmarks. However, it requires models containing billions of parameters, since all the information needs to be stored in the weights. This makes models expensive to query and train. In this paper, we investigate how much this method could benefit from having access to an external source of knowledge, such as Wikipedia.</p>
<p>Retrieval based approaches were previously considered in the context of open domain question answering with extractive models (Chen et al., 2017). In that case, systems start by retrieving
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A simple approach to open domain question answering. First, it retrieves support text passages from an external source of knowledge such as Wikipedia. Then, a generative encoder-decoder model produces the answer, conditioned on the question and the retrieved passages. This approach scales well with the number of retrieved passages, as the performance keeps improving when retrieving up to one hundred passages.
support documents, before extracting the answer from these documents. Different retrieval techniques have been considered, either using sparse representations based on TF/IDF or using dense embeddings (Guu et al., 2020; Karpukhin et al., 2020). The models which extract the answers are often based on contextualized word representations such as ELMo or BERT (Peters et al., 2018; Devlin et al., 2019), and predict a span as answer. Aggregating and combining evidence from multiple passages is not straightforward when using extractive models, and multiple techniques have been proposed to address this limitation (Clark and Gardner, 2018; Min et al., 2019a).</p>
<p>In this paper, we explore a simple approach having the best of both worlds, by building on the exciting developments in generative modeling and retrieval for open domain question answering. This method proceeds in two steps, by first retrieving supporting passages using either sparse or dense</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architecture of the Fusion-in-Decoder method.
representations. Then, a sequence-to-sequence model generates the answer, taking as input the retrieved passages in addition to the question. While conceptually simple, this method sets new state-of-the-art results on the TriviaQA and NaturalQuestions benchmarks. In particular, we show that the performance of our method significantly improves when the number of retrieved passages increases. We believe that this is evidence that generative models are good at combining evidence from multiple passages, compared to extractive ones.</p>
<h2>2 Related work</h2>
<p>Open domain question answering is the task of answering general domain questions, in which the evidence is not given as input to the system. While being a longstanding problem in natural language processing (Voorhees, 1999), this task has recently regained interest following the work by Chen et al. (2017). In that version of the problem, strong supervision is available to the learning system, in the form of spans corresponding to answers. Chen et al. (2017) proposed to solve the problem by first retrieving support document from Wikipedia, before extracting the answer from the retrieved document. Different methods were proposed to tackle the setting where no gold spans are given to the system, but only the correct answer. Clark and Gardner (2018) proposed to use a global normalization over all the span corresponding to the answer, which was later applied to BERT based models (Wang et al., 2019). Min et al. (2019a) introduced a method based on hard expectationmaximization to tackle noisy supervision from this setting. Wang et al. (2018b) described a technique to aggregate answers from different paragraphs, using confidence and coverage scores.</p>
<p>Passage retrieval is an important step in open domain question answering, and is an active area of research to improve QA systems. Initially, sparse representations based on TF/IDF were used to retrieve support documents (Chen et al., 2017). Lee et al. (2018) introduced a supervised learning
method to rerank paragraphs based on BiLSTM, while Wang et al. (2018a) trained a ranking system with reinforcement learning. A second approach to improve the retrieval step of QA systems is to used additional information such as the Wikipedia or Wikidata graphs (Min et al., 2019b; Asai et al., 2020). Recently, multiple works show that retrieval systems entirely based on dense representation and approximate nearest neighbors were competitive with traditional approaches. Such models can be trained using weak supervision in the form of question-answer pairs (Karpukhin et al., 2020), or pretrained using a cloze task and finetuned end-toend (Guu et al., 2020; Lee et al., 2019).</p>
<p>Generative question answering was mostly considered in previous work for datasets requiring to generate answers, such as NarrativeQA (Kočiskỳ et al., 2018), CoQA (Reddy et al., 2019) or ELI5 (Fan et al., 2019). These datasets were generated in a way that answers do not correspond to spans in support documents, thus requiring abstractive models. Raffel et al. (2020) showed that generative models are competitive for reading comprehension tasks such as SQuAD (Rajpurkar et al., 2016), where answers are spans. Roberts et al. (2020) proposed to use large pretrained generative models, without using additional knowledge, for open domain question answering. Closest to our work, Min et al. (2020) and Lewis et al. (2020b) introduced retrieval augmented generative models for open domain question answering. Our approach differs from these works by how the generative model processes the retrieved passages. This allows to scale to large numbers of documents, and to benefit from this large amount of evidence.</p>
<h2>3 Method</h2>
<p>In this section, we describe our approach to open domain question answering. It proceeds in two steps, first retrieving support passages before processing them with a sequence to sequence model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SQuAD Open</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">DrQA (Chen et al., 2017)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Passage BERT (Wang et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: left;">Path Retriever (Asai et al., 2020)</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{5 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Graph Retriever (Min et al., 2019b)</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Hard EM (Min et al., 2019a)</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ORQA (Lee et al., 2019)</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">REALM (Guu et al., 2020)</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DPR (Karpukhin et al., 2020)</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SpanSeqGen (Min et al., 2020)</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">RAG (Lewis et al., 2020b)</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">T5 (Roberts et al., 2020)</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 few shot (Brown et al., 2020)</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Fusion-in-Decoder (base)</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: left;">Fusion-in-Decoder (large)</td>
<td style="text-align: center;">$\mathbf{5 1 . 4}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 1}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 7}$</td>
<td style="text-align: center;">63.2</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison to state-of-the-art. On TriviaQA, we report results on the open domain test set (left), and on the hidden test set (right), competitions.codalab.org/competitions/17208#results).</p>
<p>Retrieval. For the retrieval of support passages, we consider two methods: BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020). In BM25, passages are represented as bag of words, and the ranking function is based on term and inverse document frequencies. We use the implementation from Apache Lucene ${ }^{1}$ with default parameters, and tokenize questions and passages with SpaCy. ${ }^{2}$ In DPR, passages and questions are represented as dense vector representations, computed using two BERT networks. The ranking function is the dot product between the query and passage representations. Retrieval is performed using approximate nearest neighbors with the FAISS library. ${ }^{3}$</p>
<p>Reading. Our generative model for open domain QA is based on a sequence-to-sequence network, pretrained on unsupervised data, such as T5 or BART (Raffel et al., 2020; Lewis et al., 2020a). The model takes as input the question, as well as the support passages, and generates the answer. More precisely, each retrieved passage and its title are concatenated with the question, and processed independently from other passages by the encoder. We add special tokens question:, title: and context: before the question, title and text of each passage. Finally, the decoder performs atten-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion over the concatenation of the resulting representations of all the retrieved passages. The model thus performs evidence fusion in the decoder only, and we refer to it as Fusion-in-Decoder.</p>
<p>By processing passages independently in the encoder, but jointly in the decoder, this method differs from Min et al. (2020) and Lewis et al. (2020b). Processing passages independently in the encoder allows to scale to large number of contexts, as it only performs self attention over one context at a time. This means that the computation time of the model grows linearly with the number of passages, instead of quadratically. On the other hand, processing passages jointly in the decoder allows to better aggregate evidence from multiple passages.</p>
<h2>4 Experiments</h2>
<p>In this section, we report empirical evaluations of Fusion-in-Decoder for open domain QA.</p>
<p>Datasets. We consider the following datasets, and use the same setting as Lee et al. (2019):</p>
<ul>
<li>NaturalQuestions (Kwiatkowski et al., 2019) contains questions corresponding to Google search queries. The open-domain version of this dataset is obtained by discarding answers with more than 5 tokens.</li>
<li>TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and quiz-league</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of retrieved passages.
websites. The unfiltered version of TriviaQA is used for open-domain question answering.</p>
<ul>
<li>SQuAD v1.1 (Rajpurkar et al., 2016) is a reading comprehension dataset. Given a paragraph extracted from Wikipedia, annotators were asked to write questions, for which the answer is a span from the corresponding paragraph.</li>
</ul>
<p>Following Lee et al. (2019) we use the validation as test, and keep $10 \%$ of the training set for validation. We use the Wikipedia dumps from Dec. 20, 2018 for NQ and TriviaQA and from Dec. 21, 2016 for SQuAD. We apply the same preprocessing as Chen et al. (2017); Karpukhin et al. (2020), leading to passages of 100 words, which do not overlap.</p>
<p>Evaluation. Predicted answers are evaluated with the standard exact match metric (EM), as introduced by Rajpurkar et al. (2016). A generated answer is considered correct if it matches any answer of the list of acceptable answers after normalization. This normalization step consists in lowercasing and removing articles, punctuation and duplicated whitespace.</p>
<p>Technical details. We initialize our models with the pretrained T5 models (Raffel et al., 2020), available in the HuggingFace Transformers library. ${ }^{4}$ We consider two model sizes, base and large, containing respectively 220 M and 770 M parameters. We fine-tune the models on each dataset independently, using Adam (Kingma and Ba, 2014) with a constant learning rate of $10^{-4}$ and a dropout rate of $10 \%$. We train the model for 10 k gradient steps, with a batch size of 64 , using 64 Tesla V100 32Gb. We evaluate models every 500 steps and select the best one on the validation set based on the Exact Match score. During training on NaturalQuestions</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and SQuAD, we sample the target among the list of answers, while for TriviaQA, we use the unique human-generated answer. For TriviaQA, answers in uppercase are normalized by converting all letters in lowercase except the first letter of each word, using the title Python string method. For both training and testing, we retrieve 100 passages (unless said otherwise), and truncate them to 250 word pieces. Following the results of Karpukhin et al. (2020), passages are retrieved with DPR for NQ and TriviaQA, and with BM25 for SQuAD. We generate answers by using greedy decoding.</p>
<p>Comparison to state-of-the-art. In table 1, we compare the results obtained by Fusion-in-Decoder with existing approaches for open domain question answering. We observe that while conceptually simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages need to be aggregated, compared to extractive approaches. Our method also performs better than other generative models, showing that scaling to large number of passages and processing them jointly leads to improvement in accuracy. Second, we observe that using additional knowledge in generative models by using retrieval lead to important performance gains. On NaturalQuestions, the closed book T5 model obtains $36.6 \%$ accuracy with 11 B parameters, while our approach obtains $44.1 \%$ with 770 M parameters plus Wikipedia with BM25 retrieval. Both methods use roughly the same amount of memory to store information, indicating that text based explicit memories are competitive for knowledge retrieval tasks.</p>
<p>Scaling with number of passages. In Figure 3, we report the performance with respect to the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training Passages</th>
<th style="text-align: center;">NaturalQuestions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o finetuning</td>
<td style="text-align: center;">w/ finetuning</td>
<td style="text-align: center;">w/o finetuning</td>
<td style="text-align: center;">w/ finetuning</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">64.2</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">63.6</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">64.2</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">64.3</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance depending on the number of passages used during training. Exact Match scores are reported on dev sets.
number of retrieved passages. In particular, we observe that increasing the number of passages from 10 to 100 leads to $6 \%$ improvement on TriviaQA and $3.5 \%$ improvement on NaturalQuestions. On the other hand, the performance of most extractive models seems to peak around 10 to 20 passages (Wang et al., 2019; Yang et al., 2019). We believe that this is evidence that sequence-tosequence models are good at combining informations from multiple passages.</p>
<p>Impact of the number of training passages. In the previous section, the model was trained and evaluated with the same number of passages. To reduce the training computational budget, a simple solution consists in training the model with fewer passages. In Table 2, we report the performance obtained by training with different numbers of passages, while testing with 100 passages. We observe that reducing the number of training passages leads to a decrease of accuracy. Further, we propose to finetune the previous models using 100 passages for 1000 steps. This allows to reduce the accuracy gap, while using significantly less computational resources: we can reach 46.0 EM on NaturalQuestions, using 147 GPU hours, compared to 425 GPU hours when training on 100 passages.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we study a simple approach to open domain question answering, which relies on retrieving support passages before processing them with a generative model. We show that while conceptually simple, this approach is competitive with existing methods, and that it scales well with the number of retrieved passages. In future work, we plan to make this model more efficient, in particular when scaling to large number of support passages. We also plan to integrate the retrieval in our model, and to learn the whole system end-to-end.</p>
<h2>References</h2>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1870-1879.</p>
<p>Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages $845-855$.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558-3567.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2019. How can we know what language models know? arXiv preprint arXiv:1911.12543.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly</p>
<p>supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1601-1611.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328 .</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Jinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung Ko, and Jaewoo Kang. 2018. Ranking paragraphs for improving answer recall in open-domain question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 565-569.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401.</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing, pages 2851-2864.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868.</p>
<p>Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5783-5797.</p>
<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2227-2237.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2463-2473.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Technical Report.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5418-5426.</p>
<p>Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, and Mike Gatford. 1995. Okapi at TREC-3. NIST Special Publication Sp.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019. oLMpics - on what language model pre-training captures. arXiv preprint arXiv:1912.13283.</p>
<p>Ellen M Voorhees. 1999. The TREC-8 question answering track report. In Proceedings of the Eighth Text REtrieval Conference.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018a. R ${ }^{3}$ : Reinforced ranker-reader for open-domain question answering. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2018b. Evidence aggregation for answer re-ranking in opendomain question answering. In International Conference on Learning Representations.</p>
<p>Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019. Multi-passage BERT: A globally normalized BERT model for open-domain question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages $5878-5882$.</p>
<p>Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 72-77.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ github.com/huggingface/transformers&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
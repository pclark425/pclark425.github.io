<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-737 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-737</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-737</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-270095326</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.18917v1.pdf" target="_blank">Causal Action Influence Aware Counterfactual Data Augmentation</a></p>
                <p><strong>Paper Abstract:</strong> Offline data are both valuable and practical resources for teaching robots complex behaviors. Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution. However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships. We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions. By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping action -unaffected parts of the state-space between independent trajectories in the dataset. We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift. Videos, code and data are available at https: //sites.google.com/view/caiac .</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e737.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e737.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAIAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Action Influence Aware Counterfactual Data Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation method that creates counterfactual but (mostly) dynamically-feasible offline transitions by swapping state-factors that the agent does not causally influence, using an explicit state-dependent causal-influence measure to detect which entities are action-unaffected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CAIAC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute a state-wise uncontrollable set U_s = {j | C_j(s) ≤ θ} using a pointwise conditional mutual information measure (CAI) per entity; given two transitions that share at least one uncontrollable entity, synthesize counterfactual transitions by swapping the (s_i, s'_i) for i in U_s ∩ U_ŝ between trajectories. The transition model P(S'_j | S=s, A=a) is modeled as a Gaussian neural network; the CAI is estimated as I(S'_j; A | S=s) via approximated KL divergence over action-conditioned predictive distributions. Counterfactuals are accepted without an additional feasibility re-check (authors accept a small fraction of infeasible samples).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Franka-Kitchen; Fetch-Push; Fetch-Pick&Lift (simulated robotic manipulation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated robotics manipulation environments (Franka-Kitchen, Fetch variants) using offline datasets of prerecorded trajectories; environments are interactive simulators but CAIAC is applied offline (no new online experimentation during augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detection of action-unaffected entities via conditional mutual information (state-conditioned MI) and counterfactual data augmentation by swapping those entities across trajectories to break spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations/selection biases arising from limited support in offline demonstrations (confounded co-occurrence of independent entities); irrelevant/distractor entities that are correlated with actions in the dataset but not causally affected by them.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>State-conditioned conditional mutual information (CAI): C_j(s) := I(S'_j; A | S = s), estimated by fitting Gaussian predictive models P(S'_j | S=s, A=a) and computing an empirical KL-based estimator over sampled actions (uniform actions used).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not a downweighting approach per se; instead augments the dataset with counterfactual samples to reduce the network's ability to rely on spurious correlates (functional equivalent to reducing influence of distractors via data redistribution).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>No explicit statistical refutation step; counterfactual generation is used to empirically break spurious correlations and thereby 'refute' spurious predictors during downstream policy learning. A threshold θ on CAI is tuned (grid search) and ablations on observed:counterfactual ratios study induced selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Substantially improved out-of-distribution (OOD) success rates in manipulation tasks; e.g., Franka-Kitchen OOD: Kettle 0.81±0.07, Microwave 0.75±0.09 (Table 1), and high success on Fetch-Pick&Lift and Fetch-Push compared to baselines. Authors report CAIAC is better than baselines in 6/7 Franka-Kitchen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines (NO-AUGM) perform poorly on OOD: e.g., Franka-Kitchen NO-AUGM Kettle 0.07±0.06, Microwave 0.01±0.03. In Fetch tasks baseline performance is substantially lower than CAIAC in low-data/OOD regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit detection of action influence via state-conditioned mutual information allows reliable identification of action-unaffected entities; swapping those entities to create counterfactuals increases support of the joint state distribution and prevents policies from learning spurious correlates, yielding large OOD gains. Heuristic/attention-based causal discovery baselines often misidentify influence in high-dimensional inputs and create infeasible counterfactuals; CAIAC's principled CAI measure is more stable across seeds. Ablations show optimal observed:counterfactual ratios exist (e.g., too many counterfactuals can hurt due to selection bias) and that threshold θ needs modest tuning but CAI estimates have low variance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Action Influence Aware Counterfactual Data Augmentation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e737.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e737.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Action Influence (state-conditioned mutual information)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-dependent, pointwise measure of the dependence between action and each next-state factor, defined as I(S'_j; A | S = s), used to infer whether an action-causes-change for a particular entity at a particular observed state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal influence detection for improving efficiency in reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal Action Influence (CAI)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Estimate C_j(s) := I(S'_j; A | S = s) by fitting a conditional predictive model P(S'_j | S=s, A=a) (Gaussian NN predicting mean & variance) and computing an empirical KL-based estimator over K sampled actions (authors use uniform sampling over action space); CAI is zero when S'_j ⟂⟂ A | S=s and thus used to decide controllability per entity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated robotics benchmarks (Franka-Kitchen, Fetch-Push, Fetch-Pick&Lift) where it is used to classify per-entity action influence from offline data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline simulated manipulation datasets; CAI is computed offline by querying the learned predictive model with sampled actions at observed states.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Identifies distractors as entities with near-zero CAI (action-unaffected); these are then eligible to be swapped during counterfactual augmentation to break spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlational structure between entities and actions due to limited coverage in offline datasets (confounding by co-occurrence).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Explicit information-theoretic detection: compute conditional mutual information estimated via KL between action-conditional predictive distributions and the marginal predictive distribution (approximated for Gaussian mixtures).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not applicable as a downweighting algorithm; CAI produces a binary/continuous score used to select entities for augmentation rather than directly weighting training samples.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Used indirectly to generate counterfactuals that empirically disconfirm spurious associative cues during downstream learning; no formal statistical refutation protocol provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>CAI yields high AUC (~0.9) on influence-detection ROC in Fetch-Push experiments (Fig.10), low variance across seeds, and stable threshold selection; enables CAIAC's improved downstream OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>State-conditioned MI (CAI) is a reliable, low-variance estimator of per-entity action influence in these simulated manipulation tasks, outperforming transformer-attention heuristics at detecting which entities are controllable and enabling valid counterfactual augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Action Influence Aware Counterfactual Data Augmentation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e737.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e737.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoDA (CODA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual Data Augmentation using Locally Factored Dynamics (CODA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method (Pitis et al., 2020) that generates counterfactual samples by estimating the local causal graph (via transformer attention) and swapping connected components between transitions that share local structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counterfactual data augmentation using locally factored dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CODA</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a transformer world model and use attention weights as a heuristic adjacency matrix for the local causal graph; identify connected components (subgraphs) that are 'independent' and swap those components between transitions to synthesize counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied to simulated manipulation benchmarks in the paper as a baseline (Franka-Kitchen, Fetch-Push, Fetch-Pick&Lift).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline simulated robotics datasets; CODA in its original conception is compatible with learned world models and local causal graph estimation from observational sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Heuristic discovery of local causal components using transformer attention; counterfactual swaps aimed at breaking spurious co-occurrence patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious co-occurrence/confounding due to limited demonstration support (same types targeted as CAIAC).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Transformer attention weights aggregated across layers to yield influence scores between input positions (s_i) and output positions (s'_j); thresholding yields adjacency for swapping.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>No explicit statistical downweighting; uses augmentation (swapping) to alter dataset distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>No explicit refutation step; relies on augmented data to prevent downstream models from relying on spurious correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In this paper CODA often underperforms in high-dimensional settings: e.g., Franka-Kitchen OOD Kettle 0.18±0.05, Microwave 0.07±0.05 (Table 1), substantially below CAIAC; authors report CODA attention weights often misidentify influence leading to infeasible counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformer-attention-based causal discovery can fail in high-dimensional inputs, misestimating action influence (sometimes even failing to recover temporal self-links), which leads to creation of dynamically-infeasible counterfactuals and poor downstream OOD performance; CODA can work in simpler/low-dimensional settings but is unstable and high-variance across seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Action Influence Aware Counterfactual Data Augmentation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e737.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e737.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoDA-ACTION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CODA-ACTION (action-focused CODA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablated version of CODA that attempts to estimate only action→entity influence via transformer attention restricted to the action input row, used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CODA-ACTION</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Same transformer-based attention heuristic as CODA but only inspects the attention from the action token(s) to object output positions to estimate which entities are influenced by actions; thresholded scores form the controllability classification used for swapping.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Franka-Kitchen; Fetch-Push; Fetch-Pick&Lift (used as baseline environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline simulated manipulation datasets; transformer world model trained to predict next-state components and attention inspected post-hoc.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Heuristic identification of action-influenced entities using transformer's action-row attention, then swap presumed-uncontrollable entities to create counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlational structure between entities and actions in offline data.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Transformer action-row attention aggregation across layers, thresholded to classify influence.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>No explicit downweighting; dataset augmentation via swapping.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Per authors' experiments, CODA-ACTION often performs poorly in high-dimensional tasks due to misestimated influences (e.g., Franka-Kitchen OOD results similar to CODA and worse than CAIAC). Specific reported numbers include very low OOD success in motivating examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Restricting transformer-attention heuristics to action→entity rows does not resolve instability: attention still fails to reliably detect true action influence in complex/high-dimensional offline datasets, producing infeasible counterfactuals and poor downstream generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Action Influence Aware Counterfactual Data Augmentation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e737.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e737.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robust Reinforcement Learning via State Corruption (RSC) (as reimplemented)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robustness-focused RL framework that constructs new samples by perturbing state values via a heuristic and learns a structural causal model to predict next states, used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Seeing is not believing: Robust reinforcement learning against spurious correlation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RSC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Perturb state values according to a heuristic objective to create augmented training examples, and train a dynamics model with regularization (sparsity-inducing) to predict next states under perturbed inputs; intended to reduce reliance on spurious features by exposure to perturbed states during training. The authors reimplemented RSC with some differences (transformer dynamics, object-centric perturbations).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Franka-Kitchen; Fetch-Push; Fetch-Pick&Lift (used as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline simulated manipulation datasets; RSC performs offline augmentations via state perturbations and trains regularized dynamics models.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Heuristic perturbations of state values and learning a sparse structural model to be invariant to perturbed features (regularization encourages sparsity in causal graph).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations/nuisance correlates between state features and actions/outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Heuristic maximization (perturbation objective) to identify candidate distractor dimensions; regularization during dynamics learning to promote sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Indirect via regularized model training (sparsity), and exposure to perturbed samples to reduce reliance on perturbed dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In the paper's experiments RSC underperforms relative to CAIAC in OOD settings; authors report RSC can break true cause-effect relations when perturbations are not object-aware, leading to decreased performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Heuristic perturbation approaches can degrade performance if perturbations break true causal relationships; object-centric, principled detection (like CAI) is safer than uniform heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Action Influence Aware Counterfactual Data Augmentation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e737.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e737.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBPO (model-based baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-Based Policy Optimization style model-based augmentation (MBPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard model-based data generation approach that trains a dynamics model on data and generates imagined on-policy rollout transitions to augment training; used here as a baseline and in hybrid CAIAC+MBPO experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MBPO (model-based rollouts)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a dynamics model from the available data and generate multi-step imagined rollouts (on-policy or mixed) to augment the training buffer; rollouts are typically short horizon to limit compounding model error.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Fetch-Push; Fetch-Pick&Lift (evaluated in paper as baseline/hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated robotics manipulation offline datasets; model-based rollouts performed offline using learned dynamics models.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Does not explicitly target distractors/spurious correlations; can either fail to break spurious correlations or, in low-data regimes where model is accurate, increase data coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Mixed: MBPO fails to break spurious correlations in some spurious-correlation experiments (poor OOD performance), but in some low-data regimes MBPO outperforms CAIAC by generating many useful imagined samples (authors report MBPO better in a low-data Fetch-Push case). Hybrid CAIAC+MBPO can improve performance in some low-data settings but may suffer from compounding errors if augmented samples include some infeasible CAIAC counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model-based rollouts are powerful when the learned dynamics model is accurate (esp. low-data regimes), but they do not inherently identify or remove spurious correlates; combining model-based rollouts with causally-aware augmentations (CAIAC+MBPO) can help but also risks compounding errors if augmented samples contain infeasible transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Action Influence Aware Counterfactual Data Augmentation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e737.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e737.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local Causal Model (LCM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Causal Model framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual framework that reasons about local causal structure induced by observing V=v and prunes the global causal graph to a sparser local SCM, enabling local counterfactual reasoning and componentwise augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counterfactual data augmentation using locally factored dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Local Causal Model (LCM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given an SCM and observation V = v, derive a local SCM by fixing observed variables and removing edges until the graph is causally minimal; this yields (potentially) factorized subgraphs that can be treated independently for counterfactual swaps. Pitis et al. (2020) propose estimating the local graph (heuristically via transformer attention) to perform counterfactual augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General conceptual framework; used in robotics manipulation benchmarks when instantiated (Franka-Kitchen, Fetch).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Framework applies to interactive environments but in practice requires learned estimators of local structure from offline data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Factorizes local graph into independent mechanisms and swaps components identified as independent to create counterfactuals, thereby addressing distractor co-occurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations via confounding/co-occurrence in observed trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>In original LCM proposals detection is via heuristics from learned models (e.g., transformer attention) or other estimated influence measures; paper contrasts this with CAI-based detection.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When local graph estimation is correct, LCM-based augmentations can produce valid counterfactuals and improve generalization; in practice heuristic estimators (attention) can fail, reducing efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LCM is a principled basis for counterfactual augmentation, but practical performance depends critically on reliable local-structure estimation; CAIAC adopts the LCM idea but replaces heuristic attention-based discovery with explicit CAI detection under an inductive assumption of sparse object-object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Action Influence Aware Counterfactual Data Augmentation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Counterfactual data augmentation using locally factored dynamics <em>(Rating: 2)</em></li>
                <li>Causal influence detection for improving efficiency in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Seeing is not believing: Robust reinforcement learning against spurious correlation <em>(Rating: 2)</em></li>
                <li>Mocoda: Model-based counterfactual data augmentation <em>(Rating: 2)</em></li>
                <li>Causal dynamics learning for task-independent state abstraction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-737",
    "paper_id": "paper-270095326",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "CAIAC",
            "name_full": "Causal Action Influence Aware Counterfactual Data Augmentation",
            "brief_description": "A data-augmentation method that creates counterfactual but (mostly) dynamically-feasible offline transitions by swapping state-factors that the agent does not causally influence, using an explicit state-dependent causal-influence measure to detect which entities are action-unaffected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "CAIAC",
            "method_description": "Compute a state-wise uncontrollable set U_s = {j | C_j(s) ≤ θ} using a pointwise conditional mutual information measure (CAI) per entity; given two transitions that share at least one uncontrollable entity, synthesize counterfactual transitions by swapping the (s_i, s'_i) for i in U_s ∩ U_ŝ between trajectories. The transition model P(S'_j | S=s, A=a) is modeled as a Gaussian neural network; the CAI is estimated as I(S'_j; A | S=s) via approximated KL divergence over action-conditioned predictive distributions. Counterfactuals are accepted without an additional feasibility re-check (authors accept a small fraction of infeasible samples).",
            "environment_name": "Franka-Kitchen; Fetch-Push; Fetch-Pick&Lift (simulated robotic manipulation benchmarks)",
            "environment_description": "Simulated robotics manipulation environments (Franka-Kitchen, Fetch variants) using offline datasets of prerecorded trajectories; environments are interactive simulators but CAIAC is applied offline (no new online experimentation during augmentation).",
            "handles_distractors": true,
            "distractor_handling_technique": "Detection of action-unaffected entities via conditional mutual information (state-conditioned MI) and counterfactual data augmentation by swapping those entities across trajectories to break spurious correlations.",
            "spurious_signal_types": "Spurious correlations/selection biases arising from limited support in offline demonstrations (confounded co-occurrence of independent entities); irrelevant/distractor entities that are correlated with actions in the dataset but not causally affected by them.",
            "detection_method": "State-conditioned conditional mutual information (CAI): C_j(s) := I(S'_j; A | S = s), estimated by fitting Gaussian predictive models P(S'_j | S=s, A=a) and computing an empirical KL-based estimator over sampled actions (uniform actions used).",
            "downweighting_method": "Not a downweighting approach per se; instead augments the dataset with counterfactual samples to reduce the network's ability to rely on spurious correlates (functional equivalent to reducing influence of distractors via data redistribution).",
            "refutation_method": "No explicit statistical refutation step; counterfactual generation is used to empirically break spurious correlations and thereby 'refute' spurious predictors during downstream policy learning. A threshold θ on CAI is tuned (grid search) and ablations on observed:counterfactual ratios study induced selection bias.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Substantially improved out-of-distribution (OOD) success rates in manipulation tasks; e.g., Franka-Kitchen OOD: Kettle 0.81±0.07, Microwave 0.75±0.09 (Table 1), and high success on Fetch-Pick&Lift and Fetch-Push compared to baselines. Authors report CAIAC is better than baselines in 6/7 Franka-Kitchen tasks.",
            "performance_without_robustness": "Baselines (NO-AUGM) perform poorly on OOD: e.g., Franka-Kitchen NO-AUGM Kettle 0.07±0.06, Microwave 0.01±0.03. In Fetch tasks baseline performance is substantially lower than CAIAC in low-data/OOD regimes.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Explicit detection of action influence via state-conditioned mutual information allows reliable identification of action-unaffected entities; swapping those entities to create counterfactuals increases support of the joint state distribution and prevents policies from learning spurious correlates, yielding large OOD gains. Heuristic/attention-based causal discovery baselines often misidentify influence in high-dimensional inputs and create infeasible counterfactuals; CAIAC's principled CAI measure is more stable across seeds. Ablations show optimal observed:counterfactual ratios exist (e.g., too many counterfactuals can hurt due to selection bias) and that threshold θ needs modest tuning but CAI estimates have low variance.",
            "uuid": "e737.0",
            "source_info": {
                "paper_title": "Causal Action Influence Aware Counterfactual Data Augmentation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CAI",
            "name_full": "Causal Action Influence (state-conditioned mutual information)",
            "brief_description": "A state-dependent, pointwise measure of the dependence between action and each next-state factor, defined as I(S'_j; A | S = s), used to infer whether an action-causes-change for a particular entity at a particular observed state.",
            "citation_title": "Causal influence detection for improving efficiency in reinforcement learning",
            "mention_or_use": "use",
            "method_name": "Causal Action Influence (CAI)",
            "method_description": "Estimate C_j(s) := I(S'_j; A | S = s) by fitting a conditional predictive model P(S'_j | S=s, A=a) (Gaussian NN predicting mean & variance) and computing an empirical KL-based estimator over K sampled actions (authors use uniform sampling over action space); CAI is zero when S'_j ⟂⟂ A | S=s and thus used to decide controllability per entity.",
            "environment_name": "Same simulated robotics benchmarks (Franka-Kitchen, Fetch-Push, Fetch-Pick&Lift) where it is used to classify per-entity action influence from offline data.",
            "environment_description": "Offline simulated manipulation datasets; CAI is computed offline by querying the learned predictive model with sampled actions at observed states.",
            "handles_distractors": true,
            "distractor_handling_technique": "Identifies distractors as entities with near-zero CAI (action-unaffected); these are then eligible to be swapped during counterfactual augmentation to break spurious correlations.",
            "spurious_signal_types": "Spurious correlational structure between entities and actions due to limited coverage in offline datasets (confounding by co-occurrence).",
            "detection_method": "Explicit information-theoretic detection: compute conditional mutual information estimated via KL between action-conditional predictive distributions and the marginal predictive distribution (approximated for Gaussian mixtures).",
            "downweighting_method": "Not applicable as a downweighting algorithm; CAI produces a binary/continuous score used to select entities for augmentation rather than directly weighting training samples.",
            "refutation_method": "Used indirectly to generate counterfactuals that empirically disconfirm spurious associative cues during downstream learning; no formal statistical refutation protocol provided.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "CAI yields high AUC (~0.9) on influence-detection ROC in Fetch-Push experiments (Fig.10), low variance across seeds, and stable threshold selection; enables CAIAC's improved downstream OOD performance.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "State-conditioned MI (CAI) is a reliable, low-variance estimator of per-entity action influence in these simulated manipulation tasks, outperforming transformer-attention heuristics at detecting which entities are controllable and enabling valid counterfactual augmentation.",
            "uuid": "e737.1",
            "source_info": {
                "paper_title": "Causal Action Influence Aware Counterfactual Data Augmentation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CoDA (CODA)",
            "name_full": "Counterfactual Data Augmentation using Locally Factored Dynamics (CODA)",
            "brief_description": "A prior method (Pitis et al., 2020) that generates counterfactual samples by estimating the local causal graph (via transformer attention) and swapping connected components between transitions that share local structures.",
            "citation_title": "Counterfactual data augmentation using locally factored dynamics",
            "mention_or_use": "use",
            "method_name": "CODA",
            "method_description": "Train a transformer world model and use attention weights as a heuristic adjacency matrix for the local causal graph; identify connected components (subgraphs) that are 'independent' and swap those components between transitions to synthesize counterfactuals.",
            "environment_name": "Applied to simulated manipulation benchmarks in the paper as a baseline (Franka-Kitchen, Fetch-Push, Fetch-Pick&Lift).",
            "environment_description": "Offline simulated robotics datasets; CODA in its original conception is compatible with learned world models and local causal graph estimation from observational sequences.",
            "handles_distractors": true,
            "distractor_handling_technique": "Heuristic discovery of local causal components using transformer attention; counterfactual swaps aimed at breaking spurious co-occurrence patterns.",
            "spurious_signal_types": "Spurious co-occurrence/confounding due to limited demonstration support (same types targeted as CAIAC).",
            "detection_method": "Transformer attention weights aggregated across layers to yield influence scores between input positions (s_i) and output positions (s'_j); thresholding yields adjacency for swapping.",
            "downweighting_method": "No explicit statistical downweighting; uses augmentation (swapping) to alter dataset distribution.",
            "refutation_method": "No explicit refutation step; relies on augmented data to prevent downstream models from relying on spurious correlates.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In this paper CODA often underperforms in high-dimensional settings: e.g., Franka-Kitchen OOD Kettle 0.18±0.05, Microwave 0.07±0.05 (Table 1), substantially below CAIAC; authors report CODA attention weights often misidentify influence leading to infeasible counterfactuals.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Transformer-attention-based causal discovery can fail in high-dimensional inputs, misestimating action influence (sometimes even failing to recover temporal self-links), which leads to creation of dynamically-infeasible counterfactuals and poor downstream OOD performance; CODA can work in simpler/low-dimensional settings but is unstable and high-variance across seeds.",
            "uuid": "e737.2",
            "source_info": {
                "paper_title": "Causal Action Influence Aware Counterfactual Data Augmentation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CoDA-ACTION",
            "name_full": "CODA-ACTION (action-focused CODA variant)",
            "brief_description": "An ablated version of CODA that attempts to estimate only action→entity influence via transformer attention restricted to the action input row, used as a baseline in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "CODA-ACTION",
            "method_description": "Same transformer-based attention heuristic as CODA but only inspects the attention from the action token(s) to object output positions to estimate which entities are influenced by actions; thresholded scores form the controllability classification used for swapping.",
            "environment_name": "Franka-Kitchen; Fetch-Push; Fetch-Pick&Lift (used as baseline environments)",
            "environment_description": "Offline simulated manipulation datasets; transformer world model trained to predict next-state components and attention inspected post-hoc.",
            "handles_distractors": true,
            "distractor_handling_technique": "Heuristic identification of action-influenced entities using transformer's action-row attention, then swap presumed-uncontrollable entities to create counterfactuals.",
            "spurious_signal_types": "Spurious correlational structure between entities and actions in offline data.",
            "detection_method": "Transformer action-row attention aggregation across layers, thresholded to classify influence.",
            "downweighting_method": "No explicit downweighting; dataset augmentation via swapping.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Per authors' experiments, CODA-ACTION often performs poorly in high-dimensional tasks due to misestimated influences (e.g., Franka-Kitchen OOD results similar to CODA and worse than CAIAC). Specific reported numbers include very low OOD success in motivating examples.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Restricting transformer-attention heuristics to action→entity rows does not resolve instability: attention still fails to reliably detect true action influence in complex/high-dimensional offline datasets, producing infeasible counterfactuals and poor downstream generalization.",
            "uuid": "e737.3",
            "source_info": {
                "paper_title": "Causal Action Influence Aware Counterfactual Data Augmentation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RSC",
            "name_full": "Robust Reinforcement Learning via State Corruption (RSC) (as reimplemented)",
            "brief_description": "A robustness-focused RL framework that constructs new samples by perturbing state values via a heuristic and learns a structural causal model to predict next states, used as a baseline in the paper.",
            "citation_title": "Seeing is not believing: Robust reinforcement learning against spurious correlation",
            "mention_or_use": "use",
            "method_name": "RSC",
            "method_description": "Perturb state values according to a heuristic objective to create augmented training examples, and train a dynamics model with regularization (sparsity-inducing) to predict next states under perturbed inputs; intended to reduce reliance on spurious features by exposure to perturbed states during training. The authors reimplemented RSC with some differences (transformer dynamics, object-centric perturbations).",
            "environment_name": "Franka-Kitchen; Fetch-Push; Fetch-Pick&Lift (used as baseline comparisons)",
            "environment_description": "Offline simulated manipulation datasets; RSC performs offline augmentations via state perturbations and trains regularized dynamics models.",
            "handles_distractors": true,
            "distractor_handling_technique": "Heuristic perturbations of state values and learning a sparse structural model to be invariant to perturbed features (regularization encourages sparsity in causal graph).",
            "spurious_signal_types": "Spurious correlations/nuisance correlates between state features and actions/outcomes.",
            "detection_method": "Heuristic maximization (perturbation objective) to identify candidate distractor dimensions; regularization during dynamics learning to promote sparsity.",
            "downweighting_method": "Indirect via regularized model training (sparsity), and exposure to perturbed samples to reduce reliance on perturbed dimensions.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In the paper's experiments RSC underperforms relative to CAIAC in OOD settings; authors report RSC can break true cause-effect relations when perturbations are not object-aware, leading to decreased performance.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Heuristic perturbation approaches can degrade performance if perturbations break true causal relationships; object-centric, principled detection (like CAI) is safer than uniform heuristics.",
            "uuid": "e737.4",
            "source_info": {
                "paper_title": "Causal Action Influence Aware Counterfactual Data Augmentation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MBPO (model-based baseline)",
            "name_full": "Model-Based Policy Optimization style model-based augmentation (MBPO)",
            "brief_description": "Standard model-based data generation approach that trains a dynamics model on data and generates imagined on-policy rollout transitions to augment training; used here as a baseline and in hybrid CAIAC+MBPO experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "MBPO (model-based rollouts)",
            "method_description": "Train a dynamics model from the available data and generate multi-step imagined rollouts (on-policy or mixed) to augment the training buffer; rollouts are typically short horizon to limit compounding model error.",
            "environment_name": "Fetch-Push; Fetch-Pick&Lift (evaluated in paper as baseline/hybrid)",
            "environment_description": "Simulated robotics manipulation offline datasets; model-based rollouts performed offline using learned dynamics models.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Does not explicitly target distractors/spurious correlations; can either fail to break spurious correlations or, in low-data regimes where model is accurate, increase data coverage.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Mixed: MBPO fails to break spurious correlations in some spurious-correlation experiments (poor OOD performance), but in some low-data regimes MBPO outperforms CAIAC by generating many useful imagined samples (authors report MBPO better in a low-data Fetch-Push case). Hybrid CAIAC+MBPO can improve performance in some low-data settings but may suffer from compounding errors if augmented samples include some infeasible CAIAC counterfactuals.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Model-based rollouts are powerful when the learned dynamics model is accurate (esp. low-data regimes), but they do not inherently identify or remove spurious correlates; combining model-based rollouts with causally-aware augmentations (CAIAC+MBPO) can help but also risks compounding errors if augmented samples contain infeasible transitions.",
            "uuid": "e737.5",
            "source_info": {
                "paper_title": "Causal Action Influence Aware Counterfactual Data Augmentation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Local Causal Model (LCM)",
            "name_full": "Local Causal Model framework",
            "brief_description": "A conceptual framework that reasons about local causal structure induced by observing V=v and prunes the global causal graph to a sparser local SCM, enabling local counterfactual reasoning and componentwise augmentation.",
            "citation_title": "Counterfactual data augmentation using locally factored dynamics",
            "mention_or_use": "mention",
            "method_name": "Local Causal Model (LCM)",
            "method_description": "Given an SCM and observation V = v, derive a local SCM by fixing observed variables and removing edges until the graph is causally minimal; this yields (potentially) factorized subgraphs that can be treated independently for counterfactual swaps. Pitis et al. (2020) propose estimating the local graph (heuristically via transformer attention) to perform counterfactual augmentation.",
            "environment_name": "General conceptual framework; used in robotics manipulation benchmarks when instantiated (Franka-Kitchen, Fetch).",
            "environment_description": "Framework applies to interactive environments but in practice requires learned estimators of local structure from offline data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Factorizes local graph into independent mechanisms and swaps components identified as independent to create counterfactuals, thereby addressing distractor co-occurrence.",
            "spurious_signal_types": "Spurious correlations via confounding/co-occurrence in observed trajectories.",
            "detection_method": "In original LCM proposals detection is via heuristics from learned models (e.g., transformer attention) or other estimated influence measures; paper contrasts this with CAI-based detection.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "When local graph estimation is correct, LCM-based augmentations can produce valid counterfactuals and improve generalization; in practice heuristic estimators (attention) can fail, reducing efficacy.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "LCM is a principled basis for counterfactual augmentation, but practical performance depends critically on reliable local-structure estimation; CAIAC adopts the LCM idea but replaces heuristic attention-based discovery with explicit CAI detection under an inductive assumption of sparse object-object interactions.",
            "uuid": "e737.6",
            "source_info": {
                "paper_title": "Causal Action Influence Aware Counterfactual Data Augmentation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Counterfactual data augmentation using locally factored dynamics",
            "rating": 2,
            "sanitized_title": "counterfactual_data_augmentation_using_locally_factored_dynamics"
        },
        {
            "paper_title": "Causal influence detection for improving efficiency in reinforcement learning",
            "rating": 2,
            "sanitized_title": "causal_influence_detection_for_improving_efficiency_in_reinforcement_learning"
        },
        {
            "paper_title": "Seeing is not believing: Robust reinforcement learning against spurious correlation",
            "rating": 2,
            "sanitized_title": "seeing_is_not_believing_robust_reinforcement_learning_against_spurious_correlation"
        },
        {
            "paper_title": "Mocoda: Model-based counterfactual data augmentation",
            "rating": 2,
            "sanitized_title": "mocoda_modelbased_counterfactual_data_augmentation"
        },
        {
            "paper_title": "Causal dynamics learning for task-independent state abstraction",
            "rating": 1,
            "sanitized_title": "causal_dynamics_learning_for_taskindependent_state_abstraction"
        }
    ],
    "cost": 0.01948875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Causal Action Influence Aware Counterfactual Data Augmentation
29 May 2024</p>
<p>Núria Armengol Urpí 
Marco Bagatella 
Marin Vlastelica 
Georg Martius 
Causal Action Influence Aware Counterfactual Data Augmentation
29 May 2024C2D388A74B0F96DDC53CAF31E8637939arXiv:2405.18917v1[cs.LG]
Offline data are both valuable and practical resources for teaching robots complex behaviors.Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution.However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships.We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions.By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping action-unaffected parts of the state-space between independent trajectories in the dataset.We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift.Videos, code and data are available at https: //sites.google.com/view/caiac.</p>
<p>Introduction</p>
<p>Offline learning offers the opportunity of leveraging plentiful amounts of prerecorded data in situations where environment interaction is costly (Bahl et al., 2022;Brohan et al., 2022;2023;Vlastelica et al., 2023).However, one of the fundamental challenges of such a framework is that of causal confusion.</p>
<p>Causal confusion arises when a trained agent misinterprets the underlying causal mechanics of the environment and, hence, fails to distinguish spurious correlations from causal Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).</p>
<p>Offline Dataset</p>
<p>Augmented Dataset</p>
<p>Causal Action</p>
<p>Influence Aware Data Augmentation + =</p>
<p>Figure 1: Overview of the proposed approach.Interactions between the agent and entities in the world are sparse.We use causal action influence (CAI), a local causal measure, to determine action-independent entities and create counterfactual data by swapping states of these entities from other observations in the dataset.Offline learning with these augmentations leads to better generalization.</p>
<p>links (De Haan et al., 2019;Gupta et al., 2023).When trying to reduce training loss, an agent can benefit from such spurious correlations in the data and, therefore, they can be inadvertently transferred to the mechanisms of learned models (Gupta et al., 2023).</p>
<p>Problematically, causally confused agents, are prone to catastrophic failure even in mild cases of distributional shift (De Haan et al., 2019), i.e. when the test distribution deviates from the training distribution.Subtle forms of distributional shift are common when learning from realworld data: collected demonstrations can only encompass a small fraction of the vast amount of possible configurations stemming from the inherent presence of many entities in the real world (Battaglia et al., 2018).Hence, at test times, agents are often queried on unfamiliar (i.e.out of distribution) configurations.</p>
<p>To illustrate the problem, let us imagine that we have demonstrations of a robot performing several kitchenrelated activities: opening a microwave, sliding cabinets and turning on and off a light switch.However, the demonstrations only show how to perform those activities in this pre-specified order.Hence, following the example, the sliding cabinet (Event Y) is only shown to be open when the microwave (Event X) is open as well.In this scenario, the trained agent happens to simultaneously observe independent events X and Y whenever it executes the sliding action (A), and hence may attribute the action A to X and Y occurring jointly, even though event X is independent of A. If the spurious correlation between X and Y observed during training fails to persist at test time, such a causally confused model may exhibit subpar performance.Namely, the agent might not be able to slide open the cabinet when the microwave is closed.</p>
<p>In contrast, humans are remarkably good at inferring what parts of the environment are relevant to solve a task, possibly due to relying on a causal representation of the world (Pearl &amp; Mackenzie, 2018).This hypothesis has motivated the creation of causal approaches in machine learning that aim to identify relationships in the environment that will remain invariant under changes in the data-generating distribution.</p>
<p>Existing work at the intersection between RL and causality has focused on an online learning (Lyle et al., 2021;Wang et al., 2022;Ding et al., 2023), imitation learning (De Haan et al., 2019) or partial observability setting (Forney et al., 2017;Kallus &amp; Zhou, 2018).When learning in the online setting, some works operate in the interventional setting (Lyle et al., 2021;De Haan et al., 2019), i.e. a user may be able to "experiment" in the environment in order to discover causal structures by assigning values to some subset of the variables of interest and observing the effects on the rest of the system.In contrast, we focus on the challenging offline setting, where the agent is not capable of observing the real effects of such an intervention, and we propose an observational approach.While one could also learn to predict the outcome of the interventions by using a model-based approach, we note that an uninformed dynamics model can still be sensitive to spurious correlations and suffer from approximating errors.</p>
<p>Our approach, which we refer to as Causal Influence Aware Counterfactual Data Augmentation (CAIAC), introduces counterfactual data augmentations without the need for additional environment interactions, or reliance on counterfactual model rollouts.Instead, we exploit collected data to learn a causal model that explicitly reasons about causal influence and swap locally causally independent factors across different observed trajectories.Estimating the entire causal structure remains, however, a challenging task, particularly if attempted from offline data.</p>
<p>Taking this into account, we focus on identifying the effects the agent has on the environment; we thus assume actioninfluence to be more important for policy learning than potential object-object interactions.By partially trading off generality, this inductive bias on the underlying causal structure reduces the problem of estimating the full causal structure to only measuring the influence of actions over objects.</p>
<p>Although causal discovery from observational data is known to be impossible for the general case (Pearl, 2009;Peters et al., 2017), methods exist that make use of some form of independence testing that have been successful in applications.</p>
<p>While other causal methods rely on heuristics to create new samples (Ding et al., 2023), on implicit measures for detecting causal influence among entities (Pitis et al., 2020) or on regularization of the dynamical models to suppress spurious correlations (Ding et al., 2023;Wang et al., 2021), our method is theoretically sound and relies on an explicit measure of influence, namely state-conditioned mutual information (Cover, 1999).We find this approach to be significantly more reliable at creating counterfactual samples that, not only follow the environment's dynamics (i.e. they are feasible), but also increase the support of the joint distribution over environment entities, as shown in Fig. 2.</p>
<p>Moreover, we show that by providing an offline learning agent with CAIAC's counterfactual samples, we prevent the agent from suffering from causal confusion, and we hence improve robustness to distributional shifts at test time.Our framework works as an independent module and can be used with any data-driven control algorithm.We demonstrate this through empirical results in high-dimensional offline goalconditioned tasks, applying our method to fundamentally different data distributions and learning methods.Namely, we couple our method with offline goal-conditioned skill learning on the Franka-Kitchen environment (Gupta et al., 2019), and classical offline goal-conditioned reinforcement learning on Fetch-Push and FetchPick&amp;Lift (Plappert et al., 2018).Across experiments, we show that CAIAC leads to enhanced performance in out-of-distribution settings and when learning from a modest amount of demonstrations.</p>
<p>Causal Graphical Models</p>
<p>Generally, a joint distribution P (X) has a particular independence structure which induces a specific factorization.This independence structure is a consequence of the functional relationship (also called mechanism) between the variables that can be accurately described through a Structural Causal Model (SCM).Definition 2.2 (SCM (Pearl, 2009)).A SCM is a tuple (U, V, F, P u ), where U is a set of exogenous (unobserved) variables (e.g. the unobserved source of stochasticity in the environment) sampled from P U , V is a set of endogenous (observed) variables (e.g. the observed state, the action and the reward in RL).F is the set of structural functions capturing the causal relations, such that functions f V : Pa(V ) × U → V with Pa(V ) ⊂ V denoting the set of parents of V , determine the value of endogenous variables V for each V ∈ V.</p>
<p>SCMs are usually visualized as a directed acyclic graph G whose nodes are associated with the variables in the SCM and whose edges indicate causal relationships.We say that a pair of variables v i and v j are confounded by a variable C (confounder) if they are both caused by C, i.e., C ∈ Pa(v i ) and C ∈ Pa(v j ).When two variables v i and v j do not have a direct causal link, they are still correlated if they are confounded, in which case this correlation is a spurious correlation.Given an SCM, one can make inferences about causal effects through the concept of an intervention.Definition 2.3 (do-intervention (Pearl, 2009)).An interven-
tion do(V = v) on V induces a new SCM = (U, V, F ′ , P u ),
where
F ′ = {f W ∈ F | W ̸ = V } ∪ {f V =v } and f V =v (p, u) = v ∀p ∈ Pa(V ), u ∈ U.
An intervention on a set of nodes of the SCM effectively changes their structural equations, mostly replacing them by an explicit value.Interventional queries of the form P (Y | do(X = x)) are the so-called second rung of causation (Pearl, 2009).In this work we are interested in the third, counterfactual queries.</p>
<p>Definition 2.4 (Counterfactual).A counterfactual query is a query of the form P (Y | do(X = x), U = u), where Y, X ⊂ V and U is the set of exogenous variables of the underlying SCM.</p>
<p>A counterfactual query about variables Y is asking what would have happened to Y if under the same conditions U = u the intervention do(X = x) had been performed.</p>
<p>Problem Definition</p>
<p>We assume a known and fixed state-space factorization S = S 1 × ... × S N for N entities, where each factor S i corresponds to the state of an entity.In practice, there are methods that allow to automatically determine the number of factors (Zaheer et al., 2017) and to learn latent representations of each entity (Burgess et al., 2019;Zadaianchuk et al., 2023;Locatello et al., 2020;Greff et al., 2019;Jiang et al., 2019;Seitzer et al., 2022).While we do not consider them for simplicity, our method can be applied on top of such techniques.</p>
<p>An MDP coupled with a policy π : S → A induces an SCM describing the resulting trajectory distribution.Given the Markovian property of the MDP and flow of time, there only exist direct causal links {S t , A t } → S t+1 by definition, i.e. S t+1 ⊥ ⊥ V | {S t , A t } for non-descendant nodes V / ∈ {S t , A t }.For our purposes, it suffices to look at the time slice sub-graph which is governed by the MDP transition kernel P between state S, action A and next state S ′ with state factors {S i } N i=1 .In most non-trivial environments, there exists an edge S i /A → S ′ j for most i, j (Fig. 3(a)).However, interactions often become sparse once we observe a particular state configuration.We capture these local interactions by the notion of a local causal model.Definition 3.1 (Local Causal Model (Pitis et al., 2020)).Given an SCM (U, V, F, P u ), the local SCM induced by observing V = v with V ⊂ V is the SCM with F V =v and the graph G do(V =v) resulting from removing edges from G do(V =v) until the graph is causally minimal.</p>
<p>We shall use shorthand G v for G do(V =v) and similar to reduce notational clutter.Where normally the vertex set {A} ∪ {S ′ j } N j=1 would be densely connected in the direction A → S ′ , intervening on S results in a sparser causal dependency in G s .An example of this local causal structure is given in Fig. 3(b): the robot can only influence the kettle and its own end-effector through its actions, but none of the other entities.We will make heavy use of sparsity and locality in constructing a counterfactual data augmentation.</p>
<p>Method</p>
<p>The main challenge that we aim to tackle in this work is that of learning policies in the offline regime that are more robust to distributional shift, i.e. are not susceptible to spurious correlations.We achieve this by augmenting real data with counterfactual modifications to causally action-unaffected entities, hence creating samples outside the support of the data distribution.Our method relies on the observation that performing the intervention do(S = s) reduces the number of edges between A and S ′ as per Definition 3.1, leaving some factors independent of A. Now, assuming that we observe the transition S = s, A = a, S ′ = s ′ we pose the question of how we can do counterfactual reasoning without access to the true set of structural equations F S=s , i.e. synthesize counterfactual transitions.</p>
<p>To this end, we need to learn the causal structure of the LCM, which is known to be a hard problem (Peters et al., 2017).Therefore, we make the key assumption that interactions between entities are sparse (i.e.only occur rarely) and are thus negligible.While the correctness of generated counterfactuals will rely on this assumption to hold, we argue that this is realistic in various robotics tasks.For example, in the kitchen environment depicted in Fig. 1, the entities can hardly influence each other.In fact, each entity is mostly controlled by the agent actions.This would also be the case in several manufacturing processes, in which interaction between entities should only happen under direct control of robots.Moreover, settings in which the assumption does not hold remain a significant challenge for most heuristic methods for causal discovery (Pitis et al., 2020), which underperform despite their generality.More formally, and in a graphical sense, we assume that there is no arrow S i → S ′ j , i ̸ = j as visualized by the gray dashed lines in Fig. 3(b).We note that only two groups of arrows remain in the causal graph: S j → S ′ j , which we assume to always be present, and A → S ′ j .This practical assumption allows us to reduce the hard problem of local causal discovery to the more approachable problem of local action influence detection.As a result, instead of resorting to heuristics (Pitis et al., 2020), we make use of a more principled explicit measure of local influence, the Causal Action Influence (CAI) (Seitzer et al., 2021) measure, which we introduce below.</p>
<p>Causal Action Influence Detection</p>
<p>To predict the existence of the edge A → S ′ j in the local causal graph G s , Seitzer et al. (2021) use conditional mutual information (CMI) (Cover, 1999) as a measure of dependence, which is zero if S ′ j ⊥ ⊥ A|S = s.Therefore, in each state S = s we use the point-wise CMI as a state-dependent quantity that measures causal action influence (CAI), given by
C j (s) := I(S ′ j ; A | S = s) = E a∼π D KL P S ′ j |S=s,A=a P S ′ j |S=s .
(1)</p>
<p>The transition model P S ′ j |S=s,A=a is modeled as a Gaussian neural network (predicting mean and variance) which maximizes a log-likelihood objective.The conditional distribution P S j t+1 |S=s is computed in practice using M empirical action samples with the full model: m) , a (m) ∼ π.The KL divergence in (1) can be estimated using an approximation for Gaussian mixtures from (Durrieu et al., 2012).We note that the transition model does not need to be queried autoregressively, which avoids the issue of compounding errors.We refer the reader to Seitzer et al. (2021) for more details.
P S ′ j |S=s ≈ 1 M M m=1 P S ′ j |S=s,A=a(</p>
<p>Inferring Local Factorization</p>
<p>Having introduced the concepts of locality and object independence, as well as a method to detect causal action influence, we proceed to infer the local factorization which will be leveraged to create counterfactual experience.For each state s in our data set D, we compute the uncontrollable set, as the set of entities in s for which the agent has no causal action influence, expressed as:
U s = {s j | C j (s) ≤ θ, j ∈ [1, N ]} (2)
where θ is a fixed threshold.The set U s contains all entities j for which the arrow A → S ′ j in the local causal graph G s does not exist.The remaining entities are contained in the set of controllable entities CR s = {s 1 , . . ., s N } \ U s .An illustration is given in Fig. 3(b).</p>
<p>With our assumptions and the sets U s and CR s we find that the local causal graph G s is divided into the disconnected subgraphs G CR s , that contains the entities in CR and A, and into
|U s | disconnected subgraphs G U si , i ∈ [1, |U s |],
each of which contains an entity in U s with only self-links, see Fig. 3(b).We can also compute the uncontrollable set for an extended time period κ, i.e. (I)
U st:t+κ = t+κ−1 τ =t U sτ .</p>
<p>Computing Counterfactuals</p>
<p>Given the partitioning of the graph described above, similarly to (Pitis et al., 2020), we can think of each subgraph as an independent causal mechanism that can be reasoned about separately.Assuming no unobserved exogenous variables, we may obtain counterfactuals in the following way: given two transitions (s, a, s ′ ) and (ŝ, â, ŝ′ ) ∈ D sampled for training, which have at least one uncontrollable subgraph structure in common (i.e.U s ∩ U ŝ ̸ = ∅), we generate a counterfactual transition (s, ã, s′ ) by swapping the entity transitions Our local causal graph (b) is pruned by causal action influence.Object-object interactions are assumed to be rare/not existing (gray dashed).We swap elements that are not under control (i.e. in set U) by samples from the data, thus creating counterfactual samples.We omit the exogenous variables from the global graph for compactness.
(s i , s ′ i ) with (ŝ i , ŝ′ i ) and i ∈ U s ∩ U ŝ. In
Algorithm 1: CAIAC
input Dataset D Compute uncontrollable set Us, ∀s ∈ D (Eq. 2). Sample (s, a, s ′ ) ∼ D and set (s, s′ ) ← (s, s ′ ) for si ∈ Us Sample (ŝ, â, ŝ′ ) ∼ D if ŝi ∈ U ŝ then (si, s′ i ) ← (ŝi, ŝ′ i ) yield (s, a, s ′ ) and (s, a, s′ )
this way, we observe the result of the counterfactual query P (S ′ | do(S = s, A = a)) without using the mechanism f S ′ if it remains unchanged in the new LCM.However, even when only swapping the entity transitions for U s ∩ U ŝ, the LCM resulting from the intervention do(S = s) may still contain a different mechanism f S ′ than the source LCM of s, meaning that the transition (s, ã, s′ ) becomes invalid.An additional check of causal influence would entice an outof-distribution query to the CAI measure, which is learned from transitions and therefore error-prone.In practice we do not perform this check and accept creating a small fraction of potentially infeasible transitions.The pseudocode of our method, which we call Causal Influence Aware Counterfactual Data Augmentation (CAIAC), is given in Algorithm 1.</p>
<p>Related work</p>
<p>Data Augmentation Data augmentation is a fundamental technique for achieving improved sample-efficiency and generalization to new environments, especially in highdimensional settings.In deep learning systems designed for computer vision, data augmentation can be found as early as in LeCun et al. (1998) and Krizhevsky et al. (2012), who leverage simple geometric transformations, such as random flips and crops.Naturally, a plethora of augmentation techniques (Berthelot et al., 2019;Sohn et al., 2020) have been proposed over time.To improve generalization in RL, domain randomization (Tobin et al., 2017;Pinto et al., 2017) is often used to transfer policies from simulation to the real world by utilizing diverse simulated experiences.Cobbe et al. (2019); Lee et al. (2019) showed that simple augmentation techniques, such as cutout and random convolution, can be useful to improve generalization in RL from images.Similarly to us, (Laskin et al., 2020) use data augmentation for RL without any auxiliary loss.Crucially, most data augmentations techniques in the literature require human knowledge to augment the data according to domainspecific invariances (e.g., through cropping, rotation, or color jittering), and mostly target the learning from image settings.Nevertheless, heuristics for data augmentation can be formally justified through a causal invariance assumption with respect to certain perturbation on the inputs.</p>
<p>Offline learning, distributional shift and causal confusion Offline RL and imitation learning methods rely on the availability of informative demonstrations (Lange et al., 2012;Li et al., 2023;Urpí et al., 2023;Lynch et al., 2019;Vlastelica et al., 2021b).However, in low-data regimes or task-agnostic demonstrations settings, these methods often suffer from distributional shift.This shift occurs when the agent induces a state-action distribution which deviates from the original data (Ross et al., 2011).Several offline learning methods have been proposed for fighting distributional shift, such as by minimizing deviation from the behavior policy (Fujimoto et al., 2019;Kumar et al., 2019;Kostrikov et al., 2021;Urpí et al., 2021) or minimizing risk (Vlastelica et al., 2021a).In imitation learning, several works focus on solving the causal confusion problem (De Haan et al., 2019), where a policy exploits nuisance correlates in the states for predicting expert actions (Wen et al., 2020;Seo et al., 2024).</p>
<p>Causal Reinforcement Learning Detecting causal influence involves causal discovery, which can be pictured as finding the existence of arrows in a causal graph (Pearl, 2009).While it remains an unsolved task in its broadest sense, there are assumptions that permit discovery in some settings (Peters et al., 2012;Spirtes et al., 2001).</p>
<p>Once the existence of an arrow can be detected, its impact needs to be established, for which several measures, such as transfer entropy or information flow, have been proposed (Schreiber, 2000;Lizier, 2012;Ay &amp; Polani, 2008).In our case, we use conditional mutual information (Cover, 1999) as a measure of causal action influence, as proposed by Seitzer et al. (2021).</p>
<p>The intersection of RL and causality has recently been studied to improve interpretability, sample efficiency, and to learn better representations (Buesing et al., 2018;Bareinboim et al., 2015;Lu et al., 2018;Rezende et al., 2020).Lyle et al. (2021) dealt with the problem of causal hypothesis-testing in the online setting via an exploration algorithm, Ding et al. (2023) utilize a model-based counterfactual approach (Pitis et al., 2022) for solving a robust MDP and Zhang et al. (2020) investigate how to obtain a reduced causal graph using a block structure.While Wang et al. (2022) also leverage influence to learn the causal structure, they use it for state abstraction to improve model generalization.Similarly to us, (Lu et al., 2020;Ding et al., 2022), also provide the agent with counterfactuals, but by learning and querying a model for the state transition.In particular, our work is related to that of Pitis et al. (2020), which proposes the Local Causal Model framework to generate counterfactual data, and underpins our work.However, Pitis et al. (2020) aim at estimating the entire local causal graph, which is a challenging problem.In practice, they rely on a heuristic method based on the attention weights of a transformer world model, which does not scale well to high-dimensional environments.In contrast, our method does not require learning the entire local causal graph, as it assumes that the interactions between entities (except the agent) are sparse enough to be neglected.This also implies that the agent is the only entity that can influence the rest of the entities through its actions.Therefore, this setting is related to the concept of contingency awareness from psychology (Watson, 1966), which was interestingly already considered in deep reinforcement learning methods for Atari (Song et al., 2020;Choi et al., 2018).</p>
<p>Experiments</p>
<p>We evaluate CAIAC in two goal-conditioned settings: offline RL and offline self-supervised skill learning.In particular, we are interested in evaluating whether CAIAC (i) leads to better robustness to extreme distributional shifts, (ii) enlarges the support of the joint distribution over the state space in low data regimes, and (iii) works as an independent module combinable with arbitrary learning-based control algorithms.</p>
<p>The set of benchmarks revolves around the issue of spurious correlations in the state space in manipulation domains, and is built upon the Franka-Kitchen (Gupta et al., 2019) and the Fetch (Plappert et al., 2018) platforms.The training data for each task includes spurious correlations that can usually appear during the data collection process, and could distract the policy from learning important features of the state.</p>
<p>Baselines We compare CAIAC with CODA (Pitis et al., 2020), a counterfactual data augmentation method, which uses the attention weights of a transformer model to estimate the local causal structure.Given two transitions that share local causal structures, it swaps the connected components to form new transitions.Additionally, we compare with an ablated version of CODA, CODA-ACTION, which only estimates the influences of the action using the transformer weights and thus is a 'heuristic'-sibling of our method.For the RL experiments, we also compare it with RSC (Ding et al., 2023), a framework for robust reinforcement learning that constructs new samples by perturbing the value of the states using an heuristic and learns a structural causal model to predict the next state given the perturbed state.As an ablation, we include a baseline without data augmentation (NO-AUGM).For completeness, we provide comparisons with model-based approaches in B.2.</p>
<p>Given our method and some of the baselines performances depend on an appropriate choice of the parameter θ to get a classification of influence, we provide a thorough analysis on how this parameter was chosen in Appendix A.5.</p>
<p>Tasks with Spurious Correlations</p>
<p>Our initial experiments investigate whether CAIAC can increase the generalization capabilities of algorithms when learning from demonstrations that include spurious correlation.Specifically we test whether the trained algorithms are robust to extreme state distributional shifts at test time.</p>
<p>GOAL CONDITIONED OFFLINE SELF-SUPERVISED SKILL LEARNING</p>
<p>We apply our method to the challenging Franka-Kitchen environment from Gupta et al. (2019).We make use of the data provided in the D4RL benchmark (Fu et al., 2020), which consists of a series of teleoperated sequences in which a 7-DoF robot arm manipulates different parts of the environment (e.g., it opens microwave, switches on the stove).Crucially, all demonstrations are limited to a few manipulation sequences (for example, first opening the microwave, turning on a burner, and finally the light).Thus, the support of the joint distribution over entities in the environment is reduced to only a few combinations.To illustrate this with an example, the light is only on when the microwave is open.At test time we evaluate the trained agent in unseen configurations, breaking the spurious correlations that exist in the training data.We hypothesize that CAIAC will create valid counterfactual data such that the downstream learning algorithms would be able to generalize to unseen state configurations.As a downstream learning algorithm we use LMP (Lynch et al., 2019), an offline goal-conditioned self-supervised learning algorithm, which learns to map similar behaviors (or state-action trajectories) into a latent space from which goal-conditioned plans can be sampled.Formally, LMP is a sequence-to-sequence VAE (Sohn et al., 2015;Bowman et al., 2015) autoencoding random experiences extracted from the dataset through a latent space.</p>
<p>In our case, we use experiences of fixed window length κ.</p>
<p>Given the inherent temporal abstraction of the algorithm, we generate counterfactuals of fixed length κ &gt; 1 by computing the uncontrollable set for the entire window as the intersection over all time slices, as in ((I)).For specific details on the learning algorithm and the Franka-Kitchen environment, we refer to A.1.1 and A.2.1 respectively.</p>
<p>Franka-Kitchen: A Motivating Experiment Our first experiment is designed to verify claim (i), i.e., that CA-IAC enables generalization to unseen configurations over entities.First, we showcase this in a simple and controlled environment.Thus, we create a reduced modified dataset from the original D4RL dataset (Fu et al., 2020), that contains only demonstrations for the microwave task (MW) and the kettle (K) task.During demonstrations for the (MW) task, we initialize the cabinet to be always open, whereas for demonstrations for the (K) task, it remains closed.The rest of the objects are set to the default initial configuration (see A.2.1).At inference time, we initialize the environment with its default initial configuration (crucially, the cabinet is closed), and we evaluate both tasks ((K) and (MW)), as shown in Fig. 4(left).Hence, while the (K) task was demonstrated for the current configuration (in-distribution, ID), the agent is effectively evaluated on an out-of-distribtion (OOD) configuration for the (MW) task.</p>
<p>We evaluate success rate on both tasks with CAIAC and all baselines, as shown in Fig. 4(right).All methods are able to solve the (K) task, as expected, since it is in-distribution (ID).However, we observe fundamentally different results for the OOD (MW) task.In principle, CAIAC can detect that the sliding cabinet is never under control of the agent, and will be able to create the relevant counterfactuals to prevent the policy from picking up on the spurious correlation in the data.Indeed, the performance of CAIAC in the OOD task (MW) is not affected, and it is the same as for the ID task.On the other hand, the performance of CODA and CODA-ACTION is drastically impaired in the OOD setting.Despite the simplicity of the setting, the input dimensionality of the problem is high, and the transformer attention weights are not able to recover the correct causal graph.By picking up on spurious correlations, the attention weights of the transformer estimate low influence from the action to all entities (even the agent), and hence CODA-ACTION creates dynamically-unfeasible counterfactuals which affect performance.Since the ratio of observed-counterfactuals data is 1:1 we hypothesize that there is enough in-distribution data to not affect the (K) task for CODA-ACTION.The local graph induced by CODA has at least as many edges as the one of CODA-ACTION, and hence the probability for creating unfeasible counterfactuals is lower.We hypothesize, that despite not learning correct causal influence, it might still provide some samples which benefit the learning algorithm and allow for an average OOD success rate of 0.2.We refer the reader to Appendix A.3 for further analysis on the impact of the ratio of observed:counterfactual data for this experiment.Finally, as expected, NO AUGM.fails to solve the OOD (MW) task.</p>
<p>Franka-Kitchen: All Tasks Having evaluated CAIAC in a controlled setting, we now scale up the problem to the entire Franka-Kitchen D4RL dataset.While in the standard benchmark the agent is required to execute a single fixed sequence of tasks, we train a goal-conditioned agent and evaluate on the full range of tasks, which include the microwave, the kettle, the slider, the hinge cabinet, the light switch and the bottom left burner tasks (Mendonca et al., 2021).One task is sampled for each evaluation episode.While alleviating the need for long-horizon planning, this results in a challenging setting, as only a subset of tasks is shown directly from the initial configuration.However, the largest challenge in our evaluation protocol lies in the creation of unobserved state configurations at inference time.</p>
<p>While the provided demonstrations always start from the same configuration (e.g., the microwave is always initialized as closed), at inference time, we initialize all non-target entities (with p = 0.5) to a random state, hence exposing the agent to OOD states.We expect that agents trained with 0.01 ± 0.01 0.0 ± 0.0 0.0 ± 0.0 0.00 ± 0.0 Hinge cabinet 0.0 ± 0.0 0.0 ± 0.0 0.0 ± 0.0 0.0 ± 0.0 CAIAC will show improved performance to unseen environment configurations, as those can be synthesized through counterfactual data augmentation.The results, shown in Table 1, are consistent with the challenging nature of this benchmark, as the evaluated tasks involve OOD settings in terms of states and actions.Nevertheless, we find that CAIAC is significantly better than baselines in 6/7 tasks, while the last task is unsolved by methods.We hypothesize that the low performance on some of the tasks is due to the absence of robot state and action trajectories in the dataset that show how to solve each of the tasks from the initial robot joint configuration.Hence, even with perfect counterfactual data augmentation these tasks remain challenging.We refer the reader to the Appendix A.2.1 for further analysis.As observed in the simplified setting, methods relying on heuristic-based causal discovery (CODA and CODA-ACTION) suffer from misestimation of causal influence, and thus from the creation of dynamically-unfeasible training samples.See A.7 for a further analysis on the quality of the created counterfactuals and Fig. 7 for a visualization of the computed CAI scores per each entity on one of the demonstrations for the Franka-Kitchen dataset.Finally, without any data augmentation, the learning algorithm i.e.NO AUGM.baseline) fails to perform the OOD tasks.</p>
<p>GOAL-CONDITIONED OFFLINE RL</p>
<p>Fetch-Pick&amp;Lift with 4 cubes We additionally test CA-IAC on Fetch-Pick&amp;Lift, a modified version of the Fetch-Pick&amp;Place environment (Plappert et al., 2018) were a robot needs to pick and lift a desired cube out of 4 arranged on a table (Fig. 5 (left)).For this benchmark, we include spurious correlations in the training data by always arranging the cubes in a line.At test time, the cubes are randomly positioned on the table, evaluating the agent in out of distribution states.We collect 40k trajectories using an expert policy (50%) and random policy (50%) and train an agent offline using TD3+BC (Fujimoto &amp; Gu, 2021).</p>
<p>Results are shown in Fig. 5 (left).We observe that CAIAC reaches a high success rate by creating relevant counterfactuals that augment the support of the joint distribution over entities and break the spurious correlations in the data.Conversely, the rest of the baselines exhibit subpar performance.</p>
<p>Once again, for CODA and CODA-ACTION, the attention weights of the transformer fail to recover the correct causal graph, resulting in the generation of infeasible samples.On the other hand, the causally uniformed heuristics used to perturb the states in RSC, might break the true cause and effect relationships between state dimensions, leading to performance drop, as reported in (Ding et al., 2023) Fetch-Push with 2 cubes We evaluate CAIAC in a Fetch-Push environment (Plappert et al., 2018), where a robotic arm has to slide two blocks to target locations.For this experiment we collect 20k trajectories using an expert policy (30%) and random policy (70%) and train an agent offline using TD3 (Fujimoto et al., 2018) in two data regimes: namely 100% and 20% of data.</p>
<p>More details are given in Appendices A.1.2,A.2.2 and A.5.We compare success rates between baselines and CAIAC among different data regimes in Fig. 5 (right).</p>
<p>In the high data regime, CAIAC and NO AUGM.baseline perform similarly given that there is enough coverage of the state space in the original dataset.In contrast, in the low data regime CAIAC performs significantly better.Given that the samples in the data, cover sufficient support of the marginal distribution of each entity, CAIAC can substantially increase the support of the joint distribution over entities, leading to higher performance.Transformer-based methods CODA and CODA-ACTION, and RSC create detrimental counterfactuals in all data regimes leading to decreased performance.To showcase the previous claims, the estimated influence scores for all the methods are visualized in A.4.We note that, while previous work (Pitis et al., 2020) has shown good online performance of CODA in this environment, it resorted to a handcrafted heuristic to decide about influence.</p>
<p>Discussion</p>
<p>While extracting complex behaviors from pre-collected datasets is a promising direction for robotics, data scarcity remains a principal issue in high-dimensional, multi-object settings, due to a combinatorial explosion of possible state configurations which cannot be covered densely by demonstrations.Hence, current learning methods often pick up on spurious correlations and struggle to generalize to unseen configurations.In this paper, we propose CAIAC as a method for counterfactual data augmentation without the need for additional environment interaction or counterfactual model rollouts, which can be used with any learning algorithm.By adding an inductive bias on the causal structure of the graph, we circumvent the problem of full causal discovery and reduce it to the computation of an explicit measure of the agent's causal action influence over objects.Empirically, we show that CAIAC leads to enhanced performance and generalization to unseen configurations, suggesting that further advances in addressing both partial and full causal discovery problems can be substantially beneficial for robot learning.While our current approach deems action influence to be more important than object-object interaction, in future work, we aim to explore alternative forms of independence to make our approach applicable to a wider range of tasks.Finally, we would like to further investigate rebalancing the data distribution to counteract data imbalances in the dataset.</p>
<p>Reproducibility Statement</p>
<p>In order to ensure reproducibility of our results, we make our codebase publicly available at https://sites.google.com/view/caiac, and provide detailed instructions for training and evaluating the proposed method.Furthermore, we describe algorithms and implementation details in Appendix A. Finally, as our experiments rely on offline datasets, we publish them at the same link.</p>
<p>A. Appendix</p>
<p>A.1. Implementation of downstream learning algorithms</p>
<p>In this section, we report implementation details concerning the learning algorithms.For a fine-grained description of all hyperparameters, we refer to our codebase at https://sites.google.com/view/caiac.</p>
<p>A.1.1. GOAL-CONDITIONED OFFLINE SELF-SUPERVISED SKILL LEARNING</p>
<p>For the goal-conditioned self-supervised learning experiments we used LMP (Lynch et al., 2019), a goal-conditioned self-supervised method.It consists of a stochastic sequence encoder, or learned posterior, which maps a sequence τ to a distribution in latent plan space q(z|τ ), a stochastic encoder or learned goal-conditioned prior p(z|s, g) and a decoder or plan and goal conditioned policy: π(a|z, s, g).The self-supervised goals g are relabeled from achieved goals in the trajectory.For counterfactual samples, trajectories are augmented before goal sampling.The main difference with the original implementation is that the latent goal representation is only added to the prior, but not the decoder.Additionally, we also implemented KL balancing in the loss term between the learned prior and the posterior: we minimize the KL-loss faster with respect to the prior than the posterior.Given that the KL-loss is bidirectional, in the beginning of training, we want to avoid regularizing the plans generated by the posterior towards a poorly trained prior.Hence, we use different learning rates, α = 0.8 for the prior and 1 − α for the posterior, similar to Hafner et al. (2020).These two modifications were also suggested in (Rosete-Beas et al., 2022).Additionally, our decoder was open-loop (instead of close loop): given a sampled latent plan z it decodes the whole trajectory of length skill length= N , i.e. our decoder is π(â|z), where â = a t , ..., a t+N is the sequence of decoded actions, instead of π(a|z, s, g).This modification was needed due to the skewness of the dataset.Since the demonstrations were provided from an expert agent, given most of the states, the distribution over actions is unimodal: when the robot is close to the microwave, the only sequence of actions in the dataset is the one that opens the microwave.Hence, a close-loop decoder would learn to ignore the latent plan, and only rely on the state.To solve this issue, we make the decoder open-loop.</p>
<p>A.1.2. GOAL-CONDITIONED OFFLINE RL: TD3</p>
<p>We implement the TD3 algorithm (Fujimoto et al., 2018) with HER (Andrychowicz et al., 2017).Unless specified differently, the hyperparameters used were the ones from the original TD3 implementation.We use HER (Andrychowicz et al., 2017) to relabel the goals for real data, with a future relabeling strategy with p = 0.5, where the time points were sampled from a geometric distribution with p geom = 0.2.For the counterfactual data we relabel the goals with p = 0.5 random sampling from the achieved goals in the buffer of counterfactual samples.In the experiments, we realized that the relabeling strategy had an impact on the performance of the downstream agent.To disentangle the impact of the relabeling strategy from the impact of the counterfactual data generation and to ensure a fair comparison, we also relabeled the same percentage of goals (i.e.p = 0.25) with random strategy for the No Augm.baseline.We train each method for 1.2M gradient steps, although all methods reach convergence after 600k gradient steps.For all baselines, the percentage of counterfactuals in each batch is set to 0.5.</p>
<p>A.1.3. GOAL-CONDITIONED OFFLINE RL: TD3+BC</p>
<p>We implement the TD3+BC algorithm (Fujimoto &amp; Gu, 2021) with HER (Andrychowicz et al., 2017) where a weighted behavior cloning loss is added to the policy update.After tuning, we used α BC = 2.5 (α BC → 1 recovers Behavior Cloning, while α BC → 0 recovers RL).Unless specified differently, the rest of hyperparameters used were the ones from the original TD3 implementation.We use HER (Andrychowicz et al., 2017) to relabel the goals for real data, with a future relabeling strategy with p = 0.5, where the time points were sampled from a geometric distribution with p geom = 0.2 and For the counterfactual data we relabel the goals with p = 1 with future strategy after augmenting the samples.For the No Augm.we also relabeled goals with random strategy with p = 0.25.We train each method for 120k gradient steps, although all methods reach convergence after 90k gradient steps.For all baselines, the percentage of counterfactuals in each batch is set to 0.9 unless specified.</p>
<p>A.2. Experimental details</p>
<p>A.2.1. FRANKA-KITCHEN</p>
<p>We use the kitchen environment from the D4RL benchmark (Fu et al., 2020) which was originally published by Gupta et al. (2019).The D4RL dataset contains different dataset versions: kitchen-complete, kitchen-partial, kitchen-mixed, which contain 3690, 136950 and 136950 samples respectively, making up to approximately 14 demonstrations for kitchen-complete and 400 demonstrations for each kitchen-partial and kitchen-mixed.</p>
<p>The simulation starts with all of the joint position actuators of the Franka robot set to zero.The doors of the microwave and cabinets are closed, the burners turned off, and the light switch also off.The kettle will be placed in the bottom left burner.The observation are 51-dimensional, containing the joint positions of the robot (9 dim), the positions of the all the kitchen items (21 dim) and the goal positions of all the items (21 dim).The length of the episode is 280 steps, but the episode will finish earlier if the task is completed.The task is only considered solved when all the objects are within a norm threshold of 0.3 with respect to the goal configuration.While in the standard benchmark the agent is required to execute a single fixed sequence of tasks, we train a goal-conditioned agent, and evaluate on one task per each evaluation episode.For the Franka-Kitchen motivating example (see 6.1.1)we query for either the kettle or the microwave task, in the Franka-Kitchen: All tasks (see 6.1.1)we query for the full range of tasks, which include the microwave, the kettle, the slider, the hinge cabinet, the light switch and the bottom left burner tasks.While alleviating the need for long-horizon planning, this results in a challenging setting, as only a subset of tasks is shown directly from the initial configuration.Specifically out of the 1200 demonstrations in the dataset, containing different task sequences, only 3 objects are shown to be manipulated from the initial robot configuration: 60% of the trajectories solve the microwave task first, 30% show the kettle task first and 10% show the bottom burner first.This aligns with the relative performance achieved for those tasks.For the 3 remaining tasks, namely the slide cabinet, the light and the hinge cabinet, there is no demonstration shown directly from the initial configuration and hence the low performance.</p>
<p>Franka-Kitchen: Motivating Experiment For the first experiment (see Subsection 6.1.1),we modify the dataset version kitchen-mixed to only contain ∼ 50 demonstrations of length ∼ 40 timesteps for each (mw) and (k) task.During demonstrations for the (mw) task, we initialize the cabinet to be always open, whereas for demonstrations for the (k) task, it remains closed.The rest of the objects are set to the default initial configuration.The goal configuration for all the objects was set to their initial configuration (as defined above), except for the microwave or the kettle, which were set to the default goal configuration when querying for the (mw) and (k) tasks respectively.</p>
<p>Franka-Kitchen: All Tasks For the second experiment (see Subsection 6.1.1)we merge the 3 provided datasets kitchen-complete, kitchen-partial, kitchen-mixed.For this experiment, each object (except the one related to the task at hand to ensure non-trivial completion), was randomly initialized with p = 0.5, otherwise it was initialized to the default initial configuration (as defined above).We then modify the desired goal to match the initial configuration for all non-target entities.</p>
<p>A.2.2. FETCH-PUSH WITH 2 CUBES</p>
<p>Expert data for the experiment in Subsection 6.2.1 includes 6000 episodes collected by an agent trained online using TD3 and HER up to approximately 95% success rate.We additionally collect 14000 episodes with a random agent, which make up for the random dataset.This sums up to a total of 20000 episodes (each of length 100 timesteps), with 30% expert data and 70% random data.Initial positions and goal positions of the cubes are sampled randomly on the table, whereas the robot is initialized in the center of the table with some additional initial random noise .The rewards are sparse, giving a reward of −1 for all timesteps, except a reward of 0 when the position of each of the 2 blocks are within a 2−norm threshold of 0.05.The observation space is 34-dimensional, containing the position and velocity of the end effector (6dim), of the gripper (4dim) and the object pose, linear and rotational velocities of the objects (12dim each).In contrast to the original Fetch-Push-v1 (Plappert et al., 2018) environment and similarly to Pitis et al. (2020) we do not include parts of the state space accounting for relative position or velocities of the object with respect to the gripper, which would entangle the two.The goal is 6-dimensional encoding the position for each of the objects.The action space is 4-dimensional encoding for the end-effector position and griper state.At test time, we count the episode as successful upon reaching the goal configuration (i.e., observing a non-negative reward).A.2.3.FETCH-PICK&amp;LIFT WITH 4 CUBES Expert data for the experiment in Subsection 6.1.2includes 20000 episodes collected by an agent trained online using TD3 and HER up to approximately 95% success rate.We additionally collect 20000 episodes with a random agent, which make up for the random dataset.This sums up to a total of 40000 episodes (each of length 50 timesteps), with same percentage of expert and random data.During data collection, we initialise all the 4 cubes aligned (i.e.all having the same x-axis position).</p>
<p>In each episode, we initialise this value from a categorical distribution with 5 options.During testing, the initialization process is the same, but it is done independently for each entity.As a result, the cubes are no longer aligned.The robot is initialized in the center of the table with some additional initial random noise .The rewards are sparse, giving a reward of −1 for all timesteps, except a reward of 0 when the position of each all the cubes are within a 2−norm threshold of 0.05.The observation space is 58-dimensional, containing the position and velocity of the end effector (6-dimensional), of the gripper (4-dimensional) and the object pose, linear and rotational velocities of the objects (12 dimensions each).In contrast to the original Fetch-Push-v1 (Plappert et al., 2018) environment and similarly to Pitis et al. (2020) we do not include parts of the state space accounting for relative position or velocities of the object with respect to the gripper, which would entangle the two.The goal is 12-dimensional encoding the position for each of the objects.The action space is 4-dimensional encoding for the end-effector position and griper state.At test time, we count the episode as successful upon reaching the goal configuration (i.e., observing a non-negative reward).</p>
<p>A.3. Ablation: Ratio of observed-to-counterfactual Data</p>
<p>In this section, we study the effect of the ratio of observed-to-counterfactual data generated with CAIAC, by evaluating downstream performance on the Franka-Kitchen motivating example, as presented in 6.1.1.Empirical results for this ablation are shown in Fig. 6.As expected, we observe that the ratio of counterfactuals does not have any significant impact on the success rate on the (k) task.This is because the task is evaluated in distribution, and hence the downstream learning algorithm does not require observing counterfactual experience (but still does not suffer from it).For the OOD (mw) task we see that increasing the number of counterfactuals up to a 0.9 ratio has a positive effect in performance, leading the agent to generalize better to the OOD distribution.However, when the ratio is increased up to 1, we only use synthesized counterfactual data.We observe a decrease in performance with high variance among training seeds.This hard ablation shows the need for real data during training to avoid induced selection bias, as also observed in (Pitis et al., 2020).With a ratio 0.0, we recover the performance of the No Augm.baseline.</p>
<p>A.4. Details on Influence Detection Evaluation</p>
<p>To detect causal action influence we use CAI, as described in 4.1.
C j (s) := I(S ′ j ; A | S = s) = E a∼π D KL P S ′ j |s,a P S ′ j |s .(3)
This requires learning the transition model P S ′ j |s,a .In the case of robotic manipulation environments physical contact is not needed for causal action influence as long as the agent can change the object pose, even if indirectly, in a single simulation step.</p>
<p>World model training For the Franka-Kitchen experiments all models were trained to predict the full state of the environment.For increased performance in the Fetch-Push task, all models were trained to predict the next position of the end effector of the agent gripper and of the objects (3 dimensions each).For CAIAC the transition model P S ′ j |s,a is modeled as a Gaussian neural network (predicting mean and variance) that is fitted to the training data D using negative log likelihood.We used a simple multi-layer perceptron (MLP) with two separate output layers for mean and variance.To constrain the variance to positive range, the variance output of the MLP is processed by a softplus function (given by log(1 + exp(x))), and a small positive constant of 10 −8 was added to prevent instabilities near zero.We also clip the variance to a maximum value of 200.For weight initialization, orthogonal initialization is used.For the Franka-Kitchen we use larger MLPS, with 3 layers for the simplified and 4 layers for the full experiment, each with 256 units and a learning rate of 8e −4 .</p>
<p>For CODA and CODA-action we use a self-implementation of the transformer model.We use a model with 3 layers and 4 attention heads for the Fetch-Push task and 5 layers and 4 heads for all the Franka-Kitchen tasks, with an embedding space and output space of 128 dimensions each.We also used a learning rate of 8e −4 .</p>
<p>All models were trained for 100k gradient steps, and tested to reach low MSE error for the predictions in the validation set (train-validation split of 0.9-0.1).We trained all models using the Adam optimizer (Kingma &amp; Ba, 2014), with default hyperparameters.</p>
<p>In general, the models were trained using the same data as for the downstream task for all experiments.However, for the Franka-Kitchen task, we add some additional collected data on the environment when acting with random actions.The reason is that, in order to compute CAI, we query the model on randomly sampled actions from the action space.Due to the expert nature of the kitchen dataset comes from an informed agent, the original dataset might lack random samples and hence we would query the model OOD when computing CAI.For both experiments in the Franka-Kitchen simplified experiment we added 1x the original dataset of random data.Further experiments on the impact of the amount of random data could be beneficial.This was not needed for the Fetch-Push task since the dataset already contains random action.We note that this additional data is also provided for training all transformer-based baselines.</p>
<p>CAI scores</p>
<p>In practice, we compute the CAI scores using the estimator:
C j (s) = 1 K K i=1 <a href="4">D KL p(s ′ j | s, a (i) || 1 K K k=1 p(s ′ j | s, a (k) </a>
with a ∼ π, where π(A) := U(A) (i.e. a uniform distribution over the action space) and with K = 64 actions.We refer the reader to (Seitzer et al., 2021) for more details.In Fig. 7 we show the computed CAI scores over a demonstration in the Franka-Kitchen and in Fig. 8 ( left) over an episode for the Fetch-Push task.Given the scores we need a threshold θ to get a classification of control (see Equation 2).For Fetch-Push we set θ = 0.05, for Fetch-Pick&amp;Lift we set θ = 2.0 and for Franka-Kitchen θ = 0.2 .See section A.5 for a thorough analysis on the impact of the influence threshold θ and how the parameter was chosen for each of the methods.</p>
<p>Transformer scores To compute causal influence, baselines use the attention weights of a Transformer, where the score is computed as follows.Letting A i denote the attention matrix of the i'th of N layers, the total attention matrix is computed as
N i=1 A i .
For CoDA the score is computed by checking the corresponding row i and column j for the check s i → s ′ j , whereas for CoDA-action we restrict ourselves to the row corresponding to the input position of the action component, and the output position of the object component.Our implementation follows Pitis et al. (2020), to which we refer for more details.In Fig. 8 (right) and Fig. 9, we show respectively the computed CoDA-action and CoDA scores over an episode for the Fetch-Push task.Given the scores we need a threshold θ to get a classification of control.For Fetch-Push we set θ = 0.2 for CODA and CODA-action, for Fetch-Pick&amp;Lift we set θ = 0.2 for CODA-action and θ = 0.15 for CODA.For Franka-Kitchen we set θ = 0.3 for all methods.See section A.5 for a thorough analysis on the impact of the influence threshold θ and how the parameter was chosen for each of the methods.</p>
<p>A.5. Analysis on Influence Threshold</p>
<p>To get a classification of control, we optimize the value for the threshold θ for all methods.We train 10 different world models for each method and we run a grid search over the parameter θ.We run 3 seeds for each of the 10 models and we picked the value for θ that optimizes the downstream task average performance among the 10 models and the 3 seeds.In Figure Fig. 10 we plot the ROC curves for correct action influence detection for both CAIAC and CoDA for the Fetch-Push task.In such an environment, one can specify a heuristic of influence using domain knowledge, namely the agent does not     The element i, j in the matrix shows the attention (or influence) s i → s ′ j .We see how the transformer completely fails on discovering the full causal graph, being even unable to recover influence along the diagonal, i.e., that an entity state at time t influences the entity state at time t+1.</p>
<p>In 3 we show how CAIAC increases the support of the original state space distribution by almost a factor of 2 .However, we can see that rest of the baselines also increase it significantly.Importantly, this numbers do not inform on whether these counterfactuals are actually valid, i.e. follow the true transition kernel of the environment, which was already investigated in the above Feasibility paragraph.</p>
<p>B. Implementation Details for Baselines</p>
<p>In the following we provide implementation details for RSC and additional model-based baselines.Details regarding CAIAC and CoDA and CoDA-action baselines were already provided in the respective sections above.</p>
<p>B.1.RSC (Ding et al., 2023) Due to lack of public code available by the time of the submission, we resort to reimplementing RSC (Ding et al., 2023).The implementation follows the guidelines described in the original paper, with a few differences.In the original method, the dynamics model leverages a learned causal graph as a gating mechanism to achieve better generalization to perturbed state and action pairs; training is regularized to encourage sparsity in this graph.In our case, we instead leverage a transformer dynamics model, in which sparsity is granted by the softmax in its attention mechanism.Additionally, while the dynamics model in RSC is trained to predict both rewards and next states, in our case predicting the next state is sufficient, as the ground truth reward can be computed with the relabeling function available in goal-conditioned settings.</p>
<p>A final difference that ensures a fair comparison is in the perturbation phase.The original description of RSC is not object-centric, and thus only perturbs a single dimension of the state space.In our experiments, we ensure that RSC also leverages the known decomposition of the state space in objects, and thus augment the state of a single object in a consistent manner.</p>
<p>Finally, a naive maximization of the heuristic (Equation 7in Ding et al. (2023))over the entire dataset D and K objects would require evaluating K|D| 2 distances.Due to compute constraints, we optimize it by subsampling 1000 candidate solutions, which we found to still approximate the maximum reasonably well.</p>
<p>B.2. Model based baselines</p>
<p>We present a performance comparison between CAIAC and an MBPO-style causal-unaware model-based approach, which we call MBPO for simplicity.</p>
<p>MBPO leverages a dynamical model trained from data to generate on-policy imagined N-step transition rollouts.Despite the ease of data generation, these methods often suffer from bias of model-generated data, which leads to approximation errors that compound with increasing the horizon (N).</p>
<p>In our spurious correlation experiments, we would argue that it is hard for the model to accurately autoregressively predict long horizons trajectories, that would lead the agent to OOD states useful to solve the task at hand.Additionally, if that would be the case, then a causal-agnostic model would be queried OOD suffering leading to poor generalization.Work on the direction for task independent state abstraction tackles this issue (Wang et al., 2022).</p>
<p>We show the results for the reinforcement learning experiments (namely FetchPush and Fetch-Pick&amp;Lift) in Fig. 13.</p>
<p>For the spurious correlation environment Fig. 13 (left), we indeed confirm our hypothesis, were MBPO is unable to create samples that break the spurious correlation in the data, and hence leads to poor performance compared to CAIAC.</p>
<p>However, in the low data regime environments Fig. 13 (right), we observe how MBPO thrives and outperforms CAIAC.We hypothesize that in this environment, the trained model is accurate enough even in low data regimes, and hence can generate useful trajectory rollouts that increase the amount of data substantially.Since spurious correlations are not present in the Table 3: Support of the empirical joint state space distribution using different augmentation methods (results computed over 1000 augmented samples).Values represent ratios between the estimated support and the theoretical maximum.</p>
<p>Figure 2 :
2
Figure 2: CAIAC counterfactual samples are consistent with the environment's dynamics and increase the support of the joint state space distribution, enabling the agent to be robust to distributional shift.Left: Log-likelihoods under the environment transition kernel of counterfactuals created with different methods.Right: Original data and counterfactuals augmentations with CAIAC visualized with t-SNE.Details on this evaluation are reported in Appendix A.7.</p>
<p>Figure 3 :
3
Figure 3: Illustration of counterfactual data augmentation.The global causal graph does not allow for factorization (a).Our local causal graph (b) is pruned by causal action influence.Object-object interactions are assumed to be rare/not existing (gray dashed).We swap elements that are not under control (i.e. in set U) by samples from the data, thus creating counterfactual samples.We omit the exogenous variables from the global graph for compactness.</p>
<p>Figure 4 :
4
Figure 4: Motivating Franka-Kitchen example.The experimental setup (left) and success rates for in-distribution and out-of-distribution tasks (right).Metrics are averaged over 10 seeds and 10 episodes per task, with 95% simple bootstrap confidence intervals.</p>
<p>FetchFigure 5 :
5
Figure 5: Success rates for Fetch-Pick&amp;Lift with 4 objects (left) and Fetch-Push with 2 cubes (right).Metrics are averaged over 30 seeds and 50 episodes with 95% simple bootstrap confidence intervals.</p>
<p>Figure 6 :
6
Figure 6: Performance of CAIAC on motivating Franka-Kitchen example when controlling the percentage of counterfactual samples in each batch.Metrics are averaged over 10 seeds and 10 episodes per task, with 95% simple bootstrap confidence intervals.</p>
<p>Figure 8 :
8
Figure8: Top left: Computed CAI scores per object on one of the expert demonstrations for the Fetch-Push dataset.We can observe how the influence of the agent's actions over objects changes over time.First pushing object 1, then object 2 and object 1 again.In green we show the optimized threshold θ = 0.05 for this task.Bottom left and right: We show distance from the robot end-effector to each of the objects as a domain knowledge heuristic for action influence.In green we show the heuristic distance of 7cm that we use as a threshold to consider the agent can influence the object within the next timestep.Top right: Computed CoDA-action scores on the same episode as above.In green we show the optimized threshold θ = 0.2 for this task.</p>
<p>Figure 9 :
9
Figure 9: Computed CoDA scores on the same episode as in Fig.8.We show snapshots of the attention weights of the transformer every 5 time steps as computed by the CoDA algorithm.The element i, j in the matrix shows the attention (or influence) s i → s ′ j .We see how the transformer completely fails on discovering the full causal graph, being even unable to recover influence along the diagonal, i.e., that an entity state at time t influences the entity state at time t+1.</p>
<p>Table 1 :
1
Average success rates for Franka-Kitchen tasks with OOD initial configurations, computed over 10 seeds and 20 episodes per task with 90% simple bootstrap confidence intervals.
AlgorithmCAIACCoDACoDA-action No-Augm.Kettle0.81 ± 0.07 0.18 ± 0.05 0.16 ± 0.07 0.07 ± 0.06Microwave0.75 ± 0.09 0.07 ± 0.050.0 ± 0.030.01 ± 0.03Bottom-burner 0.13 ± 0.05 0.01 ± 0.030.0 ± 0.020.01 ± 0.02Slide cabinet0.14 ± 0.04 0.1 ± 0.030.02 ± 0.02 0.07 ± 0.03Light switch</p>
<p>CAIAC CoDA CoDA-action No Augm.
Support0.520.480.420.3
Department of Computer Science, ETH Zurich, Zurich, Switzerland
Max Planck Institute for Intelligent Systems, Tübingen,
Germany 3 Department of Computer Science, University of Tübingen, Tübingen, Germany. Correspondence to: Núria Armengol Urpí <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#110;&#117;&#114;&#105;&#97;&#97;&#64;&#101;&#116;&#104;&#122;&#46;&#99;&#104;">&#110;&#117;&#114;&#105;&#97;&#97;&#64;&#101;&#116;&#104;&#122;&#46;&#99;&#104;</a>.
AcknowledgementsThe authors thank Cansu Sancaktar and Max Seitzer for their help reviewing the manuscript and the annonymous reviewers for their valuable feedback.The authors thank the Max Planck ETH Center for Learning Systems for supporting Núria Armengol and Marco Bagatella, and the International Max Planck Research School for Intelligent Systems for supporting Marin Vlastelica.Georg Martius is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC number 2064/1 -Project number 390727645.This work was supported by the ERC -101045454 REAL-RL.Impact StatementThis paper aims to push the boundaries of generalization in machine learning.Hence, it shares the many societal consequences tied to the field of machine learning, from ethical to environmental consequences.Figure10: ROC curves for CAIAC and CoDA-action (averaged over 10 trained world models and 1 standard deviation shaded).We show measures true and false positive rates (TPR and FPR) while sweeping the influence threshold θ.In black we show the corresponding TPR and FPR for the optimal θ for both methods.See alsoFigs. 11have influence on the object if 7cm apart.An accurate model generates an Area Under the Curve (AUC) close to 1, while a random model stays along the diagonal.In Fig.10we can observe that the attention weights of the transformer world model are not accurate for detecting influence.Additionally, there is a high variability on the different trained transformers (see also Fig.12) , making it hard to optimize for the threshold θ for this type of model architecture.In contrast, we see that ROC curves for CAI have an AUC ≈ 0.9 and hence it is an accurate measure for predicting influence in the Fetch-Push environment.Additionally, given its low variance across training seeds (see also Fig.11), same thresholds reach the same TPR/FPR across models, making it easy to optimize for θ.A.6.Computational Demands CAIAC relies on computing the CAI measure for data augmentation.In turn, CAI can be evaluated for all entities at once, through k forward passes for the k counterfactual actions, which are performed in a batch-wise fashion.k is a constant factor, and does not scale with the number of entities.Methods relying on a transformer world model, like CoDA and CoDA-action only need one forward pass (which internally has quadratic cost in the number of entities due to cross attention).However, CoDA also needs to compute the connected components from the adjacency matrix, which has a quadratic cost.For relatively few entities, as is common in the robotic manipulation environments, the computational overhead is relatively small.Table2reports an evaluation for the high data regime in the Fetch-Push environment, in which each method was timed while computing influence on all 2M datapoints.The algorithms were benchmarked on a 12-core Intel i7 CPU.We note that counterfactuals could be generated in parallel to the learning algorithm and hence not significantly impact runtime of the algorithm.Furthermore, in our offline setting, counterfactuals can be fully precomputed.Figure12: ROC curves for all the 10 trained world using CoDA-action.In black we show the corresponding true positive and false positive rate for the optimal threshold θ = 0.2.A.7. Analysis on the Quality of Created CounterfactualsWe provide an analysis on the created counterfactuals using CAIAC and several baselines, which investigates whether each method (i) creates feasible samples (i.e. in accordance to the true transition kernel of the environment) and (ii) increases the support of the joint state space distribution in the training data.Feasibility The procedure estimating whether the augmentation procedure is valid is as follows.A set of N trajectories (s t , a t , . . ., a t+τ −1 , s t+τ ) sampled from the Franka-Kitchen dataset is considered.Counterfactuals trajectories (s t , a t , . . ., a t+τ −1 , st+τ ) are created for each original trajectory using each method.We then leverage access to the environment's simulator, reset it to the initial counterfactual state st , and act on the environment by apply actions a t , . . ., a t+τ −1 .If the counterfactual is valid, the resulting state of the simulation s SIM t+τ should coincide with the counterfactual st+τ .To account for non-determinism in the simulator, each action sequence is simulated K = 50 times, which results in a set of K final states S ′ = {s k t+τ } K k=1 .A multivariate Gaussian distribution is then fit to the samples from S ′ via Maximum Likelihood Estimation.This allows the computation of the likelihood of the final counterfactual state st+τ under the Gaussian distribution for each method, and for each of the N initial trajectories.Fig.2presents the density of log-likelihoods for each of the methods, approximated with a Gaussian KDE.We observe that CAIAC's augmented data have high likelihood under the distribution of final states returned by the simulator, and hence are mostly valid.In contrast, while some counterfactual trajectories generated by other methods are also viable, most of their samples are associated to low log-likelihoods, which suggests a violation of the environment's dynamics.Increased support Fig.2(right) shows 1000 randomly selected samples from the Franka-Kitchen dataset, as well as one counterfactual (created using CAIAC) for each.The visualization employs t-SNE for a 2D representation of the highdimensional state space.We observe how the augmented samples cover a larger space than the observed data, suggesting that the support of the joint training distribution over entities is improved.We also provide numerical evidence by comparing the support of the empirical distributions.For each sample augmented with all methods, we first bin each of the object states into 2 categories.We then compute the support of the resulting categorical distribution with 2 N possible categories, where N is the number of objects.Table3reports ratios between the estimated support and the theoretical maximum.data, the model doesn't need to be queried OOD to create meaningful samples.We finally decide to leverage the strengths of both approaches and propose CAIAC+MBPO.With this approach, we train a dynamics model both on the original AND CAIAC's augmented data.Notably, the model is trained on perturbed yet dynamically feasible samples, potentially leading it to better generalization capabilities in OOD states.Finally, we we train our agent with MBPO using both the augmented CAIAC and the original samples.We observe that in low data regimes Fig.13(right), CAIAC+MBPO leads to a boost in performance compared to CAIAC alone.However, it still underperforms with respect to MBPO.We hypothesize that this is due to some fraction of unfeasible augmented samples used to train the model, leading to compounding errors when generating the rollout transitions with MBPO.This would also support the results for the Fetch-Pick&amp;Lift environment Fig.13(left), where despite outperforming MBPO significantly, CAIAC+MBPO doesn't achieve same performance as CAIAC alone.In terms of implementation details, all the methods share the same hyperparameters, except for the horizon length (N) and the ratio of augmented samples (R) which we tuned individually.We used N = 5 for CAIAC+MBPO and N = 10 for MBPO and R = 0.5 for the FetchPush environment, while for FetchPick&amp;Lift we crucially reduce R to 0.1 for all methods and use N = 5.We believe that combining CAIAC with model-based approaches is a very promising direction, and we leave extensive exploration of such an approach for future work.
Hindsight experience replay. Advances in neural information processing systems. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O Pieter Abbeel, W Zaremba, 201730</p>
<p>Information flows in causal networks. N Ay, D Polani, Advances in complex systems. 11012008</p>
<p>Human-to-robot imitation in the wild. S Bahl, A Gupta, D Pathak, Robotics Science and Systems (RSS). 2022</p>
<p>Bandits with unobserved confounders: A causal approach. E Bareinboim, A Forney, J Pearl, Advances in Neural Information Processing Systems. 282015</p>
<p>Relational inductive biases, deep learning, and graph networks. P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, arXiv:1806.012612018arXiv preprint</p>
<p>A holistic approach to semi-supervised learning. D Berthelot, N Carlini, I Goodfellow, N Papernot, A Oliver, C A Raffel, Mixmatch, Advances in neural information processing systems. 201932</p>
<p>S R Bowman, L Vilnis, O Vinyals, A M Dai, R Jozefowicz, S Bengio, arXiv:1511.06349Generating sentences from a continuous space. 2015arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>L Buesing, T Weber, Y Zwols, S Racaniere, A Guez, J.-B Lespiau, N Heess, Woulda, arXiv:1811.06272coulda, shoulda: Counterfactually-guided policy search. 2018arXiv preprint</p>
<p>Unsupervised scene decomposition and representation. C P Burgess, L Matthey, N Watters, R Kabra, I Higgins, M Botvinick, A Lerchner, Monet, 2019</p>
<p>Contingency-aware exploration in reinforcement learning. J Choi, Y Guo, M Moczulski, J Oh, N Wu, M Norouzi, H Lee, CoRR, abs/1811.014832018</p>
<p>Quantifying generalization in reinforcement learning. K Cobbe, O Klimov, C Hesse, T Kim, J Schulman, International Conference on Machine Learning. PMLR2019</p>
<p>Elements of information theory. T M Cover, 1999John Wiley &amp; Sons</p>
<p>Causal confusion in imitation learning. P De Haan, D Jayaraman, S Levine, Advances in Neural Information Processing Systems. 201932</p>
<p>Generalizing goal-conditioned reinforcement learning with variational causal reasoning. W Ding, H Lin, B Li, D Zhao, Advances in Neural Information Processing Systems. 202235</p>
<p>Seeing is not believing: Robust reinforcement learning against spurious correlation. W Ding, L Shi, Y Chi, D Zhao, arXiv:2307.079072023arXiv preprint</p>
<p>Lower and upper bounds for approximation of the kullback-leibler divergence between gaussian mixture models. J.-L Durrieu, J.-P Thiran, F Kelly, 10.1109/ICASSP.2012.62890012012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2012</p>
<p>Counterfactual data-fusion for online reinforcement learners. A Forney, J Pearl, E Bareinboim, International Conference on Machine Learning. PMLR2017</p>
<p>Datasets for deep data-driven reinforcement learning. J Fu, A Kumar, O Nachum, G Tucker, S Levine, D4rl, 2020</p>
<p>A minimalist approach to offline reinforcement learning. S Fujimoto, S S Gu, Advances in neural information processing systems. 20132-20145, 202134</p>
<p>Addressing function approximation error in actor-critic methods. S Fujimoto, H Hoof, D Meger, International conference on machine learning. PMLR2018</p>
<p>Off-policy deep reinforcement learning without exploration. S Fujimoto, D Meger, D Precup, International conference on machine learning. PMLR2019</p>
<p>Multi-object representation learning with iterative variational inference. K Greff, R L Kaufman, R Kabra, N Watters, C Burgess, D Zoran, L Matthey, M Botvinick, A Lerchner, International Conference on Machine Learning. PMLR2019</p>
<p>Relay policy learning: Solving long horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, Conference on Robot Learning (CoRL). 2019</p>
<p>Can active sampling reduce causal confusion in offline reinforcement learning?. G Gupta, T G Rudner, R T Mcallister, A Gaidon, Y Gal, Conference on Causal Learning and Reasoning. PMLR2023</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, arXiv:2010.021932020arXiv preprint</p>
<p>J Jiang, S Janghorbani, G De Melo, S Ahn, Scalor, arXiv:1910.02384Generative world models with scalable object representations. 2019arXiv preprint</p>
<p>Confounding-robust policy improvement. N Kallus, A Zhou, Advances in neural information processing systems. 201831</p>
<p>D P Kingma, J Ba, Adam, arXiv:1412.6980A method for stochastic optimization. 2014arXiv preprint</p>
<p>Offline reinforcement learning with implicit q-learning. I Kostrikov, A Nair, S Levine, arXiv:2110.061692021arXiv preprint</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 252012</p>
<p>Stabilizing off-policy q-learning via bootstrapping error reduction. A Kumar, J Fu, M Soh, G Tucker, S Levine, Advances in neural information processing systems. 201932</p>
<p>Batch reinforcement learning. S Lange, T Gabel, M Riedmiller, Reinforcement learning: State-of-theart. Springer2012</p>
<p>Reinforcement learning with augmented data. M Laskin, K Lee, A Stooke, L Pinto, P Abbeel, A Srinivas, Advances in neural information processing systems. 19884-19895, 202033</p>
<p>Gradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 199886</p>
<p>Network randomization: A simple technique for generalization in deep reinforcement learning. K Lee, K Lee, J Shin, H Lee, arXiv:1910.053962019arXiv preprint</p>
<p>Learning agile skills via adversarial imitation of rough partial demonstrations. C Li, M Vlastelica, S Blaes, J Frey, F Grimminger, G Martius, Proceedings of The 6th Conference on Robot Learning. K Liu, D Kulic, J Ichnowski, The 6th Conference on Robot LearningPMLR18 Dec 2023205</p>
<p>The local information dynamics of distributed computation in complex systems. J T Lizier, 2012Springer Science &amp; Business Media</p>
<p>Object-centric learning with slot attention. F Locatello, D Weissenborn, T Unterthiner, A Mahendran, G Heigold, J Uszkoreit, A Dosovitskiy, T Kipf, Advances in Neural Information Processing Systems. 202033</p>
<p>Deconfounding reinforcement learning in observational settings. C Lu, B Schölkopf, J M Hernández-Lobato, arXiv:1812.105762018arXiv preprint</p>
<p>Sample-efficient reinforcement learning via counterfactual-based data augmentation. C Lu, B Huang, K Wang, J M Hernández-Lobato, K Zhang, B Schölkopf, arXiv:2012.090922020arXiv preprint</p>
<p>Resolving causal confusion in reinforcement learning via robust exploration. C Lyle, A Zhang, M Jiang, J Pineau, Y Gal, Self-Supervision for Reinforcement Learning Workshop-ICLR. 20212021</p>
<p>Learning latent plans from play. C Lynch, M Khansari, T Xiao, V Kumar, J Tompson, S Levine, P Sermanet, Conference on Robot Learning (CoRL). 2019</p>
<p>Discovering and achieving goals via world models. R Mendonca, O Rybkin, K Daniilidis, D Hafner, D Pathak, Advances in Neural Information Processing Systems. 202134</p>
<p>. J Pearl, Causality, 2009Cambridge university press</p>
<p>The book of why: the new science of cause and effect. Basic books. J Pearl, D Mackenzie, 2018</p>
<p>Identifiability of causal graphs using functional models. J Peters, J M Mooij, D Janzing, B Schölkopf, CoRR, abs/1202.37572012</p>
<p>Elements of Causal Inference: Foundations and Learning Algorithms. Adaptive Computation and Machine Learning. J Peters, D Janzing, B Schölkopf, 2017MIT PressCambridge, MA</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, arXiv:1710.065422017arXiv preprint</p>
<p>Counterfactual data augmentation using locally factored dynamics. S Pitis, E Creager, A Garg, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>Mocoda: Model-based counterfactual data augmentation. S Pitis, E Creager, A Mandlekar, A Garg, Advances in Neural Information Processing Systems. 202235</p>
<p>M Plappert, M Andrychowicz, A Ray, B Mcgrew, B Baker, G Powell, J Schneider, J Tobin, M Chociej, P Welinder, arXiv:1802.09464Multi-goal reinforcement learning: Challenging robotics environments and request for research. 2018arXiv preprint</p>
<p>Causally correct partial models for reinforcement learning. D J Rezende, I Danihelka, G Papamakarios, N R Ke, R Jiang, T Weber, K Gregor, H Merzic, F Viola, J Wang, arXiv:2002.028362020arXiv preprint</p>
<p>Latent plans for task agnostic offline reinforcement learning. E Rosete-Beas, O Mees, G Kalweit, J Boedecker, W Burgard, Proceedings of the 6th Conference on Robot Learning (CoRL). the 6th Conference on Robot Learning (CoRL)2022</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics2011JMLR Workshop and Conference Proceedings</p>
<p>Measuring information transfer. T Schreiber, Physical review letters. 8524612000</p>
<p>Causal influence detection for improving efficiency in reinforcement learning. M Seitzer, B Schölkopf, G Martius, Advances in Neural Information Processing Systems. NeurIPS 2021. December 2021</p>
<p>Bridging the gap to real-world objectcentric learning. M Seitzer, M Horn, A Zadaianchuk, D Zietlow, T Xiao, C.-J Simon-Gabriel, T He, Z Zhang, B Schölkopf, T Brox, arXiv:2209.148602022arXiv preprint</p>
<p>Regularized behavior cloning for blocking the leakage of past action information. S Seo, H Hwang, H Yang, K.-E Kim, Advances in Neural Information Processing Systems. 202436</p>
<p>Learning structured output representation using deep conditional generative models. K Sohn, H Lee, X Yan, Advances in neural information processing systems. 282015</p>
<p>Simplifying semi-supervised learning with consistency and confidence. K Sohn, D Berthelot, N Carlini, Z Zhang, H Zhang, C A Raffel, E D Cubuk, A Kurakin, C.-L Li, Fixmatch, Advances in neural information processing systems. 202033</p>
<p>Mega-reward: Achieving human-level play without extrinsic rewards. Y Song, J Wang, T Lukasiewicz, Z Xu, S Zhang, A Wojcicki, M Xu, 10.1609/aaai.v34i04.6040Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 202034</p>
<p>Causation, prediction, and search. P Spirtes, C Glymour, R Scheines, 2001MIT press</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>N A Urpí, S Curi, A Krause, arXiv:2102.05371Risk-averse offline reinforcement learning. 2021arXiv preprint</p>
<p>Efficient learning of high level plans from play. N A Urpí, M Bagatella, O Hilliges, G Martius, S Coros, International Conference on Robotics and Automation (ICRA). 2023</p>
<p>Riskaverse zero-order trajectory optimization. M Vlastelica, S Blaes, C Pinneri, G Martius, 5th Annual Conference on Robot Learning. 2021a</p>
<p>Neuroalgorithmic policies enable fast combinatorial generalization. M Vlastelica, M Rolinek, G Martius, International Conference on Machine Learning. PMLR2021b</p>
<p>Diverse offline imitation learning. M Vlastelica, J Cheng, G Martius, P Kolev, 2023</p>
<p>Task-independent causal state abstraction. Z Wang, X Xiao, Y Zhu, P Stone, Proceedings of the 35th International Conference on Neural Information Processing Systems, Robot Learning workshop. the 35th International Conference on Neural Information Processing Systems, Robot Learning workshop2021</p>
<p>Z Wang, X Xiao, Z Xu, Y Zhu, P Stone, arXiv:2206.13452Causal dynamics learning for task-independent state abstraction. 2022arXiv preprint</p>
<p>The development and generalization of "contingency awareness" in early infancy: Some hypotheses. J S Watson, Merrill-Palmer Quarterly of Behavior and Development. 002601501221966</p>
<p>Fighting copycat agents in behavioral cloning from observation histories. C Wen, J Lin, T Darrell, D Jayaraman, Y Gao, Advances in Neural Information Processing Systems. 202033</p>
<p>Objectcentric learning for real-world videos by predicting temporal feature similarities. A Zadaianchuk, M Seitzer, G Martius, 2023</p>
<p>Deep sets. Advances in neural information processing systems. M Zaheer, S Kottur, S Ravanbakhsh, B Poczos, R R Salakhutdinov, A J Smola, 201730</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. A Zhang, R T Mcallister, R Calandra, Y Gal, S Levine, International Conference on Learning Representations. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
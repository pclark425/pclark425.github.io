<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1618 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1618</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1618</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-15416698</p>
                <p><strong>Paper Title:</strong> A comparison of machine learning techniques for survival prediction in breast cancer</p>
                <p><strong>Paper Abstract:</strong> Background The ability to accurately classify cancer patients into risk classes, i.e. to predict the outcome of the pathology on an individual basis, is a key ingredient in making therapeutic decisions. In recent years gene expression data have been successfully used to complement the clinical and histological criteria traditionally used in such prediction. Many "gene expression signatures" have been developed, i.e. sets of genes whose expression values in a tumor can be used to predict the outcome of the pathology. Here we investigate the use of several machine learning techniques to classify breast cancer patients using one of such signatures, the well established 70-gene signature. Results We show that Genetic Programming performs significantly better than Support Vector Machines, Multilayered Perceptrons and Random Forests in classifying patients from the NKI breast cancer dataset, and comparably to the scoring-based method originally proposed by the authors of the 70-gene signature. Furthermore, Genetic Programming is able to perform an automatic feature selection. Conclusions Since the performance of Genetic Programming is likely to be improvable compared to the out-of-the-box approach used here, and given the biological insight potentially provided by the Genetic Programming solutions, we conclude that Genetic Programming methods are worth further investigation as a tool for cancer patient classification based on gene expression data.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1618.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1618.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-based evolutionary algorithm that evolves programs (boolean expressions) by selection and variation (crossover and mutation) to classify patients by survival using the 70-gene signature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of machine learning techniques for survival prediction in breast cancer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Tree-based Genetic Programming (boolean-expression GP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Individuals are abstract syntax trees representing boolean expressions built from function set {AND, OR, NOT} and terminal set of 70 boolean variables (one per gene). A population of 500 individuals is evolved with tournament selection (size=10), generational replacement, crossover rate 0.9 and mutation rate 0.1 for up to 5 generations; fitness is the number of incorrectly classified instances (minimization). A weighted fitness variant (0.9*FalseNegative + 0.1*FalsePositive) was also used to bias sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (abstract syntax trees / boolean expressions over gene-terminals)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Described as tree-based crossover (crossover rate = 0.9). The paper states GP uses crossover as a genetic operator but does not give implementation details; given the tree-based GP formulation and Koza citation, the mechanism is the standard subtree-exchange (parent subtrees swapped to produce offspring) (mechanism not explicitly specified in text).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Mutation used (rate = 0.1) but specific mutation operator is not described in the paper; in the used tree-based GP framework this corresponds typically to subtree or point mutation replacing a randomly chosen subtree with a newly generated subtree (this is an inferred/standard practice; not explicitly specified).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Predictive performance metrics on held-out test sets: number of incorrectly classified instances (minimization), and classification sensitivity measured as number of false negatives; robustness assessed on an independent dataset via Fisher exact test p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Test-set results over 50 runs: best (minimum) incorrectly classified instances = 10; average incorrect = 16.40 (SEM 0.30). False negatives: average = 9.82 (SEM 0.44). Using a weighted-fitness (0.9*FN + 0.1*FP) produced average total incorrect = 16.04 (SEM 0.44) and average false negatives = 4.32 (SEM 0.346). On an independent dataset, 17 of the 50 best GP solutions were applicable and each showed significant predictive power (Fisher exact p-values between 7.6e-3 and 2.9e-4).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Implicit genotypic/terminal-usage statistics reported: distribution of number of terminals per best solutions and recurrence counts of terminals across solutions (terminal-usage frequency / genotypic diversity). No explicit formal diversity metric (e.g., behavioral diversity) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>Number of terminals used by best solutions (50 runs): range 1–23, median 4, Q1=2, Q3=7. Top recurring terminals: PRC1 in 48 solutions, RFC4 in 23 solutions, others: 16,10,9,7,6,6,6,6 occurrences (see Table 3). 50 independent runs yielded many distinct solutions; 17 of the 50 best solutions could be applied to an independent dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Bioinformatics — survival prediction / classification from 70-gene expression signature (breast cancer prognostic prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Support Vector Machines (polynomial kernels degree 1,2,3), Multilayer Perceptrons (MLP), Random Forests (2500 trees), and the 70-gene scoring method from van 't Veer et al.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GP (tree-based boolean-expression programs) outperformed other standard ML methods on average (average incorrect = 16.40 vs SVM/K1 18.32, SVM/K2 16.76, MLP 18.08, RF 17.60); GP solutions are compact (median 4 features) enabling interpretability and implicit feature selection; using GP-selected features to retrain SVM improved SVM performance, indicating the intrinsic value of GP feature selection; tuning the GP fitness to weight false negatives (0.9*FN + 0.1*FP) substantially reduced false negatives (from avg 9.82 to 4.32) although overall incorrect did not change significantly; GP-derived classifiers were robust when applied without retraining to an independent dataset (significant Fisher p-values). The paper reports operator rates (crossover=0.9, mutation=0.1) but does not provide mechanistic analysis of how crossover/mutation individually affected novelty/diversity/executability beyond these empirical outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Implementation used GPLab (Matlab GP toolbox). The paper emphasizes GP's automatic feature selection and interpretability; it does not define explicit novelty or diversity metrics beyond counts and recurrence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A comparison of machine learning techniques for survival prediction in breast cancer', 'publication_date_yy_mm': '2011-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1618.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1618.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General evolutionary algorithms that operate on fixed-length encodings using selection, crossover and mutation; mentioned as used for gene-selection/feature-selection where each allele indicates whether a gene is selected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of machine learning techniques for survival prediction in breast cancer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Algorithms (binary-feature selection GA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as being used for feature selection: individuals encode gene-selection bitstrings where each allele indicates inclusion/exclusion of a gene; GAs are used to search for optimal gene subsets for classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>feature-selection bitstrings (binary vectors indicating selected genes)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Mentioned generically (GAs 'use crossover'); no implementation details provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Mentioned generically (GAs 'use mutation'); no implementation details provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Mentioned in context of gene-selection for microarray/cancer classification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GAs are cited as prior work for gene selection; the paper does not evaluate or report experimental metrics for GAs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A comparison of machine learning techniques for survival prediction in breast cancer', 'publication_date_yy_mm': '2011-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1618.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1618.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary Algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of population-based optimization algorithms inspired by natural evolution (includes GA and GP); mentioned as applied to feature selection and classification in gene expression analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of machine learning techniques for survival prediction in breast cancer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evolutionary Algorithms (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced broadly as a class of methods applied to feature selection and classification on microarray data; no particular EA instance is executed in this study other than GP.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>various (feature subsets, classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Generic reference to crossover as a standard EA operator; no details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Generic reference to mutation as a standard EA operator; no details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Mentioned in background/related work for gene-expression analysis and classifier construction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Evolutionary algorithms are recognized in related work as useful for microarray feature selection and classification; the paper proceeds to study GP (a specific EA) in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A comparison of machine learning techniques for survival prediction in breast cancer', 'publication_date_yy_mm': '2011-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1618.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1618.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse Ensemble Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced variant of GP that builds diverse ensembles of GP classifiers for cancer classification (from related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of machine learning techniques for survival prediction in breast cancer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Diverse Ensemble Genetic Programming (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited in related work (Hong & Cho) as an approach that constructs diverse ensembles of GP-evolved classifiers for microarray-based cancer classification; specifics are in the cited reference, not in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (GP classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Cancer classification from microarray data (mentioned in references).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Only mentioned in the related-work list of GP applications; no experimental detail or metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A comparison of machine learning techniques for survival prediction in breast cancer', 'publication_date_yy_mm': '2011-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1618.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1618.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constrained-syntax GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data mining with constrained-syntax Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced GP variant that enforces syntactic constraints to produce domain-appropriate expressions for medical data mining (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of machine learning techniques for survival prediction in breast cancer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Constrained-syntax Genetic Programming (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an application of GP with constrained syntax to medical datasets; the present paper does not implement or evaluate this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (constrained syntax GP trees)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Medical data mining / microarray classification (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned in related work; no metrics or application results appear in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A comparison of machine learning techniques for survival prediction in breast cancer', 'publication_date_yy_mm': '2011-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1618.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1618.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PMBGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Model-Building Genetic Algorithm (applied to gene selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced GA variant that builds probabilistic models to guide search and has been applied to gene selection for cancer classification (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of machine learning techniques for survival prediction in breast cancer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic Model-Building Genetic Algorithm (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as a GA variant applied to gene selection; the paper cites Paul & Iba for gene selection using probabilistic model-building GA.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>feature-selection bitstrings / gene subsets</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Gene selection for cancer classification (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as relevant prior work; no experimental data or metrics provided in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A comparison of machine learning techniques for survival prediction in breast cancer', 'publication_date_yy_mm': '2011-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic Programming <em>(Rating: 2)</em></li>
                <li>Genetic Programming for Mining DNA Chip Data from Cancer Patients <em>(Rating: 2)</em></li>
                <li>The classification of cancer based on DNA microarray data that uses diverse ensemble genetic programming <em>(Rating: 2)</em></li>
                <li>Data mining with constrained-syntax genetic programming: applications to medical data sets <em>(Rating: 2)</em></li>
                <li>Gene selection for classification of cancers using probabilistic model building genetic algorithm <em>(Rating: 2)</em></li>
                <li>Feature Selection and Molecular Classification of Cancer Using Genetic Programming <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1618",
    "paper_id": "paper-15416698",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "GP",
            "name_full": "Genetic Programming",
            "brief_description": "A tree-based evolutionary algorithm that evolves programs (boolean expressions) by selection and variation (crossover and mutation) to classify patients by survival using the 70-gene signature.",
            "citation_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
            "mention_or_use": "use",
            "system_name": "Tree-based Genetic Programming (boolean-expression GP)",
            "system_description": "Individuals are abstract syntax trees representing boolean expressions built from function set {AND, OR, NOT} and terminal set of 70 boolean variables (one per gene). A population of 500 individuals is evolved with tournament selection (size=10), generational replacement, crossover rate 0.9 and mutation rate 0.1 for up to 5 generations; fitness is the number of incorrectly classified instances (minimization). A weighted fitness variant (0.9*FalseNegative + 0.1*FalsePositive) was also used to bias sensitivity.",
            "input_type": "programs (abstract syntax trees / boolean expressions over gene-terminals)",
            "crossover_operation": "Described as tree-based crossover (crossover rate = 0.9). The paper states GP uses crossover as a genetic operator but does not give implementation details; given the tree-based GP formulation and Koza citation, the mechanism is the standard subtree-exchange (parent subtrees swapped to produce offspring) (mechanism not explicitly specified in text).",
            "mutation_operation": "Mutation used (rate = 0.1) but specific mutation operator is not described in the paper; in the used tree-based GP framework this corresponds typically to subtree or point mutation replacing a randomly chosen subtree with a newly generated subtree (this is an inferred/standard practice; not explicitly specified).",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Predictive performance metrics on held-out test sets: number of incorrectly classified instances (minimization), and classification sensitivity measured as number of false negatives; robustness assessed on an independent dataset via Fisher exact test p-values.",
            "executability_results": "Test-set results over 50 runs: best (minimum) incorrectly classified instances = 10; average incorrect = 16.40 (SEM 0.30). False negatives: average = 9.82 (SEM 0.44). Using a weighted-fitness (0.9*FN + 0.1*FP) produced average total incorrect = 16.04 (SEM 0.44) and average false negatives = 4.32 (SEM 0.346). On an independent dataset, 17 of the 50 best GP solutions were applicable and each showed significant predictive power (Fisher exact p-values between 7.6e-3 and 2.9e-4).",
            "diversity_metric": "Implicit genotypic/terminal-usage statistics reported: distribution of number of terminals per best solutions and recurrence counts of terminals across solutions (terminal-usage frequency / genotypic diversity). No explicit formal diversity metric (e.g., behavioral diversity) is reported.",
            "diversity_results": "Number of terminals used by best solutions (50 runs): range 1–23, median 4, Q1=2, Q3=7. Top recurring terminals: PRC1 in 48 solutions, RFC4 in 23 solutions, others: 16,10,9,7,6,6,6,6 occurrences (see Table 3). 50 independent runs yielded many distinct solutions; 17 of the 50 best solutions could be applied to an independent dataset.",
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Bioinformatics — survival prediction / classification from 70-gene expression signature (breast cancer prognostic prediction).",
            "comparison_baseline": "Compared against Support Vector Machines (polynomial kernels degree 1,2,3), Multilayer Perceptrons (MLP), Random Forests (2500 trees), and the 70-gene scoring method from van 't Veer et al.",
            "key_findings": "GP (tree-based boolean-expression programs) outperformed other standard ML methods on average (average incorrect = 16.40 vs SVM/K1 18.32, SVM/K2 16.76, MLP 18.08, RF 17.60); GP solutions are compact (median 4 features) enabling interpretability and implicit feature selection; using GP-selected features to retrain SVM improved SVM performance, indicating the intrinsic value of GP feature selection; tuning the GP fitness to weight false negatives (0.9*FN + 0.1*FP) substantially reduced false negatives (from avg 9.82 to 4.32) although overall incorrect did not change significantly; GP-derived classifiers were robust when applied without retraining to an independent dataset (significant Fisher p-values). The paper reports operator rates (crossover=0.9, mutation=0.1) but does not provide mechanistic analysis of how crossover/mutation individually affected novelty/diversity/executability beyond these empirical outcomes.",
            "additional_notes": "Implementation used GPLab (Matlab GP toolbox). The paper emphasizes GP's automatic feature selection and interpretability; it does not define explicit novelty or diversity metrics beyond counts and recurrence statistics.",
            "uuid": "e1618.0",
            "source_info": {
                "paper_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
                "publication_date_yy_mm": "2011-05"
            }
        },
        {
            "name_short": "GAs",
            "name_full": "Genetic Algorithms",
            "brief_description": "General evolutionary algorithms that operate on fixed-length encodings using selection, crossover and mutation; mentioned as used for gene-selection/feature-selection where each allele indicates whether a gene is selected.",
            "citation_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
            "mention_or_use": "mention",
            "system_name": "Genetic Algorithms (binary-feature selection GA)",
            "system_description": "Mentioned in related work as being used for feature selection: individuals encode gene-selection bitstrings where each allele indicates inclusion/exclusion of a gene; GAs are used to search for optimal gene subsets for classification tasks.",
            "input_type": "feature-selection bitstrings (binary vectors indicating selected genes)",
            "crossover_operation": "Mentioned generically (GAs 'use crossover'); no implementation details provided in the paper.",
            "mutation_operation": "Mentioned generically (GAs 'use mutation'); no implementation details provided in the paper.",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Mentioned in context of gene-selection for microarray/cancer classification.",
            "comparison_baseline": "",
            "key_findings": "GAs are cited as prior work for gene selection; the paper does not evaluate or report experimental metrics for GAs.",
            "uuid": "e1618.1",
            "source_info": {
                "paper_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
                "publication_date_yy_mm": "2011-05"
            }
        },
        {
            "name_short": "EA",
            "name_full": "Evolutionary Algorithms",
            "brief_description": "A family of population-based optimization algorithms inspired by natural evolution (includes GA and GP); mentioned as applied to feature selection and classification in gene expression analysis.",
            "citation_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
            "mention_or_use": "mention",
            "system_name": "Evolutionary Algorithms (generic)",
            "system_description": "Referenced broadly as a class of methods applied to feature selection and classification on microarray data; no particular EA instance is executed in this study other than GP.",
            "input_type": "various (feature subsets, classifiers)",
            "crossover_operation": "Generic reference to crossover as a standard EA operator; no details provided.",
            "mutation_operation": "Generic reference to mutation as a standard EA operator; no details provided.",
            "uses_literature": false,
            "uses_code": null,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Mentioned in background/related work for gene-expression analysis and classifier construction.",
            "comparison_baseline": "",
            "key_findings": "Evolutionary algorithms are recognized in related work as useful for microarray feature selection and classification; the paper proceeds to study GP (a specific EA) in detail.",
            "uuid": "e1618.2",
            "source_info": {
                "paper_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
                "publication_date_yy_mm": "2011-05"
            }
        },
        {
            "name_short": "Ensemble GP",
            "name_full": "Diverse Ensemble Genetic Programming",
            "brief_description": "A referenced variant of GP that builds diverse ensembles of GP classifiers for cancer classification (from related work).",
            "citation_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
            "mention_or_use": "mention",
            "system_name": "Diverse Ensemble Genetic Programming (as cited)",
            "system_description": "Cited in related work (Hong & Cho) as an approach that constructs diverse ensembles of GP-evolved classifiers for microarray-based cancer classification; specifics are in the cited reference, not in the present paper.",
            "input_type": "programs (GP classifiers)",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": null,
            "uses_code": null,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Cancer classification from microarray data (mentioned in references).",
            "comparison_baseline": null,
            "key_findings": "Only mentioned in the related-work list of GP applications; no experimental detail or metrics provided in this paper.",
            "uuid": "e1618.3",
            "source_info": {
                "paper_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
                "publication_date_yy_mm": "2011-05"
            }
        },
        {
            "name_short": "Constrained-syntax GP",
            "name_full": "Data mining with constrained-syntax Genetic Programming",
            "brief_description": "A referenced GP variant that enforces syntactic constraints to produce domain-appropriate expressions for medical data mining (cited in related work).",
            "citation_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
            "mention_or_use": "mention",
            "system_name": "Constrained-syntax Genetic Programming (as cited)",
            "system_description": "Cited as an application of GP with constrained syntax to medical datasets; the present paper does not implement or evaluate this variant.",
            "input_type": "programs (constrained syntax GP trees)",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": null,
            "uses_code": null,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Medical data mining / microarray classification (cited).",
            "comparison_baseline": null,
            "key_findings": "Mentioned in related work; no metrics or application results appear in this paper.",
            "uuid": "e1618.4",
            "source_info": {
                "paper_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
                "publication_date_yy_mm": "2011-05"
            }
        },
        {
            "name_short": "PMBGA",
            "name_full": "Probabilistic Model-Building Genetic Algorithm (applied to gene selection)",
            "brief_description": "Referenced GA variant that builds probabilistic models to guide search and has been applied to gene selection for cancer classification (cited in related work).",
            "citation_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
            "mention_or_use": "mention",
            "system_name": "Probabilistic Model-Building Genetic Algorithm (as cited)",
            "system_description": "Mentioned in related work as a GA variant applied to gene selection; the paper cites Paul & Iba for gene selection using probabilistic model-building GA.",
            "input_type": "feature-selection bitstrings / gene subsets",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": null,
            "uses_code": null,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Gene selection for cancer classification (cited).",
            "comparison_baseline": null,
            "key_findings": "Cited as relevant prior work; no experimental data or metrics provided in the current paper.",
            "uuid": "e1618.5",
            "source_info": {
                "paper_title": "A comparison of machine learning techniques for survival prediction in breast cancer",
                "publication_date_yy_mm": "2011-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic Programming",
            "rating": 2,
            "sanitized_title": "genetic_programming"
        },
        {
            "paper_title": "Genetic Programming for Mining DNA Chip Data from Cancer Patients",
            "rating": 2,
            "sanitized_title": "genetic_programming_for_mining_dna_chip_data_from_cancer_patients"
        },
        {
            "paper_title": "The classification of cancer based on DNA microarray data that uses diverse ensemble genetic programming",
            "rating": 2,
            "sanitized_title": "the_classification_of_cancer_based_on_dna_microarray_data_that_uses_diverse_ensemble_genetic_programming"
        },
        {
            "paper_title": "Data mining with constrained-syntax genetic programming: applications to medical data sets",
            "rating": 2,
            "sanitized_title": "data_mining_with_constrainedsyntax_genetic_programming_applications_to_medical_data_sets"
        },
        {
            "paper_title": "Gene selection for classification of cancers using probabilistic model building genetic algorithm",
            "rating": 2,
            "sanitized_title": "gene_selection_for_classification_of_cancers_using_probabilistic_model_building_genetic_algorithm"
        },
        {
            "paper_title": "Feature Selection and Molecular Classification of Cancer Using Genetic Programming",
            "rating": 2,
            "sanitized_title": "feature_selection_and_molecular_classification_of_cancer_using_genetic_programming"
        }
    ],
    "cost": 0.01417975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A comparison of machine learning techniques for survival prediction in breast cancer</p>
<p>Leonardo Vanneschi 
Molecular Biotechnology Center
Computational Biology Unit
University of Torino
Italy</p>
<p>Antonella Farinaccio 
Molecular Biotechnology Center
Computational Biology Unit
University of Torino
Italy</p>
<p>Giancarlo Mauri 
Molecular Biotechnology Center
Computational Biology Unit
University of Torino
Italy</p>
<p>Mauro Antoniotti 
Molecular Biotechnology Center
Computational Biology Unit
University of Torino
Italy</p>
<p>Paolo Provero 
Molecular Biotechnology Center
Computational Biology Unit
University of Torino
Italy</p>
<p>Mario Giacobini mario.giacobini@unito.it 
Molecular Biotechnology Center
Computational Biology Unit
University of Torino
Italy</p>
<p>A comparison of machine learning techniques for survival prediction in breast cancer
R E S E A R C H Open Access Full list of author information is available at the end of the article
Background: The ability to accurately classify cancer patients into risk classes, i.e. to predict the outcome of the pathology on an individual basis, is a key ingredient in making therapeutic decisions.</p>
<p>Background</p>
<p>Current cancer therapies have serious side effects: ideally type and dosage of the therapy should be matched to each individual patient based on his/her risk of relapse. Therefore the classification of cancer patients into risk classes is a very active field of research, with direct clinical applications. Until recently patient classification was based on a series of clinical and histological parameters. The advent of high-throughput techniques to measure gene expression led in the last decade to a large body of research on gene expression in cancer, and in particular on the possibility of using gene expression data to improve patient classification. A gene signature is a set of genes whose levels of expression can be used to predict a biological state (see [1]): in the case of cancer, gene signatures have been developed both to distinguish cancerous from non-cancerous conditions and to classify cancer patients based on the aggressiveness of the tumor, as measured for example by the probability of relapsing within a given time.</p>
<p>While many studies have been devoted to the identification of gene signatures in various types of cancer, the question of the algorithms to be used to maximize the predictive power of a gene signature has received less attention. To investigate this issue systematically, we considered one of the best established gene signatures, the 70-gene signature for breast cancer [2], and we compared the performance of four different machine learning algorithms in using this signature to predict the survival of a cohort of breast cancer patients. The 70-gene signature is a set of microarray features selected in [2] based on correlation with survival, on which the molecular prognostic test for breast cancer "MammaPrint"™ is based. While several machine learning algorithms have been used to classify cancer samples based on gene expression data [3][4][5][6][7][8], in this work we performed a systematic comparison of the performance of four machine learning algorithms using the same features to predict the same classes. In our comparison, feature selection is thus not explicitly performed as a pre-processing phase before executing the machine learning algorithms 1 . We considered GP, Support Vector Machines, Multilayered Perceptrons and Random Forests, and we applied them to the problem of using the 70-gene signature to predict the survival of the breast cancer patients included in the NKI dataset [9]. This is considered one of the gold-standard datasets in the field, and the predictive power of the 70-gene signature on these patients was already shown in [9]. In this preliminary study we tried to use all the methods in an "out-of-the-box" version so as to obtain a first evaluation, as unbiased as possible, of the performance of the methods.</p>
<p>Previous and Related Work</p>
<p>Many different machine learning methods [10] have already been applied for microarray data analysis, like k-nearest neighbors [11], hierarchical clustering [12], self-organizing maps [13], Support Vector Machines [14,15] or Bayesian networks [16]. Furthermore, in the last few years Evolutionary Algorithms (EA) [17] have been used for solving both problems of feature selection and classification in gene expression data analysis. Genetic Algorithms (GAs) [18] have been employed for building selectors where each allele of the representation corresponds to one gene and its state denotes whether the gene is selected or not [19]. GP on the other hand has been shown to work well for recognition of structures in large data sets [20]. GP was applied to microarray data to generate programs that reliably predict the health/malignancy states of tissue, or classify different types of tissues. An intrinsic advantage of GP is that it automatically selects a small number of feature genes during the evolution [21]. The evolution of classifiers from the initial population seamlessly integrates the process of gene selection and classifier construction. In fact, in [8] GP was used on cancer expression profiling data to select potentially informative feature genes, build molecular classifiers by mathematical integration of these genes and classify tumour samples. Furthermore, GP has been shown a promising approach for discovering comprehensible rule-based classifiers from medical data [22] as well as gene expression profiling data [23]. The results presented in those contributions are encouraging and pave the way to a further investigation of GP for this kind of datasets, which is the goal of this paper.</p>
<p>Results and Discussion</p>
<p>Predictive Power of Machine Learning Methods</p>
<p>We used the NKI breast cancer dataset [9], providing gene expression and survival data for 295 consecutive breast carcinoma patients. We considered only the expression data for the genes included in the "70-gene" signature [2].</p>
<p>Both survival and gene expression data were transformed into binary form. For the survival data, we defined the outcome as the survival status of the patient at time t end = 10.3 years. By choosing this particular endpoint we balanced the number of dead and alive patients: out of 148 patients for which the status at t end is known, 74 were dead and 74 were alive. Binary expression data were obtained by replacing all positive logarithmic fold changes in the original dataset with 1 and all negative and missing ones with 0.</p>
<p>Our dataset is a matrix H = [H (i, j) ] of binary values composed by 148 rows (instances) and 71 columns (features), where each line i represents the gene signature of a patient whose binary target (0 = survived after t end years, 1 = dead for breast cancer before t end years) has been placed at position H (i,71) . In this way, the last column of matrix H represents all the known target values. Our task is now to generate a mapping F such that F (H (i,1) , H (i,2) , ..., H (i,70) ) = H (i,71) for each line i in the dataset. Of course, we also want F to have a good generalization ability, i.e. to be able to assess the target value for new patients, that have not been used in the training phase. For this reason, we used a set of machine learning techniques, as discussed in Section Methods. To compare the predictive power of the computational methods, we performed 50 independent choices of training and test set, the training set including 70% of the patients and the test set the remaining 30%. The various prediction methods were then run on these datasets, so that the choice of training and testing sets in each run was the same for all methods. Table 1 summarizes the results returned by each machine learning method on the 50 runs. The first line indicates the different methods, the second line shows the best (i.e. lowest) value of the incorrectly classified instances obtained on the test set over the 50 runs, and the third line reports the mean performances of each group of 50 runs on their test sets, together with the corresponding standard error of mean (SEM).</p>
<p>As Table 1 clearly shows, the best solutions were found by GP and Multilayered Perceptrons and the best average result was found by GP. Moreover, statistical analysis indicates that GP consistently outperforms the other methods except SVM using polynomial kernel with degree 2. In fact, as it can be seen in Table 2, the difference between the various average results is statistically significant (P-value 3.05 × 10 -5 for  ANOVA test on the 4 samples of solutions found by each method). Finally, pairwise 2tailed Student t-tests comparing GP with each other method demonstrate its general better performance. These statistical tests were performed since there was no evidence of deviation from normality or unequal variances. The solutions found by GP typically use a rather small number of features (i.e. terminals). In fact, the solutions of the 50 GP runs are functions of a number of terminal that ranges from 1 to 23, with a median value of 4, and first and third quartiles of 2 and 7 respectively. Few of these features tend to recur in several solution as it can be seen in Table 3, where the gene symbol, the gene name of each feature, together with the number of solutions where the feature occurs are shown.</p>
<p>Comparison with the Scoring Method</p>
<p>The authors of Ref. [9] used the seventy-gene signature by computing the correlation coefficient between the expression profile of the patient (limited to the 70 genes of the signature) and a previously computed typical expression profile of a good prognosis patient. To compare the performance of the various machine learning algorithms with this scoring system we proceeded as follows:</p>
<p>• We obtained the prognostic score s of the patients (excluding the ones used to train the signature in [2]) from the Supplementary Material of [9], and classified as good prognosis the patients with s &gt; 0.4 and as bad prognosis the ones with s ≤ 0.4. This is the cutoff used in [9].</p>
<p>• We generated 50 random lists of 44 patients from this set, to match the statistic used for machine learning techniques, and computed for each list the number of false predictions given by the scoring method.  The mean number of false predictions was 16.24, with a SEM of 0.37. Therefore the scoring method appears to be superior to all machine learning algorithm other than GP, and slightly superior to GP. The difference between the performances of GP and the scoring method are not statistically significant (P = 0.49, 2-tailed Student t-test).</p>
<p>The Role of Feature Selection</p>
<p>To determine to what extent feature selection is responsible for the good performance of GP, we identified the 10 features most often selected by GP among the 70 initial features and ran again both GP and SVM with quadratic kernel using only these features. The performance of both methods significantly improved: for GP, the number of incorrectly identified features decreased from 16.40 (SEM 0.30) to 12.86 (0.40); for the SVM it went from 16.76 (0.18) to 14.96 (0.41). Using this preliminary round of feature selection the performance of GP becomes significantly better than both SVM and the original scoring method.</p>
<p>These results suggests on one hand, that the feature selection performed by GP has intrinsic value, not necessarily tied to the use of syntax trees, since the SVM can take advantage of the feature selection performed by GP to improve its performance. Second, that a recursive use of GP, in which a first run is used to select the best features to be used in a second run, might be a promising way of optimizing the method.</p>
<p>Performance on Unbalanced Datasets</p>
<p>To check whether the performance of the GP is tied to the choice of a balanced dataset, we repeated the analysis using different time cutoffs (5 and 7.5 years) and compared the performance of GP with the SVM using polynomial kernel with degree 2, which was the best performing method after GP in the balanced dataset. The results are reported in Table 4. At 7.5 years there is again no significant difference between the performance of the two methods. However, at 5 years GP performs significantly better than the SVM (P = 6.46 × 10 -6 from two-sided t-test). We conclude that the balancing of the dataset is not crucial to obtain a good performance from GP.</p>
<p>Performance on an Independent Dataset</p>
<p>An important feature of any predictor based on gene expression data is its robustness with respect to the choice of dataset, since gene expression data from cancer patients come from studies using different protocols and/or microarray platforms. We thus applied the best predictors found by GP in each of the 50 runs to an independent breast cancer dataset [24]. It includes 251 breast cancer samples hybridized onto the Affymetrix HG-U133A and HG-U133B plat-forms. Gene expression and clinical data are publicly available in the Gene Expression Omnibus archive [25] under accession Table 4 Experimental comparison between the number of incorrectly classified instances found on the test sets by GP and Support Vector Machine with exponent 2 on unbalanced datasets. GSE3494. Due to the difference in gene content between platforms, only 17 of the 50 best solutions found by GP could be applied to the new dataset. We then constructed, for each GP solution, a 2 × 2 contingency table comparing the GP prediction to the true outcome at 10 years and applied the exact Fisher test to the table. All 17 GP solutions showed statistically significant predictive power (P-values between 7.6 × 10 -3 and 2.9 × 10 -4 ). Since this result was obtained with no further training, it shows the robustness of the solutions obtained by GP with respect to the choice of dataset and microarray platform.</p>
<p>Assessment of Sensitivity</p>
<p>When using gene signatures to predict the survival of a cohort of breast cancer patients, one of the main goal in clinical applications is to minimize the number of false negative predictions. Table 5 summarizes the false negative predictions returned by each machine learning method on the 50 runs. The first line indicates the different methods, while the second and the third lines show the best (i.e. lowest) and mean performances (together with the corresponding SEM)values of incorrectly classified instances.</p>
<p>The best solutions were found by GP, and statistical analysis indicates that GP consistently outperforms the other five methods as it can be seen in Table 6. The difference between the various average results is statistically significant (P-value 2.75 × 10 -9 for ANOVA test on the 4 samples of solutions found by each method). Finally, pairwise 2-tailed Student t-tests comparing GP with each other method demonstrate its better performance.</p>
<p>The original scoring method of [2,9], and in particular the suggested cutoff of 0.4, was chosen in such a way as to minimize the number of false negatives. Therefore it is not surprising that in this respect the scoring method is far superior to all machine learning methods, including GP. Indeed the average number of false negatives given by the scoring method is 1.78, to be compared to the numbers reported in Table 6.</p>
<p>Maximizing Sensitivity in GP</p>
<p>It is well know that the fitness function driving the evolutionary dynamics in a GP framework can be modified in order to let emerge solutions with different characteristics. The results presented and discussed in the previous section were obtained with the goal of minimizing all incorrectly classified instances, summing both false negative and false positive predictions obtained by the solutions. However, when using gene signatures to predict the survival of a cohort of breast cancer patients, minimizing the number of false negative predictions is recognized as one of the most important goals. For all these reasons, we modified the GP fitness function so that false negatives (positives) are penalized more than errors of the other type, hoping to tune the algorithm towards better sensitivity (sensibility). In particular, solutions with greater sensitivity can emerge if larger weights are assigned to false negatives compared to false positives. In general, we can transform the fitness function in a weighted average of the form:
Fitness = 0.9 × FalseNegative + 0.1 × FalsePositive
With respect to this new formulation, the fitness function of the GP algorithm whose results were presented in the previous section can be expressed as 0.9 × FalseNegative + 0.1 × FalsePositive. The results of 50 runs of this new version of the GP technique showed an average of 16.04 (with SEM = 0.44) of total incorrectly classified instances. Compared with the performances of the previous GP algorithm, no statistically significant difference can be highlighted (Student t-test P = 0.50). When looking only at the number of false negative incorrectly classified instances, the average performance of 4.32 (SEM = 0.346) is better than the one of standard GP reported in table 6 (Student t-test P = 6.62 × 10 -16 ), even if still worse than that of the original scoring method.</p>
<p>Conclusions</p>
<p>The goal of our investigation was to refine the set of criteria that could lead to better risk stratification in breast cancer. To reach this goal we started from the well known "70-genes signature" and proceeded with the application of several machine learning schemes, in order to perform a comparison between them. We made some simplifying assumptions, preprocessed the data accordingly and ran several evaluation experiments.</p>
<p>Our results showed that while all the machine learning algorithms we used do have predictive power in classifying breast cancer patients into risk classes, GP clearly outperforms all other methods with the exception of SVM with polynomial kernel of degree 2, whose performance is not significantly different from GP. Of course there is no way to do such a comparison in a completely unbiased way, as one could always argue that the levels of optimization are uneven. To minimize the possible bias, we tried to use default implementation of all the methods.</p>
<p>The survival endpoint was initially chosen so as to produce a balanced dataset with the same number of samples in each outcome class. However this choice turned out not to be crucial to the good performance of GP since also on unbalanced datasets GP turned out to perform comparably to or better than the SVM. Moreover, the predictive solutions found by GP in a dataset turned out to be significantly predictive of survival also in another, independent dataset without any further training.</p>
<p>A unique characteristic of GP is its ability to perform automatic feature selection. We showed that the feature selection performed by GP was quite dramatic (the median number of features used by the best GP solution was~4 out of a total of 70 features available) and of intrinsic value, i.e. not necessarily tied to the use of syntax trees: indeed the performance of both GP and SVM significantly improved when run using only the features selected most often in a preliminary GP run. The improvement in performance shown by GP compared to the original scoring method was rather small and not statistically significant. As expected, the scoring method was superior to all machine learning algorithms in minimizing false negatives. In a second phase, we tried to enrich GP by changing its fitness function into a weighted average between false negatives and false positives. We showed that, when larger weight is given to false negatives, it is possible to tune the GP algorithm towards greater sensitivity. While the sensitivity of GP is still less than the original scoring method, the possibility of tuning the fitness function is another intrinsic advantage of this technique with respect to the other machine learning ones considered in this article.</p>
<p>Nevertheless we believe our results warrant further investigation into the use of GP in this context for at least three reasons:</p>
<p>• As stated above, our implementation of GP was purposely not optimized, and we can expect substantial improvements in performance from further work aimed at tuning the various GP parameters.</p>
<p>• Maybe more importantly, GP can potentially offer biological insight and generate hypotheses for experimental work (see also [8]). Indeed an important result of our analysis is that the trees produced by GP tend to contain a limited number of features, and therefore are easily interpretable in biological terms. For example, the bestperforming tree is shown in Figure 1 and includes 7 genes (features).</p>
<p>• Finally within the context of GP there is a natural way to tune the algorithm towards better sensitivity (specificity), simply by defining a fitness function in which false negatives (positives) are penalized more than errors of the other type.</p>
<p>Future work along these lines should therefore focus on both improving the performance of GP and interpreting the results from the biological point of view. An obvious first step towards optimization would be to abandon the binarization of the data (which here was used to produce trees that are easier to interpret) and build a GP based on continuous expression values. The biological interpretation might benefit from a statistical and functional analysis of the most recurring subtrees in optimal GP solutions. Figure 1 The best-fitness model. Tree representation and the traditional Lisp representation of the model with the best fitness found by GP over the studied 50 independent runs. In conclusion we have shown that Genetic Programming outperforms other machine learning methods as a tool to extract predictions from an established breast cancer gene signature. Given the possibility of generating biological insight and hypotheses that is intrinsic to the method, it deserves deeper investigation along the lines described above. Finally, it will be our task to test the GP approach on other features/ gene sets that account for other cancers or other diseases, always with the objective of providing clinicians with more precise and individualized diagnosis criteria.</p>
<p>Methods</p>
<p>The machine learning methods we considered are described here, with references to more detailed expositions.</p>
<p>Genetic Programming</p>
<p>Genetic Programming (GP) [26][27][28] is an evolutionary approach which extends Genetic Algorithms (GAs) [17,18] to the space of programs. Like any other evolutionary algorithm, GP works by defining a goal in the form of a quality criterion (or fitness) and then using this criterion to evolve a set (also called population) of solution candidates (also called individuals) by mimic the basic principles of Darwin's theory of evolution [29]. The most common version of GP, and also the one used here, considers individuals as abstract syntax tree structures 2 that can be built recursively from a set of function symbols ℱ = {f 1 , f 2 , ..., f n } (used to label internal tree nodes) and a set of terminal symbols T = {t 1 , t 2 , . . . , t m } (used to label tree leaves). GP breeds these solutions to solve problems by executing an iterative process involving the probabilistic selection of the fittest solutions and their variation by means of a set of genetic operators, usually crossover and mutation.</p>
<p>We used a tree-based GP configuration inspired by boolean problems introduced in [26]: each feature in the dataset was represented as a boolean value and thus our set of terminals T was composed by 70 boolean variables (i.e. one for each feature of our dataset). Potential solutions (GP individuals) were built using the set of boolean functions ℱ = {AND, OR, NOT}. The fitness function is the number of incorrectly classified instances, which turns the problem into a minimization one (lower values are better) 3 .</p>
<p>Finally no explicit feature selection strategy was employed, since we want to point out GP's ability to automatically perform an implicit feature selection. The mechanism allowing GP to perform feature selection, already pointed out for instance in [21,[30][31][32], is simple: GP searches over the space of all boolean expressions of 70 variables. This search space includes the expressions that use all the 70 variables, but also the ones that use a smaller number of variables. In principle there is no reason why an expression using a smaller number of variables could not have a better fitness value than an expression using all the 70 variables. If expressions using smaller number of variables have a better fitness, they survive into the population, given that fitness is the only principle used by GP for selecting genes. If it happens that GP finds expressions using a small number of variables with a better fitness value than the ones using all variables, the former expressions survive into the population, while the latter ones are extinguished.</p>
<p>The parameters used in our GP experiments are reported in Table 7, together with the parameters used by the other machine learning methods we studied. There is no particular justification for the choice of those parameter values, if not the fact that they are standard for the computational tool we used, i.e. GPLab: a public domain GP system implemented in MatLab (for the GPLab software and documentation, see [33]).</p>
<p>Support Vector Machines</p>
<p>Support Vector Machines (SVM) are a set of related supervised learning methods used for classification and regression. They were originally introduced in [34]. Their aim is to devise a computationally efficient way of identifying separating hyperplanes in a high dimensional feature space. In particular, the method seeks separating hyperplanes maximizing the margin between sets of data. This should ensure a good generalization ability of the method, under the hypothesis of consistent target function between training and testing data. To calculate the margin between data belonging to two different classes, two parallel hyperplanes are constructed, one on each side of the separating hyperplane, which are "pushed up against" the two data sets. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the neighboring data points of both classes, since in general the larger the margin the lower the generalization error of the classifier. The parameters of the maximum-margin hyperplane are derived by solving large quadratic programming (QP) optimization problems. There exist several specialized algorithms for quickly solving these problems that arise from SVMs, mostly reliant on heuristics for breaking the problem down into smaller, more manageable chunks. In this work we used the implementation of John Platt's [35]  sequential minimal optimization (SMO) algorithm for training the support vector classifier. SMO works by breaking the large QP problem into a series of smaller 2-dimensional sub-problems that may be solved analytically, eliminating the need for numerical optimization algorithms such as conjugate gradient methods. The implementation we used is the one contained in the Weka public domain software [36]. This implementation globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes by default (in that case the coefficients in the output are based on the normalized data, not the original data and this is important for interpreting the classifier).</p>
<p>The main parameter values used in this work are reported in Table 7. All these parameter values correspond to the standard values offered by the Weka software [36] and they are defined for instance in [35]. Being aware that in several application domains, SVM have been shown to outperform competing techniques by using nonlinear kernels, which implicitly map the instances to very high (even infinite) dimensional spaces, we used polynomials kernels with degrees 1, 2, and 3.</p>
<p>Multilayered Perceptron</p>
<p>Multilayered Perceptron is a feed-forward artificial neural network model [37]. It is a modification of the standard linear perceptron in that it uses three or more layers of neurons (nodes) with nonlinear activation functions, and is more powerful than simple perceptron in that it can distinguish data that are not linearly separable, or separable by a hyperplane. It consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes. Each node in one layer connects with a certain weight to every other node in the following layer. The implementation we have adopted is the one included in the Weka software distribution [36]. We used the Backpropagation learning algorithm [37] and the values used for all the parameters are reported in Table 7. As for the previously discussed machine learning methods, also in the case of Multilayered Perceptron it is important to point out that we used a parameter setting as standard as possible, without doing any fine parameter tuning for this particular application. Our goal is, in fact, to compare different computational methods under standard conditions and not to solve in the best possible way the application itself. In particular, all the values reported in Table 7 correspond to the default ones adopted by the Weka software.</p>
<p>Random Forests</p>
<p>Random Forests denotes an improved Classification and Regression Trees method [38]. It works by creating a large number of classification trees or regression trees. Every tree is built using a deterministic algorithm and the trees are different owing to two factors. First, at each node, a best split is chosen from a random subset of the predictors rather than from all of them. Secondly, every tree is built using a bootstrap sample of the observations. The out-of-bag data, approximately one-third of the observations, are then used to estimate the prediction accuracy. Unlike other tree algorithms, no pruning or trimming of the fully grown tree is involved. In this work we use the Breiman model presented in [39] and implemented in the Weka software [36]. As it can be seen from Table 7, this method, compared to the other ones, has the advantage of a smaller amount of parameter setting required. In order to allow a fair comparison with GP, we have considered random forests composed by 2500 trees (given that the GP population is composed by 500 trees and it runs for 5 generations, 2500 trees are globally inspected by GP too) and such that each node corresponds to exactly one feature (as it is for GP). All the other parameters eported in Table 7 were set to the standard values offered by the Weka software. Endnotes 1. As we will discuss later, Genetic Programming (GP) is the only method, among the ones studied in this paper, hat is able to perform automatically a further feature selection and thus identify small subsets of the original signature characterized by high predictive power.</p>
<ol>
<li>Traditionally represented in Lisp notation. 3. We are aware that, in case of minimization problems, the term "fitness" might be inappropriate, given that a fitness is usually a measure that has to be maximized. Nevertheless, we chose to use this term for simplicity.</li>
</ol>
<p>method was independently run 50 times using each time a different training/test partition of the validation dataset (see text for details). The first line indicates the method: Genetic Programming (GP), Support Vector Machine with exponent for the polynomial kernel 1.0 (SVM-K1), 2.0 (SVM-K2), and 3.0 (SVM-K3), Multilayer Perceptrons (MP), and Random Forest (RF). The second line shows the best value of the incorrectly classified instances obtained on the test set over the 50 runs, and the third line reports the average performances of each group of 50 runs on their test sets (standard error of mean is shown in parentheses).</p>
<p>SEM) 15.04 (0.41) 17.84 (0.42) 21.18 (0.49) 20.7 (0.46)The datasets are defined by survival status at endpoints 5 and 7.5 years.</p>
<p>Table 1
1Experimental comparison between the number of incorrectly classified instances found on the test sets by the different machine learning methods.GP 
SVM-K1 
SVM-K2 
SVM-K3 
MP 
RF </p>
<p>best 
10 
13 
14 
15 
10 
12 </p>
<p>average (SEM) 
16.40 (0.30) 
18.32 (0.37) 
16.76 (0.18) 
17.62 (0.17) 
18.08 (0.39) 
17.60 (0.35) </p>
<p>Table 2
2Statistical significance of the difference in performance between the methods.ANOVA 
P = 3.05 × 10 -5 </p>
<p>GP vs. SVM-K1 
P = 0.0001 </p>
<p>GP vs. SVM-K2 
P = 0.3107 </p>
<p>GP vs. SVM-K3 
P = 0.0008 </p>
<p>GP vs. MP 
P = 0.0009 </p>
<p>GP vs. RF 
P = 0.0103 </p>
<p>First line shows ANOVA test on the 6 samples of solutions found by each method, while second line depicts pairwise 2-
tailed Student t-tests comparing GP with each other method. </p>
<p>Table 3
3The 10 most recurring features in the solutions found by GP.The four columns show: accession ID, gene name, gene description, and number of solutions where that feature occurs.Accession ID Gene name Gene description 
Solutions </p>
<p>NM_003981 
PRC1 protein regulator of cytokinesis 1 
48 </p>
<p>NM_002916 
RFC4 replication factor C (activator 1) 4, 37 kDa 
23 </p>
<h2>AI992158</h2>
<p>16 </p>
<h2>AI554061</h2>
<p>10 </p>
<p>NM_006101 
NDC80 NDC80 homolog, kinetochore complex component (S. cerevisiae) 
9 </p>
<p>NM_015984 
UCHL5 ubiquitin carboxyl-terminal hydrolase L5 
7 </p>
<p>NM_020188 
C16orf61 chromosome 16 open reading frame 61 
6 </p>
<p>NM_016448 
DTL denticleless homolog (Drosophila) 
6 </p>
<p>NM_014791 
MELK maternal embryonic leucine zipper kinase 
6 </p>
<h2>NM_004702</h2>
<p>6 </p>
<p>Vanneschi et al. BioData Mining 2011, 4:12 
http://www.biodatamining.org/content/4/1/12 </p>
<p>Table 5
5Experimental comparison between the number of false negatives found on the test sets by the different machine learning methods. RF). The second line shows the best value of the incorrectly classified instances obtained on the test set over the 50 runs, and the third line reports the average performances of each group of 50 runs on their test sets (standard error of mean is shown in parentheses).GP 
SVM-K1 
SVM-K2 
SVM-K3 
MP 
RF </p>
<p>best 
2 
6 
6 
6 
5 
6 </p>
<p>average (SEM) 
9.82 (0.44) 
13.26 (0.51) 
12.60 (0.35) 
14.08 (0.39) 
12.88 (0.51) 
13.38 (0.49) </p>
<p>Each method was independently run 50 times using each time a different training/test partition of the validation dataset 
(see text for details). The first line indicates the method: Genetic Programming (GP), Support Vector Machine (SVM), 
Multilayer Perceptrons (MP), and Random Forest (Vanneschi et al. BioData Mining 2011, 4:12 
http://www.biodatamining.org/content/4/1/12 </p>
<p>Table 6
6False negative prediction: statistical significance of the difference in performance between the methods.ANOVA 
P = 2.75 × 10 -9 </p>
<p>GP vs. SVM-K1 
GP vs. SVM-K2 
GP vs. SVM-K3 
GP vs. MP 
GP vs. RF </p>
<p>P = 2.74 × 10 -6 
P = 3.32 × 10 -6 
P = 1.27 × 10 -10 
P = 8.53 × 10 -6 
P = 4.65 × 10 -7 </p>
<p>First line shows ANOVA test on the 6 samples of solutions found by each method, while second line depicts pairwise 2-
tailed Student t-tests comparing GP with each other method. </p>
<p>Vanneschi et al. BioData Mining 2011, 4:12 
http://www.biodatamining.org/content/4/1/12 </p>
<p>Table 7
7Parameters used in the experiments.GP Parameters </p>
<p>population size 
500 individuals </p>
<p>population initialization 
ramped half and half [26] </p>
<p>selection method 
tournament (tournament size = 10) </p>
<p>crossover rate 
0.9 </p>
<p>mutation rate 
0.1 </p>
<p>maximum number of generations 
5 </p>
<p>algorithm 
generational tree based GP with no elitism </p>
<p>SVM Parameters </p>
<p>complexity parameter 
0.1 </p>
<p>size of the kernel cache 
10 7 </p>
<p>epsilon value for the round-off error 
10 -12 </p>
<p>exponent for the polynomial kernel 
1.0,2.0, 3.0 </p>
<p>tolerance parameter 
0.001 </p>
<p>Multilayered Perceptron Parameters </p>
<p>learning algorithm 
Back-propagation </p>
<p>learning rate 
0:03 </p>
<p>activation function for all the neurons in the net 
sigmoid </p>
<p>momentum 
0.2 progressively decreasing until 0.0001 </p>
<p>hidden layers 
(number of attributes + number of classes)/2 </p>
<p>number of epochs of training 
500 </p>
<p>Random Forest Parameters </p>
<p>number of trees 
2500 </p>
<p>number of attributes per node 
1 </p>
<p>Authors' contributions LV, PP, and MG conceived the design of the study, participated in the interpretation of the results, and coordinated the participants' contributions. AF participated in the design of the study, implemented the computational model and carried out the simulations. GM and MA participated in the design of the study. All authors equally participated in writing the manuscript and approved it.Competing interestsThe authors declare that they have no competing interests.
Mining gene expression profiles: expression signatures as cancer phenotypes. J R Nevins, A Potti, Nat Rev Genet. 88Nevins JR, Potti A: Mining gene expression profiles: expression signatures as cancer phenotypes. Nat Rev Genet 2007, 8(8):601-609.</p>
<p>Gene expression profiling predicts clinical outcome of breast cancer. L J Van &apos;t Veer, H Dai, M J Van De Vijver, Y D He, Aam Hart, M Mao, H L Peterse, K Van Der Kooy, M J Marton, A T Witteveen, G J Schreiber, R M Kerkhoven, C Roberts, P S Linsley, R Bernards, S H Friend, Nature. 4156871van 't Veer LJ, Dai H, van de Vijver MJ, He YD, Hart AAM, Mao M, Peterse HL, van der Kooy K, Marton MJ, Witteveen AT, Schreiber GJ, Kerkhoven RM, Roberts C, Linsley PS, Bernards R, Friend SH: Gene expression profiling predicts clinical outcome of breast cancer. Nature 2002, 415(6871):530-536.</p>
<p>Applications of support vector machines to cancer classification with microarray data. F Chu, L Wang, Int J Neural Syst. 156Chu F, Wang L: Applications of support vector machines to cancer classification with microarray data. Int J Neural Syst 2005, 15(6):475-484.</p>
<p>Reliable classification of two-class cancer data using evolutionary algorithms. K Deb, A R Reddy, Biosystems. 721-2Deb K, Reddy AR: Reliable classification of two-class cancer data using evolutionary algorithms. Biosystems 2003, 72(1-2):111-129.</p>
<p>Evolutionary algorithms for finding optimal gene sets in microarray prediction. J M Deutsch, Bioinformatics. 19Deutsch JM: Evolutionary algorithms for finding optimal gene sets in microarray prediction. Bioinformatics 2003, 19:45-52.</p>
<p>Genetic Programming for Mining DNA Chip Data from Cancer Patients. Genetic Programming and Evolvable Machines. W B Langdon, B F Buxton, 5Langdon WB, Buxton BF: Genetic Programming for Mining DNA Chip Data from Cancer Patients. Genetic Programming and Evolvable Machines 2004, 5(3):251-257.</p>
<p>Gene selection for classification of cancers using probabilistic model building genetic algorithm. T K Paul, H Iba, Biosystems. 823Paul TK, Iba H: Gene selection for classification of cancers using probabilistic model building genetic algorithm. Biosystems 2005, 82(3):208-225.</p>
<p>Feature Selection and Molecular Classification of Cancer Using Genetic Programming. J Yu, J Yu, A A Almal, S M Dhanasekaran, D Ghosh, W P Worzel, A M Chinnaiyan, Neoplasia. 94Yu J, Yu J, Almal AA, Dhanasekaran SM, Ghosh D, Worzel WP, Chinnaiyan AM: Feature Selection and Molecular Classification of Cancer Using Genetic Programming. Neoplasia 2007, 9(4):292-303.</p>
<p>Bernards R: A gene-expression signature as a predictor of survival in breast cancer. M J Van De Vijver, Y D He, L J Van&apos;t Veer, H Dai, Aam Hart, D W Voskuil, G J Schreiber, J L Peterse, C Roberts, M J Marton, M Parrish, D Atsma, A Witteveen, A Glas, L Delahaye, T Van Der Velde, H Bartelink, S Rodenhuis, E T Rutgers, S H Friend, N Engl J Med. 34725van de Vijver MJ, He YD, van't Veer LJ, Dai H, Hart AAM, Voskuil DW, Schreiber GJ, Peterse JL, Roberts C, Marton MJ, Parrish M, Atsma D, Witteveen A, Glas A, Delahaye L, van der Velde T, Bartelink H, Rodenhuis S, Rutgers ET, Friend SH, Bernards R: A gene-expression signature as a predictor of survival in breast cancer. N Engl J Med 2002, 347(25):1999-2009.</p>
<p>Cancer classification using gene expression data. Y Lu, J Han, Inf Syst. 284Lu Y, Han J: Cancer classification using gene expression data. Inf Syst 2003, 28(4):243-268.</p>
<p>. Vanneschi, BioData Mining. 412Vanneschi et al. BioData Mining 2011, 4:12</p>
<p>Machine learning, neural and statistical classification Prentice Hall. D Michie, D Spiegelhalter, C Taylor, Michie D, Spiegelhalter D, Taylor C: Machine learning, neural and statistical classification Prentice Hall; 1994.</p>
<p>Broad patterns of gene expression revealed by clustering analysis of tumour and normal colon tissues probed by oligonucleotide arrays. U Alon, N Barkai, D Notterman, K Gish, S Ybarra, D Mack, A J Levine, Proc Nat Acad Sci. 96Alon U, Barkai N, Notterman D, Gish K, Ybarra S, Mack D, Levine AJ: Broad patterns of gene expression revealed by clustering analysis of tumour and normal colon tissues probed by oligonucleotide arrays. Proc Nat Acad Sci USA 1999, 96:6745-6750.</p>
<p>An unsupervised hierarchical dynamic self-organizing approach to cancer class discovery and marker gene identification in microarray data. A Hsu, S Tang, S Halgamuge, Bioinformatics. 1916Hsu A, Tang S, Halgamuge S: An unsupervised hierarchical dynamic self-organizing approach to cancer class discovery and marker gene identification in microarray data. Bioinformatics 2003, 19(16):2131-40.</p>
<p>Gene selection for cancer classification using support vector machines. I Guyon, J Weston, S Barnhill, V Vapnik, Machine Learning. 46Guyon I, Weston J, Barnhill S, Vapnik V: Gene selection for cancer classification using support vector machines. Machine Learning 2002, 46:389-422.</p>
<p>A genetic embedded approach for gene selection and classification of microarray data. Jch Hernandez, B Duval, J Hao, Lecture Notes in Computer Science. 4447Hernandez JCH, Duval B, Hao J: A genetic embedded approach for gene selection and classification of microarray data. Lecture Notes in Computer Science 2007, 4447:90-101.</p>
<p>Using Bayesian Networks to Analyze Expression Data. N Friedman, M Linial, I Nachmann, D Peer, J Computational Biology. 7Friedman N, Linial M, Nachmann I, Peer D: Using Bayesian Networks to Analyze Expression Data. J Computational Biology 2000, 7:601-620.</p>
<p>J H Holland, Adaptation in Natural and Artificial Systems. Ann Arbor, MichiganThe University of Michigan PressHolland JH: Adaptation in Natural and Artificial Systems Ann Arbor, Michigan: The University of Michigan Press; 1975.</p>
<p>Genetic Algorithms in Search, Optimization and Machine Learning Addison-Wesley. D E Goldberg, Goldberg DE: Genetic Algorithms in Search, Optimization and Machine Learning Addison-Wesley; 1989.</p>
<p>Multiclass cancer classification and biomarker discovery using GA-based algorithms. J Liu, G Cutler, W Li, Z Pan, S Peng, T Hoey, L Chen, X B Ling, Bioinformatics. 21Liu J, Cutler G, Li W, Pan Z, Peng S, Hoey T, Chen L, Ling XB: Multiclass cancer classification and biomarker discovery using GA-based algorithms. Bioinformatics 2005, 21:2691-2697.</p>
<p>Symbolic discriminant analysis for mining gene expression patterns. J Moore, J Parker, L Hahn, Lecture Notes in Artificial Intelligence. 2167Moore J, Parker J, Hahn L: Symbolic discriminant analysis for mining gene expression patterns. Lecture Notes in Artificial Intelligence 2001, 2167:372-381.</p>
<p>Genetic Programming based DNA Microarray Analysis for classification of tumour tissues. M Rosskopf, H Schmidt, U Feldkamp, W Banzhaf, 2007-03Memorial University of NewfoundlandTechnical ReportRosskopf M, Schmidt H, Feldkamp U, Banzhaf W: Genetic Programming based DNA Microarray Analysis for classification of tumour tissues. Tech. Rep. Technical Report 2007-03, Memorial University of Newfoundland; 2007.</p>
<p>Data mining with constrained-syntax genetic programming: applications to medical data sets. C Bojarczuk, H Lopes, A Freitas, Proceedings Intelligent Data Analysis in Medicine and Pharmacology. Intelligent Data Analysis in Medicine and Pharmacology1Bojarczuk C, Lopes H, Freitas A: Data mining with constrained-syntax genetic programming: applications to medical data sets. Proceedings Intelligent Data Analysis in Medicine and Pharmacology 2001, 1.</p>
<p>The classification of cancer based on DNA microarray data that uses diverse ensemble genetic programming. J Hong, S Cho, Artif Intell Med. 36Hong J, Cho S: The classification of cancer based on DNA microarray data that uses diverse ensemble genetic programming. Artif Intell Med 2006, 36:43-58.</p>
<p>An expression signature for p53 status in human breast cancer predicts mutation status, transcriptional effects, and patient survival. L D Miller, J Smeds, J George, V B Vega, L Vergara, A Ploner, Y Pawitan, P Hall, S Klaar, E T Liu, J Bergh, Proc Natl Acad Sci. 10238Miller LD, Smeds J, George J, Vega VB, Vergara L, Ploner A, Pawitan Y, Hall P, Klaar S, Liu ET, Bergh J: An expression signature for p53 status in human breast cancer predicts mutation status, transcriptional effects, and patient survival. Proc Natl Acad Sci USA 2005, 102(38):13550-13555.</p>
<p>NCBI GEO: mining tens of millions of expression profiles-database and tools update. T Barrett, D B Troup, S E Wilhite, P Ledoux, D Rudnev, C Evangelista, I F Kim, A Soboleva, M Tomashevsky, R Edgar, Nucleic Acids Res. Barrett T, Troup DB, Wilhite SE, Ledoux P, Rudnev D, Evangelista C, Kim IF, Soboleva A, Tomashevsky M, Edgar R: NCBI GEO: mining tens of millions of expression profiles-database and tools update. Nucleic Acids Res 2007, , 35 Database: D760-D765.</p>
<p>J R Koza, Genetic Programming. Cambridge, MassachusettsThe MIT PressKoza JR: Genetic Programming Cambridge, Massachusetts: The MIT Press; 1992.</p>
<p>A field guide to genetic programming. R Poli, W B Langdon, N F Mcphee, With contributions by J. R. Koza)Poli R, Langdon WB, McPhee NF: A field guide to genetic programming , Published via http://lulu.com and freely available at http://www.gp-field-guide.org.uk 2008. [(With contributions by J. R. Koza)].</p>
<p>Vanneschi L: Theory and Practice for Efficient Genetic Programming. Lausanne, SwitzerlandFaculty of Sciences University ofPh.D. thesisVanneschi L: Theory and Practice for Efficient Genetic Programming. Ph.D. thesis, Faculty of Sciences University of Lausanne, Switzerland; 2004.</p>
<p>On the Origin of Species by Means of Natural Selection John Murray. C Darwin, 1859Darwin C: On the Origin of Species by Means of Natural Selection John Murray; 1859.</p>
<p>Genetic programming for human oral bioavailability of drugs. F Archetti, S Lanzeni, E Messina, L Vanneschi, M Cattolico, Proceedings of the 8th annual conference on Genetic and Evolutionary Computation. the 8th annual conference on Genetic and Evolutionary ComputationSeattle, Washington, USAArchetti F, Lanzeni S, Messina E, Vanneschi L: Genetic programming for human oral bioavailability of drugs. In Proceedings of the 8th annual conference on Genetic and Evolutionary Computation. Edited by: Cattolico M et al. Seattle, Washington, USA; 2006:255-262.</p>
<p>Genetic Programming and other Machine Learning approaches to predict Median Oral Lethal Dose (LD50) and Plasma Protein Binding levels (%PPB) of drugs. F Archetti, E Messina, S Lanzeni, L Vanneschi, E Marchiori, Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics. Proceedings of the Fifth European Conference. Berlin, Heidelberg, New York4447LNCSArchetti F, Messina E, Lanzeni S, Vanneschi L: Genetic Programming and other Machine Learning approaches to predict Median Oral Lethal Dose (LD50) and Plasma Protein Binding levels (%PPB) of drugs. In Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics. Proceedings of the Fifth European Conference, EvoBIO 2007, Lecture Notes in Computer Science, LNCS 4447. Edited by: Marchiori E et al. Springer, Berlin, Heidelberg, New York; 2007:11-23.</p>
<p>Genetic Programming for Computational Pharmacokinetics in Drug Discovery and Development. Genetic Programming and Evolvable Machines. F Archetti, E Messina, S Lanzeni, L Vanneschi, 8Archetti F, Messina E, Lanzeni S, Vanneschi L: Genetic Programming for Computational Pharmacokinetics in Drug Discovery and Development. Genetic Programming and Evolvable Machines 2007, 8(4):17-26.</p>
<p>GPLAB -A Genetic Programming Toolbox for MATLAB, version 3.0. S Silva, Silva S: GPLAB -A Genetic Programming Toolbox for MATLAB, version 3.0. 2007.</p>
<p>V Vapnik, Statistical Learning Theory Wiley. New York, NYVapnik V: Statistical Learning Theory Wiley, New York, NY; 1998.</p>
<p>Fast Training of Support Vector Machines using Sequential Minimal Optimization. J Platt, Advances in Kernel Methods -Support Vector Learning. Platt J: Fast Training of Support Vector Machines using Sequential Minimal Optimization. Advances in Kernel Methods -Support Vector Learning 1998.</p>
<p>Weka: A multi-task machine learning software developed by Waikato University. Weka: A multi-task machine learning software developed by Waikato University. 2006, [See http://www.cs.waikato.ac. nz/ml/weka].</p>
<p>S Haykin, Neural Networks: a comprehensive foundation. Prentice Hall, LondonHaykin S: Neural Networks: a comprehensive foundation Prentice Hall, London; 1999.</p>
<p>L Breiman, J Friedman, R Olshen, C Stone, Classification and Regression Trees. Belmont, California, Wadsworth International GroupBreiman L, Friedman J, Olshen R, Stone C: Classification and Regression Trees Belmont, California, Wadsworth International Group; 1984.</p>
<p>L Breiman, 10.1186/1756-0381-4-12Random Forests. 45Breiman L: Random Forests. Machine Learning 2001, 45:5-32. doi:10.1186/1756-0381-4-12</p>
<p>A comparison of machine learning techniques for survival prediction in breast cancer. Vanneschi, BioData Mining. 412Cite this article asCite this article as: Vanneschi et al.: A comparison of machine learning techniques for survival prediction in breast cancer. BioData Mining 2011 4:12.</p>            </div>
        </div>

    </div>
</body>
</html>
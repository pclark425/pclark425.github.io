<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1723 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1723</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1723</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-256389594</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2301.12507v2.pdf" target="_blank">Distilling Internet-Scale Vision-Language Models into Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1723.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1723.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flamingo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flamingo: a visual language model for few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large generative vision-language model that accepts interleaved images and text and produces natural-language outputs; used here as a relabeling function to retroactively describe agent behaviors for HER-style training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Flamingo: a visual language model for few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Flamingo (80B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Generative vision-language model (80B parameter variant used in this paper) that conditions on interleaved images and text to produce free-form text; supports zero-shot and few-shot prompting and returns confidence-calibrated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Internet-scale image-text (vision-language) data with natural-language captions</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Described as the Alayrac et al. (2022) Flamingo model; the paper states they used the 80B-parameter Flamingo model and that it was pretrained on large internet image-text data (exact dataset sizes and splits are not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playhouse "Lift" task</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A Unity-based Playhouse environment (procedurally-generated domestic scenes) where episodes place the agent in a room with 5–10 objects and the instruction is to lift a target object; variants tested include lifting by object name, by color (attribute), by category (food vs toy), and by user preference (ad-hoc categories). Episodes end on lift or after 120s.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for Flamingo pretraining — Flamingo is conditioned on images and text captions (natural language), not actions; pretraining does not include an explicit action space.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous low-level motor controls in the Unity Playhouse environment; the embodied agent uses a task-agnostic motor policy (learned via behavioral cloning) capable of lifting, with actions represented as continuous control commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Indirect: Flamingo is used as a relabeling function for hindsight experience replay (HER). It generates natural-language descriptions for final observations; those text labels are used as instructions in imitation learning (behavioral cloning) to train the policy to map language instructions to the agent's low-level motor actions. There is no explicit semantic-to-action module—mapping is achieved implicitly by retraining on relabeled trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Egocentric RGB visual observations from the Unity Playhouse renderer; close-up and unusual viewpoints occur, creating domain-shift challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Varies by task and prompting: Object-name (10-object set) Zeroshot Flamingo relabel → retrained agent success = 64.4% (versus Oracle 97.9%); for a 5-object subset, Name task = 83.0% and Color task = 80.6%; Category (food vs toy) Zeroshot = 60.8%, Fewshot-1 = 73.3%, Fewshot-2 = 82.8%, plateau ≈ 85%; Ad-hoc preferences: Aligned = 76.7%, Arbitrary = 64.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Original agent (no VLM relabeling) baseline: object-names ≈ 7.1% success; name/color tasks baseline ≈ 19% success; category baseline ≈ 46.0%. Oracle retraining using environment ground-truth labels on the same ~10k trajectories reached near-ceiling performance (e.g., 97.9% on object names).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Experiments used an initial set of ≈10,000 trajectories produced by the original motor policy; Flamingo relabeled the final image of each trajectory and retraining was done via behavioral cloning on truncated trajectories (last 5s) derived from these ≈10k examples. Evaluations used 10,000 rollouts per test condition.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Oracle experiments used the same ≈10,000 trajectories but annotated with environment ground-truth labels (achieving near-ceiling performance). The original agent was trained by behavioral cloning on a human-human dataset (Interactive Agents Team, 2021) but the size/number of examples for that BC pretraining is not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Using Flamingo to relabel the same ≈10k unlabeled/generic trajectories produced large downstream gains over the original (e.g., object-name success from 7.1% → 64.4%) with no additional environment interactions; relative to oracle labels performance was lower, so Flamingo reduced the need for costly human/environmental labeling but did not fully match ground-truth supervision. The paper does not report a multiplicative sample-reduction factor relative to alternate pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key contributors: (1) Flamingo's internet-scale, domain-general language-vision grounding that generalizes to many object concepts; (2) ability to control relabeling via prompt engineering (zero-shot and few-shot); (3) few-shot examples to mitigate domain shift to 3D rendered visuals; (4) using language as an interpretable supervision signal for imitation learning (HER-style relabeling).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations included domain shift between Flamingo's pretraining images and Playhouse renderings, noisy or extraneous VLM outputs, and the fact that few-shot prompting can induce incorrect task-relevant labels (reducing label precision) which harms downstream learning. Detection-based alternatives (e.g., OWL-ViT) suffered from domain shift and required predefined label sets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative vision-language models pretrained on internet image-text data can be repurposed as relabelers to teach embodied agents language-grounded behaviors from a modest offline dataset (~10k trajectories); label precision is more predictive of downstream HER performance than raw label accuracy; few-shot prompting helps adapt to rendered visuals but may reduce precision unless filtered by confidence; Flamingo relabeling substantially improves task success relative to an ungrounded policy but does not fully match oracle ground-truth supervision.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1723.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OWL-ViT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OWL-ViT (Open-vocabulary detection model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-vocabulary object detection model (contrastive/detection approach) used here as an alternative relabeler; requires a predefined list of candidate labels and outputs detections with confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simple open-vocabulary object detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OWL-ViT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Open-vocabulary detection architecture (Minderer et al., 2022) that produces bounding-box detections and label confidences given candidate labels; not generative text but returns selected labels from a specified vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Object-detection and open-vocabulary detection datasets (detection supervision), not described in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper references Minderer et al. (2022) OWL-ViT; specifics of datasets used for OWL-ViT pretraining are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playhouse "Lift" (relabeling comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same Unity Playhouse Lift trajectories: OWL-ViT was supplied with the list of 10 object names and used to detect/predict which object was held in final images to produce labels for retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A — OWL-ViT is a vision detector trained on images and label vocabularies, not a text-action model.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Same continuous motor control action space of Playhouse agent (no change); OWL-ViT only provided labels used for imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Detect-then-label: OWL-ViT was given the 10 candidate object names and the top-confidence detection was used as the relabel for the trajectory (no free-form text generation).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images from the final observation; detector expected real-world-like visual statistics and suffered domain shift on Unity renders.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>On a subset of 300 trajectories OWL-ViT correctly predicted the held object only 7.2% of the time; overall OWL-ViT predictions had very low precision and only covered a small subset of object labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Flamingo zero-shot predictions were 54.8% accuracy on the same subset (reported in paper) and substantially outperformed OWL-ViT; Oracle ground-truth yields near-ceiling when used to retrain.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Evaluated on a subset of 300 trajectories for the OWL-ViT comparison; full retraining experiments were not reported for OWL-ViT due to poor label quality.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>N/A in this direct comparison beyond the 300-trajectory evaluation subset.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>None observed; OWL-ViT's poor relabel quality made it ineffective as a relabeler in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Detection-based method would be aided when target visuals align with detector's training data and when candidate label vocabulary matches appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Severe domain shift from real-world training images to Unity renders, requirement to pre-specify candidate labels, and inability to produce free-form task-appropriate language labels caused very poor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrastive/detection approaches like OWL-ViT can fail under domain shift in rendered 3D environments and are less flexible than generative VLM relabelers; in this paper OWL-ViT produced very low accuracy and precision compared to Flamingo.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1723.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive vision-language model trained on image-text pairs used in the literature to create semantic visual representations and occasionally integrated into embodied agents' vision encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Contrastive VLM trained to align images and text embeddings; commonly used to construct semantic visual features or as a frozen encoder in downstream embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale image-text pairs from the web (contrastive pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Referenced as Radford et al. (2021) CLIP; this paper notes CLIP has been used to drive exploration, construct semantic representations, or be integrated into embodied agents, but details of datasets/size are not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mentioned as prior work integrating pretrained vision encoders into embodied agents (e.g., for exploration or representation), but this paper does not run CLIP-transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images (visual encoder usage).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>CLIP-like models provide semantic visual features from large image-text pretraining, which can support generalization in embodied tasks when representations capture task-relevant properties.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretrained visual encoders may not encode task-relevant geometric or spatial details (e.g., fine spatial positions) and can introduce biases; representation vectors are sometimes hard to adapt for specific downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrastive VLMs like CLIP offer strong semantic representations but can miss task-relevant information for embodied control; this motivates using generative VLMs for interpretable language supervision in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1723.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale multi-modal/language models (GPT-4 cited) are mentioned as capable models comparable to Flamingo for generating language-conditioned supervision, though not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Large language (and multi-modal) model from OpenAI; mentioned as a capable generative model that could substitute for Flamingo for similar relabeling roles.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale internet text and multi-modal datasets (paper references GPT-4 technical report); specifics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Cited as OpenAI (2023) GPT-4 technical report; no dataset sizes or details are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mentioned as a possible comparable generative VLM that could be used for relabeling or producing text observations for embodied agents; no experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language modeling (no action space in pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong language grounding and generative capabilities could enable flexible relabeling and instruction generation for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Model size and compute may limit direct use in low-level motor control; domain shift and multi-modality mismatch remain considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper notes GPT-4 as a comparable publicly-released model that could replicate Flamingo's role, but provides no experimental evidence in this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1723.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prismer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prismer: A vision-language model with an ensemble of experts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent vision-language model cited as an open-source alternative with comparable capabilities to Flamingo for multi-modal tasks; mentioned but not used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prismer: A vision-language model with an ensemble of experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Prismer</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Vision-language model composed of an ensemble of experts designed for multi-modal understanding; referenced as an alternative to Flamingo for similar relabeling or captioning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language/image-text pretraining (paper cites Liu et al., 2023); specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Referenced as an open-source model with capabilities comparable to Flamingo; no dataset sizes or training details given here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mentioned as a potential substitute for Flamingo for relabeling or generating language supervision for embodied agents; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Ensemble-of-experts architecture and multi-modal pretraining might afford robust relabeling and captioning across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain shift to rendered environments and model availability/size may constrain direct use.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prismer is cited as a potentially usable VLM for similar distillation approaches, but no experiments or metrics are provided here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1723.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R3M: A universal visual representation for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model for universal visual representations in robot manipulation, cited as related work that pretrains visual features useful for robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R3M: A universal visual representation for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pretrained visual representation intended to transfer to robot manipulation tasks; mentioned in the related work as an example of pretrained visual models used in robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Visual/video or image datasets relevant to manipulation (paper reference only; details not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Cited as Nair et al. (2022) R3M; this paper does not provide dataset sizes or pretraining specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mentioned as prior work producing transferable visual features for manipulation; not experimentally evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB (and possibly video) visual inputs for robotics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained visual representations can accelerate robot learning when they capture task-relevant features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretrained features may lack task-relevant geometry/affordance details depending on pretraining data and objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>R3M is cited as an example of leveraging pretrained visual models for robot tasks; paper does not report experiments using R3M.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1723.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MineDojo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Building open-ended embodied agents with internet-scale knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform and effort to create open-ended embodied agents that leverage internet-scale knowledge; cited as related work showing internet data can bootstrap embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Building open-ended embodied agents with internet-scale knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>MineDojo</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A system/framework that trains agents using large-scale internet knowledge to enable open-ended embodied behavior (cited as Fan et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Internet-scale knowledge, including language and multi-modal web data (paper reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Referenced as Fan et al. (2022) MineDojo; no dataset/size details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mentioned as an example of using internet-scale pretraining for embodied agents; not directly evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Access to internet-scale knowledge and large multimodal corpora can provide broad semantic grounding useful for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Bridging the gap between passive internet data and agent-specific embodied representations remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MineDojo is cited as related work illustrating large-scale internet-derived knowledge for embodied agents; no direct experiments in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1723.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent effort to use large pretrained models of language, vision and action for robotic navigation; cited as related work demonstrating pretrained models applied to embodied navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robotic navigation with large pre-trained models of language, vision, and action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LM-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>System leveraging large pretrained models that combine language, vision, and action priors to perform robotic navigation tasks (referenced Shah et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained language, vision, and action models (paper reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Referenced as Shah et al. (2022) LM-Nav; details on pretraining corpora are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mentioned as an example of transferring pretrained multi-modal models to robotic navigation; not experimentally evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Combining learned priors across language, vision, and action can provide useful inductive biases for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Model scale and mismatch between pretraining tasks and embodied dynamics can complicate direct transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LM-Nav is cited as an approach for applying large pretrained models to navigation, showing the paper's broader context of leveraging pretrained models for embodied tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1723.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1723.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xiao et al. (2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic skill acquisition via instruction augmentation with vision-language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that fine-tuned a contrastive vision-language model with many human-relabeled trajectories to enable relabeling-based supervision; cited as related HER/relabeling work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robotic skill acquisition via instruction augmentation with vision-language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Contrastive VLM fine-tuned for instruction selection (Xiao et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Previous approach trained a domain-specific relabeling model / fine-tuned a contrastive VLM on thousands of human-relabeled trajectories to select best instructions for relabeling.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Contrastive image-text pretraining followed by domain-specific fine-tuning on human-relabeled trajectories (per Xiao et al. description in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper states Xiao et al. (2022) used thousands of human-relabeled trajectories to fine-tune a contrastive VLM; exact dataset sizes and compositions are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Referenced as a HER-style relabeling method that requires extensive human relabeling to produce usable relabelers for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Fine-tuned contrastive scoring to choose best instruction from candidates for relabeling (per prior work; contrasted with the generative free-form relabeling used here).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Fine-tuning on human-relabeled trajectories can produce reliable relabelers for HER at the cost of significant human labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Requires thousands of human-relabeled trajectories and is less flexible than free-form generative relabeling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an alternative relabeling approach that is more data/human-label intensive compared to the generative VLM relabeling proposed in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Flamingo: a visual language model for few-shot learning <em>(Rating: 2)</em></li>
                <li>Simple open-vocabulary object detection <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Robotic skill acquisition via instruction augmentation with vision-language models <em>(Rating: 2)</em></li>
                <li>R3M: A universal visual representation for robot manipulation <em>(Rating: 2)</em></li>
                <li>Building open-ended embodied agents with internet-scale knowledge <em>(Rating: 2)</em></li>
                <li>Robotic navigation with large pre-trained models of language, vision, and action <em>(Rating: 2)</em></li>
                <li>Prismer: A vision-language model with an ensemble of experts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1723",
    "paper_id": "paper-256389594",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "Flamingo",
            "name_full": "Flamingo: a visual language model for few-shot learning",
            "brief_description": "A large generative vision-language model that accepts interleaved images and text and produces natural-language outputs; used here as a relabeling function to retroactively describe agent behaviors for HER-style training.",
            "citation_title": "Flamingo: a visual language model for few-shot learning",
            "mention_or_use": "use",
            "model_agent_name": "Flamingo (80B)",
            "model_agent_description": "Generative vision-language model (80B parameter variant used in this paper) that conditions on interleaved images and text to produce free-form text; supports zero-shot and few-shot prompting and returns confidence-calibrated outputs.",
            "pretraining_data_type": "Internet-scale image-text (vision-language) data with natural-language captions",
            "pretraining_data_details": "Described as the Alayrac et al. (2022) Flamingo model; the paper states they used the 80B-parameter Flamingo model and that it was pretrained on large internet image-text data (exact dataset sizes and splits are not specified in this paper).",
            "embodied_task_name": "Playhouse \"Lift\" task",
            "embodied_task_description": "A Unity-based Playhouse environment (procedurally-generated domestic scenes) where episodes place the agent in a room with 5–10 objects and the instruction is to lift a target object; variants tested include lifting by object name, by color (attribute), by category (food vs toy), and by user preference (ad-hoc categories). Episodes end on lift or after 120s.",
            "action_space_text": "N/A for Flamingo pretraining — Flamingo is conditioned on images and text captions (natural language), not actions; pretraining does not include an explicit action space.",
            "action_space_embodied": "Continuous low-level motor controls in the Unity Playhouse environment; the embodied agent uses a task-agnostic motor policy (learned via behavioral cloning) capable of lifting, with actions represented as continuous control commands.",
            "action_mapping_method": "Indirect: Flamingo is used as a relabeling function for hindsight experience replay (HER). It generates natural-language descriptions for final observations; those text labels are used as instructions in imitation learning (behavioral cloning) to train the policy to map language instructions to the agent's low-level motor actions. There is no explicit semantic-to-action module—mapping is achieved implicitly by retraining on relabeled trajectories.",
            "perception_requirements": "Egocentric RGB visual observations from the Unity Playhouse renderer; close-up and unusual viewpoints occur, creating domain-shift challenges.",
            "transfer_successful": true,
            "performance_with_pretraining": "Varies by task and prompting: Object-name (10-object set) Zeroshot Flamingo relabel → retrained agent success = 64.4% (versus Oracle 97.9%); for a 5-object subset, Name task = 83.0% and Color task = 80.6%; Category (food vs toy) Zeroshot = 60.8%, Fewshot-1 = 73.3%, Fewshot-2 = 82.8%, plateau ≈ 85%; Ad-hoc preferences: Aligned = 76.7%, Arbitrary = 64.2%.",
            "performance_without_pretraining": "Original agent (no VLM relabeling) baseline: object-names ≈ 7.1% success; name/color tasks baseline ≈ 19% success; category baseline ≈ 46.0%. Oracle retraining using environment ground-truth labels on the same ~10k trajectories reached near-ceiling performance (e.g., 97.9% on object names).",
            "sample_complexity_with_pretraining": "Experiments used an initial set of ≈10,000 trajectories produced by the original motor policy; Flamingo relabeled the final image of each trajectory and retraining was done via behavioral cloning on truncated trajectories (last 5s) derived from these ≈10k examples. Evaluations used 10,000 rollouts per test condition.",
            "sample_complexity_without_pretraining": "Oracle experiments used the same ≈10,000 trajectories but annotated with environment ground-truth labels (achieving near-ceiling performance). The original agent was trained by behavioral cloning on a human-human dataset (Interactive Agents Team, 2021) but the size/number of examples for that BC pretraining is not specified in this paper.",
            "sample_complexity_gain": "Using Flamingo to relabel the same ≈10k unlabeled/generic trajectories produced large downstream gains over the original (e.g., object-name success from 7.1% → 64.4%) with no additional environment interactions; relative to oracle labels performance was lower, so Flamingo reduced the need for costly human/environmental labeling but did not fully match ground-truth supervision. The paper does not report a multiplicative sample-reduction factor relative to alternate pipelines.",
            "transfer_success_factors": "Key contributors: (1) Flamingo's internet-scale, domain-general language-vision grounding that generalizes to many object concepts; (2) ability to control relabeling via prompt engineering (zero-shot and few-shot); (3) few-shot examples to mitigate domain shift to 3D rendered visuals; (4) using language as an interpretable supervision signal for imitation learning (HER-style relabeling).",
            "transfer_failure_factors": "Limitations included domain shift between Flamingo's pretraining images and Playhouse renderings, noisy or extraneous VLM outputs, and the fact that few-shot prompting can induce incorrect task-relevant labels (reducing label precision) which harms downstream learning. Detection-based alternatives (e.g., OWL-ViT) suffered from domain shift and required predefined label sets.",
            "key_findings": "Generative vision-language models pretrained on internet image-text data can be repurposed as relabelers to teach embodied agents language-grounded behaviors from a modest offline dataset (~10k trajectories); label precision is more predictive of downstream HER performance than raw label accuracy; few-shot prompting helps adapt to rendered visuals but may reduce precision unless filtered by confidence; Flamingo relabeling substantially improves task success relative to an ungrounded policy but does not fully match oracle ground-truth supervision.",
            "uuid": "e1723.0"
        },
        {
            "name_short": "OWL-ViT",
            "name_full": "OWL-ViT (Open-vocabulary detection model)",
            "brief_description": "An open-vocabulary object detection model (contrastive/detection approach) used here as an alternative relabeler; requires a predefined list of candidate labels and outputs detections with confidences.",
            "citation_title": "Simple open-vocabulary object detection",
            "mention_or_use": "use",
            "model_agent_name": "OWL-ViT",
            "model_agent_description": "Open-vocabulary detection architecture (Minderer et al., 2022) that produces bounding-box detections and label confidences given candidate labels; not generative text but returns selected labels from a specified vocabulary.",
            "pretraining_data_type": "Object-detection and open-vocabulary detection datasets (detection supervision), not described in detail in this paper.",
            "pretraining_data_details": "Paper references Minderer et al. (2022) OWL-ViT; specifics of datasets used for OWL-ViT pretraining are not provided in this paper.",
            "embodied_task_name": "Playhouse \"Lift\" (relabeling comparison)",
            "embodied_task_description": "Same Unity Playhouse Lift trajectories: OWL-ViT was supplied with the list of 10 object names and used to detect/predict which object was held in final images to produce labels for retraining.",
            "action_space_text": "N/A — OWL-ViT is a vision detector trained on images and label vocabularies, not a text-action model.",
            "action_space_embodied": "Same continuous motor control action space of Playhouse agent (no change); OWL-ViT only provided labels used for imitation learning.",
            "action_mapping_method": "Detect-then-label: OWL-ViT was given the 10 candidate object names and the top-confidence detection was used as the relabel for the trajectory (no free-form text generation).",
            "perception_requirements": "RGB images from the final observation; detector expected real-world-like visual statistics and suffered domain shift on Unity renders.",
            "transfer_successful": false,
            "performance_with_pretraining": "On a subset of 300 trajectories OWL-ViT correctly predicted the held object only 7.2% of the time; overall OWL-ViT predictions had very low precision and only covered a small subset of object labels.",
            "performance_without_pretraining": "Flamingo zero-shot predictions were 54.8% accuracy on the same subset (reported in paper) and substantially outperformed OWL-ViT; Oracle ground-truth yields near-ceiling when used to retrain.",
            "sample_complexity_with_pretraining": "Evaluated on a subset of 300 trajectories for the OWL-ViT comparison; full retraining experiments were not reported for OWL-ViT due to poor label quality.",
            "sample_complexity_without_pretraining": "N/A in this direct comparison beyond the 300-trajectory evaluation subset.",
            "sample_complexity_gain": "None observed; OWL-ViT's poor relabel quality made it ineffective as a relabeler in this domain.",
            "transfer_success_factors": "Detection-based method would be aided when target visuals align with detector's training data and when candidate label vocabulary matches appearance.",
            "transfer_failure_factors": "Severe domain shift from real-world training images to Unity renders, requirement to pre-specify candidate labels, and inability to produce free-form task-appropriate language labels caused very poor performance.",
            "key_findings": "Contrastive/detection approaches like OWL-ViT can fail under domain shift in rendered 3D environments and are less flexible than generative VLM relabelers; in this paper OWL-ViT produced very low accuracy and precision compared to Flamingo.",
            "uuid": "e1723.1"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A contrastive vision-language model trained on image-text pairs used in the literature to create semantic visual representations and occasionally integrated into embodied agents' vision encoders.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "mention",
            "model_agent_name": "CLIP",
            "model_agent_description": "Contrastive VLM trained to align images and text embeddings; commonly used to construct semantic visual features or as a frozen encoder in downstream embodied tasks.",
            "pretraining_data_type": "Large-scale image-text pairs from the web (contrastive pretraining).",
            "pretraining_data_details": "Referenced as Radford et al. (2021) CLIP; this paper notes CLIP has been used to drive exploration, construct semantic representations, or be integrated into embodied agents, but details of datasets/size are not given here.",
            "embodied_task_name": null,
            "embodied_task_description": "Mentioned as prior work integrating pretrained vision encoders into embodied agents (e.g., for exploration or representation), but this paper does not run CLIP-transfer experiments.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": "RGB images (visual encoder usage).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "CLIP-like models provide semantic visual features from large image-text pretraining, which can support generalization in embodied tasks when representations capture task-relevant properties.",
            "transfer_failure_factors": "Pretrained visual encoders may not encode task-relevant geometric or spatial details (e.g., fine spatial positions) and can introduce biases; representation vectors are sometimes hard to adapt for specific downstream tasks.",
            "key_findings": "Contrastive VLMs like CLIP offer strong semantic representations but can miss task-relevant information for embodied control; this motivates using generative VLMs for interpretable language supervision in this paper.",
            "uuid": "e1723.2"
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Large-scale multi-modal/language models (GPT-4 cited) are mentioned as capable models comparable to Flamingo for generating language-conditioned supervision, though not used experimentally in this paper.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "mention",
            "model_agent_name": "GPT-4",
            "model_agent_description": "Large language (and multi-modal) model from OpenAI; mentioned as a capable generative model that could substitute for Flamingo for similar relabeling roles.",
            "pretraining_data_type": "Large-scale internet text and multi-modal datasets (paper references GPT-4 technical report); specifics are not provided in this paper.",
            "pretraining_data_details": "Cited as OpenAI (2023) GPT-4 technical report; no dataset sizes or details are given here.",
            "embodied_task_name": null,
            "embodied_task_description": "Mentioned as a possible comparable generative VLM that could be used for relabeling or producing text observations for embodied agents; no experiments reported.",
            "action_space_text": "Natural language modeling (no action space in pretraining).",
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong language grounding and generative capabilities could enable flexible relabeling and instruction generation for embodied agents.",
            "transfer_failure_factors": "Model size and compute may limit direct use in low-level motor control; domain shift and multi-modality mismatch remain considerations.",
            "key_findings": "Paper notes GPT-4 as a comparable publicly-released model that could replicate Flamingo's role, but provides no experimental evidence in this work.",
            "uuid": "e1723.3"
        },
        {
            "name_short": "Prismer",
            "name_full": "Prismer: A vision-language model with an ensemble of experts",
            "brief_description": "A recent vision-language model cited as an open-source alternative with comparable capabilities to Flamingo for multi-modal tasks; mentioned but not used.",
            "citation_title": "Prismer: A vision-language model with an ensemble of experts",
            "mention_or_use": "mention",
            "model_agent_name": "Prismer",
            "model_agent_description": "Vision-language model composed of an ensemble of experts designed for multi-modal understanding; referenced as an alternative to Flamingo for similar relabeling or captioning tasks.",
            "pretraining_data_type": "Vision-language/image-text pretraining (paper cites Liu et al., 2023); specifics not provided in this paper.",
            "pretraining_data_details": "Referenced as an open-source model with capabilities comparable to Flamingo; no dataset sizes or training details given here.",
            "embodied_task_name": null,
            "embodied_task_description": "Mentioned as a potential substitute for Flamingo for relabeling or generating language supervision for embodied agents; not evaluated in this paper.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Ensemble-of-experts architecture and multi-modal pretraining might afford robust relabeling and captioning across domains.",
            "transfer_failure_factors": "Domain shift to rendered environments and model availability/size may constrain direct use.",
            "key_findings": "Prismer is cited as a potentially usable VLM for similar distillation approaches, but no experiments or metrics are provided here.",
            "uuid": "e1723.4"
        },
        {
            "name_short": "R3M",
            "name_full": "R3M: A universal visual representation for robot manipulation",
            "brief_description": "A model for universal visual representations in robot manipulation, cited as related work that pretrains visual features useful for robotic tasks.",
            "citation_title": "R3M: A universal visual representation for robot manipulation",
            "mention_or_use": "mention",
            "model_agent_name": "R3M",
            "model_agent_description": "A pretrained visual representation intended to transfer to robot manipulation tasks; mentioned in the related work as an example of pretrained visual models used in robotics.",
            "pretraining_data_type": "Visual/video or image datasets relevant to manipulation (paper reference only; details not provided here).",
            "pretraining_data_details": "Cited as Nair et al. (2022) R3M; this paper does not provide dataset sizes or pretraining specifics.",
            "embodied_task_name": null,
            "embodied_task_description": "Mentioned as prior work producing transferable visual features for manipulation; not experimentally evaluated in this work.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": "RGB (and possibly video) visual inputs for robotics tasks.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretrained visual representations can accelerate robot learning when they capture task-relevant features.",
            "transfer_failure_factors": "Pretrained features may lack task-relevant geometry/affordance details depending on pretraining data and objectives.",
            "key_findings": "R3M is cited as an example of leveraging pretrained visual models for robot tasks; paper does not report experiments using R3M.",
            "uuid": "e1723.5"
        },
        {
            "name_short": "MineDojo",
            "name_full": "Building open-ended embodied agents with internet-scale knowledge",
            "brief_description": "A platform and effort to create open-ended embodied agents that leverage internet-scale knowledge; cited as related work showing internet data can bootstrap embodied agents.",
            "citation_title": "Building open-ended embodied agents with internet-scale knowledge",
            "mention_or_use": "mention",
            "model_agent_name": "MineDojo",
            "model_agent_description": "A system/framework that trains agents using large-scale internet knowledge to enable open-ended embodied behavior (cited as Fan et al., 2022).",
            "pretraining_data_type": "Internet-scale knowledge, including language and multi-modal web data (paper reference only).",
            "pretraining_data_details": "Referenced as Fan et al. (2022) MineDojo; no dataset/size details are provided in this paper.",
            "embodied_task_name": null,
            "embodied_task_description": "Mentioned as an example of using internet-scale pretraining for embodied agents; not directly evaluated in this work.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Access to internet-scale knowledge and large multimodal corpora can provide broad semantic grounding useful for embodied tasks.",
            "transfer_failure_factors": "Bridging the gap between passive internet data and agent-specific embodied representations remains challenging.",
            "key_findings": "MineDojo is cited as related work illustrating large-scale internet-derived knowledge for embodied agents; no direct experiments in this paper.",
            "uuid": "e1723.6"
        },
        {
            "name_short": "LM-Nav",
            "name_full": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "brief_description": "A recent effort to use large pretrained models of language, vision and action for robotic navigation; cited as related work demonstrating pretrained models applied to embodied navigation tasks.",
            "citation_title": "Robotic navigation with large pre-trained models of language, vision, and action",
            "mention_or_use": "mention",
            "model_agent_name": "LM-Nav",
            "model_agent_description": "System leveraging large pretrained models that combine language, vision, and action priors to perform robotic navigation tasks (referenced Shah et al., 2022).",
            "pretraining_data_type": "Pretrained language, vision, and action models (paper reference only).",
            "pretraining_data_details": "Referenced as Shah et al. (2022) LM-Nav; details on pretraining corpora are not provided in this paper.",
            "embodied_task_name": null,
            "embodied_task_description": "Mentioned as an example of transferring pretrained multi-modal models to robotic navigation; not experimentally evaluated here.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Combining learned priors across language, vision, and action can provide useful inductive biases for navigation.",
            "transfer_failure_factors": "Model scale and mismatch between pretraining tasks and embodied dynamics can complicate direct transfer.",
            "key_findings": "LM-Nav is cited as an approach for applying large pretrained models to navigation, showing the paper's broader context of leveraging pretrained models for embodied tasks.",
            "uuid": "e1723.7"
        },
        {
            "name_short": "Xiao et al. (2022)",
            "name_full": "Robotic skill acquisition via instruction augmentation with vision-language models",
            "brief_description": "Prior work that fine-tuned a contrastive vision-language model with many human-relabeled trajectories to enable relabeling-based supervision; cited as related HER/relabeling work.",
            "citation_title": "Robotic skill acquisition via instruction augmentation with vision-language models",
            "mention_or_use": "mention",
            "model_agent_name": "Contrastive VLM fine-tuned for instruction selection (Xiao et al., 2022)",
            "model_agent_description": "Previous approach trained a domain-specific relabeling model / fine-tuned a contrastive VLM on thousands of human-relabeled trajectories to select best instructions for relabeling.",
            "pretraining_data_type": "Contrastive image-text pretraining followed by domain-specific fine-tuning on human-relabeled trajectories (per Xiao et al. description in this paper).",
            "pretraining_data_details": "Paper states Xiao et al. (2022) used thousands of human-relabeled trajectories to fine-tune a contrastive VLM; exact dataset sizes and compositions are not provided here.",
            "embodied_task_name": null,
            "embodied_task_description": "Referenced as a HER-style relabeling method that requires extensive human relabeling to produce usable relabelers for embodied tasks.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "Fine-tuned contrastive scoring to choose best instruction from candidates for relabeling (per prior work; contrasted with the generative free-form relabeling used here).",
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Fine-tuning on human-relabeled trajectories can produce reliable relabelers for HER at the cost of significant human labeling.",
            "transfer_failure_factors": "Requires thousands of human-relabeled trajectories and is less flexible than free-form generative relabeling.",
            "key_findings": "Cited as an alternative relabeling approach that is more data/human-label intensive compared to the generative VLM relabeling proposed in this paper.",
            "uuid": "e1723.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Flamingo: a visual language model for few-shot learning",
            "rating": 2,
            "sanitized_title": "flamingo_a_visual_language_model_for_fewshot_learning"
        },
        {
            "paper_title": "Simple open-vocabulary object detection",
            "rating": 2,
            "sanitized_title": "simple_openvocabulary_object_detection"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Robotic skill acquisition via instruction augmentation with vision-language models",
            "rating": 2,
            "sanitized_title": "robotic_skill_acquisition_via_instruction_augmentation_with_visionlanguage_models"
        },
        {
            "paper_title": "R3M: A universal visual representation for robot manipulation",
            "rating": 2,
            "sanitized_title": "r3m_a_universal_visual_representation_for_robot_manipulation"
        },
        {
            "paper_title": "Building open-ended embodied agents with internet-scale knowledge",
            "rating": 2,
            "sanitized_title": "building_openended_embodied_agents_with_internetscale_knowledge"
        },
        {
            "paper_title": "Robotic navigation with large pre-trained models of language, vision, and action",
            "rating": 2,
            "sanitized_title": "robotic_navigation_with_large_pretrained_models_of_language_vision_and_action"
        },
        {
            "paper_title": "Prismer: A vision-language model with an ensemble of experts",
            "rating": 1,
            "sanitized_title": "prismer_a_visionlanguage_model_with_an_ensemble_of_experts"
        }
    ],
    "cost": 0.024710249999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Distilling Internet-Scale Vision-Language Models into Embodied Agents</p>
<p>Theodore Sumers 
Kenneth Marino 
Arun Ahuja 
Rob Fergus 
Ishita Dasgupta 
Distilling Internet-Scale Vision-Language Models into Embodied Agents</p>
<p>Instruction-following agents must ground language into their observation and action spaces. Yet learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.</p>
<p>Introduction</p>
<p>Embodied agents capable of understanding and fulfilling natural language instructions are a longstanding goal for artificial intelligence (Winograd, 1972). Such agents must ground language (Harnad, 1990;Mooney, 2008) by correctly associating words with corresponding referents in their environment. But grounding language is both philosophically (Quine, 1960) and practically challenging: methods to learn such groundings remain an active area of research (Tellex et al., 2020).  Embodied language grounding is particularly difficult because training data are scarce. Passive learning from internet data has driven a series of revolutions in natural language processing (Mikolov et al., 2013;Devlin et al., 2018;Brown et al., 2020), but embodied agents must map language into their own idiosyncratic observation and action spaces.</p>
<p>Training data for embodied agents thus typically consist of trajectories paired with linguistic instructions (Tellex et al., 2011;Abramson et al., 2020, inter alia) or descriptions (Nguyen et al., 2021;Sharma et al., 2022;Zhong et al., 2022). Providing an agent with aligned behavior and language allows it to learn a mapping between the two. Unfortunately, such data are expensive (typically requiring human annotation), under-specified (a trajectory can correspond to several different instructions or descriptions), and agent-specific (trajectory representations are tightly coupled with the agent architecture).</p>
<p>We propose using large-scale vision-language models (VLMs) to address some of these challenges. Unlike embodied agents, VLMs can be trained on massive internet data (Radford et al., 2021;Ramesh et al., 2022;Alayrac et al., 2022;OpenAI, 2023;Liu et al., 2023). Here, we use the Flamingo VLM (Alayrac et al., 2022) to annotate trajectories, leveraging its internet-derived language grounding to generate training data for the embodied agent (Fig. 1).</p>
<p>Our approach distills the VLM's domain-general language 1 arXiv:2301.12507v2 [cs.AI] 14 Jun 2023 grounding into domain-specific embodied agents, thus ameliorating data cost and scarcity. We use prompting and few-shot learning to control which aspects of a trajectory to relabel, mitigating label under-specification and allowing new task definitions on the fly. Finally, unlike approaches which integrate encoders such as CLIP (Radford et al., 2021) directly into the embodied agent, our method operates purely as data augmentation. This allows us to remain agnostic to changes in the agent architecture and affords substantial interpretability, making interventions and diagnosis easier.</p>
<p>After reviewing related work (Sec. 2), we make the following contributions:</p>
<p>• A novel method using a generative VLM to supervise training of language-conditioned agents (Sec. 3). • Experiments using our method to flexibly teach new language groundings, including object names (Sec. 5.1), attributes (Sec. 5.2), category membership (Sec. 5.3) and even ad-hoc user preferences (Sec. 5.4). • An analysis of this imperfect supervision signal, including transferable insight into how different types of noise affect downstream task performance (Sec. 5.5). Taken together, our work demonstrates that generic language grounding acquired from internet-scale pretraining can be controlled and distilled into embodied agents, allowing us to teach task-specific language groundings without the burden of extensive human supervision.</p>
<p>Related Work</p>
<p>Our work studies language grounding within the classic instruction following setting, where an embodied agent is given a natural language instruction and must generate a trajectory satisfying it (Winograd, 1972). 1</p>
<p>Learning from Human Interactions</p>
<p>Typical approaches use a human-generated dataset of aligned language and trajectories to learn a mapping between them (Kollar et al., 2010;Tellex et al., 2011;Chen &amp; Mooney, 2011;Artzi &amp; Zettlemoyer, 2013;Mei et al., 2016;Anderson et al., 2018;Blukis et al., 2019;Lynch &amp; Sermanet, 2020;Abramson et al., 2020;Shridhar et al., 2020;Fried et al., 2018, inter alia). Language-conditioned reinforcement learning (RL) learns this grounding from trial and error, either assuming an environment-generated reward signal (Misra et al., 2017;Chaplot et al., 2018;Yu et al., 2018), or learning a reward function from aligned languagetrajectory data (Bahdanau et al., 2018).</p>
<p>While these works developed agents capable of fulfilling natural language instructions within circumscribed domains, 1 Outside instruction following, a related body of work grounds descriptive language from documents (Branavan et al., 2012;Zhong et al., 2020; or interactions (Narasimhan et al., 2018;Sumers et al., 2021;Lin et al., 2022) to learn general policies. they are limited by training on static environments and scarce, expensive datasets. The resulting learned language groundings are often tightly coupled to the agent's observations and actions, making them inflexible to new objects or concepts. Researchers have explored numerous methods to mitigate this, including data augmentation (Blukis et al., 2020;Chen et al., 2022b), dual-coding memory (Hill et al., 2021), auxiliary language generation objectives (Yan et al., 2022;Bigazzi et al., 2021), or interactive supervision (Kulick et al., 2013;Mohan &amp; Laird, 2014;She et al., 2014;Thomason et al., 2017;Co-Reyes et al., 2018;Chai et al., 2018;Nguyen et al., 2021). Recent approaches instead leverage internet-scale language models to achieve generalization.</p>
<p>Leveraging Pretrained Models</p>
<p>Several lines of work use language-only knowledge from pretrained models. For example, word embeddings can be used to generalize representation of linguistic instructions (Chen et al., 2020;Li et al., 2019), while language models can be used to break complex instructions into simpler ones (Ahn et al., 2022;Huang et al., 2022a;Dasgupta et al., 2022;Singh et al., 2022). Because these approaches use pretrained weights over language only, they can help the agent generalize paraphrases or combinations of existing concepts, but cannot themselves provide groundings for novel visual concepts.</p>
<p>Unlike these language-only models, vision-language models (VLMs) acquire multi-modal grounding from passive internet-scale learning. Contrastive VLMs (e.g. CLIP, Radford et al., 2021) can be used to drive exploration (Tam et al., 2022), construct a semantic representation from visual imagery (Chen et al., 2022a;Singh et al., 2022), or directly as the agent's vision and text encoders, thus inheriting this grounding (Majumdar et al., 2022;Shah et al., 2022;Shridhar et al., 2021;Bucker et al., 2022). However, such pretrained vision encoders are often trained for object recognition and may not encode task-relevant information (e.g., spatial positions, Shridhar et al., 2021). Further, since this information is encoded as hidden vectors, it is nontrivial to determine which representation will be optimal for a particular downstream task (Hsu et al., 2022) or avoid biases arising from their training data (Bommasani et al., 2021). Our approach-using generative VLMs to produce linguistic annotations-is inherently interpretable and allows us to use simple prompts to focus the VLM on task-relevant aspects of the visual scene.</p>
<p>Finally, other approaches have trained new VLMs on internet-scale data to serve as general (Nair et al., 2022) or domain-specific (Fan et al., 2022;Guhur et al., 2021;Hao et al., 2020) visual representations. In contrast, our approach uses an off-the-shelf pretrained VLM to generate task-relevant annotations.</p>
<p>Hindsight Experience Replay</p>
<p>Our approach builds on Hindsight Experience Replay (HER, Andrychowicz et al., 2017) in a language-conditioned setting (Chan et al., 2019). HER's key insight is that early in training, rewards are sparse because agents rarely (if ever) achieve the specified goal. HER densifies rewards by converting these failed trajectories into successful ones by retroactively "relabeling" them, assigning a goal achieved by the agent's behavior.</p>
<p>The central challenge of applying HER to languageconditioned agents is implementing a relabeling function that maps from states to natural language instructions. First, the mapping is not 1:1, as multiple instructions may be compatible with the same state (for example, "Lift a banana" and "Lift something yellow", Fig. 1). Second, the mapping is not known: there is no clear way of converting an agent's state to a linguistic instruction fulfilled by that state.</p>
<p>Prior work has used a reward signal to train a domainspecific relabeling model (Cideron et al., 2020), a mix of image-and human-relabeling (Lynch &amp; Sermanet, 2020), or thousands of human-relabeled trajectories to fine-tune a contrastive VLM which can then be used to select the best instruction from a candidate set (Xiao et al., 2022). We instead use a pretrained generative VLM as the relabeling function, omitting the need for environmental rewards or domain-specific human labelled data. Our approach also offers free-form language generation (rather than selecting from limited options) and on-the-fly task specifications via prompting (allowing us to relabel specific aspects of the trajectory, such that each can have multiple language labels depending on the task specification; Fig. 1B). This allows better use of trajectories.</p>
<p>Method</p>
<p>Our approach uses a pretrained generative VLM as the relabeling function for HER (Sec. 2.3). We first formally specify HER. HER assumes that goals g ∈ G correspond to predicates f g : S → {0,1}, so that goals are completed by reaching a set of states. During training, an agent is provided with a goal g and executes a trajectory ξ = s 0 , ...s T . Because the goal is rarely accomplished, most trajectories receive no reward: ∀ s∈ξ f g (s) = 0. HER solves this by relabeling the trajectory, assigning a new goal g which was accomplished by the last state in the sequence: f g (s T ) = 1. 2 The relabeling function m takes a state as input and returns a goal fulfilled by that state m : S → G s.t. ∀ s∈S f m(s) (s) = 1.</p>
<p>In our work, we use a VLM as the relabeling function. We 2 Alternative HER formulations relabel randomly sampled states. In our setting, the last state is generally the most informative, but applications to other domains such as vision-language navigation may benefit from such strategies. relax the assumption that the environment is fully observable, and consider observation sequences o 1 , ..., o T (the agent's visual inputs) generated by an observation function O : S → Ω. The VLM serves as a proxy relabeling functioñ m, which now takes an observation and returns a goal (a natural language string) satisfied by the state underlying that observation:m : Ω → G s.t. ∀ s∈S fm (O(s)) (s) = 1. Intuitively, this means that we retroactively generate language that describes the agent's actual behavior (Fig. 1A).</p>
<p>This approach is conceptually straightforward and fully compatible with any procedure for training languageconditioned agents, as the original instructions can be directly replaced by the generated ones. Indeed, while HER was developed for reinforcement learning, we use the relabeled trajectories to train with imitation learning instead.</p>
<p>Relative to prior work, our approach makes few assumptions and is highly data efficient. Unlike Cideron et al. (2020) we do not use a reward signal from the environment; and unlike Lynch &amp; Sermanet (2020); Xiao et al. (2022) we do not crowdsource relabeling. However, in settings with a reward signal or preexisting datasets, it would be possible to incorporate such information into our method (e.g., using an annotated dataset to fine-tune the generative VLM).</p>
<p>Finally, while we use a generative VLM, object classification or detection models (Minderer et al., 2022;Kuo et al., 2022;Kirillov et al., 2023) may be used instead. Such models have two important structural limitations: they require a priori specification of possible goals (unlike our zero-shot experiments in Sections 5.1-5.3) and cannot be used for adhoc categories (Section 5.4). Empirically, we found that the classification / detection paradigm was less effective than our generative approach: substituting the OWL-ViT detection model (Minderer et al., 2022) as a relabeling function yielded substantially worse performance (Appendix B).</p>
<p>Experimental Setup</p>
<p>In this section, we describe our experimental framework employing a VLM to relabel trajectories for HER. Following the classical HER formulation (Andrychowicz et al., 2017) we use a simple task structure which can be re-labeled on the basis of the final observation only. We return to extensions requiring multiple observations in the discussion.</p>
<p>The Playhouse Environment</p>
<p>We use the Playhouse environment from Abramson et al.</p>
<p>(2020), a Unity-based environment with a continuous action space. Each episode takes place in a procedurally-generated home with multiple rooms and a wide range of everyday domestic objects ( Fig. 2A). This 3D environment is challenging for RL agents and the VLM, as the agent's egocentric perspective often yields unusual or close-up perspectives on objects (Figs. S3, S4, S5). Figure 2. Using a pretrained VLM to teach an agent novel object names (Sec. 5.1). A: Four of the ten objects. The agent has never seen these words before, and the VLM is not given any information about what might be present. B:</p>
<p>We use an open-ended prompt to relabel trajectories, then use the VLM outputs (bold) to retrain the agent. C: Results from the original agent ("Original"), after retraining with VLM labels ("Zeroshot"), and an upper bound from retraining on ground truth labels from the environment ("Oracle"). Here and throughout, the red line shows baseline performance (lifting a random object) and error bars show 95% CIs.</p>
<p>The "Lift" Task</p>
<p>To isolate the effects of relabeling we use the same task structure across our experiments. We chose the "Lift" task as it gives a direct measure of our method's effectiveness: we can evaluate the agent's language grounding on an objectby-object basis.</p>
<p>At the start of each episode, agents are placed in a room with 5-10 objects and instructed to lift a target object. To demonstrate the flexibility of our method, we vary the task specification: using object names ("Lift a plane"; Sec 5.1), attributes ("Lift a red object"; Sec 5.2), categories ("Lift a toy"; Sec 5.3), or preferences ("Lift something John Doe likes"; Sec 5.4). Episodes end when the agent lifts an object, or after 120 seconds.</p>
<p>Flamingo VLM</p>
<p>We use Flamingo (Alayrac et al., 2022), a state-of-the-art language-generative VLM. Flamingo accepts interleaved images and text as input, and produces text output. This allows us to experiment with both "Zeroshot" prompts (containing only the image to be relabeled and a text prompt) and "Fewshot" prompts (including up to 32 in-context imagetext examples). We use the 80B parameter model described by Alayrac et al. (2022) with greedy sampling.</p>
<p>HER Implementation</p>
<p>We first need an agent that generates structured behaviors that can be interpretably relabelled. We use human-human data to learn a task-agnostic motor policy: e.g., an agent that knows how to lift something, but not what a plane is. We refer to this as the "original" agent, and train it via behavioral cloning (BC) on the human-human dataset described by Interactive Agents Team (2021); for details on the dataset, agent architecture, and BC implementation, please refer to that work. To ensure the "original" agent lacks task-relevant groundings, we filter out episodes with relevant utterances before performing BC. However, we note that the "original" agent could be generated in any way, such as with RL on a different set of tasks; our approach uses it as a starting point.</p>
<p>To compare the effects of different relabeling functions, we use a batched HER approach (Fig. S1). For each experiment, we generate an initial set of approximately 10,000 trajectories with a generic "Lift an object" instruction (due to implementation details, the actual number varied from 10,000 to 11,500). Across all experiments, around 3% of these initial trajectories timed out as the agent did not lift an object. This was not enough to meaningfully affect results, so for simplicity we discarded them. This is equivalent to assuming the agent can detect a successful grasp, which is reasonable even in robotics (Pinto &amp; Gupta, 2016).</p>
<p>We then use the VLM to relabel the final image in each trajectory. This generates a new annotation describing the agent's actual behavior (Fig. 1A, 2B). We perform light post-processing on the VLM outputs: Flamingo sometimes generates multiple responses (always separated by a newline), so we truncate the response to the first newline and prepend "Lift a ". We then use these VLM-generated strings as the instruction in a second round of BC. Finally, performing BC on the full trajectories is computationally expensive. Preliminary analysis showed that using the full trajectories provides only minor performance gains compared to truncating them to the last 5 seconds; we therefore adopt this truncation throughout. . Using a pretrained VLM to flexibly teach object attributes (name or color) from a single set of trajectories (Sec. 5.2). A: We randomize color mappings so each object can appear red, green, blue, pink, or yellow. B: We again use generic prompts, adding a second color-oriented prompt to obtain color labels. C: Results on the "Name" and "Color" tasks from the "Original" agent and after retraining with VLM labels ("Zeroshot") or ground truth labels from the environment ("Oracle").</p>
<p>Results</p>
<p>To demonstrate the flexibility of VLMs for relabeling, we vary the goal structure of the "Lift" task. Our first two experiments focus on visible object attributes, using simple "Zeroshot" prompts to teach object names (Sec. 5.1) and attributes (Sec. 5.2). Our third and fourth use "Fewshot" prompts to teach category membership (Sec. 5.3) and finally novel user preferences over objects (Sec. 5.4). We close with an analysis of label noise and task performance (Section 5.5). This highlights an advantage of our approach over contrastive VLMs (Radford et al., 2021): our method produces human-legible language annotations with corresponding confidence scores, allowing us to analyze and filter labels to improve downstream task performance.</p>
<p>Teaching Object Names</p>
<p>Our first experiment uses relabeling to teach the agent to lift one of 10 objects: a table, a chair, a book, a basketball, a racket, a plane, a car, a banana, a carrot, and a pear ( Fig. 2A).</p>
<p>Setup.</p>
<p>We begin by training an "original" agent, filtering out any episodes containing one of the 10 target words. 3 We use this agent to generate 10,000 initial trajectories using the generic "Lift a object" instruction in a room with all 10 objects. We provide the VLM with the final image in each trajectory and use a simple QAstyle zero-shot prompt: [IMG 0] Q: What is this object? A: (Fig. 2B). We then re-train the agent on these new VLM labels.</p>
<p>Results. We test our retrained agent by again placing it in a room with all 10 objects, but now instructing it to lift a specific one (e.g., "Lift a car") and generate 10,000 evaluation trajectories. We first test the "Original" agent to ensure it has no knowledge of the objects. We find that it achieves near-chance performance (7.1% task success), confirming that it does not possess task-relevant language groundings. We then test an "Oracle" agent retrained on ground-truth relabeling from the environment itself. This agent performs near ceiling (97.9% success), confirming there are enough trajectories to teach the task with perfect relabeling. Finally, we find that our VLM-based relabeling method-using only a simple zero-shot prompt and no information about what objects might appear-conveys a significant fraction of the task, resulting in 64.4% success (Fig. 2C). The VLM-retrained agent performs well above chance on all ten objects (Fig. S2).</p>
<p>Intriguingly, we achieve this performance despite substantial noise in the relabeling. VLM-generated strings are relatively low accuracy (only 54.7% contain the canonical object name used in instructions), and frequently contain extraneous words (Fig. 2B, S3; Table S1). We conduct a deeper analysis of label noise and downstream task performance in Sec. 5.5.</p>
<p>Teaching Object Attributes</p>
<p>Section 5.1 demonstrated that VLMs can teach basic object names. But how controllable is the VLM relabeling? Can we, for example, teach an agent to recognize an object's attributes rather than the objects themselves? We now show that the VLM can be used to relabel a single set of trajectories with their name or color respectively.</p>
<p>Setup. We use a subset of five objects from Section 5.1 (plane, racket, chair, table, and basketball). Previously, these items' colors were fixed across all episodes. We now render them in different colors (red, green, blue, pink, or yellow). This gives us a total of 25 object-color combinations (green chair, red chair, green plane, etc.; Fig. 3A). As before, we train an original agent filtering out episodes containing any of these object name or color terms. We create a level with all five objects, and randomly assign each color to a object in each episode. We use this task to generate a set of 10,000 trajectories with a generic "Lift an object" prompt. Now, however, we relabel each trajectory twice (Fig. 3B, S4). One relabeling uses the original prompt from Section 5.1: [IMG 0] Q: What is this object? A: . The second introduces a slight variation, adding the word "color": [IMG 0] Q: What color is this object? A: . We follow the same procedure and retrain two separate agents: one using the labels generated by the original prompt, and one using the "color" variation.</p>
<p>Results. We again test our agents by placing them in a room with all five objects and instructing them to lift one. However, we now test two forms of instructions: an object name task ("Lift a {plane, racket, chair, table, basketball}") or an object color task ("Lift a {red, green, blue, pink, yel-low} object"). We use the same comparisons, checking the original agent and an agent retrained on oracle color and object name labels.</p>
<p>We again find that the original agent performs at chance (19.6% task success on names and 18.4% on colors) and the oracle-relabeled agent performs at ceiling (97.6% on names and 99.5% on color). Our VLM relabeling achieves 83.0% and 80.6% respectively (Fig. 3C).</p>
<p>Teaching Real-World Categories</p>
<p>Sections 5.1 and 5.2 show the VLM can be used to re-label visual object attributes such as shape and color. But many important properties, such as category membership, are not readily visible. We next test whether our method can be used for tasks depending on such properties.</p>
<p>Setup. We use a set of 10 items, five "food" (pear, banana, carrot, lemon, and grapes) and five "toys" (plane, train, car, robot, dice; Fig. 4A). We train an original agent, filtering out episodes with references to any of these, as well as "food" or "toy." Again, we generate a set of initial trajectories using a "Lift an object" prompt in a room with all 10 objects.</p>
<p>We again use a simple prompt to re-label the trajectories: [IMG 0] Q: Is this food or a toy? A: . However, initial results suggested that these categories caused an interaction effect with the 3D rendered graphics: Flamingo recognized that the "food" items were really toy food items and frequently labeled them as "toys" or "both" (Fig. 4B, Table S2). This domain shift issue could be obviated by using a different category structure (e.g. food vs furniture), but we instead experimented with few-shot prompting. We generated three example images of the agent lifting each of the ten items and ran a series of relabelings with incrementally more examples. Our "Fewshot-1" prompt gives Flamingo three examples each of one food and one toy (carrots and robots) for a total of 6 in-context examples. "Fewshot-2" added lemons and dice (a total of 12 examples); "Fewshot-3" added planes and bananas (total of 18); "Fewshot-4" added grapes and cars (total of 24), and finally "Fewshot-5" added trains and pears for a total of 30. "Fewshot-5" therefore saw three examples each of all 10 objects. Few-shot relabeling resulted in short VLM genera- Figure 5. Using a VLM to teach ad-hoc categories (Sec. 5.4). A: "Aligned" preferences follow existing category structure, while "Arbitrary" preferences cut across it. B: The VLM is able to teach tasks requiring new category structure from fewshot examples, but alignment with existing structure helps. tions with no extraneous words (Fig. 4B, S5, Table S3).</p>
<p>As before, we re-train agents with each of the resulting label sets. We evaluate them by generating 10,000 rollouts each for "Lift a food" and "Lift a toy" instructions.</p>
<p>Results. As expected, our original agent performs near chance (46.0% task success). Zeroshot relabeling lifts performance above chance to 60.8%. Adding in-context examples provides another substantial boost: "Fewshot-1" lifts performance to 73.3%, and "Fewshot-2" to 82.8% (Fig. 4C).</p>
<p>Adding additional examples has only a marginal effect, with performance plateauing around 85%. Notably, even in the "Fewshot-2" condition, Flamingo is readily able to generalize category structure to the remaining three items in each category. This suggests that the few-shot examples help Flamingo adapt to the 3D rendered visuals, while Flamingo inherits information about category membership from its pretraining experience.</p>
<p>Teaching Ad-hoc Categories</p>
<p>Our first three experiments show that simple prompting and a handful of few-shot examples allow our VLM to recognize and teach canonical properties such as names, colors, and category membership. In general, however, we cannot expect tasks to conform to existing canonical categories. Evidence from psychology suggests that ad-hoc categories (Barsalou, 1983) play a crucial role in human cognition, allowing us to create new conceptual groupings on the fly. Such context-dependent categories are often based on usecases ("things to take on a camping trip") or affordances ("things that can be used as firewood in an emergency"). Can our method be used to teach such flexible category structures?</p>
<p>Our final experiment tests the VLM's ability to re-label based on new and arbitrary category structure: here, instantiated as a user's preferences over a set of objects. Such dynamic relabeling would allow individuals to provide personalized task specifications. For example, a user could provide a list of items to bring on a camping trip, or express preferences (or allergies) over food items. The agent could then re-label its previous experience with such items and re-train its policy to learn the new groundings, allowing it to map user-level requests into its action space.</p>
<p>Setup. We aim to test Flamingo's ability to learn (and then teach) new categories via in-context examples. We re-use the 10 objects from Sec. 5.3 but re-formulate the task in terms of user preferences. We introduce two sets of preferences: "Aligned" preferences, which respect existing category structure (John Doe likes food and dislikes toys), and "Arbitrary" preferences, which cut across it (John Doe likes robots, planes, carrots, lemons, and bananas; and dislikes cars, dice, trains, grapes, and pears; Fig. 5A).</p>
<p>We use the 10,000 rollouts generated in Section 5.3 but re-label them with new prompts:</p>
<p>[IMG 0] Q: Would John Doe like this? A:. We include preambles John Doe likes food.</p>
<p>for "Aligned" and John Doe likes robots, planes, carrots, lemons, and bananas.</p>
<p>for "Arbitrary". We use fewshot examples for all 10 items (equivalent to "Fewshot-5" from Sec. 5.3) with "yes" or "no" responses according to the category structure being used. To produce task-appropriate labels, we transform Flamingo's response by mapping "yes" to "an object John Doe likes" and "no" to "an object John Doe hates."</p>
<p>We re-train agents using these labels, and again evaluate by averaging 10,000 rollouts on the two tasks: "Lift something John Doe likes", and "Lift something John Doe hates."</p>
<p>Results. Our original agent performs near chance (50%) for both category structures (Fig. 5B). We find that Flamingo relabeling improves performance for both category structures, with "Aligned" (76.7%) outperforming "Arbitrary" (64.2%). These results demonstrate that Flamingo is aided by, but not solely dependent on, real-world category structure.</p>
<p>It is helpful to compare these results with the "Fewshot-5" results from Section 5.3. These experiments each use the same few-shot examples to relabel the same trajectories, but vary the nature of the category structure. Flamingo does very well with explicit real-world category structure ("Food-Toy" in Sec. 5.3; task performance 84.7%); next best with implicit real-world category structure ("Aligned" in Sec. 5.4; 76.7%); and modestly with no real-world category structure ("Arbitrary" in Sec. 5.4; 64.2%).</p>
<p>Finally, we tested the VLM's ability to generalize categories to new instances by permuting the toys' colors. We found that for both preference structures, the VLM successfully relabeled recolored objects with equivalent accuracy (Appendix C). Figure 6. Comparing "Zeroshot" (ZS) and "Fewshot" (FS) relabeling (Sec. 5.5). A: Zeroshot typically generates text that reflects the image contents, but is often not task-relevant. Fewshot prompting encourages the VLM to generate one of the 10 object labels. However, when the foreground is challenging it often labels background objects instead. B: Label quality and task performance. Zeroshot yields many irrelevant labels; Fewshot instead produces more "Correct" and "Wrong" labels. Agents trained on Fewshot labels perform worse.</p>
<p>Analyzing the Relabeling Function</p>
<p>In this final section, we use the setup from Section 5.1 to conduct an analysis of Flamingo as a relabeling function.</p>
<p>ZEROSHOT VS FEWSHOT FLAMINGO</p>
<p>We conduct a "Fewshot" relabeling version of "Zeroshot" experiments in Section 5.1, with 32 examples (3-4 examples each of the 10 items in the task) included in context. We find that "Fewshot" relabeling increases accuracy, but-surprisingly-decreases downstream task performance. Concretely, 77.2% of "Fewshot" labels contain the canonical object name (compared to 54.7% of "Zeroshot"), yet the "Fewshot"-retrained agent achieves only 52.9% task success (compared to 64.4% for the "Zeroshot" agent; Fig. 6).</p>
<p>A closer look at the labels reveals an important difference. "Zeroshot" labels that are not correct are often irrelevant (i.e. they do not contain any of the 10 task-relevant object names). In contrast, incorrect "Fewshot" labels are almost always wrong (i.e., they contain a different task-relevant object name). Few-shot prompting encourages Flamingo's generation towards task relevant labels, causing it to "guess" a label when uncertain (Fig. 6B, S3, S7). While "Zeroshot" Flamingo is noisier, these irrelevant labels have little effect on downstream performance. In contrast, "Fewshot" generates wrong task-relevant labels that actively interfere with grounding.</p>
<p>Because many "Zeroshot" relabelings are irrelevant, "Fewshot" is higher accuracy ( # of correct labels # of trajectories ) but lower precision ( # of correct labels # of task-relevant labels ). Therefore "Zeroshot" Flamingo produces more reliable data: more examples relabeled "Lift a car" will actually reflect the appropriate behavior.</p>
<p>While initial results suggest that "Fewshot" is actually a worse relabeling function, we find that "Fewshot" provides an important benefit: it helps calibrate Flamingo's confidence in its relabeling (Fig. S6). We experiment with filtering out low-confidence labels by progressively dropping the lowest decile. We find that this filtering dramatically increases "Fewshot" precision, but only slightly increases "Zeroshot" precision ( Fig. 7A). We filter out the least-confident 50% of the labels and retrain agents on the remaining trajectories. This substantially improves "Fewshot" performance (from 52.9% to 78.6%) while only marginally improving "Zeroshot" (64.4% to 66.1%; Fig. 7B).</p>
<p>LABEL PROPERTIES AND TASK PERFORMANCE</p>
<p>These results suggest that relabeling precision may be more important than accuracy for HER. We quantify this by looking at task success resulting from different label sets.</p>
<p>Formally, we are interested in whether label accuracy or label precision is a better predictor of downstream task performance. We have 10 tasks (each of the individual objects used in Section 5.1, i.e. "Lift a car", "Lift a plane"), and four sets of labels for each (Zeroshot, Zeroshot filtered, Fewshot, and Fewshot filtered). We use a mixed-effects linear regression to predict task success for each of these 40 data points, with fixed effects of label precision and recall, and random effects for each of the 10 tasks. We find that both accuracy and precision are significant, but the effect size of precision is nearly ten times that of accuracy (accuracy: β = .16, t(35.88) = 3.416, p &lt; .01; precision: β = 1.44, t(35.76) = 11.5, p &lt; 1e − 10; see Table S4). Fig 7C plots regression lines for accuracy and precision respectively. This result yields the valuable general insight that relabeling precision is more important than relabeling accuracy for downstream task performance with HER. Figure 7. Analysis of label noise and downstream performance (Sec. 5.5). A: Filtering out low-confidence labels dramatically increases "Fewshot" label precision, but only marginally improves "Zeroshot." B: Filtering out the least confident 50% of labels and re-training agents results in a substantial performance gain for "Fewshot" but not "Zeroshot." C: Analysis of label characteristics against downstream task performance on a per-object basis suggests that relabeling precision is more important than accuracy.</p>
<p>Discussion</p>
<p>In this work, we used a VLM pretrained on internet data to teach an embodied agent language groundings. We used prompting and fewshot learning to guide the VLM's text generation, focusing it on specific dimensions of the visual stream. This allows us to flexibly distill task-relevant subsets of the VLM's language grounding into the embodied agent.</p>
<p>We note several limitations to our work. First, we focused on English instructions; future work could experiment with pretrained translation models to develop multilingual grounded agents. Second, we demonstrated our method within the classic HER formulation (Andrychowicz et al., 2017) and thus used a task structure that permits relabeling of the final observation only. We additionally used the dataset from Interactive Agents Team (2021) to learn a low-level motor policy before using our method to teach task semantics. Future work could extend our method beyond traditional HER, using a VLM to annotate observation pairs or full videos. Such extensions would facilitate teaching temporally-extended tasks, making our method suitable for training motor policies in addition to task semantics (e.g., teaching both how to lift an object and which object to lift). Finally, the Flamingo model (Alayrac et al., 2022) used in this work is not publicly available. However, the recently released GPT-4 (OpenAI, 2023) and open-source Prismer models provide comparable capabilities (Liu et al., 2023). Object classification or detection models (Minderer et al., 2022) may also be used to replicate a subset of our method's functionality.</p>
<p>Future work may experiment with other ways to leverage pretrained generative VLMs for embodied agents. Following our offline supervision approach, VLMs could provide a reward signal during training on the basis of vision and text alignment. Alternatively, VLMs could be used directly in the agent. While large model size 4 might mitigate their use for low-level motor policies, they could be used to produce language observations for input to other models (Zeng et al., 2022;Huang et al., 2022b;Dasgupta et al., 2022). Our method may also be straightforwardly applied to visionlanguage navigation, another popular testbed for embodied agents (Anderson et al., 2018). Overall, the confluence of more naturalistic environments (Shridhar et al., 2020;Savva et al., 2019;Li et al., 2021) with strong and flexible pretrained VLMs (Alayrac et al., 2022;Liu et al., 2023;OpenAI, 2023) makes these models an appealing source of domain-general language groundings. We hope that our method spurs further research leveraging their strengths for embodied agents.</p>
<p>A. Appendix Figure S1. Schematic of our batched HER implementation (Section 4). We generate a batch of trajectories, relabel those trajectories with a VLM, and then re-train the agent with behavioral cloning. We then rollout the retrained agent on target tasks to evaluate performance.   Table S4. Regressing task performance on label accuracy and precision shows that the effects of label precision far outweigh those of accuracy (Section 5.5). Figure S3. Additional "Zeroshot" (ZS, Section 5.1) and "Fewshot" (FS, Section 5.5) relabeling examples. Regular text is the prompt and bold text is the VLM generation. "Correct" relabelings are green, "Wrong" relabelings are red, and "Irrelevant" text is gray. "Zeroshot" relabeling generally results in reasonable text, but is distracted by unusual visual features (such as the cube that appears around objects, top left; or the yellow ray indicating the agent is grasping something; top right). Figure S4. Additional examples for Section 5.2. VLM relabelings using zeroshot "names" (top) and "color" (bottom) prompts. Regular text is part of the prompt and bold text is the VLM generation. "Correct" relabelings are green, "Wrong" relabelings are red, and "Irrelevant" text is gray. We conduct parallel VLM-based relabelings of the same initial trajectories in order to teach the agent to recognize objects or colors respectively. Figure S5. Additional examples for Section 5.3. The basic prompt template was the same for all; few-shot Flamingo provided in-context examples of food or toy items. Ironically, zero-shot Flamingo claimed that most food items were either "toys" or some variant of "both", likely due to their 3D rendered appearances (Table S2). Providing in-context examples readily overcame this issue (Table S3). Figure S6. Stacked histograms showing label quality as a function of Flamingo's confidence. For both, wrong or irrelevant labels tend to be low confidence. However, "Fewshot" is better calibrated: virtually all correct labels are high-confidence. Figure S7. Unigram frequency (lowercased and stopword-filtered) for "Zeroshot" (Sec. 5.1) and "Fewshot" (Sec. 5.5) relabeling. There were 11051 relabeled trajectories.</p>
<p>B. OWL-ViT Comparisons</p>
<p>We compared OWL-ViT (Minderer et al., 2022) based relabeling to Flamingo in our Experiment 1 (Section 5.1). We provided OWL-ViT with the list of 10 objects used in the experiment (plane, basketball, chair, table. . . ) and took the highest-confidence detection. We found that OWL-ViT performed poorly: on a subset of 300 trajectories, it predicted the held object only 7.2% of the time, compared to Flamingo's zero-shot 54.8% performance. OWL-ViT only ever predicted a limited subset of objects and achieved extremely low precision on these (Table S5). Flamingo had higher accuracy than OWL-ViT across 9/10 objects, often by wide margins (Table S6).  Table S6. Flamingo achieved substantially higher accuracy than OWL-ViT on 9/10 objects.</p>
<p>OWL-ViT</p>
<p>Our OWL-ViT implementation performed as expected on real-world images, so we hypotheize its poor performance here is due to domain shift from real-world to Unity-generated environments. Notably, Flamingo often produced captions which reflected this domain shift (e.g. "a 3d model of a car", Fig. 2). It may thus be possible to improve OWL-ViT's performance by changing the target labels to reflect the objects' idiosyncratic visual appearances. However, this necessity reflects the brittleness of the object-detection approach, compared to zero-shot generative captioning.</p>
<p>C. Testing Preference Generalization</p>
<p>We investigated whether the system is able to generalize to other instances of preferred objects (Section 5.4). Specifically, we kept the few-shot prompt the same, but varied the color of the objects in the environment by permuting the toys' colors. We then examined the VLM's ability to re-label them. We found that across all 5 toys, for both "aligned" and "arbitrary" preference structures, the VLM readily generalized to the new colors (Table S7). The VLM's accuracy is consistent across objects (Table S8) </p>
<p>Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).</p>
<p>Figure 1 .
1Overview of our approach. A: We use a generative VLM to re-label agent trajectories for hindsight experience replay. B: Varying the prompt allows us to relabel trajectories along multiple dimensions to teach the embodied agent different tasks.</p>
<p>Figure 3
3Figure 3. Using a pretrained VLM to flexibly teach object attributes (name or color) from a single set of trajectories (Sec. 5.2). A: We randomize color mappings so each object can appear red, green, blue, pink, or yellow. B: We again use generic prompts, adding a second color-oriented prompt to obtain color labels. C: Results on the "Name" and "Color" tasks from the "Original" agent and after retraining with VLM labels ("Zeroshot") or ground truth labels from the environment ("Oracle").</p>
<p>Figure 4 .
4Using a pretrained VLM to teach category structure (Sec. 5.3). A: We use a set of 10 objects, 5 "food" and 5 "toys." B: We experiment with both "Zeroshot" (ZS) and "Fewshot" (FS) prompting. We vary the number of fewshot examples from each category from one (FS-1 is given 3 examples each of carrots and robots) to all five (FS-5 is given 3 examples each of all 5 objects in each category). C: Results for the "Original" agent and after retraining with different relabelings. Performance increases substantially from Zeroshot to FS-2 then plateaus. This suggests that partial information about the category is sufficient for Flamingo to extrapolate to new food and toys.</p>
<p>Figure S2 .
S2Original and retrained task success per object (Section 5.1).</p>
<p>Table S2 .
S2All "food or toy" "Zeroshot" captions (lowercased and punctuation-stripped). There were 10002 total relabeled trajectories (Section 5.3).Table S3. All "food or toy" "Fewshot-1" captions (lowercased and punctuation-stripped). There were 10002 total relabeled trajectories (Section 5.3).Fewshot-1 Caption Frequency </p>
<p>toy 
6186 
food 
3816 </p>
<p>Effect 
Group 
Term 
Estimate Std. Error Statistic DOF 
P Value 
1 fixed 
(Intercept) 
-0.70 
0.11 
-6.43 36.50 1.74e-07 *** 
2 fixed 
Label Precision 
1.44 
0.13 
11.50 35.76 1.43e-13 *** 
3 fixed 
Label Accuracy 
0.16 
0.05 
3.42 35.88 
0.00159 ** 
4 ran pars Task 
sd (Intercept) 
0.05 
5 ran pars Residual sd Observation 
0.05 </p>
<p>Table S5. OWL-ViT consistently predicted a small subset of the 10 object labels, achieving very low precision.Label Count Precision </p>
<p>basketball 
39 
0.10 
book 
69 
0.12 
chair 
44 
0.07 
table 
140 
0.04 </p>
<p>Object 
Count OWL-ViT Accuracy Flamingo Accuracy </p>
<p>banana 
34 
0.00 
0.76 
basketball 
47 
0.09 
1.00 
book 
27 
0.30 
0.11 
car 
23 
0.00 
0.65 
carrot 
28 
0.00 
0.14 
chair 
24 
0.12 
0.58 
pear 
14 
0.00 
0.64 
plane 
43 
0.00 
0.58 
racket 
34 
0.00 
0.24 
table 
18 
0.33 
0.50 </p>
<p>. Table S7. The VLM readily generalized to relabeling recolored objects with ad-hoc category structures.Table S8. The VLM readily generalized across all objects, achieving comparable accuracy on original and recolored versions.Preference Structure Coloring 
Accuracy </p>
<p>Aligned 
Canonical 
0.91 
Aligned 
Recolored 
0.92 
Arbitrary 
Canonical 
0.71 
Arbitrary 
Recolored 
0.70 </p>
<p>preferences unity object name color 
correct </p>
<p>Aligned 
Car 
Canonical -aquamarine 
0.91 
Aligned 
Car 
Recolored -red 
0.92 
Aligned 
Dice 
Canonical -white 
0.97 
Aligned 
Dice 
Recolored -purple 
0.98 
Aligned 
Plane 
Canonical -orange 
0.85 
Aligned 
Plane 
Recolored -aquamarine 
0.87 
Aligned 
Robot 
Canonical -purple 
0.96 
Aligned 
Robot 
Recolored -orange 
0.92 
Aligned 
Train 
Canonical -none 
0.87 
Aligned 
Train 
Recolored -white 
0.92 
Arbitrary 
Car 
Canonical -aquamarine 
0.46 
Arbitrary 
Car 
Recolored -red 
0.47 
Arbitrary 
Dice 
Canonical -white 
0.94 
Arbitrary 
Dice 
Recolored -purple 
0.96 
Arbitrary 
Plane 
Canonical -orange 
0.85 
Arbitrary 
Plane 
Recolored -aquamarine 
0.80 
Arbitrary 
Robot 
Canonical -purple 
0.70 
Arbitrary 
Robot 
Recolored -orange 
0.64 
Arbitrary 
Train 
Canonical -none 
0.59 
Arbitrary 
Train 
Recolored -white 
0.55 </p>
<p>Due to the tokenizer used in the agent, we filtered out all episodes containing the substring "ball" rather than "basketball."
Our VLM(Alayrac et al., 2022) contains 80B parameters, while our agent (Interactive Agents Team, 2021) contains 57M.
Table S1. Top 25 Zeroshot captions (lowercased and punctuation-stripped). There were 11051 total relabeled trajectories (Section 5.1).
AcknowledgementsWe thank Christine Kaeser-Chen, Nathaniel Wong, Fede Carnevale, Alexandre Fréchette, Felix Hill, and the Deep-Mind NYC team for their advice and assistance.ReferencesAbramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M., Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., et al. Imitating interactive intelligence. arXiv preprint arXiv:2012.05672, 2020.Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.Zeroshot Caption Frequency its a toy 7704 it is a toy 1022 its both 676 both 400 its a food toy 63 its a toy but its also food 30 its food 22 it is both 20 it is a toy but it is also food 9 its a toy but its also a food 8 its a food 7 its a toy but it can be used as a food 5 its a toy but it is also food 3 food 3 its a toy but its not a toy 2 its food but its also a toy 2 its a toy carrot 2 its food but its not edible 2 its a toy but its a toy that you can eat 2 its a toy but it can be used as food 2 its a food simulator 2 its a toy its a food toy 1 its a toy but it can be used as food if you wan... 1 its a toy that is also food 1 its a food that is also a toy 1 its a toy that looks like food 1 its a toy its a toy 1 it is a food 1 its food for your mind 1 its a toy but it can be used to feed your pet 1 its a toy but its not a toy you can play with 1 its a food but its not edible 1 it is a food toy 1 its a toy but it can be a food too 1 it is a toy but the food is coming soon 1 it is a toy it is not edible 1 its a toy but its not a ball 1
Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, R Ring, E Rutherford, S Cabi, T Han, Z Gong, S Samangooei, M Monteiro, J Menick, S Borgeaud, A Brock, A Nematzadeh, S Sharifzadeh, M Binkowski, R Barreira, O Vinyals, A Zisserman, K Simonyan, Advances in Neural Information Processing Systems. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Has- son, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Si- monyan, K. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, Van Den, A Hengel, CVPR. Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., and van den Hengel, A. Vision-and-language navigation: Interpreting visually- grounded navigation instructions in real environments. In CVPR, pp. 3674-3683, 2018.</p>
<p>Hindsight experience replay. Advances in neural information processing systems. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O Pieter Abbeel, W Zaremba, 30Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O., and Zaremba, W. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.</p>
<p>Weakly supervised learning of semantic parsers for mapping instructions to actions. Y Artzi, L Zettlemoyer, Transactions of the Association for Computational Linguistics. 1Artzi, Y. and Zettlemoyer, L. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Lin- guistics, 1:49-62, 2013.</p>
<p>Learning to understand goal specifications by modelling reward. D Bahdanau, F Hill, J Leike, E Hughes, A Hosseini, P Kohli, E Grefenstette, International Conference on Learning Representations. Bahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and Grefenstette, E. Learning to understand goal specifications by modelling reward. In International Conference on Learning Representations, 2018.</p>
<p>Ad hoc categories. L W Barsalou, Memory &amp; cognition. 11Barsalou, L. W. Ad hoc categories. Memory &amp; cognition, 11:211-227, 1983.</p>
<p>Explore and explain: Selfsupervised navigation and recounting. R Bigazzi, F Landi, M Cornia, S Cascianelli, L Baraldi, R Cucchiara, 25th International Conference on Pattern Recognition (ICPR). Bigazzi, R., Landi, F., Cornia, M., Cascianelli, S., Baraldi, L., and Cucchiara, R. Explore and explain: Self- supervised navigation and recounting. In 25th Interna- tional Conference on Pattern Recognition (ICPR), pp. 1152-1159, 2021.</p>
<p>Learning to map natural language instructions to physical quadcopter control using simulated flight. V Blukis, Y Terme, E Niklasson, R A Knepper, Artzi , Y , CoRLBlukis, V., Terme, Y., Niklasson, E., Knepper, R. A., and Artzi, Y. Learning to map natural language instructions to physical quadcopter control using simulated flight. In CoRL, 2019.</p>
<p>Few-shot object grounding and mapping for natural language robot instruction following. V Blukis, R A Knepper, Artzi , Y , arXiv:2011.07384abs/2011.07384arXiv preprintBlukis, V., Knepper, R. A., and Artzi, Y. Few-shot object grounding and mapping for natural language robot in- struction following. arXiv preprint arXiv:2011.07384, abs/2011.07384, 2020.</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse- lut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Learning to win by reading manuals in a Monte-Carlo framework. S Branavan, D Silver, R Barzilay, T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T J Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, Amodei , D , Journal of Artificial Intelligence Research. 43Language models are few-shot learners. ArXiv, abs/2005.14165, 2020Branavan, S., Silver, D., and Barzilay, R. Learning to win by reading manuals in a Monte-Carlo framework. Journal of Artificial Intelligence Research, 43:661-704, 2012. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T. J., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>A F C Bucker, L F C Figueredo, S Haddadin, A Kapoor, S Ma, S Vemprala, R Bonatti, Latte, abs/2208.02918Language trajectory transformEr. Bucker, A. F. C., Figueredo, L. F. C., Haddadin, S., Kapoor, A., Ma, S., Vemprala, S., and Bonatti, R. LaTTe: Lan- guage trajectory transformEr. ArXiv, abs/2208.02918, 2022.</p>
<p>Language to action: Towards interactive task learning with physical agents. J Y Chai, Q Gao, L She, S Yang, S Saba-Sadiya, G Xu, IJCAI. Chai, J. Y., Gao, Q., She, L., Yang, S., Saba-Sadiya, S., and Xu, G. Language to action: Towards interactive task learning with physical agents. In IJCAI, pp. 2-9, 2018.</p>
<p>Augmenting experience via teacher's advice for multi-goal reinforcement learning. H Chan, Y Wu, J Kiros, S Fidler, J Ba, Actrce, arXiv:1902.04546arXiv preprintChan, H., Wu, Y., Kiros, J., Fidler, S., and Ba, J. Actrce: Augmenting experience via teacher's advice for multi-goal reinforcement learning. arXiv preprint arXiv:1902.04546, 2019.</p>
<p>Gated-attention architectures for task-oriented language grounding. D S Chaplot, K M Sathyendra, R K Pasumarthi, D Rajagopal, R Salakhutdinov, AAAI. Chaplot, D. S., Sathyendra, K. M., Pasumarthi, R. K., Ra- jagopal, D., and Salakhutdinov, R. Gated-attention archi- tectures for task-oriented language grounding. In AAAI, 2018.</p>
<p>B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Stone, D Kappler, arXiv:2209.09874Open-vocabulary queryable scene representations for real world planning. arXiv preprintChen, B., Xia, F., Ichter, B., Rao, K., Gopalakrishnan, K., Ryoo, M. S., Stone, A., and Kappler, D. Open-vocabulary queryable scene representations for real world planning. arXiv preprint arXiv:2209.09874, 2022a.</p>
<p>Learning to interpret natural language navigation instructions from observations. D Chen, R Mooney, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence25Chen, D. and Mooney, R. Learning to interpret natural language navigation instructions from observations. In Proceedings of the AAAI Conference on Artificial Intelli- gence, volume 25, pp. 859-865, 2011.</p>
<p>Learning from unlabeled 3d environments for visionand-language navigation. S Chen, P.-L Guhur, M Tapaswi, C Schmid, I Laptev, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part XXXIXChen, S., Guhur, P.-L., Tapaswi, M., Schmid, C., and Laptev, I. Learning from unlabeled 3d environments for vision- and-language navigation. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, Octo- ber 23-27, 2022, Proceedings, Part XXXIX, pp. 638-655. Springer, 2022b.</p>
<p>Ask your humans: Using human instructions to improve generalization in reinforcement learning. V Chen, A K Gupta, K Marino, 2020Chen, V., Gupta, A. K., and Marino, K. Ask your humans: Using human instructions to improve generalization in reinforcement learning. ICLR, 2020.</p>
<p>Higher: Improving instruction following with hindsight generation for experience replay. G Cideron, M Seurin, F Strub, O Pietquin, 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEECideron, G., Seurin, M., Strub, F., and Pietquin, O. Higher: Improving instruction following with hindsight genera- tion for experience replay. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 225- 232. IEEE, 2020.</p>
<p>Guiding policies with language via meta-learning. J D Co-Reyes, A Gupta, S Sanjeev, N Altieri, J Andreas, J Denero, P Abbeel, S Levine, International Conference on Learning Representations. Co-Reyes, J. D., Gupta, A., Sanjeev, S., Altieri, N., Andreas, J., DeNero, J., Abbeel, P., and Levine, S. Guiding poli- cies with language via meta-learning. In International Conference on Learning Representations, 2018.</p>
<p>Collaborating with language models for embodied reasoning. I Dasgupta, C Kaeser-Chen, K Marino, A Ahuja, S Babayan, F Hill, Fergus , R , Second Workshop on Language and Reinforcement Learning. Dasgupta, I., Kaeser-Chen, C., Marino, K., Ahuja, A., Babayan, S., Hill, F., and Fergus, R. Collaborating with language models for embodied reasoning. In Sec- ond Workshop on Language and Reinforcement Learning, 2022.</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K N Toutanova, Bert, Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. N. BERT: Pre-training of deep bidirectional transformers for language understanding. 2018. URL https://arxiv. org/abs/1810.04805.</p>
<p>Building open-ended embodied agents with internet-scale knowledge. L J Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Minedojo, Advances in Neural Information Processing Systems. Fan, L. J., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anand- kumar, A. MineDojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 2022.</p>
<p>Speaker-follower models for visionand-language navigation. D Fried, R Hu, V Cirik, A Rohrbach, J Andreas, L.-P Morency, T Berg-Kirkpatrick, K Saenko, D Klein, Darrell , T , Advances in Neural Information Processing Systems. 31Fried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L.-P., Berg-Kirkpatrick, T., Saenko, K., Klein, D., and Darrell, T. Speaker-follower models for vision- and-language navigation. Advances in Neural Informa- tion Processing Systems, 31, 2018.</p>
<p>Airbert: In-domain pretraining for vision-andlanguage navigation. P.-L Guhur, M Tapaswi, S Chen, I Laptev, C Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionGuhur, P.-L., Tapaswi, M., Chen, S., Laptev, I., and Schmid, C. Airbert: In-domain pretraining for vision-and- language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1634- 1643, 2021.</p>
<p>Towards learning a generic agent for vision-and-language navigation via pre-training. W Hao, C Li, X Li, L Carin, J Gao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHao, W., Li, C., Li, X., Carin, L., and Gao, J. Towards learning a generic agent for vision-and-language naviga- tion via pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13137-13146, 2020.</p>
<p>The symbol grounding problem. S Harnad, Physica D: Nonlinear Phenomena. 421-3Harnad, S. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335-346, 1990.</p>
<p>Grounded language learning fast and slow. F Hill, O Tieleman, T Von Glehn, N Wong, H Merzic, Clark , S , International Conference on Learning Representations. Hill, F., Tieleman, O., von Glehn, T., Wong, N., Merzic, H., and Clark, S. Grounded language learning fast and slow. In International Conference on Learning Representations, 2021.</p>
<p>What makes certain pre-trained visual representations better for robotic learning?. K Hsu, T G W Lum, R Gao, S S Gu, J Wu, C Finn, NeurIPS 2022 Foundation Models for Decision Making Workshop. Hsu, K., Lum, T. G. W., Gao, R., Gu, S. S., Wu, J., and Finn, C. What makes certain pre-trained visual representations better for robotic learning? In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, abs/2201.07207ArXiv. Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. ArXiv, abs/2201.07207, 2022a.</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, T Jackson, N Brown, L Luu, S Levine, K Hausman, B Ichter, 6th Annual Conference on Robot Learning. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T., Brown, N., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner monologue: Embodied reasoning through planning with language models. In 6th Annual Conference on Robot Learning, 2022b.</p>
<p>Interactive Agents Team, D. Creating multimodal interactive agents with imitation and self-supervised learning. arXiv:2112.03763arXiv preprintInteractive Agents Team, D. Creating multimodal interactive agents with imitation and self-supervised learning. arXiv preprint arXiv:2112.03763, 2021.</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, arXiv:2304.02643Segment anything. arXiv preprintKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.</p>
<p>Toward understanding natural language directions. T Kollar, S Tellex, D Roy, Roy , N , 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEEKollar, T., Tellex, S., Roy, D., and Roy, N. Toward un- derstanding natural language directions. In 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 259-266. IEEE, 2010.</p>
<p>Active learning for teaching a robot grounded relational symbols. J Kulick, M Toussaint, T Lang, M Lopes, IJCAI. CiteseerKulick, J., Toussaint, M., Lang, T., and Lopes, M. Active learning for teaching a robot grounded relational symbols. In IJCAI, pp. 1451-1457. Citeseer, 2013.</p>
<p>W Kuo, Y Cui, X Gu, A Piergiovanni, A Angelova, arXiv:2209.15639Open-vocabulary object detection upon frozen vision and language models. arXiv preprintKuo, W., Cui, Y., Gu, X., Piergiovanni, A., and An- gelova, A. F-vlm: Open-vocabulary object detection upon frozen vision and language models. arXiv preprint arXiv:2209.15639, 2022.</p>
<p>igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. C Li, F Xia, R Martín-Martín, M Lingelbach, S Srivastava, B Shen, K Vainio, C Gokmen, G Dharan, T Jain, arXiv:2108.03272arXiv preprintLi, C., Xia, F., Martín-Martín, R., Lingelbach, M., Srivas- tava, S., Shen, B., Vainio, K., Gokmen, C., Dharan, G., Jain, T., et al. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. arXiv preprint arXiv:2108.03272, 2021.</p>
<p>Robust navigation with language pretraining and stochastic sampling. X Li, C Li, Q Xia, Y Bisk, A Celikyilmaz, J Gao, N A Smith, Y Choi, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingLi, X., Li, C., Xia, Q., Bisk, Y., Celikyilmaz, A., Gao, J., Smith, N. A., and Choi, Y. Robust navigation with language pretraining and stochastic sampling. In Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1494-1499, 2019.</p>
<p>Inferring rewards from language in context. J Lin, D Fried, D Klein, A Dragan, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandLong Papers1Association for Computational LinguisticsLin, J., Fried, D., Klein, D., and Dragan, A. Inferring re- wards from language in context. In Proceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pp. 8546- 8560, Dublin, Ireland, May 2022. Association for Com- putational Linguistics.</p>
<p>Prismer: A vision-language model with an ensemble of experts. S Liu, L Fan, E Johns, Z Yu, C Xiao, Anandkumar , A , arXiv:2303.02506arXiv preprintLiu, S., Fan, L., Johns, E., Yu, Z., Xiao, C., and Anand- kumar, A. Prismer: A vision-language model with an ensemble of experts. arXiv preprint arXiv:2303.02506, 2023.</p>
<p>Grounding language in play. C Lynch, P Sermanet, arXiv:2005.07648arXiv preprintLynch, C. and Sermanet, P. Grounding language in play. arXiv preprint arXiv:2005.07648, 2020.</p>
<p>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B S Devnani, J Hoffman, D Batra, Advances in Neural Information Processing Systems. Majumdar, A., Aggarwal, G., Devnani, B. S., Hoffman, J., and Batra, D. ZSON: Zero-shot object-goal navigation us- ing multimodal goal embeddings. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. H Mei, M Bansal, M R Walter, AAAI. Mei, H., Bansal, M., and Walter, M. R. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI, 2016.</p>
<p>Linguistic regularities in continuous space word representations. T Mikolov, W.-T Yih, G Zweig, Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies. the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologiesMikolov, T., Yih, W.-t., and Zweig, G. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pp. 746-751, 2013.</p>
<p>Simple open-vocabulary object detection. M Minderer, A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy, A Mahendran, A Arnab, M Dehghani, Z Shen, X Wang, X Zhai, T Kipf, N Houlsby, Computer Vision -ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringer-VerlagProceedings, Part XMinderer, M., Gritsenko, A., Stone, A., Neumann, M., Weis- senborn, D., Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z., Wang, X., Zhai, X., Kipf, T., and Houlsby, N. Simple open-vocabulary object detection. In Computer Vision -ECCV 2022: 17th European Confer- ence, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part X, pp. 728-755. Springer-Verlag, 2022.</p>
<p>Mapping instructions and visual observations to actions with reinforcement learning. D Misra, J Langford, Artzi , Y , Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingMisra, D., Langford, J., and Artzi, Y. Mapping instructions and visual observations to actions with reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1004-1015, 2017.</p>
<p>Learning goal-oriented hierarchical tasks from situated interactive instruction. S Mohan, J Laird, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence28Mohan, S. and Laird, J. Learning goal-oriented hierarchical tasks from situated interactive instruction. In Proceed- ings of the AAAI Conference on Artificial Intelligence, volume 28, 2014.</p>
<p>Learning to connect language and perception. R J Mooney, AAAI. ChicagoMooney, R. J. Learning to connect language and perception. In AAAI, pp. 1598-1601. Chicago, 2008.</p>
<p>R3M: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, arXiv:2203.12601arXiv preprintNair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A. R3M: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.</p>
<p>Grounding language for transfer in deep reinforcement learning. K Narasimhan, R Barzilay, T Jaakkola, 1076- 9757J. Artif. Int. Res. 631Narasimhan, K., Barzilay, R., and Jaakkola, T. Grounding language for transfer in deep reinforcement learning. J. Artif. Int. Res., 63(1):849-874, sep 2018. ISSN 1076- 9757.</p>
<p>Interactive learning from activity description. K X Nguyen, D Misra, R Schapire, M Dudík, P Shafto, International Conference on Machine Learning. PMLRNguyen, K. X., Misra, D., Schapire, R., Dudík, M., and Shafto, P. Interactive learning from activity description. In International Conference on Machine Learning, pp. 8096-8108. PMLR, 2021.</p>
<p>. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.</p>
<p>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. L Pinto, A Gupta, 10.1109/ICRA.2016.74875172016 IEEE International Conference on Robotics and Automation (ICRA). Pinto, L. and Gupta, A. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 3406-3413, 2016. doi: 10.1109/ICRA.2016.7487517.</p>
<p>. W V O Quine, Word, Object, MIT PressQuine, W. V. O. Word &amp; Object. MIT Press, 1960.</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, International Conference on Machine Learning. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021.</p>
<p>Hierarchical text-conditional image generation with clip latents. A Ramesh, P Dhariwal, A Nichol, C Chu, Chen , M , abs/2204.06125ArXiv. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022.</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionSavva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., et al. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9339-9347, 2019.</p>
<p>Robotic navigation with large pre-trained models of language, vision, and action. D Shah, B Osiński, S Levine, 6th Annual Conference on Robot Learning. Shah, D., Osiński, B., Levine, S., et al. LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action. In 6th Annual Conference on Robot Learning, 2022.</p>
<p>Skill induction and planning with latent language. P Sharma, A Torralba, Andreas , J , Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1Sharma, P., Torralba, A., and Andreas, J. Skill induction and planning with latent language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1713-1726, 2022.</p>
<p>Teaching robots new actions through natural language instructions. L She, Y Cheng, J Y Chai, Y Jia, S Yang, N Xi, The 23rd IEEE International Symposium on Robot and Human Interactive Communication. IEEEShe, L., Cheng, Y., Chai, J. Y., Jia, Y., Yang, S., and Xi, N. Teaching robots new actions through natural language instructions. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication, pp. 868-873. IEEE, 2014.</p>
<p>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionShridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., and Fox, D. ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pp. 10740-10749, 2020.</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Shridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. CoRL, 2021.</p>
<p>I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, arXiv:2209.11302Generating situated robot task plans using large language models. arXiv preprintSingh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A. Prog- prompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.</p>
<p>Learning rewards from linguistic feedback. T R Sumers, M K Ho, R D Hawkins, K Narasimhan, T L Griffiths, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Sumers, T. R., Ho, M. K., Hawkins, R. D., Narasimhan, K., and Griffiths, T. L. Learning rewards from linguistic feedback. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 6002-6010, 2021.</p>
<p>. A Tam, N C Rabinowitz, A K Lampinen, N A Roy, S C Chan, D Strouse, J X Wang, Banino, A., andTam, A., Rabinowitz, N. C., Lampinen, A. K., Roy, N. A., Chan, S. C., Strouse, D., Wang, J. X., Banino, A., and</p>
<p>Semantic exploration from language abstractions and pretrained representations. F Hill, Advances in Neural Information Processing Systems. Hill, F. Semantic exploration from language abstractions and pretrained representations. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, Roy , N , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence25Tellex, S., Kollar, T., Dickerson, S., Walter, M., Baner- jee, A., Teller, S., and Roy, N. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pp. 1507-1514, 2011.</p>
<p>Robots that use language. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Robotics, and Autonomous Systems. 312020Annual Review of ControlTellex, S., Gopalan, N., Kress-Gazit, H., and Matuszek, C. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems, 3(1), 2020.</p>
<p>Opportunistic active learning for grounding natural language descriptions. J Thomason, A Padmakumar, J Sinapov, J Hart, P Stone, R Mooney, Proceedings of the 1st Annual Conference on Robot Learning. the 1st Annual Conference on Robot LearningThomason, J., Padmakumar, A., Sinapov, J., Hart, J., Stone, P., and Mooney, R. Opportunistic active learning for grounding natural language descriptions. In Proceedings of the 1st Annual Conference on Robot Learning (CoRL- 17), 2017.</p>
<p>Understanding natural language. T Winograd, Cognitive psychology. 31Winograd, T. Understanding natural language. Cognitive psychology, 3(1):1-191, 1972.</p>
<p>Robotic skill acquisition via instruction augmentation with visionlanguage models. T Xiao, H Chan, P Sermanet, A Wahid, A Brohan, K Hausman, S Levine, J Tompson, arXiv:2211.11736arXiv preprintXiao, T., Chan, H., Sermanet, P., Wahid, A., Brohan, A., Hausman, K., Levine, S., and Tompson, J. Robotic skill acquisition via instruction augmentation with vision- language models. arXiv preprint arXiv:2211.11736, 2022.</p>
<p>Intra-agent speech permits zero-shot task acquisition. C Yan, F Carnevale, P Georgiev, A Santoro, A Guy, A Muldal, C.-C Hung, J S Abramson, T P Lillicrap, Wayne , G , Advances in Neural Information Processing Systems. Yan, C., Carnevale, F., Georgiev, P., Santoro, A., Guy, A., Muldal, A., Hung, C.-C., Abramson, J. S., Lillicrap, T. P., and Wayne, G. Intra-agent speech permits zero-shot task acquisition. In Advances in Neural Information Process- ing Systems, 2022.</p>
<p>Interactive grounded language acquisition and generalization in a 2D world. H Yu, H Zhang, W Xu, In ICLR. Yu, H., Zhang, H., and Xu, W. Interactive grounded lan- guage acquisition and generalization in a 2D world. In ICLR, 2018.</p>
<p>Socratic models: Composing zeroshot multimodal reasoning with language. A Zeng, A Wong, S Welker, K Choromanski, F Tombari, A Purohit, M Ryoo, V Sindhwani, J Lee, V Vanhoucke, arXiv:2204.00598arXiv preprintZeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., Lee, J., Van- houcke, V., et al. Socratic models: Composing zero- shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.</p>
<p>RTFM: Generalising to new environment dynamics via reading. V Zhong, T Rocktäschel, E Grefenstette, ICLR. Zhong, V., Rocktäschel, T., and Grefenstette, E. RTFM: Generalising to new environment dynamics via reading. In ICLR, 2020.</p>
<p>SILG: The multi-domain symbolic interactive language grounding benchmark. V Zhong, A W Hanjie, S Wang, K Narasimhan, L Zettlemoyer, Advances in Neural Information Processing Systems. 34Zhong, V., Hanjie, A. W., Wang, S., Narasimhan, K., and Zettlemoyer, L. SILG: The multi-domain symbolic inter- active language grounding benchmark. Advances in Neu- ral Information Processing Systems, 34:21505-21519, 2021.</p>
<p>Improving policy learning via language dynamics distillation. V Zhong, J Mu, L Zettlemoyer, E Grefenstette, T Rocktäschel, Advances in Neural Information Processing Systems. Zhong, V., Mu, J., Zettlemoyer, L., Grefenstette, E., and Rocktäschel, T. Improving policy learning via language dynamics distillation. In Advances in Neural Information Processing Systems, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
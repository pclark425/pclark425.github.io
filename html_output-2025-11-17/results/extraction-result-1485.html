<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1485 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1485</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1485</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-235420514</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2108.01928v1.pdf" target="_blank">How to Query Language Models?</a></p>
                <p><strong>Paper Abstract:</strong> Large pre-trained language models (LMs) are capable of not only recovering linguistic but also factual and commonsense knowledge. To access the knowledge stored in mask-based LMs, we can use cloze-style questions and let the model fill in the blank. The flexibility advantage over structured knowledge bases comes with the drawback of finding the right query for a certain information need. Inspired by human behavior to disambiguate a question, we propose to query LMs by example. To clarify the ambivalent question"Who does Neuer play for?", a successful strategy is to demonstrate the relation using another subject, e.g.,"Ronaldo plays for Portugal. Who does Neuer play for?". We apply this approach of querying by example to the LAMA probe and obtain substantial improvements of up to 37.8% for BERT-large on the T-REx data when providing only 10 demonstrations--even outperforming a baseline that queries the model with up to 40 paraphrases of the question. The examples are provided through the model's context and thus require neither fine-tuning nor an additional forward pass. This suggests that LMs contain more factual and commonsense knowledge than previously assumed--if we query the model in the right way.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1485.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1485.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWC LM-prior static agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TextWorld Commonsense (TWC) static agent using LM-computed object-location priors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A static TextWorld agent that picks up misplaced objects at random and places them according to a prior p(l|o) computed by a pre-trained language model (LM) conditioned with in-context demonstrations; used to evaluate whether LM-derived commonsense priors improve performance in TWC tidying tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Static agent with LM-computed prior</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A non-learning (static) agent that: (1) selects a misplaced object uniformly at random, (2) queries a masked LM (using an arrow/=> style or natural-language template with k demonstrations in-context) to compute a prior probability distribution p(l|o) over possible locations, and (3) places the object into a location sampled/selected according to that prior. No RL or fine-tuning is performed for the agent; the LM is only used to compute priors at episode start.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>An interactive text-based house environment (TextWorld-based) where an agent must tidy a modern-house scene by moving objects to their commonsense locations. Interactions consist of language commands like pick-up, put/place to locations (e.g., cupboard, dishwasher); episodes include multiple objects and locations and are configurable (easy/hard).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Tidying tasks such as moving dirty dishes to the dishwasher, placing clean dishes in the cupboard, and generally moving misplaced household objects to their commonsense locations.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Implicitly compositional: episode goal decomposes into multiple per-object pick-and-place subtasks (sequentially moving each misplaced object to its appropriate location); complexity grows with the number of objects/locations in an episode.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Not specified as a curriculum; experiments report performance on 'hard' TWC games (multi-object tidying episodes) and compare across episode difficulty, but no explicit curriculum of tasks is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>The paper reports that LM-derived priors generalize to object-location pairs not explicitly listed in static KBs and therefore can handle slight deviations of typical entities better than a static KB-based approach; this is stated qualitatively (LM priors improve the static agent's performance and provide better generalization than a rigid KB).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an LM to compute commonsense object-location priors (via in-context demonstrations) significantly improves the static agent's normalized score on the hard TWC games compared to a uniform prior baseline; the benefit of providing demonstrations to the LM saturates after ~10–15 in-context examples. The LM approach is highlighted for its flexibility and generalization to object-location pairs not enumerated in static KBs. The paper does not present this as a curriculum-learning approach for task sequencing, and no curriculum-based training or ordering of tasks is used or compared.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Query Language Models?', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TextWorld Commonsense <em>(Rating: 2)</em></li>
                <li>TextWorld <em>(Rating: 2)</em></li>
                <li>Learning to Play Text-based Games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1485",
    "paper_id": "paper-235420514",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "TWC LM-prior static agent",
            "name_full": "TextWorld Commonsense (TWC) static agent using LM-computed object-location priors",
            "brief_description": "A static TextWorld agent that picks up misplaced objects at random and places them according to a prior p(l|o) computed by a pre-trained language model (LM) conditioned with in-context demonstrations; used to evaluate whether LM-derived commonsense priors improve performance in TWC tidying tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Static agent with LM-computed prior",
            "agent_description": "A non-learning (static) agent that: (1) selects a misplaced object uniformly at random, (2) queries a masked LM (using an arrow/=&gt; style or natural-language template with k demonstrations in-context) to compute a prior probability distribution p(l|o) over possible locations, and (3) places the object into a location sampled/selected according to that prior. No RL or fine-tuning is performed for the agent; the LM is only used to compute priors at episode start.",
            "agent_size": null,
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "An interactive text-based house environment (TextWorld-based) where an agent must tidy a modern-house scene by moving objects to their commonsense locations. Interactions consist of language commands like pick-up, put/place to locations (e.g., cupboard, dishwasher); episodes include multiple objects and locations and are configurable (easy/hard).",
            "procedure_type": "commonsense procedures",
            "procedure_examples": "Tidying tasks such as moving dirty dishes to the dishwasher, placing clean dishes in the cupboard, and generally moving misplaced household objects to their commonsense locations.",
            "compositional_structure": "Implicitly compositional: episode goal decomposes into multiple per-object pick-and-place subtasks (sequentially moving each misplaced object to its appropriate location); complexity grows with the number of objects/locations in an episode.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Not specified as a curriculum; experiments report performance on 'hard' TWC games (multi-object tidying episodes) and compare across episode difficulty, but no explicit curriculum of tasks is used.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "The paper reports that LM-derived priors generalize to object-location pairs not explicitly listed in static KBs and therefore can handle slight deviations of typical entities better than a static KB-based approach; this is stated qualitatively (LM priors improve the static agent's performance and provide better generalization than a rigid KB).",
            "key_findings": "Using an LM to compute commonsense object-location priors (via in-context demonstrations) significantly improves the static agent's normalized score on the hard TWC games compared to a uniform prior baseline; the benefit of providing demonstrations to the LM saturates after ~10–15 in-context examples. The LM approach is highlighted for its flexibility and generalization to object-location pairs not enumerated in static KBs. The paper does not present this as a curriculum-learning approach for task sequencing, and no curriculum-based training or ordering of tasks is used or compared.",
            "uuid": "e1485.0",
            "source_info": {
                "paper_title": "How to Query Language Models?",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TextWorld Commonsense",
            "rating": 2,
            "sanitized_title": "textworld_commonsense"
        },
        {
            "paper_title": "TextWorld",
            "rating": 2
        },
        {
            "paper_title": "Learning to Play Text-based Games",
            "rating": 1,
            "sanitized_title": "learning_to_play_textbased_games"
        }
    ],
    "cost": 0.01021275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How to Query Language Models?</p>
<p>Leonard Adolphs 
Department of Computer Science
ETH Zürich</p>
<p>Shehzaad Dhuliawala 
Department of Computer Science
ETH Zürich</p>
<p>Thomas Hofmann 
Department of Computer Science
ETH Zürich</p>
<p>How to Query Language Models?</p>
<p>Large pre-trained language models (LMs) are capable of not only recovering linguistic but also factual and commonsense knowledge. To access the knowledge stored in mask-based LMs, we can use cloze-style questions and let the model fill in the blank. The flexibility advantage over structured knowledge bases comes with the drawback of finding the right query for a certain information need. Inspired by human behavior to disambiguate a question, we propose to query LMs by example. To clarify the ambivalent question Who does Neuer play for?, a successful strategy is to demonstrate the relation using another subject, e.g., Ronaldo plays for Portugal. Who does Neuer play for?. We apply this approach of querying by example to the LAMA probe and obtain substantial improvements of up to 37.8% for BERT-large on the T-REx data when providing only 10 demonstrations-even outperforming a baseline that queries the model with up to 40 paraphrases of the question. The examples are provided through the model's context and thus require neither fine-tuning nor an additional forward pass. This suggests that LMs contain more factual and commonsense knowledge than previously assumed-if we query the model in the right way.</p>
<p>Introduction</p>
<p>Language Models (LM) are omnipresent in modern NLP systems. In just a few years, they've been established as the standard feature extractor for many different language understanding tasks (Karpukhin et al., 2020;Zhang et al., 2020;Wang et al., 2019;He et al., 2020). Typically, they are used to create a latent representation of natural language input and then fine-tuned to the task at hand. However, recent work (Petroni et al., 2019;Jiang et al., 2020;Brown et al., 2020;Roberts et al., 2020) has shown that off-the-shelve language models capture not only linguistic features but also large amounts of relational knowledge, not requiring any form of re-training. The LAMA probe by Petroni et al. (2019) was designed to quantify the amount of relational knowledge present in (mask-based) language models. While the task of predicting the right object for a subject-relation tuple remains the same as for a standard knowledge base (KB) completion query, the input is structured in a cloze-style sentence. For example, a KB completion query of the form (Dante, born-in, X) becomes "Dante was born in [MASK].". Petroni et al. (2019) show that BERT (Devlin et al., 2019) performs on par with competitive specialized models on factual and commonsense knowledge. The performance on this task can only be seen as a lower bound to the actual knowledge present in language models as the choice of natural language template for a given relation might be suboptimal (Petroni et al., 2019;Jiang et al., 2020). The more general question here is "How to query an LM for a specific information need?". Jiang et al. (2020) propose to use multiple paraphrases of the probe and then aggregate the solutions. Petroni et al. (2020), on the other hand, add relevant context. Both approaches can be linked to common human behavior. In human dialog, a question can be made more precise both by paraphrasing or adding additional context information. Since language models are trained on large amounts of human-generated data, the intuition of phrasing the information need most naturally seems obvious. Humans excel at pattern recognition and pattern continuation for many different modes of representation (Shugen, 2002). Concepts embedded in language are no exception to this. Therefore, another common way to probe a human's knowledge is by providing examples and asking them to transfer the relation provided to a new object. For example, asking Who plays Neuer for? is ambiguous as both Bayern Munich and Germany would be correct answers. However, when contextualizing the question with an example, the answer is clear: I know Ronaldo plays for Portugal. Who plays Neuer for?.</p>
<p>In this work, we apply the concept of querying by example to probe language models. Additional to the cloze-style question, we provide other examples of the same relation to the model's input. The previous example's input then becomes "Ronaldo plays for Portugal. Neuer plays for [MASK].". We show that by providing only a few demonstrations, standard language models' prediction performance improves drastically. So much so that for the TREx dataset, it becomes an even more powerful technique to retrieve knowledge than using an ensemble of up to 40 different paraphrases (Jiang et al., 2020), while requiring only a single forward pass instead of 40.</p>
<p>Related Work</p>
<p>Language Model Probes Petroni et al. (2019) started to investigate how much factual and commonsense knowledge LMs posses. They released the LAMA probe, which is a dataset consisting of T-REx (Elsahar et al., 2018), Google-RE, Concept-Net (Speer et al., 2018), andSQuAD (Rajpurkar et al., 2016). Each dataset is transformed to be a collection of subject, relation, object -triplets and pruned to only contain single token objects present in BERT's vocabulary. Additionally, they provide templates in natural language for each relation. Their investigation reveals that BERT-large has remarkable capabilities in recalling factual knowledge, competitive to supervised baseline systems. Since there is usually more than one way to express a relation, the LAMA probe score can only be regarded as a lower bound (Petroni et al., 2019;Jiang et al., 2020). To tighten this lower bound, Jiang et al. (2020) propose an automatic discovering mechanism for paraphrases together with an aggregation scheme. By querying the LM with a diverse set of prompts, they significantly improve the LAMA probe's baseline numbers for BERT models. However, this approach incurs the cost of additional queries to the LM, an optimization procedure to aggregate the results, and the extraction of paraphrases. Machine reading comprehension (MRC) and opendomain question answering (QA) are fields in NLP dominated by large pre-trained LMs. Here, the premise typically is that the model is capable of extracting the answer from the provided context, rather than having it stored in its parameters 1 . Petroni et al. (2020) extend this line of thought to retrieve factual knowledge from LMs by providing relevant context but without fine-tuning the model. Their experiments show that providing relevant passages significantly improves the scores on the LAMA probe for BERT models.</p>
<p>Few-Shot Learning The term few-shot learning refers to the practice of only providing a few examples when training a model, compared to the typical approach of using large datasets (Wang et al., 2020). In the NLP domain, recent work by Brown et al. (2020) suggests to use these few examples only in the context, as opposed to actually training with it. Fittingly, they call this approach in-context learning. Here, they condition the model on a natural language description of the task together with a few demonstrations. Their experiments reveal that the larger the model, the better its in-context learning capabilities. Our approach is very similar to in-context learning, with the difference that we do not provide a description of the task and utilize natural language templates for the relations. The motivation is that this should closely resemble human behavior of providing examples of a relation: instead of providing a list of subject and objects and let the other person figure out the relation, a human typically provides the subject and objects embedded in the template relation. Moreover, we understand our approach not as a learning method, but rather as a querying technique that disambiguates the information need. Schick and Schütze (2020b) argue that small LMs can be effective for few-shot learning too. However, they approach the problem of limited examples differently; instead of providing it as conditioning in the input, they actually train with it. By embedding the data into relation templates, they obtain training data that is closer in style to the pre-training data and, thus, can learn with fewer samples. Gao et al. (2020) take this concept even further and automate the template generation. Additionally, they also find that-when fine-tuning with few samples-providing good demonstrations in the context improves the model's performance.</p>
<p>Background</p>
<p>Language Models for cloze-style QA</p>
<p>In this work, we probe mask-based language models for their relational knowledge. The considered facts are triplets consisting of a subject, a relation, and an object s, r, o . Language models are trained to predict the most probable word given the (surrounding) context. Hence, to test a model's factual knowledge, we feed it natural text with the object masked out. This requires a mapping from the relation r to a natural language prompt t r with placeholders for subject and object, e.g., the relation r = age becomes t r = [s] is [o] years old. When probing for a single s, r, o -triplet, the input to the language model is the natural language prompt t r of the relation r together with the subject s. It outputs a likelihood score P LM for each token in its vocabulary V which we use to construct a top-k prediction subset V for the object o:
V = arg max V ⊂V,|V |=k o ∈V P LM (o |s, t r )(1)
The language model succeeds for the triplet @k if o ∈ V . For example, we say that it knows the fact s = Tiger Woods, r = age, o = 45 @3, if for the query "Tiger Woods is [MASK] years old" it ranks the token "45" within the top-3 of the vocabulary.</p>
<p>Datasets</p>
<p>We use the LAMA probe in our experiments ( Google-RE 3 , and ConceptNet (Speer et al., 2018) are provided in Table 1.</p>
<p>Models</p>
<p>We investigate the usefulness of querying by example, for three individual language models: BERT-base, BERT-large (Devlin et al., 2019), and ALBERT-xxl (Lan et al., 2020). These models are among the most frequently used language models these days 4 . For both BERT models, we consider the cased variant, unless explicitly noted otherwise.</p>
<p>Method</p>
<p>Our proposed method for querying relational knowledge from LMs is simple yet effective. When we construct the query for the triplet s, r, o , we provide the model with additional samples { s , r, o , s , r, o , . . . } of the same relation r.</p>
<p>These additional examples are converted to their natural language equivalent using the template t r and prepend to the cloze-style sentence representation of s, r, o . The intuition is that the non-masked examples provide the model with an idea of filling in the gap for the relation at hand. As can be seen in Figure 1, providing a single example in the same structure clarifies the object requested for both humans and BERT. This is particularly useful when the template t r does not capture the desired relation r between subject s and object o unambiguously, which in natural language is likely to be the case for many relations. In this sense, it tries to solve the same problem as paraphrasing. A query is paraphrased multiple times to align the model's understanding of the query with the actual information need. When we provide additional examples, we do the same by showing the model how to apply the relation to other instances and ask it to generalize. Of course, the model does not reason in this exact way; rather, through its training data, it is biased towards completing patterns as this is a ubiquitous behavior in human writing.  Since we only adjust the context fed to the model, we do not incur the cost of additional forward passes. When paraphrasing, on the other hand, each individual template requires another query to the model. Moreover, our approach does not require any learning, i.e., backward passes, and hence is very different from the classic fine-tuning approach and pattern-exploiting training (Schick and Schütze, 2020a,b).</p>
<p>Query Predictions</p>
<p>No Example</p>
<p>In Table 2, we compare different approaches of querying by example. The left column shows the input to the model, i.e., the query. The right column shows BERT-large's top-2 prediction, with its corresponding probabilities 6 . The first row of the table shows that completing the is-a relation for the village Rodmarton is tricky for the model. Its top predictions are not even close to the correct answer suggesting that BERT either does not know about this particular village or that 5 A village in South West England. 6 The probabilities are obtained by applying a softmax on the logit output over the token vocabulary. the information need is not well enough specified. Interestingly, when prepending the query with another random example of the same relation (2nd row), the model's top predictions are town and the ground-truth village. This proves that BERT knows the type of instance Rodmarton is; only the extraction method (the cloze-style template) was not expressive enough.</p>
<p>Close Examples When humans use examples, they typically do not use a completely random subject but use one that is, by some measure, close to the subject at hand. In our introductory example, we used Ronaldo to exemplify an information need about Neuer. It would have been unnatural to use a musician here, even when describing a formally correct plays-for relation with them. We extend our approach by only using examples for which the subject is close in latent space to the subject querying for. We use the cosine similarity between the subject encodings using BERT-base. More formally, we encode a subject s using (2) with B(x) CLS being the BERT encoding of the CLS-token for the input x, and θ being the BERT model's parameters. We then obtain the top-k most similar subjects to s in the dataset D through maximizing the cosine similarity, i.e.,
f θ (s) = B θ ([CLS] + s + [SEP]) CLS ,D = arg max D ⊂D{s},|D |=k s ∈D f θ (s) f θ (s ) f θ (s) f θ (s )(3)
From the top-k subset of most similar subjects D , we randomly sample to obtain our priming examples. Table 2 (3rd row) shows the chosen close example to Rodmarton, which is Nantmor, another small village in the UK. Provided with this particular example, BERT-large predicts the ground-truth label village with more than 75% probability. Brown et al. (2020) propose to use LMs as in-context learners. They suggest providing "training" examples in the model's context using the arrow operator, i.e., to express an s, r, o triplet they provide the model with s ⇒ o. We can apply this concept to the LAMA data by using the same template t r =" [s] ⇒ [o]" ∀r. In Table 2 (last row), we see that by providing a few examples of the is-a relation, BERT-large can rank the groundtruth highest even though the relationship is never explicitly described in natural language. However, not using a natural language template makes the model less confident in its prediction, as can be seen by the lower probability mass it puts on the target.</p>
<p>Arrow Operator</p>
<p>Results</p>
<p>We focus the reporting of the results on the mean precision at k (P@k) metric. In line with previous work (Petroni et al., 2019(Petroni et al., , 2020Jiang et al., 2020) 7 , we compute the results per relation and then average across all relations of the dataset. More formally, for the dataset D = {R 1 , . . . , R n } that consists of n relations where each relation has multiple datapoints x, y , we compute the P@k score as: (4) where 1 denotes the indicator function that is 1 if the ground truth y is in the top-k prediction set V for the input x and 0 otherwise. Google-RE For the Google-RE subset of the data, querying by example hurts the predictive capabilities of LMs. In the following, we provide an intuition of why we think this is the case. Looking at the baseline numbers of the individual relations for this data, we see that the performance is largely driven by predicting a person's birth and death place; the birth-date relation doesn't play a significant role because BERT is incapable of accurately predicting numbers (i.e., dates) (Lin et al., 2020;Wallace et al., 2019). The birth and death place of a person BERT-large predicts correctly 16.1% and 14.0% of the time, respectively; significantly lower than the 32.5% P@1 score among the relations of the T-REx data. Recent work describes that BERT has a bias to predict that a person with, e.g., an Italian sounding name is Italian (Rogers et al., 2020;Poerner et al., 2020). We suspect that this bias helps BERT predict birth and death places without knowing the actual person, and therefore it is not an adequate test of probing an LMs factual knowledge. As a consequence, the predictions it makes are more prone to errors when influenced by previous examples.  Table 4 shows the improvement in P@1 score for the individual relations that most (and least) benefit from additional examples for BERT-large. The relations for which demonstrations improve the performance the most typically have one thing in common: they are ambiguous. Prototypical ambiguous relations like located-in or is-a are among the top benefiting relations. One rather untypical improvement candidate is the top-scoring one of religion-affiliation. Suspiciously, this is also the most improved relation by the paraphrasing of Jiang et al. (2020)    12%, 7.5%, and 25% relative improvement for BERT-base, BERT-large, and Albert-xxlarge.
P @k = 1 |D| R i ∈D 1 |R i | x,y ∈R i 1 V x (y),</p>
<p>T-REx</p>
<p>More detailed plots for all the corpora and several metrics are provided in Appendix A.4.</p>
<p>The Change of Embedding</p>
<p>To further investigate the disambiguation effect of additional examples, we take a look at the latent space. In particular, we're interested in how the clusters of particular relations, formed by the queries' embeddings, change when providing the context with additional examples. Figure 3 visualizes BERT-large's [CLS]-token embedding for queries from the T-REx corpus, using t-SNE (van der Maaten and Hinton, 2008). The individual colors represent the relations of the queries. The first two images depict the clustering when  using the natural language template without additional demonstrations (left) and ten demonstrations (right). The fact that the clusters become better separated is visual proof that providing examples disambiguates the information need expressed by the queries. The two plots on the right show the clustering when instead of a natural language template, the subject and object are only separated by the arrow operator "⇒". Here, we see an even more significant change in separability when providing additional demonstrations, as the actual information need is more ambiguous.</p>
<p>TextWorld Commonsense Evaluation</p>
<p>An emerging field of interest inside the NLP community is text-based games (TBG). An agent is placed inside an interactive text environment in these games and tries to complete specified goalsonly using language commands. To succeed, it  requires a deep language understanding to decide what are reasonable actions to take in the scene that move it closer to its final goal. These environments are often modeled on real-world scenes to foster the commonsense-learning capabilities of an agent. The TextWorld Commonsense (TWC) game world by Murugesan et al. (2020) focus specifically on this aspect. There, the agent is placed in a typical modern-house environment to tidy up the room. This involves moving all the objects in the scene to their commonsense location, e.g., the dirty dishes belong in the dishwasher and not in the cupboard. Murugesan et al. (2020) approach this problem by equipping the agent with access to a commonsense knowledge base. Replacing a traditional KB with an LM for this task is very intriguing as the LM has relational knowledge stored implicitly and is capable of generalizing to similar objects. To test the feasibility of using LMs as commonsense knowledge source in the TWC environment, we design the following experiment 9 : We use a static agent that picks up any misplaced object o at random and puts it to one of the possible locations l in the scene according to a specific prior p(l|o). This prior p(l|o) is computed at the start of an episode for all objectlocation combinations in the scene, using an LM. We use the arrow operator as described in Table 2 </p>
<p>Word Analogy Evaluation</p>
<p>To evaluate the usefulness of querying pre-trained language models by examples for linguistic knowledge, we move to the word analogy task-a standard benchmark for non-contextual word embeddings. This evaluation is based on the premise that a good global word embedding defines a latent space in which basic arithmetic operations correspond to linguistic relations (Mikolov et al., 2013b). With the rise of contextual word embeddings and large pre-trained language models, this evaluation has lost significance. However, we consider approaching this task from the angle of querying linguistic knowledge from an LM instead of performing arithmetics in latent space. By providing examples of the linguistic relation with a regular pattern in the context of the LM, we prime it to apply the relation to the final word with its masked out correspondence. </p>
<p>Discussion</p>
<p>Augmenting the context of LMs with demonstrations is a very successful strategy to disambiguate the query. Notably, it is as successful, on TRE-x, as using an ensemble of multiple paraphrases. We showed that the usefulness of providing additional demonstrations quickly vanishes. Hence, when having access to more labeled data and the option to re-train the model, a fine-tuning strategy is still better suited to maximize the performance on a given task. Moreover, casting NLP problems as language modeling tasks only works as long as the target is a single-token word of the LM's vocabulary. While technically large generation-based LMs as GPT (Brown et al., 2020;Radford et al., 2018) or T5 (Raffel et al., 2019) can generate longer sequences, it is not clear how to compare solutions of varying length.</p>
<p>Conclusion</p>
<p>In this work, we explored the effect of providing examples to probing LMs relational knowledge. We showed that already a few demonstrationssupplied in the context of the LM-disambiguate the query to the same extent as using an optimized ensemble of multiple paraphrases. We base our findings on experimental results of the LAMA probe, the BATS word analogy test, and a TBG commonsense evaluation. On the T-REx corpus' factual relations, providing 10 demonstrations improves BERT's P@1 performance by 37.8%. Similarly, on ConceptNet's commonsense relations, Albert's performance improves by 25% with access to 10 examples. We conclude that providing demonstrations is a simple yet effective strategy to clarify ambiguous prompts to a language model.</p>
<p>A Appendices</p>
<p>A.1 Implementation Details</p>
<p>The source code to reproduce all the experiments is available at https://github.com/leox1v/ lmkb_public. All individual runs reported in the paper can be carried out on a single GPU (TESLA P100 16GB), though speedups can be realized when using multiple GPUs in parallel. The wall-clock runtime for the corpora of the LAMA probe is shown in Table 5. All models used in this work are accessed from the Huggingface's list of pre-trained models for PyTorch (Wolf et al., 2019). Further details about these models are provided on the following webpage: https: //huggingface.co/transformers/pretrained_models.html.  of Brown et al. (2020) shows that large LMs can complete patterns even when not provided in natural language. In particular, they use the "=&gt;"-operator to express the relation between input and output.</p>
<p>Corpus</p>
<p>In Figure 6, we compare the natural language cloze-style template against three different non-language templates:
(i) [s] =&gt; [o], (ii) [s] -&gt; [o], (iii) ([s]; [o])
. Surprisingly, Brown et al. (2020)'s "=&gt;"-operator performs the worst for BERT-large on T-TREx, while separating the subject and objects by a semicolon works best-almost on par with the performance of the natural language template after providing just a single example. This result underlines BERT's remarkable pattern-matching capabilities and suggests that a natural language description of the relation is not always needed-even when querying relatively small LMs.</p>
<p>A.3 Details TextWorld Commonsense Evaluation</p>
<p>Text-based games (TBG) are computer games where the sole modality of interaction is text. Classic games like Zork (Infocom, 1980)  Here, the obvious downside of static KBs for commonsense knowledge extraction becomes apparent: it does not generalize to not listed object-location pairs. Hence, slight deviations of typical entities require additional processing to be able to query the KB. A large pre-trained LM seems to be better suited for this task due to its querying flexibility and generalization capabilities. We test these abilities by designing a static agent as described in the following Algorithm 1, that has access to a large pre-trained LM.     </p>
<p>A.4 Omitted Figures</p>
<p>Google-RE T-REx</p>
<p>Figure 1 :
1BERT's top-3 predictions with probabilites when prompted with the cloze-style question (top) versus when prompted with one additional example of the same relation (bottom).</p>
<p>Figure 2 depicts the mean precision at 1 on the T-REx corpus for a varying number of examples provided. It shows that even a few additional examples can significantly improve the performance of the LMs. However, there is a saturation of usefulness for more examples that seems to be reached at around 10 examples already. Interestingly, with 10 examples, BERT-large even slightly improves upon the optimized paraphrase baseline from Jiang et al. (2020), while only requiring a single forward pass.</p>
<p>3 :
3Mean precision at one (P@1) in percent across the different corpora of the LAMA probe. The baseline models shown are BERT-base (Bb), BERT-large (Bl), Albert-xxlarge-v2 (Al), and the best versions of BERT-large and BERT-base by Jiang et al. (2020) that are optimized across multiple paraphrases 8 (Bb opt and Bl opt ). The LM section on the right shows the results for different querying by example approaches. Here, the superscript denotes the number of examples used and the subscript ce denotes that only close examples have been used. Since the choice of examples alters the predictions of the model and thus introduces randomness, we provide the standard deviation measured over 10 evaluations.</p>
<p>Figure 2 :
2P@1 score for TREx over the number of examples provided. The dashed line shows the baseline value for when no additional example is given.</p>
<p>Figure 3 :
3BERT-large's [CLS]-token embedding of a subset of T-REx queries visualized in two dimensions using t-SNE (van der Maaten and Hinton, 2008). Each point is a single query and the color represents the corresponding relation class. The ellipses depict the 2-std confidence intervals. The individual images show the clustering for both the natural language and the ([s]; [o]) template with either no examples or ten examples provided.</p>
<p>and vary the number of examples provided. In Figure 4, we show the result for albert-xxlarge on the hard games of TWC, compared to a simple uniform prior (i.e., p(l i |o) = const. ∀i), and Murugesan et al. (2020)'s RL agent with access to a commonsense KB. We see the same trend as in the LAMA experiments: providing additional examples of the same relation boosts performance significantly and saturates after 10-15 instances. 9 Details and the pseudocode are provided in Apendix A.</p>
<p>Figure 4 :
4Normalized score for the hard games of the TWC environment over the number of examples provided for albert-xxlarge. The dashed baselines are the static agent with a uniform prior and the TWC commonsense agent by Murugesan et al. (2020). The shaded regions depict the standard deviation over 10 runs.</p>
<p>Figure 5 :
5P@1 score on BATS over the number of examples provided. The performance of the GloVe and SVD benchmark models byGladkova et al. (2016)  is shown with the black, dashed lines.We consider the Bigger Analogy Test Set (BATS)(Gladkova et al., 2016)  for our experiments. BATS consists of 40 different relations covering inflectional and derivational morphology, as well as lexicographic and encyclopedic semantics. Each relation consists of 50 unique word pairs. However, since most pre-trained LMs, including BERT and Albert, use subword-level tokens for their vocabulary, not all examples can be solved. In particular, 76.1% and 76.2% of the targets are contained in BERT's and Albert's vocabulary, respectivelyupper bounding their P@1 performance.Figure 5 depicts the P@1 score 10 for the individual LMs on BATS. Noticeably, also on this task, the LMs benefit from additional examples up to a certain threshold for which the usefulness stagnates. Both BERT models do not beat Gladkova et al. (2016)'s GloVe (Pennington et al., 2014) benchmark. This is in part because not all targets are present in the token vocabulary. Considering only the solvable word pairs, BERT-large achieves a P@1 score of 30.6% with 15 examples-beating the GloVe baseline achieving 28.5%. Interestingly, Albert-xxlarge outperforms all other models, including the baselines, by a large margin.Figure 7in Appendix A.4 breaks down the LM's performance across the different relations of BATS and compares it against the GloVe baseline. Albert beats GloVe on almost all relations where its vocabulary does not limit it; the most significant improvements are in the derivational morphology and lexicographic semantics categories. It is outperformed by GloVe only on two relations: coun-try:capital and UK city:county. Especially the former country:capital category is very prominent and constituted 56.7% of all semantic questions of the original Google test set(Mikolov et al., 2013a)potentially influencing the design and tuning of non-contextual word embeddings.</p>
<p>6 :
6P@1 score for the different corpora of the LAMA probe over the number of examples provided. The dashed line shows the baseline values for when no additional example is given. The upper row depicts the scores for when the examples are chosen randomly among the same relation, while the lower row only considers examples from close subjects as defined in Section 4.</p>
<p>Table 2 :
2Example queries with predictions (from BERT-
large) for the different querying methods. The correct 
answer is marked in bold. </p>
<p>Table 3
3shows the P@1 scores of different models and querying approaches across the LAMA probe's corpora. While for the Google-RE data, providing additional examples shows to be detrimental, we see massive prediction performance gains for T-REx and ConceptNet. Most notably, the P@1 score of BERT-large on T-REx increases by 37.8% to 44.8% when providing 10 close examples. Similarly, the lower bound on Albert's performance for T-REx (ConceptNet) can be improved by up to 72.3% (25.0%) with 10 close examples.</p>
<p>. A closer look at the examples reveals the cause: the target object labels for the religions are provided as nouns (e.g., Christianity, Islam), while the template ([s] is affiliated with the [o] religion) indicates to use the religion as an adjective (e.g., Christian, Islamic). Hence, both paraphrasing the sentence such that it is clear to use a noun or providing example sentences that complete the template with nouns alleviate this problem. The relations that benefit the least from demonstrations are unambiguous, like capital-of or developed-by.ConceptNet While T-REx probes for factual knowledge, the ConceptNet corpus is concerned with commonsense relations. The improvements of querying by example are significant withCorpus </p>
<p>Relation 
Baselines 
LM 
Bb 
Bl 
Al 
Bb opt Bl opt 
Bb 3 
Bb 10 
Bb 10 </p>
<p>ce </p>
<p>Bl 3 
Bl 10 
Bl 10 </p>
<p>ce </p>
<p>Al 10 </p>
<p>ce </p>
<p>Google-RE </p>
<p>birth-place 
14.9 16.1 
6.3 
-
-
10.5 ±0.4 13.2 ±0.3 11.7 ±0.3 
8.9 ±0.5 
11.5 ±0.3 11.0 ±0.3 
7.0 ±0.3 
birth-date 
1.6 
1.5 
1.5 
-
-
1.1 ±0.3 
1.1 ±0.2 
1.2 ±0.1 
1.4 ±0.3 
1.4 ±0.2 
1.5 ±0.1 
1.4 ±0.3 
death-place 13.1 14.0 
2.0 
-
-
9.2 ±0.5 
11.8 ±0.7 10.4 ±1.0 
7.2 ±0.7 
9.1 ±0.5 
8.5 ±1.1 
5.0 ±0.6 </p>
<p>Total 
9.9 
10.5 
3.3 
10.4 11.3 
6.9 ±0.1 
8.7 ±0.2 
7.8 ±0.4 
5.8 ±0.4 
7.4 ±0.1 
7.0 ±0.4 
4.5 ±0.3 </p>
<p>T-REx </p>
<p>1-1 
68.0 74.5 71.2 
-
-
59.7 ±0.6 62.0 ±0.6 62.6 ±0.8 66.4 ±0.9 67.6 ±0.6 68.7 ±0.7 69.0 ±0.7 
N -1 
32.4 34.2 24.9 
-
-
32.3 ±0.1 37.9 ±0.2 41.7 ±0.4 38.8 ±0.2 44.8 ±0.2 47.9 ±0.2 45.0 ±0.2 
N -M 
24.7 24.8 17.2 
-
-
27.9 ±0.4 31.3 ±0.2 34.8 ±0.1 31.4 ±0.4 35.0 ±0.1 37.2 ±0.3 33.5 ±0.2 </p>
<p>Total 
31.1 32.5 24.2 39.6 43.9 31.9 ±0.2 36.5 ±0.2 40.0 ±0.2 37.3 ±0.2 42.1 ±0.2 44.8 ±0.1 41.7 ±0.1 </p>
<p>ConceptNet Total 
15.9 19.5 21.2 
-
-
15.2 ±0.2 16.2 ±0.2 17.1 ±0.2 19.6 ±0.3 21.2 ±0.2 22.0 ±0.3 26.5 ±0.2 </p>
<p>Table</p>
<p>Table 4 :
4List of relations of T-REx that benefit the most(least) by additional examples. The right column pro-
vides the improvement in precision at 1 score when {1, 
3, 5} examples are provided for BERT-large. </p>
<p>Table 5 :
5The runtime in seconds to go once through the full data from the LAMA probe on a single TESLA P100 GPU with a batch size of 32. The superscript of the model represents the number of examples used for querying and the subscript of ce indicates that close examples are used.When providing examples, we give the model the chance to understand the relationship for which we query without providing additional instructions. This naturally raises the question of whether or not natural language templates are even necessary to query LMs. Most prominently, the in-context learningFigure 6: P@1 score for BERT-large on TREx over the number of examples provided. Each line corresponds to one template determining how the examples are provided: (i) with the natural language templates from the LAMA probe (NL Template), (ii) separated by a semicolon (([s]; [o])), (iii) separated by a one-lined arrow ([s] -&gt; [o]), or (iv) separated by a double-lined arrow ([s] =&gt; [o]). The dashed line shows the baseline value for when no additional example is given.A.2 The Choice of Template </p>
<p>used to be played by a large fan base worldwide. Today, they provide interesting challenges for the research field of interactive NLP. With the TextWorld framework byCôté  et al. (2018), it is possible to design custom TBGs; allowing to adapt the objects, locations, and goals around the investigated research objectives. TBGs of this framework can vary from treasure hunting(Côté et al., 2018)  to cooking recipes (Adhikari et al., 2021; Adolphs and Hofmann, 2019), or-as in the experiment at hand-tidying up a room (Murugesan et al., 2020). Murugesan et al. (2020) designed the TextWorld Commonsense environment TWC around the task of cleaning up a modern house environment to probe an agent about its commonsense abilities. For example, a successful agent should understand that dirty dishes belong in the dishwasher while clean dishes in the cupboard. Murugesan et al. (2020) approach this problem by developing an agent that, through a graph-based network, has access to relevant facts from the ConceptNet (Speer et al., 2018) commonsense knowledge base.</p>
<p>Table</p>
<p>Table 7 :
7Mean reciprocal rank (MRR) score for the different corpora of the LAMA probe over the number of examples provided. The dashed line shows the baseline values for when no additional example is given. The upper row depicts the scores for when the examples are chosen randomly among the same relation, while the lower row only considers examples from close subjects as defined in Section 4.Google-RE 
T-REx 
ConceptNet </p>
<p>Random </p>
<p>0 
5 
10 
15 
20 </p>
<h1>Examples</h1>
<p>0.00 </p>
<p>0.01 </p>
<p>0.02 </p>
<p>0.03 </p>
<p>0.04 </p>
<p>Avg. Probability </p>
<p>Google_RE </p>
<p>bert-large 
bert-base 
albert-xxlarge </p>
<p>0 
5 
10 
15 
20 </p>
<h1>Examples</h1>
<p>0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>Avg. Probability </p>
<p>TREx </p>
<p>bert-large 
bert-base 
albert-xxlarge 
0 
5 
10 
15 
20 </p>
<h1>Examples</h1>
<p>0.00 </p>
<p>0.05 </p>
<p>0.10 </p>
<p>0.15 </p>
<p>Avg. Probability </p>
<p>ConceptNet </p>
<p>bert-large 
bert-base 
albert-xxlarge </p>
<p>Close </p>
<p>0 
5 
10 
15 
20 </p>
<h1>Examples</h1>
<p>0.00 </p>
<p>0.01 </p>
<p>0.02 </p>
<p>0.03 </p>
<p>0.04 </p>
<p>Avg. Probability </p>
<p>Google_RE </p>
<p>bert-large 
bert-base 
albert-xxlarge </p>
<p>0 
5 
10 
15 
20 </p>
<h1>Examples</h1>
<p>0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>Avg. Probability </p>
<p>TREx </p>
<p>bert-large 
bert-base 
albert-xxlarge 
0 
5 
10 
15 
20 </p>
<h1>Examples</h1>
<p>0.00 </p>
<p>0.05 </p>
<p>0.10 </p>
<p>0.15 </p>
<p>Avg. Probability </p>
<p>ConceptNet </p>
<p>bert-large 
bert-base 
albert-xxlarge </p>
<p>Table 8 :
8Probability assigned to the ground-truth object for the different corpora of the LAMA probe over the number of examples provided. The dashed line shows the baseline values for when no additional example is given. The upper row depicts the scores for when the examples are chosen randomly among the same relation, while the lower row only considers examples from close subjects as defined in Section 4.
With the notable exception of the work of Roberts et al. (2020), which uses a T-5 model without any access to an additional knowledge base.
https://github.com/google-research-datasets/ relation-extraction-corpus 4 According to the statistics from https: //huggingface.co/models?filter=pytorch, masked-lm.
The P@1 score corresponds to Jiang et al. (2020)'s microaveraged accuracy
These models involve one query to the model per paraphrase.
The P@1 score corresponds to Gladkova et al. (2016)'s reported accuracy score.</p>
<p>Figure 7: P@1 score on BATS for Albert-xxlarge. with 10 examples that use the "([sFigure 7: P@1 score on BATS for Albert-xxlarge with 10 examples that use the "([s];</p>
<p>The frame around the bar indicates the maximum possible score that the Albert model could have scored because not all targets are tokens in its vocabulary. -template. The x-axis breaks down the performance for the individual relations of the BATS dataset. As a benchmark, we use the GloVe model from Gladkova[o])"-template. The x-axis breaks down the performance for the individual relations of the BATS dataset. As a benchmark, we use the GloVe model from Gladkova et al. (2016). The frame around the bar indicates the maximum possible score that the Albert model could have scored because not all targets are tokens in its vocabulary.</p>            </div>
        </div>

    </div>
</body>
</html>
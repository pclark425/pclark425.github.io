<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5066 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5066</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5066</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-1fa084781277c90cfa0f7665c5528bc9f882be06</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1fa084781277c90cfa0f7665c5528bc9f882be06" target="_blank">Do Language Models Know the Way to Rome?</a></p>
                <p><strong>Paper Venue:</strong> BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</p>
                <p><strong>Paper TL;DR:</strong> This paper evaluates the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome?</p>
                <p><strong>Paper Abstract:</strong> The global geometry of language models is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in geography, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger models performing the best, suggesting that geographic knowledge can be induced from higher-order co-occurrence statistics.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5066.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5066.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional transformer-based contextual language model (BERT) used in base and large sizes to probe whether distributional representations encode geographic (spatial) information about cities and countries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (base and large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based bidirectional masked language model. Base: 12 layers, 768-dimensional output; Large: 24 layers, 1024-dimensional output. Pretrained on BookCorpus and English Wikipedia as in the original paper; the pretrained checkpoints from the original BERT paper were used without further fine-tuning for the core experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Not a puzzle game but spatial reasoning tasks: (1) predict latitude/longitude (2D) of cities/countries from name embeddings (non-local, continuous spatial inference); (2) predict whether two countries share a land border (local relational spatial reasoning); (3) measure similarity between city representations to test whether intra-country cities are closer in representation space than inter-country cities.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Extract contextualized representations by placing location names in simple contexts ('He lives in X', 'She moved to X', 'I come from X'), average last 4 layers' hidden states and mean-pool subword tokens; train simple probes: Lasso linear regressor (with L2 penalty) and single-hidden-layer MLP (100 units) to predict GPS coordinates or population, and a single-layer 100-unit MLP classifier for border prediction. Use control tasks (random-permuted targets) and compute probe selectivity and probe error reduction (PER) to baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Probes trained on BERT representations achieve positive probe error reduction (PER>0) for GPS prediction and population prediction, and high accuracy (>85%) on country-border classification with probe selectivity ≈0.34–0.37, indicating models encode some geographic/neighborhood information. Similarity analysis shows higher average cosine similarity for cities in the same country (intra=0.83 base, 0.77 large) than across countries (inter=0.77 base, 0.65 large) with a non-zero gap, supporting relational spatial structure in representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPS prediction (cities, MLP): mean error distances — BERT base 4195 km (probe), control 8057 km; BERT large 3315 km (probe), control 7997 km. PER (GPS, MLP, city): BERT base 0.479, BERT large 0.585 (Table 1 / Appendix A.1). Country-border classification accuracy: BERT 0.856 (probe), control 0.51, selectivity 0.34 (Table 2). Similarity: intra 0.83, inter 0.77, gap 0.06 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Absolute accuracy for GPS prediction is poor (errors of several thousand kilometers), i.e., coarse/continent-level rather than precise; Lasso (linear) probes perform worse than MLPs, suggesting linear isomorphism to physical space is limited; contextualized BERT shows anisotropic/highly similar representations that reduce discriminative power (Ethayarajh-style effects referenced); increasing model size improves PER but does not yield precise geographic localization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Word2Vec, BERT does not uniformly outperform: Word2Vec obtains competitive or better PER/absolute errors on some GPS/population tasks. BERT large outperforms BERT base (larger model yields better PER). GPT-2 performs worse than both BERT and Word2Vec on these spatial tasks. MLP probes outperform Lasso in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Know the Way to Rome?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5066.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5066.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Roberta: A robustly optimized bert pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robustly optimized variant of BERT (RoBERTa) used in base and large sizes to probe for encoded geographic/spatial knowledge via the same GPS, population, and border prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Roberta: A robustly optimized bert pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (base and large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based masked language model (an optimized BERT variant). Base and large sizes analogous to BERT (base: 12 layers, 768 dims; large: 24 layers, 1024 dims). Pretrained on large corpora (BookCorpus and English Wikipedia in the paper's usage); pretrained checkpoints used as-is.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same set of spatial tasks as with BERT: regress 2D GPS coordinates for cities/countries; classify whether two countries share a land border; analyze representational similarity between cities to assess encoding of geographic neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Same probing pipeline as other models: contextual prompts with the location name, average last-4-layer hidden states, mean subword pooling; probes are Lasso regressor and single-hidden-layer MLP for regression, 100-unit MLP classifier for borders; control tasks via random permutation to compute selectivity and PER.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Positive PER for GPS and population tasks; high accuracy (>81%) on country-border classification with selectivity ≈0.31–0.32; similarity analyses show intra-country similarities larger than inter-country similarities (RoBERTa intra 0.92, inter 0.89, small gap), indicating some encoded neighborhood structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPS prediction (cities, MLP): mean error distances — RoBERTa base 4278 km (probe), control 8007 km; RoBERTa large 3876 km (probe), control 8029 km (Appendix A.1). PER (GPS, MLP, city): RoBERTa base 0.466, RoBERTa large 0.517 (Table 1). Country-border classification accuracy: RoBERTa 0.817 (probe), control 0.51, selectivity 0.31; RoBERTa-L accuracy 0.843, control 0.52, selectivity 0.32 (Table 2). Similarity: intra 0.92, inter 0.89, gap 0.03 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Absolute GPS errors are large (thousands of km) — coarse-level geographic information only; similarity gaps are small (representations are highly similar/anistropic), making fine-grained spatial discrimination difficult; larger size does not consistently yield a larger similarity gap (inconsistent size effect for representational gap).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>RoBERTa performs better when larger (RoBERTa-L > RoBERTa base in PER and border accuracy), but static Word2Vec remains a strong baseline on some tasks. GPT-2 underperforms relative to RoBERTa variants. MLP probes generally yield better error reduction than Lasso.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Know the Way to Rome?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5066.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5066.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models are unsupervised multitask learners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive (unidirectional) transformer language model (GPT-2) used to probe whether next-token trained representations encode geographic/spatial information about places.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are unsupervised multitask learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unidirectional (next-token) transformer language model pretrained on large corpora (BookCorpus among training data mentioned). The paper uses a standard pretrained GPT-2 checkpoint; architecture details follow the original GPT-2 (multiple transformer layers; exact parameter count not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same spatial tasks as for other models: regress GPS coordinates of cities/countries from name embeddings; classify shared country borders; analyze intra/inter-city representational similarity to test for encoded geographic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Same pipeline for contextualized embeddings: put location names in simple contexts, average last 4 layers' hidden states, mean-pool subwords; train Lasso and MLP probes for regression and a 100-unit MLP classifier for border detection; random-permutation control tasks and compute PER/selectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>GPT-2 achieves positive PER in some tasks but consistently has the lowest PER and the worst absolute errors across most tasks. Nevertheless, border classification accuracy remains above chance (~0.808) indicating some neighborhood information is present but weaker than for BERT/RoBERTa/Word2Vec.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPS prediction (cities, MLP): mean error distances — GPT-2 4613 km (probe), control 8011 km (Appendix A.1). PER (GPS, MLP, city): 0.424 (Table 1). Country-border classification accuracy: 0.808 (probe), control 0.51, selectivity 0.30 (Table 2). Similarity: intra 0.90, inter 0.89, gap 0.01 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Worst-performing contextualized LM in these spatial tasks: largest absolute errors for GPS regression and lowest PERs. Representational similarity gap is near zero (0.01), indicating almost no separable intra/inter-country structure. Autoregressive training objective (unidirectional) may be less conducive to encoding the tested geographic relations compared to masked bidirectional objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs worse than BERT and RoBERTa variants and often worse than Word2Vec on GPS and population prediction; border classification accuracy still above 80% but lower selectivity than BERT large. MLP probes improve results compared to Lasso but GPT-2 remains weakest among tested LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Know the Way to Rome?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5066.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5066.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word2Vec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient estimation of word representations in vector space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A static 300-dimensional word embedding (Word2Vec) trained on Google News used as a baseline to compare whether static distributional vectors encode geographic/spatial relations comparably to contextualized LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient estimation of word representations in vector space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Word2Vec (Google News pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Static word embeddings (skip-gram/CBOW family) pretrained on the Google News corpus (150B words); vectors of dimension 300 were used (gensim-provided pretrained vectors). Representations are non-contextual and directly used for probes.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same geographic prediction tasks: regress GPS coordinates, predict country borders, and measure representational similarity among cities to infer spatial/neighborhood structure.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Use static 300-dim Word2Vec vectors for location names; train Lasso regressor and single-hidden-layer MLP probes for regression tasks and a 100-unit MLP classifier for border prediction; include random-permutation control tasks and report PER/selectivity and absolute errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Word2Vec achieves substantial PER and in some cases better absolute GPS errors than contextualized LMs (e.g., cities MLP mean error 2612 km, PER 0.666). Border classification accuracy 0.849 with selectivity 0.36 indicates clear encoded neighborhood relations; similarity analysis shows a larger intra/inter gap (intra 0.51, inter 0.24, gap 0.27) relative to contextualized LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPS prediction (cities, MLP): mean error distance 2612 km (probe) vs control 7825 km; PER 0.666 (Table 1 and Appendix A.1). Country GPS (MLP) mean error 3738 km (Appendix A.1). Border classification accuracy 0.849 (probe), control 0.49, selectivity 0.36 (Table 2). Similarity: intra 0.51, inter 0.24, gap 0.27 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although competitive or superior on some metrics, Word2Vec is a static representation and lacks contextual sensitivity; absolute GPS prediction errors remain large (thousands km). The static vectors may encode frequent co-occurrence/geographic collocations (e.g., city-country pairs) but cannot condition on sentence context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Acts as a strong baseline: in several GPS/population tasks Word2Vec matches or outperforms contextualized LMs. Compared to BERT/RoBERTa, Word2Vec sometimes yields lower absolute errors and higher PER for GPS/country tasks; however, contextualized large models improve over base variants, indicating capacity and pretraining data matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Know the Way to Rome?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do neural language representations learn physical commonsense? <em>(Rating: 2)</em></li>
                <li>Commonsense knowledge mining from pretrained models <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5066",
    "paper_id": "paper-1fa084781277c90cfa0f7665c5528bc9f882be06",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "BERT",
            "name_full": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "brief_description": "A bidirectional transformer-based contextual language model (BERT) used in base and large sizes to probe whether distributional representations encode geographic (spatial) information about cities and countries.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT (base and large)",
            "model_description": "Transformer-based bidirectional masked language model. Base: 12 layers, 768-dimensional output; Large: 24 layers, 1024-dimensional output. Pretrained on BookCorpus and English Wikipedia as in the original paper; the pretrained checkpoints from the original BERT paper were used without further fine-tuning for the core experiments.",
            "puzzle_name": "Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)",
            "puzzle_description": "Not a puzzle game but spatial reasoning tasks: (1) predict latitude/longitude (2D) of cities/countries from name embeddings (non-local, continuous spatial inference); (2) predict whether two countries share a land border (local relational spatial reasoning); (3) measure similarity between city representations to test whether intra-country cities are closer in representation space than inter-country cities.",
            "mechanism_or_strategy": "Extract contextualized representations by placing location names in simple contexts ('He lives in X', 'She moved to X', 'I come from X'), average last 4 layers' hidden states and mean-pool subword tokens; train simple probes: Lasso linear regressor (with L2 penalty) and single-hidden-layer MLP (100 units) to predict GPS coordinates or population, and a single-layer 100-unit MLP classifier for border prediction. Use control tasks (random-permuted targets) and compute probe selectivity and probe error reduction (PER) to baseline performance.",
            "evidence_of_spatial_reasoning": "Probes trained on BERT representations achieve positive probe error reduction (PER&gt;0) for GPS prediction and population prediction, and high accuracy (&gt;85%) on country-border classification with probe selectivity ≈0.34–0.37, indicating models encode some geographic/neighborhood information. Similarity analysis shows higher average cosine similarity for cities in the same country (intra=0.83 base, 0.77 large) than across countries (inter=0.77 base, 0.65 large) with a non-zero gap, supporting relational spatial structure in representations.",
            "performance_metrics": "GPS prediction (cities, MLP): mean error distances — BERT base 4195 km (probe), control 8057 km; BERT large 3315 km (probe), control 7997 km. PER (GPS, MLP, city): BERT base 0.479, BERT large 0.585 (Table 1 / Appendix A.1). Country-border classification accuracy: BERT 0.856 (probe), control 0.51, selectivity 0.34 (Table 2). Similarity: intra 0.83, inter 0.77, gap 0.06 (Table 3).",
            "limitations_or_failure_cases": "Absolute accuracy for GPS prediction is poor (errors of several thousand kilometers), i.e., coarse/continent-level rather than precise; Lasso (linear) probes perform worse than MLPs, suggesting linear isomorphism to physical space is limited; contextualized BERT shows anisotropic/highly similar representations that reduce discriminative power (Ethayarajh-style effects referenced); increasing model size improves PER but does not yield precise geographic localization.",
            "comparison_baseline": "Compared to Word2Vec, BERT does not uniformly outperform: Word2Vec obtains competitive or better PER/absolute errors on some GPS/population tasks. BERT large outperforms BERT base (larger model yields better PER). GPT-2 performs worse than both BERT and Word2Vec on these spatial tasks. MLP probes outperform Lasso in many settings.",
            "uuid": "e5066.0",
            "source_info": {
                "paper_title": "Do Language Models Know the Way to Rome?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "RoBERTa",
            "name_full": "Roberta: A robustly optimized bert pretraining approach",
            "brief_description": "A robustly optimized variant of BERT (RoBERTa) used in base and large sizes to probe for encoded geographic/spatial knowledge via the same GPS, population, and border prediction tasks.",
            "citation_title": "Roberta: A robustly optimized bert pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa (base and large)",
            "model_description": "Transformer-based masked language model (an optimized BERT variant). Base and large sizes analogous to BERT (base: 12 layers, 768 dims; large: 24 layers, 1024 dims). Pretrained on large corpora (BookCorpus and English Wikipedia in the paper's usage); pretrained checkpoints used as-is.",
            "puzzle_name": "Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)",
            "puzzle_description": "Same set of spatial tasks as with BERT: regress 2D GPS coordinates for cities/countries; classify whether two countries share a land border; analyze representational similarity between cities to assess encoding of geographic neighborhoods.",
            "mechanism_or_strategy": "Same probing pipeline as other models: contextual prompts with the location name, average last-4-layer hidden states, mean subword pooling; probes are Lasso regressor and single-hidden-layer MLP for regression, 100-unit MLP classifier for borders; control tasks via random permutation to compute selectivity and PER.",
            "evidence_of_spatial_reasoning": "Positive PER for GPS and population tasks; high accuracy (&gt;81%) on country-border classification with selectivity ≈0.31–0.32; similarity analyses show intra-country similarities larger than inter-country similarities (RoBERTa intra 0.92, inter 0.89, small gap), indicating some encoded neighborhood structure.",
            "performance_metrics": "GPS prediction (cities, MLP): mean error distances — RoBERTa base 4278 km (probe), control 8007 km; RoBERTa large 3876 km (probe), control 8029 km (Appendix A.1). PER (GPS, MLP, city): RoBERTa base 0.466, RoBERTa large 0.517 (Table 1). Country-border classification accuracy: RoBERTa 0.817 (probe), control 0.51, selectivity 0.31; RoBERTa-L accuracy 0.843, control 0.52, selectivity 0.32 (Table 2). Similarity: intra 0.92, inter 0.89, gap 0.03 (Table 3).",
            "limitations_or_failure_cases": "Absolute GPS errors are large (thousands of km) — coarse-level geographic information only; similarity gaps are small (representations are highly similar/anistropic), making fine-grained spatial discrimination difficult; larger size does not consistently yield a larger similarity gap (inconsistent size effect for representational gap).",
            "comparison_baseline": "RoBERTa performs better when larger (RoBERTa-L &gt; RoBERTa base in PER and border accuracy), but static Word2Vec remains a strong baseline on some tasks. GPT-2 underperforms relative to RoBERTa variants. MLP probes generally yield better error reduction than Lasso.",
            "uuid": "e5066.1",
            "source_info": {
                "paper_title": "Do Language Models Know the Way to Rome?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "GPT-2",
            "name_full": "Language models are unsupervised multitask learners",
            "brief_description": "An autoregressive (unidirectional) transformer language model (GPT-2) used to probe whether next-token trained representations encode geographic/spatial information about places.",
            "citation_title": "Language models are unsupervised multitask learners",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_description": "Unidirectional (next-token) transformer language model pretrained on large corpora (BookCorpus among training data mentioned). The paper uses a standard pretrained GPT-2 checkpoint; architecture details follow the original GPT-2 (multiple transformer layers; exact parameter count not specified in this paper).",
            "puzzle_name": "Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)",
            "puzzle_description": "Same spatial tasks as for other models: regress GPS coordinates of cities/countries from name embeddings; classify shared country borders; analyze intra/inter-city representational similarity to test for encoded geographic structure.",
            "mechanism_or_strategy": "Same pipeline for contextualized embeddings: put location names in simple contexts, average last 4 layers' hidden states, mean-pool subwords; train Lasso and MLP probes for regression and a 100-unit MLP classifier for border detection; random-permutation control tasks and compute PER/selectivity.",
            "evidence_of_spatial_reasoning": "GPT-2 achieves positive PER in some tasks but consistently has the lowest PER and the worst absolute errors across most tasks. Nevertheless, border classification accuracy remains above chance (~0.808) indicating some neighborhood information is present but weaker than for BERT/RoBERTa/Word2Vec.",
            "performance_metrics": "GPS prediction (cities, MLP): mean error distances — GPT-2 4613 km (probe), control 8011 km (Appendix A.1). PER (GPS, MLP, city): 0.424 (Table 1). Country-border classification accuracy: 0.808 (probe), control 0.51, selectivity 0.30 (Table 2). Similarity: intra 0.90, inter 0.89, gap 0.01 (Table 3).",
            "limitations_or_failure_cases": "Worst-performing contextualized LM in these spatial tasks: largest absolute errors for GPS regression and lowest PERs. Representational similarity gap is near zero (0.01), indicating almost no separable intra/inter-country structure. Autoregressive training objective (unidirectional) may be less conducive to encoding the tested geographic relations compared to masked bidirectional objectives.",
            "comparison_baseline": "Performs worse than BERT and RoBERTa variants and often worse than Word2Vec on GPS and population prediction; border classification accuracy still above 80% but lower selectivity than BERT large. MLP probes improve results compared to Lasso but GPT-2 remains weakest among tested LMs.",
            "uuid": "e5066.2",
            "source_info": {
                "paper_title": "Do Language Models Know the Way to Rome?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Word2Vec",
            "name_full": "Efficient estimation of word representations in vector space",
            "brief_description": "A static 300-dimensional word embedding (Word2Vec) trained on Google News used as a baseline to compare whether static distributional vectors encode geographic/spatial relations comparably to contextualized LMs.",
            "citation_title": "Efficient estimation of word representations in vector space",
            "mention_or_use": "use",
            "model_name": "Word2Vec (Google News pretrained)",
            "model_description": "Static word embeddings (skip-gram/CBOW family) pretrained on the Google News corpus (150B words); vectors of dimension 300 were used (gensim-provided pretrained vectors). Representations are non-contextual and directly used for probes.",
            "puzzle_name": "Geographic spatial prediction tasks (GPS coordinate prediction; country-border prediction; city similarity analysis)",
            "puzzle_description": "Same geographic prediction tasks: regress GPS coordinates, predict country borders, and measure representational similarity among cities to infer spatial/neighborhood structure.",
            "mechanism_or_strategy": "Use static 300-dim Word2Vec vectors for location names; train Lasso regressor and single-hidden-layer MLP probes for regression tasks and a 100-unit MLP classifier for border prediction; include random-permutation control tasks and report PER/selectivity and absolute errors.",
            "evidence_of_spatial_reasoning": "Word2Vec achieves substantial PER and in some cases better absolute GPS errors than contextualized LMs (e.g., cities MLP mean error 2612 km, PER 0.666). Border classification accuracy 0.849 with selectivity 0.36 indicates clear encoded neighborhood relations; similarity analysis shows a larger intra/inter gap (intra 0.51, inter 0.24, gap 0.27) relative to contextualized LMs.",
            "performance_metrics": "GPS prediction (cities, MLP): mean error distance 2612 km (probe) vs control 7825 km; PER 0.666 (Table 1 and Appendix A.1). Country GPS (MLP) mean error 3738 km (Appendix A.1). Border classification accuracy 0.849 (probe), control 0.49, selectivity 0.36 (Table 2). Similarity: intra 0.51, inter 0.24, gap 0.27 (Table 3).",
            "limitations_or_failure_cases": "Although competitive or superior on some metrics, Word2Vec is a static representation and lacks contextual sensitivity; absolute GPS prediction errors remain large (thousands km). The static vectors may encode frequent co-occurrence/geographic collocations (e.g., city-country pairs) but cannot condition on sentence context.",
            "comparison_baseline": "Acts as a strong baseline: in several GPS/population tasks Word2Vec matches or outperforms contextualized LMs. Compared to BERT/RoBERTa, Word2Vec sometimes yields lower absolute errors and higher PER for GPS/country tasks; however, contextualized large models improve over base variants, indicating capacity and pretraining data matter.",
            "uuid": "e5066.3",
            "source_info": {
                "paper_title": "Do Language Models Know the Way to Rome?",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do neural language representations learn physical commonsense?",
            "rating": 2
        },
        {
            "paper_title": "Commonsense knowledge mining from pretrained models",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
            "rating": 1
        }
    ],
    "cost": 0.011973499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do Language Models Know the Way to Rome?</h1>
<p>Bastien Liétard<br>University of Lille<br>bastien.lietard.etu<br>@univ-lille.fr</p>
<h2>Mostafa Abdou and Anders Søgaard University of Copenhagen<br>{madbou, soegaard}@di.ku.dk</h2>
<h4>Abstract</h4>
<p>The global geometry of language models is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in geography, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger models performing the best, suggesting that geographic knowledge can be induced from higher-order cooccurrence statistics.</p>
<h2>1 Introduction</h2>
<p>Language models (LMs) are fundamental building blocks in state-of-the-art natural language processing. These models are trained to predict words in context, and as a side product, they learn to compress higher-order co-occurrence statistics to represent the distributional properties of words and phrases.</p>
<p>It is well-documented that the representations of modern language models encode some syntactic (Tenney et al., 2019) and semantic knowledge ((Reif et al., 2019)), as well as some real-world knowledge (Davison et al., 2019; Petroni et al., 2019; Jiang et al., 2020; Roberts et al., 2020), i.e., some knowledge base relations can be extracted directly from language models. Much of this information was available in older language models such as Word2Vec, too (Mikolov et al., 2013).</p>
<p>The probes that have been designed for the above studies, however, only probe for one-dimensional information (e.g., part of speech or semantic class) or very local relations (e.g., is Daniel Kehlmann
a writer?). It is an open question whether higherorder co-occurrence statistics found in textual corpora can induce representations show isomorphism to human mental representations of entities and relations. To test this we operationalize a dataset of geographic information. Geographic knowledge is a domain in which we can go beyond local relations, since we can access global representations of geographic knowledge (to the extent it is isomorphic to the physical world).</p>
<p>Cross-lingual language model alignment provides additional motivation for probing for geographic knowledge: We know that language models of different languages are sometimes nearisomorphic (Søgaard et al., 2018; Vulić et al., 2020), and it is tempting to think that this is because language models reflect real-world structures, and that the (occasional) near-isomorphism of language models is simply the result of the fact that we use language(s) to talk about the same world. Is the isomorphism of language models a result of them being in part isomorphic to the physical world?</p>
<p>Why would language models potentially encode physical geometry? Higher-order co-occurrence statistics can be surprisingly informative. Copenhagen and Malmö are in different countries, but connected by a bridge, often referred to as the bridge between Copenhagen and Malmö. Other cities belong to the same municipalities and their names therefore co-occur with the name of the municipality. Other cities were, for instance, potentially impacted by the same natural disaster or part of the same development project. By distilling thousands, or maybe hundreds of thousands, of such co-occurrences, we conjecture that language models might be able to induce somewhat fine-grained maps of physical geometry.</p>
<p>In this work, we study the extent to which geographical information is encoded by language models. Based on geographic information from multiple sources, we train probing models on language</p>
<p>model representations and evaluate their ability to predict the locations of cities and countries, as well as their ability to predict (one-dimensional) population numbers and (local) neighbor relations, relative to control probe performance levels.</p>
<p>Contributions We collect geographic information from multiple sources and design experimental protocols to probe language models for such information. We evaluate two BERT models (Devlin et al., 2019) of different sizes, two RoBERTa models (Liu et al., 2019) of different sizes, as well as GPT-2 (Radford et al., 2019) and Word2Vec (Mikolov et al., 2013) across three different tasks and various protocols. Our results show that modern language models do not encode much more geographic information than Word2Vec, but that larger (non-autoregressive) language models encode more information than their smaller counterparts, suggesting that such information is nevertheless available through higher-order co-occurrence statistics.</p>
<h2>2 Related Work</h2>
<p>Word representations based on distributional statistics have been theorised to capture a wide range of information (Schütze, 1992). To evaluate this, a considerable body of literature has made use of semantic similarity, relatedness, and analogy datasets (Agirre et al., 2009; Bruni et al., 2012; Baroni et al., 2014; Faruqui et al., 2014; Hill et al., 2015; Drozd et al., 2016; Abdou et al., 2018). Asking a broader question, Rubinstein et al. (2015) investigated the types of semantic information which are encoded by different classes of word embedding models, finding that taxonomic properties (such as animacy) are well-modelled. In a similar direction, Collell Talleda and Moens (2016) and Lucy and Gauthier (2017) draw on semantic norm datasets to test how well these models can encode a range of perceptual and conceptual features.</p>
<p>In the context of the neural language models, several recent works such as Davison et al. (2019) and Petroni et al. (2019) have attempted to extract factual and commonsense knowledge from them by posing knowledge base triplets as close statements which are used to query the models. Most related to this work, Forbes et al. (2019) investigate whether LMs can learn physical commonsense through language. They find that LM representations do indeed encode information regarding object properties (e.g., bananas are yellow), and
affordances (e.g. bananas can be eaten) but they do not to capture the more subtle interplay between the two.</p>
<p>Studies investigating the geometry of word representations have focused on intrinsic dimensionality and subspaces (Yaghoobzadeh and Schütze, 2016; Coenen et al., 2019), embedding concentration in narrow cones and anisotropy (Mimno and Thompson, 2017; Ethayarajh, 2019), and comparisons to the geometry of cognitive measurements or perceptual spaces (Abnar et al., 2019; Abdou et al., 2021). In this work we investigate the degree of isomorphism between language model representations of geographic location names and their real-world counterparts. See Vulić et al. (2020) for a general discussion of isomorphism across language models, what explains this, and what it depends on.</p>
<h2>3 Methodology</h2>
<p>We probe language model representations of city and country names. Below, we propose three classification/regression probing tasks, as an well as an analysis based on (relational) similarity.</p>
<h3>3.1 Probing tasks</h3>
<p>Task 1: Predicting geo-coordinates We probe LMs' representation about whether they convey information about the actual position of the location. To this extent, we train a model to predict GPS coordinates (latitude and longitude) given the location's representation through language models. We evaluate prediction error by computing the distance in kilometers between the predicted GPS point and the expected point, and compute the average of error distances. Since we use linear models (Lasso ${ }^{1}$ ) to predict geo-coordinates, we effectively evaluate the isomorphism of language model representations to the physical world. Our target variable is two-dimensional, and we evaluate non-local relations.</p>
<p>Task 2: Predicting population sizes In the physical world, cities are characterized not only by their location, but also by their size and population. There is indeed a major difference between Shanghai ( 27 M people) and Worcester ( 101 K people). Given the representation of a country name or a city name, we train a probe to predict the population living in the location. We evaluate the performance by computing the mean squared error.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This is a one-dimensional attribute included here for comparison.</p>
<p>Task 3: Predicting neighboring countries Borders between countries are also an important geographic relation that is closely related to a country's international policy. It also provides information about the continental situation ; indeed, a country with no land borders must be an island. To probe language models for neighboring relations, we train an classifier to predict whether two countries share a border or not, given the pair of representations. We report the probe accuracy. Neighboring relations are local and included here for comparison.</p>
<h3>3.2 Control tasks and scores</h3>
<p>Hewitt and Liang (2019) have shown the importance of a control task to balance the probe accuracy. We construct our control tasks by randomly permuting the target variables and train the model on this randomly-permuted dataset. We repeat this control task 10 times and take the mean error/accuracy.</p>
<p>Probe classifier selectivity For the country borders classification task, we define the probe selectivity as the difference between the probe accuracy on the original task and the accuracy on the control task (Hewitt and Liang, 2019).</p>
<p>Probe error reduction Similarly, we define the probe error reduction, which measures how well the probe performs on the original task compared to the control task, by comparing positive error measures of both tasks through error reduction, i.e., the proportion of control task error that is reduced in our probe.</p>
<p>$$
P E R=1-\frac{\text { Probing task error }}{\text { Control task error }}
$$</p>
<p>Less formally, if the PER is between 0 and 1, its upper bound, it indicates to what extent the probe is doing better on the original task than on the control task ; a negative PER indicates the probe is doing worse on the original task than on the control task. We use PER for regression tasks rather than absolute error or MSE, because figures are more easily interpretable when properly baselined. Note that if absolute error is 0 , PER is 1 ; if absolute error is as high as the random baseline, PER is 0 . Under the assumption performance is better than random, PER thus ranges between 0 and 1.</p>
<h3>3.3 Similarity Analysis</h3>
<p>For each language models, we compute the cosine similarity between each pair of cities. Our hypothesis is that cities belonging to the same country will be more similar than two cities from different countries. We report both average similarities (cities in the same country and cities in different countries) and also compute the histogram of the distribution of these two sets of similarities.</p>
<h2>4 Experimental Settings and Data</h2>
<p>Language models We compare three different state-of-the-art contextualized LMs, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and GPT-2 (Radford et al., 2019). A major difference between GPT-2 and the two other models is that it is trained to be unidirectional (i.e. using next-token language modelling), whereas BERT and RoBERTa are trained to be bidirectionnal (i.e. using masked language modelling). For BERT and RoBERTa, we used both base ( $L=12$ layers and $D=768$ output dimensions) and large ( $L=24$ layers and $D=1024$ output dimensions) version, so we can study the influence of the model's size. We probe widely-used pre-trained versions of these models, as they have been trained in their respective original papers. Both versions of BERT and RoBERTa have been pre-trained on the BookCorpus (800M words) and English Wikipedia (2.5B words). GPT-2 is also trained on the Book Corpus.</p>
<p>Geographic data We get a list of names of cities around the world having a population of 100 K or more people, with their GPS coordinates (Latitude and Longitude), their country and their population, from a MaxMind database ${ }^{2}$. Country geocoordinates correspond to the centroid of each country. We obtain population size data from the United Nations' Human Development Data Center ${ }^{3}$, given in millions of inhabitants. Our datasets are then composed of 3527 cities and 249 countries and territories.</p>
<p>Representations extraction Since the LMs provide contextualised token representations, we provide three linguistically-basic contexts : He lives in $X$, She moved to $X$ and I come from $X$, replacing $X$ with either a city name or a country name.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For each context, we extract the representation corresponding to the name by averaging the hidden states of the last 4 layers of the LMs. Following Bommasani et al. (2020), we use a mean pooling of subwords' representations when necessary.</p>
<p>Word2Vec To compare static and contextualized models, we also use pre-trained Word2Vec static representations learned over the Google News dataset (150B words), provided by the gensim ${ }^{4}$ library. The representational vectors are in 300 dimensions.</p>
<p>Probe models For the classification problem (task 3), we use a 100-units single-layered MLP as a classifier. As a probe for regression task (1 and 2), we train both a Lasso regressor with a L2-penalty of $\alpha$ and a single hidden layer MLP with 100 units. We arbitrarily choose $\alpha=1$, except for task 1 for which we remarked that $\alpha=0.5$ significantly decreased the training time without affecting to much the error ratio. No other hyperparameters-tuning was done, since we are not interested in designing the best task-specific models. Probes are trained on a random split of $80 \%$ of the dataset, and evaluate on the $20 \%$ remaining.</p>
<h2>5 Results</h2>
<p>In Table 1, probe error reduction scores are displayed for both regression tasks, namely Task 1 and 2. The full list of performances (error distances in kilometers) for Task 1 is provided in the appendix A.1. In Appendix A.2, error values for population prediction with countries are reported. Since the number of countries is relatively small, we perform a 5 -fold cross-validation with this dataset.</p>
<p>When predicting a location's GPS coordinates, all models have an error reduction score significantly larger than 0 , across both families of regressors and across cities and countries. The Word2Vec model especially has often the lowest error and a high PER, showing no real weakness compared to contextualised models. GPT-2 has the lowest PER and the worst error value, across almost all tasks and probes. The two regressors globally lead to the same error values, even though MLP allows a slightly bigger error reduction than Lasso. We can also note that, for both BERT and RoBERTa, increasing model size (moving from base models</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to large), one increases the gap between the performance on the task and on the random control, reducing overall error. Finally, we can also see that the models are generally not very accurate for predicting GPS coordinates, leading to an average error of 2'500-5'000 kilometers. It is relatively accurate continent-wise, but not precise.</p>
<p>In sum, we can conclude that it is non-trivial to learn linear maps from language models to physical world geo-coordinates, presumably because representations are non-isomorphic. On the other hand, the improvements from larger models seem to suggest that geographical knowledge is available from higher-order co-occurrence statistics. Static model also provide a strong baseline compared to contextualised models.</p>
<p>In Task 2, the gap between the two regressors is more noticeable compared to Task 1: MLP exhibits much higher error reductions between the original task and the random control, and generally allow for lower error numbers. We remark that our Lasso model is particularly ineffective for country population prediction, leading only to low PER and high errors. On this task and dataset, GPT-2 random control error is high, leading to an outlying PER for this LM, even if its probing-task error is still the biggest. The scheme observed in Task 1 between base and large version of BERT and RoBERTa seems to hold, with only an exception for countries with MLP. This overall indicates that a one-dimensional attribute is easier to learn than a two-dimensional space, giving more expressive models an upper hand.</p>
<p>Table 2 shows the probe's accuracy and selectivity on Task 3, predicting country borders. This is a local relation, included for comparison. Probe accuracy scores exhibit little variance, and control accuracies are almost identical, close to $\mathbf{0 . 5}$, which is expected for random binary classification. All language models allow exhibit probe accuracies above $80 \%$. This indicates the insufficiency of probing only for local relations.</p>
<p>The results of the similarity analysis on city names are displayed in Table 3 and in Figure 1. It is to be observed that representations are highly similar in contextualized LMs, especially compared to those from Word2Vec, which is in accordance with Ethayarajh (2019) results about anisotropy in higher layers of LMs. On the other hand, we observe that representations of cities in the same country are always more similar than cities in dif-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Probe</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Word2Vec</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">BERT-L</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">RoBERTa</th>
<th style="text-align: center;">RoBERTa-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPS</td>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">city</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.517</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">country</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.468</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lasso</td>
<td style="text-align: center;">city</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.466</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">country</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.454</td>
</tr>
<tr>
<td style="text-align: center;">Population</td>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">city</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.707</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">country</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.505</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lasso</td>
<td style="text-align: center;">city</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.531</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">country</td>
<td style="text-align: center;">-0.242</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.116</td>
</tr>
</tbody>
</table>
<p>Table 1: Probe error reduction. "BERT-L" and "RoBERTa-L" denote large versions of BERT and RoBERTa. Column name "Probe" designates the different regression architectures used for probing. For GPS coordinates, error reduction is computed with average error in kilometers, whereas it is computed using MSE for Population. Control tasks are performed on $n=10$ trials, and the control error is obtained by averaging all trials errors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Probe</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prb.</td>
<td style="text-align: center;">Ctrl.</td>
<td style="text-align: center;">Selectivity</td>
</tr>
<tr>
<td style="text-align: center;">Word2Vec</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.34</td>
</tr>
<tr>
<td style="text-align: center;">BERT-L</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 3}$</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">$\mathbf{0 . 3 7}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-L</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.32</td>
</tr>
</tbody>
</table>
<p>Table 2: Probe accuracy (Prb.), control accuracy (Ctrl.) and probe selectivity for country borders prediction.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">intra</th>
<th style="text-align: center;">inter</th>
<th style="text-align: center;">gap</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Word2Vec</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: center;">BERT-L</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.12</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-L</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.01</td>
</tr>
</tbody>
</table>
<p>Table 3: Average representational similarities between cities in the same country (intra), in different countries (inter) and the gap between them.
ferent countries, even if the gap is small for some models. Finally, we can note that no conclusion can be made about the effect of increasing the LM's size in this analysis: the gap is bigger in large version of BERT than in the base version, but this relation does not hold for RoBERTa.</p>
<h2>6 Discussion</h2>
<p>Tasks 1 to 3 suggest that the amount of geographic knowledge learned by the language models is limited. On the one hand, trained predictors for both GPS coordinates and population are of poor quality,
making average errors of thousands of kilometers or hundreds of millions of people. On the other hand, the probes are doing significantly better on these tasks than during random control, and information about country borders have been shown by results of Task 3 to be embedded in pre-trained language models' representations. This suggests that approximate geographic positions and their neighborhood are learned during training by these models. Similarity analysis also indicates that pretrained models have learned to associate representations of cities in the same country.</p>
<p>In the end, language models, whether they are static or contextualized, seem to be able to extract limited geographical knowledge from higher-order co-occurrence statistics. The size of the language models, as well as the amount of data they were trained on, seem to be important for how well they encode this information. Increasing the model's size leads to best performances and error reduction scores, indicating a better learning of geographic knowledge. Overall, context-sensitive language models do not show significantly better performances than Word2Vec. Finally, our experiments demonstrate the importance of probing for global, multi-dimensional relations that require near-isomorphism and are not easily saturated.</p>
<p>In the future, it would be of interest to test models finetuned on corpora which contain a wealth of geographic information (e.g. a corpus of atlases) in order to evaluate a) whether representational alignment to real-world geography can be affected by seeing factual expressions such as "Italy, France, and Switzerland share borders." and b) how exactly representations of entities (e.g. France) alter when the model sees factual statements as the one above.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Representational similarity histograms for BERT, BERT large, RoBERTa and Word2Vec. The similarity is computed through cosine similarity. Blue is the curve of intra-country (same country) similarity; orange is the one for inter-countries (different countries) similarity.</p>
<h2>Acknowledgements</h2>
<p>We thank the anonymous reviewers. Mostafa Abdou was funded by a Google Focused Research Award. We used data created by MaxMind, available from http://www. maxmind.com/.</p>
<h2>References</h2>
<p>Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders Søgaard. 2021. Can language models encode perceptual structure without grounding? a case study in color.</p>
<p>Mostafa Abdou, Artur Kulmizev, and Vinit Ravishankar. 2018. Mgad: Multilingual generation of analogy datasets. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>Samira Abnar, Lisa Beinborn, Rochelle Choenni, and Willem Zuidema. 2019. Blackbox meets blackbox: Representational similarity and stability analysis of neural language models and brains. arXiv preprint arXiv:1906.01539.</p>
<p>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches.</p>
<p>Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014. Don't count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 238-247.</p>
<p>Rishi Bommasani, Kelly Davis, and Claire Cardie. 2020. Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 47584781, Online. Association for Computational Linguistics.</p>
<p>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 136-145.</p>
<p>Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, and Martin Wattenberg. 2019. Visualizing and measuring the geometry of bert. arXiv preprint arXiv:1906.02715.</p>
<p>Guillem Collell Talleda and Marie-Francine Moens. 2016. Is an image worth more than a thousand</p>
<p>words? on the fine-grain semantic differences between visual and linguistic representations. In Proceedings of the 26th International Conference on Computational Linguistics, pages 2807-2817. ACL.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding.</p>
<p>Aleksandr Drozd, Anna Gladkova, and Satoshi Matsuoka. 2016. Word embeddings, analogies, and machine learning: Beyond king-man+ woman= queen. In Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical papers, pages 3519-3530.</p>
<p>Kawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 55-65. Association for Computational Linguistics.</p>
<p>Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith. 2014. Retrofitting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166.</p>
<p>Maxwell Forbes, Ari Holtzman, and Yejin Choi. 2019. Do neural language representations learn physical commonsense? arXiv preprint arXiv:1908.02899.</p>
<p>John Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733-2743, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4):665-695.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.</p>
<p>Roberta: A robustly optimized bert pretraining approach.</p>
<p>Li Lucy and Jon Gauthier. 2017. Are distributional representations ready for the real world? evaluating word vectors for grounded perceptual meaning. arXiv preprint arXiv:1705.11168.</p>
<p>Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings.</p>
<p>David Mimno and Laure Thompson. 2017. The strange geometry of skip-gram with negative sampling. In Empirical Methods in Natural Language Processing.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeff Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. 2019. Visualizing and measuring the geometry of bert. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910.</p>
<p>Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari Rappoport. 2015. How well do distributional models capture different types of semantic knowledge? In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages $726-730$.</p>
<p>Hinrich Schütze. 1992. Dimensions of meaning. In SC, pages 787-796.</p>
<p>Anders Søgaard, Sebastian Ruder, and Ivan Vulić. 2018. On the limitations of unsupervised bilingual dictionary induction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 778788, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ivan Vulić, Sebastian Ruder, and Anders Søgaard. 2020. Are all good word vector spaces isomorphic? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3178-3192, Online. Association for Computational Linguistics.</p>
<p>Yadollah Yaghoobzadeh and Hinrich Schütze. 2016. Intrinsic subspace evaluation of word embedding representations. arXiv preprint arXiv:1606.07902.</p>
<h2>A Appendices</h2>
<h2>A. 1 Task 1 performances</h2>
<p>On the following tables, Prb. stands for "probing task" and Ctl. for "control task".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MLP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Lasso</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prb.</td>
<td style="text-align: center;">Ctl.</td>
<td style="text-align: center;">Prb.</td>
<td style="text-align: center;">Ctl.</td>
</tr>
<tr>
<td style="text-align: center;">Word2Vec</td>
<td style="text-align: center;">2612</td>
<td style="text-align: center;">7825</td>
<td style="text-align: center;">3447</td>
<td style="text-align: center;">6870</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">4195</td>
<td style="text-align: center;">8057</td>
<td style="text-align: center;">3780</td>
<td style="text-align: center;">6920</td>
</tr>
<tr>
<td style="text-align: center;">BERT-L</td>
<td style="text-align: center;">3315</td>
<td style="text-align: center;">7997</td>
<td style="text-align: center;">3077</td>
<td style="text-align: center;">6911</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">4613</td>
<td style="text-align: center;">8011</td>
<td style="text-align: center;">4498</td>
<td style="text-align: center;">7070</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">4278</td>
<td style="text-align: center;">8007</td>
<td style="text-align: center;">4148</td>
<td style="text-align: center;">6894</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-L</td>
<td style="text-align: center;">3876</td>
<td style="text-align: center;">8029</td>
<td style="text-align: center;">3686</td>
<td style="text-align: center;">6903</td>
</tr>
</tbody>
</table>
<p>Mean error distances in kilometers for GPS prediction of cities</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MLP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Lasso</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prb.</td>
<td style="text-align: center;">Ctrl.</td>
<td style="text-align: center;">Prb.</td>
<td style="text-align: center;">Ctrl.</td>
</tr>
<tr>
<td style="text-align: center;">Word2Vec</td>
<td style="text-align: center;">3738</td>
<td style="text-align: center;">8695</td>
<td style="text-align: center;">$\mathbf{4 3 7 9}$</td>
<td style="text-align: center;">7234</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">4950</td>
<td style="text-align: center;">8598</td>
<td style="text-align: center;">4944</td>
<td style="text-align: center;">8152</td>
</tr>
<tr>
<td style="text-align: center;">BERT-L</td>
<td style="text-align: center;">$\mathbf{3 6 0 3}$</td>
<td style="text-align: center;">8578</td>
<td style="text-align: center;">4488</td>
<td style="text-align: center;">8394</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">5111</td>
<td style="text-align: center;">8840</td>
<td style="text-align: center;">5658</td>
<td style="text-align: center;">8684</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">5522</td>
<td style="text-align: center;">9275</td>
<td style="text-align: center;">5036</td>
<td style="text-align: center;">6091</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-L</td>
<td style="text-align: center;">4764</td>
<td style="text-align: center;">8960</td>
<td style="text-align: center;">4433</td>
<td style="text-align: center;">8125</td>
</tr>
</tbody>
</table>
<p>Mean error distances in kilometers for GPS prediction of countries</p>
<h2>A. 2 Task 2 performances on countries</h2>
<p>Reported results are mean squared errors. Therefore, mean absolute errors are at most of the magnitude of few hundreds (of millions) of people.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MLP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Lasso</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prb.</td>
<td style="text-align: center;">Ctrl.</td>
<td style="text-align: center;">Prb.</td>
<td style="text-align: center;">Ctrl.</td>
</tr>
<tr>
<td style="text-align: center;">Word2Vec</td>
<td style="text-align: center;">12142</td>
<td style="text-align: center;">31815</td>
<td style="text-align: center;">22112</td>
<td style="text-align: center;">17810</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">16382</td>
<td style="text-align: center;">39952</td>
<td style="text-align: center;">26583</td>
<td style="text-align: center;">30063</td>
</tr>
<tr>
<td style="text-align: center;">BERT-L</td>
<td style="text-align: center;">17166</td>
<td style="text-align: center;">46365</td>
<td style="text-align: center;">22559</td>
<td style="text-align: center;">31174</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">15264</td>
<td style="text-align: center;">3566</td>
<td style="text-align: center;">32130</td>
<td style="text-align: center;">51171</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">15266</td>
<td style="text-align: center;">32338</td>
<td style="text-align: center;">26375</td>
<td style="text-align: center;">28592</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-L</td>
<td style="text-align: center;">16390</td>
<td style="text-align: center;">33112</td>
<td style="text-align: center;">22927</td>
<td style="text-align: center;">25923</td>
</tr>
</tbody>
</table>
<p>Mean squared error distances (in millions of people) for prediction of country's population.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://radimrehurek.com/gensim/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
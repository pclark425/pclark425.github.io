<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5382 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5382</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5382</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-baa8f524c82735f174b8d1ab512ac5750146d67e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/baa8f524c82735f174b8d1ab512ac5750146d67e" target="_blank">KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A novel knowledge graph augmented pre-trained language generation model KG-BART is proposed, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output and can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets.</p>
                <p><strong>Paper Abstract:</strong> Generative commonsense reasoning which aims to empower machines to generate sentences with the capacity of reasoning over a set of concepts is a critical bottleneck for text generation. Even the state-of-the-art pre-trained language generation models struggle at this task and often produce implausible and anomalous sentences. One reason is that they rarely consider incorporating the knowledge graph which can provide rich relational information among the commonsense concepts. To promote the ability of commonsense reasoning for text generation, we propose a novel knowledge graph augmented pre-trained language generation model KG-BART, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output. Moreover, KG-BART can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets. Experiments on benchmark CommonGen dataset verify the effectiveness of our proposed approach by comparing with several strong pre-trained language generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in terms of BLEU-3, 4. Moreover, we also show that the generated context by our model can work as background scenarios to benefit downstream commonsense QA tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5382.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5382.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph-Augmented BART</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-augmented encoder-decoder built on top of BART that incorporates ConceptNet subgraphs (concept-reasoning and concept-expanding graphs) via graph-aware attention and special integration/disintegration layers to improve generative commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG-augmented encoder-decoder (graph-to-text integration/disintegration)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert KG subgraphs into continuous representations and fuse them with textual token streams: (1) map input concept set to ConceptNet to build a concept-reasoning subgraph G^R (collect 1–3 hop triplets between concepts) and a concept-expanding graph G^E (G^R plus top-k neighboring nodes selected by GloVe cosine similarity, with adjective/adverb filtering); (2) learn node and relation embeddings with TransE on the selected triplets; (3) in the encoder use Subword-to-Concept Integration (CNN+max-pooling) to get word-level textual embeddings, apply a modified multi-head graph attention (MGAT) that incorporates relation embeddings to update concept representations, then Concept-to-Subword Disintegration (Deconv1D) to project updated concept signals back to subword token positions; (4) in the decoder apply Multi-Head Hierarchical Graph Attention (MHGAT) to first incorporate neighboring adjunct nodes then intra-concept relations, and attend from decoder states both to updated KG concept embeddings and to textual encoder states; (5) node and relation embeddings are held fixed during KG-BART fine-tuning; pre-training is performed by masking up to five concept tokens and reconstructing the original text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (ConceptNet subgraphs: concept-reasoning graph G^R and concept-expanding graph G^E)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves explicit multi-hop relational structure via subgraph extraction; uses learned KG embeddings (TransE) to encode relations; aligns cross-granularity information (tokens ↔ concepts) via CNN pooling and Deconv upsampling; hierarchical attention separates neighbor (adjunct) vs intra-concept relations; top-k neighbor selection (by GloVe similarity) trades off richness vs noise; representation is continuous (embeddings) not literal linearized text; node/relation embeddings are fixed during downstream training (stability but less end-to-end KG adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Generative commonsense text generation (CommonGen) and transfer to commonsense question answering (CSQA) via generated context augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated with BLEU (BLEU-3, BLEU-4), ROUGE (ROUGE-2, ROUGE-L), METEOR, CIDEr, SPICE, Coverage, plus human ranking. Reported highlights: KG-BART BLEU-3 = 42.10 and BLEU-4 = 30.90 (table values reported in paper); Coverage = 98.68. Relative improvements reported in text: +5.80 BLEU-3 and +4.60 BLEU-4 over BART (abstract); outperforms T5-large by 7.95%/8.04% on BLEU-3/4, +12.50% on CIDEr and +3.48% on SPICE. Human evaluation average rating 4.27 (KG-BART) vs 4.02 (BART) and 3.91 (T5-large).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to strong pre-trained seq2seq baselines (BART, T5-large, UniLM, GPT-2, etc.). KG-BART consistently outperforms those baselines on CommonGen metrics (automatic + human). Ablation experiments show benefits of KG-augmented encoder+decoder and each designed component (SCI/CSD, MGAT/MHGAT, pre-training).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on mapping concepts to KG unigram entities (requires exact match); neighbor expansion is heuristic (GloVe similarity + POS filter) and may include or exclude relevant adjuncts; TransE embeddings are trained separately and then frozen (not end-to-end KG adaptation); scalability and sensitivity to top-k choice not fully explored; representation is not an explicit text linearization of the graph but a continuous fusion, so direct interpretability as text is indirect; some concept pairs may lack direct KG connections, requiring multi-hop collection which could introduce noisy paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5382.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5382.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G^E (concept-expanding graph)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concept-Expanding Graph (G^E)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical expansion of the concept-reasoning graph that augments each input concept with a ranked set of neighboring nodes from ConceptNet to provide adjunct descriptive information for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>concept-expanding graph (top-k neighbor expansion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>G^E = G^R ∪ N(v^R) where for each concept node v^R neighbors in ConceptNet are ranked by cosine similarity of GloVe embeddings (sum of similarities to all concepts) and the top-k neighbors are selected; when collecting candidate triples, if the concept is a noun only adjective neighbors are considered, and if a verb only adverb neighbors are considered. The selected neighbor nodes and their relation triplets are added to the decoder-side graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (neighbor-expanded ConceptNet subgraph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Provides adjunct, descriptive lexical cues (adjectives/adverbs) to enrich generation; reduces noise compared to including all neighbors by ranking with GloVe similarity and POS-based filtering; hierarchical (neighbors used first then intra-concept relations) to control influence during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used within KG-BART for CommonGen generation and for generating context in CSQA transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No standalone metric; its contribution is measured via KG-BART ablations and overall KG-BART performance. Ablation analyses indirectly measure its effect through the decoder graph attention (MHGAT) removal results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared implicitly against using only G^R (no expansion) via ablation of decoder KG components; full KG-BART (with G^E + MHGAT) outperforms variants without the hierarchical expansion/attention.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Neighbor ranking depends on pre-trained GloVe similarity and a heuristic POS filter (adjective/adverb), which may omit other relevant relation types; choice of top-k and the reliance on surface-form embeddings may bias which adjuncts are selected; adding many neighbors risks introducing irrelevant facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5382.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5382.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCI + CSD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subword-to-Concept Integration (SCI) and Concept-to-Subword Disintegration (CSD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pair of modules that align BART's subword token representations with word-level KG concept representations: SCI pools subword embeddings to a word-level concept vector; CSD upsamples updated concept vectors back to subword positions for further token-level processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>granularity alignment (SCI/CSD)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>SCI: group subword tokens for each concept, apply a Conv1D with max-pooling to produce a single word-level embedding per concept. After KG-aware updates (MGAT), CSD: upsample the updated word-level hidden state by repeating (m-l+1) times and apply a Deconv1D (transpose convolution using the same kernel parameters) to reconstruct subword-level hidden states, followed by a feed-forward + residual + layer-norm to integrate with original token embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>N/A (alignment mechanism between token sequence and KG concept nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables cross-granularity fusion while preserving token-level sequence for the Transformer; reduces naive copying of the same concept embedding to all subword positions (a more structured up/down sampling); trainable CNN/Deconv kernels allow learned alignment patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ablation tests on CommonGen (integrated into KG-BART).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation (2) removing SCI/CSD resulted in degraded performance compared to full model: (paper reports) ablation (2) BLEU-3/4 = 41.20 / 29.70 and ROUGE-2/L = 23.15 / 43.57, which is worse than full KG-BART (full numbers reported elsewhere in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared against a naive strategy that uses the same entity representation at each subword position (i.e., no SCI/CSD). SCI/CSD gives better results than that naive approach according to ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires segmentation of concepts into subword groups and assumes reliable alignment between concept spans and tokenization; convolution/deconvolution parameters and kernel sizes introduce hyperparameters; may not generalize if concepts are multiword phrases with complex tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5382.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5382.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGAT / MHGAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Head Graph Attention (MGAT) and Multi-Head Hierarchical Graph Attention (MHGAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph-attention layers adapted to incorporate relation embeddings into attention for updating concept node representations; MGAT is used in the encoder, and MHGAT is a two-stage hierarchical attention used in the decoder (neighbor-then-intra-concept).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-attention with relation embeddings (MGAT / MHGAT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>MGAT: modify GAT attention scoring to include relation embedding r_ij alongside query/key transforms of node hidden states: z_ij = LeakyReLU(W_a [W_q h_i; W_k h_j; W_r r_ij]). Use multi-head concatenation. MHGAT (decoder): two stacked graph-attention layers—first update concept nodes from their inter-concept neighboring (adjunct) nodes with r^N relations, then update concepts considering intra-concept relations r^R. Updated concept embeddings are then attended to by decoder tokens via standard multi-head attention.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (ConceptNet subgraphs, relation-labeled edges)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicitly incorporates relation embeddings into attention scores (so attention is relation-aware); hierarchical ordering lets adjunct neighbor information and concept-to-concept relations have separate updates; compatible with Transformer multi-head attention paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used inside KG-BART; contribution evaluated by ablation (removing MGAT and MHGAT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation (3) removing MGAT/MHGAT reported BLEU-3/4 = 40.90 / 29.30 and ROUGE-2/L = 22.96 / 43.78 (worse than full KG-BART), indicating the graph attention layers contribute to final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared implicitly against concatenating entity embeddings with word embeddings (naive fusion). MGAT/MHGAT outperform simple concatenation per ablation results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Adds computational overhead relative to vanilla Transformer blocks; effectiveness depends on quality of relation embeddings; relation-aware attention weights may be sensitive to noise in chosen neighbor relations or incorrect KG links.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5382.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5382.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TransE (as used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TransE (Translating Embeddings for Modeling Multi-Relational Data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A translational knowledge-embedding method used to produce continuous node and relation embeddings for the extracted ConceptNet subgraphs before being consumed by MGAT/MHGAT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translating embeddings for modeling multi-relational data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG → continuous embeddings via TransE</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train TransE on selected triplets collected from 1-hop, 2-hop, and 3-hop paths between concept pairs and on triplets between concept nodes and selected neighboring nodes; TransE produces node vectors v_i ∈ R^d_e and relation vectors r_ij ∈ R^d_r, which are then used directly in graph-attention layers (paper notes for G^R they set r_ij^R = v_i^R - v_j^R 'instead of the output of TransE' to avoid missing concept relations).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph embeddings on ConceptNet subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Embeddings compress discrete triplets into continuous vectors that capture multi-relational signals; training on multi-hop paths increases connectivity but may introduce spurious correlations; embeddings are learned separately and kept fixed during KG-BART training.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as the KG representation component inside KG-BART; contribution measured indirectly via KG-BART performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>No direct comparison in the paper versus other KGE methods (e.g., DistMult, ComplEx) — TransE chosen and trained on collected triplets; for G^R relation embeddings the authors sometimes use difference of node vectors instead of TransE relation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>TransE's simple translational assumption can be limited for complex relations (e.g., 1-to-many); training TransE separately and freezing embeddings means no end-to-end adaptation to the generation objective; authors note they sometimes replace TransE relation outputs with v_i - v_j to avoid missing relations, indicating practical mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5382.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5382.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-BERT (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-BERT: Enabling Language Representation with Knowledge Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related approach (cited) that injects KG triples into Transformer input by adding triples as supplementary tokens (augmenting the input sequence) to infuse domain knowledge into language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-BERT: Enabling Language Representation with Knowledge Graph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>triple-injection (KG-as-supplementary-words)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Mentioned in related work: K-BERT injects domain knowledge by adding triples from the knowledge graph into the textual input (effectively linearizing triples as additional tokens in the input), allowing the Transformer to attend to both text and inserted KG facts.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (triples linearized into token stream)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Produces an explicit textual expansion of KG facts (easy to interpret as text), but can introduce input-length blowup and requires special masking/truncation; contrasted with KG-BART's continuous embedding fusion approach.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Mentioned in related work generally for language representation tasks (AAAI paper cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Cited as inspiration/contrast: KG-BART differs by using continuous KG embeddings + graph-attention rather than literal triple insertion. The paper does not provide empirical head-to-head comparisons with K-BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As described in related work, naive triple insertion can disrupt language context and increase input length; not directly assessed by this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5382.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5382.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERNIE / KEPLER (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERNIE; KEPLER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ERNIE and KEPLER are cited pre-trained approaches that incorporate entities/knowledge into language representations: ERNIE aligns informative entities with context; KEPLER jointly optimizes knowledge embedding and language modeling by encoding textual descriptions of entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ERNIE: Enhanced language representation with informative entities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>entity-aware pretraining / joint KG-text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Mentioned in related work: ERNIE incorporates KG entities aligned with text to enhance pretraining; KEPLER encodes textual descriptions of entities and jointly optimizes KGE and language model objectives to learn unified representations. These are entity-aware pretraining strategies (contrast to KG-BART's graph-attention fusion in a seq2seq generation model).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph / entity descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Jointly learns/aligns entity and text spaces (in KEPLER) or augments text with entity signals (ERNIE); tends toward tighter latent alignment compared to frozen-embedding approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Mentioned as related work for language understanding and representation; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Cited for context: KG-BART follows the spirit of integrating knowledge into pre-trained models but focuses on generation; the paper does not present direct empirical comparisons to ERNIE/KEPLER on CommonGen.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not evaluated within this paper; mentioned to position KG-BART among knowledge-enhanced pretraining efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ConceptNet 5.5: an open multilingual graph of general knowledge <em>(Rating: 2)</em></li>
                <li>Translating embeddings for modeling multi-relational data <em>(Rating: 2)</em></li>
                <li>K-BERT: Enabling Language Representation with Knowledge Graph <em>(Rating: 2)</em></li>
                <li>ERNIE: Enhanced language representation with informative entities <em>(Rating: 2)</em></li>
                <li>KEPLER: A unified model for knowledge embedding and pre-trained language representation <em>(Rating: 2)</em></li>
                <li>CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning <em>(Rating: 2)</em></li>
                <li>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5382",
    "paper_id": "paper-baa8f524c82735f174b8d1ab512ac5750146d67e",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "KG-BART",
            "name_full": "Knowledge Graph-Augmented BART",
            "brief_description": "A graph-augmented encoder-decoder built on top of BART that incorporates ConceptNet subgraphs (concept-reasoning and concept-expanding graphs) via graph-aware attention and special integration/disintegration layers to improve generative commonsense reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "KG-augmented encoder-decoder (graph-to-text integration/disintegration)",
            "representation_description": "Convert KG subgraphs into continuous representations and fuse them with textual token streams: (1) map input concept set to ConceptNet to build a concept-reasoning subgraph G^R (collect 1–3 hop triplets between concepts) and a concept-expanding graph G^E (G^R plus top-k neighboring nodes selected by GloVe cosine similarity, with adjective/adverb filtering); (2) learn node and relation embeddings with TransE on the selected triplets; (3) in the encoder use Subword-to-Concept Integration (CNN+max-pooling) to get word-level textual embeddings, apply a modified multi-head graph attention (MGAT) that incorporates relation embeddings to update concept representations, then Concept-to-Subword Disintegration (Deconv1D) to project updated concept signals back to subword token positions; (4) in the decoder apply Multi-Head Hierarchical Graph Attention (MHGAT) to first incorporate neighboring adjunct nodes then intra-concept relations, and attend from decoder states both to updated KG concept embeddings and to textual encoder states; (5) node and relation embeddings are held fixed during KG-BART fine-tuning; pre-training is performed by masking up to five concept tokens and reconstructing the original text.",
            "graph_type": "Knowledge graph (ConceptNet subgraphs: concept-reasoning graph G^R and concept-expanding graph G^E)",
            "representation_properties": "Preserves explicit multi-hop relational structure via subgraph extraction; uses learned KG embeddings (TransE) to encode relations; aligns cross-granularity information (tokens ↔ concepts) via CNN pooling and Deconv upsampling; hierarchical attention separates neighbor (adjunct) vs intra-concept relations; top-k neighbor selection (by GloVe similarity) trades off richness vs noise; representation is continuous (embeddings) not literal linearized text; node/relation embeddings are fixed during downstream training (stability but less end-to-end KG adaptation).",
            "evaluation_task": "Generative commonsense text generation (CommonGen) and transfer to commonsense question answering (CSQA) via generated context augmentation.",
            "performance_metrics": "Evaluated with BLEU (BLEU-3, BLEU-4), ROUGE (ROUGE-2, ROUGE-L), METEOR, CIDEr, SPICE, Coverage, plus human ranking. Reported highlights: KG-BART BLEU-3 = 42.10 and BLEU-4 = 30.90 (table values reported in paper); Coverage = 98.68. Relative improvements reported in text: +5.80 BLEU-3 and +4.60 BLEU-4 over BART (abstract); outperforms T5-large by 7.95%/8.04% on BLEU-3/4, +12.50% on CIDEr and +3.48% on SPICE. Human evaluation average rating 4.27 (KG-BART) vs 4.02 (BART) and 3.91 (T5-large).",
            "comparison_to_other_representations": "Compared to strong pre-trained seq2seq baselines (BART, T5-large, UniLM, GPT-2, etc.). KG-BART consistently outperforms those baselines on CommonGen metrics (automatic + human). Ablation experiments show benefits of KG-augmented encoder+decoder and each designed component (SCI/CSD, MGAT/MHGAT, pre-training).",
            "limitations_or_challenges": "Relies on mapping concepts to KG unigram entities (requires exact match); neighbor expansion is heuristic (GloVe similarity + POS filter) and may include or exclude relevant adjuncts; TransE embeddings are trained separately and then frozen (not end-to-end KG adaptation); scalability and sensitivity to top-k choice not fully explored; representation is not an explicit text linearization of the graph but a continuous fusion, so direct interpretability as text is indirect; some concept pairs may lack direct KG connections, requiring multi-hop collection which could introduce noisy paths.",
            "uuid": "e5382.0",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "G^E (concept-expanding graph)",
            "name_full": "Concept-Expanding Graph (G^E)",
            "brief_description": "A hierarchical expansion of the concept-reasoning graph that augments each input concept with a ranked set of neighboring nodes from ConceptNet to provide adjunct descriptive information for decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "concept-expanding graph (top-k neighbor expansion)",
            "representation_description": "G^E = G^R ∪ N(v^R) where for each concept node v^R neighbors in ConceptNet are ranked by cosine similarity of GloVe embeddings (sum of similarities to all concepts) and the top-k neighbors are selected; when collecting candidate triples, if the concept is a noun only adjective neighbors are considered, and if a verb only adverb neighbors are considered. The selected neighbor nodes and their relation triplets are added to the decoder-side graph.",
            "graph_type": "Knowledge graph (neighbor-expanded ConceptNet subgraph)",
            "representation_properties": "Provides adjunct, descriptive lexical cues (adjectives/adverbs) to enrich generation; reduces noise compared to including all neighbors by ranking with GloVe similarity and POS-based filtering; hierarchical (neighbors used first then intra-concept relations) to control influence during decoding.",
            "evaluation_task": "Used within KG-BART for CommonGen generation and for generating context in CSQA transfer experiments.",
            "performance_metrics": "No standalone metric; its contribution is measured via KG-BART ablations and overall KG-BART performance. Ablation analyses indirectly measure its effect through the decoder graph attention (MHGAT) removal results.",
            "comparison_to_other_representations": "Compared implicitly against using only G^R (no expansion) via ablation of decoder KG components; full KG-BART (with G^E + MHGAT) outperforms variants without the hierarchical expansion/attention.",
            "limitations_or_challenges": "Neighbor ranking depends on pre-trained GloVe similarity and a heuristic POS filter (adjective/adverb), which may omit other relevant relation types; choice of top-k and the reliance on surface-form embeddings may bias which adjuncts are selected; adding many neighbors risks introducing irrelevant facts.",
            "uuid": "e5382.1",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "SCI + CSD",
            "name_full": "Subword-to-Concept Integration (SCI) and Concept-to-Subword Disintegration (CSD)",
            "brief_description": "A pair of modules that align BART's subword token representations with word-level KG concept representations: SCI pools subword embeddings to a word-level concept vector; CSD upsamples updated concept vectors back to subword positions for further token-level processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "granularity alignment (SCI/CSD)",
            "representation_description": "SCI: group subword tokens for each concept, apply a Conv1D with max-pooling to produce a single word-level embedding per concept. After KG-aware updates (MGAT), CSD: upsample the updated word-level hidden state by repeating (m-l+1) times and apply a Deconv1D (transpose convolution using the same kernel parameters) to reconstruct subword-level hidden states, followed by a feed-forward + residual + layer-norm to integrate with original token embedding.",
            "graph_type": "N/A (alignment mechanism between token sequence and KG concept nodes)",
            "representation_properties": "Enables cross-granularity fusion while preserving token-level sequence for the Transformer; reduces naive copying of the same concept embedding to all subword positions (a more structured up/down sampling); trainable CNN/Deconv kernels allow learned alignment patterns.",
            "evaluation_task": "Ablation tests on CommonGen (integrated into KG-BART).",
            "performance_metrics": "Ablation (2) removing SCI/CSD resulted in degraded performance compared to full model: (paper reports) ablation (2) BLEU-3/4 = 41.20 / 29.70 and ROUGE-2/L = 23.15 / 43.57, which is worse than full KG-BART (full numbers reported elsewhere in paper).",
            "comparison_to_other_representations": "Compared against a naive strategy that uses the same entity representation at each subword position (i.e., no SCI/CSD). SCI/CSD gives better results than that naive approach according to ablation.",
            "limitations_or_challenges": "Requires segmentation of concepts into subword groups and assumes reliable alignment between concept spans and tokenization; convolution/deconvolution parameters and kernel sizes introduce hyperparameters; may not generalize if concepts are multiword phrases with complex tokenization.",
            "uuid": "e5382.2",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "MGAT / MHGAT",
            "name_full": "Multi-Head Graph Attention (MGAT) and Multi-Head Hierarchical Graph Attention (MHGAT)",
            "brief_description": "Graph-attention layers adapted to incorporate relation embeddings into attention for updating concept node representations; MGAT is used in the encoder, and MHGAT is a two-stage hierarchical attention used in the decoder (neighbor-then-intra-concept).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph-attention with relation embeddings (MGAT / MHGAT)",
            "representation_description": "MGAT: modify GAT attention scoring to include relation embedding r_ij alongside query/key transforms of node hidden states: z_ij = LeakyReLU(W_a [W_q h_i; W_k h_j; W_r r_ij]). Use multi-head concatenation. MHGAT (decoder): two stacked graph-attention layers—first update concept nodes from their inter-concept neighboring (adjunct) nodes with r^N relations, then update concepts considering intra-concept relations r^R. Updated concept embeddings are then attended to by decoder tokens via standard multi-head attention.",
            "graph_type": "Knowledge graph (ConceptNet subgraphs, relation-labeled edges)",
            "representation_properties": "Explicitly incorporates relation embeddings into attention scores (so attention is relation-aware); hierarchical ordering lets adjunct neighbor information and concept-to-concept relations have separate updates; compatible with Transformer multi-head attention paradigm.",
            "evaluation_task": "Used inside KG-BART; contribution evaluated by ablation (removing MGAT and MHGAT).",
            "performance_metrics": "Ablation (3) removing MGAT/MHGAT reported BLEU-3/4 = 40.90 / 29.30 and ROUGE-2/L = 22.96 / 43.78 (worse than full KG-BART), indicating the graph attention layers contribute to final performance.",
            "comparison_to_other_representations": "Compared implicitly against concatenating entity embeddings with word embeddings (naive fusion). MGAT/MHGAT outperform simple concatenation per ablation results.",
            "limitations_or_challenges": "Adds computational overhead relative to vanilla Transformer blocks; effectiveness depends on quality of relation embeddings; relation-aware attention weights may be sensitive to noise in chosen neighbor relations or incorrect KG links.",
            "uuid": "e5382.3",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "TransE (as used)",
            "name_full": "TransE (Translating Embeddings for Modeling Multi-Relational Data)",
            "brief_description": "A translational knowledge-embedding method used to produce continuous node and relation embeddings for the extracted ConceptNet subgraphs before being consumed by MGAT/MHGAT.",
            "citation_title": "Translating embeddings for modeling multi-relational data",
            "mention_or_use": "use",
            "representation_name": "KG → continuous embeddings via TransE",
            "representation_description": "Train TransE on selected triplets collected from 1-hop, 2-hop, and 3-hop paths between concept pairs and on triplets between concept nodes and selected neighboring nodes; TransE produces node vectors v_i ∈ R^d_e and relation vectors r_ij ∈ R^d_r, which are then used directly in graph-attention layers (paper notes for G^R they set r_ij^R = v_i^R - v_j^R 'instead of the output of TransE' to avoid missing concept relations).",
            "graph_type": "Knowledge graph embeddings on ConceptNet subgraphs",
            "representation_properties": "Embeddings compress discrete triplets into continuous vectors that capture multi-relational signals; training on multi-hop paths increases connectivity but may introduce spurious correlations; embeddings are learned separately and kept fixed during KG-BART training.",
            "evaluation_task": "Used as the KG representation component inside KG-BART; contribution measured indirectly via KG-BART performance.",
            "performance_metrics": null,
            "comparison_to_other_representations": "No direct comparison in the paper versus other KGE methods (e.g., DistMult, ComplEx) — TransE chosen and trained on collected triplets; for G^R relation embeddings the authors sometimes use difference of node vectors instead of TransE relation outputs.",
            "limitations_or_challenges": "TransE's simple translational assumption can be limited for complex relations (e.g., 1-to-many); training TransE separately and freezing embeddings means no end-to-end adaptation to the generation objective; authors note they sometimes replace TransE relation outputs with v_i - v_j to avoid missing relations, indicating practical mismatches.",
            "uuid": "e5382.4",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "K-BERT (related work)",
            "name_full": "K-BERT: Enabling Language Representation with Knowledge Graph",
            "brief_description": "A related approach (cited) that injects KG triples into Transformer input by adding triples as supplementary tokens (augmenting the input sequence) to infuse domain knowledge into language models.",
            "citation_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
            "mention_or_use": "mention",
            "representation_name": "triple-injection (KG-as-supplementary-words)",
            "representation_description": "Mentioned in related work: K-BERT injects domain knowledge by adding triples from the knowledge graph into the textual input (effectively linearizing triples as additional tokens in the input), allowing the Transformer to attend to both text and inserted KG facts.",
            "graph_type": "Knowledge graph (triples linearized into token stream)",
            "representation_properties": "Produces an explicit textual expansion of KG facts (easy to interpret as text), but can introduce input-length blowup and requires special masking/truncation; contrasted with KG-BART's continuous embedding fusion approach.",
            "evaluation_task": "Mentioned in related work generally for language representation tasks (AAAI paper cited).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Cited as inspiration/contrast: KG-BART differs by using continuous KG embeddings + graph-attention rather than literal triple insertion. The paper does not provide empirical head-to-head comparisons with K-BERT.",
            "limitations_or_challenges": "As described in related work, naive triple insertion can disrupt language context and increase input length; not directly assessed by this paper.",
            "uuid": "e5382.5",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "ERNIE / KEPLER (related work)",
            "name_full": "ERNIE; KEPLER",
            "brief_description": "ERNIE and KEPLER are cited pre-trained approaches that incorporate entities/knowledge into language representations: ERNIE aligns informative entities with context; KEPLER jointly optimizes knowledge embedding and language modeling by encoding textual descriptions of entities.",
            "citation_title": "ERNIE: Enhanced language representation with informative entities",
            "mention_or_use": "mention",
            "representation_name": "entity-aware pretraining / joint KG-text embeddings",
            "representation_description": "Mentioned in related work: ERNIE incorporates KG entities aligned with text to enhance pretraining; KEPLER encodes textual descriptions of entities and jointly optimizes KGE and language model objectives to learn unified representations. These are entity-aware pretraining strategies (contrast to KG-BART's graph-attention fusion in a seq2seq generation model).",
            "graph_type": "Knowledge graph / entity descriptions",
            "representation_properties": "Jointly learns/aligns entity and text spaces (in KEPLER) or augments text with entity signals (ERNIE); tends toward tighter latent alignment compared to frozen-embedding approaches.",
            "evaluation_task": "Mentioned as related work for language understanding and representation; not used in experiments here.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Cited for context: KG-BART follows the spirit of integrating knowledge into pre-trained models but focuses on generation; the paper does not present direct empirical comparisons to ERNIE/KEPLER on CommonGen.",
            "limitations_or_challenges": "Not evaluated within this paper; mentioned to position KG-BART among knowledge-enhanced pretraining efforts.",
            "uuid": "e5382.6",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ConceptNet 5.5: an open multilingual graph of general knowledge",
            "rating": 2
        },
        {
            "paper_title": "Translating embeddings for modeling multi-relational data",
            "rating": 2
        },
        {
            "paper_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
            "rating": 2
        },
        {
            "paper_title": "ERNIE: Enhanced language representation with informative entities",
            "rating": 2
        },
        {
            "paper_title": "KEPLER: A unified model for knowledge embedding and pre-trained language representation",
            "rating": 2
        },
        {
            "paper_title": "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
            "rating": 2
        },
        {
            "paper_title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "rating": 1
        }
    ],
    "cost": 0.01888225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning</h1>
<p>Ye Liu ${ }^{1}$, Yao Wan ${ }^{2}$, Lifang $\mathrm{He}^{3}$, Hao Peng ${ }^{1}$, Philip S. Yu ${ }^{1}$<br>${ }^{1}$ University of Illinois at Chicago, Chicago, IL, USA<br>${ }^{2}$ Huazhong University of Science and Technology, Wuhan, China<br>${ }^{3}$ Lehigh University, Bethlehem, PA, USA ${ }^{1}$ Beihang University, Beijing, China<br>{yliu279, psyu}@uic.edu, wanyao@hust.edu.cn, lih319@lehigh.edu, penghao@act.buaa.edu.cn</p>
<h4>Abstract</h4>
<p>Generative commonsense reasoning which aims to empower machines to generate sentences with the capacity of reasoning over a set of concepts is a critical bottleneck for text generation. Even the state-of-the-art pre-trained language generation models struggle at this task and often produce implausible and anomalous sentences. One reason is that they rarely consider incorporating the knowledge graph which can provide rich relational information among the commonsense concepts. To promote the ability of commonsense reasoning for text generation, we propose a novel knowledge graphaugmented pre-trained language generation model KG-BART, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output. Moreover, KG-BART can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets. Experiments on benchmark CommonGen dataset verify the effectiveness of our proposed approach by comparing with several strong pre-trained language generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in terms of BLEU-3, 4. Moreover, we also show that the generated context by our model can work as background scenarios to benefit downstream commonsense QA tasks. ${ }^{1}$</p>
<h2>Introduction</h2>
<p>Nowadays, numerous benchmarks for commonsense reasoning have been developed to make computers more competent and human-aware. In particular, various pre-trained approaches have achieved impressive performance on the discriminative commonsense tasks - i.e., AI systems are required to choose the correct option from a set of choices based on a given context (Lin et al. 2020), such as CommonsenseQA (Talmor et al. 2019) and COSMOSQA (Huang et al. 2019). However, commonsense reasoning in text generation, known as generative commonsense reasoning, still remains a challenge to existing models, which requires machines to generate a sentence describing a day-to-day scene using concepts from a given concept set.</p>
<p>In recent years, many pre-trained language generation models have been presented for text generation tasks, such as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of the generation outputs of our KGBART model (blue dotted box) and the existing models without knowledge graph augmentation (red dotted box).</p>
<p>GPTs (Radford et al. 2019; Brown et al. 2020), UniLM (Dong et al. 2019), T5 (Raffel et al. 2020) and BART (Lewis et al. 2020). Although they can capture rich language information from text sentence corpus and generate accurate language texts, almost all of them ignore knowledge information and thereby fail to generate output towards capturing the human commonsense. For example, as shown in Figure 1, given a set of commonsense concepts {river, fish, net, catch}, the task is to generate a coherent sentence describing a scenario covering all given concepts, such as "Fisherman uses a strong net to catch plentiful fishes in the river". From our analysis, we note that the state-of-the-art pre-trained models generate implausible and anomalous sentences in this task (red dotted box) - e.g., GPT-2 generated "A fish is catching in a net", UniLM generated "A net catches fish", etc. Moreover, the generated sentences by the pre-trained models are simple and rigid, while the human sentence is more natural and rich, like "plentiful fishes", "wide river", etc.</p>
<p>In this paper, we argue that only using pre-trained language models with textual concepts alone cannot provide sufficient information for generative commonsense reasoning. The commonsense knowledge graphs (KGs) (Speer, Chin, and Havasi 2017) have been developed especially for knowledge representation in symbolic systems, and they provide a lot of candidate commonsense facts mined from corpora, which have been widely used in commonsense QA tasks (Lin</p>
<p>et al. 2019). It would be beneficial to develop a model that can exploit commonsense KGs for generative commonsense reasoning task. For example, as shown in Figure 1, by considering knowledge facts " $&lt;$ fish, HasPrerequisite, using net $&gt;$ " and " $&lt;$ fish, HasSubevent, catch $&gt;$ ", it is easy to recognize the relation between concepts ${$ fish, net, catch $}$, namely using the net to catch fish. Furthermore, the commonsense relation, like " $&lt;$ river, RelatedTo, clean $&gt;$ ", can provide the adjunct word to facilitate generating a more natural and plausible daily scenario sentence.</p>
<p>In light of the fact that the knowledge graph can provide the relational information to enhance the reasoning capacity and provide adjunct words to the concept, we propose a novel Knowledge Graph-Augmented framework for generative commonsense reasoning. It has two major steps: knowledge graph grounding and graph-based encoder-decoder modeling. We first construct two KGs, one is the concept-reasoning graph and another is the concept-expanding graph, both of which encode the entity representations and their dependency relations. Secondly, we propose an encoder-decoder neural architecture, named (KG-BART), by incorporating the grounded KGs into the state-of-the-art pre-trained language generation model BART. KG-BART follows the BART architecture, but instead of using the traditional Transformer, we introduce an effective Knowledge Graph-Augmented Transformer to capture the relations between concept set, where the grounded KGs are used as the additional inputs to the graph attention mechanism. Besides, since the token and concept entity are at different granularity levels, we integrate the text representation with the knowledge concept for relational reasoning and then disintegrate to the token-level.</p>
<p>Overall, the main contributions of this paper are as follows:</p>
<ul>
<li>To the best of our knowledge, this is the first time that the KG is incorporated into the pre-trained model to improve the ability of commonsense reasoning in text generation.</li>
<li>We build the concept-reasoning graph to guide the pretrained model to better reasoning the relationships among concepts. Moreover, we build the concept-expanding graph which considers both the inter-concept relation and intraconcept relation for KG-Augmented decoder to generate more natural and plausible output.</li>
<li>We propose KG-BART, a pre-trained method that is designed to better generate language via knowledge graphs and texts, and enhance the model generalization on unseen concept sets. Particularly, the integration and disintegration components are introduced to fuse the heterogeneous information between the token and concept entity.</li>
<li>The experimental results show that KG-BART significantly outperforms the state-of-the-art pre-trained models on the task of generative commonsense reasoning. Additionally, we show that KG-BART can benefit downstream tasks (e.g., commonsense QA) via generating useful context as background scenarios.</li>
</ul>
<h2>Problem Formulation</h2>
<p>Notation. We use $\mathcal{X}$ to denote the space of all possible concept sets, and use $\mathcal{T}$ and $\mathcal{C}$ to denote the token vocabulary and
concept vocabulary, respectively. The knowledge graph (KG) is denoted as $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{R})$, where $\mathcal{V}$ is the set of entities, $\mathcal{E}$ is the set of edges and $\mathcal{R}$ is the set of relations among entities. For a pair of entities $v_{i} \in \mathcal{V}$ (subject) and $v_{j} \in \mathcal{V}$ (object), associated with the relation $r_{i j} \in \mathcal{R}$, the edge $e_{i j} \in \mathcal{E}$ can be represented as a triplet $\left(v_{i}, r_{i j}, v_{j}\right)$. Specifically, we assume the concept vocabulary is a subset of KG's unigram entities, namely $\mathcal{C} \subset \mathcal{V}$.</p>
<p>Given an unordered set of $k$ commonsense concepts $x=$ $\left{c_{1}, c_{2}, \ldots, c_{k}\right}$, where each concept $c_{i} \in \mathcal{C} \subset \mathcal{X}$ is an object (noun) or action (verb), the ultimate goal of generative commonsense reasoning is to generate a natural language output $y=\left{y_{1}, y_{2}, \ldots, y_{l}\right}$ that is both correct (or valid) and natural sounding for that scenario. This is often modeled by learning a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that maps the concept set $x \in \mathcal{X}$ into a sentence $y \in \mathcal{Y}$. Our aim is to boost the performance of this task with the help of KG database $\mathcal{G}$ which can be treated as auxiliary information.</p>
<p>More formally, we formulate the problem as follows: $h$ : ${\mathcal{X}, \mathcal{G}} \rightarrow\left{\mathcal{G}^{R}, \mathcal{G}^{E}\right}$ that takes the concept sets $x \in \mathcal{X}$ and the knowledge $\mathcal{G}$ as the input to first learn a conceptreasoning graph $\mathcal{G}^{R}$ and a hierarchical concept-expanding graph $\mathcal{G}^{E}$, and then $g:\left{\mathcal{X}, \mathcal{G}^{R}, \mathcal{G}^{E}\right} \rightarrow \mathcal{Y}$ to generate the final outputs. Specifically, $\mathcal{G}^{R} \subset \mathcal{G}$ consisting of all concept triplets $\left(v_{i}^{R}, r_{i j}^{R}, v_{j}^{R}\right)$, where $v_{i}^{R}$ and $v_{j}^{R} \in \mathcal{X}$ and $r_{i j}^{R} \in \mathcal{R}$ is the relation between each concept pairs. $\mathcal{G}^{E}=$ $\left{\mathcal{G}^{R} \cup \mathcal{N}\left(v^{R}\right)\right} \subset \mathcal{G}$ is used to enrich the graph with adjunct information, where $\mathcal{N}\left(v^{R}\right)$ characterizes the neighborhood relationship between concept $\left(v^{R}\right)$ and its adjacencies in the KG database.</p>
<h2>Knowledge Graph Grounding</h2>
<p>In this section, we explain how to construct and learn the embedding representations of the concept-reasoning graph and the hierarchical concept-expanding graph from the large commonsense KG Conceptnet (Speer, Chin, and Havasi 2017). ${ }^{2}$</p>
<p>In the generative commonsense reasoning task, traditional pre-trained methods usually encode the concept $(x)$ and decode the sentence $(y)$ based on text information alone, which ignore the structural information and relations between concepts and suffer from generating a lot of implausible sentences. In order to overcome this drawback, we propose to hybridize the KG and text information in the encoder and decoder modules. Specifically, in the encoder phase, we construct a concept-reasoning graph $\mathcal{G}^{R}$ to encompass the relations between the concept set. In the decoder phase, we construct a hierarchical concept-expanding graph $\mathcal{G}^{E}$ to enrich the concept structure with the neighborhood correlation preserved in the KG database. Based on our assumption, each concept corresponds to a KG's unigram entity, so we can directly match the concept set to the entities from KG to generate $\mathcal{G}^{R}$. In order to establish $\mathcal{G}^{E}$, we couple $\mathcal{G}^{R}$ with the association of selected neighboring nodes with each concept in KG. For many concepts, there are hundreds or thousands of neighboring nodes connected with each of them (via triplets) in KG, which provide us not only rich information but also</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The proposed KG-BART model.
less important or less relevant entities that may be undesirable. For instance, given a concept-set ${$ ski, skier, mountain $}$, considering the adjunct concepts for "mountain", "snowy" is more precise than others like "small" or "flat" according to the close semantics of "snowy" and "ski/skier". Based on this fact, we rank the neighboring nodes of each concept according to the word similarity scores and select their potential top- $k$ neighboring nodes adding to $\mathcal{G}^{R}$, so as to get $\mathcal{G}^{E}$. To calculate the word similarity scores, we use the pre-trained GloVe embedding (Pennington, Socher, and Manning 2014) as the representation of each entity node in KG. The ranking score for a particular neighboring node is the sum of similarity scores with all concepts. Here we use the cosine similarity for its simplicity and wide application.</p>
<p>Since some of concept pairs do not have a direct connection in the KG and some of the concept pairs connect by multiple relations, instead of directly using $\mathcal{G}^{R}$ and $\mathcal{G}^{E}$, we use a knowledge embedding method named TransE (Bordes et al. 2013) to learn their entity and relation embeddings. To prepare the training triplets of TransE model, we first collect the triplets in the one-hop path, two-hop path, and three-hop path between each concept pair. Moreover, we further collect the triples between each concept node and their neighboring nodes as follows: if the concept node is the object (noun), only the neighboring node containing the adjective word will be selected; if the concept node is action (verb), only the node containing adverb word will be selected. TransE model is trained based on those selected triplets, which generates the node embedding $\mathbf{v}<em e="e">{i} \in \mathbb{R}^{d</em>}}$ for each node $v_{i}$ and relation embedding $\mathbf{r<em r="r">{i j} \in \mathbb{R}^{d</em>}}$ for each edge $e_{i j}$. For $\mathcal{G}^{R}$, we denote each concept embedding as $\mathbf{v}^{R}$, and relation embeddings as $\mathbf{r<em i="i">{i j}^{R}=\mathbf{v}</em>$.}^{R}-\mathbf{v}_{j}^{R}$ instead of the output of TransE to avoid missing relations between concepts. For $\mathcal{G}^{E}$, since those neighboring nodes are connected with the concepts in the KG, we directly add their node embeddings $\mathbf{v}^{N}$ and relation embeddings $\mathbf{r}^{N}$ to $\mathcal{G}^{R</p>
<h2>Graph-Based Encoder-Decoder Modeling</h2>
<p>Overview. Figure 2 presents an overview of the proposed KG-BART model, which follows the BART encoder-decoder architecture but uses both text concepts and KG as the input. The encoder is composed of two components: one traditional textual Transformer encoder module (Vaswani et al. 2017) to represent the contextual information of each token; and another KG-augmented Transformer module based on graph attention mechanism to integrate the entity-oriented knowledge information into token representation. Similarly, the
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The KG-augmented encoder.
decoder is also composed of a stack of a textual Transformer decoder module and a KG-augmented Transformer decoder module to generate sentences with the ability of commonsense reasoning. Specially, we use a hierarchical graph attention mechanism to refine the KG-augmented decoder to capture the inherent structural correlations of intra-concept and inter-concept in the graph. Note that all the node and relation embeddings are held fixed in the training process of KG-BART. Since our textual Transformers are the same as that used in BART, here we exclude a comprehensive description of these modules and refer readers to (Lewis et al. 2020) and (Vaswani et al. 2017) for more details. In the following, we will focus on the proposed KG-augmented Transformer.</p>
<h2>KG-Augmented Encoder</h2>
<p>As shown in Figure 3, above the textual encoders, the KGaugmented encoder is designed to enrich the token representation by considering the KG structure. We propose to incorporate graph representations into the neural encoding process via a graph-informed attention mechanism. It takes advantage of the explicit relations to learn better intra-concept relations. Formally, the KG-augmented encoder integrates the input token embeddings $\left{\mathbf{x}<em n="n">{1}, \ldots, \mathbf{x}</em>}\right}$, which is the output of the textual encoders, as well as the embedding of conceptreasoning graph $\mathcal{G}^{R}$ to update the token representation as $\left{\mathbf{x<em n="n">{1}^{e}, \ldots, \mathbf{x}</em>\right}$.}^{e</p>
<p>Subword to Concept Integration (SCI) As the input token embeddings are based on a sequence of subwords, while our concepts in the KG are at word-level, we need to align these different granularity sequences. To apply the relation between concepts, we group the subwords for each concept. In particular, we adopt one convolutional neural network (CNN) (Kim 2014) with a max-pooling layer to efficiently obtain the representation in word-level.</p>
<p>Here we take a concrete concept as an example to better illustrate this process. Supposing that a concept $c_{i}$ is made up of a sequence of subwords $\left{x_{1}, x_{2}, \ldots, x_{m}\right}$, where $m$ is the number of subwords. Given the token embeddings $\mathbf{x}$ from textual encoder, we first utilize a Conv1D layer, $\mathbf{x}<em t="t">{t}^{\prime}=$ $\mathbf{Z}\left(\mathbf{x}</em>}, \mathbf{x<em t_l-1="t+l-1">{t+1}, \ldots, \mathbf{x}</em>$ is trainable parameters and $k$ is the kernel size. We then apply a max-pooling layer over a sequence of}\right)^{T}, t \in[1, m-l+1]$, where $\mathbf{Z}=$ $\left[z_{1}, \ldots, z_{l}\right] \in \mathbb{R}^{1 \times l</p>
<p>the output embeddings after Conv1D:</p>
<p>$$
\mathbf{e}\left(c_{i}\right)=\operatorname{MaxPooling}\left(\mathbf{x}<em m-l_1="m-l+1">{1}^{\prime}, \ldots, \mathbf{x}</em>\right)
$$}^{\prime</p>
<p>Therefore, the final word-level textual embedding of concept is represented as $\mathbf{e}^{w}=\left{\mathbf{e}\left(c_{1}\right), \ldots, \mathbf{e}\left(c_{l}\right)\right} \in \mathbb{R}^{k \times d_{w}}$ where $d_{w}$ denotes the dimension of concept embedding.</p>
<p>Multi-Head Graph Attention (MGAT) Given the embedding representation of concept-reasoning graph $\mathcal{G}^{R}$ with node features $\mathbf{v}^{R} \in \mathbb{R}^{k \times d_{v}}$ and relation features $\mathbf{r}^{R}$, we apply the graph attention networks (GAT) (Veličković et al. 2017) to iteratively update the representations for each concept $\mathbf{v}<em i="i">{i}^{R}$ through its neighbors $\mathcal{N}</em>}^{R}$. We denote the word-level hidden state as $\mathbf{h<em h="h">{i} \in \mathbb{R}^{d</em>}}$, where $i \in(1, \ldots, k)$. We further modify the GAT layer to infuse the pairwise relation embedding $\mathbf{r<em r="r">{i j}^{R} \in \mathbb{R}^{d</em>$. Therefore, the multi-head graph attention can be denoted as:}</p>
<p>$$
\begin{aligned}
&amp; \mathbf{H}=\left[\mathbf{e}^{w} ; \mathbf{W}<em i="i" j="j">{e} \mathbf{v}^{R}\right] \
&amp; z</em>}=\operatorname{LeakyReLU}\left(\mathbf{W<em q="q">{a}\left[\mathbf{W}</em>} \mathbf{h<em k="k">{i} ; \mathbf{W}</em>} \mathbf{h<em r="r">{j} ; \mathbf{W}</em>} \mathbf{r<em i="i" j="j">{i j}^{R}\right]\right) \
&amp; \alpha</em>}=\frac{\exp \left(z_{i j}\right)}{\sum_{l=1}^{|\mathcal{N<em i="i" l="l">{i}^{R}|} \exp \left(z</em>}\right)}, \quad \mathbf{h<em k="1">{i}^{\prime}=|</em>}^{K} \sigma\left(\sum_{j=1}^{|\mathcal{N<em i="i" j="j">{i}^{R}|} \alpha</em>}^{k} \mathbf{W<em i="i">{v}^{k} \mathbf{h}</em>\right)
\end{aligned}
$$</p>
<p>where $K$ is the multi-head number, $|<em a="a">{k=1}^{K}$ denotes an operation of multi-head used in Transformer, which concatenates the attention embeddings from different heads and feeds the result into a linear projection. $\mathbf{W}</em>}, \mathbf{W<em r="r">{e}, \mathbf{W}</em>}, \mathbf{W<em k="k">{q}, \mathbf{W}</em>}$ and $\mathbf{W<em i="i" j="j">{v}$ are trainable weights and $\alpha</em>}$ is the attention weight between $\mathbf{h<em j="j">{i}$ and $\mathbf{h}</em>$ incorporates relation representations as prior constraints into the encoding process. In this way, our model can learn better and richer concept representations containing the relationship among concepts.
Concept to Subword Disintegration (CSD) After updating the word-level hidden state considering the relation between concepts in the KG, we need to disintegrate the concept to the subword-level for the following process. We first upsample word-level hidden state $\mathbf{h}}$. The word-level hidden state $\mathbf{H}$ contains the latent dependencies between any two concepts from textual aspect information $\mathbf{e}^{w}$ and KG aspect information $\mathbf{v}^{R}$. And $\mathbf{r}^{R<em i="i">{i}^{\prime}$ with $(m-l+1)$ times (the length before MaxPooling) as $\left[\mathbf{h}</em>}^{\prime 1}, \ldots, \mathbf{h<em 0="0">{i}^{\prime m-l+1}\right]$ and utilize a Deconv1D layer with vector $\mathbf{Z}=\left[z</em>}, \ldots, z_{l}\right] \in \mathbb{R}^{1 \times l}$ used in Conv1D to form the Deconv1D matrix $\mathbf{Z<em i="i">{D} \in \mathbb{R}^{m \times(m-l+1)}$ to get the subword-level hidden state $\mathbf{u}</em>$ :</p>
<p>$$
\left[\mathbf{u}<em i="i">{i}^{1}, \ldots, \mathbf{u}</em>
z_{0} &amp; &amp; &amp; \
\cdots &amp; z_{0} &amp; &amp; \
z_{l} &amp; \cdots &amp; \cdots &amp; \
&amp; z_{l} &amp; &amp; z_{0} \
&amp; &amp; &amp; \cdots \
&amp; &amp; &amp; z_{l}
\end{array}\right) *\left(\begin{array}{c}
\mathbf{h}}^{m}\right]^{T}=\left(\begin{array}{cccc<em i="i">{i}^{\prime 1} \
\mathbf{h}</em> \
\cdot \
\cdot \
\cdot \
\mathbf{h}_{i}^{\prime m-l+1}
\end{array}\right)
$$}^{\prime 2</p>
<p>Then, a two-layer feed-forward network with GeLU activation (Hendrycks and Gimpel 2016) function and a residual layer normalization are applied to obtain the final output can be represented $\mathbf{x}_{i}^{o}$ :</p>
<p>$$
\begin{aligned}
&amp; \mathbf{p}<em 2="2" o="o">{i}=\mathbf{W}</em>} \operatorname{GeLU}\left(\mathbf{W<em i="i">{o 1}\left(\mathbf{u}</em>}+\mathbf{x<em i="i">{i}\right)\right) \
&amp; \mathbf{x}</em>}^{o}=\operatorname{LayerNorm}\left(\mathbf{p<em i="i">{i}+\mathbf{x}</em>\right)
\end{aligned}
$$</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The KG-augmented decoder.
where $\mathbf{W}<em f="f">{o 1} \in \mathbb{R}^{d</em>} \times d_{h}}$ and $\mathbf{W<em h="h">{o 2} \in \mathbb{R}^{d</em>$ is the hidden size of the feedforward layer.} \times d_{f}}$ are learnable parameters, $d_{f</p>
<h2>KG-Augmented Decoder</h2>
<p>In this section, our KG-augmented decoder, as shown in Figure 4, incorporates hierarchical graph structure into the decoding process to capture the relations between concepts and their neighboring nodes which can help to generate more precise and natural output. To embody the hierarchical conceptexpanding graph $\mathcal{G}^{E}$ with the generation process, we propose the multi-head hierarchical graph attention layer.</p>
<h2>Multi-Head Hierarchical Graph Attention (MHGAT)</h2>
<p>To contain the adjunct description for the concept node, the first layer of hierarchical graph attention is to update the concept node $\mathbf{v}<em i="i">{i}^{R} \in \mathbb{R}^{d e}$ through its inter-concept neighboring nodes $\mathcal{N}</em>$.}^{N}$ with relation embedding $\mathbf{r}_{i j}^{N} \in \mathbb{R}^{d r</p>
<p>$$
\begin{aligned}
&amp; z_{i j}=\operatorname{LeakyReLU}\left(\mathbf{W}<em q="q">{a}\left[\mathbf{W}</em>} \mathbf{v<em k="k">{i}^{R} ; \mathbf{W}</em>} \mathbf{v<em r="r">{j}^{N} ; \mathbf{W}</em>} \mathbf{r<em i="i" j="j">{i j}^{N}\right]\right) \
&amp; \alpha</em>}=\frac{\exp \left(z_{i j}\right)}{\sum_{l=1}^{|\mathcal{N<em i="i" l="l">{i}^{R}|} \exp \left(z</em>}\right)}, \quad \mathbf{v<em k="1">{i}^{R t}=|</em>}^{K} \sigma\left(\sum_{j=1}^{|\mathcal{N<em i="i" j="j">{i}^{R}|} \alpha</em>}^{k} \mathbf{W<em j="j">{v}^{k} \mathbf{v}</em>\right)
\end{aligned}
$$}^{R</p>
<p>After updating the concepts with their neighboring nodes, the concepts get their new embedding $\mathbf{v}^{R t}$. The second graph attention layer updates the concept representation considering the intra-concept relations $\mathbf{r}_{i j}^{R} \in \mathbb{R}^{d r}$.</p>
<p>$$
\begin{aligned}
&amp; z_{i j}=\operatorname{LeakyReLU}\left(\mathbf{W}<em q="q">{a}\left[\mathbf{W}</em>} \mathbf{v<em k="k">{i}^{R t} ; \mathbf{W}</em>} \mathbf{v<em r="r">{j}^{R t} ; \mathbf{W}</em>} \mathbf{r<em i="i" j="j">{i j}^{R}\right]\right) \
&amp; \alpha</em>}=\frac{\exp \left(z_{i j}\right)}{\sum_{l=1}^{|\mathcal{N<em i="i" l="l">{i}^{R}|} \exp \left(z</em>}\right)}, \quad \mathbf{v<em k="1">{i}^{R \prime \prime}=|</em>}^{K} \sigma\left(\sum_{j=1}^{|\mathcal{N<em i="i" j="j">{i}^{R}|} \alpha</em>}^{k} \mathbf{W<em j="j">{v}^{k} \mathbf{v}</em>\right)
\end{aligned}
$$}^{R r</p>
<p>We further compute the two multi-head attention (MAT) (Vaswani et al. 2017) to capture textual and KG influence. One is the attention between the encoder hidden state $\mathbf{x}^{o}$ and the previously generated token hidden state $\mathbf{y}$. The other is the attention between the updated concept embeddings $\mathbf{v}^{R \prime \prime}$ and the previously generated token hidden state $\mathbf{y}$ as follows:</p>
<p>$$
\mathrm{AT}^{\mathrm{KG}}=\mathrm{MAT}\left(\mathbf{y}, \mathbf{v}^{R \prime \prime}, \mathbf{v}^{R \prime \prime}\right), \quad \mathrm{AT}^{\mathrm{TX}}=\mathrm{MAT}\left(\mathbf{y}, \mathbf{x}^{o}, \mathbf{x}^{o}\right)
$$</p>
<p>The final decoder output is the concatenate of the two attention with a residual connection as:</p>
<p>$$
\mathbf{y}^{o}=\mathbf{W}_{a t t}\left[\mathrm{AT}^{\mathrm{KG}} ; \mathrm{AT}^{\mathrm{TX}}\right]+\mathbf{y}
$$</p>
<p>where $\mathbf{W}<em h="h">{a t t} \in \mathbb{R}^{d</em>} \times 2 d_{h}}$ is the trainable weight. $\mathbf{y}^{o}$ is used to predict the token sequence: $P_{\text {vocab }}=$ $\operatorname{softmax}\left(\mathbf{W<em _out="{out" _text="\text">{\text {out }} \mathbf{y}^{o}+\mathbf{b}</em>}}\right), \mathbf{W<em h="h">{a t t} \in \mathbb{R}^{V \times d</em>$ and $V$ is the vocabulary size.}</p>
<h2>KG-BART Model Pre-Training</h2>
<p>The embedding vectors of words in text and nodes/entities in KG are obtained in separate ways, making their vector-space inconsistent. In order to fuse the KG into BART, similar to BART, KG-BART is trained by corrupting texts and then optimizing a reconstruction loss, the cross-entropy, between the decoder's output and the original texts. We randomly select five concept nodes from our selected entities and mask some concepts among them. KG-BART still takes the entity and relation embedding of all concepts without considering whether the token is masked. Since the graph in the decoder only contains the concept set entities, the decoder is modified as without updating the concept nodes with their neighboring nodes in the pre-training stage. KG-BART is pre-trained to generate the original concept token from the masked concept nodes. For example, "[mask] wound [mask] teach soldier" in the encoder and "student wound treat teach soldier" in the decoder. The number of the masked token is randomly sampled from 0 to 5 .</p>
<h2>Experiment and Analysis</h2>
<p>Dataset CommonGen (Lin et al. 2020) is a constrained text generation task, which is to explicitly test the ability of machines on commonsense reasoning when generating a text. The dataset released in this task is constructed through a combination of crowdsourced and existing caption corpora, which consists of 77 k commonsense descriptions over 35 k unique concept sets. In average, each concept set is composed of $3 \sim 5$ unique concepts. We present the basic statistics of this dataset in Table 1. Notably, all pairs of concepts in every test concept set are unseen in training data so that it poses a challenge for text generalization.</p>
<p>Baselines We compare the performance of our proposed model with several state-of-the-art pre-trained text generation models. GPT-2 (Radford et al. 2019) is an unidirectional model to predict tokens given the input text in an auto-regressive manner. UniLM (Dong et al. 2019) proposes a unified model of language understanding and language generation using the masked language modeling. UniLM2 (Bao et al. 2020) further proposes a pseudo-masked language model to learn intra-relations between masked spans via partially auto-regressive modeling. BERT-Gen (Bao et al. 2020) fine-tunes BERT for sequence-to-sequence language generation using a similar training objective employed by UniLM. T5 (Raffel et al. 2020) introduces a unified framework that converts all text-based language problems into a text-to-text format. BART (Lewis et al. 2020) introduces a denoising autoencoder for pre-training sequence-to-sequence models. For the implementation of those models for the generative</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Concept sets</td>
<td style="text-align: center;">32,651</td>
<td style="text-align: center;">993</td>
<td style="text-align: center;">1,497</td>
</tr>
<tr>
<td style="text-align: left;"># Sentences</td>
<td style="text-align: center;">67,389</td>
<td style="text-align: center;">4,018</td>
<td style="text-align: center;">6,042</td>
</tr>
<tr>
<td style="text-align: left;">\% Unseen Concepts</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$6.53 \%$</td>
<td style="text-align: center;">$8.97 \%$</td>
</tr>
<tr>
<td style="text-align: left;">\% Unseen Concept-Paris</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$96.31 \%$</td>
<td style="text-align: center;">$100.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">\% Unseen Concept-Triples</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$99.60 \%$</td>
<td style="text-align: center;">$100.00 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: The basic statistics of the CommonGen dataset.
commonsense reasoning task, we refer readers to (Lin et al. 2020) for more details.</p>
<p>Automatic Evaluation Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al. 2002), ROUGE (Lin 2004) and METEOR (Banerjee and Lavie 2005), which mainly focus on measuring ngram similarities. We report the Coverage of concept, which is the average percentage of input concepts that are present after lemmatization. In addition, we use evaluation metrics specially designed for image captioning task, such as CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015) and SPICE (Anderson et al. 2016). These metrics focus on evaluating the associations between mentioned concepts instead of n-gram overlap. For example, the SPICE metric uses dependency parse trees as a proxy of scene graphs to measure the similarity of scenarios. To estimate human performance within each metric, we treat each reference sentence in test dataset as a system prediction and compare it with other references. It is equivalent to compute inter-annotator agreement.</p>
<p>Table 2 presents the experimental results in a variety of metrics and methods reported on the Leaderboard. ${ }^{3}$ We can see that KG-BART performs best among all the pre-trained models. KG-BART outperforms $7.95 \% / 8.04 \%$ on BLEU-3/4 than the second best model T5-large. KG-BART gains 1.15 improvements than the second best model BART on ROUGE2, the gain 0.67 than UniLM on ROUGE-L. KG-BART gains 1.50 on METEOR than the second best model BART. KGBART beats the second best model T5-large by $12.50 \%$ on CIDEr and $3.48 \%$ on SPICE. Moreover, KG-BART gets the highest Coverage 98.68 among all baseline pre-trained models. The results suggest that leveraging the pre-trained generation model with the knowledge graph can improve the performance of generative commonsense reasoning.</p>
<p>Human Evaluation The automatic evaluations are unable to measure the coherence of the generated text properly. Therefore, we also access system performance by human evaluation. We randomly select 100 instances from the CommonGen test set and invite 3 annotators to access the outputs of different models independently. Annotators access the overall quality of generative commonsense sentence by ranking them from 1 (worst) to 5 (best) taking into account the following four criteria: (1) Rationality: is the sentence the reasonable commonsense scenario? (2) Fluency: is the sentence fluent and grammatical? (3) Succinctness: does the sentence avoid repeating information? (4) Naturalness: does</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model \Metrics</th>
<th style="text-align: center;">BLEU-3/4</th>
<th style="text-align: center;">ROUGE-2/L</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">CIDEr</th>
<th style="text-align: center;">SPICE</th>
<th style="text-align: center;">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2 (Radford et al. 2019)</td>
<td style="text-align: center;">30.70</td>
<td style="text-align: center;">21.10</td>
<td style="text-align: center;">17.18</td>
<td style="text-align: center;">39.28</td>
<td style="text-align: center;">26.20</td>
<td style="text-align: center;">12.15</td>
</tr>
<tr>
<td style="text-align: left;">BERT-Gen (Bao et al. 2020)</td>
<td style="text-align: center;">30.40</td>
<td style="text-align: center;">21.10</td>
<td style="text-align: center;">18.05</td>
<td style="text-align: center;">40.49</td>
<td style="text-align: center;">27.30</td>
<td style="text-align: center;">12.49</td>
</tr>
<tr>
<td style="text-align: left;">UniLM (Dong et al. 2019)</td>
<td style="text-align: center;">38.30</td>
<td style="text-align: center;">27.70</td>
<td style="text-align: center;">21.48</td>
<td style="text-align: center;">43.87</td>
<td style="text-align: center;">29.70</td>
<td style="text-align: center;">14.85</td>
</tr>
<tr>
<td style="text-align: left;">UniLM-v2 (Bao et al. 2020)</td>
<td style="text-align: center;">31.30</td>
<td style="text-align: center;">22.10</td>
<td style="text-align: center;">18.24</td>
<td style="text-align: center;">40.62</td>
<td style="text-align: center;">28.10</td>
<td style="text-align: center;">13.10</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base (Raffel et al. 2020)</td>
<td style="text-align: center;">26.00</td>
<td style="text-align: center;">16.40</td>
<td style="text-align: center;">14.57</td>
<td style="text-align: center;">34.55</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">9.16</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large (Raffel et al. 2020)</td>
<td style="text-align: center;">39.00</td>
<td style="text-align: center;">28.60</td>
<td style="text-align: center;">22.01</td>
<td style="text-align: center;">42.97</td>
<td style="text-align: center;">30.10</td>
<td style="text-align: center;">14.96</td>
</tr>
<tr>
<td style="text-align: left;">BART (Lewis et al. 2020)</td>
<td style="text-align: center;">36.30</td>
<td style="text-align: center;">26.30</td>
<td style="text-align: center;">22.23</td>
<td style="text-align: center;">41.98</td>
<td style="text-align: center;">30.90</td>
<td style="text-align: center;">13.92</td>
</tr>
<tr>
<td style="text-align: left;">Human Performance</td>
<td style="text-align: center;">48.20</td>
<td style="text-align: center;">44.90</td>
<td style="text-align: center;">48.88</td>
<td style="text-align: center;">63.79</td>
<td style="text-align: center;">36.20</td>
<td style="text-align: center;">43.53</td>
</tr>
<tr>
<td style="text-align: left;">KG-BART</td>
<td style="text-align: center;">$\mathbf{4 2 . 1 0}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 9 0}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 3 8}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 5 4}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 4 0}$</td>
<td style="text-align: center;">$\mathbf{1 6 . 8 3}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results of different baseline methods on the CommonGen test dataset. We show the best results in boldface, and those with the second best performance are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{1}$</th>
<th style="text-align: center;">$\mathbf{2}$</th>
<th style="text-align: center;">$\mathbf{3}$</th>
<th style="text-align: center;">$\mathbf{4}$</th>
<th style="text-align: center;">$\mathbf{5}$</th>
<th style="text-align: center;">Rating</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">$22 \%$</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$19 \%$</td>
<td style="text-align: center;">2.98</td>
</tr>
<tr>
<td style="text-align: left;">UniLM</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$17 \%$</td>
<td style="text-align: center;">$22 \%$</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$32 \%$</td>
<td style="text-align: center;">3.61</td>
</tr>
<tr>
<td style="text-align: left;">T5-large</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$32 \%$</td>
<td style="text-align: center;">$39 \%$</td>
<td style="text-align: center;">3.91</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$17 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$42 \%$</td>
<td style="text-align: center;">4.02</td>
</tr>
<tr>
<td style="text-align: left;">KG-BART</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$8 \%$</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$55 \%$</td>
<td style="text-align: center;">$\mathbf{4 . 2 7}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Ranking results of system outputs by human evaluation. 1 is the worst and 5 is the best. The larger rating denotes a better summary quality.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A case study of a specific concept set {stand, hold, street, umbrella } for qualitative analysis of machine generations. Human references are collected from AMT.
the sentence use adjunct words? The rating of each system is computed by averaging the scores on all test instances.</p>
<p>Table 3 summarizes the comparison results of five methods. Both the percentage of ranking results and overall ratings are reported. The results demonstrate that KG-BART is able to generate higher quality output than other models. Specifically, the outputs generated by KG-BART usually contains more reasonable scenario and are more fluent and precise than other models. The human evaluation results further validate the effectiveness of our proposed model. Moreover, based on the 100 final scores for each approach, we conduct Wilcoxon signed-rank tests (Wilcoxon, Katti, and Wilcox 1970). Comparing KG-BART with T5-Large and BART, the $p$-values of Wilcoxon signed-rank testing at $95 \%$ confidence level are $1.2 e-4$ and $2.9 e-3$, which mean the improvements achieved by our approach are statistically significant.</p>
<p>Case Study Figure 5 gives a specific input concept set {stand, hold, street, umbrella}, together with the text genera-
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Attention weights of the last layers of BART and KG-BART encoder.
tions of different models and human references. We find that the outputs of fine-tuned pre-trained language models have several problems: (1) not covering all concepts, e.g., GPT-2 only covers "hold, umbrella, street", ignoring the "stand", (2) unreasonable commonsense relationship between concepts, e.g. in UniLM, the output "A man stands next to an umbrella on a street" is a rare scenario in daily life, and (3) repeating the same content and incorrect grammar, e.g. in BART, it uses both "holding an umbrella" and "holds an umbrella", which is repeated information, and in GPT-2, the indefinite article of "umbrella" should be "an" rather than "a". By contrast, the output generated by KG-BART covers all concepts and is a relatively reasonable scenario and is comparatively as natural and plausible as the references stated by human.</p>
<p>We also visualize the attention weights of the last layers of KG-BART and BART encoder to validate that our model can capture the better relationship between concepts, as shown in Figures 6. We can see that the related concept pairs in KG-BART attend much more attention, which is consistent with that in the knowledge graph. For example, in practice, "weight" has a strong relationship with "gym" on the knowledge graph and the attention weight between them should be large. However, this strong relationship has not been demonstrated in BART without knowledge graph. Therefore, it is reasonable to introduce a knowledge graph as relationship augmentation for better concept representation, also as a guidance to generate more reasonable sentences further.</p>
<p>Ablation Study To evaluate the contributions of individual components of our proposed framework, we conduct ablation analysis to investigate the following research questions: (1) whether the KG-augmented encoder and decoder improves</p>
<table>
<thead>
<tr>
<th>Ablation methods</th>
<th></th>
<th>BLEU-3/4</th>
<th>ROUGE-2/L</th>
</tr>
</thead>
<tbody>
<tr>
<td>(1) KG-Aug Enc. ✓</td>
<td>Dec. ✗</td>
<td>40.40/29.40</td>
<td>22.66/43.13</td>
</tr>
<tr>
<td>(2) SCI ✗</td>
<td>CSD ✗</td>
<td>41.20/29.70</td>
<td>23.15/43.57</td>
</tr>
<tr>
<td>(3) MGAT ✗</td>
<td>MHGAT ✗</td>
<td>40.90/29.30</td>
<td>22.96/43.78</td>
</tr>
<tr>
<td>(4) Pre-training✗</td>
<td></td>
<td>39.80/27.90</td>
<td>21.87/42.92</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study of the proposed model. SCI, CSD, MGAT and MHGAT are KG-BART components.</p>
<p>the performance? (2) whether KG-BART is good at incorporating entity embedding with Transformer? (3) does the KG-BART pre-training works?</p>
<p>To this end, we test on the following ablations: (1) textual Transformer with only KG-augmented encoder; (2) using the same entity representation at each subword position rather than using SCI and CSD; (3) concatenate the entity embedding with word embedding rather than using MGAT and MHGAT; and (4) without the KG-BART pre-training. Table 4 summarizes the ablation results. It shows that KG-BART can still outperform all these four variants, certifying the effectiveness of each designed component in our model and we can also see that incorporating KG with the pre-trained model can help the model achieve a better performance.</p>
<p>Transfer KG-BART to Commonsense QA We also investigate whether the ability of generative commonsense reasoning in KG-BART can benefit commonsense-centric downstream tasks such as Commonsense Question Answering (CSQA) (Talmor et al. 2019). We use the models trained on the CommonGen dataset for generating useful context to the question. We extract the nouns and verbs in questions and five choices, and combine the concepts of question q and each choice $c_{i}$ to build concept sets. Then, we construct the concept-reasoning and concept-expanding graphs based on concepts and use these concept sets and the graphs as inputs to KG-BART to generate the context sentence $g_{i}$ for each choice. Finally, we prepend the outputs in front of questions, i.e., “<s>G:g<s>Q:q</s>C:c<s>”. The RoBERTa (Liu et al. 2019) model for CSQA uses the same form without “G:g<s>” in fine-tuning.</p>
<p>We show the learning curve in Figure 7, where $X$ axis is the number of training steps and $Y$ axis is the accuracy on official dev dataset. We find that in most cases, using the context generated by pre-trained models can further improve the performance of original RoBERTa by a large margin. Especially, KG-BART converges at better accuracy from 76.22 (in original RoBERTa) to 79.31 and it outperforms other baselines. We find that the context generated by our model KG-BART can speed up training about 2.5 times, if we look at the 550th steps of KG-BART (75.51) and 1,400th steps of original RoBERTa (75.31).</p>
<h3>Related Work</h3>
<p>Incorporating Commonsense for NLG There are a few recent works that incorporate commonsense knowledge in language generation tasks such as storytelling (Guan, Wang, and Huang 2019), visual storytelling (Yang et al. 2019b), essay generation (Yang et al. 2019a), evidence generation (Liu</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The learning curve of transfer study on CSQA.</p>
<p>et al. 2020b) and conversational generation systems (Zhang et al. 2020). These works suggest that generative commonsense reasoning has great potential to benefit downstream applications. Our proposed model KG-BART, to the best of our knowledge, is the first work on equipping the pre-trained language generation model with the external commonsense knowledge for the constrained language generation.</p>
<p>Enhancing Pre-Trained Model with Knowledge Recently, several works have attempted to learn joint representation learning of words and entities for effectively leveraging external KGs on language understanding tasks and achieved promising results. ERNIE (Zhang et al. 2019) incorporates informative entities from KG aligning with context to enhance pre-training language understanding. KEPLER (Wang et al. 2020) encodes textual descriptions of entities with a pre-trained language understanding model, and then jointly optimize the knowledge embedding and language modeling objectives. K-BERT (Liu et al. 2020a) injects domain knowledge into the models by adding triples from the knowledge graph as supplementary words. Inspired by these works, we argue that extra knowledge information can effectively benefit existing pre-training models on the language understanding tasks. In this paper, we utilize KGs to train an enhanced language generation model by incorporating the entity relationships to improve the language representation.</p>
<h3>Conclusion</h3>
<p>We have presented a KG-augmented approach KG-BART based on pre-trained BART for generative commonsense reasoning. Through capturing the relations among concepts over a KG, KG-BART can generate high-quality sentences even in the unseen concept sets. KG-BART further considers the neighbor entities of each concept node as to generate more natural and logical sentences. It can also be extended to any seq2seq pre-trained language generation models, like T5 (Raffel et al. 2020) and MASS (Song et al. 2019). Experimental results demonstrate that KG-BART has better abilities of both commonsense reasoning and text generalization.</p>
<h3>Acknowledgements</h3>
<p>We would like to thank all the reviewers for their helpful comments. This work is supported by NSF under grants III-1763325, III-1909323, and SaTC-1930941.</p>
<h2>References</h2>
<p>Anderson, P.; Fernando, B.; Johnson, M.; and Gould, S. 2016. Spice: Semantic propositional image caption evaluation. In Proceedings of ECCV, 382-398. Springer.
Banerjee, S.; and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop.
Bao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu, X.; Wang, Y.; Piao, S.; Gao, J.; Zhou, M.; et al. 2020. Unilmv2: Pseudo-masked language models for unified language model pre-training. arXiv preprint arXiv:2002.12804 .
Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In Proceedings of NeurIPS, 2787-2795.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. In Proceedings of NeurIPS.
Dong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y.; Gao, J.; Zhou, M.; and Hon, H.-W. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of NeurIPS, 13063-13075.
Guan, J.; Wang, Y.; and Huang, M. 2019. Story ending generation with incremental encoding and commonsense knowledge. In Proceedings of AAAI, volume 33, 6473-6480.
Hendrycks, D.; and Gimpel, K. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 .
Huang, L.; Bras, R. L.; Bhagavatula, C.; and Choi, Y. 2019. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of EMNLP.
Kim, Y. 2014. Convolutional neural networks for sentence classification. In Proceedings of EMNLP, 1746-1751.
Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of ACL, 7871-7880.
Lin, B. Y.; Chen, X.; Chen, J.; and Ren, X. 2019. Kagnet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of EMNLP, 2829-2839.
Lin, B. Y.; Shen, M.; Zhou, W.; Zhou, P.; Bhagavatula, C.; Choi, Y.; and Ren, X. 2020. CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning. In Proceedings of EMNLP findings.
Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of Text summarization branches out, 74-81.
Liu, W.; Zhou, P.; Zhao, Z.; Wang, Z.; Ju, Q.; Deng, H.; and Wang, P. 2020a. K-BERT: Enabling Language Representation with Knowledge Graph. In Proceedings of AAAI.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 .</p>
<p>Liu, Y.; Yang, T.; You, Z.; Fan, W.; and Yu, P. S. 2020b. Commonsense Evidence Generation and Injection in Reading Comprehension. In Proceedings of SIGDIAL, 61-73.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, 311-318.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP, 1532-1543.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsupervised multitask learners. OpenAI Blog 1(8): 9.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; and Narang, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR .
Song, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y. 2019. Mass: Masked sequence to sequence pre-training for language generation. In Proceedings of ICML, 5926-5936.
Speer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet 5.5: an open multilingual graph of general knowledge. In Proceedings of AAAI, 4444-4451.
Talmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of NAACL.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Proceedings of NeurIPS, 5998-6008.
Vedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015. Cider: Consensus-based image description evaluation. In Proceedings of CVPR, 4566-4575.
Veličković, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017. Graph attention networks. In Proceedings of ICLR.
Wang, X.; Gao, T.; Zhu, Z.; Liu, Z.; Li, J.; and Tang, J. 2020. KEPLER: A unified model for knowledge embedding and pre-trained language representation. TACL .
Wilcoxon, F.; Katti, S.; and Wilcox, R. A. 1970. Critical values and probability levels for the Wilcoxon rank sum test and the Wilcoxon signed rank test. Selected tables in mathematical statistics 1: 171-259.
Yang, P.; Li, L.; Luo, F.; Liu, T.; and Sun, X. 2019a. Enhancing topic-to-essay generation with external commonsense knowledge. In Proceedings of ACL, 2002-2012.
Yang, P.; Luo, F.; Chen, P.; Li, L.; Yin, Z.; He, X.; and Sun, X. 2019b. Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling. In Proceedings of IJCAI, 5356-5362.
Zhang, H.; Liu, Z.; Xiong, C.; and Liu, Z. 2020. Grounded conversation generation as guided traverses in commonsense knowledge graphs. In Proceedings of ACL, 2031-2043.
Zhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu, Q. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of ACL, 1441-1451.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://inklab.usc.edu/CommonGen/leaderboard.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
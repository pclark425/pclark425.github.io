<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-391 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-391</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-391</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-232283218</p>
                <p><strong>Paper Title:</strong> scite: A smart citation index that displays the context of citations and classifies their intent using deep learning</p>
                <p><strong>Paper Abstract:</strong> Abstract Citation indices are tools used by the academic community for research and research evaluation that aggregate scientific literature output and measure impact by collating citation counts. Citation indices help measure the interconnections between scientific papers but fall short because they fail to communicate contextual information about a citation. The use of citations in research evaluation without consideration of context can be problematic because a citation that presents contrasting evidence to a paper is treated the same as a citation that presents supporting evidence. To solve this problem, we have used machine learning, traditional document ingestion methods, and a network of researchers to develop a “smart citation index” called scite, which categorizes citations based on context. Scite shows how a citation was used by displaying the surrounding textual context from the citing paper and a classification from our deep learning model that indicates whether the statement provides supporting or contrasting evidence for a referenced work, or simply mentions it. Scite has been developed by analyzing over 25 million full-text scientific articles and currently has a database of more than 880 million classified citation statements. Here we describe how scite works and how it can be used to further research and research evaluation.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e391.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e391.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBERT fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBERT (fine-tuned for citation classification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of a science-pretrained BERT model (SciBERT) that was fine-tuned on a manually annotated citation-intent dataset to classify citation statements as supporting, contrasting, or mentioning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Fine-tuning SciBERT for citation intent classification</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A pretrained transformer language model (SciBERT) is further trained (fine-tuned) on labeled citation-context examples so that the model maps text snippets (citation sentence ± surrounding sentences) to one of three classes: supporting, contrasting, or mentioning. Training used a curated set of ~38,925 labeled records with a fixed holdout of 9,708 examples; production predictions are tuned for class-weighting to achieve precision targets (precision > 80% per class). Predictions are done in batches by the Veracity component using TensorFlow/Keras on a GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / machine learning classification</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>natural language processing / machine learning (pretrained language models)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>bibliometrics / citation analysis / scientometrics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (fine-tuning a domain-pretrained model on domain-specific annotated data)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Fine-tuned a SciBERT model on an expert-annotated citation-intent dataset; applied oversampling of rare classes, class-weighting at prediction time to balance precision/recall, active learning to select additional training examples, and integration into the Veracity classification pipeline; tuned inference thresholds to ensure production precision >80%.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - produced substantially better performance than earlier architectures: on the fixed holdout SciBERT achieved averaged F-scores of 0.590 (contrasting), 0.648 (supporting), 0.973 (mentioning) and achieved high precision for rare contrasting class (precision reported for contrasting reaches ~85.19%); SciBERT chosen as production architecture due to best trade-off of accuracy and speed.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Extreme class imbalance (contrasting ~0.8% of examples) made learning difficult; need for high-quality, domain-specific labeled data; noisy and heterogeneous citation text (abbreviations, equations); requirement to tune thresholds to meet precision targets; section title inclusion did not improve F-scores; computational resources required (GPU).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of SciBERT (pretrained on scientific text), a large curated annotation set with reconciliation by experts, active learning to select informative examples, use of open-source tooling (DeLFT, TensorFlow/Keras), and GPU hardware for training/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to full-text citation contexts (PDF/XML), high-quality labeled examples for fine-tuning, GPU compute for training/inference, and pipeline integration (tokenization/sentence segmentation) adapted to scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately high — the approach (fine-tuning a domain-pretrained transformer on annotated data) is general and could be applied to other domain-specific classification tasks (other disciplines, other citation-typing ontologies), but performance depends on availability of representative labeled data and text preprocessing adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and technical/technical skills (how to fine-tune and deploy transformer models) with elements of tacit know-how (annotation guidelines, threshold tuning, production integration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e391.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELMo & BidGRU experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELMo embeddings with Bidirectional GRU-based classifiers (and ensemble variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of contextualized word embeddings (ELMo) combined with RNN-based classifiers (BidGRU) and ensemble methods to citation intent classification, evaluated as alternatives to transformer models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Using ELMo contextual embeddings and BidGRU (and ensemble) classifiers for citation classification</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Text is encoded using dynamic contextual embeddings (ELMo) and passed into recurrent neural networks (Bidirectional GRU) with possible metaclassifiers or ensembles to predict citation type. Ensemble versions combined multiple classifiers to improve robustness. ELMo-based approaches were evaluated for F-score improvements over simpler RNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / machine learning classification</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>natural language processing / deep learning (contextual embeddings and RNN classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>bibliometrics / citation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (apply existing NLP embedding+RNN architectures to a new classification problem)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Combined ELMo embeddings with BidGRU classifiers and experimented with ensembles and metaclassification; applied oversampling and class-weighting to handle class imbalance; tuned context windows and experimented with manual phrase selection to improve contrasting F-score.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - ELMo + BidGRU improved F-scores (contrasting from ~0.206 to ~0.405 then up to ~0.460 with ensembles) compared to initial BidGRU baseline, but was slower (ELMo prediction ~20x slower than SciBERT) and ultimately outperformed by SciBERT in production.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Inference speed (ELMo much slower), marginal gains compared to transformer fine-tuning when considering compute trade-offs, and sensitivity to class imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of pretrained ELMo representations, ensembles and metaclassifiers to boost less frequent class performance, and labeled training data for fine-tuning and selection.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Adequate compute resources for slower inference, careful hyperparameter tuning, and strategies to mitigate imbalance (oversampling / class weighting).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to similar text-classification tasks in scientific text, but practical adoption limited by speed and relative performance vs. transformer-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and technical skills (model configuration, ensembling, handling imbalanced data) and empirical heuristics (how to select context windows).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e391.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentiment-analysis-to-citation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application of generic sentiment analysis methods (e.g., SenticNet) to citation contexts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Previous attempts reused general-purpose sentiment analysis techniques (lexicon- and concept-based approaches) to infer polarity (positive/negative) of citation mentions; scite contrasts this with evidence-based citation intent classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Applying sentiment analysis (SenticNet-style) to citation contexts</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Generic sentiment analysis maps terms/concepts in text to polarity scores (positive/negative) using resources like SenticNet; applied to extracted citation contexts to label citations as positive or negative in sentiment without domain-specific customization beyond technical-term removal.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / natural language sentiment analysis</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>opinion mining / commercial sentiment analysis (business reviews, social media)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>citation analysis / scientometrics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without deep modification (with light preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Often only light preprocessing to remove technical terms; no deep customization to scientific discourse; some studies introduced domain-specific preprocessing but not full redefinition of labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful for detecting polarity but limited for capturing scientific 'argumentative' intent — scite notes sentiment methods capture affect but not the evidentiary function (supporting vs contrasting) and can misclassify critical but non-evidentiary statements as negative; hence scite chose an evidence-based three-class ontology instead.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Scientific language differs from everyday sentiment-bearing text (technical terms, neutral reporting), meaning polarity does not equate to supporting/contrasting evidence; many citation contexts lack sentiment words; concept-level mappings (SenticNet) are not tuned to scientific idioms.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Existing sentiment lexica and off-the-shelf tools made rapid prototyping possible; some preprocessing can remove domain-specific noise.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Need for domain adaptation, careful interpretation of what polarity means in scholarly contexts, and potential removal of technical vocabulary to reduce false signals.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Limited — generic sentiment analysis provides signal for polarity but cannot reliably substitute for evidence-based citation typing without substantial adaptation and redefinition of labels.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (use of lexicons/models) but requires interpretive adjustments (conceptual framing) to be valid in the target domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e391.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GROBID ingestion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GROBID (PDF-to-TEI/XML conversion for scholarly documents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source cascade-of-models tool that converts scholarly PDFs into structured TEI/XML, extracting metadata, references, and citation contexts at scale for downstream analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Using GROBID to convert PDFs into structured XML for citation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>GROBID applies supervised models to parse PDF layouts, extract in-text citation callouts, parse reference lists, and output TEI-like XML suitable for large-scale NLP and citation context extraction. The output standardizes structure (sections, references) enabling matching of in-text citations to reference list entries.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>document processing / data extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>digital libraries / document engineering / scholarly publishing tooling</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>large-scale citation extraction and scientometric analysis</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application/adaptation (used as-is but integrated into scite pipeline and combined with other tools)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>GROBID outputs were combined with Pub2TEI normalization and downstream matching (biblio-glutton) in scite's ingestion pipeline; custom postprocessing rules applied (sentence segmentation tailored to scientific text) to better fit scite needs; pipeline parameters tuned for throughput (≈5 PDFs/s on a 4-core server).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful but with limitations - GROBID is benchmarked as the best open-source bibliographical reference parser and enabled scalable extraction; end-to-end pipeline (GROBID + biblio-glutton) associates around 70% of citation contexts to cited papers with correctly identified DOIs for PDFs, with higher (~95%) match rates when publisher XML is available.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>PDFs are noisy and designed for visual layout (not text mining), causing ~30% of citation/reference extraction failures from PDFs; heterogeneous publisher XML formats require normalization; sentence segmentation in scientific text is challenging due to abbreviations and equations.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>GROBID's cascade-of-models design, open-source community adoption, benchmarking showing state-of-the-art reference parsing, and integration with Pub2TEI and biblio-glutton.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to PDF files, compute to run GROBID at scale, additional normalization steps (Pub2TEI) for diverse XML sources, and downstream DOI matching resources (Crossref).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for tasks needing structured conversions of scholarly PDFs to machine-readable formats; applicable across bibliometrics, large-scale text mining, and corpus-building tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and technical skills (how to run/scale GROBID and postprocess outputs) with some tacit know-how (tuning rules for scientific sentence segmentation and handling publisher-specific quirks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e391.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pub2TEI normalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pub2TEI (publisher XML-to-TEI normalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversion tool that standardizes a variety of native publisher XML formats into a common TEI schema so that downstream processing (citation extraction and classification) can operate consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Converting heterogeneous publisher XML to TEI using Pub2TEI</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Pub2TEI maps different publisher-specific XML schemas (JATS and proprietary variants) into a common TEI representation, normalizing inconsistent or incomplete XML encodings so that the same extraction and classification tools can be applied uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data preprocessing / document normalization</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>publishing / XML schema transformation (document engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>citation ingestion and NLP pipelines for scientometrics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application/adaptation (used to harmonize diverse XML inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied Pub2TEI's XML transformation to all incoming XML to centralize document processing; combined with GROBID outputs for PDFs to ensure consistent TEI inputs for downstream sentence segmentation and citation matching.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - enabled centralized processing across disparate XML sources and reduced errors that would arise from handling many native formats separately.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Publisher XMLs are often incomplete/inconsistent and evolve over time, requiring maintenance; some publisher files still missing or nonstandard leading to residual extraction issues.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Use of an open-source mapping tool and the ability to process both publisher XML and GROBID-derived TEI for PDFs.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to native publisher XML or converted XML, and engineering effort to maintain mapping rules as publisher formats evolve.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for any large-scale ingestion system where heterogeneous structured inputs must be normalized for downstream NLP and analytics.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and technical skills (schema mapping, XML transformation) with some maintenance-level tacit knowledge (monitoring for format changes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e391.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>biblio-glutton Crossref matching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>biblio-glutton (raw bibliographic reference matching against Crossref)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source reference matching service that takes raw bibliographic strings and parsed fields and matches them to Crossref DOIs to validate and canonicalize cited references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Matching parsed references to Crossref using biblio-glutton</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>biblio-glutton ingests raw bibliographic references (optionally with parsed metadata like title, authors) and queries the Crossref database to find the best DOI match using fuzzy matching and heuristics, returning match confidence metrics and enabling canonical DOI assignment for citation records.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / bibliographic matching</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>bibliographic metadata management / scholarly infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scite ingestion pipeline / citation verification</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application (integration into ingestion pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Integrated biblio-glutton into scite to validate reference DOIs produced by GROBID/Pub2TEI; used evaluations showing F-score ~95.4 on a test set and reported end-to-end pipeline association rates (~70% for PDFs).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - high matching accuracy for raw references with DOI ground truth; improved ability to tie citation contexts to canonical DOIs and thus to the target bibliographic records.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Some references lack DOIs or are malformed; success is higher when full-text XML is available; reliance on Crossref coverage (scite only ingests publications with DOIs and matches only DOIs).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Crossref as an industry standard bibliographic database and biblio-glutton's proven matching heuristics and open-source availability.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to Crossref and high-quality parsed reference strings; ability to handle missing or ambiguous matches with fallback heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within DOI-centric bibliographic ecosystems; less applicable where DOI coverage is sparse or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and technical skills (matching heuristics and integration) with dependency on explicit metadata resources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e391.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active learning & imbalance techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active learning, oversampling, and class-weighting for imbalanced classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adoption of ML techniques to address extreme class imbalance in citation-intent labeling by selecting informative samples, oversampling rare classes, and adjusting class weights during training and prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Active learning plus oversampling and class-weighting for rare-class detection</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Used model scores to identify likely supporting/contrasting statements (rare classes) for manual annotation (active learning), oversampled these classes in the training set, applied class-weighting and metaclassification (three binary classifiers) during model training, and adjusted class weights at prediction time using the holdout set to meet precision targets.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data-annotation strategy and class imbalance mitigation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning practice (imbalanced learning / active learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>annotation and classifier development for citation intent in scientometrics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied ML sampling & annotation strategies to curate training data and training procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Customized active learning selection criteria using classifier scores to oversample supporting and contrasting examples; constructed a working set with oversampled rare classes; used multiple binary classifiers and ensemble methods; applied production-level class-weight tuning to ensure precision thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - materially improved detection of rare classes (contrasting F-score improved from ~0.206 with initial BidGRU to ~0.590 with SciBERT plus these strategies) and enabled building a usable training set of ~38,925 labeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need for large manual annotation effort with expert annotators and reconciliation; selection bias risk if oversampling not carefully managed; empirical hyperparameter tuning required as training data evolves.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Existing literature and practices for imbalanced learning, availability of an initial classifier to bootstrap active learning, and domain experts for annotation and reconciliation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Expert annotators for high-quality labels, an existing classifier to guide active sampling, and evaluation holdouts to tune production thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High — these ML strategies are broadly applicable to other rare-event classification problems in scientific text and beyond, though success depends on available annotation resources.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (active learning workflow, oversampling and weighting schemes) plus tacit knowledge (how to pick informative samples and tune empirical parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e391.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Network analysis / visualization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directed graph network analysis with valenced edges for citation networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of network-science methods (directed graphs, valenced edges) and visualization tools to represent citation relationships enriched by scite's supporting/contrasting/mentioning classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Valenced citation network construction and visualization</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Construct directed graphs where nodes are papers and edges are citation statements labeled by type (supporting = positive weight/green, contrasting = negative/blue, mentioning = neutral). Use standard network analysis (centrality, clustering) and interactive visualizations to explore how research findings interrelate and propagate acceptance or contradiction through the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>analytical method / data visualization / network analysis</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>network science / graph theory / data visualization</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>bibliometrics / meta-science / scientometrics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (apply network analysis to enriched, valenced citation data)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Edges carry categorical valence (supporting/contrasting/mentioning) rather than simple counts; scite implements interactive visualization tools and exports for standard network libraries to compute indices like centrality and clustering on valenced graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful as a concept and prototype - scite implements visualization (example and interactive graph) and suggests richer network analyses are feasible; no quantitative network-analysis results reported in the paper, but use-cases and dashboards are implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires broad coverage of citation contexts to build complete graphs (current ingestion coverage incomplete), design decisions about how to weight valenced edges and aggregate multiple citation statements between nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of classified citation statements, existing graph-analysis libraries and visualization frameworks, and the semantic enrichment (citation types) that adds value beyond undirected citation counts.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Sufficient coverage of full-text citation contexts, tooling for graph construction/analysis, and decisions on aggregation and weighting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High — valenced network analysis can be applied to many corpora where relations have polarities (e.g., patent citations, legal citations) but quality depends on reliable edge labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>interpretive frameworks and explicit analytical procedures (how to build and analyze valenced graphs) with some tacit know-how (choosing aggregation/weighting schemes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e391.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e391.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>doccano annotation platform</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>doccano (web-based text annotation tool adapted for citation labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the general-purpose doccano annotation tool (open-source) to support multiple-blind manual annotation of citation contexts by expert annotators, followed by reconciliation to build the high-quality labeled dataset used for model training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Using doccano for multi-annotator citation-intent labeling with reconciliation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Deploy a web-based annotation platform to present citation snippets to multiple annotators, collect independent labels, record disagreements, and perform reconciliation and expert review to produce consensus labels for training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data annotation / human-in-the-loop procedure</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>NLP annotation workflows / data-labeling tools</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scientometrics training data creation for citation classification</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application (adapting an NLP annotation platform to domain-specific labels and workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Customized annotation guidelines for the three-class evidence-based ontology; required at least two blind annotators per item and a reconciliation step; curated a holdout set (9,708 items) sampled to reflect real-world class distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - produced a high-quality curated dataset (~38,925 labeled records) with reported pre-reconciliation inter-annotator agreement ~78.5% (open domain) and ~90% (biomed batches), enabling reliable model training.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Annotator disagreement on nuanced citation contexts; need for domain expertise in annotators; time-consuming reconciliation process.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of an existing mature annotation tool, clear annotation guidelines, and expert annotators to adjudicate difficult cases.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Recruiting annotators with scientific reading expertise, establishing reconciliation workflows, and building a representative sampling strategy for holdout/working sets.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High — the annotation workflow and platform can be reused for other domain-specific labeling tasks, but quality depends on domain-expert annotators and well-designed guidelines.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>tacit/practical know-how (annotation adjudication, guideline design) together with explicit procedural steps (annotation UI usage, reconciliation pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'scite: A smart citation index that displays the context of citations and classifies their intent using deep learning', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SciBERT: A pretrained language model for scientific text <em>(Rating: 2)</em></li>
                <li>Deep contextualized word representations <em>(Rating: 2)</em></li>
                <li>Structural scaffolds for citation intent classification in scientific publications <em>(Rating: 2)</em></li>
                <li>Automatic classification of citation function <em>(Rating: 2)</em></li>
                <li>Sentiment analysis of scientific citations <em>(Rating: 2)</em></li>
                <li>Machine learning vs. rules and out-of-the-box vs. retrained: An evaluation of open-source bibliographic reference and citation parsers <em>(Rating: 2)</em></li>
                <li>GROBID <em>(Rating: 1)</em></li>
                <li>Can Citation Indexing be Automated? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-391",
    "paper_id": "paper-232283218",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "SciBERT fine-tuning",
            "name_full": "SciBERT (fine-tuned for citation classification)",
            "brief_description": "Application of a science-pretrained BERT model (SciBERT) that was fine-tuned on a manually annotated citation-intent dataset to classify citation statements as supporting, contrasting, or mentioning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Fine-tuning SciBERT for citation intent classification",
            "procedure_description": "A pretrained transformer language model (SciBERT) is further trained (fine-tuned) on labeled citation-context examples so that the model maps text snippets (citation sentence ± surrounding sentences) to one of three classes: supporting, contrasting, or mentioning. Training used a curated set of ~38,925 labeled records with a fixed holdout of 9,708 examples; production predictions are tuned for class-weighting to achieve precision targets (precision &gt; 80% per class). Predictions are done in batches by the Veracity component using TensorFlow/Keras on a GPU.",
            "procedure_type": "computational method / machine learning classification",
            "source_domain": "natural language processing / machine learning (pretrained language models)",
            "target_domain": "bibliometrics / citation analysis / scientometrics",
            "transfer_type": "adapted/modified for new context (fine-tuning a domain-pretrained model on domain-specific annotated data)",
            "modifications_made": "Fine-tuned a SciBERT model on an expert-annotated citation-intent dataset; applied oversampling of rare classes, class-weighting at prediction time to balance precision/recall, active learning to select additional training examples, and integration into the Veracity classification pipeline; tuned inference thresholds to ensure production precision &gt;80%.",
            "transfer_success": "successful - produced substantially better performance than earlier architectures: on the fixed holdout SciBERT achieved averaged F-scores of 0.590 (contrasting), 0.648 (supporting), 0.973 (mentioning) and achieved high precision for rare contrasting class (precision reported for contrasting reaches ~85.19%); SciBERT chosen as production architecture due to best trade-off of accuracy and speed.",
            "barriers_encountered": "Extreme class imbalance (contrasting ~0.8% of examples) made learning difficult; need for high-quality, domain-specific labeled data; noisy and heterogeneous citation text (abbreviations, equations); requirement to tune thresholds to meet precision targets; section title inclusion did not improve F-scores; computational resources required (GPU).",
            "facilitating_factors": "Availability of SciBERT (pretrained on scientific text), a large curated annotation set with reconciliation by experts, active learning to select informative examples, use of open-source tooling (DeLFT, TensorFlow/Keras), and GPU hardware for training/inference.",
            "contextual_requirements": "Access to full-text citation contexts (PDF/XML), high-quality labeled examples for fine-tuning, GPU compute for training/inference, and pipeline integration (tokenization/sentence segmentation) adapted to scientific text.",
            "generalizability": "Moderately high — the approach (fine-tuning a domain-pretrained transformer on annotated data) is general and could be applied to other domain-specific classification tasks (other disciplines, other citation-typing ontologies), but performance depends on availability of representative labeled data and text preprocessing adaptations.",
            "knowledge_type": "explicit procedural steps and technical/technical skills (how to fine-tune and deploy transformer models) with elements of tacit know-how (annotation guidelines, threshold tuning, production integration).",
            "uuid": "e391.0",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "ELMo & BidGRU experiments",
            "name_full": "ELMo embeddings with Bidirectional GRU-based classifiers (and ensemble variants)",
            "brief_description": "Application of contextualized word embeddings (ELMo) combined with RNN-based classifiers (BidGRU) and ensemble methods to citation intent classification, evaluated as alternatives to transformer models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Using ELMo contextual embeddings and BidGRU (and ensemble) classifiers for citation classification",
            "procedure_description": "Text is encoded using dynamic contextual embeddings (ELMo) and passed into recurrent neural networks (Bidirectional GRU) with possible metaclassifiers or ensembles to predict citation type. Ensemble versions combined multiple classifiers to improve robustness. ELMo-based approaches were evaluated for F-score improvements over simpler RNNs.",
            "procedure_type": "computational method / machine learning classification",
            "source_domain": "natural language processing / deep learning (contextual embeddings and RNN classifiers)",
            "target_domain": "bibliometrics / citation analysis",
            "transfer_type": "adapted/modified for new context (apply existing NLP embedding+RNN architectures to a new classification problem)",
            "modifications_made": "Combined ELMo embeddings with BidGRU classifiers and experimented with ensembles and metaclassification; applied oversampling and class-weighting to handle class imbalance; tuned context windows and experimented with manual phrase selection to improve contrasting F-score.",
            "transfer_success": "partially successful - ELMo + BidGRU improved F-scores (contrasting from ~0.206 to ~0.405 then up to ~0.460 with ensembles) compared to initial BidGRU baseline, but was slower (ELMo prediction ~20x slower than SciBERT) and ultimately outperformed by SciBERT in production.",
            "barriers_encountered": "Inference speed (ELMo much slower), marginal gains compared to transformer fine-tuning when considering compute trade-offs, and sensitivity to class imbalance.",
            "facilitating_factors": "Availability of pretrained ELMo representations, ensembles and metaclassifiers to boost less frequent class performance, and labeled training data for fine-tuning and selection.",
            "contextual_requirements": "Adequate compute resources for slower inference, careful hyperparameter tuning, and strategies to mitigate imbalance (oversampling / class weighting).",
            "generalizability": "Applicable to similar text-classification tasks in scientific text, but practical adoption limited by speed and relative performance vs. transformer-based approaches.",
            "knowledge_type": "explicit procedural steps and technical skills (model configuration, ensembling, handling imbalanced data) and empirical heuristics (how to select context windows).",
            "uuid": "e391.1",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Sentiment-analysis-to-citation",
            "name_full": "Application of generic sentiment analysis methods (e.g., SenticNet) to citation contexts",
            "brief_description": "Previous attempts reused general-purpose sentiment analysis techniques (lexicon- and concept-based approaches) to infer polarity (positive/negative) of citation mentions; scite contrasts this with evidence-based citation intent classification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "procedure_name": "Applying sentiment analysis (SenticNet-style) to citation contexts",
            "procedure_description": "Generic sentiment analysis maps terms/concepts in text to polarity scores (positive/negative) using resources like SenticNet; applied to extracted citation contexts to label citations as positive or negative in sentiment without domain-specific customization beyond technical-term removal.",
            "procedure_type": "computational method / natural language sentiment analysis",
            "source_domain": "opinion mining / commercial sentiment analysis (business reviews, social media)",
            "target_domain": "citation analysis / scientometrics",
            "transfer_type": "direct application without deep modification (with light preprocessing)",
            "modifications_made": "Often only light preprocessing to remove technical terms; no deep customization to scientific discourse; some studies introduced domain-specific preprocessing but not full redefinition of labels.",
            "transfer_success": "partially successful for detecting polarity but limited for capturing scientific 'argumentative' intent — scite notes sentiment methods capture affect but not the evidentiary function (supporting vs contrasting) and can misclassify critical but non-evidentiary statements as negative; hence scite chose an evidence-based three-class ontology instead.",
            "barriers_encountered": "Scientific language differs from everyday sentiment-bearing text (technical terms, neutral reporting), meaning polarity does not equate to supporting/contrasting evidence; many citation contexts lack sentiment words; concept-level mappings (SenticNet) are not tuned to scientific idioms.",
            "facilitating_factors": "Existing sentiment lexica and off-the-shelf tools made rapid prototyping possible; some preprocessing can remove domain-specific noise.",
            "contextual_requirements": "Need for domain adaptation, careful interpretation of what polarity means in scholarly contexts, and potential removal of technical vocabulary to reduce false signals.",
            "generalizability": "Limited — generic sentiment analysis provides signal for polarity but cannot reliably substitute for evidence-based citation typing without substantial adaptation and redefinition of labels.",
            "knowledge_type": "explicit procedural steps (use of lexicons/models) but requires interpretive adjustments (conceptual framing) to be valid in the target domain.",
            "uuid": "e391.2",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "GROBID ingestion",
            "name_full": "GROBID (PDF-to-TEI/XML conversion for scholarly documents)",
            "brief_description": "An open-source cascade-of-models tool that converts scholarly PDFs into structured TEI/XML, extracting metadata, references, and citation contexts at scale for downstream analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Using GROBID to convert PDFs into structured XML for citation extraction",
            "procedure_description": "GROBID applies supervised models to parse PDF layouts, extract in-text citation callouts, parse reference lists, and output TEI-like XML suitable for large-scale NLP and citation context extraction. The output standardizes structure (sections, references) enabling matching of in-text citations to reference list entries.",
            "procedure_type": "document processing / data extraction pipeline",
            "source_domain": "digital libraries / document engineering / scholarly publishing tooling",
            "target_domain": "large-scale citation extraction and scientometric analysis",
            "transfer_type": "direct application/adaptation (used as-is but integrated into scite pipeline and combined with other tools)",
            "modifications_made": "GROBID outputs were combined with Pub2TEI normalization and downstream matching (biblio-glutton) in scite's ingestion pipeline; custom postprocessing rules applied (sentence segmentation tailored to scientific text) to better fit scite needs; pipeline parameters tuned for throughput (≈5 PDFs/s on a 4-core server).",
            "transfer_success": "successful but with limitations - GROBID is benchmarked as the best open-source bibliographical reference parser and enabled scalable extraction; end-to-end pipeline (GROBID + biblio-glutton) associates around 70% of citation contexts to cited papers with correctly identified DOIs for PDFs, with higher (~95%) match rates when publisher XML is available.",
            "barriers_encountered": "PDFs are noisy and designed for visual layout (not text mining), causing ~30% of citation/reference extraction failures from PDFs; heterogeneous publisher XML formats require normalization; sentence segmentation in scientific text is challenging due to abbreviations and equations.",
            "facilitating_factors": "GROBID's cascade-of-models design, open-source community adoption, benchmarking showing state-of-the-art reference parsing, and integration with Pub2TEI and biblio-glutton.",
            "contextual_requirements": "Access to PDF files, compute to run GROBID at scale, additional normalization steps (Pub2TEI) for diverse XML sources, and downstream DOI matching resources (Crossref).",
            "generalizability": "High for tasks needing structured conversions of scholarly PDFs to machine-readable formats; applicable across bibliometrics, large-scale text mining, and corpus-building tasks.",
            "knowledge_type": "explicit procedural steps and technical skills (how to run/scale GROBID and postprocess outputs) with some tacit know-how (tuning rules for scientific sentence segmentation and handling publisher-specific quirks).",
            "uuid": "e391.3",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Pub2TEI normalization",
            "name_full": "Pub2TEI (publisher XML-to-TEI normalization)",
            "brief_description": "A conversion tool that standardizes a variety of native publisher XML formats into a common TEI schema so that downstream processing (citation extraction and classification) can operate consistently.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Converting heterogeneous publisher XML to TEI using Pub2TEI",
            "procedure_description": "Pub2TEI maps different publisher-specific XML schemas (JATS and proprietary variants) into a common TEI representation, normalizing inconsistent or incomplete XML encodings so that the same extraction and classification tools can be applied uniformly.",
            "procedure_type": "data preprocessing / document normalization",
            "source_domain": "publishing / XML schema transformation (document engineering)",
            "target_domain": "citation ingestion and NLP pipelines for scientometrics",
            "transfer_type": "direct application/adaptation (used to harmonize diverse XML inputs)",
            "modifications_made": "Applied Pub2TEI's XML transformation to all incoming XML to centralize document processing; combined with GROBID outputs for PDFs to ensure consistent TEI inputs for downstream sentence segmentation and citation matching.",
            "transfer_success": "successful - enabled centralized processing across disparate XML sources and reduced errors that would arise from handling many native formats separately.",
            "barriers_encountered": "Publisher XMLs are often incomplete/inconsistent and evolve over time, requiring maintenance; some publisher files still missing or nonstandard leading to residual extraction issues.",
            "facilitating_factors": "Use of an open-source mapping tool and the ability to process both publisher XML and GROBID-derived TEI for PDFs.",
            "contextual_requirements": "Access to native publisher XML or converted XML, and engineering effort to maintain mapping rules as publisher formats evolve.",
            "generalizability": "High for any large-scale ingestion system where heterogeneous structured inputs must be normalized for downstream NLP and analytics.",
            "knowledge_type": "explicit procedural steps and technical skills (schema mapping, XML transformation) with some maintenance-level tacit knowledge (monitoring for format changes).",
            "uuid": "e391.4",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "biblio-glutton Crossref matching",
            "name_full": "biblio-glutton (raw bibliographic reference matching against Crossref)",
            "brief_description": "An open-source reference matching service that takes raw bibliographic strings and parsed fields and matches them to Crossref DOIs to validate and canonicalize cited references.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Matching parsed references to Crossref using biblio-glutton",
            "procedure_description": "biblio-glutton ingests raw bibliographic references (optionally with parsed metadata like title, authors) and queries the Crossref database to find the best DOI match using fuzzy matching and heuristics, returning match confidence metrics and enabling canonical DOI assignment for citation records.",
            "procedure_type": "computational method / bibliographic matching",
            "source_domain": "bibliographic metadata management / scholarly infrastructure",
            "target_domain": "scite ingestion pipeline / citation verification",
            "transfer_type": "direct application (integration into ingestion pipeline)",
            "modifications_made": "Integrated biblio-glutton into scite to validate reference DOIs produced by GROBID/Pub2TEI; used evaluations showing F-score ~95.4 on a test set and reported end-to-end pipeline association rates (~70% for PDFs).",
            "transfer_success": "successful - high matching accuracy for raw references with DOI ground truth; improved ability to tie citation contexts to canonical DOIs and thus to the target bibliographic records.",
            "barriers_encountered": "Some references lack DOIs or are malformed; success is higher when full-text XML is available; reliance on Crossref coverage (scite only ingests publications with DOIs and matches only DOIs).",
            "facilitating_factors": "Crossref as an industry standard bibliographic database and biblio-glutton's proven matching heuristics and open-source availability.",
            "contextual_requirements": "Access to Crossref and high-quality parsed reference strings; ability to handle missing or ambiguous matches with fallback heuristics.",
            "generalizability": "High within DOI-centric bibliographic ecosystems; less applicable where DOI coverage is sparse or absent.",
            "knowledge_type": "explicit procedural steps and technical skills (matching heuristics and integration) with dependency on explicit metadata resources.",
            "uuid": "e391.5",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Active learning & imbalance techniques",
            "name_full": "Active learning, oversampling, and class-weighting for imbalanced classification",
            "brief_description": "Adoption of ML techniques to address extreme class imbalance in citation-intent labeling by selecting informative samples, oversampling rare classes, and adjusting class weights during training and prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Active learning plus oversampling and class-weighting for rare-class detection",
            "procedure_description": "Used model scores to identify likely supporting/contrasting statements (rare classes) for manual annotation (active learning), oversampled these classes in the training set, applied class-weighting and metaclassification (three binary classifiers) during model training, and adjusted class weights at prediction time using the holdout set to meet precision targets.",
            "procedure_type": "computational method / data-annotation strategy and class imbalance mitigation",
            "source_domain": "machine learning practice (imbalanced learning / active learning)",
            "target_domain": "annotation and classifier development for citation intent in scientometrics",
            "transfer_type": "adapted/modified for new context (applied ML sampling & annotation strategies to curate training data and training procedures)",
            "modifications_made": "Customized active learning selection criteria using classifier scores to oversample supporting and contrasting examples; constructed a working set with oversampled rare classes; used multiple binary classifiers and ensemble methods; applied production-level class-weight tuning to ensure precision thresholds.",
            "transfer_success": "successful - materially improved detection of rare classes (contrasting F-score improved from ~0.206 with initial BidGRU to ~0.590 with SciBERT plus these strategies) and enabled building a usable training set of ~38,925 labeled examples.",
            "barriers_encountered": "Need for large manual annotation effort with expert annotators and reconciliation; selection bias risk if oversampling not carefully managed; empirical hyperparameter tuning required as training data evolves.",
            "facilitating_factors": "Existing literature and practices for imbalanced learning, availability of an initial classifier to bootstrap active learning, and domain experts for annotation and reconciliation.",
            "contextual_requirements": "Expert annotators for high-quality labels, an existing classifier to guide active sampling, and evaluation holdouts to tune production thresholds.",
            "generalizability": "High — these ML strategies are broadly applicable to other rare-event classification problems in scientific text and beyond, though success depends on available annotation resources.",
            "knowledge_type": "explicit procedural steps (active learning workflow, oversampling and weighting schemes) plus tacit knowledge (how to pick informative samples and tune empirical parameters).",
            "uuid": "e391.6",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Network analysis / visualization",
            "name_full": "Directed graph network analysis with valenced edges for citation networks",
            "brief_description": "Application of network-science methods (directed graphs, valenced edges) and visualization tools to represent citation relationships enriched by scite's supporting/contrasting/mentioning classifications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Valenced citation network construction and visualization",
            "procedure_description": "Construct directed graphs where nodes are papers and edges are citation statements labeled by type (supporting = positive weight/green, contrasting = negative/blue, mentioning = neutral). Use standard network analysis (centrality, clustering) and interactive visualizations to explore how research findings interrelate and propagate acceptance or contradiction through the literature.",
            "procedure_type": "analytical method / data visualization / network analysis",
            "source_domain": "network science / graph theory / data visualization",
            "target_domain": "bibliometrics / meta-science / scientometrics",
            "transfer_type": "adapted/modified for new context (apply network analysis to enriched, valenced citation data)",
            "modifications_made": "Edges carry categorical valence (supporting/contrasting/mentioning) rather than simple counts; scite implements interactive visualization tools and exports for standard network libraries to compute indices like centrality and clustering on valenced graphs.",
            "transfer_success": "successful as a concept and prototype - scite implements visualization (example and interactive graph) and suggests richer network analyses are feasible; no quantitative network-analysis results reported in the paper, but use-cases and dashboards are implemented.",
            "barriers_encountered": "Requires broad coverage of citation contexts to build complete graphs (current ingestion coverage incomplete), design decisions about how to weight valenced edges and aggregate multiple citation statements between nodes.",
            "facilitating_factors": "Availability of classified citation statements, existing graph-analysis libraries and visualization frameworks, and the semantic enrichment (citation types) that adds value beyond undirected citation counts.",
            "contextual_requirements": "Sufficient coverage of full-text citation contexts, tooling for graph construction/analysis, and decisions on aggregation and weighting strategies.",
            "generalizability": "High — valenced network analysis can be applied to many corpora where relations have polarities (e.g., patent citations, legal citations) but quality depends on reliable edge labeling.",
            "knowledge_type": "interpretive frameworks and explicit analytical procedures (how to build and analyze valenced graphs) with some tacit know-how (choosing aggregation/weighting schemes).",
            "uuid": "e391.7",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "doccano annotation platform",
            "name_full": "doccano (web-based text annotation tool adapted for citation labeling)",
            "brief_description": "Use of the general-purpose doccano annotation tool (open-source) to support multiple-blind manual annotation of citation contexts by expert annotators, followed by reconciliation to build the high-quality labeled dataset used for model training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Using doccano for multi-annotator citation-intent labeling with reconciliation",
            "procedure_description": "Deploy a web-based annotation platform to present citation snippets to multiple annotators, collect independent labels, record disagreements, and perform reconciliation and expert review to produce consensus labels for training and evaluation.",
            "procedure_type": "data annotation / human-in-the-loop procedure",
            "source_domain": "NLP annotation workflows / data-labeling tools",
            "target_domain": "scientometrics training data creation for citation classification",
            "transfer_type": "direct application (adapting an NLP annotation platform to domain-specific labels and workflows)",
            "modifications_made": "Customized annotation guidelines for the three-class evidence-based ontology; required at least two blind annotators per item and a reconciliation step; curated a holdout set (9,708 items) sampled to reflect real-world class distributions.",
            "transfer_success": "successful - produced a high-quality curated dataset (~38,925 labeled records) with reported pre-reconciliation inter-annotator agreement ~78.5% (open domain) and ~90% (biomed batches), enabling reliable model training.",
            "barriers_encountered": "Annotator disagreement on nuanced citation contexts; need for domain expertise in annotators; time-consuming reconciliation process.",
            "facilitating_factors": "Availability of an existing mature annotation tool, clear annotation guidelines, and expert annotators to adjudicate difficult cases.",
            "contextual_requirements": "Recruiting annotators with scientific reading expertise, establishing reconciliation workflows, and building a representative sampling strategy for holdout/working sets.",
            "generalizability": "High — the annotation workflow and platform can be reused for other domain-specific labeling tasks, but quality depends on domain-expert annotators and well-designed guidelines.",
            "knowledge_type": "tacit/practical know-how (annotation adjudication, guideline design) together with explicit procedural steps (annotation UI usage, reconciliation pipeline).",
            "uuid": "e391.8",
            "source_info": {
                "paper_title": "scite: A smart citation index that displays the context of citations and classifies their intent using deep learning",
                "publication_date_yy_mm": "2021-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SciBERT: A pretrained language model for scientific text",
            "rating": 2,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "Deep contextualized word representations",
            "rating": 2,
            "sanitized_title": "deep_contextualized_word_representations"
        },
        {
            "paper_title": "Structural scaffolds for citation intent classification in scientific publications",
            "rating": 2,
            "sanitized_title": "structural_scaffolds_for_citation_intent_classification_in_scientific_publications"
        },
        {
            "paper_title": "Automatic classification of citation function",
            "rating": 2,
            "sanitized_title": "automatic_classification_of_citation_function"
        },
        {
            "paper_title": "Sentiment analysis of scientific citations",
            "rating": 2,
            "sanitized_title": "sentiment_analysis_of_scientific_citations"
        },
        {
            "paper_title": "Machine learning vs. rules and out-of-the-box vs. retrained: An evaluation of open-source bibliographic reference and citation parsers",
            "rating": 2,
            "sanitized_title": "machine_learning_vs_rules_and_outofthebox_vs_retrained_an_evaluation_of_opensource_bibliographic_reference_and_citation_parsers"
        },
        {
            "paper_title": "GROBID",
            "rating": 1
        },
        {
            "paper_title": "Can Citation Indexing be Automated?",
            "rating": 1,
            "sanitized_title": "can_citation_indexing_be_automated"
        }
    ],
    "cost": 0.0200095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>scite: A smart citation index that displays the context of citations and classifies their intent using deep learning</p>
<p>Josh M Nicholson 
Milo Mordaunt 
Patrice Lopez 
science-miner
France</p>
<p>Ashish Uppala 
Domenic Rosati 
Neves P Rodrigues 
Peter Grabitz 
Charite Universitaetsmedizin Berlin
BerlinGermany</p>
<p>Sean C Rife 
Murray State University
MurrayKYUSA</p>
<p>1 sciteBrooklynNYUSA</p>
<p>scite: A smart citation index that displays the context of citations and classifies their intent using deep learning
10.1162/qss_a_00146RESEARCH ARTICLEbibliometricscitationsevaluationmachine learningpublishingscientometrics
Citation indices are tools used by the academic community for research and research evaluation that aggregate scientific literature output and measure impact by collating citation counts. Citation indices help measure the interconnections between scientific papers but fall short because they fail to communicate contextual information about a citation. The use of citations in research evaluation without consideration of context can be problematic because a citation that presents contrasting evidence to a paper is treated the same as a citation that presents supporting evidence. To solve this problem, we have used machine learning, traditional document ingestion methods, and a network of researchers to develop a "smart citation index" called scite, which categorizes citations based on context. Scite shows how a citation was used by displaying the surrounding textual context from the citing paper and a classification from our deep learning model that indicates whether the statement provides supporting or contrasting evidence for a referenced work, or simply mentions it. Scite has been developed by analyzing over 25 million full-text scientific articles and currently has a database of more than 880 million classified citation statements. Here we describe how scite works and how it can be used to further research and research evaluation.</p>
<p>INTRODUCTION</p>
<p>Citations are a critical component of scientific publishing, linking research findings across time. The first citation index in science, created in 1960 by Eugene Garfield and the Institute for Scientific Information, aimed to "be a spur to many new scientific discoveries in the service of mankind" (Garfield, 1959). Citation indices have facilitated the discovery and evaluation of scientific findings across all fields of research. Citation indices have also led to the establishment of new research fields, such as bibliometrics, scientometrics, and quantitative studies, which have been informative in better understanding science as an enterprise. From these fields have come a variety of citation-based metrics, such as the h-index, a measurement of researcher impact (Hirsch, 2005); the Journal Impact Factor ( JIF), a measurement of journal impact (Garfield, 1955(Garfield, , 1972; and the citation count, a measurement of article impact. Despite the a n o p e n a c c e s s j o u r n a l Citation: Nicholson, J. M., Mordaunt, M., Lopez, P., Uppala, A., Rosati, D., Rodrigues, N. P., Grabitz, P., &amp; Rife, S. C. widespread use of bibliometrics, there have been few improvements in citations and citation indices themselves. Such stagnation is partly because citations and publications are largely behind paywalls, making it exceedingly difficult and prohibitively expensive to introduce new innovations in citations or citation indices. This trend is changing, however, with open access publications becoming the standard (Piwowar, Priem, &amp; Orr, 2019) and organizations such as the Initiative for Open Citations (Initiative for Open Citations, 2017; Peroni &amp; Shotton, 2020) helping to make citations open. Additionally, with millions of documents being published each year, creating a citation index is a large-scale challenge involving significant financial and computational costs.</p>
<p>Historically, citation indices have only shown the connections between scientific papers without any further contextual information, such as why a citation was made. Because of the lack of context and limited metadata available beyond paper titles, authors, and the date of publications, it has only been possible to calculate how many times a work has been cited, not analyze broadly how it has been cited. This is problematic given citations' central role in the evaluation of research. In short, not all citations are made equally, yet we have been limited to treating them as such.</p>
<p>Here we describe scite (scite.ai), a new citation index and tool that takes advantage of recent advances in artificial intelligence to produce "Smart Citations." Smart Citations reveal how a scientific paper has been cited by providing the context of the citation and a classification system describing whether it provides supporting or contrasting evidence for the cited claim, or if it just mentions it.</p>
<p>Such enriched citation information is more informative than a traditional citation index. For example, when Viganó, von Schubert et al. (2018) cites Nicholson, Macedo et al. (2015), traditional citation indices report this citation by displaying the title of the citing paper and other bibliographic information, such as the journal, year published, and other metadata. Traditional citation indices do not have the capacity to examine contextual information or how the citing paper used the citation, such as whether it was made to support or contrast the findings of the cited paper or if it was made in the introduction or the discussion section of the citing paper. Smart Citations display the same bibliographical information shown in traditional citation indices while providing additional contextual information, such as the citation statement (the sentence containing the in-text citation from the citing article), the citation context (the sentences before and after the citation statement), the location of the citation within the citing article (Introduction, Materials and Methods, Results, Discussion, etc.), the citation type indicating intent (supporting, contrasting, or mentioning), and editorial information from Crossref and PubMed, such as corrections and whether the article has been retracted ( Figure 1). Scite previously relied on Retraction Watch data but moved away from this due to licensing issues. Going forward, scite will use its own approach 1 to retraction detection, as well as data from Crossref and PubMed.</p>
<p>Adding such information to citation indices has been proposed before. In 1964, Garfield described an "intelligent machine" to produce "citation markers," such as "critique" or, jokingly, "calamity for mankind" (Garfield, 1964). Citation types describing various uses of citations have been systematically described by Peroni and Shotton in CiTO, the Citation Typing Ontology (Peroni &amp; Shotton, 2012). Researchers have used these classifications or variations of them in several bibliometric studies, such as the analysis of citations (Suelzer, Deal et al., 2019) made to 1 Details of how retractions and other editorial notices can be detected through an automated examination of metadata-even when there is no explicit indication that such notice(s) exist-will be made public via a manuscript currently in preparation. the retracted Wakefield paper (Wakefield, Murch et al., 1998), which found most citations to be negative in sentiment. Leung, Macdonald et al. (2017) analyzed the citations made to a fivesentence letter purporting to show opioids as nonaddictive (Porter &amp; Jick, 1980), finding that most citations were uncritically citing the work. Based on these findings, the journal appended a public health warning to the original letter. In addition to citation analyses at the individual article level, citation analyses taking into account the citation type have also been performed on subsets of articles or even entire fields of research. Greenberg (2009) discovered that citations were being distorted, for example being used selectively to exclude contradictory studies to create a false authority in a field of research, a practice carried into grant proposals. Selective citing might be malicious, as suggested in the Greenberg study, but it might also simply reflect sloppy citation practices or citing without reading. Indeed, Letrud and Hernes (2019) recently documented many cases where people were citing reports for the opposite conclusions than the original authors made.</p>
<p>Despite the advantages of citation types, citation classification and analysis require substantial manual effort on the part of researchers to perform even small-scale analyses (Pride, Knoth, &amp; Harag, 2019). Automating the classification of citation types would allow researchers to dramatically expand the scale of citation analyses, thereby allowing researchers to quickly assess large portions of scientific literature. PLOS Labs attempted to enhance citation analysis with the introduction of "rich citations," which included various additional features to traditional citations such as retraction information and where the citation appeared in the citing paper (PLOS, 2015). However, the project seemed to be mostly a proof of principle, and work on rich citations stopped in 2015, although it is unclear why. Possible reasons that the project did not mature reflect the challenges of accessing the literature at scale, finding a suitable business model for the application, and classifying citation types with the necessary precision Figure 1. Example of scite report page. The scite report page shows citation context, citation type, and various features used to filter and organize this information, including the section where the citation appears in the citing paper, whether or not the citation is a self-citation, and the year of the publication. The example scite report shown in the figure can be accessed at the following link: https://scite.ai/reports/10.7554 /elife.05068. and recall for it to be accepted by users. It is only recently that machine learning techniques have evolved to make this task possible, as we demonstrate here. Additional resources, such as the Colil Database (Fujiwara &amp; Yamamoto, 2015) and SciRide Finder (Volanakis &amp; Krawczyk, 2018) both allow users to see the citation context from open access articles indexed in PubMed Central. However, adoption seems to be low for both tools, presumably due to limited coverage of only open access articles. In addition to the development of such tools to augment citation analysis, various researchers have performed automated citation typing. Machine learning was used in early research to identify citation intent (Teufel, Siddharthan, &amp; Tidhar, 2006) and recently Cohan, Ammar et al. (2019) used deep learning techniques. Athar (2011), Yousif, Niu et al. (2019, and Yan, Chen, and Li (2020) also used machine learning to identify positive and negative sentiments associated with the citation contexts.</p>
<p>Here, by combining the largest citation type analysis performed to date and developing a useful user interface that takes advantage of the extra contextual information available, we introduce scite, a smart citation index.</p>
<p>METHOD</p>
<p>Overview</p>
<p>Smart citations are created by extracting and analyzing citation statements from full-text scientific articles. This process is broken into four major steps (see Figure 2):</p>
<ol>
<li>The retrieval of scientific articles 2. The identification and matching of in-text citations and references within a scientific article 3. The matching of references against a bibliographic database Figure 2. The scite ingestion process. Documents are retrieved from the internet, as well as being received through file transfers directly from publishers and other aggregators. They are then processed to identify citations, which are then tied to items in a paper's reference list. Those citations are then verified, and the information is inserted into scite's database. 4. The classification of the citation statements into citation types using deep learning.</li>
</ol>
<p>We describe the four components in more detail below.</p>
<p>Retrieval of Scientific Documents</p>
<p>Access to full-text scientific articles is necessary to extract and classify citation statements and the citation context. We utilize open access repositories such as PubMed Central and a variety of open sources as identified by Unpaywall (Else, 2018), such as open access publishers' websites, university repositories, and preprint repositories, to analyze open access articles. Other relevant open access document sources, such as Crossref TDM and the Internet Archive have been and are continually evaluated as new sources for document ingestion. Subscription articles used in our analyses have been made available through indexing agreements with over a dozen publishers, including Wiley, BMJ, Karger, Sage, Europe PMC, Thieme, Cambridge University Press, Rockefeller University Press, IOP, Microbiology Society, Frontiers, and other smaller publishers. Once a source of publications is established, documents are retrieved on a regular basis as new articles become available to keep the citation record fresh. Depending on the source, documents may be retrieved and processed anywhere between daily and monthly.</p>
<p>Identification of In-Text Citations and References from PDF and XML Documents</p>
<p>A large majority of scientific articles are only available as PDF files 2 , a format designed for visual layout and printing, not text-mining. To match and extract citation statements from PDFs with high fidelity, an automated process for converting PDF files into reliable structured content is required. Such conversion is challenging, as it requires identifying in-text citations (the numerical or textual callouts that refer to a particular item in the reference list), identifying and parsing the full bibliographical references in the reference list, linking in-text citations to the correct items in this list, and linking these items to their digital object identifiers (DOIs) in a bibliographic database. As our goal is to eventually process all scientific documents, this process must be scalable and affordable. To accomplish this, we utilize GROBID, an opensource PDF-to-XML converter tool for scientific literature (Lopez, 2020a). The goal of GROBID is to automatically convert scholarly PDFs into structured XML representations suitable for large-scale analysis. The structuration process is realized by a cascade of supervised machine learning models. The tool is highly scalable (around five PDF documents per second on a fourcore server), is robust, and includes a production-level web API, a Docker image, and benchmarking facilities. GROBID is used by many large scientific information service providers, such as ResearchGate, CERN, and the Internet Archive to support their ingestion and document workflows (Lopez, 2020a). The tool is also used for creating machine-friendly data sets of research papers, for instance, the recent CORD-19 data set .</p>
<p>Particularly relevant to scite, GROBID was benchmarked as the best open source bibliographical references parser by Tkaczyk, Collins et al. (2018) and has a relatively unique focus on citation context extraction at scale, as illustrated by its usage for building the large-scale 2 As an illustration, the ISTEX project has been an effort from the French state leading to the purchase of 23 million full text articles from the mainstream publishers (Elsevier, Springer-Nature, Wiley, etc.) mainly published before 2005, corresponding to an investment of A55 million in acquisitions. The delivery of full text XML when available was a contractual requirement, but an XML format with structured body could be delivered by publishers for only around 10% of the publications. Semantic Scholar Open Research Corpus (S2ORC), a corpus of 380.5 million citations, including citation mentions excerpts from the full-text body .</p>
<p>In addition to PDFs, some scientific articles are available as XML files, such as the Journal Article Tag Suite ( JATS) format. Formatting articles in PDF and XML has become standard practice for most mainstream publishers. While structured XML can solve many issues that need to be addressed with PDFs, XML full texts appear in a variety of different native publisher XML formats, often incomplete and inconsistent from one to another, loosely constrained, and evolving over time into specific versions.</p>
<p>To standardize the variety of XML formats we receive into a common format, we rely upon the open-source tool Pub2TEI (Lopez, 2020b). Pub2TEI converts various XML styles from publishers to the same standard TEI format as the one produced by GROBID. This centralizes our document processing across PDF and XML sources.</p>
<p>Matching References Against the Bibliographic Database Crossref</p>
<p>Once we have identified and matched the in-text citation to an item in a paper's reference list, this information must be validated. We use an open-source tool, biblio-glutton (Lopez, 2020c), which takes a raw bibliographical reference, as well as optionally parsed fields (title, author names, etc.) and matches it against the Crossref database-widely regarded as the industry standard source of ground truth for scholarly publications 3 . The matching accuracy of a raw citation reaches an F-score of 95.4 on a set of 17,015 raw references associated with a DOI, extracted from a data set of 1,943 PMC articles 4 compiled by Constantin (2014). In an end-toend perspective, still based on an evaluation with the corpus of 1,943 PMC articles, combining GROBID PDF extraction of citations and bibliographical references with biblio-glutton validations, the pipeline successfully associates around 70% of citation contexts to cited papers with correctly identified DOIs in a given PDF file. When the full-text XML version of an article is available from a publisher, references and linked citation contexts are normally correctly encoded, and the proportion of fully solved citation contexts corresponding to the proportion of cited paper with correctly identified DOIs is around 95% for PMC XML JATS files. The scite platform today only ingests publications with a DOI and only matches references against bibliographical objects with a registered DOI. The given evaluation figures have been calculated relative to these types of citations.</p>
<p>Task Modeling and Training Data</p>
<p>Extracted citation statements are classified into supporting, contrasting, or mentioning, to identify studies that have tested the claim and to evaluate how a scientific claim has been evaluated in the literature by subsequent research.</p>
<p>We emphasize that scite is not doing sentiment analysis. In natural language processing, sentiment analysis is the study of affective and subjective statements. The most common affective state considered in sentiment analysis is a mere polar view from positive sentiment to negative sentiment, which appeared to be particularly useful in business applications (e.g., product reviews and movie reviews). Following this approach, a subjective polarity can be associated with a citation to try to capture an opinion about the cited paper. The evidence used for sentiment classification relies on the presence of affective words in the citation context, with an associated polarity score capturing the strength of the affective state (Athar, 2014;Halevi &amp; Schimming, 2018;Hassan, Imran et al., 2018;Yousif et al., 2019). Yan et al. (2020), for instance, use a generic method called SenticNet to identify sentiments in citation contexts extracted from PubMed Central XML files, without particular customization to the scientific domain (only a preprocessing to remove the technical terms from the citation contexts is applied). SenticNet uses a polarity measure associated with 200,000 natural language concepts, propagated to the words and multiword terms realizing these concepts.</p>
<p>In contrast, scite focuses on the authors' reasons for citing a paper. We use a discrete classification into three discursive functions relative to the scientific debate; see Murray, Lamers et al. (2019) for an example of previous work with typing citations based on rhetorical intention. We consider that for capturing the reliability of a claim, a classification decision into supporting or contrasting must be backed by scientific arguments. The evidence involved in our assessment of citation intent is directed to the factual information presented in the citation context, usually statements about experimental facts and reproducibility results or presentation of a theoretical argument against or agreeing with the cited paper.</p>
<p>Examples of supporting, contrasting, and mentioning citation statements are given in Table 1, with explanations describing why they are classified as such, including examples where researchers have expressed confusion or disagreement with our classification.</p>
<p>Importantly, just as it is critical to optimize for accuracy of our deep learning model when classifying citations, it is equally important to make sure that the right terminology is used and understood by researchers. We have undergone multiple iterations of the design and display of citation statements and even the words used to define our citation types, including using previous words such as refuting and disputing to describe contrasting citations and confirming to describe supporting citations. The reasons for these changes reflect user feedback expressing confusion over certain terms as well as our intent to limit any potentially inflammatory interpretations. Indeed, our aim with introducing these citation types is to highlight differences in research findings based on evidence, not opinion. The main challenge of this classification task is the highly imbalanced distribution of the three classes. Based on manual annotations of different publication domains and sources, we estimate the average distribution of citation statements as 92.6% mentioning, 6.5% supporting, and 0.8% contrasting statements. Obviously, the less frequent the class, the more valuable it is. Most of the efforts in the development of our automatic classification system have been directed to address this imbalanced distribution. This task has required first the creation of original training data by experts-scientists with experience in reading and interpreting scholarly papers. Focusing on data quality, the expert classification was realized by multiple-blind manual annotation (at least two annotators working in parallel on the same citation), followed by a reconciliation step where the disagreements were further discussed and analyzed by the annotators. To keep track of the progress of our automatic classification over time, we created a holdout set of 9,708 classified citation records. To maintain a class distribution as close as possible to the actual distribution in current scholarly publications, we extracted the citation contexts from Open Access PDF of Unpaywall by random sampling with a maximum of one context per document.</p>
<p>We separately developed a working set where we tried to oversample the two less frequent classes (supporting, contrasting) with the objective of addressing the difficulties implied by the imbalanced automatic classification. We exploited the classification scores of our existing classifiers to select more likely supporting and contrasting statements for manual classification. At the present time, this set contains 38,925 classified citation records. The automatic classification system was trained with this working set, and continuously evaluated with the Table 1. Real-world examples of citation statement classifications with examples explaining why a citation type has or has not been assigned. Citation classifications are based on the following two requirements: there needs to be a written indication that the statement supports or contrasts the cited paper; and there needs to be an indication that it provides evidence for this assertion.</p>
<p>Citation statement</p>
<p>Classification Explanation "In agreement with previous work (Nicholson et al., 2015), the trisomic clones showed similar aberrations, albeit to a lesser extent (Supplemental Figure S2B)."</p>
<p>Supporting "In agreement with previous work" indicates support, while "the trisomic clones showed similar aberrations, albeit to a lesser degree (Supplemental Figure S2B)" provides evidence for this supporting statement.</p>
<p>"In contrast to several studies in anxious adults that examined amygdala activation to angry faces when awareness was not restricted (Phan, Fitzgerald, Nathan, &amp; Tancer, 2006;Stein, Goldin, Sareen, Zorrilla, &amp; Brown, 2002;Stein, Simmons, Feinstein, &amp; Paulus, 2007), we found no group differences in amygdala activation."</p>
<p>Contrasting "In contrast to several studies" indicates a contrast between the study and studies cited, while "we found no group differences in amygdala activation" indicates a difference in findings.</p>
<p>"The amygdala is a key structure within a complex circuit devoted to emotional interpretation, evaluation and response (Stein et al., 2002;Phan et al., 2006)."</p>
<p>Mentioning</p>
<p>This citation statement refers to Phan et al. (2006) without providing evidence that supports or contrasts the claims made in the cited study.</p>
<p>"In social cognition, the amygdala plays a central role in social reward anticipation and processing of ambiguity [87]. Consistent with these findings, amygdala involvement has been outlined as central in the pathophysiology of social anxiety disorders [27], [88]."</p>
<p>Mentioning</p>
<p>Here, the statement "consistent with these findings" sounds supportive, but, in fact, cites two previous studies: [87] and [27] without providing evidence for either. Such cites can be valuable, as they establish connections between observations made by others, but they do not provide primary evidence to support or contrast the cited studies. Hence, this citation statement is classified as mentioning.</p>
<p>"For example, a now-discredited article purporting a link between vaccination and autism (Wakefield et al., 1998) helped to dissuade many parents from obtaining vaccination for their children."</p>
<p>Mentioning</p>
<p>This citation statement describes the cited paper critically and with negative sentiment but there is no indication that it presents primary contrasting evidence, thus this statement is classified as mentioning.</p>
<p>Quantitative Science Studies</p>
<p>immutable holdout set to avoid as much bias as possible. An n-fold cross-evaluation on the working set, for instance, would have been misleading because the distribution of the classes in this set was artificially modified to boost the classification accuracy of the less frequent classes.</p>
<p>Before reconciliation, the observed average interannotator agreement percentage was 78.5% in the open domain and close to 90% for batches in biomedicine. It is unclear what accounts for the difference. Reconciliation, further completed with expert review by core team members, resulted in highly consensual classification decisions, which contrast with typical multiround disagreement rates observed with sentiment classification. Athar (2014), for instance, reports Cohen's k annotator agreement of 0.675 and Ciancarini, Di Iorio et al. (2014) report k = 0.13 and k = 0.15 for the property groups covering confirm/supports and critiques citation classification labels. A custom open source document annotation web application, docanno (Nakayama, Kubo et al., 2018) was deployed to support the first round of annotations.</p>
<p>Overall, the creation of our current training and evaluation holdout data sets has been a major 2-year effort involving up to eight expert annotators and nearly 50,000 classified citation records. In addition to the class, each record includes the citation sentence, the full "snippet" (citation sentence plus previous and next sentences), the source and target DOI, the reference callout string, and the hierarchical list of section titles where the citation occurs.</p>
<p>Machine Learning Classifiers</p>
<p>Although deep learning text classifiers show very strong and stable results on imbalanced classification tasks compared with linear classifiers (Nizzoli, Avvenuti et al., 2019), our first experiments with an early training data set based on PLOS articles resulted in F-scores of 96.3% for mentioning citations, 55.3% for supporting, and 20.5% for contrasting. The initial accuracy for contrasting in particular raised concerns about the feasibility of the task itself at scale. We focused on multiple approaches to increase over time the accuracy of classifier for the two less frequent classes:</p>
<p>• Improving the classification architecture: After initial experiments with RNN (Recursive Neural Network) architectures such as BidGRU (Bidirectional Gated Recurrent Unit, an architecture similar to the approach of Cohan et al. (2019) for citation intent classification), we obtained significant improvements with the more recently introduced ELMo (Embeddings from Language Models) dynamic embeddings (Peters, Neumann et al., 2018) and an ensemble approach. Although the first experiments with BERT (Bidirectional Encoder Representations from Transformers) (Devlin, Chang et al., 2019), a breakthrough architecture for NLP, were disappointing, fine-tuning SciBERT (a science-pretrained base BERT model) (Beltagy, Lo, &amp; Cohan, 2019) led to the best results and is the current production architecture of the platform. • Using oversampling and class weighting techniques: It is known that the techniques developed to address imbalanced classification in traditional machine learning can be applied successfully to deep learning too (Johnson &amp; Khoshgoftaar, 2019). We introduced in our system oversampling of less frequent classes, class weighting, and metaclassification with three binary classifiers. These techniques provide some improvements, but they rely on empirical parameters that must be re-evaluated as the training data changes. • Extending the training data for less frequent classes: As mentioned previously, we use an active learning approach to select the likely less frequent citation classes based on the scores of the existing classifiers. By focusing on edge cases over months of manual annotations, we observed significant improvements in performance for predicting contrasting and supporting cases.</p>
<p>Because deep learning today is mostly an empirical effort, the improvements using the above-described techniques were driven experimentally and iteratively until reaching a plateau. Table 2 presents the model evaluation after iterations of the classification system over time using our fixed holdout set. Table 3 presents the evaluation metrics for the current SciBERT model. Reported scores are averaged over 10 runs. The F-score for the classification of "contrasting" was notably improved from 20.1% to 58.97%. The precision for predicting "contrasting" citations" in particular reaches 85.19%, a very reliable level for such a rare class.</p>
<p>Given the unique nature of scite, there are a number of additional considerations. First, scaling is a key requirement of scite, which addresses the full corpus of scientific literature. While providing good results, the prediction with the ELMo approach is 20 times slower than with SciBERT, making it less attractive for our platform. Second, we have experimented with using section titles to improve classifications-for example, one might expect to find supporting and contrasting statements more often in the Results section of a paper and mentioning statements in the Introduction. Counterintuitively, including section titles in our model had no impact on F-scores, although it did slightly improve precision. It is unclear why including section titles failed to improve F-scores. However, it might relate to the challenge of correctly identifying and normalizing section titles from documents. Third, segmenting scientific text into sentences presents unique challenges due to the prevalence of abbreviations, nomenclatures, and Note: When deploying classification models in production, we balance the precision/recall so that all the classes have a precision higher than 80%. mathematical equations. Finally, we experimented with various context windows (i.e., the amount of text used in the classification of a citation) but were only able to improve the F-score for the contrasting category by eight points by manually selecting the most relevant phrases in the context window. Automating this process might improve classifications, but doing so presents a significant technical challenge. Other possible improvements of the classifier include multitask training, refinement of classes, increase of training data via improved active learning techniques, and integration of categorical features in the transformer classifier architecture.</p>
<p>We believe that the specificity of our evidence-based citation classes, the size and the focus on the quality of our manually annotated data set (multiple rounds of blind annotations with final collective reconciliation), the customization and continuous improvement of a state of the art deep learning classifier, and finally the scale of our citation analysis distinguishes our work from existing developments in automatic citation analysis.</p>
<p>Citation Statement and Classification Pipeline</p>
<p>TEI XML data is parsed in Python using the BeautifulSoup library and further segmented into sentences using a combination of spaCy (Honnibal, Montani et al., 2018) and Natural Language Toolkit's Punkt Sentence Tokenizer (Bird, Klein, &amp; Loper, 2009). These sentence segmentation candidates are then postprocessed with custom rules to better fit scientific texts, existing text structures, and inline markups. For instance, a sentence split is forbidden inside a reference callout, around common abbreviations not supported by the general-purpose sentence segmenters, or if it is conflicting with a list item, paragraph, or section break.</p>
<p>The implementation of the classifier is realized by a component we have named Veracity, which provides a custom set of deep learning classifiers built on top of the open source DeLFT library (Lopez, 2020d). Veracity is written in Python and employs Keras and TensorFlow for text classification. It runs on a single server with an NVIDIA GP102 (GeForce GTX 1080 Ti) graphics card with 3,584 CUDA cores. This single machine is capable of classifying all citation statements as they are processed. Veracity retrieves batches of text from the scite database that have yet to be classified, processes them, and updates the database with the results. When deploying classification models in production, we balance the precision/recall so that all the classes have a precision higher than 80%. For this purpose, we use the holdout data set to adjust the class weights at the prediction level. After evaluation, we can exploit all available labeled data to maximize the quality, and the holdout set captures a real-world distribution adapted to this final tuning.</p>
<p>User Interface</p>
<p>The resulting classified citations are stored and made available on the scite platform. Data from scite can be accessed in a number of ways (downloads of citations to a particular paper; the scite API, etc.). However, users will most commonly access scite through its web interface. Scite provides a number of core features, detailed below.</p>
<p>The scite report page (Figure 1) displays summary information about a given paper. All citations in the scite database to the paper are displayed, and users can filter results by classification (supporting, mentioning, contrasting), paper section (e.g., Introduction, Results), and the type of citing article (e.g., preprint, book, etc.). Users can also search for text within citation statements and surrounding citation context. For example, if a user wishes to examine how an article has been cited with respect to a given concept (e.g., fear), they can search for citation contexts that contain that key term. Each citation statement is accompanied by a classification label, as well as an indication of how confident the model is of said classification. For example, a citation statement may be classified as supporting with 90% confidence, meaning that the model is 90% certain that the statement supports the target citation. Finally, each citation statement can be flagged by individual users as incorrect, so that users can report a classification as incorrect, as well as justify their objection. After a citation statement has been flagged as incorrect, it will be reviewed and verified by two independent reviewers, and, if both agree, the recommended change will be implemented. In this way, scite supplements machine learning with human interventions to ensure that citations are accurately classified. This is an important feature of scite that allows researchers to interact with the automated citation types, correcting classifications that might otherwise be difficult for a machine to classify. It also opens the possibility for authors and readers to add more nuance to citation typing by allowing them to annotate snippets.</p>
<p>To improve the utility and usability of the smart citation data, scite offers a wide variety of tools common to other citation platforms, such as Scopus and Web of Science and other information retrieval software. These include literature searching functionality for researchers to find supported and contrasted research, visualizations to see research in context, reference checking for automatically evaluating references with scite's data on an uploaded manuscript and more. Scite also offers plugins for popular web browsers and reference management software (e.g., Zotero) that allow easy access to scite reports and data in native research environments.</p>
<p>DISCUSSION</p>
<p>Research Applications</p>
<p>A number of researchers have already made use of scite for quantitative assessments of the literature. For example, Bordignon (2020) examined self-correction in the scientific record and operationalized "negative" citations as those that scite classified as contrasting. They found that negative citations are rare, even among works that have been retracted. In another example from our own group, Nicholson et al. (2020) examined scientific papers cited in Wikipedia articles and found that-like the scientific literature as a whole-the vast majority presented findings that have not been subsequently verified. Similar analyses could also be applied to articles in the popular press. One can imagine a number of additional metascientific applications. For example, network analyses with directed graphs, valenced edges (by type of citation-supporting, contrasting, and mentioning), and individual papers as nodes could aid in understanding how various fields and subfields are related. A simplified form of this analysis is already implemented on the scite website (see Figure 3), but more complicated analyses that assess traditional network indices, such as centrality and clustering, could be easily implemented using standard software libraries and exports of data using the scite API.</p>
<p>Implications for Scholarly Publishers</p>
<p>There are a number of implications for scholarly publishers. At a very basic level, this is evident in the features that scite provides that are of particular use to publishers. For example, the scite Reference Check parses the reference list of an uploaded document and produces a report indicating how items in the list have been cited, flagging those that have been retracted or have otherwise been the subject of editorial concern. This type of screening can help publishers and editors ensure that articles appearing in their journals do not inadvertently cite discredited works. Evidence in scite's own database indicates that this would solve a seemingly significant problem, as in 2019 alone nearly 6,000 published papers cited works that had been retracted prior to 2019. Given that over 95% of citations made to retracted articles are in error (Schneider, Ye et al., 2020), had the Reference Check tool been applied to these papers during the review process, the majority of these mistakes could have been caught.</p>
<p>However, there are additional implications for scholarly publishing that go beyond the features provided by scite. We believe that by providing insights into how articles are cited-rather than simply noting that the citation has occurred-scite can alter the way in which journals, institutions, and publishers are assessed. Scite provides journals and institutions with dashboards that indicate the extent to which papers with which they are associated have been supported or contrasted by subsequent research (Figure 4). Even without reliance on specific metrics, the approach that scite provides prompts the question: What if we normalized the assessment of journals, institutions and researchers in terms of how they were cited rather than the simple fact that they were cited alone?</p>
<p>Implications for Researchers</p>
<p>Given the fact that nearly 3 million scientific papers are published every year (Ware &amp; Mabe, 2015), researchers increasingly report feeling overwhelmed by the amount of literature they must sift through as part of their regular workflow (Landhuis, 2016). Scite can help by assisting researchers in identifying relevant, reliable work that is narrowly tailored to their interests, as well as better understanding how a given paper fits into the broader context of the scientific literature. For example, one common technique for orienting oneself to new literature is to seek out the most highly cited papers in that area. If the context of those citations is also visible, the value of a given paper can be more completely assessed and understood. There are, however, additional-although perhaps less obvious-implications. If citation types are easily visible, it is possible that researchers will be incentivized to make replication attempts easier (for example, by providing more explicit descriptions of methods or instruments) in the hope that their work will be replicated.</p>
<p>Limitations</p>
<p>At present, the biggest limitation for researchers using scite is the size of the database. At the time of this writing, scite has ingested over 880 million separate citation statements from over 25 million scholarly publications. However, there are over 70 million scientific publications in existence (Ware &amp; Mabe, 2015); scite is constantly ingesting new papers from established sources and signing new licensing agreements with publishers, so this limitation should abate over time. However, given that the ingestion pipeline fails to identify approximately 30% of citation statements/references in PDF files (~5% in XML), the platform will necessarily contain fewer references than services such as Google Scholar and Web of Science, which do not rely on ingesting the full text of papers. Even if references are reliably extracted and matched with a DOI or directly provided by publishers, a reference is currently only visible on the scite platform if it is matched with at least one citation context in the body of the article. As such, the data provided by scite will necessarily miss a measurable percentage of citations to a given paper. We are working to address these limitations in two ways: First, we are working toward ingesting more full-text XML and improving our ability to detect document structure in PDFs. Second, we have recently supplemented our Smart Citation data with "traditional" citation metadata provided by Crossref (see "Without Citation Statements" shown in Figure 1), which surfaces references that we would otherwise miss. Indeed, this Crossref data now includes references from publishers with previously closed references such as Elsevier and the American Chemical Society. These traditional citations can later be augmented to include citation contexts as we gain access to full text.</p>
<p>Another limitation is related to the classification of citations. First, as noted previously, the Veracity software does not perfectly classify citations. This can partly be explained by the fact that language in the (biomedical) sciences is little standardized (unlike law, where shepardizing is a standing term describing the "process of using a citator to discover the history of a case or statute to determine whether it is still good law"; see Lehman &amp; Phelps, 2005). However, the accuracy of the classifier will likely increase over time as technology improves and the training data set increases in size. Second, the ontology currently employed by scite (supporting, mentioning, and contrasting) necessarily misses some nuance regarding how references are cited in scientific papers. One key example relates to what "counts" as a contrasting citation: At present, this category is limited to instances where new evidence is presented (e.g., a failed replication attempt or a difference in findings). However, it might also be appropriate to include conceptual and logical arguments against a given paper in this category. Moreover, in our system, the evidence behind the supporting or contrasting citation statements is not being assessed; thus a supporting citation statement might come from a paper where the experimental evidence is weak and vice versa. We do display the citation tallies that papers have received so that users can assess this but it would be exceedingly difficult to also classify the sample size, statistics, and other parameters that define how robust a finding is.</p>
<p>CONCLUSIONS</p>
<p>The automated extraction and analysis of scientific citations is a technically challenging task, but one whose time has come. By surfacing the context of citations rather than relying on their mere existence as an indication of a paper's importance and impact, scite provides a novel approach to addressing pressing questions for the scientific community, including incentivizing replicable works, assessing an increasingly large body of literature, and quantitatively studying entire scientific fields.</p>
<p>DATA AVAILABILITY</p>
<p>Code used in the ingestion of manuscripts is available at https://github.com/kermitt2/grobid, https://github.com/kermitt2/biblio-glutton, and https://github.com/kermitt2/Pub2TEI. The classification of citation statements is performed by a modified version of DeLFT (https://github .com/kermitt2/delft). The training data used by the scite classifier is proprietary and not publicly available. The 880+ million citation statements are available at scite.ai but cannot be shared in full due to licensing arrangements made with publishers.</p>
<p>Figure 3 .
3A citation network representation using the scite Visualization tool. The nodes represent individual papers, with the edges representing supporting (green) or contrasting (blue) citation statements. The graph is interactive and can be expanded and modified for other layouts. The interactive visualization can be accessed at the following link: https://scite.ai/visualizations/global-analysis-of-genome -transcriptome-9L4dJr?dois%5B0%5D=10.1038%2Fmsb.2012.40&amp;dois%5B1%5D=10.7554%2Felife.05068&amp;focusedElement=10.7554 %2Felife.05068.</p>
<p>Figure 4 .
4A scite Journal Dashboard showing the aggregate citation information at the journal level, including editorial notices and the scite Index, a journal metric that shows the ratio of supporting citations over supporting plus contrasting citations. Access to the journal dashboard in the figure and other journal dashboards is available here: https://scite.ai/journals/0138-9130.</p>
<p>Table 2 .
2Progress on classification results over approximately 1 year, evaluated on a fixed holdout set of 9,708 examples. In parallel with these various iterations on the classification algorithms, the training data was raised from 30,665 (initial evaluation with BidGRU) to 38,925 examples (last evaluation with SciBERT) via an active learning approach.Approach </p>
<p>F-score </p>
<p>Contrasting 
Supporting 
Mentioning 
BidGRU 
.206 
.554 
.964 </p>
<p>BidGRU + metaclassifier 
.260 
.590 
.964 </p>
<p>BidGRU + ELMo 
.405 
.590 
.969 </p>
<p>BidGRU + ELMo + ensemble (10 classifiers) 
.460 
.605 
.972 </p>
<p>SciBERT 
.590 
.648 
.973 </p>
<p>Observed distribution 
0.8% 
6.5% 
92.6% </p>
<p>scite: A smart citation index
For more information on the history and prevalence of Crossref, see https://www.crossref.org/about/. 4 The evaluation data and scripts are available on the project GitHub repository; see biblio-glutton(Lopez, 2020c).Quantitative Science Studies
ACKNOWLEDGMENTSWe would like to thank Yuri Lazebnik for his help in conceptualizing and building scite.COMPETING INTERESTSThe authors are shareholders and/or consultants or employees of Scite Inc.
Sentiment analysis of citations using sentence structure-based features. A Athar, Proceedings of the ACL 2011 Student Session. the ACL 2011 Student SessionAthar, A. (2011). Sentiment analysis of citations using sentence structure-based features. Proceedings of the ACL 2011 Student Session, 81-87. Retrieved from https://www.aclweb.org /anthology/P11-3015</p>
<p>Sentiment analysis of scientific citations. A Athar, University of Cambridge, Computer LaboratoryUCAM-CL-TR-856Athar, A. (2014). Sentiment analysis of scientific citations. Technical Report (UCAM-CL-TR-856), University of Cambridge, Computer Laboratory. Retrieved from https://www.cl.cam.ac.uk/techreports /UCAM-CL-TR-856.pdf</p>
<p>SciBERT: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, 10.18653/v1/D19-1371ArXiv:1903.10676Beltagy, I., Lo, K., &amp; Cohan, A. (2019). SciBERT: A pretrained lan- guage model for scientific text. ArXiv:1903.10676 [Cs]. https:// arxiv.org/abs/1903.10676. https://doi.org/10.18653/v1/ D19 -1371</p>
<p>S Bird, E Klein, E Loper, Natural language processing with Python. O'Reilly1st edBird, S., Klein, E., &amp; Loper, E. (2009). Natural language processing with Python (1st ed). O'Reilly.</p>
<p>Self-correction of science: A comparative study of negative citations and post-publication peer review. F Bordignon, 10.1007/s11192-020-03536-zScientometrics. 1242Bordignon, F. (2020). Self-correction of science: A comparative study of negative citations and post-publication peer review. Scientometrics, 124(2), 1225-1239. https://doi.org/10.1007 /s11192-020-03536-z</p>
<p>Evaluating citation functions in CiTO: Cognitive issues. P Ciancarini, A Di Iorio, A G Nuzzolese, S Peroni, F Vitali, 10.1007/978-3-319-07443-6_39The Semantic Web: Trends and Challenges. V. Presutti, C. d'Amato, F. Gandon, M. d'Aquin, S. Staab, &amp; A. TordaiSpringer International Publishing8465Ciancarini, P., Di Iorio, A., Nuzzolese, A. G., Peroni, S., &amp; Vitali, F. (2014). Evaluating citation functions in CiTO: Cognitive issues. In V. Presutti, C. d'Amato, F. Gandon, M. d'Aquin, S. Staab, &amp; A. Tordai (Eds.), The Semantic Web: Trends and Challenges (Vol. 8465, pp. 580-594). Springer International Publishing. https://doi.org/10.1007/978-3-319-07443-6_39</p>
<p>Structural scaffolds for citation intent classification in scientific publications. A Cohan, W Ammar, M Van Zuylen, F Cady, 10.18653/v1/N19-1361Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. the 2019 Conference of the North American Chapter of the Association for Computational LinguisticsCohan, A., Ammar, W., van Zuylen, M., &amp; Cady, F. (2019). Structural scaffolds for citation intent classification in scientific publications. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1361</p>
<p>Automatic structure and keyphrase analysis of scientific publications. A Constantin, University of ManchesterConstantin, A. (2014). Automatic structure and keyphrase analysis of scientific publications. University of Manchester. https://www .research.manchester.ac.uk/portal/files/54553913/FULL_TEXT.PDF</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. the 2019 Conference of the North American Chapter of the Association for Computational LinguisticsDevlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423</p>
<p>How Unpaywall is transforming open science. H Else, 10.1038/d41586-018-05968-3Nature. 560771830111793Else, H. (2018). How Unpaywall is transforming open science. Nature, 560(7718), 290-291. https://doi.org/10.1038/d41586 -018-05968-3, PubMed: 30111793</p>
<p>Colil: A database and search service for citation contexts in the life sciences domain. T Fujiwara, Y Yamamoto, 10.1186/s13326-015-0037-xJournal of Biomedical Semantics. 6126500753Fujiwara, T., &amp; Yamamoto, Y. (2015). Colil: A database and search service for citation contexts in the life sciences domain. Journal of Biomedical Semantics, 6(1), 38. https://doi.org/10.1186 /s13326-015-0037-x, PubMed: 26500753</p>
<p>Citation indexes for science: A new dimension in documentation through association of ideas. E Garfield, 10.1126/science.122.3159.108Science. 122315914385826Garfield, E. (1955). Citation indexes for science: A new dimension in documentation through association of ideas. Science, 122(3159), 108-111. https://doi.org/10.1126/science.122.3159.108, PubMed: 14385826</p>
<p>Letter to Dr. E Garfield, Joshua Lederberg, Stanford UniversityGarfield, E. (1959). Letter to Dr. Joshua Lederberg, Stanford University. Retrieved from https://www.garfield.library.upenn .edu/lederberg/052159.html</p>
<p>Statistical association methods for mechanized documentation, symposium proceedings. E Garfield, National Bureau of Standards. M. E. Stevens, V. E. Giuliano, &amp; L. B. HeilprinCan Citation Indexing beGarfield, E. (1964). Can Citation Indexing be Automated? Reprinted from M. E. Stevens, V. E. Giuliano, &amp; L. B. Heilprin (Eds.), Statistical association methods for mechanized documentation, symposium proceedings, Washington 1964 (pp. 189-192). National Bureau of Standards. Retrieved from https://www .garfield.library.upenn.edu/essays/ V1p084y1962-73.pdf</p>
<p>Citation analysis as a tool in journal evaluation: Journals can be ranked by frequency and impact of citations for science policy studies. E Garfield, 10.1126/science.178.4060.471Science. 17840605079701Garfield, E. (1972). Citation analysis as a tool in journal evaluation: Journals can be ranked by frequency and impact of citations for science policy studies. Science, 178(4060), 471-479. https://doi .org/10.1126/science.178.4060.471, PubMed: 5079701</p>
<p>How citation distortions create unfounded authority: Analysis of a citation network. S A Greenberg, 10.1136/bmj.b2680BMJ. 33919622839Greenberg, S. A. (2009). How citation distortions create unfounded authority: Analysis of a citation network. BMJ, 339, b2680. https://doi.org/10.1136/bmj.b2680, PubMed: 19622839</p>
<p>An initiative to track sentiments in altmetrics. G Halevi, L Schimming, 10.29024/joa.1Journal of Altmetrics. 11Halevi, G., &amp; Schimming, L. (2018). An initiative to track senti- ments in altmetrics. Journal of Altmetrics, 1(1), 2. https://doi.org /10.29024/joa.1</p>
<p>Deep context of citations using machine-learning models in scholarly full-text articles. S U Hassan, M Imran, S Iqbal, N R Aljohani, R Nawaz, 10.1007/s11192-018-2944-yScientometrics. 1173Hassan, S. U., Imran, M., Iqbal, S., Aljohani, N. R., &amp; Nawaz, R. (2018). Deep context of citations using machine-learning models in scholarly full-text articles. Scientometrics, 117(3), 1645-1662. https://doi.org/10.1007/s11192-018-2944-y</p>
<p>An index to quantify an individual's scientific research output. J E Hirsch, 10.1073/pnas.0507655102Proceedings of the National Academy of Sciences. 1024616275915Hirsch, J. E. (2005). An index to quantify an individual's scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569-16572. https://doi.org/10.1073/pnas .0507655102, PubMed: 16275915</p>
<p>Explosion/paCy: V2.0.11: Alpha Vietnamese support, fixes to vectors, improved errors and more. M Honnibal, I Montani, M Honnibal, H Peters, M Samsonov, A Patel, 10.5281/ZENODO.1212304Honnibal, M., Montani, I., Honnibal, M., Peters, H., Samsonov, M., … Patel, A. (2018). Explosion/paCy: V2.0.11: Alpha Vietnamese support, fixes to vectors, improved errors and more. Zenodo. https://doi.org/10.5281/ZENODO.1212304</p>
<p>Survey on deep learning with class imbalance. J M Johnson, T M Khoshgoftaar, 10.1186/s40537-019-0192-5Journal of Big Data. 61Initiative for Open CitationsInitiative for Open Citations. (2017). https://i4oc.org/ Johnson, J. M., &amp; Khoshgoftaar, T. M. (2019). Survey on deep learning with class imbalance. Journal of Big Data, 6(1), 27. https://doi.org /10.1186/s40537-019-0192-5</p>
<p>Scientific literature: Information overload. E Landhuis, 10.1038/nj7612-457aNature. 535761227453968Landhuis, E. (2016). Scientific literature: Information overload. Nature, 535(7612), 457-458. https://doi.org/10.1038/nj7612 -457a, PubMed: 27453968</p>
<p>Shepardizing. In West's encyclopedia of American law. J Lehman, S Phelps, Thomson/ Gale9162Detroit2nd ed.Lehman, J., &amp; Phelps, S. (2005). Shepardizing. In West's encyclope- dia of American law (2nd ed., vol. 9, p. 162). Detroit: Thomson/ Gale.</p>
<p>Affirmative citation bias in scientific myth debunking: A three-in-one case study. K Letrud, S Hernes, 10.1371/journal.pone.0222213PLOS ONE. 14931498834Letrud, K., &amp; Hernes, S. (2019). Affirmative citation bias in scientific myth debunking: A three-in-one case study. PLOS ONE, 14(9), e0222213. https://doi.org/10.1371/journal.pone.0222213, PubMed: 31498834</p>
<p>A 1980 letter on the risk of opioid addiction. P T M Leung, E M Macdonald, M B Stanbrook, I A Dhalla, D N Juurlink, 10.1056/NEJMc1700150New England Journal of Medicine. 37622Leung, P. T. M., Macdonald, E. M., Stanbrook, M. B., Dhalla, I. A., &amp; Juurlink, D. N. (2017). A 1980 letter on the risk of opioid addiction. New England Journal of Medicine, 376(22), 2194-2195. https://doi.org/10.1056/NEJMc1700150</p>
<p>K Lo, L L Wang, M Neumann, R Kinney, D S Weld, ArXiv:1911.02782S2ORC: The Semantic Scholar Open Research Corpus. Lo, K., Wang, L. L., Neumann, M., Kinney, R., &amp; Weld, D. S. (2020). S2ORC: The Semantic Scholar Open Research Corpus. ArXiv:1911.02782 [Cs]. https://arxiv.org/abs/1911.02782</p>
<p>GROBID [source code. P Lopez, Lopez, P. (2020a). GROBID [source code]. Retrieved from https:// github.com/kermitt2/grobid</p>
<p>. P Lopez, Pub2TEI [source codeLopez, P. (2020b). Pub2TEI [source code]. Retrieved from https:// github.com/kermitt2/Pub2TEI</p>
<p>biblio-glutton. P Lopez, source codeLopez, P. (2020c). biblio-glutton [source code]. Retrieved from https://github.com/kermitt2/biblio-glutton</p>
<p>. P Lopez, delft [source codeLopez, P. (2020d). delft [source code]. Retrieved from https://github .com/kermitt2/delft</p>
<p>Measuring disagreement in science. D Murray, W Lamers, K Boyack, V Larivière, C R Sugimoto, 17th International Conference on Scientometrics &amp; Informetrics. Murray, D., Lamers, W., Boyack, K., Larivière, V., &amp; Sugimoto, C. R. (2019). Measuring disagreement in science. 17th International Conference on Scientometrics &amp; Informetrics (pp. 2370-2375).</p>
<p>doccano: Text annotation tool for humans. H Nakayama, T Kubo, J Kamura, Y Taniguchi, X Liang, Nakayama, H., Kubo, T., Kamura, J., Taniguchi, Y., &amp; Liang, X. (2018). doccano: Text annotation tool for humans. https://github .com/doccano/doccano</p>
<p>Chromosome mis-segregation and cytokinesis failure in trisomic human cells. eLife, 4, e05068. J M Nicholson, J C Macedo, A J Mattingly, D Wangsa, J Camps, D Cimini, 10.7554/eLife.0506825942454Nicholson, J. M., Macedo, J. C., Mattingly, A. J., Wangsa, D., Camps, J., … Cimini, D. (2015). Chromosome mis-segregation and cytokinesis failure in trisomic human cells. eLife, 4, e05068. https://doi.org/10.7554/eLife.05068, PubMed: 25942454</p>
<p>Measuring the quality of scientific references in Wikipedia: An analysis of more than 115M citations to over 800 000 scientific articles. J M Nicholson, A Uppala, M Sieber, P Grabitz, M Mordaunt, S C Rife, 10.1111/febs.15608FEBS Journal. 28814Nicholson, J. M., Uppala, A., Sieber, M., Grabitz, P., Mordaunt, M., &amp; Rife, S. C. (2020). Measuring the quality of scientific references in Wikipedia: An analysis of more than 115M citations to over 800 000 scientific articles. FEBS Journal, 288(14), 4242-4248. https://doi.org/10.1111/febs.15608</p>
<p>Extremist propaganda tweet classification with deep learning in realistic scenarios. L Nizzoli, M Avvenuti, S Cresci, M Tesconi, 10.1145/3292522.3326050Proceedings of the 10th ACM Conference on Web Science -WebSci '19. the 10th ACM Conference on Web Science -WebSci '19Nizzoli, L., Avvenuti, M., Cresci, S., &amp; Tesconi, M. (2019). Extremist propaganda tweet classification with deep learning in realistic scenarios. Proceedings of the 10th ACM Conference on Web Science -WebSci '19 (pp. 203-204). https://doi.org/10 .1145/3292522.3326050</p>
<p>FaBiO and CiTO: Ontologies for describing bibliographic resources and citations. S Peroni, D Shotton, 10.1016/j.websem.2012.08.001Journal of Web Semantics. 17Peroni, S., &amp; Shotton, D. (2012). FaBiO and CiTO: Ontologies for describing bibliographic resources and citations. Journal of Web Semantics, 17, 33-43. https://doi.org/10.1016/j.websem.2012 .08.001</p>
<p>OpenCitations, an infrastructure organization for open scholarship. S Peroni, D Shotton, 10.1162/qss_a_00023Quantitative Science Studies. 11Peroni, S., &amp; Shotton, D. (2020). OpenCitations, an infrastructure organization for open scholarship. Quantitative Science Studies, 1(1), 428-444. https://doi.org/10.1162/qss_a_00023</p>
<p>Deep contextualized word representations. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, L Zettlemoyer, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers1Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., … Zettlemoyer, L. (2018). Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 2227-2237).</p>
<p>. 10.18653/v1/N18-1202Association for Computational LinguisticsAssociation for Computational Linguistics. https://doi.org/10.18653 /v1/N18-1202</p>
<p>Association between amygdala hyperactivity to harsh faces and severity of social anxiety in generalized social phobia. K L Phan, D A Fitzgerald, P J Nathan, M E Tancer, 10.1016/j.biopsych.2005.08.012Biological Psychiatry. 59516256956Phan, K. L., Fitzgerald, D. A., Nathan, P. J., &amp; Tancer, M. E. (2006). Association between amygdala hyperactivity to harsh faces and severity of social anxiety in generalized social phobia. Biological Psychiatry, 59(5), 424-429. https://doi.org/10.1016/j.biopsych .2005.08.012, PubMed: 16256956</p>
<p>The future of OA: A largescale analysis projecting Open Access publication and readership. H Piwowar, J Priem, R Orr, 10.1101/795310PreprintPiwowar, H., Priem, J., &amp; Orr, R. (2019). The future of OA: A large- scale analysis projecting Open Access publication and reader- ship [Preprint]. Scientific Communication and Education. https://doi.org/10.1101/795310</p>
<p>Rich_citations [source code. Plos, PLOS. (2015). Rich_citations [source code]. Retrieved from https:// github.com/PLOS/rich_citations</p>
<p>Addiction rare in patients treated with narcotics. J Porter, H Jick, 10.1056/NEJM198001103020221New England Journal of Medicine. 3022123Porter, J., &amp; Jick, H. (1980). Addiction rare in patients treated with narcotics. New England Journal of Medicine, 302(2), 123. https:// doi.org/10.1056/NEJM198001103020221</p>
<p>ACT: An annotation platform for citation typing at scale. D Pride, P Knoth, J Harag, 10.1109/JCDL.2019.00055ACM/IEEE Joint Conference on Digital Libraries ( JCDL). Pride, D., Knoth, P., &amp; Harag, J. (2019). ACT: An annotation platform for citation typing at scale. 2019 ACM/IEEE Joint Conference on Digital Libraries ( JCDL) (pp. 329-330). https://doi.org/10.1109 /JCDL.2019.00055</p>
<p>Continued post-retraction citation of a fraudulent clinical trial report, 11 years after it was retracted for falsifying data. J Schneider, D Ye, A M Hill, A S Whitehorn, 10.1007/s11192-020-03631-1Scientometrics. 1253Schneider, J., Ye, D., Hill, A. M., &amp; Whitehorn, A. S. (2020). Continued post-retraction citation of a fraudulent clinical trial report, 11 years after it was retracted for falsifying data. Scientometrics, 125(3), 2877-2913. https://doi.org/10.1007/s11192-020-03631-1</p>
<p>Increased amygdala activation to angry and contemptuous faces in generalized social phobia. M B Stein, P R Goldin, J Sareen, L T Zorrilla, G G Brown, 10.1001/archpsyc.59.11.1027Archives of General Psychiatry. 5912418936Stein, M. B., Goldin, P. R., Sareen, J., Zorrilla, L. T., &amp; Brown, G. G. (2002). Increased amygdala activation to angry and contemptu- ous faces in generalized social phobia. Archives of General Psychiatry, 59, 1027-1034. https://doi.org/10.1001/archpsyc.59 .11.1027, PubMed: 12418936</p>
<p>Increased amygdala and insula activation during emotion processing in anxiety-prone subjects. M B Stein, A N Simmons, J S Feinstein, M P Paulus, 10.1176/ajp.2007.164.2.318American Journal of Psychiatry. 1642Stein, M. B., Simmons, A. N., Feinstein, J. S., &amp; Paulus, M. P. (2007). Increased amygdala and insula activation during emotion pro- cessing in anxiety-prone subjects. American Journal of Psychiatry, 164(2), 318-327. https://doi.org/10.1176/ajp.2007.164.2.318</p>
<p>Assessment of citations of the retracted article by Wakefield et al with fraudulent claims of an association between vaccination and autism. E M Suelzer, J Deal, K L Hanus, B Ruggeri, R Sieracki, E Witkowski, 10.1001/jamanetworkopen.2019.15552JAMA Network Open. 21131730183Suelzer, E. M., Deal, J., Hanus, K. L., Ruggeri, B., Sieracki, R., &amp; Witkowski, E. (2019). Assessment of citations of the retracted article by Wakefield et al with fraudulent claims of an association between vaccination and autism. JAMA Network Open, 2(11), e1915552. https://doi.org/10.1001/jamanetworkopen.2019 .15552, PubMed: 31730183</p>
<p>Automatic classification of citation function. S Teufel, A Siddharthan, D Tidhar, 10.3115/1610075.1610091Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. the 2006 Conference on Empirical Methods in Natural Language ProcessingTeufel, S., Siddharthan, A., &amp; Tidhar, D. (2006). Automatic classifica- tion of citation function. Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (pp. 103-110). https://doi.org/10.3115/1610075.1610091</p>
<p>Machine learning vs. rules and out-of-the-box vs. retrained: An evaluation of open-source bibliographic reference and citation parsers. D Tkaczyk, A Collins, P Sheridan, J Beel, 10.1145/3197026.3197048ArXiv:1802.01168Tkaczyk, D., Collins, A., Sheridan, P., &amp; Beel, J. (2018). Machine learning vs. rules and out-of-the-box vs. retrained: An evaluation of open-source bibliographic reference and citation parsers. ArXiv:1802.01168 [Cs]. https://arxiv.org/abs/1802.01168. https://doi.org/10.1145/3197026.3197048</p>
<p>Quantitative proteomic and phosphoproteomic comparison of human colon cancer DLD-1 cells differing in ploidy and chromosome stability. C Viganó, C Schubert, E Ahrné, A Schmidt, T Lorber, E A Nigg, 10.1091/mbc.E17-10-0577Molecular Biology of the Cell. 29929496963Viganó, C., von Schubert, C., Ahrné, E., Schmidt, A., Lorber, T., … Nigg, E. A. (2018). Quantitative proteomic and phosphoproteo- mic comparison of human colon cancer DLD-1 cells differing in ploidy and chromosome stability. Molecular Biology of the Cell, 29(9), 1031-1047. https://doi.org/10.1091/mbc.E17-10-0577, PubMed: 29496963</p>
<p>SciRide Finder: A citation-based paradigm in biomedical literature search. A Volanakis, K Krawczyk, 10.1038/s41598-018-24571-0Scientific Reports. 8129670147Volanakis, A., &amp; Krawczyk, K. (2018). SciRide Finder: A citation-based paradigm in biomedical literature search. Scientific Reports, 8(1), 6193. https://doi.org/10.1038/s41598-018-24571-0, PubMed: 29670147</p>
<p>RETRACTED: Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children. A Wakefield, S Murch, A Anthony, J Linnell, D Casson, J Walker-Smith, 10.1016/S0140-6736(97)11096-01016/S0140-6736(97)11096-0The Lancet. 35191039500320Wakefield, A., Murch, S., Anthony, A., Linnell, J., Casson, D., … Walker-Smith, J. (1998). RETRACTED: Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children. The Lancet, 351(9103), 637-641. https:// doi.org/10.1016/S0140-6736(97)11096-0, PubMed: 9500320</p>
<p>CORD-19: The COVID-19 Open Research Dataset. L L Wang, K Lo, Y Chandrasekhar, R Reas, J Yang, D Burdick, S Kohlmeier, 10706Wang, L. L., Lo, K., Chandrasekhar, Y., Reas, R., Yang, J., Burdick, D., … Kohlmeier, S. (2020). CORD-19: The COVID-19 Open Research Dataset. ArXiv:2004.10706 [Cs]. https://arxiv.org/abs /2004.10706</p>
<p>The STM Report: An overview of scientific and scholarly journal publishing. M Ware, M Mabe, Technical and Medical Publishers181The Hague: International Association of ScientificWare, M., &amp; Mabe, M. (2015). The STM Report: An overview of scien- tific and scholarly journal publishing, p. 181. The Hague: International Association of Scientific, Technical and Medical Publishers.</p>
<p>The relationship between journal citation impact and citation sentiment: A study of 32 million citances in PubMed Central. E Yan, Z Chen, K Li, 10.1162/qss_a_00040Quantitative Science Studies. 12Yan, E., Chen, Z., &amp; Li, K. (2020). The relationship between journal citation impact and citation sentiment: A study of 32 million citances in PubMed Central. Quantitative Science Studies, 1(2), 664-674. https://doi.org/10.1162/qss_a_00040</p>
<p>A survey on sentiment analysis of scientific citations. A Yousif, Z Niu, J K Tarus, A Ahmad, 10.1007/s10462-017-9597-8Artificial Intelligence Review. 523Yousif, A., Niu, Z., Tarus, J. K., &amp; Ahmad, A. (2019). A survey on sen- timent analysis of scientific citations. Artificial Intelligence Review, 52(3), 1805-1838. https://doi.org/10.1007/s10462-017-9597-8</p>            </div>
        </div>

    </div>
</body>
</html>
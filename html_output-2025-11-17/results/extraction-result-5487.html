<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5487 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5487</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5487</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-088a5fa00ed6c14351209da5f53e770b51fd2909</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/088a5fa00ed6c14351209da5f53e770b51fd2909" target="_blank">FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> FANToM is introduced, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering that is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain- of-thought reasoning or fine-tuning.</p>
                <p><strong>Paper Abstract:</strong> Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5487.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5487.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Jun 0613) + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI, 0613 checkpoint, June) with zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 (decoder-only transformer) evaluated with a 'let's think step by step' chain-of-thought prompt on the FANToM conversational Theory-of-Mind benchmark; shown to be the strongest LLM in this paper but still far below humans on coherent ToM metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (0613, June) + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4, OpenAI's large decoder-only transformer model (proprietary), instruction-tuned; evaluated using zero-shot chain-of-thought prompting ('let's think step by step').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not reported in-paper)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A 10K-question benchmark of multiparty conversations with information asymmetry, converted into six ToM question types (BeliefQ [free and choice], FactQ, Answerability Q [List/Y/N], InfoAccess Q [List/Y/N]); measures whether models attribute beliefs/knowledge correctly for characters who missed parts of the conversation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All Question Types (short context, CoT): 26.6%; All Answerability Qs: 40.2%; All InfoAccess Qs: 57.7%; BeliefQ[CHOICE]: 80.6%; BeliefQ[DIST.]: 66.7%; FACTQ token F1 (comprehension): 44.0%; Answerability Q[Y/N]: 88.5%; InfoAccess Q[Y/N]: 92.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human (graduate CS students, same instructions): All Question Types: 87.5%; All Answerability Qs: 90.6%; All InfoAccess Qs: 90.6%; BeliefQ[CHOICE]: 93.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperforms humans substantially on coherent across-question metrics (All Question Types 26.6% vs human 87.5%). On some individual sub-tasks (e.g., BeliefQ[CHOICE], Answerability/InfoAccess Y/N) GPT-4 + CoT approaches or exceeds many models but still typically trails human accuracy; gap remains large on multi-question coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Best-performing LLM here but still shows 'illusory ToM' — high scores on some question formats (multiple-choice, binary) but fails to be consistent across the six question types for the same information. Benefits from CoT (improves scores) but still far from human-level coherent ToM; tends to rely on accessible omniscient information and word overlap when answering belief questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5487.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5487.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Oct 0613) + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI, 0613 checkpoint, October) with zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Later checkpoint of GPT-4 evaluated with chain-of-thought prompting on FANToM; shows notably lower performance than the June checkpoint on many FANToM metrics in this paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (0613, October) + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (October checkpoint), instruction-tuned transformer from OpenAI; evaluated with chain-of-thought prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not reported in-paper)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: multiparty conversational ToM QA across six question types requiring tracking of information accessibility for characters.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All Question Types (short context, CoT): 14.8%; All Answerability Qs: 31.4%; All InfoAccess Qs: 41.1%; BeliefQ[CHOICE]: 74.7%; BeliefQ[DIST.]: 55.0%; Answerability Q[Y/N]: 86.6%; InfoAccess Q[Y/N]: 91.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human: All Question Types: 87.5%; All Answerability Qs: 90.6%; All InfoAccess Qs: 90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperforms humans by a large margin on coherent multi-question metrics; somewhat competent on binary InfoAccess/Answerability Y/N but still below human levels on the All-consistency metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Performance differences across GPT-4 checkpoints indicate variability across model releases; shows the same pattern of better performance on multiple-choice / binary than on free-response and list-type questions and poor consistency across question types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5487.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5487.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613) + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, 0613 checkpoint) with chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned GPT-3.5-based ChatGPT evaluated with zero-shot CoT; shows modest improvements from CoT but remains well below human performance on FANToM metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613) + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-based ChatGPT (instruction-tuned conversational model) evaluated with 'let's think step by step' CoT prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not reported in-paper)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same FANToM six-question-type conversational ToM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All Question Types (short context, CoT): 3.7%; All Answerability Qs: 20.7%; All InfoAccess Qs: 17.1%; BeliefQ[CHOICE]: 58.5%; BeliefQ[DIST.]: 45.2%; FACTQ token F1 ~44.7%; Answerability Q[Y/N]: 76.7%; InfoAccess Q[Y/N]: 79.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human: All Question Types: 87.5%; BeliefQ[CHOICE]: 93.8%; Answerability/InfoAccess All ~90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Substantially worse than humans on coherent ToM metrics; shows relative competence on some binary tasks but low on list-type and free-response belief tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Improvement under CoT is limited and does not close the gap to humans; exhibits the common failure modes noted in the paper (illusory ToM, reliance on context overlap, inconsistent answers across question types).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5487.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5487.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT davinci-003 + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT davinci-003 (OpenAI) with chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned davinci-003 evaluated with CoT prompting on FANToM; moderate comprehension but poor consistency on belief attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT davinci-003 + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>davinci-003: an instruction-tuned GPT-3 family model (decoder-only transformer) evaluated with 'let's think step by step' CoT prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not reported in-paper)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same FANToM benchmark with six ToM question types; BeliefQ[DIST.] free response requires matching PersonX-centric belief rather than omniscient facts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All Question Types (short context, CoT): 6.2%; BeliefQ[CHOICE]: 39.8%; BeliefQ[DIST.]: 22.2%; Answerability All: 9.3%; Answerability Q[Y/N]: 80.4%; InfoAccess All: 16.8%; InfoAccess Q[Y/N]: 86.6%; FACTQ token F1 ~41.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human: All Question Types: 87.5%; BeliefQ[CHOICE]: 93.8%; Answerability/InfoAccess All: ~90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performs far below humans overall; better at binary Y/N InfoAccess/Answerability than list or free-response belief questions, indicating format sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>High token overlap on FACTQ but fails to produce PersonX-centric belief answers for BELIEFQ[DIST.], indicating failure to withhold inaccessible information and a tendency to reproduce omniscient answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5487.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5487.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XL + FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-XL (Google, instruction-finetuned) fine-tuned on FANToM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large encoder-decoder instruction-tuned Flan-T5-XL model further fine-tuned on FANToM training splits; fine-tuning improves per-question-type scores and consistency but still lags human-level across the full-coherence metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XL + FT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flan-T5-XL: an encoder-decoder Transformer (Google Flan family) instruction-tuned; in-paper additionally fine-tuned on FANToM (topic split) using specified hyperparameters (lr=2e-5, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XL (exact parameter count not specified in-paper; Flan-T5-XL typically ~3B–11B range depending on source)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same FANToM benchmark; fine-tuning targeted to improve coherent answers across the six question types derived from the same fact.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Short-context, fine-tuned results reported: All Question Types: 53.7%; All Answerability Qs: 55.9%; All InfoAccess Qs: 54.4%; BeliefQ[CHOICE] reported up to ~93.4% in some tables; FACTQ token F1 and other submetrics improved compared to zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human: All Question Types: 87.5%; All Answerability/InfoAccess Qs: 90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Fine-tuning substantially improves performance on individual question types and some aggregated metrics, bringing some per-type scores near or above many other LLMs and (in isolated cases) approaching human performance on individual question types; however, Flan-T5-XL + FT still underperforms humans on the ALL* metric that requires consistent correctness across all six ToM question types for the same information.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Fine-tuning helps but does not solve coherent multi-question ToM reasoning; model still exhibits format-specific error patterns and inconsistency across question types, suggesting training on FANToM can patch surface behaviors without fully restoring robust ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5487.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5487.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral Instruct 7B + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral Instruct 7B (Mistral AI) with chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B instruction-tuned Mistral model evaluated with CoT; shows relatively strong open-source performance on some comprehension metrics but very low 'All Question Types' coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral Instruct 7B + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral Instruct 7B: a 7-billion-parameter open-source instruction-tuned transformer from Mistral AI; evaluated both with and without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same FANToM conversational ToM evaluation measuring belief attribution and information access under asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Short-context, CoT: All Question Types: 0.1%; All Answerability Qs: 2.4%; All InfoAccess Qs: 9.1%; FACTQ token F1: ~50.8%; BeliefQ[CHOICE]: ~58.5%; Answerability Q[Y/N]: 61.5%; InfoAccess Q[Y/N]: 70.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human: All Question Types: 87.5%; Answerability/InfoAccess All: ~90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performs far below human baseline on the coherent multi-question metric and on list-type questions, though it shows relatively stronger FACTQ comprehension F1 compared to some larger models; on binary Y/N questions performance is moderate but still below humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Open-source small model displays a dissociation between basic fact comprehension (FACTQ F1) and coherent perspective-taking (ALL*), again highlighting format sensitivity and lack of consistent ToM. CoT marginally helps some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5487.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5487.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 Chat 70B + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 Chat 70B (Meta) with chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's Llama-2 Chat 70B instruction-tuned conversational model evaluated with CoT on FANToM; achieves moderate binary-task performance but low list-type and All-consistency performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 Chat 70B + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 Chat 70B: a 70-billion-parameter instruction- and safety-tuned Llama-2 conversational model; evaluated with and without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: multiparty conversational ToM questions, requiring tracking of who knows what.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Short-context, CoT: All Question Types: 0.4%; All Answerability Qs: 6.0%; All InfoAccess Qs: 7.8%; BeliefQ[CHOICE]: ~58.5%; BeliefQ[DIST.]: ~31.5%; Answerability Q[Y/N]: 61.4%; InfoAccess Q[Y/N]: 80.4%; FACTQ token F1 ~33.9–52.7 depending on table section.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human: All Question Types: 87.5%; Answerability/InfoAccess All: ~90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperforms humans substantially across coherent ToM metrics; comparable to other open-source models on binary Y/N InfoAccess/Answerability but poor in multi-question consistency and free-response belief tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Shows format-dependent performance (choice/binary > free-response/list), and CoT sometimes increases some scores but not consistent improvement in All-consistency or per-character accuracy; tendency to include unaware characters in list responses and produce false positives that CoT can reduce at cost of other errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5487.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5487.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon Instruct 40B + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon Instruct 40B (Iraq's Technology? / TII) with chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Falcon Instruct 40B, a 40B-parameter open-source instruction-tuned model, evaluated with CoT on FANToM; shows low All-consistency and poor list-type performance though binary InfoAccess Y/N may be moderate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon Instruct 40B + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Falcon Instruct 40B: an instruction-tuned 40B-parameter open-source decoder-only transformer; evaluated with 'let's think step by step' CoT prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>FANToM (conversation-based Theory of Mind benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same FANToM six-question-type conversational ToM evaluation that emphasizes information asymmetry and belief attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Short-context, CoT: All Question Types: 0.0%; All Answerability Qs: 1.7%; All InfoAccess Qs: 2.3%; BeliefQ[CHOICE]: ~51.7%; BeliefQ[DIST.]: ~72.1% (varies by table); Answerability Q[Y/N]: 58.4%; InfoAccess Q[Y/N]: 65.1%; FACTQ token F1 ~19.5.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human: All Question Types: 87.5%; All Answerability/InfoAccess Qs: 90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Falls far short of human performance on coherent metrics; mixed per-subtask scores but overall lacks consistent perspective-taking ability compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Large open-source model shows significant weaknesses on list-type and multi-question coherence; CoT sometimes improves specific metrics but does not yield human-level ToM; models often err by including characters lacking access (false positives) more than excluding aware characters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural theory-of-mind? on the limits of social intelligence in large LMs <em>(Rating: 2)</em></li>
                <li>Revisiting the evaluation of theory of mind through question answering <em>(Rating: 2)</em></li>
                <li>Clever hans or neural theory of mind? stress testing social reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>How far are large language models from agents with theory-of-mind? <em>(Rating: 1)</em></li>
                <li>Minding language models' (lack of) theory of mind: A plug-and-play multi-character belief tracker <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5487",
    "paper_id": "paper-088a5fa00ed6c14351209da5f53e770b51fd2909",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4 (Jun 0613) + CoT",
            "name_full": "GPT-4 (OpenAI, 0613 checkpoint, June) with zero-shot chain-of-thought prompting",
            "brief_description": "OpenAI's GPT-4 (decoder-only transformer) evaluated with a 'let's think step by step' chain-of-thought prompt on the FANToM conversational Theory-of-Mind benchmark; shown to be the strongest LLM in this paper but still far below humans on coherent ToM metrics.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "GPT-4 (0613, June) + CoT",
            "model_description": "GPT-4, OpenAI's large decoder-only transformer model (proprietary), instruction-tuned; evaluated using zero-shot chain-of-thought prompting ('let's think step by step').",
            "model_size": "proprietary (not reported in-paper)",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "A 10K-question benchmark of multiparty conversations with information asymmetry, converted into six ToM question types (BeliefQ [free and choice], FactQ, Answerability Q [List/Y/N], InfoAccess Q [List/Y/N]); measures whether models attribute beliefs/knowledge correctly for characters who missed parts of the conversation.",
            "llm_performance": "All Question Types (short context, CoT): 26.6%; All Answerability Qs: 40.2%; All InfoAccess Qs: 57.7%; BeliefQ[CHOICE]: 80.6%; BeliefQ[DIST.]: 66.7%; FACTQ token F1 (comprehension): 44.0%; Answerability Q[Y/N]: 88.5%; InfoAccess Q[Y/N]: 92.1%.",
            "human_baseline_performance": "Human (graduate CS students, same instructions): All Question Types: 87.5%; All Answerability Qs: 90.6%; All InfoAccess Qs: 90.6%; BeliefQ[CHOICE]: 93.8%.",
            "performance_comparison": "Underperforms humans substantially on coherent across-question metrics (All Question Types 26.6% vs human 87.5%). On some individual sub-tasks (e.g., BeliefQ[CHOICE], Answerability/InfoAccess Y/N) GPT-4 + CoT approaches or exceeds many models but still typically trails human accuracy; gap remains large on multi-question coherence.",
            "notable_differences_or_limitations": "Best-performing LLM here but still shows 'illusory ToM' — high scores on some question formats (multiple-choice, binary) but fails to be consistent across the six question types for the same information. Benefits from CoT (improves scores) but still far from human-level coherent ToM; tends to rely on accessible omniscient information and word overlap when answering belief questions.",
            "uuid": "e5487.0",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (Oct 0613) + CoT",
            "name_full": "GPT-4 (OpenAI, 0613 checkpoint, October) with zero-shot chain-of-thought prompting",
            "brief_description": "Later checkpoint of GPT-4 evaluated with chain-of-thought prompting on FANToM; shows notably lower performance than the June checkpoint on many FANToM metrics in this paper's evaluations.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "GPT-4 (0613, October) + CoT",
            "model_description": "GPT-4 (October checkpoint), instruction-tuned transformer from OpenAI; evaluated with chain-of-thought prompt.",
            "model_size": "proprietary (not reported in-paper)",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "Same as above: multiparty conversational ToM QA across six question types requiring tracking of information accessibility for characters.",
            "llm_performance": "All Question Types (short context, CoT): 14.8%; All Answerability Qs: 31.4%; All InfoAccess Qs: 41.1%; BeliefQ[CHOICE]: 74.7%; BeliefQ[DIST.]: 55.0%; Answerability Q[Y/N]: 86.6%; InfoAccess Q[Y/N]: 91.3%.",
            "human_baseline_performance": "Human: All Question Types: 87.5%; All Answerability Qs: 90.6%; All InfoAccess Qs: 90.6%.",
            "performance_comparison": "Underperforms humans by a large margin on coherent multi-question metrics; somewhat competent on binary InfoAccess/Answerability Y/N but still below human levels on the All-consistency metrics.",
            "notable_differences_or_limitations": "Performance differences across GPT-4 checkpoints indicate variability across model releases; shows the same pattern of better performance on multiple-choice / binary than on free-response and list-type questions and poor consistency across question types.",
            "uuid": "e5487.1",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT (gpt-3.5-turbo-0613) + CoT",
            "name_full": "ChatGPT (gpt-3.5-turbo, 0613 checkpoint) with chain-of-thought prompting",
            "brief_description": "OpenAI's instruction-tuned GPT-3.5-based ChatGPT evaluated with zero-shot CoT; shows modest improvements from CoT but remains well below human performance on FANToM metrics.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0613) + CoT",
            "model_description": "GPT-3.5-based ChatGPT (instruction-tuned conversational model) evaluated with 'let's think step by step' CoT prompt.",
            "model_size": "proprietary (not reported in-paper)",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "Same FANToM six-question-type conversational ToM evaluation.",
            "llm_performance": "All Question Types (short context, CoT): 3.7%; All Answerability Qs: 20.7%; All InfoAccess Qs: 17.1%; BeliefQ[CHOICE]: 58.5%; BeliefQ[DIST.]: 45.2%; FACTQ token F1 ~44.7%; Answerability Q[Y/N]: 76.7%; InfoAccess Q[Y/N]: 79.1%.",
            "human_baseline_performance": "Human: All Question Types: 87.5%; BeliefQ[CHOICE]: 93.8%; Answerability/InfoAccess All ~90.6%.",
            "performance_comparison": "Substantially worse than humans on coherent ToM metrics; shows relative competence on some binary tasks but low on list-type and free-response belief tasks.",
            "notable_differences_or_limitations": "Improvement under CoT is limited and does not close the gap to humans; exhibits the common failure modes noted in the paper (illusory ToM, reliance on context overlap, inconsistent answers across question types).",
            "uuid": "e5487.2",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "InstructGPT davinci-003 + CoT",
            "name_full": "InstructGPT davinci-003 (OpenAI) with chain-of-thought prompting",
            "brief_description": "OpenAI's instruction-tuned davinci-003 evaluated with CoT prompting on FANToM; moderate comprehension but poor consistency on belief attribution.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "InstructGPT davinci-003 + CoT",
            "model_description": "davinci-003: an instruction-tuned GPT-3 family model (decoder-only transformer) evaluated with 'let's think step by step' CoT prompt.",
            "model_size": "proprietary (not reported in-paper)",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "Same FANToM benchmark with six ToM question types; BeliefQ[DIST.] free response requires matching PersonX-centric belief rather than omniscient facts.",
            "llm_performance": "All Question Types (short context, CoT): 6.2%; BeliefQ[CHOICE]: 39.8%; BeliefQ[DIST.]: 22.2%; Answerability All: 9.3%; Answerability Q[Y/N]: 80.4%; InfoAccess All: 16.8%; InfoAccess Q[Y/N]: 86.6%; FACTQ token F1 ~41.6%.",
            "human_baseline_performance": "Human: All Question Types: 87.5%; BeliefQ[CHOICE]: 93.8%; Answerability/InfoAccess All: ~90.6%.",
            "performance_comparison": "Performs far below humans overall; better at binary Y/N InfoAccess/Answerability than list or free-response belief questions, indicating format sensitivity.",
            "notable_differences_or_limitations": "High token overlap on FACTQ but fails to produce PersonX-centric belief answers for BELIEFQ[DIST.], indicating failure to withhold inaccessible information and a tendency to reproduce omniscient answers.",
            "uuid": "e5487.3",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Flan-T5-XL + FT",
            "name_full": "Flan-T5-XL (Google, instruction-finetuned) fine-tuned on FANToM",
            "brief_description": "A large encoder-decoder instruction-tuned Flan-T5-XL model further fine-tuned on FANToM training splits; fine-tuning improves per-question-type scores and consistency but still lags human-level across the full-coherence metric.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XL + FT",
            "model_description": "Flan-T5-XL: an encoder-decoder Transformer (Google Flan family) instruction-tuned; in-paper additionally fine-tuned on FANToM (topic split) using specified hyperparameters (lr=2e-5, etc.).",
            "model_size": "XL (exact parameter count not specified in-paper; Flan-T5-XL typically ~3B–11B range depending on source)",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "Same FANToM benchmark; fine-tuning targeted to improve coherent answers across the six question types derived from the same fact.",
            "llm_performance": "Short-context, fine-tuned results reported: All Question Types: 53.7%; All Answerability Qs: 55.9%; All InfoAccess Qs: 54.4%; BeliefQ[CHOICE] reported up to ~93.4% in some tables; FACTQ token F1 and other submetrics improved compared to zero-shot.",
            "human_baseline_performance": "Human: All Question Types: 87.5%; All Answerability/InfoAccess Qs: 90.6%.",
            "performance_comparison": "Fine-tuning substantially improves performance on individual question types and some aggregated metrics, bringing some per-type scores near or above many other LLMs and (in isolated cases) approaching human performance on individual question types; however, Flan-T5-XL + FT still underperforms humans on the ALL* metric that requires consistent correctness across all six ToM question types for the same information.",
            "notable_differences_or_limitations": "Fine-tuning helps but does not solve coherent multi-question ToM reasoning; model still exhibits format-specific error patterns and inconsistency across question types, suggesting training on FANToM can patch surface behaviors without fully restoring robust ToM.",
            "uuid": "e5487.4",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mistral Instruct 7B + CoT",
            "name_full": "Mistral Instruct 7B (Mistral AI) with chain-of-thought prompting",
            "brief_description": "Open-source 7B instruction-tuned Mistral model evaluated with CoT; shows relatively strong open-source performance on some comprehension metrics but very low 'All Question Types' coherence.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "Mistral Instruct 7B + CoT",
            "model_description": "Mistral Instruct 7B: a 7-billion-parameter open-source instruction-tuned transformer from Mistral AI; evaluated both with and without CoT.",
            "model_size": "7B",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "Same FANToM conversational ToM evaluation measuring belief attribution and information access under asymmetry.",
            "llm_performance": "Short-context, CoT: All Question Types: 0.1%; All Answerability Qs: 2.4%; All InfoAccess Qs: 9.1%; FACTQ token F1: ~50.8%; BeliefQ[CHOICE]: ~58.5%; Answerability Q[Y/N]: 61.5%; InfoAccess Q[Y/N]: 70.4%.",
            "human_baseline_performance": "Human: All Question Types: 87.5%; Answerability/InfoAccess All: ~90.6%.",
            "performance_comparison": "Performs far below human baseline on the coherent multi-question metric and on list-type questions, though it shows relatively stronger FACTQ comprehension F1 compared to some larger models; on binary Y/N questions performance is moderate but still below humans.",
            "notable_differences_or_limitations": "Open-source small model displays a dissociation between basic fact comprehension (FACTQ F1) and coherent perspective-taking (ALL*), again highlighting format sensitivity and lack of consistent ToM. CoT marginally helps some metrics.",
            "uuid": "e5487.5",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Llama-2 Chat 70B + CoT",
            "name_full": "Llama-2 Chat 70B (Meta) with chain-of-thought prompting",
            "brief_description": "Meta's Llama-2 Chat 70B instruction-tuned conversational model evaluated with CoT on FANToM; achieves moderate binary-task performance but low list-type and All-consistency performance.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "Llama-2 Chat 70B + CoT",
            "model_description": "Llama-2 Chat 70B: a 70-billion-parameter instruction- and safety-tuned Llama-2 conversational model; evaluated with and without CoT.",
            "model_size": "70B",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "Same as above: multiparty conversational ToM questions, requiring tracking of who knows what.",
            "llm_performance": "Short-context, CoT: All Question Types: 0.4%; All Answerability Qs: 6.0%; All InfoAccess Qs: 7.8%; BeliefQ[CHOICE]: ~58.5%; BeliefQ[DIST.]: ~31.5%; Answerability Q[Y/N]: 61.4%; InfoAccess Q[Y/N]: 80.4%; FACTQ token F1 ~33.9–52.7 depending on table section.",
            "human_baseline_performance": "Human: All Question Types: 87.5%; Answerability/InfoAccess All: ~90.6%.",
            "performance_comparison": "Underperforms humans substantially across coherent ToM metrics; comparable to other open-source models on binary Y/N InfoAccess/Answerability but poor in multi-question consistency and free-response belief tasks.",
            "notable_differences_or_limitations": "Shows format-dependent performance (choice/binary &gt; free-response/list), and CoT sometimes increases some scores but not consistent improvement in All-consistency or per-character accuracy; tendency to include unaware characters in list responses and produce false positives that CoT can reduce at cost of other errors.",
            "uuid": "e5487.6",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon Instruct 40B + CoT",
            "name_full": "Falcon Instruct 40B (Iraq's Technology? / TII) with chain-of-thought prompting",
            "brief_description": "Falcon Instruct 40B, a 40B-parameter open-source instruction-tuned model, evaluated with CoT on FANToM; shows low All-consistency and poor list-type performance though binary InfoAccess Y/N may be moderate.",
            "citation_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
            "mention_or_use": "use",
            "model_name": "Falcon Instruct 40B + CoT",
            "model_description": "Falcon Instruct 40B: an instruction-tuned 40B-parameter open-source decoder-only transformer; evaluated with 'let's think step by step' CoT prompt.",
            "model_size": "40B",
            "cognitive_test_name": "FANToM (conversation-based Theory of Mind benchmark)",
            "cognitive_test_type": "Theory of Mind / social reasoning",
            "cognitive_test_description": "Same FANToM six-question-type conversational ToM evaluation that emphasizes information asymmetry and belief attribution.",
            "llm_performance": "Short-context, CoT: All Question Types: 0.0%; All Answerability Qs: 1.7%; All InfoAccess Qs: 2.3%; BeliefQ[CHOICE]: ~51.7%; BeliefQ[DIST.]: ~72.1% (varies by table); Answerability Q[Y/N]: 58.4%; InfoAccess Q[Y/N]: 65.1%; FACTQ token F1 ~19.5.",
            "human_baseline_performance": "Human: All Question Types: 87.5%; All Answerability/InfoAccess Qs: 90.6%.",
            "performance_comparison": "Falls far short of human performance on coherent metrics; mixed per-subtask scores but overall lacks consistent perspective-taking ability compared to humans.",
            "notable_differences_or_limitations": "Large open-source model shows significant weaknesses on list-type and multi-question coherence; CoT sometimes improves specific metrics but does not yield human-level ToM; models often err by including characters lacking access (false positives) more than excluding aware characters.",
            "uuid": "e5487.7",
            "source_info": {
                "paper_title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural theory-of-mind? on the limits of social intelligence in large LMs",
            "rating": 2
        },
        {
            "paper_title": "Revisiting the evaluation of theory of mind through question answering",
            "rating": 2
        },
        {
            "paper_title": "Clever hans or neural theory of mind? stress testing social reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2
        },
        {
            "paper_title": "How far are large language models from agents with theory-of-mind?",
            "rating": 1
        },
        {
            "paper_title": "Minding language models' (lack of) theory of mind: A plug-and-play multi-character belief tracker",
            "rating": 1
        }
    ],
    "cost": 0.02017275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>满 FANTOM: <br> A Benchmark for Stress-testing Machine Theory of Mind in Interactions</h1>
<p>Hyunwoo Kim ${ }^{\text {M }}$ Melanie Sclar ${ }^{\text {A }}$ Xuhui Zhou ${ }^{\text { }}$<br>Ronan Le Bras ${ }^{\text {M }}$ Gunhee Kim ${ }^{\text {A }}$ Yejin Choi ${ }^{\text {M }}$ Maarten Sap ${ }^{\text {M }}$<br>$\cdot$ Allen Institute for Artificial Intelligence $\triangle$ University of Washington<br>$\diamond$ Carnegie Mellon University $\triangle$ Seoul National University</p>
<h4>Abstract</h4>
<p>Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce滣 FANTOM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Existing evaluations for language models' theory of mind (ToM) - i.e., the ability to understand the mental states (e.g., thoughts, beliefs, and intentions) of others (Premack and Woodruff, 1978), is primarily focused on using situation descriptions (i.e., narratives) as the target domain (Nematzadeh et al., 2018; Le et al., 2019; Sap et al., 2022; Shapira et al., 2023a). However, ToM capabilities play an even more important role in understanding dynamic social interactions, as they form a crucial component of effective communication (Frith, 1994; Schober, 2005). Furthermore, as narratives condense situation information into short texts, reporting biases can cause them to include spurious correlations or surface cues (Gordon and Van Durme, 2013). These can be exploited by large language models (LLMs) to display illusory ToM - i.e., a false sense of robust social reasoning by models. ${ }^{2}$</p>
<p>In this work, we introduce 满 FANToM, an English benchmark for stress-testing machine ToM</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<ul>
<li>Answerability Questions (about the Fact Question)
a: Who knows the correct answer to this question?
A: Linda, David, Sally
a: Does David know the correct answer to this question? A: Yes</li>
<li>Info Accessibility Questions (about the Full Fact Answer)
a: Who knows about this information? A: Linda, David, Sally
a: Does Sally know about this information? A: Yes</li>
</ul>
<p>Figure 1: An example question set in 满 FANToM.
in interactions - i.e., conversations. As conversations present interactions in their raw form, they are much less susceptible to reporting biases, and are more aligned with real-world scenarios requiring ToM reasoning. FANTOM consists of 10K questions covering 256 multiparty conversations around a certain topic while characters enter and leave the discussion, leading to distinct mental states between characters due to information asymmetry.</p>
<p>The goal of FANTOM is to effectively measure how well models can track the belief of multiple</p>
<p>characters in conversations where some information may be inaccessible to some participants. For example, in Figure 1, Kailey briefly steps away from the conversation to get a cup of coffee, while the others continue discussing Linda's new dog. The information exchanged during Kailey's absence remains unknown to Kailey, and only the information shared after Kailey's return becomes accessible. We convert factual question-answer pairs to obtain multiple challenging questions about characters' beliefs concerning the inaccessible information. Our aim is to design questions at different levels that evaluate a model's capability for a coherent understanding of others' mental states. In doing so, we are particularly interested in identifying instances of illusory ToM, which we define as situations where a model may answer some questions correctly but fails to answer others that require the same type of ToM reasoning.</p>
<p>The analysis of evaluation results on FANToM reveals several interesting findings (§4): (1) First, existing neural models score significantly lower than humans on individual questions and on the full set of questions by more than $70 \%$ on average. (2) While chain-of-thought reasoning (CoT) does improve performance in most models, it does not substantially bridge the gap with human performance. (3) Although our benchmark is not meant for training, we observe that fine-tuning can help models achieve scores higher than human performance on individual question types. However, when it comes to metrics that require coherent responses across multiple question types, the finetuned model still significantly underperforms compared to humans. (4) Additionally, we find that models exhibit different error types depending on the format of questions, despite all questions requiring the same underlying reasoning. (5) Moreover, our results indicate that CoT has a selective impact on performance, showing improvement only in specific scenarios.</p>
<p>To the best of our knowledge, FANToM is the first benchmark to introduce conversation-based ToM evaluation for language-based models. Our benchmark design and experiment results yield important insights into the debate around ToM (Whang, 2023) and the development of artificial general intelligence (Metz, 2023) in LLMs. We release our benchmark to spark further discussions on evaluating the ToM capabilities of LLMs.</p>
<h2>2 Design Considerations for $\mathscr{\&amp;}$ FANToM</h2>
<p>We go over the important design choices that we made when constructing FANToM. Our goal is to incorporate (1) social interactions that necessitate natural theory of mind (ToM) reasoning (§2.1), (2) essential theoretical prerequisites for validating ToM from psychology (§2.2), and (3) empirical findings that must be taken into account when evaluating large language models (§2.3).</p>
<h3>2.1 Grounding in Social Interactions</h3>
<p>To capture the interactive aspect of ToM, we ground our task in natural social interactions - i.e., conversations. By doing so, we gain two key benefits: (1) minimizing reporting bias (Gordon and Van Durme, 2013) and (2) aligning with real-world scenarios.</p>
<p>Since narratives are condensed descriptions of interactions, the process of deciding what to include or exclude can introduce reporting bias, resulting in artifacts that models exploit. For instance, including "Carlos did not see this, so he does not know currently where the apple is." in a narrative for ToM evaluation provides a significant clue about the other's mental state. However, such explicit hints are rarely present in real-world interactions.</p>
<p>Conversations, on the other hand, present interactions in their raw form, without those explicit hints about others' mental states. During conversations, we reason through the intermediate steps from scratch, thereby grounding the benchmark in conversations enables a more realistic and unbiased assessment of ToM.</p>
<h3>2.2 Meeting Theoretic Requirements</h3>
<p>We follow the two important criteria outlined by Quesque and Rossetti (2020) that must be met when designing a task to validate ToM: "nonmerging" and "mentalizing".
(1) "Non-merging": Evaluation should require the respondent to maintain a distinction between the others' mental state and its own. For example, suppose someone is asked about the other's belief regarding the location of the TV remote controller, and both are believing it to be on the sofa. If the respondent answers that the other believes it is on the sofa, it becomes unclear whether the response is based on the respondent's own belief or the other's (i.e., merging mental states). Such merging scenario is unsuitable for validating ToM.</p>
<p>Since machines lack emotions or intentions (Gros et al., 2022), we exploit information asymme-</p>
<p>try when constructing our benchmark to simulate the non-merging mental state scenarios. We design multiparty conversations where specific information is inaccessible to certain characters. While machines do not possess their own point of view, they act as omniscient observers during our evaluation since we provide the entire conversation as input. As a result, the mental states of the model and the character can be regarded as distinct with respect to that information.
(2) "Mentalizing": Lower-level processes should not be accounted for successful performance of ToM tasks. If a simpler process can explain a phenomenon, it should always be preferred over a more complex one when interpreting the results. For instance, recognizing joy by observing laughter is more of a visual discrimination than reasoning mental representations.</p>
<p>If the correct answer for a ToM task has a high degree of word correlation with a salient part of the given input, it becomes difficult to determine whether the model is accurately ascribing the other's mental state or simply following a shortcut pattern matching (i.e., the lower-level process). Therefore, such cases should be discouraged when evaluating ToM in neural language models. In FANToM, we create false answers that have high word correlation with the input to verify whether the models can overcome the shortcut pattern matching when reasoning mental states.</p>
<h3>2.3 Seeking Comprehensive Evaluation</h3>
<p>Since the performance of LLMs varies significantly based on given prompts (Webson and Pavlick, 2022), we adopt a series of reiterative questions at various levels for the same input context, including free-form response questions, multiple-choice questions, and straightforward yes or no questions. The inclusion of free-form response questions is important as it aligns with the common usage of LLMs in contrast to multiple-choice questions that are prevalent in existing benchmarks (Sakaguchi et al., 2021; Hendrycks et al., 2021). Although their formats are different, all questions in FANToM fundamentally aim to ascertain the same underlying reasoning: "who is aware of the information?" As a result, FANToM enables us to identify illusory ToM instances wherein models deliver accurate responses for one format but struggles to do so for another format.</p>
<h2>3 FANToM Overview</h2>
<p>Following the success of previous works (Kim et al., 2022; Chen et al., 2023), we automatically construct full conversations using the large language model (LLM) InstructGPT davinci-003 (Ouyang et al., 2022). We also generate theory of mind (ToM) question-answer pairs related to the conversation participants' beliefs using a specially designed pipeline. In preliminary explorations, we find off-the-shelf LLMs struggle with directly generating ToM question-answer pairs for a given conversation. Our pipeline consists of three steps: (1) generate conversations with information asymmetry (§3.1), (2) generate fact question-answer (QA) pairs (§3.2), and (3) construct ToM (e.g., belief) QA pairs from the fact QA pairs (§3.3). We use different evaluation methods for each question types (§3.4), and validate the final dataset (§3.5).</p>
<h3>3.1 Information-Asymmetric Conversations</h3>
<p>FANToM consists of small talk conversations involving multiple characters, with each conversation centered around a topic (e.g., pets, risk-taking, personal growth). Each topic has several subtopics, e.g. the topic "pets" may include subtopics "breed" and "special moves". Initially, the conversation begins with two or three characters. As the conversation progresses, characters join and leave the discussion and the conversation's subtopic changes over time. Conversations include explicit indications of leaving and joining, such as utterances like "Hey guys, I'll go grab a coffee." or "Hey, I'm back, what are you guys discussing now?" shown in Figure 1. During the absence of a character, the conversation continues and information is shared among the remaining participants, creating a natural information asymmetry that reflects real-life interactions. After a series of utterances, the character who was absent (re)joins the conversation, unaware of the information that was previously shared with other participants. More details are in Appendix A.1.</p>
<p>Many existing ToM tasks involve some form of asymmetry between characters (Braüner et al., 2020). For example, in the Sally-Anne task, Sally does not know that Anne relocated the object, while the observer is aware of the action. In the Smarties task, the character in the story does not know the label changed, whereas the observer is fully aware of this situation. This inherent asymmetry ensures two distinct mental states (i.e., the non-merging criterion; §2.2) to be present during the experiments.</p>
<h3>3.2 Factual Question-Answer (QA) Pairs</h3>
<p>The conversations in FANTOM include factual question-answer pairs (FACTQ) about the inaccessible information-i.e., the information that a specific character is unaware of. An example question would be "What is the breed of Linda's dog?" in Figure 1. More details are in Appendix A.2.</p>
<p>There are two distinct types of answers for each FACTQ: (1) Full Fact A and (2) Limited Fact A. The Full Fact A incorporates the full information in the preceding conversation where the character PersonX was absent. On the other hand, Limited Fact A relies only on the conversation in which PersonX participated. The former answer is based on information that PersonX does not have access to, while the latter answer only takes into account the accessible information for PersonX. For cases where no information was shared regarding the FACTQ, the Limited Fact A indicates that no information has been provided. Examples of these answers are in Figure 1. "Linda has a golden retriever." is based on the preceding conversation where Kailey was absent, while "There is no information on the breed of Linda's dog" is based on the conversation Kailey participated.</p>
<h3>3.3 ToM QAs based on Fact QAs</h3>
<p>For each FACTQ, we build six types of ToM QA. Construction details can be found in Appendix A.3.
(1) BeliefQ ${ }<em _Choice_="{[Choice]" _text="\text">{\text {[Dist.] }}$ and (2) BeliefQ ${ }</em>}}$ : These questions are created by rephrasing the FACTQ to ask beliefs of characters in the conversation. We are particularly interested in PersonX's belief about the inaccessible information from the previous conversation, in which PersonX did not participate. For example, the FACTQ of "What is the breed of Linda's dog?" in Figure 1 is converted to "What breed would Kailey think Linda's dog is?" The BeliefQ ${ <em _Choice_="{[Choice]" _text="\text">{\text {[Dist.] }}$ requires free-form response, while BeliefQ ${ }</em>$ provides multiple-choice options for the same question.}</p>
<p>The options for BeliefQ are created by rephrasing the Full Fact A and Limited Fact A. For example, the "Linda has a golden retriever." in Figure 1 is converted to "Kailey believes Linda has a golden retriever." Since the Full Fact A reflects information that is not accessible to PersonX and the Limited Fact A incorporates only the information accessible to PersonX, we label the converted Full Fact A and Limited Fact A as
"OmnisCient-viEW Belief A" and "PERSONXCENTRIC BELIEF A", respectively.
(3) Answerability $\mathbf{Q}<em _List_="{[List]" _text="\text">{\text {[List] }}$ : Given the FACTQ, we ask models "List all the characters who know the correct answer to this question". In essence, we are interested in whether the model can identify who among the participants can correctly answer the FACTQ. This is a meta-question that necessitates two-step reasoning: first determining the answer itself, and second, identifying the characters who have access to this knowledge.
(4) InfoAccess $\mathbf{Q}</em>}}$ : Here, we provide the Full Fact A with the FactQ and ask the model "List all the characters who know this information". Essentially, this question aims to identify the individuals who have knowledge or access to this information. Since the information is explicitly provided to the model, only the second reasoning step of the ANSWERABILITY $\mathrm{Q<em _mathrm_Y="[\mathrm{Y">{\text {[LIST] }}$ is required.
(5) Answerability $\mathbf{Q}</em>$ : We ask models to determine, through a simple binary response (yes or no), whether each character is capable of answering the question or knows the information. For example, we ask models "Does David know the correct answer to this question?" and "Does Sally know about this information?" (Figure 1).} / \mathrm{N}]}$ and (6) InfoAccess $\mathbf{Q}_{[\mathrm{Y} / \mathrm{N}]</p>
<h3>3.4 Evaluation</h3>
<p>Each question is provided to the model along with the conversation as input. This makes the model an omniscient observer, having access to all information shared in the conversation. On the other hand, PersonX was absent for a while, thereby an information asymmetry naturally arises between the model and PersonX. Responses that include inaccessible information for PersonX indicate a lack of ToM in the model.</p>
<p>Input context types FANTOM comprises two types of input conversations: short and full. In the case of short input, the model is provided with the conversation that only includes the part where the specific speaker left and (re)joined, while excluding the other earlier and later parts of the conversation. On the other hand, a full conversation encompasses the entire discussion on the main topic, including all subtopics. As a result, this is significantly longer than the short input.</p>
<p>BeliefQ ${ }_{\text {[Dist.] }}$ When given a belief question regarding PersonX, the model should generate a response that incorporates only the information accessible to PersonX. We use cosine similarity to measure the distance between SentenceBERT (Reimers and Gurevych, 2019) embeddings of each option and response. A correct response should always be closer to the PersonX-Centric Belief A than the OmnisCient-viEW Belief A.</p>
<p>To accurately assess the performance of the response, we also calculate the token F1 score for responses that are considered correct based on the distance metric, following the convention of various QA tasks (Rajpurkar et al., 2016, 2018). When comparing distances in the embedding space, nonsensical responses (e.g., repetition of character names) can be deceptively closer to PersonXCENTRIC BELIEF A, resulting in misleading accuracy. Therefore, models must score high on both the distance and F1 metrics for the BeliefQ ${ }<em _Choice_="{[Choice]" _text="\text">{\text {[Dist.] }}$.
BeliefQ ${ }</em>$ The model should choose between the OmnisCient-viEW Belief A and the PersonX-Centric Belief A. The correct answer is the PersonX-Centric Belief A.}</p>
<p>Answerability $\mathbf{Q}<em _List_="{[List]" _text="\text">{\text {[List] }}$ and InfoAccess $\mathbf{Q}</em>$ A correct response must include all characters who have access to the answer or information while excluding all characters who do not. No partial marks are assigned.}</p>
<p>Answerability $\mathbf{Q}<em _mathrm_Y="[\mathrm{Y">{[\mathrm{Y} / \mathrm{N}]}$ and InfoAccess $\mathbf{Q}</em>$ The model should respond with "yes" or "true" for all characters who have access to the answer or information, and with "no" or "false" for all characters who do not. More details are in Appendix A.4.} / \mathrm{N}]</p>
<h3>3.5 Dataset Validation \&amp; Statistics</h3>
<p>Validation To ensure the quality of our benchmark, we go through a manual validation process for all conversations and question-answer pairs using Amazon Mechanical Turk (MTurk). We conduct validation on the entire conversations in our dataset using 32 annotators who passed a qualification test for assessing conversation coherence. We ask workers to flag conversations that are incoherent or unsafe (e.g., unethical, biased, harmful, dangerous, or offensive). Each conversation is validated by three workers. While 10 conversations received votes for incoherence, none achieved a majority vote indicating they were incoherent. We
refine all 10 conversations. As for safety, no conversations were voted as being unsafe. We also request workers to verify the answers provided for BeliefQ ${ }_{\text {[Choice] }}$ s. We remove all question sets that were marked as erroneous by the worker $(\sim 8.6 \%)$.</p>
<p>Statistics FANToM is composed of 256 conversations with 1,415 BeliefQ ${ }<em _Choice_="{[Choice]" _text="\text">{\text {[Dist.] }}$ s and BeliefQ ${ }</em>}}$ s, 703 FactQs, AnswerABILITY $\mathrm{Q<em _List_="{[List]" _text="\text">{\text {[List] }}$ s, and InfoAccess $\mathrm{Q}</em>}}$ s, respectively. Additionally, there are 2,689 ANSWERABILITY $\mathrm{Q<em _mathrm_Y="[\mathrm{Y">{[\mathrm{Y} / \mathrm{N}]}$ S and InfoAccess $\mathrm{Q}</em>} / \mathrm{N}]}$ s. Given that the ANSWERABILITY $\mathrm{Q<em _mathrm_Y="[\mathrm{Y">{[\mathrm{Y} / \mathrm{N}]}$ S and InfoAccess $\mathrm{Q}</em>$ s iterate over all characters present in the conversations, they have the highest count among all the question types.} / \mathrm{N}]</p>
<p>The average number of turns in the input context is 13.8 (short conversation), and the average number of words in each turn is 21.9 . For reference, the corresponding statistics for ToMi (Le et al., 2019) are 4.9 and 4.2 , respectively. More statistics can be found in Appendix A.5.</p>
<h2>4 Experiments</h2>
<p>Baseline Models We test a total of thirteen recent instruction-tuned neural language models: GPT-4 (gpt-4-0613 and gpt-4-0314; OpenAI, 2023), ChatGPT (gpt-3.5-turbo-0613; OpenAI, 2022), InstructGPT (davinci-003 and curie-001; Ouyang et al., 2022), Flan-T5-XL and Flan-T5-XXL (Chung et al., 2022), Flan-UL2 (Tay et al., 2023), Falcon Instruct (7B and 40B; Almazrouei et al., 2023), Mistral Instruct 7B (Jiang et al., 2023), Zephyr 7B (HuggingFace, 2023), and Llama-2 Chat 70B (Touvron et al., 2023). Descriptions for each model are in Appendix B.</p>
<p>Although our benchmark is not meant for training, we also fine-tune Flan-T5-XL (Chung et al., 2022) by randomly splitting FANToM according to the conversation's main topics. We then test the model on unseen conversation topics. More details can be found in Appendix B.</p>
<p>Human Performance We also measure human performance by asking graduate students in computer science. We ask BeliefQ ${ }<em _List_="{[List]" _text="\text">{\text {[Choice] }}$, ANSWERABILITY $\mathrm{Q}</em>}}$, and InfoAccess $\mathrm{Q<em _List_="{[List]" _text="\text">{\text {[List] }}$, given a conversation. As it is redundant to ask human testees binary questions when they have already been asked ANSWERABILITY $\mathrm{Q}</em>}}$ and InfoAccess $\mathrm{Q<em _mathrm_Y="[\mathrm{Y">{\text {[List] }}$, we do not ask ANSWERABILITY $\mathrm{Q}</em>$} / \mathrm{N}]</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of BELIEFQ[CHOICE], ANSWERABILITY Q[LIST] and INFOACCESS Q[LIST], given the short conversation context. Full results with all models, input types, and metrics are in Table 9.</p>
<p>and INFOACCESS Q[Y/N]. To ensure a fair comparison with the models, we give the same instructions to humans and no other tutorials, examples, or extra instructions were given. Student volunteers solved 32 sets in total.</p>
<p><strong>Metrics</strong> We report accuracy for BELIEFQ[DIST.], BELIEFQ[CHOICE], ANSWERABILITY Q[LIST], and INFOACCESS Q[LIST]. The weighted F1 scores are reported for ANSWERABILITY Q[Y/N] and INFOACCESS Q[Y/N]. We additionally report the "<em>All</em>" score for the ANSWERABILITY Q and INFOACCESS Q requiring models to be correct on both list-type and binary-type questions. For BELIEFQ[DIST.] and FACTQ, we also report the token F1 scores to measure the word overlap between the answer and model's free-form response.</p>
<p>Moreover, we report the ALL<em> score which requires the models to answer all six ToM question types (§3.3) in the set correctly for the same information piece in the conversation. This metric aims to measure how well the models show consistent understanding across different types of questions. To compare with human performance, we also report the ALL score, which only excludes the BELIEFQ[DIST.] from the ALL</em> score.</p>
<h3>4.1 Results</h3>
<p>All the models exhibit scores that are significantly worse than human performance. Table 9 shows the full results of state-of-the-art large language models (LLMs) on FANTOM. We break down the table and highlight each discussion point below.</p>
<p><strong>Illusory Theory of Mind</strong> Figure 2 shows the results of a few selected models. We find models perform significantly better on BELIEFQ[CHOICE] compared to ANSWERABILITY Q[LIST] and INFOACCESS Q[LIST]. Despite the ANSWERABILITY Q[LIST] and INFOACCESS Q[LIST] being prerequisites for solving BELIEFQ[CHOICE], they are much more challenging for models. Furthermore,</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>All Question Types</th>
<th>All Answerability Qs [List + Y/N]</th>
<th>All InfoAccess Qs [List + Y/N]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human</td>
<td>87.5</td>
<td>90.6</td>
<td>90.6</td>
</tr>
<tr>
<td>Mistral Instruct + CoT</td>
<td>0.1</td>
<td>2.4</td>
<td>9.1</td>
</tr>
<tr>
<td>Falcon Instruct + CoT</td>
<td>0.0</td>
<td>1.7</td>
<td>2.3</td>
</tr>
<tr>
<td>Llama-2 Chat + CoT</td>
<td>0.4</td>
<td>6.0</td>
<td>7.8</td>
</tr>
<tr>
<td>ChatGPT 0613 + CoT</td>
<td>3.7</td>
<td>20.7</td>
<td>17.1</td>
</tr>
<tr>
<td>GPT-4 0613 + CoT (Jun)</td>
<td>26.6</td>
<td>40.2</td>
<td>57.7</td>
</tr>
<tr>
<td>GPT-4 0613 + CoT (Oct)</td>
<td>14.8</td>
<td>31.4</td>
<td>41.1</td>
</tr>
<tr>
<td>Flan-T5 XL + FT</td>
<td>53.7</td>
<td>55.9</td>
<td>54.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of models with zero-shot chain-of-thought (CoT) and fine-tuning (FT) for the short conversation context. Full results with all models, input types, and metrics are in Table 9.</p>
<p>Models' performance sharply drops when evaluated for coherent reasoning across multiple question types with the same underlying theory of mind (ToM) reasoning (i.e., <em>All Question Types</em>). These findings suggest that some instances of successful LLM ToM reasoning in FANTOM should be interpreted as illusory.</p>
<p><strong>Chain-of-thought and Fine-tuning</strong> Table 1 summarizes the results when we apply zero-shot chain-of-thought (CoT) reasoning or fine-tuning to models. For CoT, we follow Kojima et al. (2022) and use the prompt "<em>let's think step by step</em>". We observe an improvement in scores with CoT applied. However, there are still significant score gaps compared to human performance.</p>
<p>We also find fine-tuned Flan-T5 XL still falls short of human performance in metrics that demand consistent accuracy across multiple questions—i.e., the <em>All</em> scores. Although our benchmark is not intended for training purposes, developing models with a coherent ToM reasoning remains challenging, even with explicit training on the data.</p>
<p><strong>Comprehending Facts vs. Distinct Beliefs</strong> Figure 3 shows the token F1 scores for FACTQ and ac-</p>
<p>^{3}We find fine-tuning achieves scores comparable with human performance on individual question types (see Table 9).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results of FACTQ and BELIEFQ[DIST.] for models given the short conversation context. Full results with all models, input types, and metrics are in Table 9.</p>
<p>The results of ELIEFQ[DIST.] and F1 scores for FACTQ can be seen as a measure of a model's basic comprehension capability for interactions. Scoring high in FACTQ indicates the model is good at identifying the most relevant information piece to answering the question. Despite its small size, Mistral Instruct 7B shows the strongest performance among the open-source models.</p>
<p>On the other hand, BELIEFQ[DIST.] aims to measure a model's understanding of individual characters' perspective of a particular information—i.e., belief. To meet the <em>mentalizing</em> criterion (see §2.2), we deliberately design the incorrect answers in BELIEFQ[DIST.] to have greater word overlap with the context than correct answers. Also, BELIEFQ[DIST.] are rephrased questions inquiring about PersonX's belief for the facts in FACTQ, thereby the two question types share significant word overlap. However, the same information that was used to answer FACTQ should not be included in the response for BELIEFQ[DIST.] on PersonX as it is from the conversation that PersonX missed. As a result, certain models with higher token F1 scores for FACTQ have lower scores for BELIEFQ[DIST.] compared to models that perform worse on FACTQ (e.g., InstructGPT davinci-003 vs. Llama-2 Chat and Mistral Instruct). This suggests the models lack the ability to comprehend distinct perspectives of individual characters, leading them to reproduce similar responses to FACTQ for BELIEFQ[DIST.].</p>
<p><strong>Free-Response vs. Choice</strong> We observe a pattern where models score significantly worse in free-response questions than choice questions (BELIEFQ[DIST.] vs. BELIEFQ[CHOICE]; Figure 3 and 2).^{4} However, many of them still achieve scores either below or around 50, which is the random baseline for those binary choice questions.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>AnswerabilityQs [Y/N]</th>
<th>InfoAccessQs [Y/N]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mistral Instruct 7B</td>
<td>61.5</td>
<td>70.4</td>
</tr>
<tr>
<td>Falcon Instruct 40B</td>
<td>59.4</td>
<td>72.2</td>
</tr>
<tr>
<td>Llama-2 Chat 70B</td>
<td>61.4</td>
<td>80.4</td>
</tr>
<tr>
<td>InstructGPT davinci-003</td>
<td>67.0</td>
<td>78.4</td>
</tr>
<tr>
<td>ChatGPT 0613</td>
<td>64.2</td>
<td>73.2</td>
</tr>
<tr>
<td>GPT-4 0314</td>
<td>64.0</td>
<td>76.3</td>
</tr>
<tr>
<td>GPT-4 0613 (June)</td>
<td>85.9</td>
<td>90.3</td>
</tr>
<tr>
<td>GPT-4 0613 (October)</td>
<td>75.7</td>
<td>91.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of ANSWERABILITY Q[Y/N] and INFOACCESS Q[Y/N] when given the short conversation context. Full results with all models, input types, and metrics are in Table 9.</p>
<p><strong>Reasoning Complexity</strong> Table 2 compares models' performance between ANSWERABILITY Q[Y/N] and INFOACCESS Q[Y/N]. As ANSWERABILITY Qs require an additional step of reasoning compared to INFOACCESS Qs, models consistently perform worse on ANSWERABILITY Q[Y/N] compared to INFOACCESS Q[Y/N]. However, this pattern is not consistent across models for ANSWERABILITY Q[LIST] and INFOACCESS Q[LIST] (see Figure 2). This may be because models significantly struggle with ANSWERABILITY Q[LIST] and INFOACCESS Q[LIST], potentially resulting in the absence of meaningful performance patterns.</p>
<p><strong>Short vs. Full Conversations</strong> When a model is provided with the full conversation (Table 9, bottom), its performance noticeably decreases compared to when it is given only the relevant parts of the conversation (Table 9, top). The decrease can be attributed to the model's need to identify the relevant information within the full conversation, whereas it does not have to do so for the short conversations. This indicates theory of mind reasoning becomes even more challenging for models when it needs to be combined with different types of reasoning (e.g., search).</p>
<h3>4.2 In-depth Analysis</h3>
<p><strong>What types of errors do models make?</strong> Figure 4 and 5 summarize the error types of ANSWERABILITY Q and INFOACCESS Q for each model with and without chain-of-thought (CoT) reasoning. For list-type questions, models make more errors by including characters who are unaware of the information in the responses, rather than excluding characters who are aware. Interestingly, when CoT is applied, the error of including unaware characters decreases, whereas the error of excluding characters who are aware increases for most models.</p>
<p>^{4}This pattern is consistent for ANSWERABILITY Q[LIST] and ANSWERABILITY Q[Y/N], as well as for INFOACCESS Q[LIST] and INFOACCESS Q[Y/N] (see Table 9).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Analysis of model errors for ANSWERABILITY $\mathrm{Q}<em I="I" S="S" T_="T]" _L="[L">{[L I S T]}$ and INFOACCESS $\mathrm{Q}</em>$.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Analysis of model errors for ANSWERABILITY $\mathrm{Q}<em N_="N]" _="/" _Y="[Y">{[Y / N]}$ and INFOACCESS $\mathrm{Q}</em>$.</p>
<p>In the case of binary questions, false positives and false negatives correspond to including characters who are unaware and excluding characters who are aware in the response for list-type questions, respectively. If the model fails to generate a yes or no response, we mark it as irrelevant. Models tend to exhibit false negative responses more frequently for binary questions compared to listtype questions. Similarly, CoT primarily helps the model in reducing the false positive error rates, but the reduction in false negative error rates is not consistent across models. This suggests that CoT selectively improves reasoning specifically for determining characters who are unaware of the information, rather than characters who are aware.</p>
<p>How accurate and consistent are models' answers for a given character? For accuracy, we report the All for Each Character score which is determined by whether the models are able</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">All for <br> Each <br> Character</th>
<th style="text-align: center;">Answer <br> Consistency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mistral Instruct</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: left;">Mistral Instruct + CoT</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">41.9</td>
</tr>
<tr>
<td style="text-align: left;">Falcon Instruct 40B</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">19.1</td>
</tr>
<tr>
<td style="text-align: left;">Falcon Instruct 40B + CoT</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 Chat 70B</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 Chat 70B + CoT</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT davinci-003</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">55.2</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT davinci-003 + CoT</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">58.4</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT 0613</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">51.6</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT 0613 + CoT</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 (June)</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">66.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 + CoT (June)</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">73.4</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 (October)</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">62.2</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 + CoT (October)</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">66.9</td>
</tr>
</tbody>
</table>
<p>Table 3: The accuracy and consistency (\%) of the models' responses for each character within the given conversation context.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">First-Order</th>
<th style="text-align: center;">Second-Order</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Cyclic</td>
<td style="text-align: center;">Acyclic</td>
</tr>
<tr>
<td style="text-align: left;">Mistral Instruct</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">25.1</td>
</tr>
<tr>
<td style="text-align: left;">Mistral Instruct + CoT</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: left;">Falcon Instruct 40B</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">39.6</td>
</tr>
<tr>
<td style="text-align: left;">Falcon Instruct 40B + CoT</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">77.2</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 Chat</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 Chat + CoT</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT davinci-003</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT davinci-003 + CoT</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">17.3</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT 0613</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT 0613 + CoT</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 (June)</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">67.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 + CoT (June)</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 (October)</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">62.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613 + CoT (October)</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">65.4</td>
</tr>
</tbody>
</table>
<p>Table 4: BeliefQ results for first and second order ToM beliefs.
to answer all six types of ToM questions correctly regarding the specific character. For consistency, we measure the ratio of consistent model responses across ANSWERABILITY Q and INFOACCESS Q for each character. Table 3 shows the accuracy and consistency of the models' responses for each character within the given conversation context. Overall, we observe a pattern where models that score low in accuracy also show low consistency. While CoT generally improves model performance (see Table 9), we find that it does not always lead to improved accuracy and consistency. The decrease in All for Each Character score when CoT is applied suggests that CoT has a selective impact on different question types.</p>
<p>Are there differences in performance in terms of the order of ToM beliefs? Table 4 presents the results of BeliefQ with respect to different</p>
<p>orders of ToM beliefs. Similar to Le et al. (2019), models perform better on the second-order belief questions than those with first-order beliefs. To further investigate the performance on second-order belief questions, we analyze the results based on the cyclic and acyclic patterns in them. The cyclic second-order belief questions inquire about Character 1's belief regarding Character 2's belief about Character 1 (e.g., What does Linda think about Kailey's belief on the breed of Linda's dog?); while the acyclic second-order questions focus on Character 1's belief about Character 2's belief regarding Character 3 (e.g., What does David think about Kailey's belief on the breed of Linda's dog?). Models show better performance on the cyclic questions than acyclic ones, which include more characters to track. However, when CoT is applied, the increase in score for acyclic questions is greater than that of cyclic ones, suggesting CoT helps multi-tracking.</p>
<h2>5 Related Work</h2>
<p>Existing Theory of Mind Benchmarks Many theory of mind (ToM) benchmarks evaluate models on false beliefs with narratives (Grant et al., 2017; Nematzadeh et al., 2018; Le et al., 2019; Gandhi et al., 2023; Zhou et al., 2023). Other works such as Shapira et al. (2023b) build benchmarks based on the Faux Pas Test (Baron-Cohen et al., 1999). Also, ToM-related benchmarks focus on reasoning emotions and mental states in narratives (Rashkin et al., 2018; Sap et al., 2019).</p>
<p>Theory of Mind in Large Language Models Although qualitative assessments might imply a degree of ToM in large language models (LLMs; Whang, 2023), more comprehensive quantitative investigations reveal that they have yet to achieve human-level ToM across various benchmarks (Sap et al., 2022; Shapira et al., 2023a). LLMs struggle to reason ToM robustly (Ullman, 2023), though their performance can be improved through fewshot samples and chain-of-thought prompting (Sap et al., 2022; Moghaddam and Honey, 2023) as well as specific inference methods (Sclar et al., 2023).</p>
<h2>6 Conclusion \&amp; Discussion</h2>
<p>We introduced 2 FANTOM, a new benchmark for stress-testing theory of mind (ToM) capabilities of neural language models in conversations via question answering. Our benchmark is built upon essential theoretical requisites and empirical considerations required for validating ToM in large language models (LLMs). The conversations in our benchmark involve information asymmetry, with characters joining and leaving the discussion while it continues, to simulate distinct mental states. To identify illusory ToM, we crafted multiple types of challenging belief questions regarding the conversation participants' mental states by converting factual questions. Our evaluation results show that coherent ToM reasoning is challenging for current LLMs, performing significantly worse than humans even when using chain-of-thought reasoning or fine-tuning.</p>
<p>Although there has been recent debates around whether current LLMs possess ToM capabilities or not (Whang, 2023), our results indicate that this capacity has not yet emerged in any manner. Previous instances of success on well-known psychology ToM tests may be attributed to exposure during the pretraining phase (Ullman, 2023). Our work highlights the need for novel interaction-oriented benchmarks that introduce scenarios not encountered during training, and also aligning more closely with real-world use cases as LLMs are increasingly being deployed in interactive settings.</p>
<p>Our results also shed light on a broader issue in neural models - the lack of internal consistency (Elazar et al., 2021). We find they often fail to provide consistent answers to questions requiring the same underlying ToM reasoning. To address this concern, future works can explore various directions, such as grounding reasoning in pragmatics (Kim et al., 2020), visual information (Bisk et al., 2020), or belief graphs (Sclar et al., 2023).</p>
<p>Another issue that our work touches upon is the reporting biases inherent in language models. We observed that models often exhibit biases in their responses, showing a tendency to overly rely on the information they are conditioned on, such as preferring answers that have high overlap with the context (Sugawara et al., 2018). However, to achieve successful ToM reasoning, it is crucial to distinguish between accessible and inaccessible information for a particular agent, rather than blindly using all information available to the model. One potential approach to mitigate this is to combine pretraining with interactive learning (Sap et al., 2022).</p>
<p>In the spirit of encouraging future research in this direction, we make our benchmark publicly available at https://hyunw.kim/fantom.</p>
<h2>7 Limitations</h2>
<p>Although FANTOM is the first benchmark, to the best of our knowledge, to cover theory of mind (ToM) reasoning in conversational interactions, it is currently limited to small talks on specific topics. Additionally, our benchmark only considers only a single type of relationship between conversation participants, where they do not have prior knowledge of each other. However, social reasoning can become much more dynamic when variables such as relationships (e.g., family, friends, co-workers) are introduced. ToM is essential in all conversational interactions, hence we strongly encourage future works to evaluate ToM in a wider range of diverse conversation scenarios.</p>
<p>Our evaluation solely focuses on language-based models. However, it is important to note that ToM extends beyond a single modality (Piaget, 1956; Wu and Keysar, 2007). For instance, the wellknown Sally-Anne test (Wimmer and Perner, 1983; Baron-Cohen et al., 1985) is typically conducted as a face-to-face experiment, where visual cues affect the performance of the participants. Therefore, interesting future work will involve examining the capabilities of multi-modal models in relation to ToM reasoning.</p>
<p>Lastly, as we generate full conversations with large language models, conversations may contain offensive contents (Weidinger et al., 2021). However, we specifically select casual topics for small talks (e.g., pets, personal growth, traveling) to minimize the likelihood of offensive content generation. Also, we manually validate all conversations in our benchmark with crowdworkers from Amazon Mechanical Turk.</p>
<h2>8 Societal and Ethical Considerations</h2>
<p>We acknowledge that the term "theory of mind" (ToM) may evoke anthropomorphic connotations regarding AI models. However, we emphasize that the purpose of our work is not to promote anthropomorphism of AI models. Rather, our focus lies in exploring the limitations of existing language models in social reasoning. While the concept of ToM attempts to capture the ability to attribute mental states to oneself and others (Premack and Woodruff, 1978), it is important to clarify that AI models do not possess subjective consciousness or true understanding of intentions, beliefs, or desires. Our experiment results also demonstrate that current large language models do not exhibit any
coherent ToM reasoning; instead, they primarily rely on word correlations.</p>
<h2>Acknowledgement</h2>
<p>We thank the participants who contributed to the human performance measurement. We also appreciate our colleagues on the Beaker Team at the Allen Institute for AI for helping with the compute infrastructure. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031). Hyunwoo Kim and Gunhee Kim are supported by the Institute of Information \&amp; communications Technology Planning \&amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-01082, SW StarLab; and No.2022-0-00156, Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation). Lastly, we also thank OpenAI, as well as Google Cloud Compute.</p>
<h2>References</h2>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.</p>
<p>Simon Baron-Cohen, Alan M Leslie, and Uta Frith. 1985. Does the autistic child have a "theory of mind"? Cognition, 21(1):37-46.</p>
<p>Simon Baron-Cohen, Michelle O'riordan, Valerie Stone, Rosie Jones, and Kate Plaisted. 1999. Recognition of faux pas by normally developing children and children with asperger syndrome or high-functioning autism. Journal of autism and developmental disorders, 29(5):407-418.</p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience grounds language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8718-8735, Online. Association for Computational Linguistics.</p>
<p>Torben Braüner, Patrick Blackburn, and Irina Polyanskaya. 2020. Being deceived: Information asymmetry in second-order false belief tasks. Topics in Cognitive Science, 12(2):504-534.</p>
<p>Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. PLACES:</p>
<p>Prompting language models for social conversation synthesis. In Findings of the Association for Computational Linguistics: EACL 2023, pages 844-868, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Shengding Hu, Yujia Qin, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Ultrachat: A large-scale auto-generated multi-round dialogue data. https://github.com/ thunlp/ultrachat.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031.</p>
<p>Uta Frith. 1994. Autism and theory of mind in everyday life. Social development, 3(2):108-124.</p>
<p>Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah D Goodman. 2023. Understanding social reasoning in language models with language models. arXiv preprint arXiv:2306.15448.</p>
<p>Jonathan Gordon and Benjamin Van Durme. 2013. Reporting bias and knowledge acquisition. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25-30.</p>
<p>Erin Grant, Aida Nematzadeh, and Thomas L Griffiths. 2017. How can memory-augmented neural networks pass a false-belief task? In $\operatorname{CogSci}$.</p>
<p>David Gros, Yu Li, and Zhou Yu. 2022. Robots-dontcry: Understanding falsely anthropomorphic utterances in dialog systems. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3266-3284, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>HuggingFace. 2023. Zephyr 7b alpha.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2022. Soda: Million-scale dialogue distillation with social commonsense contextualization. ArXiv, abs/2212.10465.</p>
<p>Hyunwoo Kim, Byeongchang Kim, and Gunhee Kim. 2020. Will I sound like me? improving persona consistency in dialogues through pragmatic selfconsciousness. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 904-916, Online. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213.</p>
<p>Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5872-5877, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Cade Metz. 2023. Microsoft says new A.I. shows signs of human reasoning. The New York Times.</p>
<p>Shima Rahimi Moghaddam and Christopher J Honey. 2023. Boosting theory-of-mind performance in large language models via prompting. arXiv preprint arXiv:2304.11490.</p>
<p>Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Tom Griffiths. 2018. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392-2400, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.</p>
<p>OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training Language Models to Follow Instructions with Human Feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.</p>
<p>Jean Piaget. 1956. Child's Conception of Space. Routledge.</p>
<p>David Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515-526.</p>
<p>François Quesque and Yves Rossetti. 2020. What do theory-of-mind tasks actually measure? theory and practice. Perspectives on Psychological Science, 15(2):384-396.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, and Yejin Choi. 2018. Modeling naive psychology of characters in simple commonsense stories. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2289-2299, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106.</p>
<p>Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762-3780, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Michael F Schober. 2005. Conceptual alignment in conversation. Other minds: How humans bridge the divide between self and others, pages 239-252.</p>
<p>Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. 2023. Minding language models' (lack of) theory of mind: A plug-andplay multi-character belief tracker. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13960-13980, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023a. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763.</p>
<p>Natalie Shapira, Guy Zwirn, and Yoav Goldberg. 2023b. How well do large language models perform on faux pas tests. In Findings of the Association for Computational Linguistics: ACL 2023.</p>
<p>Saku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. 2018. What makes reading comprehension questions easier? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208-4219, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. 2023. Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.</p>
<p>Albert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for</p>
<p>Computational Linguistics: Human Language Technologies, pages 2300-2344, Seattle, United States. Association for Computational Linguistics.</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.</p>
<p>Oliver Whang. 2023. Can a machine know that we know what it knows? The New York Times.</p>
<p>Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Cognition, 13(1):103-128.</p>
<p>Shali Wu and Boaz Keysar. 2007. The effect of culture on perspective taking. Psychological science, 18(7):600-606.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.</p>
<p>Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R McKee, Ari Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, et al. 2023. How far are large language models from agents with theory-of-mind? arXiv preprint arXiv:2310.03051.</p>
<h2>A FANToM Construction</h2>
<p>Full examples of question sets in FANToM can be found in Table 5 and Table 6.</p>
<h2>A. 1 Generating Conversations with Information Asymmetry</h2>
<p>Information-asymmetric conversations To create the conversations in our benchmark, we use a predefined set of subtopics for each main topic and employ templates to generate scripts. For example, for the topic "pets" subtopics may include "breed", "special moves", and "favorite food". Following Kim et al. (2022), we use specific speaker prefixes with English names sampled from the Top-1K names in the US SSN database for more natural conversations. We append each utterance with speaker prefixes. We randomly shuffle the subtopics for each topic and generate conversations for each subtopic. We generate the first conversation with the following prompt: "{Character 1}, {Character 2}, ... {Character n} met for the first time at this social event. They are having a conversation on their {topic}. They now discuss
{subtopic}. \n{Character 1}:" The initial conversation starts with two or three characters and there can be up to five characters who are participating in the conversation at the same time.</p>
<p>Then, for each subtopic, we randomly select characters to join or leave the conversation. We use the following prompt when a character is selected to leave: "Now, {leaving character} leaves the conversation because of the reason '{leaving reason}'. They now discuss {subtopic}. Remember to indicate that {leaving character} is leaving the conversation. {Conversation history} \n{leaving character}:". We use a predefined list of 64 reasons for leaving the conversation. Table 7 shows all reasons for leaving. We append the previous conversation history to the input prompt to make the conversation continue from the previous one.</p>
<p>We use the following prompt when a character is selected to join: "Now {joining character} comes back after leaving the conversation because of the reason {leaving reason}. They now discuss {subtopic}. Remember to indicate that {joining character} is joining the conversation. Do not mention the details in the previous conversations. {Conversation history} \n{joining character}:".</p>
<p>Extracting the inaccessible information for PersonX Whenever a character (re)joins the conversation, we extract the inaccessible information by asking GPT-4 (gpt-4-0314) what information was shared in the preceding conversation where the character PersonX did not participate. We provide the previous conversation and the current one as input to GPT-4 with the prompt "What information was shared before PersonX joined, but was not mentioned after PersonX joined?" appended to it. To ease the task, the joining of the character is explicitly denoted by inserting a script between the conversations, as follows: "Previous conversation\n[PersonX joined the conversation]\nCurrent conversation". We observe quality improvements for the output generated by GPT-4 with the inclusion of the hint script. The returned result can be viewed as a conversation summary explicitly covering the previous context.</p>
<p>Sabrina: So, what was the most challenging workout experience you ever had?
Anna: Definitely when I decided to try out CrossFit. I'm not going to lie, it kicked my butt!
Sabrina: Wow, that sounds intense. What kind of exercises did you do?
Anna: We did a lot of different things like high intensity interval training and Olympic lifting with barbells and dumbbells.
Sabrina: That definitely takes dedication! How did you stay motivated during it?
Anna: It was tough but I kept reminding myself why I wanted to get fit in the first place and that helped me stay focused on my goals.
Gina: Hey, I'm back! What were you guys talking about?
Sabrina: We were just discussing our most challenging workout experiences. What do you think about when it comes to making a good workout playlist?
Anna: Music is really important when it comes to getting in the zone while working out. For me, I like upbeat and energetic songs that get me going.
Gina: Yeah, something with a high tempo can really help push you during those tough workouts! I also like adding in some of my favorite classic songs that give me extra motivation to keep going.</p>
<p>FACTQ: In what ways did Anna motivate herself when faced with difficult workouts?</p>
<p>BeliefQ: What does Gina believe are the ways in which Anna motivated herself when faced with difficult workouts? OMNISCIENT-VIEW Belief A: Gina believes that Anna motivated herself by reminding herself why she wanted to get fit in the first place and staying focused on her goals. PersonX-CENTRIC Belief A: Gina believes that Anna motivated herself by listening to upbeat and energetic songs during her workouts.</p>
<p>ANSWERABILITY $\mathbf{Q}<em _List_="{[List]" _text="\text">{\text {[List] }}$ : List all the characters who know the correct answer to this question.
InfoAccess $\mathbf{Q}</em>$ : List all the characters who know this information.
Answer: Sabrina, Anna
ANSWERABILITY $\mathbf{Q}}<em N_="N]" _="/" _Y="[Y">{[Y / N]}$ : Does Sabrina know the correct answer to this question?
InfoAccess $\mathbf{Q}</em>$ : Does Sabrina know this information? Answer: Yes</p>
<p>Table 5: A sample from 2 FANTOM.</p>
<h2>A. 2 Generating Factual QA Pairs</h2>
<p>We construct factual question-answer (QA) pairs related to the inaccessible information. First, we generate three non-yes-or-no questions and denote these as "FACTQs" and obtain them by prompting GPT-4, given the inaccessible information text. We obtain "FACTQs" by prompting GPT-4 with the following: "{inaccessible information}\n\nBased on this, formulate three non-yes-or-no questions that can be answered by this conversation summary."</p>
<p>Next, we generate two distinct types of answers for each FACTQ with GPT-4. (1) First, we gener-
ate an answer denoted as "FULL FACT A", which is based on the preceding conversation where PersonX was absent. This answer incorporates the full information by providing GPT-4 with the previous conversation - i.e., the source of the inaccessible information for PersonX. (2) Second, we generate another answer referred to as "LIMITED FACT A", which relies only on the conversation where PersonX participated. In this case, we give GPT-4 the PersonX-participating conversation along with the FACTQ. We prompt GPT-4 with the following: "{context}\n\nQuestion: {FACTQ } $\backslash$ nAnswer:"</p>
<h2>A. 3 Constructing Belief QAs with Factual QAs</h2>
<p>BeliefQ ${ }<em _Choice_="{[Choice]" _text="\text">{\text {[Dist.] }}$ and BeliefQ ${ }</em>$ We first convert FACTQs into first-order or second-order ToM questions asking about beliefs of characters in the conversation. We are particularly interested in PersonX's belief or knowledge about the inaccessible information from the previous conversation, in which PersonX did not participate. We prompt GPT-4 with the following: "{FACTQ } $\backslash$ n \nConvert this into a theory of mind question asking {character name}'s belief about this."}</p>
<p>Next, we convert the Full Fact As and LiMITEd FACT As into answers about beliefs. Since the Full Fact As reflect information that is not accessible to PersonX and the LiMited FACT A incorporates only the information accessible to PersonX, we label the converted Full Fact A and Limited Fact A as "OmnisCient-view Belief A" and "PersonX-CENTRIC Belief A", respectively. For the conversion, we prompt GPT-4 with the following format: "Question: FACTQ \n\nAnswer the question using the following sentence. {Full Fact A or LiMITEd FACT A $} \backslash$ nAnswer:".</p>
<h2>A. 4 Evaluation for ANSWERABILITY $\mathbf{Q}<em N_="N]" _="/" _Y="[Y">{[Y / N]}$ and InfoAccess $\mathbf{Q}</em>$</h2>
<p>We use pattern matching to parse the yes or no answers from model responses. We regard "yes", "knows", "does know", and "true" as responses representing "yes". Similarly, we regard "no", "does not know", "doesn't know", and "false" as responses representing " $n o$ ".</p>
<h2>A. 5 Statistics for 2 FANToM</h2>
<p>Table 8 compares the basic statistics of FANToM and ToMi (Le et al., 2019).</p>
<h2>B Experiments</h2>
<p>Human performance evaluation A total of 11 student volunteers participated in the evaluation. For each question set, we assign a single testee. They solved a total of 32 sets. To ensure a fair comparison, no additional tutorials, examples, or extra instructions were provided beyond what was given to the models.</p>
<p>Baseline models The GPT models are proprietary models from OpenAI based on the decoderonly transformer architecture. Flan-T5 and FlanUL2 are open-source (i.e., Apache 2.0) models from Google trained on instruction-phrased datasets. They are based on the encoder-decoder transformer architecture. Falcon Instruct is another open-source (i.e., Apache 2.0) model trained on RedefinedWeb (Penedo et al., 2023) and Baize (Xu et al., 2023). Llama-2 Chat (Touvron et al., 2023) is a fine-tuned 70B large language model, optimized for following user requests in dialogue format. Mistral Instruction (Jiang et al., 2023) is a 7B language model fine-tuned to follow instructions, which is reported to surpass the Llama-2 Chat 13B model. Zephyr (HuggingFace, 2023) is a model based on Mistral, further fine-tuned on UltraChat (Ding et al., 2023) and aligned with UltraFeedback (Cui et al., 2023).</p>
<p>Results of other models Table 9 shows the results for other large language models not included in Figure 2. Given the random baseline score is 50 for BeliefQ $<em _mathrm_Y="[\mathrm{Y">{\text {[Choice] }}$, Answerability $\mathrm{Q}</em>$, most of the models show low performance on our benchmark.} / \mathrm{N}]}$, and InfoACCESS $\mathrm{Q}_{[\mathrm{Y} / \mathrm{N}]</p>
<p>Fine-tuning details We fine-tune Flan-T5XL with learning rate=2e-5 and weight decay=0.01, evaluating per epoch and using early stopping with patience 1 (batch size $=3$ for Flan-T5-XL). We observe an increase in validation loss after the first epoch. We also add special tokens before and after the completions to prevent the model from over-generating, which we find in early experiments. We also fine-tune text-curie-001 (Ouyang et al., 2022) for two epochs using standard parameters from the OpenAI API.</p>
<p>Zachary: Have you guys thought about how much money you'll need for retirement?
Hazel: I'm still trying to figure that out. I know it's important to save, but it can be hard when there are so many other expenses.
Cory: Yeah, that's true. It's also good to plan for how you want your lifestyle to look when you retire too. Will you travel or stay close to home?
Hazel: That's a tough one! I think a combination of both might work best for me - travel while we're younger and then spend more time closer to family as we get older.
Zachary: Sounds like a great plan! What kind of investments have each of you made?
Cory: Well, I've started investing in some mutual funds and my $401(\mathrm{k})$. How about the two of you?
Hazel: I've been contributing regularly to my IRA and also investing in index funds. Zachary?
Zachary: Same here - mutual funds and an IRA with occasional stock investments as well.
Hazel: Sorry guys, but my pet needs attention. I need to go check on him.
Cory: Oh, okay. Have a good one!
Hazel: See you later.
Cory: Well, now that Hazel is gone, we can talk about the psychology of retirement. What are some of the emotional aspects to consider when planning for retirement?
Zachary: It can be hard to think about the future and envision what life will be like in retirement without having any prior experience with it. People also tend to underestimate how much money they'll need for a comfortable lifestyle and overestimate their ability to save for it over time.
Alec: Hey, I'm back! What were you guys talking about?
Cory: We were discussing retirement planning. Zachary was just mentioning how difficult it can be to think about the future and envision what life will be like in retirement without having any prior experience with it.
Alec: Yeah, that's true. It's important to save as much as possible now so you have enough money for retirement later. Have you guys started investing yet?
Zachary: Yes, we both have investments in mutual funds and IRAs, and Cory has a $401(\mathrm{k})$. How about you Alec?
Alec: I've been saving some money into a savings account and I'm looking into investing in index funds soon. What other advice do you have for someone starting out with their retirement planning?
Cory: Do your research on different investment options available to figure out which ones are best for your goals. There are also some tax benefits associated with certain types of investments that could make them more attractive than others depending on your situation.</p>
<p>FACTQ: What kind of funds has Hazel been investing in?
BeliefQ: What does Alec believe are the types of funds Hazel has been investing in?
OmnisciENT-VIEW Belief A: Alec believes that Hazel has been investing in index funds and contributing regularly to her IRA.
PersonX-Centric Belief A: Alec does not know the types of funds Hazel has been investing in.
ANSWERABILITY $\mathbf{Q}<em _List_="{List]" _text="[\text">{[\text {List] }}$ : List all the characters who know the correct answer to this question.
InfoACCESS $\mathbf{Q}</em>$ : List all the characters who know this information.
Answer: Hazel, Zachary, Cory
ANSWERABILITY $\mathbf{Q}}<em _mathrm_Y="[\mathrm{Y">{[\mathrm{Y} / \mathrm{N}]}$ : Does Alec know the correct answer to this question?
InfoACCESS $\mathbf{Q}</em>$ : Does Alec know this information?
Answer: No
Table 6: Another sample from 2 FANToM.} / \mathrm{N}]</p>
<table>
<thead>
<tr>
<th style="text-align: left;">bathroom break</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">coffee break</td>
</tr>
<tr>
<td style="text-align: left;">forgot something important</td>
</tr>
<tr>
<td style="text-align: left;">forgot to print some documents</td>
</tr>
<tr>
<td style="text-align: left;">forgot to recieve a package</td>
</tr>
<tr>
<td style="text-align: left;">forgot to return a package</td>
</tr>
<tr>
<td style="text-align: left;">forgot to run errands</td>
</tr>
<tr>
<td style="text-align: left;">forgot to submit documents</td>
</tr>
<tr>
<td style="text-align: left;">have a meeting starting soon that I need to prepare for</td>
</tr>
<tr>
<td style="text-align: left;">have a previous engagement that I need to attend to quickly</td>
</tr>
<tr>
<td style="text-align: left;">have a work-related emergency that requires my immediate</td>
</tr>
<tr>
<td style="text-align: left;">attention</td>
</tr>
<tr>
<td style="text-align: left;">have an unexpected visitor at my door</td>
</tr>
<tr>
<td style="text-align: left;">have errands to run</td>
</tr>
<tr>
<td style="text-align: left;">have to attend to someone who just walked in</td>
</tr>
<tr>
<td style="text-align: left;">have to check on something</td>
</tr>
<tr>
<td style="text-align: left;">have to go to the restroom</td>
</tr>
<tr>
<td style="text-align: left;">have to pick up a prescription</td>
</tr>
<tr>
<td style="text-align: left;">have to pick up dry cleaning</td>
</tr>
<tr>
<td style="text-align: left;">have to print or scan documents</td>
</tr>
<tr>
<td style="text-align: left;">have to receive a delivery</td>
</tr>
<tr>
<td style="text-align: left;">have to recharge laptop</td>
</tr>
<tr>
<td style="text-align: left;">have to return a borrowed item</td>
</tr>
<tr>
<td style="text-align: left;">have to take care of a family matter</td>
</tr>
<tr>
<td style="text-align: left;">have to take care of an unexpected task</td>
</tr>
<tr>
<td style="text-align: left;">have unexpected visitor</td>
</tr>
<tr>
<td style="text-align: left;">his/her pet needs attention</td>
</tr>
<tr>
<td style="text-align: left;">his/her family is calling</td>
</tr>
<tr>
<td style="text-align: left;">incoming delivery</td>
</tr>
<tr>
<td style="text-align: left;">must respond to a phone call</td>
</tr>
<tr>
<td style="text-align: left;">need to check on a friend or family member who needs</td>
</tr>
<tr>
<td style="text-align: left;">assistance</td>
</tr>
<tr>
<td style="text-align: left;">need to finish a task that's time-sensitive</td>
</tr>
<tr>
<td style="text-align: left;">need to get a phone call</td>
</tr>
<tr>
<td style="text-align: left;">need to get some coffee</td>
</tr>
<tr>
<td style="text-align: left;">need to go to the toilet</td>
</tr>
<tr>
<td style="text-align: left;">need to grab a snack or a drink</td>
</tr>
<tr>
<td style="text-align: left;">need to have a quick chat with someone else</td>
</tr>
<tr>
<td style="text-align: left;">need to make a phone call</td>
</tr>
<tr>
<td style="text-align: left;">need to make a quick trip to the drug store</td>
</tr>
<tr>
<td style="text-align: left;">need to make a quick trip to the grocery store</td>
</tr>
<tr>
<td style="text-align: left;">need to pick up a package</td>
</tr>
<tr>
<td style="text-align: left;">need to receive a parcel</td>
</tr>
<tr>
<td style="text-align: left;">need to recharge cellphone</td>
</tr>
<tr>
<td style="text-align: left;">need to register for an event</td>
</tr>
<tr>
<td style="text-align: left;">need to schedule a haircut or salon appointment</td>
</tr>
<tr>
<td style="text-align: left;">need to schedule another appointment</td>
</tr>
<tr>
<td style="text-align: left;">need to step away for a moment to stretch and clear my mind</td>
</tr>
<tr>
<td style="text-align: left;">need to step out for a moment</td>
</tr>
<tr>
<td style="text-align: left;">need to submit some papers</td>
</tr>
<tr>
<td style="text-align: left;">need to take care of some paperwork or documents</td>
</tr>
<tr>
<td style="text-align: left;">need to take care of some personal matters</td>
</tr>
<tr>
<td style="text-align: left;">need to take care of something related to my health</td>
</tr>
<tr>
<td style="text-align: left;">need to take care of something urgent</td>
</tr>
<tr>
<td style="text-align: left;">need to troubleshoot something</td>
</tr>
<tr>
<td style="text-align: left;">parking meter expiring</td>
</tr>
<tr>
<td style="text-align: left;">remembered something that needs to be taken care of</td>
</tr>
<tr>
<td style="text-align: left;">remembered to receive a package</td>
</tr>
<tr>
<td style="text-align: left;">remembered to submit some papers</td>
</tr>
<tr>
<td style="text-align: left;">remembered to take care of some paperwork or documents</td>
</tr>
<tr>
<td style="text-align: left;">remembered to take care of some personal matters</td>
</tr>
<tr>
<td style="text-align: left;">remembered to take care of something urgent</td>
</tr>
<tr>
<td style="text-align: left;">want to go grab a drink</td>
</tr>
<tr>
<td style="text-align: left;">want to go grab a coffee</td>
</tr>
<tr>
<td style="text-align: left;">want to go take some fresh air</td>
</tr>
<tr>
<td style="text-align: left;">want to go to the bathroom</td>
</tr>
</tbody>
</table>
<p>Table 7: Predefined reasons for characters leaving the conversation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Total <br> #Questions</th>
<th style="text-align: center;">Avg. <br> #Questions <br> per Context</th>
<th style="text-align: center;">Avg. <br> #Turns <br> (Partial)</th>
<th style="text-align: center;">Avg. <br> #Turns <br> (Full)</th>
<th style="text-align: center;">Avg. <br> Turn <br> Length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ToMi</td>
<td style="text-align: center;">6 K</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">4.7</td>
</tr>
<tr>
<td style="text-align: left;">家 FANToM</td>
<td style="text-align: center;">10 K</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">21.9</td>
</tr>
</tbody>
</table>
<p>Table 8: Statistics of FANToM and ToMi (Le et al., 2019).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">All* <br> Question <br> Types</th>
<th style="text-align: center;">All <br> Question <br> Types</th>
<th style="text-align: center;">Belief Questions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answerability Questions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Info Access Questions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fact Questions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Choice</td>
<td style="text-align: center;">Dist.</td>
<td style="text-align: center;">TokenF1</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">List</td>
<td style="text-align: center;">Y/N</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">List</td>
<td style="text-align: center;">Y/N</td>
<td style="text-align: center;">TokenF1</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">22.9</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XXL</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">22.9</td>
</tr>
<tr>
<td style="text-align: center;">Flan-UL2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: center;">Mistral Instruct 7B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">56.6</td>
</tr>
<tr>
<td style="text-align: center;">Zephyr 7B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">55.0</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 7B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 40B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2 Chat 70B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT curie-001</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">47.3</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT davinci-003</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT 0613</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">59.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0314</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">77.6</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (June)</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">62.9</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (October)</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">64.9</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">21.5</td>
</tr>
<tr>
<td style="text-align: center;">Flan-UL2 + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: center;">Mistral Instruct 7B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">Zephyr 7B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 7B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">19.0</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 40B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">19.5</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2 Chat 70B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT curie-001 + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT davinci-003 + CoT</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">49.9</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT 0613 + CoT</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">53.4</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0314 + CoT</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">56.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (June) + CoT</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">54.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (October) + CoT</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">52.8</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT curie-001 + FT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL + FT</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">49.3</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">8.7</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XXL</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: center;">Flan-UL2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">10.6</td>
</tr>
<tr>
<td style="text-align: center;">Mistral Instruct 7B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">Zephyr 7B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 7B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">10.7</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 40B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2 Chat 70B</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">40.0</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT curie-001</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">46.1</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT davinci-003</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT 0613</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (June)</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">61.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (October)</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">8.9</td>
</tr>
<tr>
<td style="text-align: center;">Flan-UL2 + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">10.5</td>
</tr>
<tr>
<td style="text-align: center;">Mistral Instruct 7B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">Zephyr 7B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 7B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">17.2</td>
</tr>
<tr>
<td style="text-align: center;">Falcon Instruct 40B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">19.0</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2 Chat 70B + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">32.2</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT curie-001 + CoT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT davinci-003 + CoT</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">47.1</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT 0613 + CoT</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">51.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (June) + CoT</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613 (October) + CoT</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT curie-001 + FT</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">25.4</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL + FT</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">50.2</td>
</tr>
</tbody>
</table>
<p>Table 9: Zero-shot results from humans and large language models on 䇣 FANToM with the same instructions. CoT denotes chain-of-thought reasoning and FT denotes fine-tuning.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://hyunw.kim/fantom
${ }^{2}$ We do not believe that current LLMs possess an actual ToM. Please see $\S 8$ for further discussions.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
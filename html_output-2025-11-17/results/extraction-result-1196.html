<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1196 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1196</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1196</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-53669964</p>
                <p><strong>Paper Title:</strong> On the Complexity of Exploration in Goal-Driven Navigation</p>
                <p><strong>Paper Abstract:</strong> Building agents that can explore their environments intelligently is a challenging open problem. In this paper, we make a step towards understanding how a hierarchical design of the agent's policy can affect its exploration capabilities. First, we design EscapeRoom environments, where the agent must figure out how to navigate to the exit by accomplishing a number of intermediate tasks (\emph{subgoals}), such as finding keys or opening doors. Our environments are procedurally generated and vary in complexity, which can be controlled by the number of subgoals and relationships between them. Next, we propose to measure the complexity of each environment by constructing dependency graphs between the goals and analytically computing \emph{hitting times} of a random walk in the graph. We empirically evaluate Proximal Policy Optimization (PPO) with sparse and shaped rewards, a variation of policy sketches, and a hierarchical version of PPO (called HiPPO) akin to h-DQN. We show that analytically estimated \emph{hitting time} in goal dependency graphs is an informative metric of the environment complexity. We conjecture that the result should hold for environments other than navigation. Finally, we show that solving environments beyond certain level of complexity requires hierarchical approaches.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1196.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1196.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeRoom-(a)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeRoom environment (a) from On the Complexity of Exploration in Goal-Driven Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedurally generated 2-room grid-world where the agent must pick up a key, open a door, and reach an exit; used to study goal-dependency graphs and exploration complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeRoom (a)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Grid-world navigation (procedurally generated MiniGrid) with 2 rooms, objects include keys and doors (colored); partially observable first-person 7x7 view.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Locked doors that can only be opened by picking up a key of the same color; 'pick-up' only succeeds when key is in front, 'open' only succeeds when matching key has been picked up.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Goal-dependency graph is an acyclic, tree-like dependency (start → key → door → exit); for random-walk analysis graph is augmented to be strongly connected with transition probabilities (80% self-loop, 19% follow outgoing edges, 1% return to start).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>2 rooms; therefore 1 key and 1 door (n rooms implies n-1 keys/doors).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO, PPO+Bonus, PPO+Sketch, HiPPO (hierarchical PPO), Random-walk (analytical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PPO variants: LSTM policy (hidden dim 64) trained with PPO; PPO+Bonus uses shaped rewards for intermediate goals; PPO+Sketch conditions observations on a provided subgoal; HiPPO is hierarchical with a meta-controller selecting subgoals and a controller receiving intrinsic rewards (meta-controller in experiments was fixed to DFS sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Analytical expected hitting time (random-walk hitting time on goal dependency graph); empirical metrics: success rate, average episode length (% of max), timesteps to reach intermediate goals.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HT w/o drop-key = 8.4 (random-walk expected steps to exit). HT w/ drop-key = 16.5.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HiPPO: 74.9% (average success rate over last 10 episodes). Other agents: PPO 56.1%, PPO+Bonus 9.0%, PPO+Sketch 23.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical policies (HiPPO) perform best in this environment.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Shorter exit depth and small width produce smaller hitting times and higher success rates; here low HT correlates with higher empirical performance (HiPPO highest success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Compared across multiple EscapeRoom topologies; environments with greater exit depth and additional non-exit paths have higher HT and lower agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>A hierarchical meta-controller/controller structure (HiPPO) outperforms flat policies; intrinsic rewards for subgoals and explicit hierarchical structure aid exploration even when HT is moderate.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1196.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1196.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeRoom-(b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeRoom environment (b) from On the Complexity of Exploration in Goal-Driven Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedurally generated 3-room grid-world variant where the dependency graph has branching (width=2) and exit depth 2; used to study effect of graph width on exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeRoom (b)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>MiniGrid-based partially observable navigation with 3 rooms, multiple colored keys/doors creating branching goal-dependency structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Locked doors require keys of matching color; pick-up and open actions constrained by proximity and inventory.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Original goal-dependency graph is acyclic with branching (width=2); augmented for analysis to be strongly connected (80% self-loop, 19% follow outgoing edges, 1% return to start).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>3 rooms; 2 keys and 2 doors (n-1 keys/doors where n=3).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same PPO-family agents as other experiments; HiPPO uses meta-controller selecting goals and controller with intrinsic rewards; PPO baselines are flat LSTM policies.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Hitting time of goal-dependency graph; empirical success rate and average episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HT w/o drop-key = 12.1; HT w/ drop-key = 25.2.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HiPPO: 57.0%; PPO: 28.2%; PPO+Bonus: 6.0%; PPO+Sketch: 14.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical policies (HiPPO) are optimal/best-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Increased branching (width) raises HT and reduces performance, but impact is smaller than that of increased depth; HT correlates with empirical difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Branching (width=2) produced larger HT and lower success rates compared to narrower graphs with equal depth; depth remains dominant factor for difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Hierarchy helps exploration across branching structures; flat policies struggle more as width increases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1196.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1196.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeRoom-(c)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeRoom environment (c) from On the Complexity of Exploration in Goal-Driven Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3-room EscapeRoom variant with deeper dependency (exit depth 4) used to demonstrate the effect of depth on exploration time and agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeRoom (c)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated MiniGrid navigation with 3 rooms but a longer chain of dependent subgoals (depth 4), partially observable.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Locked doors require matching-color keys; actions require proximity and possession of key.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Acyclic dependency graph with longer path (depth=4); analysis uses an augmented strongly connected graph (80% stay, 19% outgoing, 1% back to start).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>3 rooms (structured so effective goal chain length is longer); keys/doors count per procedural generation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LSTM-PPO variants (flat) and HiPPO hierarchical agent; HiPPO fastest to reach intermediate goals in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Hitting time; empirical average number of timesteps to reach each intermediate goal; success rate and average episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HT w/o drop-key = 15.1; HT w/ drop-key = 39.5. Empirically HiPPO reaches intermediate goals fastest (figures reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HiPPO: 60.8%; PPO: 0.2%; PPO+Bonus: 0.0%; PPO+Sketch: 0.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical policies (HiPPO) — necessary to make substantial progress when depth is high.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Greater exit depth substantially increases HT and causes flat agents to fail; depth is a stronger predictor of difficulty than width here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Depth increases (to 4) lead to steep growth in HT and collapse of flat-policy performance; HiPPO still succeeds whereas flat PPO baselines fail.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that incorporate temporal abstraction and subgoal conditioning (hierarchical controller with intrinsic rewards) are required to handle high-depth dependency graphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1196.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1196.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeRoom-(d)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeRoom environment (d) from On the Complexity of Exploration in Goal-Driven Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 4-room EscapeRoom variant with moderate exit depth (2) and graph width 2, used to study combined effects of room count and branching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeRoom (d)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>MiniGrid-based partially observable navigation with 4 rooms and branching dependencies (multiple keys/doors).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Doors locked by color, require picking up matching colored key before opening; pick-up/open actions constrained to proximity.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Original dependency graph is acyclic with width=2; analysis uses augmented strongly connected random-walk graph (80% stay, 19% move, 1% back to start).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>4 rooms; 3 keys and 3 doors (n-1 keys/doors where n=4).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same set of agents; HiPPO uses hierarchical goal selection (meta-controller) and a controller receiving intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Hitting time; empirical success rate and average episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HT w/o drop-key = 13.1; HT w/ drop-key = 27.5.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HiPPO: 48.0%; PPO: 22.7%; PPO+Bonus: 11.0%; PPO+Sketch: 12.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical policies (HiPPO) perform best.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Moderate depth with branching yields intermediate HT and moderate empirical difficulty; hierarchical approaches improve exploration efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Compared to shallower/simpler graphs, this topology yields higher HT and worse flat-agent performance; hierarchy mitigates the effect.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Hierarchy reduces episode length and increases success rate versus flat policies, especially when multiple doors/keys are present.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1196.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1196.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeRoom-(e)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeRoom environment (e) from On the Complexity of Exploration in Goal-Driven Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 4-room EscapeRoom variant with exit depth 2 but larger graph width (3) to test effects of many alternative non-exit paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeRoom (e)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>MiniGrid partially observable navigation with 4 rooms and wider branching (width=3), multiple colored keys/doors and an exit.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Each door requires key of matching color; pick-up/open actions must satisfy proximity and inventory conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Acyclic, tree-like dependency graph (wider branching); for analysis augmented to be strongly connected with specified random-walk probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>4 rooms; 3 keys and 3 doors (n-1 keys/doors).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Flat PPO variants vs hierarchical HiPPO; HiPPO uses intrinsic rewards; PPO+Sketch supplies subgoals as observations but no intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Hitting time; empirical success rate and average episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HT w/o drop-key = 13.9; HT w/ drop-key = 26.7.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HiPPO: 29.9%; PPO: 19.1%; PPO+Bonus: 4.5%; PPO+Sketch: 10.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical policies (HiPPO) perform best but overall task is harder than narrower graphs with same depth.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Greater width increases HT and reduces success rate, though the effect is less pronounced than increases in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Wider branching yields increased exploration time and degraded flat agent performance; hierarchy helps but success rates drop relative to narrower graphs of same depth.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Hierarchical structure improves performance but scaling to very wide dependency graphs remains challenging for all agents.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1196.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1196.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeRoom-(f)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeRoom environment (f) from On the Complexity of Exploration in Goal-Driven Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 4-room EscapeRoom instance with large exit depth (4) producing a high exploration complexity and high hitting time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeRoom (f)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>MiniGrid navigation with 4 rooms arranged so that the goal dependency chain is long (exit depth 4), requiring multiple sequential subgoals (keys/doors) to reach exit.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Locked doors require matching-color keys; pick-up and open actions constrained by front-facing proximity and inventory.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Acyclic deep dependency graph (depth=4); augmented graph for analysis is strongly connected (80% self-loop, 19% outgoing, 1% back to start).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>4 rooms (n=4) with 3 keys and 3 doors; dependency chain depth is 4 (long sequence of subgoals).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical HiPPO vs flat PPO baselines; HiPPO uses goal-conditioned controller and meta-controller (fixed DFS in experiments); flat policies are memory-based LSTMs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Hitting time; empirical success rate and average episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HT w/o drop-key = 29.2; HT w/ drop-key = 86.1.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HiPPO: 11.2%; PPO: 0.0%; PPO+Bonus: 0.5%; PPO+Sketch: 0.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical policies are necessary; flat policies fail on this high-depth topology.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>High exit depth dramatically increases HT and causes flat PPO agents to fail; HiPPO still makes some progress though success rates are low, indicating depth is the dominant difficulty driver.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Deep graphs (high exit depth) produce the largest HTs and worst performance; adding a 'drop-key' action multiplies HT and makes learning substantially harder.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Temporal abstraction and intrinsic subgoal rewards (hierarchical policy) are critical to achieve any learning in deep dependency graphs; flat policies fail even with reward shaping or sketches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1196.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1196.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeRoom-(g)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeRoom environment (g) from On the Complexity of Exploration in Goal-Driven Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 4-room EscapeRoom instance with the largest exit depth tested (6), producing high hitting time and challenging exploration requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeRoom (g)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated MiniGrid navigation with deep goal-dependency (exit depth 6), multiple keys/doors and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Doors locked by color; opening a door requires possession of matching key; pick-up/open actions depend on facing and proximity.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Acyclic deep dependency graph (depth=6 in enumeration) augmented to strongly connected random-walk graph for HT computation (80% self-loop, 19% outgoing, 1% back to start).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>4-room layout variant with effective exit depth 6 (procedural composition yields longer dependency chain); 3 keys/doors for n=4 rooms as base, but dependency chain is longer in this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>HiPPO hierarchical agent (meta-controller selects subgoals via fixed DFS in experiments) vs flat PPO variants; all policies use LSTM backbones for partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Hitting time; empirical success rate and average episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HT w/o drop-key = 27.5; HT w/ drop-key = 82.5.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HiPPO: 19.0%; PPO: 0.0%; PPO+Bonus: 0.0%; PPO+Sketch: 0.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical (HiPPO) is the only approach that makes measurable progress; flat policies fail.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Very high exit depth yields the largest HTs and lowest empirical performance; depth correlates strongly with required hierarchical structure in policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Among evaluated topologies, (f) and (g) with greatest depths produced highest HTs and poorest flat-agent outcomes; hierarchy remains beneficial but absolute success rates are low.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Deep dependency graphs require policies with temporal abstraction and explicit subgoal handling; flat agents (even with shaped rewards or sketches) fail to learn effective behaviors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation <em>(Rating: 2)</em></li>
                <li>Modular multitask reinforcement learning with policy sketches <em>(Rating: 2)</em></li>
                <li>Random walks on graphs <em>(Rating: 1)</em></li>
                <li>Proximal policy optimization algorithms <em>(Rating: 1)</em></li>
                <li>Minimalistic Gridworld Environment for OpenAI Gym <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1196",
    "paper_id": "paper-53669964",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "EscapeRoom-(a)",
            "name_full": "EscapeRoom environment (a) from On the Complexity of Exploration in Goal-Driven Navigation",
            "brief_description": "A procedurally generated 2-room grid-world where the agent must pick up a key, open a door, and reach an exit; used to study goal-dependency graphs and exploration complexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "EscapeRoom (a)",
            "environment_description": "Grid-world navigation (procedurally generated MiniGrid) with 2 rooms, objects include keys and doors (colored); partially observable first-person 7x7 view.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Locked doors that can only be opened by picking up a key of the same color; 'pick-up' only succeeds when key is in front, 'open' only succeeds when matching key has been picked up.",
            "graph_connectivity": "Goal-dependency graph is an acyclic, tree-like dependency (start → key → door → exit); for random-walk analysis graph is augmented to be strongly connected with transition probabilities (80% self-loop, 19% follow outgoing edges, 1% return to start).",
            "environment_size": "2 rooms; therefore 1 key and 1 door (n rooms implies n-1 keys/doors).",
            "agent_name": "PPO, PPO+Bonus, PPO+Sketch, HiPPO (hierarchical PPO), Random-walk (analytical)",
            "agent_description": "PPO variants: LSTM policy (hidden dim 64) trained with PPO; PPO+Bonus uses shaped rewards for intermediate goals; PPO+Sketch conditions observations on a provided subgoal; HiPPO is hierarchical with a meta-controller selecting subgoals and a controller receiving intrinsic rewards (meta-controller in experiments was fixed to DFS sequences).",
            "exploration_efficiency_metric": "Analytical expected hitting time (random-walk hitting time on goal dependency graph); empirical metrics: success rate, average episode length (% of max), timesteps to reach intermediate goals.",
            "exploration_efficiency_value": "HT w/o drop-key = 8.4 (random-walk expected steps to exit). HT w/ drop-key = 16.5.",
            "success_rate": "HiPPO: 74.9% (average success rate over last 10 episodes). Other agents: PPO 56.1%, PPO+Bonus 9.0%, PPO+Sketch 23.2%.",
            "optimal_policy_type": "Hierarchical policies (HiPPO) perform best in this environment.",
            "topology_performance_relationship": "Shorter exit depth and small width produce smaller hitting times and higher success rates; here low HT correlates with higher empirical performance (HiPPO highest success rate).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Compared across multiple EscapeRoom topologies; environments with greater exit depth and additional non-exit paths have higher HT and lower agent performance.",
            "policy_structure_findings": "A hierarchical meta-controller/controller structure (HiPPO) outperforms flat policies; intrinsic rewards for subgoals and explicit hierarchical structure aid exploration even when HT is moderate.",
            "uuid": "e1196.0"
        },
        {
            "name_short": "EscapeRoom-(b)",
            "name_full": "EscapeRoom environment (b) from On the Complexity of Exploration in Goal-Driven Navigation",
            "brief_description": "A procedurally generated 3-room grid-world variant where the dependency graph has branching (width=2) and exit depth 2; used to study effect of graph width on exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "EscapeRoom (b)",
            "environment_description": "MiniGrid-based partially observable navigation with 3 rooms, multiple colored keys/doors creating branching goal-dependency structure.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Locked doors require keys of matching color; pick-up and open actions constrained by proximity and inventory.",
            "graph_connectivity": "Original goal-dependency graph is acyclic with branching (width=2); augmented for analysis to be strongly connected (80% self-loop, 19% follow outgoing edges, 1% return to start).",
            "environment_size": "3 rooms; 2 keys and 2 doors (n-1 keys/doors where n=3).",
            "agent_name": "PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)",
            "agent_description": "Same PPO-family agents as other experiments; HiPPO uses meta-controller selecting goals and controller with intrinsic rewards; PPO baselines are flat LSTM policies.",
            "exploration_efficiency_metric": "Hitting time of goal-dependency graph; empirical success rate and average episode length.",
            "exploration_efficiency_value": "HT w/o drop-key = 12.1; HT w/ drop-key = 25.2.",
            "success_rate": "HiPPO: 57.0%; PPO: 28.2%; PPO+Bonus: 6.0%; PPO+Sketch: 14.5%.",
            "optimal_policy_type": "Hierarchical policies (HiPPO) are optimal/best-performing.",
            "topology_performance_relationship": "Increased branching (width) raises HT and reduces performance, but impact is smaller than that of increased depth; HT correlates with empirical difficulty.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Branching (width=2) produced larger HT and lower success rates compared to narrower graphs with equal depth; depth remains dominant factor for difficulty.",
            "policy_structure_findings": "Hierarchy helps exploration across branching structures; flat policies struggle more as width increases.",
            "uuid": "e1196.1"
        },
        {
            "name_short": "EscapeRoom-(c)",
            "name_full": "EscapeRoom environment (c) from On the Complexity of Exploration in Goal-Driven Navigation",
            "brief_description": "A 3-room EscapeRoom variant with deeper dependency (exit depth 4) used to demonstrate the effect of depth on exploration time and agent performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "EscapeRoom (c)",
            "environment_description": "Procedurally generated MiniGrid navigation with 3 rooms but a longer chain of dependent subgoals (depth 4), partially observable.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Locked doors require matching-color keys; actions require proximity and possession of key.",
            "graph_connectivity": "Acyclic dependency graph with longer path (depth=4); analysis uses an augmented strongly connected graph (80% stay, 19% outgoing, 1% back to start).",
            "environment_size": "3 rooms (structured so effective goal chain length is longer); keys/doors count per procedural generation.",
            "agent_name": "PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)",
            "agent_description": "LSTM-PPO variants (flat) and HiPPO hierarchical agent; HiPPO fastest to reach intermediate goals in experiments.",
            "exploration_efficiency_metric": "Hitting time; empirical average number of timesteps to reach each intermediate goal; success rate and average episode length.",
            "exploration_efficiency_value": "HT w/o drop-key = 15.1; HT w/ drop-key = 39.5. Empirically HiPPO reaches intermediate goals fastest (figures reported in paper).",
            "success_rate": "HiPPO: 60.8%; PPO: 0.2%; PPO+Bonus: 0.0%; PPO+Sketch: 0.4%.",
            "optimal_policy_type": "Hierarchical policies (HiPPO) — necessary to make substantial progress when depth is high.",
            "topology_performance_relationship": "Greater exit depth substantially increases HT and causes flat agents to fail; depth is a stronger predictor of difficulty than width here.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Depth increases (to 4) lead to steep growth in HT and collapse of flat-policy performance; HiPPO still succeeds whereas flat PPO baselines fail.",
            "policy_structure_findings": "Policies that incorporate temporal abstraction and subgoal conditioning (hierarchical controller with intrinsic rewards) are required to handle high-depth dependency graphs.",
            "uuid": "e1196.2"
        },
        {
            "name_short": "EscapeRoom-(d)",
            "name_full": "EscapeRoom environment (d) from On the Complexity of Exploration in Goal-Driven Navigation",
            "brief_description": "A 4-room EscapeRoom variant with moderate exit depth (2) and graph width 2, used to study combined effects of room count and branching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "EscapeRoom (d)",
            "environment_description": "MiniGrid-based partially observable navigation with 4 rooms and branching dependencies (multiple keys/doors).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Doors locked by color, require picking up matching colored key before opening; pick-up/open actions constrained to proximity.",
            "graph_connectivity": "Original dependency graph is acyclic with width=2; analysis uses augmented strongly connected random-walk graph (80% stay, 19% move, 1% back to start).",
            "environment_size": "4 rooms; 3 keys and 3 doors (n-1 keys/doors where n=4).",
            "agent_name": "PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)",
            "agent_description": "Same set of agents; HiPPO uses hierarchical goal selection (meta-controller) and a controller receiving intrinsic rewards.",
            "exploration_efficiency_metric": "Hitting time; empirical success rate and average episode length.",
            "exploration_efficiency_value": "HT w/o drop-key = 13.1; HT w/ drop-key = 27.5.",
            "success_rate": "HiPPO: 48.0%; PPO: 22.7%; PPO+Bonus: 11.0%; PPO+Sketch: 12.6%.",
            "optimal_policy_type": "Hierarchical policies (HiPPO) perform best.",
            "topology_performance_relationship": "Moderate depth with branching yields intermediate HT and moderate empirical difficulty; hierarchical approaches improve exploration efficiency.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Compared to shallower/simpler graphs, this topology yields higher HT and worse flat-agent performance; hierarchy mitigates the effect.",
            "policy_structure_findings": "Hierarchy reduces episode length and increases success rate versus flat policies, especially when multiple doors/keys are present.",
            "uuid": "e1196.3"
        },
        {
            "name_short": "EscapeRoom-(e)",
            "name_full": "EscapeRoom environment (e) from On the Complexity of Exploration in Goal-Driven Navigation",
            "brief_description": "A 4-room EscapeRoom variant with exit depth 2 but larger graph width (3) to test effects of many alternative non-exit paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "EscapeRoom (e)",
            "environment_description": "MiniGrid partially observable navigation with 4 rooms and wider branching (width=3), multiple colored keys/doors and an exit.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Each door requires key of matching color; pick-up/open actions must satisfy proximity and inventory conditions.",
            "graph_connectivity": "Acyclic, tree-like dependency graph (wider branching); for analysis augmented to be strongly connected with specified random-walk probabilities.",
            "environment_size": "4 rooms; 3 keys and 3 doors (n-1 keys/doors).",
            "agent_name": "PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)",
            "agent_description": "Flat PPO variants vs hierarchical HiPPO; HiPPO uses intrinsic rewards; PPO+Sketch supplies subgoals as observations but no intrinsic rewards.",
            "exploration_efficiency_metric": "Hitting time; empirical success rate and average episode length.",
            "exploration_efficiency_value": "HT w/o drop-key = 13.9; HT w/ drop-key = 26.7.",
            "success_rate": "HiPPO: 29.9%; PPO: 19.1%; PPO+Bonus: 4.5%; PPO+Sketch: 10.7%.",
            "optimal_policy_type": "Hierarchical policies (HiPPO) perform best but overall task is harder than narrower graphs with same depth.",
            "topology_performance_relationship": "Greater width increases HT and reduces success rate, though the effect is less pronounced than increases in depth.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Wider branching yields increased exploration time and degraded flat agent performance; hierarchy helps but success rates drop relative to narrower graphs of same depth.",
            "policy_structure_findings": "Hierarchical structure improves performance but scaling to very wide dependency graphs remains challenging for all agents.",
            "uuid": "e1196.4"
        },
        {
            "name_short": "EscapeRoom-(f)",
            "name_full": "EscapeRoom environment (f) from On the Complexity of Exploration in Goal-Driven Navigation",
            "brief_description": "A 4-room EscapeRoom instance with large exit depth (4) producing a high exploration complexity and high hitting time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "EscapeRoom (f)",
            "environment_description": "MiniGrid navigation with 4 rooms arranged so that the goal dependency chain is long (exit depth 4), requiring multiple sequential subgoals (keys/doors) to reach exit.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Locked doors require matching-color keys; pick-up and open actions constrained by front-facing proximity and inventory.",
            "graph_connectivity": "Acyclic deep dependency graph (depth=4); augmented graph for analysis is strongly connected (80% self-loop, 19% outgoing, 1% back to start).",
            "environment_size": "4 rooms (n=4) with 3 keys and 3 doors; dependency chain depth is 4 (long sequence of subgoals).",
            "agent_name": "PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)",
            "agent_description": "Hierarchical HiPPO vs flat PPO baselines; HiPPO uses goal-conditioned controller and meta-controller (fixed DFS in experiments); flat policies are memory-based LSTMs.",
            "exploration_efficiency_metric": "Hitting time; empirical success rate and average episode length.",
            "exploration_efficiency_value": "HT w/o drop-key = 29.2; HT w/ drop-key = 86.1.",
            "success_rate": "HiPPO: 11.2%; PPO: 0.0%; PPO+Bonus: 0.5%; PPO+Sketch: 0.1%.",
            "optimal_policy_type": "Hierarchical policies are necessary; flat policies fail on this high-depth topology.",
            "topology_performance_relationship": "High exit depth dramatically increases HT and causes flat PPO agents to fail; HiPPO still makes some progress though success rates are low, indicating depth is the dominant difficulty driver.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Deep graphs (high exit depth) produce the largest HTs and worst performance; adding a 'drop-key' action multiplies HT and makes learning substantially harder.",
            "policy_structure_findings": "Temporal abstraction and intrinsic subgoal rewards (hierarchical policy) are critical to achieve any learning in deep dependency graphs; flat policies fail even with reward shaping or sketches.",
            "uuid": "e1196.5"
        },
        {
            "name_short": "EscapeRoom-(g)",
            "name_full": "EscapeRoom environment (g) from On the Complexity of Exploration in Goal-Driven Navigation",
            "brief_description": "A 4-room EscapeRoom instance with the largest exit depth tested (6), producing high hitting time and challenging exploration requirements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "EscapeRoom (g)",
            "environment_description": "Procedurally generated MiniGrid navigation with deep goal-dependency (exit depth 6), multiple keys/doors and partial observability.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Doors locked by color; opening a door requires possession of matching key; pick-up/open actions depend on facing and proximity.",
            "graph_connectivity": "Acyclic deep dependency graph (depth=6 in enumeration) augmented to strongly connected random-walk graph for HT computation (80% self-loop, 19% outgoing, 1% back to start).",
            "environment_size": "4-room layout variant with effective exit depth 6 (procedural composition yields longer dependency chain); 3 keys/doors for n=4 rooms as base, but dependency chain is longer in this variant.",
            "agent_name": "PPO, PPO+Bonus, PPO+Sketch, HiPPO, Random-walk (analytical)",
            "agent_description": "HiPPO hierarchical agent (meta-controller selects subgoals via fixed DFS in experiments) vs flat PPO variants; all policies use LSTM backbones for partial observability.",
            "exploration_efficiency_metric": "Hitting time; empirical success rate and average episode length.",
            "exploration_efficiency_value": "HT w/o drop-key = 27.5; HT w/ drop-key = 82.5.",
            "success_rate": "HiPPO: 19.0%; PPO: 0.0%; PPO+Bonus: 0.0%; PPO+Sketch: 0.0%.",
            "optimal_policy_type": "Hierarchical (HiPPO) is the only approach that makes measurable progress; flat policies fail.",
            "topology_performance_relationship": "Very high exit depth yields the largest HTs and lowest empirical performance; depth correlates strongly with required hierarchical structure in policy.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Among evaluated topologies, (f) and (g) with greatest depths produced highest HTs and poorest flat-agent outcomes; hierarchy remains beneficial but absolute success rates are low.",
            "policy_structure_findings": "Deep dependency graphs require policies with temporal abstraction and explicit subgoal handling; flat agents (even with shaped rewards or sketches) fail to learn effective behaviors.",
            "uuid": "e1196.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
            "rating": 2,
            "sanitized_title": "hierarchical_deep_reinforcement_learning_integrating_temporal_abstraction_and_intrinsic_motivation"
        },
        {
            "paper_title": "Modular multitask reinforcement learning with policy sketches",
            "rating": 2,
            "sanitized_title": "modular_multitask_reinforcement_learning_with_policy_sketches"
        },
        {
            "paper_title": "Random walks on graphs",
            "rating": 1,
            "sanitized_title": "random_walks_on_graphs"
        },
        {
            "paper_title": "Proximal policy optimization algorithms",
            "rating": 1,
            "sanitized_title": "proximal_policy_optimization_algorithms"
        },
        {
            "paper_title": "Minimalistic Gridworld Environment for OpenAI Gym",
            "rating": 1,
            "sanitized_title": "minimalistic_gridworld_environment_for_openai_gym"
        }
    ],
    "cost": 0.0149205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Complexity of Exploration in Goal-Driven Navigation</p>
<p>Maruan Al-Shedivat 
Carnegie Mellon University
2 Apple</p>
<p>Lisa Lee 
Carnegie Mellon University
2 Apple</p>
<p>Ruslan Salakhutdinov 
Carnegie Mellon University
2 Apple</p>
<p>Eric P Xing 
Carnegie Mellon University
2 Apple</p>
<p>Petuum Inc</p>
<p>On the Complexity of Exploration in Goal-Driven Navigation</p>
<p>Building agents that can explore their environments intelligently is a challenging open problem. In this paper, we make a step towards understanding how a hierarchical design of the agent's policy can affect its exploration capabilities. First, we design EscapeRoom environments, where the agent must figure out how to navigate to the exit by accomplishing a number of intermediate tasks (subgoals), such as finding keys or opening doors. Our environments are procedurally generated and vary in complexity, which can be controlled by the number of subgoals and relationships between them. Next, we propose to measure the complexity of each environment by constructing dependency graphs between the goals and analytically computing hitting times of a random walk in the graph. We empirically evaluate Proximal Policy Optimization (PPO) with sparse and shaped rewards, a variation of policy sketches, and a hierarchical version of PPO (called HiPPO) akin to h-DQN. We show that analytically estimated hitting time in goal dependency graphs is an informative metric of the environment complexity. We conjecture that the result should hold for environments other than navigation. Finally, we show that solving environments beyond certain level of complexity requires hierarchical approaches.</p>
<p>Introduction</p>
<p>Deep reinforcement learning research has led us to discover general-purpose algorithms for learning how to control robots [1] and solve games [2,3], surpassing human abilities. These results indicate a significant progress in the field. However, building agents capable of intelligent exploration even in simple environments is still an unreached milestone.</p>
<p>To make progress towards this goal, first we need to understand and be able to measure when exploration is necessary. For instance, while Atari games seem like a challenging benchmark, it turns out that having a memoryless reactive policy is often sufficient for solving most of these games [4]. On the other hand, there are environments (e.g., Montezuma's Revenge) that can only be solved by achieving some intermediate goals (subgoals). Learning about the dependencies between the subgoals requires executing a consistent exploration strategy, reasoning, and multi-step planning, beyond vanilla deep RL methods.</p>
<p>Broadly, exploration is a mechanism used by an agent to reduce uncertainty about its environment (i.e., rewards and state transitions). Notable approaches to exploration include: (1) count-based and intrinsic motivation methods [5], where the agent (approximately) quantifies uncertainty of the states and actions and tends to visit the states it is least certain about; and (2) various policy-perturbation heuristics, such as ε-greedy, Boltzmann, and parameter-noise methods [6,7]. All these approaches function on the level of atomic actions and hence are limited when it comes to complex structured tasks with delayed and sparse rewards. To overcome such limitations, it is possible to use the framework of temporal abstractions (options) [8,9]. In particular, Kulkarni et al. [10] argued for hierarchical methods that enable exploration in the space of goals, which is also our focus.</p>
<p>In this paper, we aim to understand and measure the complexity of exploration in environments with multiple dependent subgoals, and the effects of hierarchical design of the agent's policy in such environments. To do so, we introduce a collection of procedurally generated, simple grid-world environments called EscapeRooms (Table 1). We represent the goal space with dependency graphs, and propose to measure complexity of exploration as the time it takes a random walk in this abstract space to reach the final goal state from the start state in expectation (i.e., the expected hitting time). This measure captures a simple intuition: the more complex the goal dependencies are, the more time it would take the agent to explore how to solve the environment.</p>
<p>To verify that the hitting time is a useful measure of complexity in RL scenarios, we train a few hierarchical and non-hierarchical policies using methods based on proximal policy optimization [PPO,11] on EscapeRooms and measure their exploration capabilities. We use metrics such as success rate and the number of timesteps it takes the agent to achieve each goal. Our results demonstrate that information about the goals is crucial to enable learning in our environments. Moreover, we show that our complexity measure correlates with the performance of the policies-agents perform worse and hierarchy becomes more important in environments with higher exploration complexity.</p>
<p>Methods</p>
<p>Given an environment, we would like to quantify how much exploration is needed to solve the task. In this section, we introduce the notion of goal-dependency graphs, describe EscapeRoom environments, and compute different measures of the exploration complexity for these environments.</p>
<p>Goal-dependency graphs &amp; exploration complexity</p>
<p>We are interested in a scenario where the agent can achieve the final goal only after having accomplished a number of intermediate goals. Assuming that the goals and dependencies are given, we can construct a graph G(V, E) with nodes V representing the goals and edges E representing the relationships between the goals. From this perspective, we can treat the agent executing a (stochastic) policy in an environment with these subgoals as a random walk on the corresponding goal-dependency graph. To measure complexity of exploration in the given environment, we introduce the following notation. Let n be the number of nodes in the graph, and W ∈ R n×n the adjacency matrix of the graph weighted by the probabilities of transition from one goal to the other (according to the policy π executed by the agent). Let D ∈ R n×n be the corresponding diagonal weighted degree matrix:
D ii := n j=1 W ij , D ij := 0, ∀i = j
Now, we can use the graph Laplacian, L := W − D, to compute the expected time it would take the random walk to reach a given goal node with index t from the initial node with index s in the graph for the first time (also known as the hitting time) [12]. To do so, we can solve the following linear system (subscripts denote indices):
Lx = b s.t. x t = 0, (1) where b s = 1, b t = −1, b k = 0 ∀k / ∈ {s, t} where x, b ∈ R n .
The solution, x s , will be the hitting time from s to t. Solving (1) for each goal in the graph allows us to analytically compute different statistics for any goal-dependency graph under a given random walk, e.g., the expected number of states reachable under a given time limit.</p>
<p>EscapeRoom environments</p>
<p>We design a set of grid-world environments (Table 1)  EscapeRoom environments are based on Gym MiniGrid [13] and follow the OpenAI Gym API [14]. In each episode, we procedurally generate a new environment; the object locations, colors of the keys and doors, and the room layouts are all randomized. The agent always begins at a random cell in the center room, which branches out to 1-3 other rooms, one of which contains the exit. Each of the branching rooms is initially blocked by a locked door, so an environment with n rooms has exactly n − 1 keys and doors. Each open cell can contain up to one of 3 object types (Exit, Key, or Door) with one of 6 possible colors. The environment is partially observable, meaning that the agent can only observe its local surroundings and cannot see through walls. In our experiments, each observation is a 7 × 7 × 3 array representing the 7×7 view in front of the agent with three channels (object IDs, color IDs, and a binary matrix capturing whether a door is open).</p>
<p>Goal dependencies in EscapeRooms. In Table 1, we enumerate all possible goal dependency graphs for different EscapeRoom environments with up to 4 rooms. Each goal is represented as one-hot encodings of (color, object); for example, (yellow key) means to pick up the yellow key, and (blue door) means to open the blue door. To understand how complex each environment is from the stand point of exploration in the goal space, we compute the hitting time (HT) for each graph ( Table 2). Note that dependency graphs in Table 1 are simplified for illustration purposes (each goal node is assumed to be visited only once). Computing properties of random walks requires strongly connected graphs, and hence we construct augmented goal-dependency graphs and use those for estimating the hitting times of interest (see Appendix A).</p>
<p>Complexity of EscapeRooms. Based on Table 2, we make a few observations. First, longer paths from the start to the exit nodes result in slower discovery of how to solve the environment. Similarly, adding alternative paths that do not lead to the exit (proportional to the graph width) also increase the complexity and is reflected by the hitting time metric. Finally, the environment complexity depends not only on the spatial map design but also on the action space. We experimented with adding an extra drop (key) action which significantly increased the hitting time for the exit node in goal-dependency graph ( Table 2, last row).</p>
<p>RL algorithms</p>
<p>In this work, we focus on a class of policy gradient methods known as Proximal Policy Optimization (PPO) algorithms [11]. First, we evaluate the vanilla PPO trained using two different reward functions:</p>
<p>(1) PPO is trained using sparse rewards, where the agent receives +1 reward upon achieving the final goal (e.g., reaching the exit); and (2) PPO+Bonus is trained using reward shaping where in addition to the reward for achieving the final goal, the agent also receives +1 reward for achieving intermediate goals (e.g., picking up keys or opening doors).</p>
<p>Algorithm 1 Hierarchical PPO 1: Input: Meta-controller policy π M , Controller policy π C 2: for i = 1 to num_episodes do 3: subgoal g ← π M (s) 4: while s is not terminal do while not (g is reached) do 8: a ← π C ({s, g}) 9: state s , reward f ← Env(a) 10: intrinsic reward r ← Critic(s , g) 11: PPO_update(π C , s, a, s , r) Next, we introduce a variant of PPO called HiPPO (Hierarchical PPO) which borrows the hierarchical framework from [10], but replaces the hierarchical value functions in the h-DQN with hierarchical PPO policies. In more detail, HiPPO uses a meta-controller policy to choose intermediate goals for the lower-level controller policy to achieve 2 . The controller receives one-hot encoded goals as part of its observation and intrinsic rewards for achieving intermediate goals chosen by the meta-controller. The meta-controller receives sparse extrinsic rewards from the environment for achieving the final goal and is prompted to submit a new action (i.e., a new goal) each time the lower-level controller accomplishes the previous goal. The pseudocode for HiPPO is given in Algorithm 1. In our experiments, we used a fixed meta-controller that chooses a sequence of goals along a random depth-first search path on the goal dependency graph, rather than a trainable metacontroller policy.</p>
<p>Lastly, PPO+Sketch is a variation of policy sketches [15] where the agent is provided with a sequence of goals that leads to achieving the final goal. PPO+Sketch is identical to PPO except that in each timestep, the current observation is concatenated with the current intermediate goal 3 , i.e., the actions produced by the policy are always conditional on the current goal. Similar to PPO, and unlike HiPPO and PPO+Bonus, PPO+Sketch does not use intrinsic rewards for achieving intermediate goals.</p>
<p>Experiments</p>
<p>We evaluate PPO, PPO+Bonus, PPO+Sketch, and HiPPO on EscapeRoom environments (a)-(g). We limit the episode length to 1000 time steps. For each method and environment, we use the LSTM policy with hidden dimension 64, and train for 10M total time steps on 128 vectorized environments using the Adam optimizer, learning rate 2.5e-4, discount factor γ = 0.9, and TD λ = 0.95. We evaluated each method and environment over 5 trials with different random seeds.</p>
<p>In Figures 1 and 2, we see that HiPPO consistently achieves the smallest average episode length and highest success rate on all environments, thus demonstrating the benefit of using hierarchical policies that operate at different temporal scales. Surprisingly, PPO with sparse rewards performs better than PPO+Bonus, showing that the bonus rewards for achieving intermediate goals does not help a non-hierarchical policy. We also find that PPO+Sketch performs worse than PPO indicating that merely conditioning on subgoals might be suboptimal and destructively interferes with optimization. Environments (f) and (g) are more challenging for RL agents due to greater exit depth of their goal dependency graphs, i.e., the agent must achieve a longer sequence of intermediate goals before it can reach the exit. Similarly, the width of the dependency graph introduces complexity (due to paths that don't lead anywhere), but not as much as the depth. We find that the analytically estimated hitting times given in Table 2 are in agreement with the observed empirical performance of the RL algorithms. We also note that despite the complexity of the environments, HiPPO is still able to make some progress on (f) and (g), while the other flat PPO baselines (with or without reward shaping and/or policy sketches) fail to solve them (Figure 1).     (Table 2) vs. the average success rate and average episode length (Table 3) of HiPPO for EscapeRoom environments (a)-(g) from Table 1. This verifies that the hitting time is a useful measure of complexity for RL environments.</p>
<p>In Figure 3, we illustrate the correlation between the hitting time on goal dependency graphs (Table 2) and the empirical performance of HiPPO (Table 3) for different EscapeRoom environments, which demonstrates that analytically estimated hitting time is an informative metric for measuring the complexity of an environment.</p>
<p>Discussion</p>
<p>We designed a simple grid-world EscapeRoom environment where it is easy to measure the exploration complexity by analyzing the corresponding goal dependency graphs. We showed that hitting times in goal dependency graphs are consistent with the empirical performance of PPObased methods, and is therefore a useful metric to measure the complexity of the environment. Finally, we showed the performance improvement of HiPPO over other flat PPO baselines, demonstrating the benefit of using hierarchical policies that operate at different temporal scales.</p>
<p>A Details on computing hitting times</p>
<p>As we mentioned in Section 2.3, to compute the hitting time of random walk we need an augmented goal-dependency graph (which can be generated procedurally from the graphs given in the main text). An example augmented graph for EscapeRoom (c) from Table 1  The main difference from the goal dependency graph given in Table 1 is that when the agent picks up a key and opens the corresponding door, it transitions into a subgraph that corresponds to the new layout of the rooms accessible to the agent. Self-loops and transitions between the rooms represent the moving behavior.</p>
<p>We set the following parameters for the random walk. With 80% chance, no transition happens. With 19% chance, the walk transitions from the current node along one of the outgoing edges. Finally, to ensure strong connectivity of the graph, we add 1% chance of the agent moving back to the root start node from any other node in the graph 4 . This corresponds to the situation where the agent is not able to reach the exit within the time limit and must start a new episode.</p>
<p>where the agent must pick up keys and open locked doors (i.e., accomplish intermediate goals) in order to reach the exit (the final goal). The agent has 5 actions: move-forward, turn-left, turn-right, pick-up (key), and open (door). The pick-up action only succeeds if the key is in front of the agent. The open action only succeeds if a locked door is in front of the agent and the agent has already picked up a key of the same color as the door. Upon arriving at the exit, the agent receives a reward of 1 and the episode terminates. Our</p>
<p>Figure 2 :
2Average number of timesteps to reach each intermediate goal on EscapeRoom (c). HiPPO is the quickest method to achieve each goal.</p>
<p>Figure 3 :
3Correlation between the hitting time</p>
<p>Table 1 :
1EscapeRoom environments. On the right, we enumerate all possible dependency graphs (up to a permutation of colors) for environments with two rooms (a), three rooms (b, c), and four rooms (d, e, f ,g). Each node in the dependency graph can be traversed at most once (i.e., no cyclic paths are allowed). The agent (red triangle) must pick up keys and open locked doors in order to reach the exit (green square). Each door can only be opened by a key of the corresponding color.Goal dependency graphs 
Environment </p>
<p>(a) </p>
<p>start key door exit </p>
<p>Sample environment (a) </p>
<p>(b) </p>
<p>start </p>
<p>key door </p>
<p>exit </p>
<p>(c) </p>
<p>Sample environment (b) </p>
<p>(g) </p>
<p>(f) </p>
<p>(d) </p>
<p>start </p>
<p>key door exit </p>
<p>(e) </p>
<p>Sample environment (f) </p>
<p>Table 2 :
2Depth, width, and hitting time (HT) statistics 
computed for EscapeRoom environments (a)-(g). </p>
<p>(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
(g) </p>
<p>exit depth 
2 
2 
4 
2 
2 
4 
6 
graph width 
1 
2 
1 
2 
3 
2 
1 
HT (w/o drop-key) 
8.4 12.1 15.1 13.1 13.9 29.2 27.5 
HT (w/ drop-key) 
16.5 25.2 39.5 27.5 26.7 86.1 82.5 </p>
<p>Table 3 :
3Left: Average success rate (%) to reach the final goal over the last 10 training episodes. Right: Average episode length (% of the max length, smaller is more efficient) over the last 10 training episodes. "-" indicates that the method failed to reach the final goal within 1000 steps.Average Success Rate 
Average Episode Length 
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
(g) 
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
(g) </p>
<p>PPO 
56.1 
28.2 
0.2 
22.7 
19.1 
0.0 
0.0 
78.5 
88.0 
-
90.7 
91.4 
-
-
PPO+Bonus 
9.0 
6.0 
0.0 
11.0 
4.5 
0.5 
0.0 
97.8 
96.7 
99.9 
97.3 
98.0 
99.9 
-
PPO+Sketch 
23.2 
14.5 
0.4 
12.6 
10.7 
0.1 
0.0 
91.9 
94.4 
99.9 
95.0 
95.7 
-
-
HiPPO 
74.9 
57.0 
60.8 
48.0 
29.9 
11.2 
19.0 
48.2 
67.4 
69.1 
71.0 
85.3 
96.4 
93.8 </p>
<p>400 </p>
<p>500 </p>
<p>600 </p>
<p>700 </p>
<p>800 </p>
<p>900 </p>
<p>1000 </p>
<p>episode length </p>
<p>(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
(g) </p>
<p>2 
4 
6 </p>
<h1>timesteps</h1>
<p>1e6 </p>
<p>0.0 </p>
<p>0.2 </p>
<p>0.4 </p>
<p>0.6 </p>
<p>0.8 </p>
<p>1.0 </p>
<p>success rate </p>
<p>2 
4 
6 </p>
<h1>timesteps</h1>
<p>1e6 
2 
4 
6 </p>
<h1>timesteps</h1>
<p>1e6 
2 
4 
6 </p>
<h1>timesteps</h1>
<p>1e6 
2 
4 
6 </p>
<h1>timesteps</h1>
<p>1e6 
2 
4 
6 </p>
<h1>timesteps</h1>
<p>1e6 
2 
4 
6 </p>
<h1>timesteps</h1>
<p>1e6 </p>
<p>PPO 
PPO+Bonus 
HiPPO 
PPO+Sketch </p>
<p>Figure 1: Average episode length and success rate on EscapeRoom environments with goal dependency graphs 
(a)-(g) from Table 1. In all environments, HiPPO achieves the best performance (smallest episode length and 
highest success rate). In the most complex environments (f) and (g), HiPPO still makes some learning progress. </p>
<p>100 
200 
300 
400 
500 </p>
<h1>episodes</h1>
<p>100 </p>
<p>200 </p>
<p>300 </p>
<p>400 </p>
<p>500 </p>
<p>600 </p>
<p>700 </p>
<p>800 </p>
<p>900 </p>
<p>1000 </p>
<h1>steps till goal</h1>
<p>Goal #1: Get first key </p>
<p>100 
200 
300 
400 
500 </p>
<h1>episodes</h1>
<p>Goal #2: Open first door </p>
<p>100 
200 
300 
400 
500 </p>
<h1>episodes</h1>
<p>Goal #3: Get second key </p>
<p>100 
200 
300 
400 
500 </p>
<h1>episodes</h1>
<p>Goal #4: Open second door </p>
<p>100 
200 
300 
400 
500 </p>
<h1>episodes</h1>
<p>Goal #5: Go to exit </p>
<p>PPO 
PPO+Bonus 
HiPPO 
PPO+Sketch </p>
<p>is presented below.start </p>
<p>All doors closed </p>
<p>key 1 </p>
<p>room 1 </p>
<p>start </p>
<p>First door open </p>
<p>key 2 </p>
<p>room 2 
room 1 </p>
<p>start </p>
<p>Both doors open </p>
<p>exit </p>
<p>Found exit </p>
<p>The action space of the meta-controller is the space of goals. The controller uses available primitive actions.3 Feeding goals as observations into the policy network is slightly different from the original design of Andreas et al.[15]. We plan to investigate the original policy sketch architecture in future work.
A similar approach is taken by the PageRank algorithm.</p>
<p>End-to-end training of deep visuomotor policies. Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel, The Journal of Machine Learning Research. 171Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Nature. 5507676354David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.</p>
<p>Memory lens: How much memory does an agent use?. Christoph Dann, Katja Hofmann, Sebastian Nowozin, arXiv:1611.06928arXiv preprintChristoph Dann, Katja Hofmann, and Sebastian Nowozin. Memory lens: How much memory does an agent use? arXiv preprint arXiv:1611.06928, 2016.</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, Advances in Neural Information Processing Systems. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471-1479, 2016.</p>
<p>Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, arXiv:1706.10295Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprintMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.</p>
<p>Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Y Richard, Xi Chen, Chen, arXiv:1706.01905Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprintMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 1121-2Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2): 181-211, 1999.</p>
<p>The option-critic architecture. Pierre-Luc Bacon, Jean Harb, Doina Precup, AAAI. Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pages 1726-1734, 2017.</p>
<p>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. D Tejas, Karthik Kulkarni, Ardavan Narasimhan, Josh Saeedi, Tenenbaum, Advances in neural information processing systems. Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pages 3675-3683, 2016.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Random walks on graphs. László Lovász, Combinatorics, Paul erdos is eighty. 24László Lovász. Random walks on graphs. Combinatorics, Paul erdos is eighty, 2(1-46):4, 1993.</p>
<p>Minimalistic Gridworld Environment for OpenAI Gym. Maxime Chevalier, - Boisvert, Lucas Willems, Maxime Chevalier-Boisvert and Lucas Willems. Minimalistic Gridworld Environment for OpenAI Gym. https://github.com/maximecb/gym-minigrid, 2018.</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.01540Openai gym. arXiv preprintGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Modular multitask reinforcement learning with policy sketches. Jacob Andreas, Dan Klein, Sergey Levine, arXiv:1611.01796arXiv preprintJacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. arXiv preprint arXiv:1611.01796, 2016.</p>            </div>
        </div>

    </div>
</body>
</html>
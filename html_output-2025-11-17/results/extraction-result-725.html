<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-725 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-725</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-725</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-5f614777d25efd14b7426e99cb2544f2d6be133e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5f614777d25efd14b7426e99cb2544f2d6be133e" target="_blank">A Benchmark for Interpretability Methods in Deep Neural Networks</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> An empirical measure of the approximate accuracy of feature importance estimates in deep neural networks is proposed and it is shown that some approaches do no better then the underlying method but carry a far higher computational burden.</p>
                <p><strong>Paper Abstract:</strong> We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e725.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e725.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>deletion-no-retrain-gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation-by-deletion without retraining causes distribution shift (ROAR motivation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mismatch between the common natural-language experimental protocol that measures feature importance by deleting features at inference time and the implementation practice of not retraining the model on the modified data; this creates a distribution shift that confounds attribution evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>interpretability evaluation pipeline (deletion-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental pipeline that assesses feature-importance estimators by replacing (deleting) input features and measuring model accuracy on the modified inputs without retraining the model.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / prior-evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment script / evaluation code (deletion at inference-time)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>distribution-shift caused by omission of retraining (method vs implementation mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior natural-language descriptions and commonly used evaluation scripts replace important input features in validation/test images and measure accuracy drop on the same pre-trained model (no retraining). The paper shows this violates the train/test i.i.d. assumption: the modified inputs live off the original training distribution, so the measured accuracy drop conflates damage from distribution shift with true information removal. Thus the evaluation protocol described in papers (and implemented in many experiments) is not faithful to the claim that accuracy drop measures feature importance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation procedure / data preprocessing and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility comparison experiments: direct empirical comparison between (a) deletion-without-retraining (traditional deletion metric) and (b) Remove-And-Retrain (ROAR) across toy data and large-scale datasets; manual inspection of performance behaviour</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparative accuracy measurements across settings and thresholds t (fraction of inputs modified). Key quantitative comparisons include: random 90% modification -> no retrain accuracy ≈ 0.5% (on ImageNet model originally trained on clean data) vs retrain (ROAR) accuracy ≈ 63.53% ± 0.13; toy-data plots showing worst-case ranking appears informative without retraining but is identified as poor by ROAR until informative features removed (~75%).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: without retraining deletion metrics can massively overstate the impact of removing features and thus incorrectly credit attribution methods. Example: random 90% deletion without retraining reduced accuracy to 0.5% vs ROAR-retrained models retaining ~63.5%, leading to false conclusions about estimator effectiveness and misleading comparisons between methods.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common in prior modification-based evaluations (the paper cites earlier works [30] and subsequent variations that use deletion without retraining), i.e., a standard practice in the literature prior to this critique (no explicit numeric prevalence provided).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implicit assumption in natural-language protocols that removing inputs at test time is an adequate proxy for removing information; incomplete experimental specification (omission of retraining), and underappreciation of distribution-shift effects.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Retrain models on the modified dataset for each ranking/threshold (the ROAR protocol), repeat training multiple times to control variance, and when applicable recompute importance after retraining for models that perform implicit feature selection.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated effective in the paper: ROAR decouples distribution-shift artifacts from true information removal, correctly identifies poor rankings on toy data (no degradation until informative features removed) and yields different, more reliable comparative results (e.g., base estimators perform no better than random under ROAR, despite appearing to work under no-retrain deletion metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Benchmark for Interpretability Methods in Deep Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e725.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e725.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>smoothgrad-impl-mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between published SmoothGrad description and default open-source implementation (SG vs SG-SQ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrepancy where the SmoothGrad paper describes averaging noisy gradient estimates, but the default open-source implementation squares estimates before averaging (SmoothGrad-Squared), an undocumented variant with materially different behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>interpretability estimator implementation (SmoothGrad)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Algorithmic implementation of SmoothGrad: ensemble-based estimator that aggregates gradients computed on noisy copies of the input to produce a smoothed saliency map.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper description vs open-source implementation README/code</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>open-source library / reference implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation divergence from published specification (undocumented variant)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The published SmoothGrad method averages noisy gradient estimates (mean), but the open-source default code used by practitioners squares each noisy estimate before averaging (SG-SQ). The paper notes SG-SQ is not described in the original publication, yet it is the default open-source behavior — a mismatch between the natural-language method specification and the actual code used by many users.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>estimator implementation / algorithmic aggregation step</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>code inspection and comparison of algorithm description versus open-source implementation; empirical evaluation (ROAR) showing different outcomes for SG and SG-SQ</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical comparison using ROAR across datasets and thresholds: SG (classic SmoothGrad) often performs worse than a random baseline and sometimes worse than a single estimate, while SG-SQ (squared-then-average) substantially outperforms both SG and base methods in ROAR accuracy degradation metrics (table and plots in paper). The paper reports large accuracy gaps (see Table 2 and Fig. 4) demonstrating different performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Significant: users running the open-source SmoothGrad code (SG-SQ) will observe different empirical behavior than expected from the original paper; evaluations and claims about SmoothGrad in the literature may be inconsistent depending on which implementation was used, potentially leading to incorrect attributions about the method's effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Specific to SmoothGrad and its open-source forks; prevalence depends on how widely that default implementation is used. Paper indicates the SG-SQ variant is the default in at least one popular open-source SmoothGrad repository.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Undocumented change in implementation (squaring before averaging) relative to the published description; insufficient documentation tying code to paper algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Audit and document implementation differences between code and paper; when evaluating methods, explicitly report the exact implementation used (including preprocessing and aggregation operations); use benchmarks like ROAR to test the empirical effect of such implementation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Paper demonstrates that distinguishing SG vs SG-SQ matters empirically (SG-SQ and VarGrad perform much better under ROAR), so documenting and choosing the intended variant is an effective mitigation to prevent confusion; no numeric 'fix rate' provided but empirical results show large performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning interpretability / software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Benchmark for Interpretability Methods in Deep Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e725.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e725.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>configuration-mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Method configuration and hyperparameter mismatch (wrong configuration diminishes effectiveness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discrepancy arising when papers or documentation omit or under-specify critical configuration choices (e.g., number of integration steps, reference point, noise scale, ensembling count), causing code users to run different variants that alter empirical effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>interpretability method configuration in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The set of hyperparameters and procedural choices required to execute interpretability methods (e.g., k steps for Integrated Gradients, number of noisy samples J for SmoothGrad, noise scale, reference baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / algorithm specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment script / library API usage</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing or ambiguous hyperparameter specification / incomplete experimental specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Theoretical properties or algorithm descriptions often assume particular parameter settings (e.g., IG requires a reference image and k steps), but papers or docs may omit exact default values or recommendations; wrong or inconsistent settings in code (or failures to follow recommended configuration) can render methods ineffective. The paper cites prior work showing wrong configuration easily renders methods ineffective.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters / method configuration in code and experiment scripts</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical benchmarking across different method variants and configurations using ROAR; references to prior unit-test/sanity-check studies that demonstrate sensitivity to parameter settings.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Performance measured as model accuracy after feature removal and retraining across different parameterizations (e.g., IG with k=25 as used here); comparison against random and sobel baselines. The paper reports that base estimators performed worse than random under their chosen config, and that ensembling counts (they used J=15) and squaring affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>High: incorrect/missing configuration can flip conclusions about which estimator is better (e.g., SmoothGrad can be worse than single estimate under some settings; squaring or variance aggregation dramatically changes performance), leading to inconsistent or irreproducible research claims.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widely relevant across interpretability literature; the paper references multiple works and sanity-check studies indicating this is a recurrent problem (no numeric prevalence provided).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or incomplete natural-language descriptions of recommended/required hyperparameters, and insufficient reporting in code/papers of exact settings used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report exact hyperparameters and code, provide open-source reference implementations, run robustness checks across reasonable parameter ranges, and use empirical benchmarks (like ROAR) to validate configuration choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective when adopted: the paper uses recommended settings (e.g., IG k=25, SG J=15) and demonstrates stable comparative results across repeated runs; however no general numeric measure of mitigation effectiveness is given beyond empirical demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Benchmark for Interpretability Methods in Deep Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e725.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e725.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>replacement-value-underspec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Underspecified replacement value and its artifacts (uninformative-value assumption mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gap where natural-language descriptions treat a replacement value as 'uninformative' without ensuring the implementation and training ensure the model treats it as such; omission leads to artifacts that distort evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>input-modification step in attribution evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure that replaces removed/kept pixels with a fixed value (the 'replacement value' c) when modifying images for deletion/keep experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol description / methods section</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data preprocessing script / experiment dataset generation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification of replacement value semantics; assumption that replacement is uninformative without training to make it so</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often state that removed pixels are replaced with an uninformative value (e.g., per-channel mean or black image) but do not state that the model must be trained on data containing that replacement value to ensure it is interpreted as uninformative. If the model is not retrained (or if replacement selection introduces artifacts), the replacement itself can cause large accuracy drops unrelated to information removal.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / replacement-value choice and downstream training</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical comparison (retrain vs no-retrain) showing large differences in degradation attributable to replacement artifacts; toy-data experiments demonstrating spurious evaluator signals when replacement introduces out-of-distribution artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Accuracy under different regimes: e.g., on ImageNet, random 90% replacement without retraining reduced accuracy to ~0.5% whereas retraining with the replacement value resulted in ~63.53% accuracy — quantifying the artifact effect.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Severe: artifacts from replacement values can dominate measured accuracy drop, leading to incorrect conclusions about the importance of removed features and invalid comparisons between interpretability methods.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common across modification-based evaluations where replacement is used but retraining is not performed; the paper argues this is a core drawback of many prior deletion-based evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implicit assumption in natural-language protocols that a chosen 'uninformative' replacement is truly uninformative for the model even without retraining; incomplete documentation of the need to align replacement with training.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Train (or retrain) the model on datasets containing the replacement value so the model can learn to treat it as uninformative (ROAR). Carefully choose and document replacement values and run controls (random replacement, sobel) to set baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Shown effective in paper: retraining (ROAR) markedly reduces artifact-driven accuracy loss and yields more faithful evaluations. Quantitatively demonstrated via the example accuracies above.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / experimental methodology</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Benchmark for Interpretability Methods in Deep Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e725.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e725.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>feature-selection-models-limit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations when applying ROAR to models that perform implicit feature selection (e.g., decision stumps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A documented mismatch where the ROAR protocol (remove-and-retrain without recomputing importance) can give misleading results for models that ignore subsets of features at inference time unless importance is recomputed after retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ROAR evaluation pipeline when applied to feature-selecting models</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ROAR workflow that replaces ranked pixels/features and retrains models from scratch but does not recompute the feature ranking after retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / applicability claims</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation pipeline / training scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>assumption-violation: protocol applicability mismatch for models with built-in feature selection</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>ROAR assumes evaluated models allow all features to contribute at test time. For models that perform explicit/implicit feature selection (e.g., decision stumps or models that ignore features), masking and retraining without recomputing feature importance can produce misleading evaluations (a random estimator can appear better than best estimator).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation pipeline / model class applicability</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>theoretical reasoning supplemented by discussion and a worked example (decision stump) in the supplement; recognition that for such models feature importance must be recomputed after retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative assessment and warning; no large-scale quantitative measurement provided for this limitation in the paper, but authors state recomputing importance after each retrain would be required to make ROAR valid for such models.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially large for affected model classes: ROAR can be invalid if applied naively to models that ignore subsets of features, causing incorrect ranking evaluations (e.g., random estimator appearing better).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Limited to classes of models that perform feature selection or ignore inputs at inference; not common for the deep neural networks evaluated in the paper but relevant for models like decision stumps or heavily regularized sparse models.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mismatch between protocol assumptions (all features can influence prediction) and specific model behaviors (feature selection / ignoring features) and omission of a recomputation step in the protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>For models that ignore features, recompute feature importance after each retraining step or mask features known to be ignored before retraining; document applicability limits of ROAR.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Conceptually effective; the authors state recomputing importance after retraining makes ROAR valid for such models, but they did not implement this at scale in the paper due to computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / interpretability / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Benchmark for Interpretability Methods in Deep Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating the Visualization of What a Deep Neural Network Has Learned <em>(Rating: 2)</em></li>
                <li>SmoothGrad: removing noise by adding noise <em>(Rating: 2)</em></li>
                <li>Sanity checks for saliency maps <em>(Rating: 2)</em></li>
                <li>The (Un)reliability of saliency methods <em>(Rating: 2)</em></li>
                <li>Axiomatic attribution for deep networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-725",
    "paper_id": "paper-5f614777d25efd14b7426e99cb2544f2d6be133e",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "deletion-no-retrain-gap",
            "name_full": "Evaluation-by-deletion without retraining causes distribution shift (ROAR motivation)",
            "brief_description": "A mismatch between the common natural-language experimental protocol that measures feature importance by deleting features at inference time and the implementation practice of not retraining the model on the modified data; this creates a distribution shift that confounds attribution evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "interpretability evaluation pipeline (deletion-based evaluation)",
            "system_description": "Experimental pipeline that assesses feature-importance estimators by replacing (deleting) input features and measuring model accuracy on the modified inputs without retraining the model.",
            "nl_description_type": "research paper methods section / prior-evaluation protocol",
            "code_implementation_type": "experiment script / evaluation code (deletion at inference-time)",
            "gap_type": "distribution-shift caused by omission of retraining (method vs implementation mismatch)",
            "gap_description": "Prior natural-language descriptions and commonly used evaluation scripts replace important input features in validation/test images and measure accuracy drop on the same pre-trained model (no retraining). The paper shows this violates the train/test i.i.d. assumption: the modified inputs live off the original training distribution, so the measured accuracy drop conflates damage from distribution shift with true information removal. Thus the evaluation protocol described in papers (and implemented in many experiments) is not faithful to the claim that accuracy drop measures feature importance.",
            "gap_location": "evaluation procedure / data preprocessing and evaluation",
            "detection_method": "reproducibility comparison experiments: direct empirical comparison between (a) deletion-without-retraining (traditional deletion metric) and (b) Remove-And-Retrain (ROAR) across toy data and large-scale datasets; manual inspection of performance behaviour",
            "measurement_method": "Comparative accuracy measurements across settings and thresholds t (fraction of inputs modified). Key quantitative comparisons include: random 90% modification -&gt; no retrain accuracy ≈ 0.5% (on ImageNet model originally trained on clean data) vs retrain (ROAR) accuracy ≈ 63.53% ± 0.13; toy-data plots showing worst-case ranking appears informative without retraining but is identified as poor by ROAR until informative features removed (~75%).",
            "impact_on_results": "Substantial: without retraining deletion metrics can massively overstate the impact of removing features and thus incorrectly credit attribution methods. Example: random 90% deletion without retraining reduced accuracy to 0.5% vs ROAR-retrained models retaining ~63.5%, leading to false conclusions about estimator effectiveness and misleading comparisons between methods.",
            "frequency_or_prevalence": "Common in prior modification-based evaluations (the paper cites earlier works [30] and subsequent variations that use deletion without retraining), i.e., a standard practice in the literature prior to this critique (no explicit numeric prevalence provided).",
            "root_cause": "Implicit assumption in natural-language protocols that removing inputs at test time is an adequate proxy for removing information; incomplete experimental specification (omission of retraining), and underappreciation of distribution-shift effects.",
            "mitigation_approach": "Retrain models on the modified dataset for each ranking/threshold (the ROAR protocol), repeat training multiple times to control variance, and when applicable recompute importance after retraining for models that perform implicit feature selection.",
            "mitigation_effectiveness": "Demonstrated effective in the paper: ROAR decouples distribution-shift artifacts from true information removal, correctly identifies poor rankings on toy data (no degradation until informative features removed) and yields different, more reliable comparative results (e.g., base estimators perform no better than random under ROAR, despite appearing to work under no-retrain deletion metrics).",
            "domain_or_field": "machine learning / deep learning interpretability",
            "reproducibility_impact": true,
            "uuid": "e725.0",
            "source_info": {
                "paper_title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "smoothgrad-impl-mismatch",
            "name_full": "Mismatch between published SmoothGrad description and default open-source implementation (SG vs SG-SQ)",
            "brief_description": "A discrepancy where the SmoothGrad paper describes averaging noisy gradient estimates, but the default open-source implementation squares estimates before averaging (SmoothGrad-Squared), an undocumented variant with materially different behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "interpretability estimator implementation (SmoothGrad)",
            "system_description": "Algorithmic implementation of SmoothGrad: ensemble-based estimator that aggregates gradients computed on noisy copies of the input to produce a smoothed saliency map.",
            "nl_description_type": "research paper description vs open-source implementation README/code",
            "code_implementation_type": "open-source library / reference implementation",
            "gap_type": "implementation divergence from published specification (undocumented variant)",
            "gap_description": "The published SmoothGrad method averages noisy gradient estimates (mean), but the open-source default code used by practitioners squares each noisy estimate before averaging (SG-SQ). The paper notes SG-SQ is not described in the original publication, yet it is the default open-source behavior — a mismatch between the natural-language method specification and the actual code used by many users.",
            "gap_location": "estimator implementation / algorithmic aggregation step",
            "detection_method": "code inspection and comparison of algorithm description versus open-source implementation; empirical evaluation (ROAR) showing different outcomes for SG and SG-SQ",
            "measurement_method": "Empirical comparison using ROAR across datasets and thresholds: SG (classic SmoothGrad) often performs worse than a random baseline and sometimes worse than a single estimate, while SG-SQ (squared-then-average) substantially outperforms both SG and base methods in ROAR accuracy degradation metrics (table and plots in paper). The paper reports large accuracy gaps (see Table 2 and Fig. 4) demonstrating different performance.",
            "impact_on_results": "Significant: users running the open-source SmoothGrad code (SG-SQ) will observe different empirical behavior than expected from the original paper; evaluations and claims about SmoothGrad in the literature may be inconsistent depending on which implementation was used, potentially leading to incorrect attributions about the method's effectiveness.",
            "frequency_or_prevalence": "Specific to SmoothGrad and its open-source forks; prevalence depends on how widely that default implementation is used. Paper indicates the SG-SQ variant is the default in at least one popular open-source SmoothGrad repository.",
            "root_cause": "Undocumented change in implementation (squaring before averaging) relative to the published description; insufficient documentation tying code to paper algorithm.",
            "mitigation_approach": "Audit and document implementation differences between code and paper; when evaluating methods, explicitly report the exact implementation used (including preprocessing and aggregation operations); use benchmarks like ROAR to test the empirical effect of such implementation choices.",
            "mitigation_effectiveness": "Paper demonstrates that distinguishing SG vs SG-SQ matters empirically (SG-SQ and VarGrad perform much better under ROAR), so documenting and choosing the intended variant is an effective mitigation to prevent confusion; no numeric 'fix rate' provided but empirical results show large performance differences.",
            "domain_or_field": "machine learning / deep learning interpretability / software engineering",
            "reproducibility_impact": true,
            "uuid": "e725.1",
            "source_info": {
                "paper_title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "configuration-mismatch",
            "name_full": "Method configuration and hyperparameter mismatch (wrong configuration diminishes effectiveness)",
            "brief_description": "Discrepancy arising when papers or documentation omit or under-specify critical configuration choices (e.g., number of integration steps, reference point, noise scale, ensembling count), causing code users to run different variants that alter empirical effectiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "interpretability method configuration in experiments",
            "system_description": "The set of hyperparameters and procedural choices required to execute interpretability methods (e.g., k steps for Integrated Gradients, number of noisy samples J for SmoothGrad, noise scale, reference baseline).",
            "nl_description_type": "research paper methods section / algorithm specification",
            "code_implementation_type": "experiment script / library API usage",
            "gap_type": "missing or ambiguous hyperparameter specification / incomplete experimental specification",
            "gap_description": "Theoretical properties or algorithm descriptions often assume particular parameter settings (e.g., IG requires a reference image and k steps), but papers or docs may omit exact default values or recommendations; wrong or inconsistent settings in code (or failures to follow recommended configuration) can render methods ineffective. The paper cites prior work showing wrong configuration easily renders methods ineffective.",
            "gap_location": "hyperparameters / method configuration in code and experiment scripts",
            "detection_method": "empirical benchmarking across different method variants and configurations using ROAR; references to prior unit-test/sanity-check studies that demonstrate sensitivity to parameter settings.",
            "measurement_method": "Performance measured as model accuracy after feature removal and retraining across different parameterizations (e.g., IG with k=25 as used here); comparison against random and sobel baselines. The paper reports that base estimators performed worse than random under their chosen config, and that ensembling counts (they used J=15) and squaring affect outcomes.",
            "impact_on_results": "High: incorrect/missing configuration can flip conclusions about which estimator is better (e.g., SmoothGrad can be worse than single estimate under some settings; squaring or variance aggregation dramatically changes performance), leading to inconsistent or irreproducible research claims.",
            "frequency_or_prevalence": "Widely relevant across interpretability literature; the paper references multiple works and sanity-check studies indicating this is a recurrent problem (no numeric prevalence provided).",
            "root_cause": "Ambiguous or incomplete natural-language descriptions of recommended/required hyperparameters, and insufficient reporting in code/papers of exact settings used in experiments.",
            "mitigation_approach": "Report exact hyperparameters and code, provide open-source reference implementations, run robustness checks across reasonable parameter ranges, and use empirical benchmarks (like ROAR) to validate configuration choices.",
            "mitigation_effectiveness": "Effective when adopted: the paper uses recommended settings (e.g., IG k=25, SG J=15) and demonstrates stable comparative results across repeated runs; however no general numeric measure of mitigation effectiveness is given beyond empirical demonstration.",
            "domain_or_field": "machine learning / deep learning interpretability",
            "reproducibility_impact": true,
            "uuid": "e725.2",
            "source_info": {
                "paper_title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "replacement-value-underspec",
            "name_full": "Underspecified replacement value and its artifacts (uninformative-value assumption mismatch)",
            "brief_description": "A gap where natural-language descriptions treat a replacement value as 'uninformative' without ensuring the implementation and training ensure the model treats it as such; omission leads to artifacts that distort evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "input-modification step in attribution evaluations",
            "system_description": "Procedure that replaces removed/kept pixels with a fixed value (the 'replacement value' c) when modifying images for deletion/keep experiments.",
            "nl_description_type": "experimental protocol description / methods section",
            "code_implementation_type": "data preprocessing script / experiment dataset generation",
            "gap_type": "incomplete specification of replacement value semantics; assumption that replacement is uninformative without training to make it so",
            "gap_description": "Papers often state that removed pixels are replaced with an uninformative value (e.g., per-channel mean or black image) but do not state that the model must be trained on data containing that replacement value to ensure it is interpreted as uninformative. If the model is not retrained (or if replacement selection introduces artifacts), the replacement itself can cause large accuracy drops unrelated to information removal.",
            "gap_location": "data preprocessing / replacement-value choice and downstream training",
            "detection_method": "empirical comparison (retrain vs no-retrain) showing large differences in degradation attributable to replacement artifacts; toy-data experiments demonstrating spurious evaluator signals when replacement introduces out-of-distribution artifacts.",
            "measurement_method": "Accuracy under different regimes: e.g., on ImageNet, random 90% replacement without retraining reduced accuracy to ~0.5% whereas retraining with the replacement value resulted in ~63.53% accuracy — quantifying the artifact effect.",
            "impact_on_results": "Severe: artifacts from replacement values can dominate measured accuracy drop, leading to incorrect conclusions about the importance of removed features and invalid comparisons between interpretability methods.",
            "frequency_or_prevalence": "Common across modification-based evaluations where replacement is used but retraining is not performed; the paper argues this is a core drawback of many prior deletion-based evaluations.",
            "root_cause": "Implicit assumption in natural-language protocols that a chosen 'uninformative' replacement is truly uninformative for the model even without retraining; incomplete documentation of the need to align replacement with training.",
            "mitigation_approach": "Train (or retrain) the model on datasets containing the replacement value so the model can learn to treat it as uninformative (ROAR). Carefully choose and document replacement values and run controls (random replacement, sobel) to set baselines.",
            "mitigation_effectiveness": "Shown effective in paper: retraining (ROAR) markedly reduces artifact-driven accuracy loss and yields more faithful evaluations. Quantitatively demonstrated via the example accuracies above.",
            "domain_or_field": "machine learning / experimental methodology",
            "reproducibility_impact": true,
            "uuid": "e725.3",
            "source_info": {
                "paper_title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "feature-selection-models-limit",
            "name_full": "Limitations when applying ROAR to models that perform implicit feature selection (e.g., decision stumps)",
            "brief_description": "A documented mismatch where the ROAR protocol (remove-and-retrain without recomputing importance) can give misleading results for models that ignore subsets of features at inference time unless importance is recomputed after retraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ROAR evaluation pipeline when applied to feature-selecting models",
            "system_description": "ROAR workflow that replaces ranked pixels/features and retrains models from scratch but does not recompute the feature ranking after retraining.",
            "nl_description_type": "experimental protocol / applicability claims",
            "code_implementation_type": "evaluation pipeline / training scripts",
            "gap_type": "assumption-violation: protocol applicability mismatch for models with built-in feature selection",
            "gap_description": "ROAR assumes evaluated models allow all features to contribute at test time. For models that perform explicit/implicit feature selection (e.g., decision stumps or models that ignore features), masking and retraining without recomputing feature importance can produce misleading evaluations (a random estimator can appear better than best estimator).",
            "gap_location": "evaluation pipeline / model class applicability",
            "detection_method": "theoretical reasoning supplemented by discussion and a worked example (decision stump) in the supplement; recognition that for such models feature importance must be recomputed after retraining.",
            "measurement_method": "Qualitative assessment and warning; no large-scale quantitative measurement provided for this limitation in the paper, but authors state recomputing importance after each retrain would be required to make ROAR valid for such models.",
            "impact_on_results": "Potentially large for affected model classes: ROAR can be invalid if applied naively to models that ignore subsets of features, causing incorrect ranking evaluations (e.g., random estimator appearing better).",
            "frequency_or_prevalence": "Limited to classes of models that perform feature selection or ignore inputs at inference; not common for the deep neural networks evaluated in the paper but relevant for models like decision stumps or heavily regularized sparse models.",
            "root_cause": "Mismatch between protocol assumptions (all features can influence prediction) and specific model behaviors (feature selection / ignoring features) and omission of a recomputation step in the protocol.",
            "mitigation_approach": "For models that ignore features, recompute feature importance after each retraining step or mask features known to be ignored before retraining; document applicability limits of ROAR.",
            "mitigation_effectiveness": "Conceptually effective; the authors state recomputing importance after retraining makes ROAR valid for such models, but they did not implement this at scale in the paper due to computational cost.",
            "domain_or_field": "machine learning / interpretability / evaluation methodology",
            "reproducibility_impact": true,
            "uuid": "e725.4",
            "source_info": {
                "paper_title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating the Visualization of What a Deep Neural Network Has Learned",
            "rating": 2
        },
        {
            "paper_title": "SmoothGrad: removing noise by adding noise",
            "rating": 2
        },
        {
            "paper_title": "Sanity checks for saliency maps",
            "rating": 2
        },
        {
            "paper_title": "The (Un)reliability of saliency methods",
            "rating": 2
        },
        {
            "paper_title": "Axiomatic attribution for deep networks",
            "rating": 1
        }
    ],
    "cost": 0.014842,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Benchmark for Interpretability Methods in Deep Neural Networks</h1>
<p>Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim<br>Google Brain<br>shooker,dumitru,pikinder,beenkim@google.com</p>
<h4>Abstract</h4>
<p>We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches-VarGrad and SmoothGrad-Squared-outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.</p>
<h2>1 Introduction</h2>
<p>In a machine learning setting, a question of great interest is estimating the influence of a given input feature to the prediction made by a model. Understanding what features are important helps improve our models, builds trust in the model prediction and isolates undesirable behavior. Unfortunately, it is challenging to evaluate whether an explanation of model behavior is reliable. First, there is no ground truth. If we knew what was important to the model, we would not need to estimate feature importance in the first place. Second, it is unclear which of the numerous proposed interpretability methods that estimate feature importance one should select $[7,6,44,31,38,34,40,37,20,23,12$, $10,41,32,42,28,35,3]$. Many feature importance estimators have interesting theoretical properties e.g. preservation of relevance [6] or implementation invariance [38]. However even these methods need to be configured correctly [23, 38] and it has been shown that using the wrong configuration can easily render them ineffective [19]. For this reason, it is important that we build a framework to empirically validate the relative merits and reliability of these methods.</p>
<p>A commonly used strategy is to remove the supposedly informative features from the input and look at how the classifier degrades [30]. This method is cheap to evaluate but comes at a significant drawback. Samples where a subset of the features are removed come from a different distribution (as can be seen in Fig. 1). Therefore, this approach clearly violates one of the key assumptions in machine learning: the training and evaluation data come from the same distribution. Without re-training, it is unclear whether the degradation in model performance comes from the distribution shift or because the features that were removed are truly informative [10, 12].</p>
<p>For this reason we decided to verify how much information can be removed in a typical dataset before accuracy of a retrained model breaks down completely. In this experiment, we applied ResNet-50 [17], one of the most commonly used models, to ImageNet. It turns out that removing information is quite hard. With $90 \%$ of the inputs removed the network still achieves $63.53 \%$ accuracy compared to $76.68 \%$ on clean data. This implies that a strong performance degradation without re-training might be caused by a shift in distribution instead of removal of information.</p>
<p>Instead, in this work we evaluate interpretability methods by verifying how the accuracy of a retrained model degrades as features estimated to be important are removed. We term this approach ROAR, RemOve And Retrain. For each feature importance estimator, ROAR replaces the fraction of the</p>
<p>pixels estimated to be most important with a fixed uninformative value. This modification (shown in Fig. 1) is repeated for each image in both the training and test set. To measure the change to model behavior after the removal of these input features, we separately train new models on the modified dataset such that train and test data comes from a similar distribution. More accurate estimators will identify as important input pixels whose subsequent removal causes the sharpest degradation in accuracy. We also compare each method performance to a random assignment of importance and a sobel edge filter [36]. Both of these control variants produce rankings that are independent of the properties of the model we aim to interpret. Given that these methods do not depend upon the model, the performance of these variants respresent a lower bound of accuracy that a interpretability method could be expected to achieve. In particular, a random baseline allows us to answer the question: is the interpretability method more accurate than a random guess as to which features are important? In Section 3 we will elaborate on the motivation and the limitations of ROAR.</p>
<p>We applied ROAR in a broad set of experiments across three large scale, open source image datasets: ImageNet [11], Food 101 [9] and Birdsnap [8]. In our experiments we show the following.</p>
<ul>
<li>Training performance is quite robust to removing input features. For example, after randomly replacing $90\%$ of all ImageNet input features, we can still train a model that achieves $63.53 \pm 0.13$ (average across 5 independent runs). This implies that a small subset of features are sufficient for the actual decision making. Our observation is consistent across datasets.</li>
<li>The base methods we evaluate are no better or on par with a random estimate at finding the core set of informative features. However, we show that SmoothGrad-Squared (an unpublished variant of Classic SmoothGrad [35]) and Vargrad [3], methods which ensemble a set of estimates produced by basic methods, far outperform both the underlying method and a random guess. These results are consistent across datasets and methods.</li>
<li>Not all ensemble estimators improve performance. Classic SmoothGrad [35] is worse than a single estimate despite being more computationally intensive.</li>
</ul>
<h1>2 Related Work</h1>
<p>Interpretability research is diverse, and many different approaches are used to gain intuition about the function implemented by a neural network. For example, one can distill or constrain a model into a functional form that is considered more interpretable [5, 13, 39, 29]. Other methods explore the role of neurons or activations in hidden layers of the network [25, 27, 24, 43], while others use high level concepts to explain prediction results [18]. Finally there are also the input feature importance estimators that we evaluate in this work. These methods estimate the importance of an input feature for a specified output activation.</p>
<p>While there is no clear way to measure "correctness", comparing the relative merit of different estimators is often based upon human studies [31, 28, 22] which interrogate whether the ranking is meaningful to a human. Recently, there have been efforts to evaluate whether interpretability methods are both reliable and meaningful to human. For example, in [19] a unit test for interpretability methods is proposed which detects whether the explanation can be manipulated by factors that are not affecting the decision making process. Another approach considers a set of sanity checks that measure the change to an estimate as parameters in a model or dataset labels are randomized [3]. Closely related to this manuscript are the modification-based evaluation measures proposed originally by [30] with subsequent variations [20, 26]. In this line of work, one replaces the inputs estimated to be most important with a value considered meaningless to the task. These methods measure the subsequent degradation to the trained model at inference time. Recursive feature elimination methods [16] are a greedy search where the algorithm is trained on an iteratively altered subset of features. Recursive feature elimination does not scale to high dimensional datasets (one would have to retrain removing one pixel at a time) and unlike our work is a method to estimate feature importance (rather than evaluate different existing interpretability methods).</p>
<p>To the best of our knowledge, unlike prior modification based evaluation measures, our benchmark requires retraining the model from random initialization on the modified dataset rather than re-scoring the modified image at inference time. Without this step, we argue that one cannot decouple whether the model's degradation in performance is due to artifacts introduced by the value used to replace</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A single ImageNet image modified according to the ROAR framework. The fraction of pixels estimated to be most important by each interpretability method is replaced with the mean. Above each image, we include the average test-set accuracy for 5 ResNet-50 models independently trained on the modified dataset. From left to right: base estimators (gradient heatmap (GRAD), Integrated Gradients (IG), Guided Backprop (GB)), derivative approaches that ensemble a set of estimates (SmoothGrad Integrated Gradients (SG-SQ-IG), SmoothGrad-Squared Integrated Gradients (SG-SQ-IG), VarGrad Integrated Gradients (Var-IG)) and control variants (random modification (Random) and a sobel edge filter (Sobel)). This image is best visualized in digital format.</p>
<p>The pixels that are removed or due to the approximate accuracy of the estimator. Our work considers several large scale datasets, whereas all previous evaluations have involved a far smaller subset of data [4, 30].</p>
<h1>3 ROAR: Remove And Retrain</h1>
<p>To evaluate a feature importance estimate using ROAR, we sort the input dimensions according to the estimated importance. We compute an estimate <strong>e</strong> of feature importance for every input in the training and test set. We rank each <strong>e</strong> into an ordered set {e<sup>01</sup><sub>i</sub>}<sup>N</sup><sub>i=1</sub>. For the top t fraction of this ordered set, we replace the corresponding pixels in the raw image with the per channel mean. We generate new train and test datasets at different degradation levels t = [0., 10, . . . , 100] (where t is a percentage of all features modified). Afterwards the model is re-trained from random initialization on the new dataset and evaluated on the new test data.</p>
<p>Of course, because re-training can result in slightly different models, it is essential to repeat the training process multiple times to ensure that the variance in accuracy is low. To control for this, we repeat training 5 times for each interpretability method <strong>e</strong> and level of degradation t. We introduce the methodology and motivation for ROAR in the context of linear models and deep neural networks. However, we note that the properties of ROAR differ given an algorithm that explicitly uses feature selection (e.g. L1 regularization or any mechanism which limits the features available to the model at inference time). In this case one should of course mask the inputs that are known to be ignored by the model, before re-training. This will prevent them from being used after re-training, which could otherwise corrupt the ROAR metric. For the remainder of this paper, we focus on the performance of ROAR given deep neural networks and linear models which do not present this limitation.</p>
<p>What would happen without re-training? The re-training is the most computationally expensive aspect of ROAR. One should question whether it is actually needed. We argue that re-training is needed because machine learning models typically assume that the train and the test data comes from a similar distribution.</p>
<p>The replacement value $c$ can only be considered uninformative if the model is trained to learn it as such. Without retraining, it is unclear whether degradation in performance is due to the introduction of artifacts outside of the original training distribution or because we actually removed information.This is made explicit in our experiment in Section 4.3.1, we show that without retraining the degradation is far higher than the modest decrease in performance observed with re-training. This suggests retraining has better controlled for artefacts introduced by the modification.</p>
<p>Are we evaluating the right aspects? Re-training does have limitations. For one, while the architecture is the same, the model used during evaluation is not the same as the model on which the feature importance estimates were originally obtained. To understand why ROAR is still meaningful we have to think about what happens when the accuracy degrades, especially when we compare it to a random baseline. The possibilities are:</p>
<ol>
<li>We remove input dimensions and the accuracy drops. In this case, it is very likely that the removed inputs were informative to the original model. ROAR thus gives a good indication that the importance estimate is of high quality.</li>
<li>We remove inputs and the accuracy does not drop. This can be explained as either:
(a) It could be caused by removal of an input that was uninformative to the model. This includes the case where the input might have been informative but not in a way that is useful to the model, for example, when a linear model is used and the relation between the feature and the output is non-linear. Since in such a case the information was not used by the model and it does not show in ROAR we can assume ROAR behaves as intended.
(b) There might be redundancy in the inputs. The same information could represented in another feature. This behavior can be detected with ROAR as we will show in our toy data experiment.</li>
</ol>
<p>Validating the behavior of ROAR on artificial data. To demonstrate the difference between ROAR and an approach without re-training in a controlled environment we generate a 16 dimensional dataset with 4 informative features. Each datapoint $\mathbf{x}$ and its label $y$ was generated as follows:</p>
<p>$$
\mathbf{x}=\frac{\mathbf{a} z}{10}+\mathbf{d} \eta+\frac{\epsilon}{10}, \quad y=(z&gt;0)
$$</p>
<p>All random variables were sampled from a standard normal distribution. The vectors $\mathbf{a}$ and $\mathbf{d}$ are 16 dimensional vectors that were sampled once to generate the dataset. In a only the first 4 values have nonzero values to ensure that there are exactly 4 informative features. The values $\eta, \epsilon$ were sampled independently for each example.
We use a least squares model as this problem can be solved linearly. We compare three rankings: the ground truth importance ranking, random ranking and the inverted ground truth ranking (the worst possible estimate of importance). In the left plot of Fig. 2 we can observe that without re-training the worst case estimator is shown to degrade performance relatively quickly. In contrast, ROAR shows no degradation until informative features begin to be removed at $75 \%$. This correctly shows that this estimator has ranked feature importance poorly (ranked uninformative features as most important).
Finally, we consider ROAR performance given a set of variables that are completely redundant. We note that ROAR might not decrease until all of them are removed. To account for this we measure ROAR at different levels of degradation, with the expectation that across this interval we would be able to detect inflection points in performance that would indicate a set of redundant features. If this happens, we believe that it could be detected easily by the sharp decrease as shown in Fig. 2. Now that we have validated ROAR in a controlled setup, we can move on to our large scale experiments.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A comparison between not retraining and ROAR on artificial data. In the case where the model is not retrained, test-set accuracy quickly erodes despite the worst case ranking of redundant features as most important. This incorrectly evaluates a completely incorrect feature ranking as being informative. ROAR is far better at identifying this worst case estimator, showing no degradation until the features which are informative are removed at 75%. This plot also shows the limitation of ROAR, an accuracy decrease might not happen until a complete set of fully redundant features is removed. To account for this we measure ROAR at different levels of degradation, with the expectation that across this interval we would be able to control for performance given a set of redundant features.</p>
<h1>4 Large scale experiments</h1>
<h3>4.1 Estimators under consideration</h3>
<p>Our evaluation is constrained to a subset of estimators of feature importance. We selected these based on the availability of open source code, consistent guidelines on how to apply them and the ease of implementation given a ResNet-50 architecture [17]. Due to the breadth of the experimental setup it was not possible to include additional methods. However, we welcome the opportunity to consider additional estimators in the future, and in order to make it easy to apply ROAR to additional estimators we have open sourced our code https://bit.ly/2ttLLZB. Below, we briefly introduce each of the methods we evaluate.</p>
<p>Base estimators are estimators that compute a single estimate of importance (as opposed to ensemble methods). While we note that guided backprop and integrated gradients are examples of signal and attribution methods respectively, the performance of these estimators should not be considered representative of other methods, which should be evaluated separately.</p>
<ul>
<li>Gradients or Sensitivity heatmaps [34, 7] (GRAD) are the gradient of the output activation of interest $A_{n}^{l}$ with respect to $x_{i}$ :</li>
</ul>
<p>$$
\mathbf{e}=\frac{\partial A_{n}^{l}}{\partial x_{i}}
$$</p>
<ul>
<li>Guided Backprop [37] (GB) is an example of a signal method that aim to visualize the input patterns that cause the neuron activation $A_{n}^{l}$ in higher layers [37, 40, 20]. GB computes this by using a modified backpropagation step that stops the flow of gradients when less than zero at a ReLu gate.</li>
<li>Integrated Gradients [38] (IG)is an example of an attribution method which assign importance to input features by decomposing the output activation $A_{n}^{l}$ into contributions from the individual input features [6, 38, 23, 33, 20]. Integrated gradients interpolate a set of estimates for values between a non-informative reference point $\mathbf{x}^{0}$ to the actual input $\mathbf{x}$. This integral can be approximated by summing a set of $k$ points at small intervals between $\mathbf{x}^{0}$ and $\mathbf{x}$ :</li>
</ul>
<p>$$
\mathbf{e}=\left(\mathbf{x}<em i="i">{i}-\mathbf{x}</em>
$$}^{0}\right) \times \sum_{i=1}^{k} \frac{\partial f_{w}\left(\mathbf{x}^{0}+\frac{i}{k}\left(\mathbf{x}-\mathbf{x}^{0}\right)\right)}{\partial \mathbf{x}_{\mathbf{i}}} \times \frac{1}{k</p>
<p>The final estimate $\mathbf{e}$ will depend upon both the choice of $k$ and the reference point $\mathbf{x}^{0}$. As suggested by [38], we use a black image as the reference point and set $k$ to be 25 .</p>
<p>Ensembling methods In addition to the base approaches we also evaluate three ensembling methods for feature importance. For all the ensemble approaches that we describe below (SG, SG-SQ, Var), we average over a set of 15 estimates as suggested by in the original SmoothGrad publication [35].</p>
<ul>
<li>Classic SmoothGrad (SG) [35] SG averages a set $J$ noisy estimates of feature importance (constructed by injecting a single input with Gaussian noise $\eta$ independently $J$ times):</li>
</ul>
<p>$$
\mathbf{e}=\sum_{i=1}^{J}\left(g_{i}\left(\mathbf{x}+\eta, A_{n}^{l}\right)\right)
$$</p>
<ul>
<li>SmoothGrad ${ }^{2}$ (SG-SQ) is an unpublished variant of classic SmoothGrad SG which squares each estimate $\mathbf{e}$ before averaging the estimates:</li>
</ul>
<p>$$
\mathbf{e}=\sum_{i=1}^{J}\left(g_{i}\left(\mathbf{x}+\eta, A_{n}^{l}\right)^{2}\right)
$$</p>
<p>Although SG-SQ is not described in the original publication, it is the default open-source implementation of the open source code for SG: https://bit.ly/2Hpx5ob.</p>
<ul>
<li>VarGrad (Var) [3] employs the same methodology as classic SmoothGrad (SG) to construct a set of $\mathrm{t} J$ noisy estimates. However, VarGrad aggregates the estimates by computing the variance of the noisy set rather than the mean.</li>
</ul>
<p>$$
\mathbf{e}=\operatorname{Var}\left(g_{i}\left(\mathbf{x}+\eta, A_{n}^{l}\right)\right)
$$</p>
<p>Control Variants As a control, we compare each estimator to two rankings (a random assignment of importance and a sobel edge filter) that do not depend at all on the model parameters. These controls represent a lower bound in performance that we would expect all interpretability methods to outperform.</p>
<ul>
<li>Random A random estimator $g^{R}$ assigns a random binary importance probability $\mathbf{e} \mapsto 0,1$. This amounts to a binary vector $\mathbf{e} \sim \operatorname{Bernoulli}(1-t)$ where $(1-t)$ is the probability of $e_{i}=1$. The formulation of $g^{R}$ does not depend on either the model parameters or the input image (beyond the number of pixels in the image).</li>
<li>Sobel Edge Filter convolves a hard-coded, separable, integer filter over an image to produce a mask of derivatives that emphasizes the edges in an image. A sobel mask treated as a ranking $\mathbf{e}$ will assign a high score to areas of the image with a high gradient (likely edges).</li>
</ul>
<h1>4.2 Experimental setup</h1>
<p>We use a ResNet-50 model for both generating the feature importance estimates and subsequent training on the modified inputs. ResNet-50 was chosen because of the public code implementations (in both PyTorch [15] and Tensorflow [1]) and because it can be trained to give near to state of art performance in a reasonable amount of time [14].
For all train and validation images in the dataset we first apply test time pre-processing as used by Goyal et al. [14]. We evaluate ROAR on three open source image datasets: ImageNet, Birdsnap and Food 101. For each dataset and estimator, we generate new train and test sets that each correspond to a different fraction of feature modification $t=[0,10,30,50,70,90]$. We evaluate 18 estimators in total (this includes the base estimators, a set of ensemble approaches wrapped around each base and finally a set of squared estimates). In total, we generate 540 large-scale modified image datasets in order to consider all experiment variants ( 180 new test/train for each original dataset).
We independently train 5 ResNet-50 models from random initialization on each of these modified dataset and report test accuracy as the average of these 5 runs. In the base implementation, the ResNet-50 trained on an unmodified ImageNet dataset achieves a mean accuracy of $76.68 \%$. This is comparable to the performance reported by [14]. On Birdsnap and Food 101, our unmodified datasets achieve $66.65 \%$ and $84.54 \%$ respectively (average of 10 independent runs). This baseline performance is comparable to that reported by Kornblith et al. [21].</p>
<h1>4.3 Experimental results</h1>
<h3>4.3.1 Evaluating the random ranking</h3>
<p>Comparing estimators to the random ranking allows us to answer the question: is the estimate of importance more accurate than a random guess? It is firstly worthwhile noting that model performance is remarkably robust to random modification. After replacing a large portion of all inputs with a constant value, the model not only trains but still retains most of the original predictive power. For example, on ImageNet, when only $10 \%$ of all features are retained, the trained model still attains $63.53 \%$ accuracy (relative to unmodified baseline of $76.68 \%$ ). The ability of the model to extract a meaningful representation from a small random fraction of inputs suggests a case where many inputs are likely redundant. The nature of our input-an image where correlations between pixels are expected - provides one possible readons for redundancy.
The results for our random baseline provides additional support for the need to re-train. We can compare random ranking on ROAR vs. a traditional deletion metric [30], i.e. the setting where we do not retain. These results are given in Fig. 3. Without retraining, a random modification of $90 \%$ degrades accuracy to $0.5 \%$ for the model that was not retrained. Keep in mind that on clean data we achieve $76.68 \%$ accuracy. This large discrepancy illustrates that without retraining the model, it is not possible to decouple the performance of the ranking from the degradation caused by the modification itself.</p>
<h3>4.3.2 Evaluating Base Estimators</h3>
<p>Now that we have established the baselines, we can start evaluating the base estimators: GB, IG, GRAD. Surprisingly, the left inset of Fig. 4 shows that these estimators consistently perform worse than the random assignment of feature importance across all datasets and for all thresholds $t=[0.1,0.3,0.5,0.7,0.9]$. Furthermore, our estimators fall further behind the accuracy of random guess as a larger fraction $t$ of inputs is modified. The gap is widest when $t=0.9$.
Our base estimators also do not compare favorably to the performance of a sobel edge filter SOBEL. Both the sobel filter and the random ranking have formulations that are entirely independent of the model parameters. All the base estimators that we consider have formulations that depend upon the trained model weights, and thus we would expect them to have a clear advantage in outperforming the control variants. However, across all datasets and thresholds $t$, the base estimators GB, IG, GRAD perform on par or worse than SOBEL.
Base estimators perform within a very narrow range. Despite the very different formulations of base estimators that we consider, the difference between the performance of the base estimators is in a strikingly narrow range. For example, as can be seen in the left column of Fig. 4, for Birdsnap, the difference in accuracy between the best and worst base estimator at $t=90 \%$ is only $4.22 \%$. This range remains narrow for both Food101 and ImageNet, with a gap of $5.17 \%$ and 3.62 respectively. Our base estimator results are remarkably consistent results across datasets, methods and for all fractions of $t$ considered. The variance is very low across independent runs for all datasets and estimators. The maximum variance observed for ImageNet was a variance of $1.32 \%$ using SG-SQGRAD at $70 \%$ of inputs removed. On Birdsnap the highest variance was $0.12 \%$ using VAR-GRAD at $90 \%$ removed. For food101 it was $1.52 \%$ using SG-SQ-GRAD at $70 \%$ removed.
Finally, we compare performance of the base estimators using ROAR re-training vs. a traditional deletion metric [30], again the setting where we do not retain. In Fig. 3 we see a behavior for the base estimators on all datasets that is similar to the behavior of the inverse (worst possible) ranking on the toy data in Fig. 2. The base estimators appear to be working when we do not retrain, but they are clearly not better than the random baseline when evaluated using ROAR. This provides additional support for the need to re-train.</p>
<h3>4.3.3 Evaluating Ensemble Approaches</h3>
<p>Since the base estimators do not appear to perform well, we move on to ensemble estimators. Ensemble approaches inevitably carry a higher computational approach, as the methodology requires the aggregation of a set of individual estimates. However, these methods are often preferred by humans because they appear to produce "less noisy" explanations. However, there is limited theoretical understanding of what these methods are actually doing or how this is related to the accuracy of the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: On the left we evaluate three base estimators and the random baseline without retraining. All of the methods appear to reduce accuracy at quite a high rate. On the right, we see, using ROAR, that after re-training most of the information is actually still present. It is also striking that in this case the base estimators perform worse than the random baseline.</p>
<p>Only Certain Ensemble Approaches Benefit Performance
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Left: Grad (GRAD), Integrated Gradients (IG) and Guided Backprop (GB) perform worse than a random assignment of feature importance. Middle: SmoothGrad (SG) is less accurate than a random assignment of importance and often worse than a single estimate (in the case of raw gradients SG-Grad and Integrated Gradients SG-IG). Right: SmoothGrad Squared (SG-SQ) and VarGrad (VAR) produce a dramatic improvement in approximate accuracy and far outperform the other methods in all datasets considered, regardless of the underlying estimator.
explanation. We evaluate ensemble estimators and produce results that are remarkably consistent results across datasets, methods and for all fractions of $t$ considered.</p>
<p>Classic Smoothgrad is less accurate or on par with a single estimate. In the middle column in Fig. 4 we evaluate Classic SmoothGrad (SG). It average 15 estimates computed according to an underlying base method (GRAD, IG or GB). However, despite averaging SG degrades test-set accuracy still less than a random guess. In addition, for GRAD and IG SmoothGrad performs worse than a single estimate.</p>
<p>SmoothGrad-Squared and VarGrad produce large gains in accuracy. In the right inset of Fig. 4, we show that both VarGrad (VAR) and SmoothGrad-Squared (SG-SQ) far outperform the two control variants. In addition, for all the interpretability methods we consider, VAR or SG-SQ far outperform the approximate accuracy of a single estimate. However, while VAR and SG-SQ benefit the accuracy of all base estimators, the overall ranking of estimator performance differs by dataset. For ImageNet and Food101, the best performing estimators are VAR or SG-SQ when wrapped around GRAD. However, for the Birdsnap dataset, the most approximately accurate estimates are these ensemble approaches wrapped around GB. This suggests that while the VAR and SG-SQ consistently improve performance, the choice of the best underlying estimator may vary by task.</p>
<p>Now, why do both of these methods work so well? First, these methods are highly similar. If the average (squared) gradient over the noisy samples is zero then VAR and SG-SQ reduce to the same method. For many images it appears that the mean gradient is much smaller than the mean squared gradient. This implies that the final output should be similar. Qualitatively this seems to be the case as well. In Fig. 1 we observe that both methods appear to remove whole objects. The other methods removed inputs that are less concentrated but spread more widely over the image. It is important to note that these methods were not forced to behave as such. It is emergent behavior. Understanding why this happens and why this is beneficial should be the focus of future work.</p>
<p>Squaring estimates The final question we consider is why SmoothGrad-Squared SG-SQ dramatically improves upon the performance of SmoothGrad SG despite little difference in formulation. The only difference between the two estimates is that SG-SQ squares the estimates before averaging. We consider the effect of only squaring estimates (no ensembling). We find that while squaring improves the accuracy of all estimators, the transformation does not adequately explain the large gains that we observe when applying VAR or SG-SQ. When base estimators are squared they slightly outperform the random baseline (all results included in the supplementary materials).</p>
<h1>5 Conclusion and Future Work</h1>
<p>In this work, we propose ROAR to evaluate the quality of input feature importance estimators. Surprisingly, we find that the commonly used base estimators, Gradients, Integrated Gradients and Guided BackProp are worse or on par with a random assignment of importance. Furthermore, certain ensemble approaches such as SmoothGrad are far more computationally intensive but do not improve upon a single estimate (and in some cases are worse). However, we do find that VarGrad and SmoothGrad-Squared strongly improve the quality of these methods and far outperform a random guess. While the low effectiveness of many methods could be seen as a negative result, we view the remarkable effectiveness of SmoothGrad-Squared and VarGrad as important progress within the community. Our findings are particularly pertinent for sensitive domains where the accuracy of a explanation of model behavior is paramount. While we venture some initial consideration of why certain ensemble methods far outperform other estimator, the divergence in performance between the ensemble estimators is an important direction of future research.</p>
<h2>Acknowledgments</h2>
<p>We thank Gabriel Bender, Kevin Swersky, Andrew Ross, Douglas Eck, Jonas Kemp, Melissa Fabros, Julius Adebayo, Simon Kornblith, Prajit Ramachandran, Niru Maheswaranathan, Gamaleldin Elsayed, Hugo Larochelle, Varun Vasudevan for their thoughtful feedback on earlier iterations of this work. In addition, thanks to Sally Jesmonth, Dan Nanas and Alexander Popper for institutional support and encouragement.</p>
<h2>References</h2>
<p>[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow:</p>
<p>Large-scale machine learning on heterogeneous systems, January 2015. Software available from tensorflow.org.
[2] Julius Adebayo, Justin Gilmer, Ian Goodfellow, and Been Kim. Local explanation methods for deep neural networks lack sensitivity to parameter values. ICLR Workshop, 2018.
[3] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In NeurIPS, 2019.
[4] M. Ancona, E. Ceolini, C. Öztireli, and M. Gross. Towards better understanding of gradientbased attribution methods for Deep Neural Networks. ArXiv e-prints, November 2017.
[5] Lei Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS'14, pages 2654-2662, Cambridge, MA, USA, 2014. MIT Press.
[6] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.
[7] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert Müller. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun):1803-1831, 2010.
[8] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 2019-2026, 2014.
[9] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative components with random forests. In European Conference on Computer Vision, 2014.
[10] P. Dabkowski and Y. Gal. Real Time Image Saliency for Black Box Classifiers. ArXiv e-prints, May 2017.
[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.
[12] Ruth C. Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In ICCV, pages 3449-3457. IEEE Computer Society, 2017.
[13] N. Frosst and G. Hinton. Distilling a Neural Network Into a Soft Decision Tree. ArXiv e-prints, November 2017.
[14] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. ArXiv e-prints, June 2017.
[15] S. Gross and M. Wilber. Training and investigating Residual Nets. https://github.com/facebook/fb.resnet.torch, January 2017.
[16] Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. Gene selection for cancer classification using support vector machines. Machine Learning, 46(1):389-422, Jan 2002.
[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. ArXiv e-prints, December 2015.
[18] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2668-2677, Stockholmsmässan, Stockholm Sweden, 10-15 Jul 2018. PMLR.</p>
<p>[19] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Schütt, S. Dähne, D. Erhan, and B. Kim. The (Un)reliability of saliency methods. ArXiv e-prints, November 2017.
[20] Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, and Sven Dähne. Learning how to explain neural networks: Patternnet and patternattribution. In International Conference on Learning Representations, 2018.
[21] Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do Better ImageNet Models Transfer Better? arXiv e-prints, page arXiv:1805.08974, May 2018.
[22] Isaac Lage, Andrew Slavin Ross, Been Kim, Samuel J. Gershman, and Finale Doshi-Velez. Human-in-the-Loop Interpretability Prior. arXiv e-prints, page arXiv:1805.11571, May 2018.
[23] Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and KlausRobert Müller. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition, 65:211-222, 2017.
[24] A. S. Morcos, D. G. T. Barrett, N. C. Rabinowitz, and M. Botvinick. On the importance of single directions for generalization. ArXiv e-prints, March 2018.
[25] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. https://distill.pub/2017/feature-visualization.
[26] Vitali Petsiuk, Abir Das, and Kate Saenko. RISE: randomized input sampling for explanation of black-box models. CoRR, abs/1806.07421, 2018.
[27] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. SVCCA: singular vector canonical correlation analysis for deep learning dynamics and interpretability. In NIPS, pages 6078-6087, 2017.
[28] Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. CoRR, abs/1711.09404, 2017.
[29] Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. Right for the right reasons: Training differentiable models by constraining their explanations. In Proceedings of the TwentySixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, pages 2662-2670, 2017.
[30] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K. R. Müller. Evaluating the Visualization of What a Deep Neural Network Has Learned. IEEE Transactions on Neural Networks and Learning Systems, 28(11):2660-2673, Nov 2017.
[31] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.
[32] A. Shrikumar, P. Greenside, and A. Kundaje. Learning Important Features Through Propagating Activation Differences. ArXiv e-prints, April 2017.
[33] A. Shrikumar, P. Greenside, A. Shcherbina, and A. Kundaje. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. ArXiv e-prints, May 2016.
[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In $I C L R, 2015$.
[35] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
[36] Irwin Sobel. An isotropic 3x3 image gradient operator. Presentation at Stanford A.I. Project 1968, 022014.
[37] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. In $I C L R, 2015$.</p>
<p>[38] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365, 2017.
[39] M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth, and F. Doshi-Velez. Beyond Sparsity: Tree Regularization of Deep Models for Interpretability. ArXiv e-prints, November 2017.
[40] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision, pages 818-833. Springer, 2014.
[41] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. ArXiv e-prints, November 2016.
[42] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Object Detectors Emerge in Deep Scene CNNs. ArXiv e-prints, 2014.
[43] Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Revisiting the importance of individual units in cnns via ablation. CoRR, abs/1806.02891, 2018.
[44] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. In ICLR, 2017.</p>
<h1>6 Supplementary Charts and Experiments</h1>
<p>We include supplementary experiments and additional details about our training procedure, the estimators we evaluate, the image modification process and test-set accuracy below. In addition, as can be seen in Fig. 5, we also consider the scenario where pixels are kept according to importance rather than removed.</p>
<h3>6.1 Training Procedure</h3>
<p>We carefully tuned the hyperparamters of each dataset ImageNet, Birdsnap and Food 101 separately. We find that the Birdsnap and Food 101 converge within the same amount of training steps and a larger learning rate than ImageNet. These are detailed in Table. 6.2. These hyper parameters, along with the mean accuracy reported on the unmodified dataset, are used consistently across all estimators. ImageNet dataset achieves a mean accuracy of $76.68 \%$. This is comparable to the performance reported by [14]. On Birdsnap and Food 101, our unmodified datasets achieve $66.65 \%$ and $84.54 \%$ respectively. The baseline test-set accuracy for Food101 or Birdsnap is comparable to that reported by [21]. In Table. 2, we include the test-set performance for each experiment variant that we consider. The test-set accuracy reported is the average of 5 independent runs.</p>
<h3>6.2 Generation of New Dataset</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Top1Accuracy</th>
<th style="text-align: center;">Train Size</th>
<th style="text-align: center;">Test Size</th>
<th style="text-align: center;">Learning Rate</th>
<th style="text-align: center;">Training Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Birdsnap</td>
<td style="text-align: center;">66.65</td>
<td style="text-align: center;">47,386</td>
<td style="text-align: center;">2,443</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">20,000</td>
</tr>
<tr>
<td style="text-align: center;">Food_101</td>
<td style="text-align: center;">84.54</td>
<td style="text-align: center;">75,750</td>
<td style="text-align: center;">25,250</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">20,000</td>
</tr>
<tr>
<td style="text-align: center;">ImageNet</td>
<td style="text-align: center;">76.68</td>
<td style="text-align: center;">$1,281,167$</td>
<td style="text-align: center;">50,000</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">32,000</td>
</tr>
</tbody>
</table>
<p>Table 1: The training procedure was carefully finetuned for each dataset. These hyperparameters are consistently used across all experiment variants. The baseline accuracy of each unmodified data set is reported as the average of 10 independent runs.</p>
<p>We evaluate ROAR on three open source image datasets: ImageNet, Birdsnap and Food 101. For each dataset and estimator, we generate 10 new train and test sets that each correspond to a different fraction of feature modification $t=[0.1,0.3,0.5,0.7,0.9]$ and whether the most important pixels are removed or kept. This requires first generating a ranking of input importance for each input image according to each estimator. All of the estimators that we consider evaluate feature importance post-training. Thus, we generate the rankings according to each intepretability method using a stored checkpoint for each dataset.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Evaluation of all estimators according to Keep and Retrain KAR vs. ROAR. Left inset: For KAR, Keep And Retrain, we keep a fraction of features estimated to be most important and replace the remaining features with a constant mean value. The most accurate estimator is the one that preserves model performance the most for a given fraction of inputs removed (the highest test-set accuracy).Right inset: For ROAR, Rem0ve And Retrain we remove features by replacing a fraction of the inputs estimated to be most important according to each estimator with a constant mean value. The most accurate estimator is the one that degrades model performance the most for a given fraction of inputs removed. Inputs modified according to KAR result in a very narrow range of model accuracy. ROAR is a more discriminative benchmark, which suggests that retaining performance when the most important pixels are removed (rather than retained) is a harder task.</p>
<p>We use the ranking produced by the interpretability method to modify each image in the dataset (both train and test). We rank each estimate, $\mathbf{e}$ into an ordered set $\left{e_{i}^{o}\right}_{i=1}^{N}$. For the top $t$ fraction of this ordered set, we replace the corresponding pixels in the raw image with a per channel mean. Fig. 7 and Fig. 8 show an example of the type of modification applied to each image in the dataset for BirdSnap and Food 101 respectively. In the paper itself, we show an example of a single image from each ImageNet modification.</p>
<p>We evaluate 18 estimators in total (this includes the base estimators, a set of ensemble approaches wrapped around each base and finally a set of squared estimates). In total, we generate 540 large-scale modified image datasets in order to consider all experiment variants (180 new test/train for each original dataset).</p>
<h3>6.3 Evaluating Keeping Rather Than Removing Information</h3>
<p>In addition to ROAR, as can be seen in Fig. 5, we evaluate the opposite approach of KAR, Keep And Retrain. While ROAR removes features by replacing a fraction of inputs estimated to be most important, KAR preserves the inputs considered to be most important. Since we keep the important information rather than remove it, <em>minimizing</em> degradation to test-set accuracy is desirable.</p>
<p>In the right inset chart of Fig. 5 we plot KAR on the same curve as ROAR to enable a more intuitive comparison between the benchmarks. The comparison suggests that KAR appears to be</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Certain transformations of the estimate can substantially improve accuracy of all estimators. Squaring alone provides small gains to the accuracy of all estimators, and is slightly better than a random guess. Left inset: The three base estimators that we consider (Gradients (GRAD), Integrated Gradients (IG) and Guided Backprop (GB)) perform worse than a random assignment of feature importance. At all fractions considered, a random assignment of importance degrades performance more than removing the pixels estimated to be most important by base methods. Right inset: Average test-set accuracy across 5 independent iterations for estimates that are squared before ranking and subsequent removal. When squared, base estimators perform slightly better than a random guess. However, this does not compare to the gains in accuracy of averaging a set of noisy estimates that are squared (SmoothGrad-Squared)
a poor discriminator between estimators. The x -axis indicates the fraction of features that are preserved/removed for KAR/ROAR respectively.
We find that KAR is a far weaker discriminator of performance; all base estimators and the ensemble variants perform in a similar range to each other. These findings suggest that the task of identifying features to preserve is an easier benchmark to fulfill than accurately identifying a fraction of input that will cause the maximum damage to the model performance.</p>
<h1>6.4 Squaring Alone Slightly Improves the Performance of All Base Variants</h1>
<p>The surprising performance of SmoothGrad-Squared (SG-SQ) deserves further investigation; why is averaging a set of squared noisy estimates so effective at improving the accuracy of the ranking? To disentangle whether both squaring and then averaging are required, we explore whether we achieve similar performance gains by only squaring the estimate.
Squaring of a single estimate, with no ensembling, benefits the accuracy of all estimators that we considered. In the right inset chart of Fig. 6, we can see that squared estimates perform better than the raw estimate. When squared, an estimate gains slightly more accuracy than a random ranking of input features. In particular, squaring benefits GB; at $t=.9$ performance of SQ-GB relative to GB improves by $8.43 \% \pm 0.97$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: A single example from each dataset generated from modifying Food 101 according to both ROAR and KAR. We show the modification for base estimators (Integrated Gradients (IG), Guided Backprop (GB), Gradient Heatmap (GRAD) and derivative ensemble approaches - SmoothGrad, (SG-GRAD, SG-IG, SG-GB), SmoothGrad-Squared (SG-SQ-GRAD, SG-SQ-IG, SG-SQ-GB) and VarGrad (VAR-GRAD, VAR-IG, VAR-GB. In addition, we consider two control variants a random baseline, a sobel edge filter.</p>
<p>Squaring is an equivalent transformation to taking the absolute value of the estimate before ranking all inputs. After squaring, negative estimates become positive, and the ranking then only depends upon the magnitude and not the direction of the estimate. The benefits gained by squaring furthers our understanding of how the direction of GB, IG and GRAD values should be treated. For all these estimators, estimates are very much a reflection of the weights of the network. The magnitude may be far more telling of feature importance than direction; a negative signal may be just as important as positive contributions towards a model's prediction. While squaring improves the accuracy of all estimators, the transformation does not explain the large gains in accuracy that we observe when we average a set of noisy squared estimates.</p>
<h1>6.5 Limitations on the use of ROAR</h1>
<p>In this work we propose ROAR as a method for estimating feature importance in deep neural networks. However, we do note that ROAR is not suitable for certain algorithms such as decision stump $\mathrm{Y}=(\mathrm{A}$ or D) where there is also feature redundancy. For these algorithms, in order to use ROAR correctly feature importance must be recomputed after each re-training step. This is because a decision stump ignores a subset of input features at inference time which means it is possible for a random estimator</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: A single example from each dataset generated from modifying Imagenet according to the ROAR and KAR. We show the modification for base estimators (Integrated Gradients (IG), Guided Backprop (GB), Gradient Heatmap (GRAD) and derivative ensemble approaches - SmoothGrad, (SG-GRAD, SG-IG, SG-GB), SmoothGrad-Squared (SG-SQ-GRAD, SG-SQ-IG, SG-SQ-GB) and VarGrad (VAR-GRAD, VAR-IG, VAR-GB. In addition, we consider two control variants a random baseline, a sobel edge filter.
to appear to perform better than the best possible estimator. For the class of models evaluated in the paper (linear models, multi-layer perceptrons and deep neural networks) as well as any model that allows all features to contribute to a prediction at test time ROAR remains valid. To make ROAR valid for decisions stumps, one can re-compute the feature importance after each re-training step. The scale of our experiments preclude this, and our experiments show that it is not necessary for deep neural networks (DNNs).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Keep</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Remove</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">Birdsnap</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">37.24</td>
<td style="text-align: center;">46.41</td>
<td style="text-align: center;">51.29</td>
<td style="text-align: center;">55.38</td>
<td style="text-align: center;">59.92</td>
<td style="text-align: center;">60.11</td>
<td style="text-align: center;">55.65</td>
<td style="text-align: center;">51.10</td>
<td style="text-align: center;">46.45</td>
<td style="text-align: center;">38.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sobel</td>
<td style="text-align: center;">44.81</td>
<td style="text-align: center;">52.11</td>
<td style="text-align: center;">55.36</td>
<td style="text-align: center;">55.69</td>
<td style="text-align: center;">59.08</td>
<td style="text-align: center;">59.73</td>
<td style="text-align: center;">56.94</td>
<td style="text-align: center;">56.30</td>
<td style="text-align: center;">53.82</td>
<td style="text-align: center;">44.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAD</td>
<td style="text-align: center;">57.51</td>
<td style="text-align: center;">61.10</td>
<td style="text-align: center;">60.79</td>
<td style="text-align: center;">61.96</td>
<td style="text-align: center;">62.49</td>
<td style="text-align: center;">62.12</td>
<td style="text-align: center;">61.82</td>
<td style="text-align: center;">58.29</td>
<td style="text-align: center;">58.91</td>
<td style="text-align: center;">56.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IG</td>
<td style="text-align: center;">62.64</td>
<td style="text-align: center;">65.02</td>
<td style="text-align: center;">65.42</td>
<td style="text-align: center;">65.46</td>
<td style="text-align: center;">65.50</td>
<td style="text-align: center;">64.79</td>
<td style="text-align: center;">64.91</td>
<td style="text-align: center;">64.12</td>
<td style="text-align: center;">63.64</td>
<td style="text-align: center;">60.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GP</td>
<td style="text-align: center;">62.59</td>
<td style="text-align: center;">62.35</td>
<td style="text-align: center;">60.76</td>
<td style="text-align: center;">61.78</td>
<td style="text-align: center;">62.44</td>
<td style="text-align: center;">58.47</td>
<td style="text-align: center;">57.64</td>
<td style="text-align: center;">55.47</td>
<td style="text-align: center;">57.28</td>
<td style="text-align: center;">59.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-GRAD</td>
<td style="text-align: center;">64.64</td>
<td style="text-align: center;">65.87</td>
<td style="text-align: center;">65.32</td>
<td style="text-align: center;">65.49</td>
<td style="text-align: center;">65.78</td>
<td style="text-align: center;">65.44</td>
<td style="text-align: center;">66.08</td>
<td style="text-align: center;">65.33</td>
<td style="text-align: center;">65.44</td>
<td style="text-align: center;">65.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-IG</td>
<td style="text-align: center;">65.36</td>
<td style="text-align: center;">66.45</td>
<td style="text-align: center;">66.38</td>
<td style="text-align: center;">66.37</td>
<td style="text-align: center;">66.35</td>
<td style="text-align: center;">66.11</td>
<td style="text-align: center;">66.56</td>
<td style="text-align: center;">66.65</td>
<td style="text-align: center;">66.37</td>
<td style="text-align: center;">64.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-GB</td>
<td style="text-align: center;">52.86</td>
<td style="text-align: center;">56.44</td>
<td style="text-align: center;">58.32</td>
<td style="text-align: center;">59.20</td>
<td style="text-align: center;">60.35</td>
<td style="text-align: center;">54.67</td>
<td style="text-align: center;">53.37</td>
<td style="text-align: center;">51.13</td>
<td style="text-align: center;">50.07</td>
<td style="text-align: center;">47.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-GRAD</td>
<td style="text-align: center;">55.32</td>
<td style="text-align: center;">60.79</td>
<td style="text-align: center;">62.13</td>
<td style="text-align: center;">63.63</td>
<td style="text-align: center;">64.99</td>
<td style="text-align: center;">42.88</td>
<td style="text-align: center;">39.14</td>
<td style="text-align: center;">32.98</td>
<td style="text-align: center;">25.34</td>
<td style="text-align: center;">12.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-IG</td>
<td style="text-align: center;">55.89</td>
<td style="text-align: center;">61.02</td>
<td style="text-align: center;">62.68</td>
<td style="text-align: center;">63.63</td>
<td style="text-align: center;">64.43</td>
<td style="text-align: center;">40.85</td>
<td style="text-align: center;">36.94</td>
<td style="text-align: center;">33.37</td>
<td style="text-align: center;">27.38</td>
<td style="text-align: center;">14.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-GB</td>
<td style="text-align: center;">49.32</td>
<td style="text-align: center;">54.94</td>
<td style="text-align: center;">57.62</td>
<td style="text-align: center;">59.41</td>
<td style="text-align: center;">61.66</td>
<td style="text-align: center;">38.80</td>
<td style="text-align: center;">24.09</td>
<td style="text-align: center;">16.54</td>
<td style="text-align: center;">10.11</td>
<td style="text-align: center;">5.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-GRAD</td>
<td style="text-align: center;">55.03</td>
<td style="text-align: center;">60.36</td>
<td style="text-align: center;">62.59</td>
<td style="text-align: center;">63.16</td>
<td style="text-align: center;">64.85</td>
<td style="text-align: center;">41.71</td>
<td style="text-align: center;">37.04</td>
<td style="text-align: center;">33.24</td>
<td style="text-align: center;">24.84</td>
<td style="text-align: center;">9.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-IG</td>
<td style="text-align: center;">55.21</td>
<td style="text-align: center;">61.22</td>
<td style="text-align: center;">63.04</td>
<td style="text-align: center;">64.29</td>
<td style="text-align: center;">64.31</td>
<td style="text-align: center;">40.21</td>
<td style="text-align: center;">36.85</td>
<td style="text-align: center;">34.09</td>
<td style="text-align: center;">27.71</td>
<td style="text-align: center;">16.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-GB</td>
<td style="text-align: center;">47.76</td>
<td style="text-align: center;">53.27</td>
<td style="text-align: center;">56.53</td>
<td style="text-align: center;">58.68</td>
<td style="text-align: center;">61.69</td>
<td style="text-align: center;">38.63</td>
<td style="text-align: center;">24.12</td>
<td style="text-align: center;">16.29</td>
<td style="text-align: center;">10.16</td>
<td style="text-align: center;">5.20</td>
</tr>
<tr>
<td style="text-align: center;">Food_101</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">68.13</td>
<td style="text-align: center;">73.15</td>
<td style="text-align: center;">76.00</td>
<td style="text-align: center;">78.21</td>
<td style="text-align: center;">80.61</td>
<td style="text-align: center;">80.66</td>
<td style="text-align: center;">78.30</td>
<td style="text-align: center;">75.80</td>
<td style="text-align: center;">72.98</td>
<td style="text-align: center;">68.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sobel</td>
<td style="text-align: center;">69.08</td>
<td style="text-align: center;">76.70</td>
<td style="text-align: center;">78.16</td>
<td style="text-align: center;">79.30</td>
<td style="text-align: center;">80.90</td>
<td style="text-align: center;">81.17</td>
<td style="text-align: center;">79.69</td>
<td style="text-align: center;">78.91</td>
<td style="text-align: center;">77.06</td>
<td style="text-align: center;">69.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAD</td>
<td style="text-align: center;">78.82</td>
<td style="text-align: center;">82.89</td>
<td style="text-align: center;">83.43</td>
<td style="text-align: center;">83.68</td>
<td style="text-align: center;">83.88</td>
<td style="text-align: center;">83.79</td>
<td style="text-align: center;">83.50</td>
<td style="text-align: center;">83.09</td>
<td style="text-align: center;">82.48</td>
<td style="text-align: center;">78.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IG</td>
<td style="text-align: center;">82.35</td>
<td style="text-align: center;">83.80</td>
<td style="text-align: center;">83.90</td>
<td style="text-align: center;">83.99</td>
<td style="text-align: center;">84.07</td>
<td style="text-align: center;">84.01</td>
<td style="text-align: center;">83.95</td>
<td style="text-align: center;">83.78</td>
<td style="text-align: center;">83.52</td>
<td style="text-align: center;">80.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GP</td>
<td style="text-align: center;">77.31</td>
<td style="text-align: center;">79.00</td>
<td style="text-align: center;">78.33</td>
<td style="text-align: center;">79.86</td>
<td style="text-align: center;">81.16</td>
<td style="text-align: center;">80.06</td>
<td style="text-align: center;">79.12</td>
<td style="text-align: center;">77.25</td>
<td style="text-align: center;">78.43</td>
<td style="text-align: center;">75.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-GRAD</td>
<td style="text-align: center;">83.30</td>
<td style="text-align: center;">83.87</td>
<td style="text-align: center;">84.01</td>
<td style="text-align: center;">84.05</td>
<td style="text-align: center;">83.96</td>
<td style="text-align: center;">83.97</td>
<td style="text-align: center;">84.00</td>
<td style="text-align: center;">83.97</td>
<td style="text-align: center;">83.83</td>
<td style="text-align: center;">83.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-IG</td>
<td style="text-align: center;">83.27</td>
<td style="text-align: center;">83.91</td>
<td style="text-align: center;">84.06</td>
<td style="text-align: center;">84.05</td>
<td style="text-align: center;">83.96</td>
<td style="text-align: center;">83.98</td>
<td style="text-align: center;">84.04</td>
<td style="text-align: center;">84.05</td>
<td style="text-align: center;">83.90</td>
<td style="text-align: center;">82.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-GB</td>
<td style="text-align: center;">71.44</td>
<td style="text-align: center;">75.96</td>
<td style="text-align: center;">77.26</td>
<td style="text-align: center;">78.65</td>
<td style="text-align: center;">80.12</td>
<td style="text-align: center;">78.35</td>
<td style="text-align: center;">76.39</td>
<td style="text-align: center;">75.44</td>
<td style="text-align: center;">74.50</td>
<td style="text-align: center;">69.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-GRAD</td>
<td style="text-align: center;">73.05</td>
<td style="text-align: center;">79.20</td>
<td style="text-align: center;">80.18</td>
<td style="text-align: center;">80.80</td>
<td style="text-align: center;">82.13</td>
<td style="text-align: center;">79.29</td>
<td style="text-align: center;">75.83</td>
<td style="text-align: center;">64.83</td>
<td style="text-align: center;">38.88</td>
<td style="text-align: center;">8.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-IG</td>
<td style="text-align: center;">72.93</td>
<td style="text-align: center;">78.36</td>
<td style="text-align: center;">79.33</td>
<td style="text-align: center;">80.02</td>
<td style="text-align: center;">81.30</td>
<td style="text-align: center;">79.73</td>
<td style="text-align: center;">76.73</td>
<td style="text-align: center;">70.98</td>
<td style="text-align: center;">59.55</td>
<td style="text-align: center;">27.81</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-GB</td>
<td style="text-align: center;">68.10</td>
<td style="text-align: center;">73.69</td>
<td style="text-align: center;">76.02</td>
<td style="text-align: center;">78.51</td>
<td style="text-align: center;">81.22</td>
<td style="text-align: center;">77.68</td>
<td style="text-align: center;">72.81</td>
<td style="text-align: center;">66.24</td>
<td style="text-align: center;">55.73</td>
<td style="text-align: center;">24.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-GRAD</td>
<td style="text-align: center;">74.24</td>
<td style="text-align: center;">78.86</td>
<td style="text-align: center;">79.97</td>
<td style="text-align: center;">80.61</td>
<td style="text-align: center;">82.10</td>
<td style="text-align: center;">79.55</td>
<td style="text-align: center;">75.67</td>
<td style="text-align: center;">67.40</td>
<td style="text-align: center;">52.05</td>
<td style="text-align: center;">15.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-IG</td>
<td style="text-align: center;">73.65</td>
<td style="text-align: center;">78.28</td>
<td style="text-align: center;">79.31</td>
<td style="text-align: center;">79.99</td>
<td style="text-align: center;">81.23</td>
<td style="text-align: center;">79.87</td>
<td style="text-align: center;">76.60</td>
<td style="text-align: center;">70.85</td>
<td style="text-align: center;">59.57</td>
<td style="text-align: center;">25.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-GB</td>
<td style="text-align: center;">67.08</td>
<td style="text-align: center;">73.00</td>
<td style="text-align: center;">76.01</td>
<td style="text-align: center;">78.54</td>
<td style="text-align: center;">81.44</td>
<td style="text-align: center;">77.76</td>
<td style="text-align: center;">72.56</td>
<td style="text-align: center;">66.36</td>
<td style="text-align: center;">54.18</td>
<td style="text-align: center;">23.88</td>
</tr>
<tr>
<td style="text-align: center;">ImageNet</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">63.60</td>
<td style="text-align: center;">66.98</td>
<td style="text-align: center;">69.18</td>
<td style="text-align: center;">71.03</td>
<td style="text-align: center;">72.69</td>
<td style="text-align: center;">72.65</td>
<td style="text-align: center;">71.02</td>
<td style="text-align: center;">69.13</td>
<td style="text-align: center;">67.06</td>
<td style="text-align: center;">63.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sobel</td>
<td style="text-align: center;">65.79</td>
<td style="text-align: center;">70.40</td>
<td style="text-align: center;">71.40</td>
<td style="text-align: center;">71.60</td>
<td style="text-align: center;">72.65</td>
<td style="text-align: center;">72.89</td>
<td style="text-align: center;">71.94</td>
<td style="text-align: center;">71.61</td>
<td style="text-align: center;">70.56</td>
<td style="text-align: center;">65.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAD</td>
<td style="text-align: center;">67.63</td>
<td style="text-align: center;">71.45</td>
<td style="text-align: center;">72.02</td>
<td style="text-align: center;">72.85</td>
<td style="text-align: center;">73.46</td>
<td style="text-align: center;">72.94</td>
<td style="text-align: center;">72.22</td>
<td style="text-align: center;">70.97</td>
<td style="text-align: center;">70.72</td>
<td style="text-align: center;">66.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IG</td>
<td style="text-align: center;">70.38</td>
<td style="text-align: center;">72.51</td>
<td style="text-align: center;">72.66</td>
<td style="text-align: center;">72.88</td>
<td style="text-align: center;">73.32</td>
<td style="text-align: center;">73.17</td>
<td style="text-align: center;">72.72</td>
<td style="text-align: center;">72.03</td>
<td style="text-align: center;">71.68</td>
<td style="text-align: center;">68.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GP</td>
<td style="text-align: center;">71.03</td>
<td style="text-align: center;">72.45</td>
<td style="text-align: center;">72.28</td>
<td style="text-align: center;">72.69</td>
<td style="text-align: center;">71.56</td>
<td style="text-align: center;">72.29</td>
<td style="text-align: center;">71.91</td>
<td style="text-align: center;">71.18</td>
<td style="text-align: center;">71.48</td>
<td style="text-align: center;">70.38</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-GRAD</td>
<td style="text-align: center;">70.47</td>
<td style="text-align: center;">71.94</td>
<td style="text-align: center;">72.14</td>
<td style="text-align: center;">72.35</td>
<td style="text-align: center;">72.44</td>
<td style="text-align: center;">72.08</td>
<td style="text-align: center;">71.94</td>
<td style="text-align: center;">71.77</td>
<td style="text-align: center;">71.51</td>
<td style="text-align: center;">70.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-IG</td>
<td style="text-align: center;">70.98</td>
<td style="text-align: center;">72.30</td>
<td style="text-align: center;">72.49</td>
<td style="text-align: center;">72.60</td>
<td style="text-align: center;">72.67</td>
<td style="text-align: center;">72.49</td>
<td style="text-align: center;">72.39</td>
<td style="text-align: center;">72.26</td>
<td style="text-align: center;">72.02</td>
<td style="text-align: center;">69.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-GB</td>
<td style="text-align: center;">66.97</td>
<td style="text-align: center;">70.68</td>
<td style="text-align: center;">71.52</td>
<td style="text-align: center;">71.86</td>
<td style="text-align: center;">72.57</td>
<td style="text-align: center;">71.28</td>
<td style="text-align: center;">70.45</td>
<td style="text-align: center;">69.98</td>
<td style="text-align: center;">69.02</td>
<td style="text-align: center;">64.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-GRAD</td>
<td style="text-align: center;">63.25</td>
<td style="text-align: center;">69.79</td>
<td style="text-align: center;">72.20</td>
<td style="text-align: center;">73.18</td>
<td style="text-align: center;">73.96</td>
<td style="text-align: center;">69.35</td>
<td style="text-align: center;">60.28</td>
<td style="text-align: center;">41.55</td>
<td style="text-align: center;">29.45</td>
<td style="text-align: center;">11.09</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-IG</td>
<td style="text-align: center;">67.55</td>
<td style="text-align: center;">68.96</td>
<td style="text-align: center;">72.24</td>
<td style="text-align: center;">73.09</td>
<td style="text-align: center;">73.80</td>
<td style="text-align: center;">70.76</td>
<td style="text-align: center;">65.71</td>
<td style="text-align: center;">58.34</td>
<td style="text-align: center;">43.71</td>
<td style="text-align: center;">29.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SG-SQ-GB</td>
<td style="text-align: center;">62.42</td>
<td style="text-align: center;">68.96</td>
<td style="text-align: center;">71.17</td>
<td style="text-align: center;">72.72</td>
<td style="text-align: center;">73.77</td>
<td style="text-align: center;">69.74</td>
<td style="text-align: center;">60.56</td>
<td style="text-align: center;">52.21</td>
<td style="text-align: center;">34.98</td>
<td style="text-align: center;">15.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-GRAD</td>
<td style="text-align: center;">53.38</td>
<td style="text-align: center;">69.86</td>
<td style="text-align: center;">72.15</td>
<td style="text-align: center;">73.22</td>
<td style="text-align: center;">73.92</td>
<td style="text-align: center;">69.24</td>
<td style="text-align: center;">57.48</td>
<td style="text-align: center;">39.23</td>
<td style="text-align: center;">30.13</td>
<td style="text-align: center;">10.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-IG</td>
<td style="text-align: center;">67.17</td>
<td style="text-align: center;">71.07</td>
<td style="text-align: center;">71.48</td>
<td style="text-align: center;">72.93</td>
<td style="text-align: center;">73.87</td>
<td style="text-align: center;">70.87</td>
<td style="text-align: center;">65.56</td>
<td style="text-align: center;">57.49</td>
<td style="text-align: center;">45.80</td>
<td style="text-align: center;">25.25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR-GB</td>
<td style="text-align: center;">62.09</td>
<td style="text-align: center;">68.51</td>
<td style="text-align: center;">71.09</td>
<td style="text-align: center;">72.59</td>
<td style="text-align: center;">73.85</td>
<td style="text-align: center;">69.67</td>
<td style="text-align: center;">60.94</td>
<td style="text-align: center;">47.39</td>
<td style="text-align: center;">35.68</td>
<td style="text-align: center;">14.93</td>
</tr>
</tbody>
</table>
<p>Table 2: Average test-set accuracy across 5 independent runs for all estimators and datasets considered. ROAR requires removing a fraction of pixels estimated to be most important. KAR differs in that the pixels estimated to be most important are kept rather than removed. The fraction removed/kept is indicated by the threshold. The estimators we report results for are base estimators (Integrated Gradients (IG), Guided Backprop (GB), Gradient Heatmap (GRAD) and derivative ensemble approaches - SmoothGrad, (SG-GRAD, SG-IG, SG-GB), SmoothGrad-Squared (SG-SQ-GRAD, SG-SQ-IG, SG-SQ-GB) and VarGrad (VAR-GRAD, VAR-IG, VAR-GB).</p>            </div>
        </div>

    </div>
</body>
</html>
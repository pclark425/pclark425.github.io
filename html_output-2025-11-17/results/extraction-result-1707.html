<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1707 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1707</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1707</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-216868949</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2004.14973v2.pdf" target="_blank">Improving Vision-and-Language Navigation with Image-Text Pairs from the Web</a></p>
                <p><strong>Paper Abstract:</strong> Following a navigation instruction such as 'Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g. 'stairs') to visual content in the environment (pixels corresponding to 'stairs'). We ask the following question -- can we leverage abundant 'disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn visual groundings (what do 'stairs' look like?) that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction ('...stop at the brown sofa') and a sequence of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN -- outperforming the prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful -- with their combination resulting in further positive synergistic effects.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1707.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1707.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visiolinguistic transformer-based compatibility model that scores alignment between a navigation instruction and a candidate trajectory (sequence of panoramic image regions); explicitly designed to enable transfer from language and image-text pretraining to embodied VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Two-stream Transformer (vision + language) architecture structurally similar to ViLBERT: uses 12-layer BERT-BASE encoders for both streams, 6 cross-modal co-attention layers, processes sequences of panorama image regions and instruction tokens, sums visual features with panorama-index and panoramic spatial embeddings, and produces a compatibility score f(τ,x) for path-instruction pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>language corpora (masked language modeling), web image-text pairs (visiolinguistic), and path-instruction pairs (action grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Three-stage curriculum used by this paper: Stage 1 — language-only pretraining initialized from BERT trained on English Wikipedia and BooksCorpus; Stage 2 — visual grounding initialized from ViLBERT weights pretrained on Conceptual Captions (web image-alttext pairs; paper cites Conceptual Captions ≈ 3.3M image-text pairs) using masked multimodal modelling and multimodal alignment objectives and region inputs from an object detector pretrained on Visual Genome; Stage 3 — action grounding using Room-to-Room (R2R) path-instruction pairs (VLN dataset, ≈14k path-instruction pairs) with masked multimodal objectives, then fine-tuned on a 4-way path selection classification objective.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN) — Room-to-Room (R2R) path selection</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Agent must follow a natural language navigation instruction in Matterport3D-based environments; environments represented as a navigation graph of panorama nodes and edges; agent chooses a trajectory from a set of candidate paths (beam search up to 30 beams) and succeeds if final position is within 3m of goal. In this paper the task is cast as discriminative path selection (rank candidate trajectories by compatibility with instruction).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>None / not applicable for language pretraining — masked language modelling and next-sentence prediction (text-only). For image-text pretraining there are no embodied actions (image-caption alignment and masked multimodal prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation over a navigation-graph: selecting sequences of panorama nodes / high-level navigation actions (move to neighboring panorama / follow an edge); in this work the model scores entire candidate trajectories (path-level discrete choices) rather than emitting low-level continuous motor controls.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No explicit symbolic mapping from text verbs to low-level motor commands; mapping is learned implicitly via (1) initializing language and vision streams from pretrained models, (2) representing trajectories with panorama-index and heading/elevation spatial embeddings, and (3) Stage-3 masked multimodal pretraining and supervised fine-tuning which expose the model to co-occurrence of action words (e.g. 'turn', 'stop') and trajectory visual/positional cues — the model learns correspondences in the joint representation and uses a learned compatibility scoring function to map instruction semantics to candidate paths.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic visual input (360° panoramas represented via multiple perspective projections at 36 discrete headings/elevations), region proposals/features extracted with a bottom-up attention Faster R-CNN pretrained on Visual Genome, per-region features augmented with bounding-box location (normalized), area, elevation, heading (encoded as [cosθ,sinθ]) and panorama-index embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Full three-stage curriculum -> single-model path-selection Success Rate (SR) 59.3% (reported in paper); leaderboard 3-model ensemble (speaker + follower + VLN-BERT) Test Unseen SR = 73% (VLN leaderboard). Language-only initialization (BERT) yields SR = 45.2%. Ablations reported: combining visual-grounding (image-text) and action-grounding stages in series yields a 14.1 absolute percentage point improvement (cumulative), and the authors state that pretraining on image-text pairs provides a substantial boost (reported contribution numbers in text: +4.5 and +4.9 abs points when used independently; combined gives +14.1 abs; paper also highlights a +9.2 abs point improvement attributed to image-text pretraining in some comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Training from scratch (no language or visiolinguistic pretraining) achieves SR = 30.5% (reported). Other baselines: prior state-of-the-art follower/speaker models (not using the same visiolinguistic pretraining curriculum) achieve ~54.7% SR in some reported comparisons (paper compares against follower/speaker models from Tan et al. [28]).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>1) Curriculum ordering (language → visual grounding → action grounding) that progressively exposes model to language, visual concepts, then trajectory-action referents; 2) strong pretrained language representations (BERT) and visiolinguistic representations (ViLBERT) providing object-level grounding; 3) object-centric region features and cross-modal co-attention enabling alignment; 4) positional/panoramic spatial encodings that link visual evidence to trajectory-relative geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>1) Domain shift between web images (well-composed, aesthetic viewpoints) and embodied panoramic views (agent-centric, occlusions, unusual framings); 2) absence of an explicit symbolic action-to-motor mapping (mapping is implicit and learned via examples); 3) limited size of task-specific path-instruction data (~14k R2R pairs) constrains action grounding and fine-grained procedural generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on large-scale language and web image-text corpora, then fine-tuning with a targeted action-grounding stage, produces large, synergistic gains on vision-and-language navigation: language-only pretraining gives a large jump (30.5% → 45.2% SR), adding image-text pretraining and action-grounding in series yields further gains and a single-model SR of 59.3%, and an ensemble achieves 73% SR on Test Unseen; visual-grounding from web image-caption data substantially improves grounding of rare object references despite domain shift, and the staged curriculum is critical for successful internet-to-embodied transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Vision-and-Language Navigation with Image-Text Pairs from the Web', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1707.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1707.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViLBERT (visiolinguistic BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stream BERT-like architecture for learning joint vision-and-language representations from aligned image-text corpora using masked multimodal modelling and multimodal alignment objectives; visual stream ingests object-region inputs and co-attends with language stream.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>ViLBERT (pretrained weights)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Two BERT-like streams (vision and language) connected with co-attention transformer layers; visual input is a sequence of object-region features (from a detector pretrained on Visual Genome) and language input is tokenized caption text; trained with masked multimodal modelling (predict masked text tokens and object classes for masked regions) and image-caption alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>web image-text pairs (image alt-text captions)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretrained on the Conceptual Captions dataset (web-scraped image alt-text pairs; paper cites Conceptual Captions containing ≈3.3M image-text pairs) using masked multimodal modelling and multimodal alignment objectives; visual region inputs derived from bottom-up attention detectors trained on Visual Genome.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN) — Room-to-Room (R2R) (as used to initialize VLN-BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Used as an initialization for VLN-BERT which is fine-tuned on path-instruction compatibility and path selection in the R2R VLN task (Matterport3D panoramas, navigation graph, goal within 3m success condition).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No action space in ViLBERT pretraining (image-caption prediction and alignment tasks only).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation / candidate path selection in R2R (handled during fine-tuning of VLN-BERT).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>ViLBERT weights initialize the visual and language streams of VLN-BERT; mapping from caption-language semantics to trajectory-level actions is achieved later via Stage-3 action-grounding on path-instruction pairs and fine-tuning — i.e., an implicit learned mapping through joint representation adaptation rather than an explicit translator.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Single-image region features at pretraining (object-detector regions). For transfer to VLN, these region-level priors are applied to panorama-extracted regions (Faster R-CNN features), augmented with heading/elevation and panorama-index embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Authors report that adding the image-text (ViLBERT) pretraining stage increases path-selection SR: when used independently it provides an improvement of ≈4.5 absolute percentage points (relative to the compared baseline in their ablations), and when combined with Stage-3 action-grounding the series yields a cumulative improvement of 14.1 abs points; full curriculum single-model SR = 59.3%; authors emphasize a substantial boost from web image-text pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Without the visual-grounding stage, models fail to ground some rare object phrases and select unsuccessful paths in ablated examples; the paper reports the from-scratch baseline SR = 30.5% and language-only SR = 45.2% (so ViLBERT stage contributes further improvements on top of language-only).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Object-centric region supervision from Visual Genome detectors, large-scale image-caption co-occurrences (Conceptual Captions) that cover a wide range of visual concepts, and co-attention architecture that can be adapted to sequential panoramic inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain gap between curated web images (aesthetic framing) and agent-centric panoramic views (different viewpoints/occlusions/framing), which can limit direct transfer of appearance priors for some concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initializing VLN-BERT with ViLBERT weights pretrained on large web image-text corpora yields measurable, non-trivial improvements in an embodied VLN task, especially for grounding rare object references; the transfer is effective when followed by an action-grounding fine-tuning stage that adapts visiolinguistic priors to trajectory-relative spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Vision-and-Language Navigation with Image-Text Pairs from the Web', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1707.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1707.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (Bidirectional Encoder Representations from Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large bidirectional Transformer-based language representation model trained with masked language modelling and next-sentence prediction on large text corpora; used here to initialize the language stream of VLN-BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BERT (pretrained weights)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>12-layer Transformer encoder (BERT-BASE in experiments) trained on masked language modelling and next-sentence prediction; provides strong contextualized language representations used to initialize VLN-BERT's language stream.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large-scale text corpora for language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining corpora cited: English Wikipedia and BooksCorpus (as in BERT); used to initialize the language stream of VLN-BERT (Stage 1).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN) — Room-to-Room (R2R) (used as initializer for VLN-BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See VLN-BERT entry: path selection in Matterport3D-derived R2R environments where instructions describe trajectories between panorama nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No action space — BERT is trained on text-only objectives (masked LM, next-sentence prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation / path selection over panoramas during VLN fine-tuning and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Language representations from BERT provide robust encoding of instruction tokens; during subsequent multimodal pretraining and fine-tuning these representations are co-attended with visual features and trajectory encodings to implicitly map language phrases (including action words) to trajectory correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Not applicable for BERT pretraining; when transferred to VLN it is used together with panoramic RGB region features and spatial encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Initializing with BERT (language-only pretraining) substantially improves VLN path-selection SR to 45.2% (from 30.5% when training from scratch); the paper highlights this as a major contributor to downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>From-scratch training (no BERT initialization) achieves SR = 30.5% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong contextual language priors that help parsing and grounding of instructions, reducing reliance on limited task-specific instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>BERT alone provides language understanding but requires additional visual-grounding and action-grounding stages to map language to embodied perception and trajectories; language-only pretraining cannot resolve visual domain-shift.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-only pretraining (BERT) yields a large single-stage improvement on embodied VLN (≈14.7 absolute point SR increase over scratch), demonstrating that robust language representations are a crucial component of successful transfer to navigation tasks, but must be combined with visual and action grounding for maximal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Vision-and-Language Navigation with Image-Text Pairs from the Web', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset for Automatic Image Captioning <em>(Rating: 1)</em></li>
                <li>Towards learning a generic agent for vision-and-language navigation via pre-training <em>(Rating: 2)</em></li>
                <li>Robust navigation with language pretraining and stochastic sampling <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1707",
    "paper_id": "paper-216868949",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "VLN-BERT",
            "name_full": "VLN-BERT",
            "brief_description": "A visiolinguistic transformer-based compatibility model that scores alignment between a navigation instruction and a candidate trajectory (sequence of panoramic image regions); explicitly designed to enable transfer from language and image-text pretraining to embodied VLN.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "VLN-BERT",
            "model_agent_description": "Two-stream Transformer (vision + language) architecture structurally similar to ViLBERT: uses 12-layer BERT-BASE encoders for both streams, 6 cross-modal co-attention layers, processes sequences of panorama image regions and instruction tokens, sums visual features with panorama-index and panoramic spatial embeddings, and produces a compatibility score f(τ,x) for path-instruction pairs.",
            "pretraining_data_type": "language corpora (masked language modeling), web image-text pairs (visiolinguistic), and path-instruction pairs (action grounding)",
            "pretraining_data_details": "Three-stage curriculum used by this paper: Stage 1 — language-only pretraining initialized from BERT trained on English Wikipedia and BooksCorpus; Stage 2 — visual grounding initialized from ViLBERT weights pretrained on Conceptual Captions (web image-alttext pairs; paper cites Conceptual Captions ≈ 3.3M image-text pairs) using masked multimodal modelling and multimodal alignment objectives and region inputs from an object detector pretrained on Visual Genome; Stage 3 — action grounding using Room-to-Room (R2R) path-instruction pairs (VLN dataset, ≈14k path-instruction pairs) with masked multimodal objectives, then fine-tuned on a 4-way path selection classification objective.",
            "embodied_task_name": "Vision-and-Language Navigation (VLN) — Room-to-Room (R2R) path selection",
            "embodied_task_description": "Agent must follow a natural language navigation instruction in Matterport3D-based environments; environments represented as a navigation graph of panorama nodes and edges; agent chooses a trajectory from a set of candidate paths (beam search up to 30 beams) and succeeds if final position is within 3m of goal. In this paper the task is cast as discriminative path selection (rank candidate trajectories by compatibility with instruction).",
            "action_space_text": "None / not applicable for language pretraining — masked language modelling and next-sentence prediction (text-only). For image-text pretraining there are no embodied actions (image-caption alignment and masked multimodal prediction).",
            "action_space_embodied": "Discrete navigation over a navigation-graph: selecting sequences of panorama nodes / high-level navigation actions (move to neighboring panorama / follow an edge); in this work the model scores entire candidate trajectories (path-level discrete choices) rather than emitting low-level continuous motor controls.",
            "action_mapping_method": "No explicit symbolic mapping from text verbs to low-level motor commands; mapping is learned implicitly via (1) initializing language and vision streams from pretrained models, (2) representing trajectories with panorama-index and heading/elevation spatial embeddings, and (3) Stage-3 masked multimodal pretraining and supervised fine-tuning which expose the model to co-occurrence of action words (e.g. 'turn', 'stop') and trajectory visual/positional cues — the model learns correspondences in the joint representation and uses a learned compatibility scoring function to map instruction semantics to candidate paths.",
            "perception_requirements": "RGB panoramic visual input (360° panoramas represented via multiple perspective projections at 36 discrete headings/elevations), region proposals/features extracted with a bottom-up attention Faster R-CNN pretrained on Visual Genome, per-region features augmented with bounding-box location (normalized), area, elevation, heading (encoded as [cosθ,sinθ]) and panorama-index embeddings.",
            "transfer_successful": true,
            "performance_with_pretraining": "Full three-stage curriculum -&gt; single-model path-selection Success Rate (SR) 59.3% (reported in paper); leaderboard 3-model ensemble (speaker + follower + VLN-BERT) Test Unseen SR = 73% (VLN leaderboard). Language-only initialization (BERT) yields SR = 45.2%. Ablations reported: combining visual-grounding (image-text) and action-grounding stages in series yields a 14.1 absolute percentage point improvement (cumulative), and the authors state that pretraining on image-text pairs provides a substantial boost (reported contribution numbers in text: +4.5 and +4.9 abs points when used independently; combined gives +14.1 abs; paper also highlights a +9.2 abs point improvement attributed to image-text pretraining in some comparisons).",
            "performance_without_pretraining": "Training from scratch (no language or visiolinguistic pretraining) achieves SR = 30.5% (reported). Other baselines: prior state-of-the-art follower/speaker models (not using the same visiolinguistic pretraining curriculum) achieve ~54.7% SR in some reported comparisons (paper compares against follower/speaker models from Tan et al. [28]).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "1) Curriculum ordering (language → visual grounding → action grounding) that progressively exposes model to language, visual concepts, then trajectory-action referents; 2) strong pretrained language representations (BERT) and visiolinguistic representations (ViLBERT) providing object-level grounding; 3) object-centric region features and cross-modal co-attention enabling alignment; 4) positional/panoramic spatial encodings that link visual evidence to trajectory-relative geometry.",
            "transfer_failure_factors": "1) Domain shift between web images (well-composed, aesthetic viewpoints) and embodied panoramic views (agent-centric, occlusions, unusual framings); 2) absence of an explicit symbolic action-to-motor mapping (mapping is implicit and learned via examples); 3) limited size of task-specific path-instruction data (~14k R2R pairs) constrains action grounding and fine-grained procedural generalization.",
            "key_findings": "Pretraining on large-scale language and web image-text corpora, then fine-tuning with a targeted action-grounding stage, produces large, synergistic gains on vision-and-language navigation: language-only pretraining gives a large jump (30.5% → 45.2% SR), adding image-text pretraining and action-grounding in series yields further gains and a single-model SR of 59.3%, and an ensemble achieves 73% SR on Test Unseen; visual-grounding from web image-caption data substantially improves grounding of rare object references despite domain shift, and the staged curriculum is critical for successful internet-to-embodied transfer.",
            "uuid": "e1707.0",
            "source_info": {
                "paper_title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "ViLBERT",
            "name_full": "ViLBERT (visiolinguistic BERT)",
            "brief_description": "A two-stream BERT-like architecture for learning joint vision-and-language representations from aligned image-text corpora using masked multimodal modelling and multimodal alignment objectives; visual stream ingests object-region inputs and co-attends with language stream.",
            "citation_title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "mention_or_use": "use",
            "model_agent_name": "ViLBERT (pretrained weights)",
            "model_agent_description": "Two BERT-like streams (vision and language) connected with co-attention transformer layers; visual input is a sequence of object-region features (from a detector pretrained on Visual Genome) and language input is tokenized caption text; trained with masked multimodal modelling (predict masked text tokens and object classes for masked regions) and image-caption alignment.",
            "pretraining_data_type": "web image-text pairs (image alt-text captions)",
            "pretraining_data_details": "Pretrained on the Conceptual Captions dataset (web-scraped image alt-text pairs; paper cites Conceptual Captions containing ≈3.3M image-text pairs) using masked multimodal modelling and multimodal alignment objectives; visual region inputs derived from bottom-up attention detectors trained on Visual Genome.",
            "embodied_task_name": "Vision-and-Language Navigation (VLN) — Room-to-Room (R2R) (as used to initialize VLN-BERT)",
            "embodied_task_description": "Used as an initialization for VLN-BERT which is fine-tuned on path-instruction compatibility and path selection in the R2R VLN task (Matterport3D panoramas, navigation graph, goal within 3m success condition).",
            "action_space_text": "No action space in ViLBERT pretraining (image-caption prediction and alignment tasks only).",
            "action_space_embodied": "Discrete navigation / candidate path selection in R2R (handled during fine-tuning of VLN-BERT).",
            "action_mapping_method": "ViLBERT weights initialize the visual and language streams of VLN-BERT; mapping from caption-language semantics to trajectory-level actions is achieved later via Stage-3 action-grounding on path-instruction pairs and fine-tuning — i.e., an implicit learned mapping through joint representation adaptation rather than an explicit translator.",
            "perception_requirements": "Single-image region features at pretraining (object-detector regions). For transfer to VLN, these region-level priors are applied to panorama-extracted regions (Faster R-CNN features), augmented with heading/elevation and panorama-index embeddings.",
            "transfer_successful": true,
            "performance_with_pretraining": "Authors report that adding the image-text (ViLBERT) pretraining stage increases path-selection SR: when used independently it provides an improvement of ≈4.5 absolute percentage points (relative to the compared baseline in their ablations), and when combined with Stage-3 action-grounding the series yields a cumulative improvement of 14.1 abs points; full curriculum single-model SR = 59.3%; authors emphasize a substantial boost from web image-text pretraining.",
            "performance_without_pretraining": "Without the visual-grounding stage, models fail to ground some rare object phrases and select unsuccessful paths in ablated examples; the paper reports the from-scratch baseline SR = 30.5% and language-only SR = 45.2% (so ViLBERT stage contributes further improvements on top of language-only).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Object-centric region supervision from Visual Genome detectors, large-scale image-caption co-occurrences (Conceptual Captions) that cover a wide range of visual concepts, and co-attention architecture that can be adapted to sequential panoramic inputs.",
            "transfer_failure_factors": "Domain gap between curated web images (aesthetic framing) and agent-centric panoramic views (different viewpoints/occlusions/framing), which can limit direct transfer of appearance priors for some concepts.",
            "key_findings": "Initializing VLN-BERT with ViLBERT weights pretrained on large web image-text corpora yields measurable, non-trivial improvements in an embodied VLN task, especially for grounding rare object references; the transfer is effective when followed by an action-grounding fine-tuning stage that adapts visiolinguistic priors to trajectory-relative spatial reasoning.",
            "uuid": "e1707.1",
            "source_info": {
                "paper_title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (Bidirectional Encoder Representations from Transformers)",
            "brief_description": "A large bidirectional Transformer-based language representation model trained with masked language modelling and next-sentence prediction on large text corpora; used here to initialize the language stream of VLN-BERT.",
            "citation_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "mention_or_use": "use",
            "model_agent_name": "BERT (pretrained weights)",
            "model_agent_description": "12-layer Transformer encoder (BERT-BASE in experiments) trained on masked language modelling and next-sentence prediction; provides strong contextualized language representations used to initialize VLN-BERT's language stream.",
            "pretraining_data_type": "large-scale text corpora for language modeling",
            "pretraining_data_details": "Pretraining corpora cited: English Wikipedia and BooksCorpus (as in BERT); used to initialize the language stream of VLN-BERT (Stage 1).",
            "embodied_task_name": "Vision-and-Language Navigation (VLN) — Room-to-Room (R2R) (used as initializer for VLN-BERT)",
            "embodied_task_description": "See VLN-BERT entry: path selection in Matterport3D-derived R2R environments where instructions describe trajectories between panorama nodes.",
            "action_space_text": "No action space — BERT is trained on text-only objectives (masked LM, next-sentence prediction).",
            "action_space_embodied": "Discrete navigation / path selection over panoramas during VLN fine-tuning and evaluation.",
            "action_mapping_method": "Language representations from BERT provide robust encoding of instruction tokens; during subsequent multimodal pretraining and fine-tuning these representations are co-attended with visual features and trajectory encodings to implicitly map language phrases (including action words) to trajectory correlates.",
            "perception_requirements": "Not applicable for BERT pretraining; when transferred to VLN it is used together with panoramic RGB region features and spatial encodings.",
            "transfer_successful": true,
            "performance_with_pretraining": "Initializing with BERT (language-only pretraining) substantially improves VLN path-selection SR to 45.2% (from 30.5% when training from scratch); the paper highlights this as a major contributor to downstream performance.",
            "performance_without_pretraining": "From-scratch training (no BERT initialization) achieves SR = 30.5% (reported).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong contextual language priors that help parsing and grounding of instructions, reducing reliance on limited task-specific instruction data.",
            "transfer_failure_factors": "BERT alone provides language understanding but requires additional visual-grounding and action-grounding stages to map language to embodied perception and trajectories; language-only pretraining cannot resolve visual domain-shift.",
            "key_findings": "Language-only pretraining (BERT) yields a large single-stage improvement on embodied VLN (≈14.7 absolute point SR increase over scratch), demonstrating that robust language representations are a crucial component of successful transfer to navigation tasks, but must be combined with visual and action grounding for maximal gains.",
            "uuid": "e1707.2",
            "source_info": {
                "paper_title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
            "rating": 2,
            "sanitized_title": "vilbert_pretraining_taskagnostic_visiolinguistic_representations_for_visionandlanguage_tasks"
        },
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2,
            "sanitized_title": "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding"
        },
        {
            "paper_title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset for Automatic Image Captioning",
            "rating": 1,
            "sanitized_title": "conceptual_captions_a_cleaned_hypernymed_image_alttext_dataset_for_automatic_image_captioning"
        },
        {
            "paper_title": "Towards learning a generic agent for vision-and-language navigation via pre-training",
            "rating": 2,
            "sanitized_title": "towards_learning_a_generic_agent_for_visionandlanguage_navigation_via_pretraining"
        },
        {
            "paper_title": "Robust navigation with language pretraining and stochastic sampling",
            "rating": 2,
            "sanitized_title": "robust_navigation_with_language_pretraining_and_stochastic_sampling"
        }
    ],
    "cost": 0.019518999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving Vision-and-Language Navigation with Image-Text Pairs from the Web</p>
<p>Arjun Majumdar 
Georgia Institute of Technology</p>
<p>Ayush Shrivastava 
Georgia Institute of Technology</p>
<p>Stefan Lee 
Oregon State University</p>
<p>Peter Anderson 
Georgia Institute of Technology</p>
<p>Devi Parikh 
Georgia Institute of Technology</p>
<p>Facebook AI Research (FAIR)</p>
<p>Dhruv Batra 
Georgia Institute of Technology</p>
<p>Facebook AI Research (FAIR)</p>
<p>Improving Vision-and-Language Navigation with Image-Text Pairs from the Web
vision-and-language navigationtransfer learningBERTembodied AI
Following a navigation instruction such as 'Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g.'stairs') to visual content in the environment (pixels corresponding to 'stairs').We ask the following question -can we leverage abundant 'disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions [24]) to learn visual groundings (what do 'stairs' look like?) that improve performance on a relatively data-starved embodied perception task (Visionand-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction ('...stop at the brown sofa') and a sequence of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN -outperforming the prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful -with their combination resulting in further positive synergistic effects.</p>
<p>Introduction</p>
<p>Consider the navigation instruction in Figure 1, 'Walk through the bedroom and out of the door into the hallway. Walk down the hall along the banister rail through the open door. Continue into the bedroom with a round mirror on the wall and butterfly sculpture.' In vision-and-language navigation (VLN) [4], agents must interpret such instructions to navigate through photo-realistic environments. In this instance, the agent needs to exit the bedroom, walk past something called a 'banister rail' and find the bedroom containing a 'round mirror' and 'butterfly sculpture.' But what if the agent has never seen a butterfly  We propose a compatibility model (right) for path selection in vision-andlanguage navigation (VLN). In contrast to the follower (left) and speaker (center) models that have typically been used in prior work, our model takes a path and instruction pair as input and produces a score that reflects their alignment. Based on this model we describe a training curriculum that leverages internet data in the form of image-caption pairs to improve performance on VLN.</p>
<p>before, let alone a sculpture of one? To solve this task, an agent needs to determine if the visual evidence along a path matches the descriptions provided in the instructions. As such, the ability to ground references to objects and scene elements like 'butterfly sculpture' and 'banister rail' is central to success. Existing work has focused on learning this grounding solely from a task-specific training dataset of path-instruction pairs [3,8,13,20,21,28,30,31] -which are expensive, laborious, and time-consuming to collect at scale and thus tend to be relatively small (e.g. the VLN dataset contains around 14k path-instruction pairs for training). As an alternative, we propose learning visual grounding from freely-available internet data, such as the web images with alt-text captured in the Conceptual Captions dataset [24], containing around 3.3M image-text pairs. Conceptually, transfer learning from large-scale web data to embodied AI tasks such as VLN is an attractive alternative to collecting more data. Empirically, however, the effectiveness of this strategy remains open to question -would such a transfer even work? Unlike web images, which are highly-curated and stick closely to aesthetic biases, embodied data contains content and viewpoints that are not widely published online. For example, as shown in Figure 2, an embodied agent may perceive doors via a close-up view of a door frame rather than as a carefully composed image of a (typically closed) door. In VLN, image framing is a consequence of the agent's position rather than an aesthetic choice made by a photographer. Consequently, in this paper we investigate this question -to what degree can webly-supervised visual grounding learned on static images be transferred to the embodied VLN task? Put more succinctly, can 'disembodied' web data be used to improve visual grounding for embodied agents?</p>
<p>To answer this question, we introduce VLN-BERT, a joint visiolinguistic transformer-based compatibility model for scoring the alignment between an instruction and an agent's observations along a trajectory. We structure VLN-  [24] (top) and Matterport3D (MP3D) [5] (bottom) datasets illustrate the differences between the two domains. Images from CC are typically well-lit, well-composed and aesthetically pleasing, while for MP3D images (used in VLN) the framing depends on the position of the agent (e.g. a couch (left) in CC is typically viewed head-on, whereas in MP3D they may be hidden to the side as an agent navigates past them).</p>
<p>BERT to enable straight-forward transfer learning from a model from prior work on general visiolinguistic representation learning [18], and explore a training curriculum that incorporates both large-scale internet data and embodied pathinstruction pairs. VLN-BERT is sequentially trained using 1) language-only data (Wikipedia and BooksCorpus [34] as in BERT [7]), 2) web image-text pairs (Conceptual Captions [24] as in ViLBERT [18]), and 3) path-instruction pairs from the VLN dataset [4]. Following this protocol the model progressively learns to represent language, then to ground visual concepts, and finally to ground visual concepts alongside action descriptions. We evaluate VLN-BERT on a path selection task in VLN, demonstrating that this training procedure leads to significant gains over prior work (4 absolute percentage points on leaderboard success rate).</p>
<p>Contributions. Concretely, we make the following main contributions:</p>
<p>-We develop VLN-BERT, a visiolinguistic transformer-based model for scoring path-instruction pairs. We show that VLN-BERT outperforms strong single-model baselines from prior work on the path selection task -increasing success rate (SR) by 4.6 absolute percentage points.</p>
<p>-We demonstrate that in an ensemble of diverse models VLN-BERT improves SR by 3.0 absolute percentage points on "unseen" validation, leading to a SR of 73% on the VLN leaderboard (4 absolute percentage points higher than previously published work) 4 .</p>
<p>-We ablate the proposed training curriculum, and find that each stage contributes significantly to the final outcome, with a cumulative benefit that is greater than the sum of the individual effects. Notably, we find that pretraining on image-text pairs from the web provides a significant boost in path selection performance -improving SR by 9.2 absolute percentage points.</p>
<p>-We provide qualitative evidence that our model learns to ground object references. Specifically, using gradient-based methods [25] we visualize how image-region importance shifts under modifications to the instructions given to our model, demonstrating reasonable responses to these interventions. For example, if we modify the instruction 'Walk down the stairs, then stop next to the fridge.' by removing 'stop next to the fridge' we observe that image regions containing the fridge become less important.</p>
<p>Related Work</p>
<p>Path Selection in VLN. In VLN [4], an agent is required to follow a navigation instruction from a start location to a goal. While most existing works focus on the setting in which the test environments are previously unseen, many also consider the scenario in which the test environment is previously explored and stored in memory (i.e., fully observable). In this setting, a high-probability path is typically generated by performing beam search through the environment and ranking paths according to either: (1) their probability under a 'follower' model [3,8,13,20,21,30,31], as in Figure 1 (left), or (2) by how well they explain the instruction according to a 'speaker' (instruction generation) model [8,28], as in Figure 1 (center). In contrast, we use beam search with an existing agent model [28] to generate a set of candidate paths, which we then evaluate using our discriminative path-instruction compatibility model, as in Figure 1 (right).</p>
<p>Data Augmentation and Auxiliary Tasks in VLN.</p>
<p>To compensate for the small size of existing VLN datasets, previous works have investigated various data augmentation strategies and auxiliary tasks. Many papers report results trained on augmented data including instructions synthesized by a speaker model [8,20,21,28]. Tan et al. [28] use environmental dropout to mimic additional training environments to improve generalization. Li et al. [17] incorporate language-only pretraining using a BERT model. Several existing papers [11,30] and one concurrent hitherto-unpublished work [10] consider path-instruction compatibility as an auxiliary loss function or reward for VLN agents. We focus on path-instruction compatibility in the context of transfer learning from large-scale internet data, which has not been previously explored.</p>
<p>Vision-and-Language Pretraining. There has been significant recent progress towards learning transferable joint representations of images and text [15,16,18,26,27,32]. Using BERT-like [7] self-supervised objectives and Transformer [29] architectures, these models have achieved state-of-the-art results on multiple vision-and-language tasks by pretraining on aligned image-and-text data collected from the web [24] and transferring the base architecture to other tasks such as VQA [9], referring expressions [12], and caption-based image retrieval [6]. However, these tasks are all based on single images. The extent to which these pretrained models can generalize from human-composed and curated internet images to embodied AI tasks has not been investigated. In this work we propose a training curriculum to handle potential domain-shift and augment a previ-ous model architecture to process panoramic image sequences, extending the progress in vision-and-language to vision-and-language navigation (VLN).</p>
<p>3 Preliminaries: Self-Supervised Learning from the Web Recent works have demonstrated that high-capacity models trained under selfsupervised objectives on large-scale web data can learn strong, generalizable representations for both language and images [7,15,16,18,26,27,32]. We build upon these works as a basis for transfer and describe them briefly here.</p>
<p>Language Modeling with BERT. The BERT [7] model is a large transformerbased [29] architecture for language modeling. The model takes as input sequences of tokenized words augmented with positional embeddings and outputs a representation for each. For example, a two sentence input could be written as
<CLS> w (1) 1 , . . . , w (1) L1 <SEP> w (2) 1 , . . . , w (2) L2 <SEP>(1)
where CLS, and SEP are special tokens. To train this approach, [7] introduce two self-supervised objectives -masked language modelling and next sentence prediction. Given two input sentences from a text corpus, the masked language modelling objective masks out some percentage of tokens and tasks the model to predict their values given the remaining tokens as context. The next sentence prediction objective requires the model to predict whether the two sentences follow each other in the original corpus or not. BERT is then trained under these objectives on large language corpuses from the web (Wikipedia and BooksCorpus [34]). This model forms the basis for both our approach and the visiolinguistic representation learning discussed next.</p>
<p>Visiolinguistic Representations Learning with ViLBERT. Extending BERT, ViLBERT [18] (and a number of similar approaches [15,16,18,26,27,32]) focuses on learning joint visiolinguistic representations from paired image-text data, specifically web images and their associated alt-text collected in the Conceptual Captions dataset [24]. ViLBERT is composed of two BERT-like processing streams that operate on visual and textual inputs, respectively. The input to the visual stream is composed of image regions (generated by an object detector [2,22] pretrained on Visual Genome [14]) that act as "words" in the visual domain. Concretely, given a single image I consisting of a set of image regions {v 1 , . . . , v k } and a text sequence (i.e. a caption) w 1 , . . . , w L , we can write the input to ViLBERT as the sequence
<IMG> v 1 , . . . , v k <CLS> w 1 , . . . , w L <SEP>(2)
where IMG, CLS, and SEP are special tokens marking the different modality subsequences. The two streams are connected using co-attention [19] transformer layers, which attend from the visual stream over language stream and vice versa. Notably, the language stream of ViLBERT is designed to mirror BERT such that it can be initialized by a pretrained BERT model. After processing, the model produces a contextualized output representation for each input token. </p>
<p>Energy(path, instruction)</p>
<p>Training Curriculum</p>
<p>Language-Only (Wikipedia and BookCorpus)</p>
<p>A couch, also known as a sofa is a piece of furniture for seating two or three people...</p>
<p>Path-Instruction Pairs</p>
<p>(Room-to-Room)</p>
<p>Turn right and into the living room. Walk past the sofa and stop by the door.</p>
<p>Image-Caption Pairs (Conceptual Captions)</p>
<p>blue sofa in the living room Fig. 3. We propose VLN-BERT (top), a visiolinguistic transformer-based model similar to the model from [18], to process image regions from a sequence of panoramas and words from an instruction. We demonstrate that with the proposed training curriculum (bottom) visual grounding learned from image-text pairs from the web (center) can be transferred to significantly improve performance in VLN.</p>
<p>In analogy to the training objectives in BERT, ViLBERT introduces the masked multimodal modelling and multimodal alignment tasks. In the first, a random subset of language tokens and image regions are masked and must be predicted given the remaining context. For image regions, this amounts to predicting a distribution over object classes present in the masked region. Masked text tokens are handled as in BERT. The multimodal alignment objective trains the model to determine if an image-text pair matches, i.e. if the text describes the image content. Individual token outputs are used to predict masked inputs in the masking objective, and the IMG and CLS tokens are used for the image-caption alignment objective. We build upon this model extensively in this work.</p>
<p>Approach</p>
<p>We first describe the path selection setting in Vision-and-Language Navigation (Section 4.1), then our proposed model architecture (Section 4.2), and finally our transfer learning curriculum (Section 4.3).</p>
<p>Vision-and-Language Navigation as Path Selection</p>
<p>In the Vision-and-Language Navigation (VLN) [4] task, agents are placed in an environment specified as a navigation-graph G = {V, E}. Nodes v ∈ V represent different positions within the environment, and are represented by 360-degree panoramas taken at that viewpoint. Meanwhile, edges delineate navigable paths between panorama positions. The agent is provided with a navigation instruction x that describes the shortest-path between a starting position v s and goal position v g (as illustrated in the bottom right of Figure 3). Agents are considered to have succeeded if they traverse a path τ =
[v s , v 1 , v 2 , . . . , v N ] with a final position v N that is within 3m of the goal v g .
Much of the work in VLN focuses on this problem as an exploration task in new environments; however, many practical deployments of robotic agents would be long-term in relatively fixed environments (e.g. an assistant operating in a single home). In this paper, we consider the setting in which the environment is previously explored with the navigation-graph and panoramas are stored in the agent's memory. This setting has been studied in prior work [8,13,20,28,30] and is operationalized by providing the agent unrestricted access to the Matterport3D Simulator [4] during inference such that it can consider arbitrarily many valid paths originating from the starting position v s , before selecting one to follow.</p>
<p>In this setting, the navigation task becomes one of identifying the path best aligned with the instructions. Concretely, given a set of valid paths T with the same starting position v s and an instruction x, the problem of navigation is to identify a trajectory τ * such that
τ * = argmax τ ∈T f (τ, x)(3)
for some compatibility function f that determines if the trajectory follows the instruction and terminates near the goal. The two major challenges are how to learn a compatibility function f and how to efficiently search through the large set of possible paths. Given that our focus is on transfer learning, we address the first challenge within a simple path selection setting. Specifically, we consider a small set of paths T = {τ 1 , τ 2 , . . . , τ M } for each instruction, which are generated using beam-search with a greedy instruction-following agent [28], and task f with selecting the path that best aligns with the instruction from this set. Future work might explore how f could be further used as a heuristic to efficiently search through the larger, exhaustive set of candidate paths T .</p>
<p>Modeling Instruction-Path Compatibility</p>
<p>To formalize the task, we consider a function f that maps a trajectory τ and an instruction x to compatibility score f (·, ·). We model f (τ, x) as a visiolinguistic transformer-based model denoted as VLN-BERT. The architecture of VLN-BERT is structural similar to ViLBERT [18]; this is by design because it enables straight-forward transfer of visual grounding learned from large-scale web data. Specifically, we make a number of VLN-specific adaptations to ViLBERT, but they are all structured as augmentations (adding modules) rather than ablations (removing existing network components) so that pretrained weights can be transferred to initialize large portions of the model.</p>
<p>Representing Trajectories and Instructions. Predicting path-instruction compatibility requires jointly reasoning over a sequence of observations and a sequence of instruction words. As in prior work [8], a trajectory is represented as a sequence of panoramic images (as in Figure 3  are poses. Further, we represent each panorama I i as a set of image regions {r
(i) 1 , . . . , r (i) K }.
Let an instruction x be a sequence of tokens w 1 , . . . , w L . We can thus write a path-instruction pair for VLN-BERT as the input sequence
<IMG> r (1) 1 , . . . , r (1) K , . . . , <IMG> r (N ) 1 , . . . , r (N ) K , <CLS> w 1 , . . . , w L <SEP> (4)
where IMG, CLS, and SEP are special tokens as before.</p>
<p>The transformer models on which VLN-BERT (as well as BERT and ViL-BERT) is based are inherently invariant to sequence order -only representing interactions between inputs as a function of their values. The common practice to introduce this information is to add positional embeddings to the input token representations. For language, this is straight-forward and amounts to an indexin-sequence encoding. Panorama trajectories on the other have significantly more complex relationships. While the panoramas themselves are a sequence, there are also geometric relationships between them (e.g. two panoramas being 1.2 meters apart at 10 degrees off north). Further, each individual image region not only has a position in the image (as modelled in ViLBERT) but also an angle relative to the heading of an agent as it traverses the trajectory. These are important considerations for language-guided navigation -after all, something on your left going one way is on your right if you go in the opposite direction. Being able to reason about the order of panoramas and the relative heading of image content is integral for following instructions like 'Go down the hallway on the right then stop when you see a table on your left.'.</p>
<p>To address this, as visualized in Figure 4(a), we encode the spatial location of each image region r i in terms of its location in the panorama (top-left and bottom-right corners in normalized coordinates as well as area of the image covered), its elevation relative to the horizon, and its heading relative to the agents current and next viewing directions. All angles are encoded as [cos(θ), sin(θ)]. The resulting 11-dimensional vector S i is projected into 2048 dimensions using a learned projection W S . To capture the sequential order of the panoramas within a trajectory, we project the scalar panorama index to 2048 dimensions using a learned embedding W P . As shown in Figure 4(b), the complete visual input representation for the image region is the element-wise sum of the visual features, panorama index embedding, and panoramic spatial embedding.</p>
<p>Extracting Image Regions from Panoramas. To extract the image regions {v
(i) 1 , . . . , v (i)
k } for each panorama, we process the panoramas by first generating 600 × 600 pixel perspective projections using an 80 degree field of view at the 36 discrete heading and elevation directions used in previous work [4]. Similarly to ViLBERT, we then use the bottom-up attention model [2,22] pretrained on Visual Genome [14] to independently extract a set of image regions and features from each perspective image. Since the perspective images have substantial overlap we remove redundant regions within each panorama. First, we discard regions that are centred more than 20 degrees away from the center of the image (i.e. we discard regions along the boarders). We assume that the discarded regions will be captured in a neighboring perspective image (spaced at 30 degree heading increments), with more visual context. Next, we examine pairs of image regions within each panorama in order of decreasing feature similarity. We discard the region in the pair with the lower bottom-up attention class detection score, until a maximum of 100 regions per panorama remain. We define similarity as the cosine distance between image features to which we add the absolute difference in region heading and elevation. Including heading and elevation differences ensures that visually similar features found in different regions of the panorama are unlikely to be classified as redundant.</p>
<p>Training for Path Selection. To train VLN-BERT for path selection, we consider a 4-way multiple-choice task. Given an instruction x, we sample four trajectories out of which only one is successful {τ
+ 1 , τ − 2 , τ − 3 , τ − 4 }.
We run VLN-BERT on each instruction-trajectory pair and extract their corresponding final representations. We denote these outputs for the CLS and the first IMG token as h CLS and h IMG respectively and compute a compatibility score s i as
s i = f (τ i , x) = W h (i) CLS h (i) IMG(5)
where denotes element-wise multiplication and W is a learned transformation matrix. Scores, normalized via a softmax, are supervised with cross-entropy loss,
p = softmax(s) (6) L x, {τ + 1 , τ − 2 , τ − 3 , τ − 4 } = CrossEntropy p, 1<a href="7">τ + 1 </a>
where 1[τ + 1 ] is a 1-hot vector with mass at the index of τ + . At inference, we simply sort trajectories by their compatibility scores s i .</p>
<p>Mining Negative Examples. We find that choosing an appropriate set of path-instruction pairs is critical to performance. Ideally, samples would span the space of all possible pairs, including hard negatives such as a hypothetical example where the agent must select between paths that end at two different 'butterfly sculptures'. The question is how to find varied path-instruction pairs with semantically meaningful differences? We find that using beam search with an instruction-following model yields a diverse set of paths that are effective for training. Furthermore, the paths are conditioned on the instructions, and we find that in practice the incorrect paths often make semantically meaningful mistakes. Specifically, we sample up to 30 beams per instruction from the follower model of Tan et al. [28] and label the path as successful if it meets the VLN success criteria (i.e. it ends &lt; 3m from the goal). Finally, one positive and three negatives pairs are sampled uniformly at random for training.</p>
<p>Internet-to-Embodied Transfer Learning</p>
<p>While VLN-BERT can be trained from scratch, as described above we designed the model to specifically enable transfer learning from language [7] and visiolinguistic [18] models trained on large-scale web corpora. This transfer is especially important in the VLN task which is relatively data-sparse (containing approximately 14k path-instruction pairs for training) and has a natural bias towards describing only objects present in training environments (i.e. objects that are unique to the testing environments are never referenced in the training instructions). In this section, we describe a pretraining curriculum for transferring models learned on 'disembodied' web data to the embodied VLN task.</p>
<p>We summarize the pretraining process in Figure 3. In total, we consider three stages focused on learning language, visual grounding, and action grounding.  [24] under the masked multimodal language modelling and multimodal alignment objectives. In this stage, we initialize model weights with a ViLBERT model trained in this manner. Transferring directly from this stage provides an initialization that can associate descriptions with image regions.</p>
<p>-Stage 3: Action Grounding. In the final stage, we pair paths and instructions from VLN and train the model under the masked multimodal modelling objective from [18]. While the previous stage learns to ground visual concepts, this stage additionally exposes the model to actions and their trajectory-based referents. For example, correctly predicting a masked instruction phrase like 'turn ' or 'stop at the ' requires the model to reason about the agent's path from the visual inputs and positional encodings.</p>
<p>After these pretraining stages, we fine-tune our VLN-BERT model for path selection as described in the previous section.</p>
<p>Experiments</p>
<p>Our experiments primarily address following questions:</p>
<ol>
<li>Does pretraining on web image-text pairs improve VLN performance? 2. How does the performance of VLN-BERT compare with strong baselines? 3. Does VLN-BERT consider relevant image regions to produce alignment scores?</li>
</ol>
<p>Dataset</p>
<p>We conduct experiments using the Room-to-Room (R2R) navigation task [4] that was generated using the Matterport3D dataset [5]. R2R contains humanannotated path-instruction pairs that are divided into training, seen and unseen validation, and unseen testing sets. To generate a dataset for path selection we run beam search on the instruction-follower model from [28], to produce a set of up to 30 candidate paths for each instruction in R2R. We find that with a beam size of 30 over 99% of the candidate sets contain one path that reaches the goal, which places an acceptable upper bound on path selection performance. In all of the experiments that follow, results are reported for selecting one path from the set of candidates.</p>
<p>Evaluation Metrics</p>
<p>We compare the performance of different models using standard VLN metrics. Note that for path selection we calculate metrics using only the selected path, which corresponds with the pre-explored environment setting. However, for the VLN leaderboard results we follow the required approach of prepending the exploration path to the selected path (which affects path length based metrics).</p>
<p>-Success rate (SR) measures the percentage of selected paths that stop within 3m of the goal. In path selection this is our primary metric of interest.</p>
<p>-Oracle Success rate (SR) measures the percentage of selected paths with any position that passes within 3m of the goal.</p>
<p>-Navigation error (NE) measures the average distance of the shortest path from the last position in the selected path to the goal position.</p>
<p>-Path length (PL) measures the average length of the selected path -a lower PL is preferred if the trajectory is successful.</p>
<p>-Success rate weighted by path length (SPL), as defined in [1], provides a measure of success normalized by the ratio between the length of the shortest path and the selected path.</p>
<p>Training Baseline Models</p>
<p>We compare with the follower and speaker models from [28], which achieve stateof-the-art performance on the VLN test set in an ensemble model setting. The only auxiliary dataset used to train these baseline models is ImageNet [23] (used to pretrain an image feature extractor). All of the other components are trained from scratch (including word embeddings). Data augmentation, via environmental dropout [28], is used to train the follower model and greatly improves performance. We report results using code and weights provided by Tan et al. [28].</p>
<p>Results</p>
<p>Does pretraining on web image-text pairs improve VLN performance?</p>
<p>To answer this question we dissect our proposed training curriculum as indicated in Table 1, and find that in general each stage of training does contribute to performance. First, we find that our model has limited performance learning from scratch, achieving only 30.5% SR (compared with the 54.7% SR achieved by the speaker model from [28]). However, language-only pretraining, which corresponds to initializing our model with BERT [7] weights, improves performance substantially to 45.2% SR (an improvement of 14.7 absolute percentage points) -indicating that language understanding plays an important role in VLN.</p>
<p>Next, we find that both pretraining on image-text pairs from the Conceptual Captions [24] (visual grounding) and pretraining on path-instruction pairs from VLN [4] (action grounding) similarly improve success rate (by 4.5 and 4.9 absolute percentage points, respectively) when used independently. However, when the two pretraining stages are combined in series the improvement jumps to 14.1 absolute percentage points in success rate or 9.2 absolute percentage points over the next best setting. The substantial level of improvement that results from our full training curriculum suggests that not only does pretraining on weblysupervised image-text pairs from [24] improve path selection performance, but it also constructively supports the action grounding stage (Stage 3) of pretraining.</p>
<p>How does VLN-BERT compare with strong baseline methods? The results in Table 2 compare path selection performance of VLN-BERT with the state-of-the-art speaker and follower models from [28]. We evaluate path selection using the set of up to 30 candidate paths generated with beam search using the follower from [28]. For the follower model results this amounts to taking the top beam from the candidate set. In the single model setting we see that VLN-BERT, trained with our full curriculum, achieves 59.3% SR, which is 4.6 absolute percentage points better than either of the other two methods.</p>
<p>In the pre-explored setting, the speaker and follower models are typically combined in an ensemble to further improve path selection performance [8,28]. The two models are typically combined as a linear combination using a hyperparameter α that is selected through grid search on the val unseen split of R2R [8]. Table 2. Results comparing VLN-BERT with the follower and speaker from [28]. Notably, in the ensemble models setting, combining VLN-BERT with the speaker and follower results in a 3 absolute percentage point improvement in Val Unseen Success Rate (SR) over the next best three-model ensemble (compare rows 6 and 7).</p>
<p>Val Seen</p>
<p>Val Unseen  In the ensemble models section of Table 2, the speaker + follower line (row 4) represents our execution of the state-of-the-art ensemble model from [28]. In rows 5-7, we consider three model ensembles composed of a speaker, follower, and one additional model combined using two hyperparameters α and β (again selected through grid search on val unseen). We find that adding another (randomly seeded) speaker or follower model yields modest improvements of 1.2 and 2.7 absolute percentage points in SR (rows 5 and 6). In contrast, adding VLN-BERT results in a 5.7 absolute percentage point boost in SR (row 7), which is 3.0 absolute percentage points higher on success rate than the next best ensemble. In Table 3 we report results on the VLN test set via the VLN leaderboard. In the leaderboard setting we use a three-model ensemble that includes a speaker, follower, and VLN-BERT. The ensemble achieves a success rate of 73%, which is 4 absolute percentage points greater than previously published work [28], and 2 absolute percentage points greater than concurrent, unpublished work [33].</p>
<h1>Re-ranking Model PL NE ↓ SPL ↑ OSR ↑ SR ↑ PL NE ↓ SPL ↑ OSR ↑ SR ↑ Single</h1>
<p>Does VLN-BERT consider relevant image regions to produce alignment scores? One motivation behind our proposed approach is to improve the grounding of object references in VLN instruction. To test whether this actually happens and to gain insight into the improved performance of VLN-BERT, we visualize which parts of the visual input affect the compatibility score. We perform this analysis using a simple gradient-based visualization technique [25]. We take the gradient of our learned score f (x, τ ) with respect to the feature repre-sentation for each region from each panorama, and sum this 2048-dimensional gradient vector over the feature dimension to produce a scalar measure of region importance. To gain deeper insight we analyze how the region importance changes when the instruction is perturbed by removing parts of the description.</p>
<p>Three examples of this analysis are illustrated in Figure 5. The left panel provides the original and modified versions of the instruction with high-importance regions highlighted in green and purple, respectively. In each row, the original instruction is modified by removing a phrase that references a high-importance region (e.g. in the first row 'then stop next to the fridge' is removed). In the middle and right panels, the region importance histograms and top 5 regions illustrate which parts of the visual input most influence the compatibility score. For example, in the first row regions containing a 'fridge' are important for the original instruction, whereas for the modified instruction the importance shifts to the 'stairs'. Independently, the center and right panels demonstrate that VLN-BERT produces compatibility scores based on relevant image regions. Furthermore, by comparing the top 5 regions before and after the instruction is modified, we see that the grounding of object references appropriately shifts under linguistic interventions. This analysis provides qualitative evidence that VLN-BERT properly learns to associate instruction phrases with image regions.</p>
<p>Conclusion</p>
<p>In this work, we demonstrated internet-to-embodied transfer of visual concept grounding -leveraging large-scale image-text data from the web to improve a discriminative path-instruction alignment model for VLN. In our path re-ranking setting, this model improves over prior work and our ablations show each stage of our transfer curriculum contributes significantly.  Fig. 5. We compare region importance histograms for three path-instruction pairs under perturbations of the instruction -removing a phrase or sentence. The region importance histogram is calculated by taking the gradient of the compatibility score with respect to the image region features. The images above each histogram correspond to the most influential regions (i.e. the peaks in the histogram). The underlined instruction phrases correspond with the regions outlined in green and purple to provide a qualitative assessment of the visiolinguistic grounding learning by VLN-BERT. In the first example, removing the reference to the 'fridge' (in green) shifts the importance to other regions along the path (i.e. the 'stairs' in purple), suggesting that VLN-BERT considers visually relevant image regions to score path-instruction pairs.</p>
<p>A Supplementary Material</p>
<p>In this section we provide further implementation details for VLN-BERT (Section A.1) and additional qualitative analysis of performance (Section A.2) and the pretraining curriculum (Section A.3). Code will be provided to reproduce the results of all experiments.</p>
<p>A.1 Implementation Details</p>
<p>The experiments described in Section 5 utilize a 12-layer BERT BASE [7] architecture for both the vision and language streams in the model. Following [18], we use 6 cross-modal attention layers to connect the two streams. To operationalize the language-only pretraining stage (Stage 1), VLN-BERT is initialized with BERT weights that result from pretraining on English Wikipedia and BooksCorpus [34]. Similarly, for the visual grounding stage (Stage 2), VLN-BERT is initialized with ViLBERT weights, which result from pretraining on the Conceptual Captions [24] dataset. For action grounding pretraining and path-selection finetuning (Stage 3) we use path-instruction pairs from the Room-to-Room [4] dataset. During this stage, models are trained with the Adam optimizer with a learning rate of 4e-5 and a batch size of 64. We use a learning rate schedule with a linear warmup and cooldown. We train for 50 epochs in pretraining and 20 epochs in finetuning. During finetuning, we utilize early stopping based on the success rate on the unseen split of the validation set. Using 8 Titan X GPUs Stage 3 of training takes approximately 66 hours.</p>
<p>A.2 Qualitative Examples of Success and Failure</p>
<p>This section provides qualitative examples of the path selection performance of VLN-BERT using the full training curriculum described in Section 4.3. To gain insight at the region-level, we estimate region importance using the gradientbased visualization technique described in Section 5.4 (i.e. importance is calculated as the sum of the gradient of the model's output score with respect to the features for each region). In all examples, the model selects one path from a set of up to 30 candidate paths for a given set of instructions. The selected path is successful if it terminates within 3m of the goal location.</p>
<p>Three examples of successful path selection are illustrated in Figure 6. In the first example, VLN-BERT selects a path that does not initially follow the ground truth path, but correctly stops at the goal location. In this example, the model accurately grounds the phrase 'antelope head', which does not appear in the VLN [4] training dataset (the term 'antelope' appears 3 times). In the second and third examples, the selected paths closely match the ground truth, and the top 5 regions include key objects mentioned in the instructions -'freezers' and 'statue'. The term 'freezers' (with and without the 's') does not occur the VLN training dataset, and 'statue' appears 105 times. These examples suggest that VLN-BERT is able to transfer visual grounding learned on the image-text pairs in the Conceptual Captions [24] dataset to the embodied task of path selection. Fig. 6. Examples of successful paths selected by VLN-BERT (middle -blue), with ground truth paths (middle -orange) and navigation errors (middle -red) provided for comparison. The right column illustrates the top 5 regions that influence the model's predictions -importance is determined by taking the gradient of the score for a pathinstruction pair with respect to the input region features (as in Section 5.4). A qualitative assessment of accurate visiolinguistic grounding (in green) highlights phrases that rarely occur in VLN training dataset: 'antelope head' (0 occurrences), 'antelope' (3 occurrences), 'freezer(s)' (0 occurrences), 'statue' (105 occurrences). These results suggest that VLN-BERT has effectively learned to transfer grounding from image-text pairs from the web to the embodied task of VLN.</p>
<p>Three unsuccessful examples are shown in Figure 7. In each example the characteristics of the errors are qualitatively different. In the first row, VLN-BERT selects a path that does not stop at the correct goal location. However, the top 5 regions include key visual landmarks mentioned in the instruction (e.g.'treadmill' and 'sofa'). In contrast, in the second row, VLN-BERT selects a path that does not reach the goal bedroom. In this instance, the top 5 regions include 'curtains' from a different location, which may have led to this particular error. In the last row every aspect of the selected path seems mismatched from the instructions: the path goes to the left of the table not right and objects mentioned in the instructions are missing from the top 5 regions. Fig. 7. Examples of unsuccessful paths selected by VLN-BERT (middle -blue), with ground truth paths (middle -orange) and navigation errors (middle -red) provided for comparison. The right column illustrates the top 5 regions that influence the model's predictions -importance is determined by taking the gradient of the score for a pathinstruction pair with respect to the input region features (as in Section 5.4). In the first row, the selected path goes past the goal location, but the visual grounding appears accurate. In the second row, the selected path does not enter the correct bedroom and the visual grounding appears to identify the wrong curtains. In the final example the model fails completely -selecting a path going to the left (not right) of the table.</p>
<p>A.3 Qualitative Analysis of the Pretraining Curriculum</p>
<p>In section Section 5.4 we demonstrated that the visual grounding pretraining stage (Stage 2) quantitatively improves performance. In Figure 8 we qualitatively compare the visual grounding that is learned with and without stage 2 of pretraining. In the first example, the model trained without stage 2 of pretraining (right) fails to ground the phrase 'mini fridge', which only occurs 1 time in the VLN training dataset. Similarly, in the second example, without stage 2 pretraining, the model fails to ground the phrase 'massage table' (29 occurrences in the VLN training dataset). In both cases, the model without stage 2 pretraining selects an unsuccessful path. In contrast, when trained with the full curriculum, VLN-BERT correctly grounds these key phrases and selects successful paths for these two examples.</p>
<p>Fig. 1 .
1Fig. 1. We propose a compatibility model (right) for path selection in vision-andlanguage navigation (VLN). In contrast to the follower (left) and speaker (center) models that have typically been used in prior work, our model takes a path and instruction pair as input and produces a score that reflects their alignment. Based on this model we describe a training curriculum that leverages internet data in the form of image-caption pairs to improve performance on VLN.</p>
<p>Fig. 4 .
4bottom right) with positional information -i.e. τ = [(I 1 , p 1 ), . . . , (I N , p N )] where (I i ) are panoramas and (p i ) We encode spatial information for each region to include not only the region position, but also its relation to the trajectory path (a). We form overall region encoding by summing visual features, an embedding indicating the index of the source panorama in the trajectory, and an embedding of panoramic spatial information (b).</p>
<p>Now at Google. arXiv:2004.14973v2 [cs.CV] 1 May 2020 Instructions: Walk through the bedroom and out of the door into the hallway. Walk down the hall along the banister rail through the open door. Continue into the bedroom with a round mirror on the wall and butterfly sculpture.Walk through 
the bedroom... 
Walk through 
the bedroom... </p>
<p>score: 1.87 
Walk out of 
the room... </p>
<p>Follower Model 
Speaker Model 
Compatibility Model (Ours) </p>
<p>-
Stage 1: Language. To capture strong language understanding capabilities, we initialize the language stream of our model with weights from a BERT[7] model trained on Wikipedia and the BooksCorpus[34] under the masked language modelling and next sentence prediction objectives. Directly training on the path selection task after this stage is analogous to introducing a BERT encoder to represent instructions.-Stage 2: Visual Grounding. Starting from a pretrained BERT model, Lu et al. train both streams of ViLBERT on the Conceptual Captions dataset</p>
<p>Table 1 .
1We compare the contribution from the different stages of pretraining.Grounding PL NE ↓ SPL ↑ OSR ↑ SR ↑ PL NE ↓ SPL ↑ OSR ↑ SR ↑We find </p>
<p>Table 3 .
3Leaderboard results on Test Unseen for methods using beam search.Test Unseen </p>
<p>Original Instruction: Leave the kitchen, and walk through the pantry. In the hall take a left, and take a right at the end of the hall. Stop next to the plant on the table in the entryway. Modified Instruction: Leave the kitchen, and walk through the pantry. In the hall take a left, and take a right at the end of the hall.Region Index (colored by panorama index) region importance Original Instruction: Exit the room, turn left, and go into the left room at the end of the hall. Proceed through this room and wait by the sink in the adjacent bathroom. Modified Instruction: Exit the room, turn left, and go into the left room at the end of the hall.Region Index 
(colored by panorama index) 
region importance </p>
<p>Region Index 
(colored by panorama index) </p>
<p>Original Instruction: Moving towards 
the stairs, but without using them, 
take a left, walk down the stairs, then 
stop next to the fridge. </p>
<p>Modified Instruction: Moving towards 
the stairs, but without using them, 
take a left, walk down the stairs. </p>
<p>key regions </p>
<p>region importance </p>
<p>key regions </p>
<p>Instruction Intervention 
Original Importance 
Modified Importance </p>
<p>Region Index 
(colored by panorama index) 
region importance </p>
<p>key regions </p>
<p>key regions </p>
<p>Region Index 
(colored by panorama index) 
region importance </p>
<p>key regions </p>
<p>Region Index 
(colored by panorama index) 
region importance </p>
<p>key regions </p>
<p>evalai.cloudcv.org/web/challenges/challenge-page/97/leaderboard/270
AcknowledgementsThe Georgia Tech effort was supported in part by NSF, AFRL, DARPA, ONR YIPs, ARO PECASE, Amazon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.Fig. 8. Comparison of pretraining with the proposed curriculum (middle) vs. omitting the visual grounding stage (right). Region importance histograms are estimated as in Section 5.4. The top 5 regions correspond with the successful path selected by VLN-BERT when pretrained with the full curriculum. Without the visual grounding pretraining stage (right), the model fails to ground phrases that rarely occur in the VLN training dataset (e.g.'mini fridge' (1 occurrence) and 'massage table' (29 occurrences)). Furthermore, in both cases without the visual grounding pretraining stage, VLN-BERT selects an unsuccessful path (not illustrated) for the given instructions.
P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.06757On evaluation of embodied navigation agents. arXiv preprintAnderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., et al.: On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757 (2018)</p>
<p>Bottom-up and top-down attention for image captioning and visual question answering. P Anderson, X He, C Buehler, D Teney, M Johnson, S Gould, L Zhang, CVPRAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and visual question answering. In: CVPR (2018)</p>
<p>Chasing ghosts: Instruction following as bayesian state tracking. P Anderson, A Shrivastava, D Parikh, D Batra, S Lee, Advances in Neural Information Processing Systems. Anderson, P., Shrivastava, A., Parikh, D., Batra, D., Lee, S.: Chasing ghosts: In- struction following as bayesian state tracking. In: Advances in Neural Information Processing Systems. pp. 369-379 (2019)</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, A Van Den Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAnderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., van den Hengel, A.: Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018)</p>
<p>A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision. Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from rgb-d data in indoor envi- ronments. International Conference on 3D Vision (3DV) (2017)</p>
<p>Microsoft coco captions: Data collection and evaluation server. X Chen, H Fang, T Y Lin, R Vedantam, S Gupta, P Dollár, C L Zitnick, arXiv:1504.00325arXiv preprintChen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)</p>
<p>J Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)</p>
<p>Speaker-follower models for vision-and-language navigation. D Fried, R Hu, V Cirik, A Rohrbach, J Andreas, L P Morency, T Berg-Kirkpatrick, K Saenko, D Klein, T Darrell, Advances in Neural Information Processing Systems. Fried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L.P., Berg- Kirkpatrick, T., Saenko, K., Klein, D., Darrell, T.: Speaker-follower models for vision-and-language navigation. In: Advances in Neural Information Processing Systems. pp. 3314-3325 (2018)</p>
<p>Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, Conference on Computer Vision and Pattern Recognition (CVPR. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in VQA matter: Elevating the role of image understanding in Visual Question An- swering. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2017)</p>
<p>Towards learning a generic agent for vision-and-language navigation via pre-training. W Hao, C Li, X Li, L Carin, J Gao, arXiv:2002.10638arXiv preprintHao, W., Li, C., Li, X., Carin, L., Gao, J.: Towards learning a generic agent for vision-and-language navigation via pre-training. arXiv preprint arXiv:2002.10638 (2020)</p>
<p>Multi-modal discriminative model for vision-and-language navigation. H Huang, V Jain, H Mehta, J Baldridge, E Ie, arXiv:1905.13358arXiv preprintHuang, H., Jain, V., Mehta, H., Baldridge, J., Ie, E.: Multi-modal discriminative model for vision-and-language navigation. arXiv preprint arXiv:1905.13358 (2019)</p>
<p>Referit game: Referring to objects in photographs of natural scenes. S Kazemzadeh, V Ordonez, M Matten, T L Berg, EMNLPKazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L.: Referit game: Referring to objects in photographs of natural scenes. In: EMNLP (2014)</p>
<p>Tactical rewind: Self-correction via backtracking in vision-and-language navigation. L Ke, X Li, Y Bisk, A Holtzman, Z Gan, J Liu, J Gao, Y Choi, S Srinivasa, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKe, L., Li, X., Bisk, Y., Holtzman, A., Gan, Z., Liu, J., Gao, J., Choi, Y., Srinivasa, S.: Tactical rewind: Self-correction via backtracking in vision-and-language navi- gation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6741-6749 (2019)</p>
<p>R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L J Li, D A Shamma, M Bernstein, L Fei-Fei, arXiv:1602.07332Visual genome: Connecting language and vision using crowdsourced dense image annotations. arXiv preprintKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalan- tidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome: Con- necting language and vision using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332 (2016)</p>
<p>Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. G Li, N Duan, Y Fang, D Jiang, M Zhou, arXiv:1908.06066arXiv preprintLi, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.: Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. arXiv preprint arXiv:1908.06066 (2019)</p>
<p>L H Li, M Yatskar, D Yin, C J Hsieh, K W Chang, arXiv:1908.03557Visualbert: A simple and performant baseline for vision and language. arXiv preprintLi, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019)</p>
<p>X Li, C Li, Q Xia, Y Bisk, A Celikyilmaz, J Gao, N Smith, Y Choi, arXiv:1909.02244Robust navigation with language pretraining and stochastic sampling. arXiv preprintLi, X., Li, C., Xia, Q., Bisk, Y., Celikyilmaz, A., Gao, J., Smith, N., Choi, Y.: Ro- bust navigation with language pretraining and stochastic sampling. arXiv preprint arXiv:1909.02244 (2019)</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Advances in Neural Information Processing Systems. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin- guistic representations for vision-and-language tasks. In: Advances in Neural In- formation Processing Systems. pp. 13-23 (2019)</p>
<p>Hierarchical question-image co-attention for visual question answering. J Lu, J Yang, D Batra, D Parikh, Advances in neural information processing systems. Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image co-attention for visual question answering. In: Advances in neural information processing systems. pp. 289-297 (2016)</p>
<p>Selfmonitoring navigation agent via auxiliary progress estimation. C Y Ma, J Lu, Z Wu, G Alregib, Z Kira, R Socher, C Xiong, arXiv:1901.03035arXiv preprintMa, C.Y., Lu, J., Wu, Z., AlRegib, G., Kira, Z., Socher, R., Xiong, C.: Self- monitoring navigation agent via auxiliary progress estimation. arXiv preprint arXiv:1901.03035 (2019)</p>
<p>The regretful agent: Heuristicaided navigation through progress estimation. C Y Ma, Z Wu, G Alregib, C Xiong, Z Kira, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMa, C.Y., Wu, Z., AlRegib, G., Xiong, C., Kira, Z.: The regretful agent: Heuristic- aided navigation through progress estimation. In: Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition. pp. 6732-6740 (2019)</p>
<p>Faster R-CNN: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in Neural Information Processing Systems (NIPS). Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: Advances in Neural Information Pro- cessing Systems (NIPS) (2015)</p>
<p>Imagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International journal of computer vision. 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog- nition challenge. International journal of computer vision 115(3), 211-252 (2015)</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2556-2565 (2018)</p>
<p>K Simonyan, A Vedaldi, A Zisserman, arXiv:1312.6034Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprintSimonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional net- works: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013)</p>
<p>W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, J Dai, arXiv:1908.08530Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprintSu, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530 (2019)</p>
<p>Lxmert: Learning cross-modality encoder representations from transformers. H Tan, M Bansal, EMNLPTan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from transformers. In: EMNLP (2019)</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. H Tan, L Yu, M Bansal, arXiv:1904.04195arXiv preprintTan, H., Yu, L., Bansal, M.: Learning to navigate unseen environments: Back translation with environmental dropout. arXiv preprint arXiv:1904.04195 (2019)</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems. pp. 5998-6008 (2017)</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. X Wang, Q Huang, A Celikyilmaz, J Gao, D Shen, Y F Wang, W Y Wang, L Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.F., Wang, W.Y., Zhang, L.: Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6629-6638 (2019)</p>
<p>Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-andlanguage navigation. X Wang, W Xiong, H Wang, Yang Wang, W , Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Wang, X., Xiong, W., Wang, H., Yang Wang, W.: Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and- language navigation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 37-53 (2018)</p>
<p>Unified vision-language pre-training for image captioning and vqa. L Zhou, H Palangi, L Zhang, H Hu, J J Corso, J Gao, arXiv:1909.11059arXiv preprintZhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J.J., Gao, J.: Unified vision-language pre-training for image captioning and vqa. arXiv preprint arXiv:1909.11059 (2019)</p>
<p>Vision-language navigation with selfsupervised auxiliary reasoning tasks. F Zhu, Y Zhu, X Chang, X Liang, arXiv:1911.07883arXiv preprintZhu, F., Zhu, Y., Chang, X., Liang, X.: Vision-language navigation with self- supervised auxiliary reasoning tasks. arXiv preprint arXiv:1911.07883 (2019)</p>
<p>Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. Y Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun, A Torralba, S Fidler, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Fidler, S.: Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In: Proceedings of the IEEE international conference on computer vision. pp. 19-27 (2015)</p>            </div>
        </div>

    </div>
</body>
</html>
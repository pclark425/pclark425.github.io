<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-567 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-567</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-567</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-e2dba792360873aef125572812f3673b1a85d850</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e2dba792360873aef125572812f3673b1a85d850" target="_blank">Enriching Word Vectors with Subword Information</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A new approach based on the skipgram model, where each word is represented as a bag of character n-grams, with words being represented as the sum of these representations, which achieves state-of-the-art performance on word similarity and analogy tasks.</p>
                <p><strong>Paper Abstract:</strong> Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e567.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e567.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sisg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subword Information Skip-Gram</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the skip-gram word embedding model that represents each word as the sum of vector representations of its character n-grams, enabling parameter sharing across words and vector computation for out-of-vocabulary forms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Subword-augmented Skip-gram (character n-gram based word embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Modify the standard skip-gram with negative sampling so that the input vector for a word is the sum of vectors associated with its character n-grams (including special boundary symbols and the full word token). During training, scoring between a target and a context word is computed as the dot-product between the summed n-gram vector of the target and the context word vector; negative sampling and logistic loss are used as in skip-gram. Character n-grams of lengths 3 to 6 (by default) are used; an FNV-1a hashing function maps n-grams to a fixed-size table (K = 2e6) to bound memory. Training is performed with asynchronous stochastic gradient descent (Hogwild) and linear learning-rate decay.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / machine learning algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>distributional word representation / neural language modeling (word-level skip-gram)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>subword-aware word representation for morphologically rich languages and improved downstream language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Replaced the atomic word input vector by a sum of trainable character n-gram vectors; added special begin/end boundary symbols to distinguish prefixes/suffixes; included the whole word form as an n-gram; limited n-gram lengths (default 3-6); applied a hashing function (FNV-1a) to map n-grams into a fixed table of size K=2,000,000 to bound memory; retained skip-gram negative sampling objective and training schedule but with a larger parameter set (n-gram vectors) and slightly increased learning rate (0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - produced consistent empirical improvements across multiple evaluations: higher Spearman correlation on word similarity datasets across nine languages, better syntactic analogy accuracy (notably large gains), ability to compute vectors for OOV words (sisg vs sisg-), and lower test perplexity when used to initialize an LSTM language model. Quantitative examples: language-model perplexity reductions vs sg: Czech 339 -> 312 (~8%); Russian 237 -> 206 (~13%); training speed: ~105k words/sec/thread for sisg vs ~145k for skipgram baseline (approx. 1.5x slower).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Increased computational cost and memory footprint from maintaining n-gram vectors (mitigated by hashing); choosing n-gram length range involves a trade-off (semantic vs. syntactic performance), requiring empirical tuning; potential hash collisions from fixed-size hashing table; slight slowdown in training throughput (~1.5x slower in reported setting).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Morphological regularities in many languages make character-level sharing effective; simple additive composition (sum of n-gram vectors) is computationally cheap and compatible with skip-gram training; availability of large unlabeled corpora (Wikipedia) for pretraining; use of hashing permitted bounded memory while keeping many distinct n-grams.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires character-level tokenization with boundary markers, choice of n-gram range (paper uses 3-6), a hashing implementation (FNV-1a) and a sufficiently large hash table (K~2e6), asynchronous SGD infrastructure (Hogwild) for efficient parallel training, and typical skip-gram hyperparameters (negatives, window sizes, subsampling).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly generalizable within NLP: the method applied across nine languages and to downstream tasks (word similarity, analogies, language modeling). The approach is likely applicable to other sequence modeling tasks where sub-token structure matters (e.g., morphological analysis, machine translation, OOV handling). Some tuning (n-gram lengths, hash size) required per language/task.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skills (algorithmic adaptation, hashing, training regimen), plus some interpretive understanding of morphology</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriching Word Vectors with Subword Information', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e567.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e567.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FNV-1a hashing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fowler–Noll–Vo (FNV-1a) hashing function</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-cryptographic string hashing function used to map character n-gram strings into integers in a fixed range to bound memory for the n-gram embedding table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>FNV-1a string hashing for n-gram index mapping</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Apply the FNV-1a hashing algorithm to each extracted character n-gram to obtain an integer index in the range [1..K]; use that index to access and update a fixed-size table of n-gram embedding vectors instead of maintaining a full dictionary of distinct n-grams.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational technique / data structure optimization</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general computer science / hashing algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>natural language processing - subword embedding memory management</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Selected a specific hash (FNV-1a) and a table size K = 2,000,000 to balance collision rate and memory. Integrated hashing into the subword skip-gram pipeline so n-gram strings are never stored explicitly as dictionary keys.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - enabled bounded-memory storage of n-gram vectors and made the model practical for large corpora; no explicit collision-rate metrics reported in the paper but overall empirical results indicate effective performance despite hashing.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Potential for hash collisions which can mix representations of distinct n-grams; trade-off between K (memory) and collision rate not explored in depth in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simplicity and speed of FNV-1a hashing; large but finite memory budget available; embedding lookup pattern fits hashed indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>An implementation of FNV-1a and a design decision for hash-table size (paper uses K=2e6); acceptance of some collision risk in exchange for bounded memory.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to other NLP subword/vector tasks that need to store many string-identified parameters under memory constraints; applicable anywhere string features need fixed-size indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural knowledge / instrumental technical skill</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriching Word Vectors with Subword Information', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e567.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e567.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hogwild</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hogwild: lock-free asynchronous SGD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel asynchronous stochastic gradient descent approach where multiple threads update shared parameters without locks, enabling scalable training of large models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hogwild: A lock-free approach to parallelizing stochastic gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Hogwild asynchronous SGD for embedding training</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Run multiple threads in parallel that each sample training examples and perform SGD updates on shared parameter arrays without locking; rely on sparsity and statistical robustness to tolerate overwrites and achieve near-linear speedup.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / parallel optimization</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>parallel optimization / large-scale machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>training of subword-aware skip-gram embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied Hogwild directly to the embedding training code; no novel changes described beyond integrating it into the C++ implementation of the model.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - enabled practical parallel training of the model; paper reports throughput values (sisg processes ~105k words/sec/thread vs baseline ~145k for skipgram), indicating training was parallelized effectively (though sisg is slower due to extra n-gram processing, not Hogwild itself).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Asynchronous updates can introduce noise/overwrites, but the method worked well in practice for these dense embedding updates; the paper does not report specific convergence issues attributable to Hogwild.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Sparsity of individual example updates, robustness of SGD to asynchronous noise, availability of multi-core CPUs and a lock-free implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Multi-threaded environment, shared-memory architecture, and an implementation of lock-free parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly generalizable to other large-scale embedding and neural-network training tasks in NLP and beyond where shared-memory parallelism is available.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps / instrumental technical skill</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriching Word Vectors with Subword Information', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e567.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e567.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM init with embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Initializing language model lookup table with pre-trained word vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use pre-trained word embeddings (both standard skip-gram and the subword-enhanced sisg) to initialize the input lookup table of an LSTM language model, improving perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pre-trained embedding initialization of recurrent language model</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Train word vectors independently (either plain skip-gram or sisg with character n-grams) on the language-modeling training data and then use these vectors to initialize the input embedding (lookup) matrix of a recurrent neural network language model (650 LSTM units, dropout, Adagrad). Continue training the language model (with other weights randomly initialized) to optimize perplexity on the task.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / model initialization strategy</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>unsupervised word embedding learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>supervised / sequence modeling language modeling (LSTM networks)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (initialization and fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Trained word vectors on the LM training set with the sisg or sg procedure, then used resulting vectors to populate the LSTM model input embedding matrix prior to LM training; no architectural changes to the LSTM were required.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - initializing the LSTM with pre-trained vectors improved test perplexity compared to random initialization, and sisg-initialized LSTM outperformed sg-initialized LSTM. Reported test perplexities: Cs LSTM baseline 366 -> sg 339 -> sisg 312; DE 222 -> 216 -> 206; Es 157 -> 150 -> 145; Fr 173 -> 162 -> 159; Ru 262 -> 237 -> 206. Percent improvements vs sg: Czech ~8%, Russian ~13%, smaller gains for Romance languages.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires training data to produce useful pre-trained embeddings; vocabulary mismatch and OOV handling require consistent preprocessing; gains depend on morphological richness of language.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Pre-trained embeddings capture lexical/subword information beneficial for initial embedding parameters; morphological advantages of sisg carry over into LM task; same-tokenization and preprocessing ensures compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Consistent preprocessing and vocabulary between embedding training and LM; sufficient data to train embeddings (paper used ~1M tokens for LM datasets but retrained embeddings on that set), and LSTM training infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Method is broadly applicable across sequence modeling tasks where input lookup tables can be initialized (other RNNs, transformer input embeddings); benefits are larger for morphologically rich languages but present across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skill</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriching Word Vectors with Subword Information', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e567.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e567.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subword MT (Sennrich)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural machine translation of rare words with subword units</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of subword segmentation/units in neural machine translation to handle rare and out-of-vocabulary words by breaking words into smaller units.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation of rare words with subword units</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Use of subword units in machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Segment words into subword units (e.g., BPE or other algorithms) and use subword tokens as the modeling units in neural MT models to allow open-vocabulary translation and better handling of rare forms.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / sequence tokenization technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine translation / neural sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>handling rare words in translation and related NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification (as described in cited work; here only referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Not applied in this paper; cited as related work showing subword unit utility in MT.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported successful in the cited work (Sennrich et al. 2016) — cited here as prior evidence motivating subword approaches; no new outcomes measured in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Mentioned indirectly: segmentation choices and balance between vocabulary size and sequence length are relevant design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared motivation: subword decomposition addresses rare/OOV token issues.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Subword segmentation algorithm and integration into MT pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Cited as evidence that subword methods transfer across sequence modeling tasks; paper treats it as related work motivating their subword skip-gram.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>mention of explicit methodological practice</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriching Word Vectors with Subword Information', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e567.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e567.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Char-level models (RNN/CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level recurrent and convolutional neural networks applied to NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that operate on raw character sequences (RNNs or CNNs) to learn representations for tasks such as language modeling, tagging, parsing, sentiment and classification, avoiding fixed word segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Character-level neural models for NLP</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Train neural models (e.g., RNNs with LSTM/GRU or CNNs) over character sequences to produce word or sentence representations; apply to tasks like language modeling, POS tagging, parsing, sentiment analysis and text classification to better handle morphology and rare words.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / modeling approach</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>neural sequence modeling / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>various NLP tasks (language modeling, tagging, parsing, classification)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>mention of cross-application within NLP (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Not applied in this paper; discussed as related approaches that operate at character level rather than augmenting word-level skip-gram.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Cited works report success on respective tasks (e.g., Kim et al. 2016, Mikolov et al. 2012); no new outcomes here.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Mentioned in related work: character-level models are sometimes more computationally intensive and require architectural design decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Intrinsic ability to model morphology and capture subword regularities.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Design of neural architectures and sufficient training data for end-to-end character-level learning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>These approaches are general across NLP tasks; here they are cited to contextualize the sisg approach which keeps word segmentation but uses character subwords.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>mention of methodological family (explicit procedural/theoretical)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriching Word Vectors with Subword Information', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e567.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e567.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schütze 1993 n-grams</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character n-gram representations via SVD (Hinrich Schütze 1993)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early method to learn representations of character four-grams via singular value decomposition and derive word vectors by summing n-gram representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Word space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Character n-gram representation via matrix factorization (SVD)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Construct count-based vectors for character n-grams and apply singular value decomposition (or similar factorization) to obtain dense representations; derive word representations by summing the n-gram vectors corresponding to the word.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data-analytic technique / dimensionality reduction</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>distributional semantics / matrix factorization</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>subword-aware word representations (motivation for current work)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>analogical transfer with significant changes (paper cites this as prior related idea and diverges by using neural skip-gram objective rather than SVD)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>The current paper departs from SVD/count-based approach by using an online skip-gram objective with negative sampling and learnable n-gram embeddings; the basic idea of composing word vectors from n-grams is shared.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Historical success in motivating subword approaches; in this paper the authors claim similarity in spirit but use a different learning objective and report better scalability and applicability to large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>SVD-based methods scale less well to very large corpora and large n-gram vocabularies; this motivated the neural, online approach.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Conceptual similarity (words as sum of subword components) made transfer of the idea natural; advances in online neural training and hashing allowed scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Transition from batch matrix factorization to online negative-sampling skip-gram requires different computational machinery and design choices (hashing, SGD, negative sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>The conceptual transfer (compose from n-grams) is general; the specific training machinery differs.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principle and explicit procedural contrast</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriching Word Vectors with Subword Information', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hogwild: A lock-free approach to parallelizing stochastic gradient descent. <em>(Rating: 2)</em></li>
                <li>Neural machine translation of rare words with subword units <em>(Rating: 2)</em></li>
                <li>Achieving open vocabulary neural machine translation with hybrid word-character models <em>(Rating: 2)</em></li>
                <li>Word space <em>(Rating: 2)</em></li>
                <li>Character-aware neural language models <em>(Rating: 2)</em></li>
                <li>Compositional morphology for word representations and language modelling <em>(Rating: 2)</em></li>
                <li>Unsupervised morphology induction using word embeddings <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-567",
    "paper_id": "paper-e2dba792360873aef125572812f3673b1a85d850",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "sisg",
            "name_full": "Subword Information Skip-Gram",
            "brief_description": "An extension of the skip-gram word embedding model that represents each word as the sum of vector representations of its character n-grams, enabling parameter sharing across words and vector computation for out-of-vocabulary forms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Subword-augmented Skip-gram (character n-gram based word embeddings)",
            "procedure_description": "Modify the standard skip-gram with negative sampling so that the input vector for a word is the sum of vectors associated with its character n-grams (including special boundary symbols and the full word token). During training, scoring between a target and a context word is computed as the dot-product between the summed n-gram vector of the target and the context word vector; negative sampling and logistic loss are used as in skip-gram. Character n-grams of lengths 3 to 6 (by default) are used; an FNV-1a hashing function maps n-grams to a fixed-size table (K = 2e6) to bound memory. Training is performed with asynchronous stochastic gradient descent (Hogwild) and linear learning-rate decay.",
            "procedure_type": "computational method / machine learning algorithm",
            "source_domain": "distributional word representation / neural language modeling (word-level skip-gram)",
            "target_domain": "subword-aware word representation for morphologically rich languages and improved downstream language modeling",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Replaced the atomic word input vector by a sum of trainable character n-gram vectors; added special begin/end boundary symbols to distinguish prefixes/suffixes; included the whole word form as an n-gram; limited n-gram lengths (default 3-6); applied a hashing function (FNV-1a) to map n-grams into a fixed table of size K=2,000,000 to bound memory; retained skip-gram negative sampling objective and training schedule but with a larger parameter set (n-gram vectors) and slightly increased learning rate (0.05).",
            "transfer_success": "successful - produced consistent empirical improvements across multiple evaluations: higher Spearman correlation on word similarity datasets across nine languages, better syntactic analogy accuracy (notably large gains), ability to compute vectors for OOV words (sisg vs sisg-), and lower test perplexity when used to initialize an LSTM language model. Quantitative examples: language-model perplexity reductions vs sg: Czech 339 -&gt; 312 (~8%); Russian 237 -&gt; 206 (~13%); training speed: ~105k words/sec/thread for sisg vs ~145k for skipgram baseline (approx. 1.5x slower).",
            "barriers_encountered": "Increased computational cost and memory footprint from maintaining n-gram vectors (mitigated by hashing); choosing n-gram length range involves a trade-off (semantic vs. syntactic performance), requiring empirical tuning; potential hash collisions from fixed-size hashing table; slight slowdown in training throughput (~1.5x slower in reported setting).",
            "facilitating_factors": "Morphological regularities in many languages make character-level sharing effective; simple additive composition (sum of n-gram vectors) is computationally cheap and compatible with skip-gram training; availability of large unlabeled corpora (Wikipedia) for pretraining; use of hashing permitted bounded memory while keeping many distinct n-grams.",
            "contextual_requirements": "Requires character-level tokenization with boundary markers, choice of n-gram range (paper uses 3-6), a hashing implementation (FNV-1a) and a sufficiently large hash table (K~2e6), asynchronous SGD infrastructure (Hogwild) for efficient parallel training, and typical skip-gram hyperparameters (negatives, window sizes, subsampling).",
            "generalizability": "Highly generalizable within NLP: the method applied across nine languages and to downstream tasks (word similarity, analogies, language modeling). The approach is likely applicable to other sequence modeling tasks where sub-token structure matters (e.g., morphological analysis, machine translation, OOV handling). Some tuning (n-gram lengths, hash size) required per language/task.",
            "knowledge_type": "explicit procedural steps and instrumental/technical skills (algorithmic adaptation, hashing, training regimen), plus some interpretive understanding of morphology",
            "uuid": "e567.0",
            "source_info": {
                "paper_title": "Enriching Word Vectors with Subword Information",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "FNV-1a hashing",
            "name_full": "Fowler–Noll–Vo (FNV-1a) hashing function",
            "brief_description": "A non-cryptographic string hashing function used to map character n-gram strings into integers in a fixed range to bound memory for the n-gram embedding table.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "FNV-1a string hashing for n-gram index mapping",
            "procedure_description": "Apply the FNV-1a hashing algorithm to each extracted character n-gram to obtain an integer index in the range [1..K]; use that index to access and update a fixed-size table of n-gram embedding vectors instead of maintaining a full dictionary of distinct n-grams.",
            "procedure_type": "computational technique / data structure optimization",
            "source_domain": "general computer science / hashing algorithms",
            "target_domain": "natural language processing - subword embedding memory management",
            "transfer_type": "direct application without modification",
            "modifications_made": "Selected a specific hash (FNV-1a) and a table size K = 2,000,000 to balance collision rate and memory. Integrated hashing into the subword skip-gram pipeline so n-gram strings are never stored explicitly as dictionary keys.",
            "transfer_success": "successful - enabled bounded-memory storage of n-gram vectors and made the model practical for large corpora; no explicit collision-rate metrics reported in the paper but overall empirical results indicate effective performance despite hashing.",
            "barriers_encountered": "Potential for hash collisions which can mix representations of distinct n-grams; trade-off between K (memory) and collision rate not explored in depth in the paper.",
            "facilitating_factors": "Simplicity and speed of FNV-1a hashing; large but finite memory budget available; embedding lookup pattern fits hashed indexing.",
            "contextual_requirements": "An implementation of FNV-1a and a design decision for hash-table size (paper uses K=2e6); acceptance of some collision risk in exchange for bounded memory.",
            "generalizability": "Generalizable to other NLP subword/vector tasks that need to store many string-identified parameters under memory constraints; applicable anywhere string features need fixed-size indexing.",
            "knowledge_type": "explicit procedural knowledge / instrumental technical skill",
            "uuid": "e567.1",
            "source_info": {
                "paper_title": "Enriching Word Vectors with Subword Information",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "Hogwild",
            "name_full": "Hogwild: lock-free asynchronous SGD",
            "brief_description": "A parallel asynchronous stochastic gradient descent approach where multiple threads update shared parameters without locks, enabling scalable training of large models.",
            "citation_title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent.",
            "mention_or_use": "use",
            "procedure_name": "Hogwild asynchronous SGD for embedding training",
            "procedure_description": "Run multiple threads in parallel that each sample training examples and perform SGD updates on shared parameter arrays without locking; rely on sparsity and statistical robustness to tolerate overwrites and achieve near-linear speedup.",
            "procedure_type": "computational method / parallel optimization",
            "source_domain": "parallel optimization / large-scale machine learning",
            "target_domain": "training of subword-aware skip-gram embeddings",
            "transfer_type": "direct application without modification",
            "modifications_made": "Applied Hogwild directly to the embedding training code; no novel changes described beyond integrating it into the C++ implementation of the model.",
            "transfer_success": "successful - enabled practical parallel training of the model; paper reports throughput values (sisg processes ~105k words/sec/thread vs baseline ~145k for skipgram), indicating training was parallelized effectively (though sisg is slower due to extra n-gram processing, not Hogwild itself).",
            "barriers_encountered": "Asynchronous updates can introduce noise/overwrites, but the method worked well in practice for these dense embedding updates; the paper does not report specific convergence issues attributable to Hogwild.",
            "facilitating_factors": "Sparsity of individual example updates, robustness of SGD to asynchronous noise, availability of multi-core CPUs and a lock-free implementation.",
            "contextual_requirements": "Multi-threaded environment, shared-memory architecture, and an implementation of lock-free parameter updates.",
            "generalizability": "Highly generalizable to other large-scale embedding and neural-network training tasks in NLP and beyond where shared-memory parallelism is available.",
            "knowledge_type": "explicit procedural steps / instrumental technical skill",
            "uuid": "e567.2",
            "source_info": {
                "paper_title": "Enriching Word Vectors with Subword Information",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "LM init with embeddings",
            "name_full": "Initializing language model lookup table with pre-trained word vectors",
            "brief_description": "Use pre-trained word embeddings (both standard skip-gram and the subword-enhanced sisg) to initialize the input lookup table of an LSTM language model, improving perplexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Pre-trained embedding initialization of recurrent language model",
            "procedure_description": "Train word vectors independently (either plain skip-gram or sisg with character n-grams) on the language-modeling training data and then use these vectors to initialize the input embedding (lookup) matrix of a recurrent neural network language model (650 LSTM units, dropout, Adagrad). Continue training the language model (with other weights randomly initialized) to optimize perplexity on the task.",
            "procedure_type": "computational method / model initialization strategy",
            "source_domain": "unsupervised word embedding learning",
            "target_domain": "supervised / sequence modeling language modeling (LSTM networks)",
            "transfer_type": "adapted/modified for new context (initialization and fine-tuning)",
            "modifications_made": "Trained word vectors on the LM training set with the sisg or sg procedure, then used resulting vectors to populate the LSTM model input embedding matrix prior to LM training; no architectural changes to the LSTM were required.",
            "transfer_success": "successful - initializing the LSTM with pre-trained vectors improved test perplexity compared to random initialization, and sisg-initialized LSTM outperformed sg-initialized LSTM. Reported test perplexities: Cs LSTM baseline 366 -&gt; sg 339 -&gt; sisg 312; DE 222 -&gt; 216 -&gt; 206; Es 157 -&gt; 150 -&gt; 145; Fr 173 -&gt; 162 -&gt; 159; Ru 262 -&gt; 237 -&gt; 206. Percent improvements vs sg: Czech ~8%, Russian ~13%, smaller gains for Romance languages.",
            "barriers_encountered": "Requires training data to produce useful pre-trained embeddings; vocabulary mismatch and OOV handling require consistent preprocessing; gains depend on morphological richness of language.",
            "facilitating_factors": "Pre-trained embeddings capture lexical/subword information beneficial for initial embedding parameters; morphological advantages of sisg carry over into LM task; same-tokenization and preprocessing ensures compatibility.",
            "contextual_requirements": "Consistent preprocessing and vocabulary between embedding training and LM; sufficient data to train embeddings (paper used ~1M tokens for LM datasets but retrained embeddings on that set), and LSTM training infrastructure.",
            "generalizability": "Method is broadly applicable across sequence modeling tasks where input lookup tables can be initialized (other RNNs, transformer input embeddings); benefits are larger for morphologically rich languages but present across languages.",
            "knowledge_type": "explicit procedural steps and instrumental/technical skill",
            "uuid": "e567.3",
            "source_info": {
                "paper_title": "Enriching Word Vectors with Subword Information",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "Subword MT (Sennrich)",
            "name_full": "Neural machine translation of rare words with subword units",
            "brief_description": "Application of subword segmentation/units in neural machine translation to handle rare and out-of-vocabulary words by breaking words into smaller units.",
            "citation_title": "Neural machine translation of rare words with subword units",
            "mention_or_use": "mention",
            "procedure_name": "Use of subword units in machine translation",
            "procedure_description": "Segment words into subword units (e.g., BPE or other algorithms) and use subword tokens as the modeling units in neural MT models to allow open-vocabulary translation and better handling of rare forms.",
            "procedure_type": "computational method / sequence tokenization technique",
            "source_domain": "machine translation / neural sequence modeling",
            "target_domain": "handling rare words in translation and related NLP tasks",
            "transfer_type": "direct application without modification (as described in cited work; here only referenced)",
            "modifications_made": "Not applied in this paper; cited as related work showing subword unit utility in MT.",
            "transfer_success": "reported successful in the cited work (Sennrich et al. 2016) — cited here as prior evidence motivating subword approaches; no new outcomes measured in this paper.",
            "barriers_encountered": "Mentioned indirectly: segmentation choices and balance between vocabulary size and sequence length are relevant design choices.",
            "facilitating_factors": "Shared motivation: subword decomposition addresses rare/OOV token issues.",
            "contextual_requirements": "Subword segmentation algorithm and integration into MT pipeline.",
            "generalizability": "Cited as evidence that subword methods transfer across sequence modeling tasks; paper treats it as related work motivating their subword skip-gram.",
            "knowledge_type": "mention of explicit methodological practice",
            "uuid": "e567.4",
            "source_info": {
                "paper_title": "Enriching Word Vectors with Subword Information",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "Char-level models (RNN/CNN)",
            "name_full": "Character-level recurrent and convolutional neural networks applied to NLP tasks",
            "brief_description": "Models that operate on raw character sequences (RNNs or CNNs) to learn representations for tasks such as language modeling, tagging, parsing, sentiment and classification, avoiding fixed word segmentation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "procedure_name": "Character-level neural models for NLP",
            "procedure_description": "Train neural models (e.g., RNNs with LSTM/GRU or CNNs) over character sequences to produce word or sentence representations; apply to tasks like language modeling, POS tagging, parsing, sentiment analysis and text classification to better handle morphology and rare words.",
            "procedure_type": "computational method / modeling approach",
            "source_domain": "neural sequence modeling / deep learning",
            "target_domain": "various NLP tasks (language modeling, tagging, parsing, classification)",
            "transfer_type": "mention of cross-application within NLP (related work)",
            "modifications_made": "Not applied in this paper; discussed as related approaches that operate at character level rather than augmenting word-level skip-gram.",
            "transfer_success": "Cited works report success on respective tasks (e.g., Kim et al. 2016, Mikolov et al. 2012); no new outcomes here.",
            "barriers_encountered": "Mentioned in related work: character-level models are sometimes more computationally intensive and require architectural design decisions.",
            "facilitating_factors": "Intrinsic ability to model morphology and capture subword regularities.",
            "contextual_requirements": "Design of neural architectures and sufficient training data for end-to-end character-level learning.",
            "generalizability": "These approaches are general across NLP tasks; here they are cited to contextualize the sisg approach which keeps word segmentation but uses character subwords.",
            "knowledge_type": "mention of methodological family (explicit procedural/theoretical)",
            "uuid": "e567.5",
            "source_info": {
                "paper_title": "Enriching Word Vectors with Subword Information",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "Schütze 1993 n-grams",
            "name_full": "Character n-gram representations via SVD (Hinrich Schütze 1993)",
            "brief_description": "Early method to learn representations of character four-grams via singular value decomposition and derive word vectors by summing n-gram representations.",
            "citation_title": "Word space",
            "mention_or_use": "mention",
            "procedure_name": "Character n-gram representation via matrix factorization (SVD)",
            "procedure_description": "Construct count-based vectors for character n-grams and apply singular value decomposition (or similar factorization) to obtain dense representations; derive word representations by summing the n-gram vectors corresponding to the word.",
            "procedure_type": "data-analytic technique / dimensionality reduction",
            "source_domain": "distributional semantics / matrix factorization",
            "target_domain": "subword-aware word representations (motivation for current work)",
            "transfer_type": "analogical transfer with significant changes (paper cites this as prior related idea and diverges by using neural skip-gram objective rather than SVD)",
            "modifications_made": "The current paper departs from SVD/count-based approach by using an online skip-gram objective with negative sampling and learnable n-gram embeddings; the basic idea of composing word vectors from n-grams is shared.",
            "transfer_success": "Historical success in motivating subword approaches; in this paper the authors claim similarity in spirit but use a different learning objective and report better scalability and applicability to large corpora.",
            "barriers_encountered": "SVD-based methods scale less well to very large corpora and large n-gram vocabularies; this motivated the neural, online approach.",
            "facilitating_factors": "Conceptual similarity (words as sum of subword components) made transfer of the idea natural; advances in online neural training and hashing allowed scaling.",
            "contextual_requirements": "Transition from batch matrix factorization to online negative-sampling skip-gram requires different computational machinery and design choices (hashing, SGD, negative sampling).",
            "generalizability": "The conceptual transfer (compose from n-grams) is general; the specific training machinery differs.",
            "knowledge_type": "theoretical principle and explicit procedural contrast",
            "uuid": "e567.6",
            "source_info": {
                "paper_title": "Enriching Word Vectors with Subword Information",
                "publication_date_yy_mm": "2016-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent.",
            "rating": 2
        },
        {
            "paper_title": "Neural machine translation of rare words with subword units",
            "rating": 2
        },
        {
            "paper_title": "Achieving open vocabulary neural machine translation with hybrid word-character models",
            "rating": 2
        },
        {
            "paper_title": "Word space",
            "rating": 2
        },
        {
            "paper_title": "Character-aware neural language models",
            "rating": 2
        },
        {
            "paper_title": "Compositional morphology for word representations and language modelling",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised morphology induction using word embeddings",
            "rating": 2
        }
    ],
    "cost": 0.01676575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Enriching Word Vectors with Subword Information</h1>
<p>Piotr Bojanowski ${ }^{\star}$ and Edouard Grave ${ }^{\star}$ and Armand Joulin and Tomas Mikolov<br>Facebook AI Research<br>{bojanowski,egrave,ajoulin,tmikolov}@fb.com</p>
<h4>Abstract</h4>
<p>Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.</p>
<h2>1 Introduction</h2>
<p>Learning continuous representations of words has a long history in natural language processing (Rumelhart et al., 1988). These representations are typically derived from large unlabeled corpora using co-occurrence statistics (Deerwester et al., 1990; Schütze, 1992; Lund and Burgess, 1996). A large body of work, known as distributional semantics, has studied the properties of these methods (Turney</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2010; Baroni and Lenci, 2010). In the neural network community, Collobert and Weston (2008) proposed to learn word embeddings using a feedforward neural network, by predicting a word based on the two words on the left and two words on the right. More recently, Mikolov et al. (2013b) proposed simple log-bilinear models to learn continuous representations of words on very large corpora efficiently.</p>
<p>Most of these techniques represent each word of the vocabulary by a distinct vector, without parameter sharing. In particular, they ignore the internal structure of words, which is an important limitation for morphologically rich languages, such as Turkish or Finnish. For example, in French or Spanish, most verbs have more than forty different inflected forms, while the Finnish language has fifteen cases for nouns. These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difficult to learn good word representations. Because many word formations follow rules, it is possible to improve vector representations for morphologically rich languages by using character level information.</p>
<p>In this paper, we propose to learn representations for character $n$-grams, and to represent words as the sum of the $n$-gram vectors. Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information. We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach.</p>
<h2>2 Related work</h2>
<p>Morphological word representations. In recent years, many methods have been proposed to incorporate morphological information into word representations. To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features. These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turkish (Sak et al., 2010). Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014). These different approaches rely on a morphological decomposition of words, while ours does not. Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations. Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules. Word representations trained on morphologically annotated data were introduced by Cotterell and Schütze (2015). Closest to our approach, Schütze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing the four-grams representations. Very recently, Wieting et al. (2016) also proposed to represent words using character $n$-gram count vectors. However, the objective function used to learn these representations is based on paraphrase pairs, while our model can be trained on any text corpus.</p>
<p>Character level features for NLP. Another area of research closely related to our work are characterlevel models for natural language processing. These models discard the segmentation into words and aim at learning language representations directly from characters. A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupała, 2014), part-of-speech tag-
ging (Ling et al., 2015) and parsing (Ballesteros et al., 2015). Another family of models are convolutional neural networks trained on characters, which were applied to part-of-speech tagging (dos Santos and Zadrozny, 2014), sentiment analysis (dos Santos and Gatti, 2014), text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016). Sperr et al. (2013) introduced a language model based on restricted Boltzmann machines, in which words are encoded as a set of character $n$ grams. Finally, recent works in machine translation have proposed using subword units to obtain representations of rare words (Sennrich et al., 2016; Luong and Manning, 2016).</p>
<h2>3 Model</h2>
<p>In this section, we propose our model to learn word representations while taking into account morphology. We model morphology by considering subword units, and representing words by a sum of its character $n$-grams. We will begin by presenting the general framework that we use to train word vectors, then present our subword model and eventually describe how we handle the dictionary of character $n$-grams.</p>
<h3>3.1 General model</h3>
<p>We start by briefly reviewing the continuous skipgram model introduced by Mikolov et al. (2013b), from which our model is derived. Given a word vocabulary of size $W$, where a word is identified by its index $w \in{1, \ldots, W}$, the goal is to learn a vectorial representation for each word $w$. Inspired by the distributional hypothesis (Harris, 1954), word representations are trained to predict well words that appear in its context. More formally, given a large training corpus represented as a sequence of words $w_{1}, \ldots, w_{T}$, the objective of the skipgram model is to maximize the following log-likelihood:</p>
<p>$$
\sum_{t=1}^{T} \sum_{c \in \mathcal{C}<em c="c">{t}} \log p\left(w</em>\right)
$$} \mid w_{t</p>
<p>where the context $\mathcal{C}<em t="t">{t}$ is the set of indices of words surrounding word $w</em>$.}$. The probability of observing a context word $w_{c}$ given $w_{t}$ will be parameterized using the aforementioned word vectors. For now, let us consider that we are given a scoring function $s$ which maps pairs of (word, context) to scores in $\mathbb{R</p>
<p>One possible choice to define the probability of a context word is the softmax:</p>
<p>$$
p\left(w_{c} \mid w_{t}\right)=\frac{e^{s\left(w_{t}, w_{c}\right)}}{\sum_{j=1}^{W} e^{s\left(w_{t}, j\right)}}
$$</p>
<p>However, such a model is not adapted to our case as it implies that, given a word $w_{t}$, we only predict one context word $w_{c}$.</p>
<p>The problem of predicting context words can instead be framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. For the word at position $t$ we consider all context words as positive examples and sample negatives at random from the dictionary. For a chosen context position $c$, using the binary logistic loss, we obtain the following negative log-likelihood:</p>
<p>$$
\log \left(1+e^{-s\left(w_{t}, w_{c}\right)}\right)+\sum_{n \in \mathcal{N}<em t="t">{t, c}} \log \left(1+e^{s\left(w</em>\right)
$$}, n\right)</p>
<p>where $\mathcal{N}_{t, c}$ is a set of negative examples sampled from the vocabulary. By denoting the logistic loss function $\ell: x \mapsto \log \left(1+e^{-x}\right)$, we can re-write the objective as:</p>
<p>$$
\sum_{t=1}^{T}\left[\sum_{c \in \mathcal{C}<em t="t">{t}} \ell\left(s\left(w</em>}, w_{c}\right)\right)+\sum_{n \in \mathcal{N<em t="t">{t, c}} \ell\left(-s\left(w</em>, n\right)\right)\right]
$$</p>
<p>A natural parameterization for the scoring function $s$ between a word $w_{t}$ and a context word $w_{c}$ is to use word vectors. Let us define for each word $w$ in the vocabulary two vectors $u_{w}$ and $v_{w}$ in $\mathbb{R}^{d}$. These two vectors are sometimes referred to as input and output vectors in the literature. In particular, we have vectors $\mathbf{u}<em t="t">{w</em>}}$ and $\mathbf{v<em c="c">{w</em>}}$, corresponding, respectively, to words $w_{t}$ and $w_{c}$. Then the score can be computed as the scalar product between word and context vectors as $s\left(w_{t}, w_{c}\right)=\mathbf{u<em t="t">{w</em>}}^{\top} \mathbf{v<em c="c">{w</em>$. The model described in this section is the skipgram model with negative sampling, introduced by Mikolov et al. (2013b).}</p>
<h3>3.2 Subword model</h3>
<p>By using a distinct vector representation for each word, the skipgram model ignores the internal structure of words. In this section, we propose a different scoring function $s$, in order to take into account this information.</p>
<p>Each word $w$ is represented as a bag of character $n$-gram. We add special boundary symbols &lt; and &gt; at the beginning and end of words, allowing to distinguish prefixes and suffixes from other character sequences. We also include the word $w$ itself in the set of its $n$-grams, to learn a representation for each word (in addition to character $n$-grams). Taking the word where and $n=3$ as an example, it will be represented by the character $n$-grams:</p>
<div class="codehilite"><pre><span></span><code>&lt;wh, whe, her, ere, re&gt;
</code></pre></div>

<p>and the special sequence</p>
<p>$$
&lt;\text { where }&gt;
$$</p>
<p>Note that the sequence <her>, corresponding to the word her is different from the tri-gram her from the word where. In practice, we extract all the $n$-grams for $n$ greater or equal to 3 and smaller or equal to 6 . This is a very simple approach, and different sets of $n$-grams could be considered, for example taking all prefixes and suffixes.</p>
<p>Suppose that you are given a dictionary of $n$ grams of size $G$. Given a word $w$, let us denote by $\mathcal{G}<em g="g">{w} \subset{1, \ldots, G}$ the set of $n$-grams appearing in $w$. We associate a vector representation $\mathbf{z}</em>$ to each $n$-gram $g$. We represent a word by the sum of the vector representations of its $n$-grams. We thus obtain the scoring function:</p>
<p>$$
s(w, c)=\sum_{g \in \mathcal{G}<em g="g">{w}} \mathbf{z}</em>
$$}^{\top} \mathbf{v}_{c</p>
<p>This simple model allows sharing the representations across words, thus allowing to learn reliable representation for rare words.</p>
<p>In order to bound the memory requirements of our model, we use a hashing function that maps $n$-grams to integers in 1 to $K$. We hash character sequences using the Fowler-Noll-Vo hashing function (specifically the FNV-1a variant). ${ }^{1}$ We set $K=2.10^{6}$ below. Ultimately, a word is represented by its index in the word dictionary and the set of hashed $n$-grams it contains.</p>
<h2>4 Experimental setup</h2>
<h3>4.1 Baseline</h3>
<p>In most experiments (except in Sec. 5.3), we compare our model to the C implementation</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>of the skipgram and cbow models from the word2vec ${ }^{2}$ package.</p>
<h3>4.2 Optimization</h3>
<p>We solve our optimization problem by performing stochastic gradient descent on the negative log likelihood presented before. As in the baseline skipgram model, we use a linear decay of the step size. Given a training set containing $T$ words and a number of passes over the data equal to $P$, the step size at time $t$ is equal to $\gamma_{0}\left(1-\frac{t}{T P}\right)$, where $\gamma_{0}$ is a fixed parameter. We carry out the optimization in parallel, by resorting to Hogwild (Recht et al., 2011). All threads share parameters and update vectors in an asynchronous manner.</p>
<h3>4.3 Implementation details</h3>
<p>For both our model and the baseline experiments, we use the following parameters: the word vectors have dimension 300. For each positive example, we sample 5 negatives at random, with probability proportional to the square root of the uni-gram frequency. We use a context window of size $c$, and uniformly sample the size $c$ between 1 and 5 . In order to subsample the most frequent words, we use a rejection threshold of $10^{-4}$ (for more details, see (Mikolov et al., 2013b)). When building the word dictionary, we keep the words that appear at least 5 times in the training set. The step size $\gamma_{0}$ is set to 0.025 for the skipgram baseline and to 0.05 for both our model and the cbow baseline. These are the default values in the word2vec package and work well for our model too.</p>
<p>Using this setting on English data, our model with character $n$-grams is approximately $1.5 \times$ slower to train than the skipgram baseline. Indeed, we process 105 k words/second/thread versus 145 k words/second/thread for the baseline. Our model is implemented in $\mathrm{C}++$, and is publicly available. ${ }^{3}$</p>
<h3>4.4 Datasets</h3>
<p>Except for the comparison to previous work (Sec. 5.3), we train our models on Wikipedia data. ${ }^{4}$ We downloaded Wikipedia dumps in nine languages: Arabic, Czech, German, English,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Spanish, French, Italian, Romanian and Russian. We normalize the raw Wikipedia data using Matt Mahoney's pre-processing perl script. ${ }^{5}$ All the datasets are shuffled, and we train our models by doing five passes over them.</p>
<h2>5 Results</h2>
<p>We evaluate our model in five experiments: an evaluation of word similarity and word analogies, a comparison to state-of-the-art methods, an analysis of the effect of the size of training data and of the size of character $n$-grams that we consider. We will describe these experiments in detail in the following sections.</p>
<h3>5.1 Human similarity judgement</h3>
<p>We first evaluate the quality of our representations on the task of word similarity / relatedness. We do so by computing Spearman's rank correlation coefficient (Spearman, 1904) between human judgement and the cosine similarity between the vector representations. For German, we compare the different models on three datasets: Gur65, Gur350 and ZG222 (Gurevych, 2005; Zesch and Gurevych, 2006). For English, we use the WS353 dataset introduced by Finkelstein et al. (2001) and the rare word dataset (RW), introduced by Luong et al. (2013). We evaluate the French word vectors on the translated dataset RG65 (Joubarne and Inkpen, 2011). Spanish, Arabic and Romanian word vectors are evaluated using the datasets described in (Hassan and Mihalcea, 2009). Russian word vectors are evaluated using the HJ dataset introduced by Panchenko et al. (2016).</p>
<p>We report results for our method and baselines for all datasets in Table 1. Some words from these datasets do not appear in our training data, and thus, we cannot obtain word representation for these words using the cbow and skipgram baselines. In order to provide comparable results, we propose by default to use null vectors for these words. Since our model exploits subword information, we can also compute valid representations for out-of-vocabulary words. We do so by taking the sum of its $n$-gram vectors. When OOV words are represented using</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">sg</th>
<th style="text-align: center;">cbow</th>
<th style="text-align: center;">sisg-</th>
<th style="text-align: center;">sisg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">WS353</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">$\mathbf{5 5}$</td>
</tr>
<tr>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">GUR350</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">$\mathbf{7 0}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GUR65</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">$\mathbf{8 1}$</td>
<td style="text-align: center;">$\mathbf{8 1}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZG222</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">$\mathbf{4 4}$</td>
</tr>
<tr>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">RW</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">$\mathbf{4 7}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WS353</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">$\mathbf{7 3}$</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">ES</td>
<td style="text-align: center;">WS353</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">$\mathbf{5 9}$</td>
</tr>
<tr>
<td style="text-align: center;">FR</td>
<td style="text-align: center;">RG65</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">$\mathbf{7 5}$</td>
<td style="text-align: center;">$\mathbf{7 5}$</td>
</tr>
<tr>
<td style="text-align: center;">RO</td>
<td style="text-align: center;">WS353</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">$\mathbf{5 4}$</td>
</tr>
<tr>
<td style="text-align: center;">RU</td>
<td style="text-align: center;">HJ</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$\mathbf{6 6}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Correlation between human judgement and similarity scores on word similarity datasets. We train both our model and the word2vec baseline on normalized Wikipedia dumps. Evaluation datasets contain words that are not part of the training set, so we represent them using null vectors (sisg-). With our model, we also compute vectors for unseen words by summing the $n$-gram vectors (sisg).
null vectors we refer to our method as sisg- and sisg otherwise (Subword Information Skip Gram).</p>
<p>First, by looking at Table 1, we notice that the proposed model (sisg), which uses subword information, outperforms the baselines on all datasets except the English WS353 dataset. Moreover, computing vectors for out-of-vocabulary words (sisg) is always at least as good as not doing so (sisg-). This proves the advantage of using subword information in the form of character $n$-grams.</p>
<p>Second, we observe that the effect of using character $n$-grams is more important for Arabic, German and Russian than for English, French or Spanish. German and Russian exhibit grammatical declensions with four cases for German and six for Russian. Also, many German words are compound words; for instance the nominal phrase "table tennis" is written in a single word as "Tischtennis". By exploiting the character-level similarities between "Tischtennis" and "Tennis", our model does not represent the two words as completely different words.</p>
<p>Finally, we observe that on the English Rare Words dataset (RW), our approach outperforms the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">sg</th>
<th style="text-align: center;">cbow</th>
<th style="text-align: center;">sisg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">Semantic</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Syntactic</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">77.8</td>
</tr>
<tr>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">Semantic</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">62.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Syntactic</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">56.4</td>
</tr>
<tr>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">Semantic</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">77.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Syntactic</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: center;">IT</td>
<td style="text-align: center;">Semantic</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">52.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Syntactic</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">62.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy of our model and baselines on word analogy tasks for Czech, German, English and Italian. We report results for semantic and syntactic analogies separately.
baselines while it does not on the English WS353 dataset. This is due to the fact that words in the English WS353 dataset are common words for which good vectors can be obtained without exploiting subword information. When evaluating on less frequent words, we see that using similarities at the character level between words can help learning good word vectors.</p>
<h3>5.2 Word analogy tasks</h3>
<p>We now evaluate our approach on word analogy questions, of the form $A$ is to $B$ as $C$ is to $D$, where $D$ must be predicted by the models. We use the datasets introduced by Mikolov et al. (2013a) for English, by Svoboda and Brychcin (2016) for Czech, by Köper et al. (2015) for German and by Berardi et al. (2015) for Italian. Some questions contain words that do not appear in our training corpus, and we thus excluded these questions from the evaluation.</p>
<p>We report accuracy for the different models in Table 2. We observe that morphological information significantly improves the syntactic tasks; our approach outperforms the baselines. In contrast, it does not help for semantic questions, and even degrades the performance for German and Italian. Note that this is tightly related to the choice of the length of character $n$-grams that we consider. We show in Sec. 5.5 that when the size of the $n$-grams is chosen optimally, the semantic analogies degrade</p>
<table>
<thead>
<tr>
<th></th>
<th>DE</th>
<th></th>
<th>EN</th>
<th></th>
<th>ES</th>
<th>FR</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GUR350</td>
<td>ZG222</td>
<td>WS353</td>
<td>RW</td>
<td>WS353</td>
<td>RG65</td>
</tr>
<tr>
<td>Luong et al. (2013)</td>
<td>-</td>
<td>-</td>
<td>64</td>
<td>34</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Qiu et al. (2014)</td>
<td>-</td>
<td>-</td>
<td>65</td>
<td>33</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Soricut and Och (2015)</td>
<td>64</td>
<td>22</td>
<td>71</td>
<td>42</td>
<td>47</td>
<td>67</td>
</tr>
<tr>
<td>sisg</td>
<td>73</td>
<td>43</td>
<td>73</td>
<td>48</td>
<td>54</td>
<td>69</td>
</tr>
<tr>
<td>Botha and Blunsom (2014)</td>
<td>56</td>
<td>25</td>
<td>39</td>
<td>30</td>
<td>28</td>
<td>45</td>
</tr>
<tr>
<td>sisg</td>
<td>66</td>
<td>34</td>
<td>54</td>
<td>41</td>
<td>49</td>
<td>52</td>
</tr>
</tbody>
</table>
<p>Table 3: Spearman's rank correlation coefficient between human judgement and model scores for different methods using morphology to learn word representations. We keep all the word pairs of the evaluation set and obtain representations for out-of-vocabulary words with our model by summing the vectors of character $n$-grams. Our model was trained on the same datasets as the methods we are comparing to (hence the two lines of results for our approach).
less. Another interesting observation is that, as expected, the improvement over the baselines is more important for morphologically rich languages, such as Czech and German.</p>
<h3>5.3 Comparison with morphological representations</h3>
<p>We also compare our approach to previous work on word vectors incorporating subword information on word similarity tasks. The methods used are: the recursive neural network of Luong et al. (2013), the morpheme cbow of Qiu et al. (2014) and the morphological transformations of Soricut and Och (2015). In order to make the results comparable, we trained our model on the same datasets as the methods we are comparing to: the English Wikipedia data released by Shaoul and Westbury (2010), and the news crawl data from the 2013 WMT shared task for German, Spanish and French. We also compare our approach to the log-bilinear language model introduced by Botha and Blunsom (2014), which was trained on the Europarl and news commentary corpora. Again, we trained our model on the same data to make the results comparable. Using our model, we obtain representations of out-ofvocabulary words by summing the representations of character $n$-grams. We report results in Table 3. We observe that our simple approach performs well relative to techniques based on subword information obtained from morphological segmentors. We also observe that our approach outperforms the Soricut
and Och (2015) method, which is based on prefix and suffix analysis. The large improvement for German is due to the fact that their approach does not model noun compounding, contrary to ours.</p>
<h3>5.4 Effect of the size of the training data</h3>
<p>Since we exploit character-level similarities between words, we are able to better model infrequent words. Therefore, we should also be more robust to the size of the training data that we use. In order to assess that, we propose to evaluate the performance of our word vectors on the similarity task as a function of the training data size. To this end, we train our model and the cbow baseline on portions of Wikipedia of increasing size. We use the Wikipedia corpus described above and isolate the first $1,2,5$, 10,20 , and 50 percent of the data. Since we don't reshuffle the dataset, they are all subsets of each other. We report results in Fig. 1.</p>
<p>As in the experiment presented in Sec. 5.1, not all words from the evaluation set are present in the Wikipedia data. Again, by default, we use a null vector for these words (sisg-) or compute a vector by summing the $n$-gram representations (sisg). The out-of-vocabulary rate is growing as the dataset shrinks, and therefore the performance of sisgand cbow necessarily degrades. However, the proposed model (sisg) assigns non-trivial vectors to previously unseen words.</p>
<p>First, we notice that for all datasets, and all sizes, the proposed approach (sisg) performs better than</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Influence of size of the training data on performance. We compute word vectors following the proposed model using datasets of increasing size. In this experiment, we train models on a fraction of the full Wikipedia dump.
the baseline. However, the performance of the baseline cbow model gets better as more and more data is available. Our model, on the other hand, seems to quickly saturate and adding more data does not always lead to improved results.</p>
<p>Second, and most importantly, we notice that the proposed approach provides very good word vectors even when using very small training datasets. For instance, on the German GUR350 dataset, our model (sisg) trained on $5 \%$ of the data achieves better performance (66) than the cbow baseline trained on the full dataset (62). On the other hand, on the English RW dataset, using $1 \%$ of the Wikipedia corpus we achieve a correlation coefficient of 45 which is better than the performance of cbow trained on the full dataset (43). This has a very important practical implication: well performing word vectors can be computed on datasets of a restricted size and still work well on previously unseen words. In general, when using vectorial word representations in specific applications, it is recommended to retrain the model on textual data relevant for the application. However, this kind of relevant task-specific data is often very scarce and learning from a reduced amount of training data is a great advantage.</p>
<h3>5.5 Effect of the size of $n$-grams</h3>
<p>The proposed model relies on the use of character $n$ grams to represent words as vectors. As mentioned in Sec. 3.2, we decided to use $n$-grams ranging from 3 to 6 characters. This choice was arbitrary, moti-
vated by the fact that $n$-grams of these lengths will cover a wide range of information. They would include short suffixes (corresponding to conjugations and declensions for instance) as well as longer roots. In this experiment, we empirically check for the influence of the range of $n$-grams that we use on performance. We report our results in Table 4 for English and German on word similarity and analogy datasets.</p>
<p>We observe that for both English and German, our arbitrary choice of 3-6 was a reasonable decision, as it provides satisfactory performance across languages. The optimal choice of length ranges depends on the considered task and language and should be tuned appropriately. However, due to the scarcity of test data, we did not implement any proper validation procedure to automatically select the best parameters. Nonetheless, taking a large range such as $3-6$ provides a reasonable amount of subword information.</p>
<p>This experiment also shows that it is important to include long $n$-grams, as columns corresponding to $n \leq 5$ and $n \leq 6$ work best. This is especially true for German, as many nouns are compounds made up from several units that can only be captured by longer character sequences. On analogy tasks, we observe that using larger $n$-grams helps for semantic analogies. However, results are always improved by taking $n \geq 3$ rather than $n \geq 2$, which shows that character 2-grams are not informative for that task. As described in Sec. 3.2, before computing</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">69</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">(a) DE-GUR350</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">48</td>
</tr>
</tbody>
</table>
<p>(d) EN-RW</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">60</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">63</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65</td>
</tr>
</tbody>
</table>
<p>(b) DE Semantic</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">76</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">77</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">79</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(e) EN Semantic</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">73</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">74</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(f) EN Syntactic</p>
<p>Table 4: Study of the effect of sizes of $n$-grams considered on performance. We compute word vectors by using character $n$-grams with $n$ in ${i, \ldots, j}$ and report performance for various values of $i$ and $j$. We evaluate this effect on German and English, and represent out-of-vocabulary words using subword information.
character $n$-grams, we prepend and append special positional characters to represent the beginning and end of word. Therefore, 2-grams will not be enough to properly capture suffixes that correspond to conjugations or declensions, since they are composed of a single proper character and a positional one.</p>
<h3>5.6 Language modeling</h3>
<p>In this section, we describe an evaluation of the word vectors obtained with our method on a language modeling task. We evaluate our language model on five languages (Cs, DE, Es, Fr, Ru) using the datasets introduced by Botha and Blunsom (2014). Each dataset contains roughly one million training tokens, and we use the same preprocessing and data splits as Botha and Blunsom (2014).</p>
<p>Our model is a recurrent neural network with 650 LSTM units, regularized with dropout (with probability of 0.5 ) and weight decay (regularization parameter of $10^{-5}$ ). We learn the parameters using the Adagrad algorithm with a learning rate of 0.1 , clipping the gradients which have a norm larger than 1.0. We initialize the weight of the network in the range $[-0.05,0.05]$, and use a batch size of 20 . Two baselines are considered: we compare our ap-
proach to the log-bilinear language model of Botha and Blunsom (2014) and the character aware language model of Kim et al. (2016). We trained word vectors with character $n$-grams on the training set of the language modeling task and use them to initialize the lookup table of our language model. We report the test perplexity of our model without using pre-trained word vectors (LSTM), with word vectors pre-trained without subword information (sg) and with our vectors (sisg). The results are presented in Table 5.</p>
<p>We observe that initializing the lookup table of the language model with pre-trained word representations improves the test perplexity over the baseline LSTM. The most important observation is that using word representations trained with subword information outperforms the plain skipgram model. We observe that this improvement is most significant for morphologically rich Slavic languages such as Czech ( $8 \%$ reduction of perplexity over sg) and Russian ( $13 \%$ reduction). The improvement is less significant for Roman languages such as Spanish ( $3 \%$ reduction) or French ( $2 \%$ reduction). This shows the importance of subword information on the language modeling task and exhibits the usefulness</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cs</th>
<th style="text-align: center;">DE</th>
<th style="text-align: center;">Es</th>
<th style="text-align: center;">Fr</th>
<th style="text-align: center;">Ru</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vocab. size</td>
<td style="text-align: center;">46 k</td>
<td style="text-align: center;">37 k</td>
<td style="text-align: center;">27 k</td>
<td style="text-align: center;">25 k</td>
<td style="text-align: center;">63 k</td>
</tr>
<tr>
<td style="text-align: center;">CLBL</td>
<td style="text-align: center;">465</td>
<td style="text-align: center;">296</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">225</td>
<td style="text-align: center;">304</td>
</tr>
<tr>
<td style="text-align: center;">CANLM</td>
<td style="text-align: center;">371</td>
<td style="text-align: center;">239</td>
<td style="text-align: center;">165</td>
<td style="text-align: center;">184</td>
<td style="text-align: center;">261</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">366</td>
<td style="text-align: center;">222</td>
<td style="text-align: center;">157</td>
<td style="text-align: center;">173</td>
<td style="text-align: center;">262</td>
</tr>
<tr>
<td style="text-align: center;">sg</td>
<td style="text-align: center;">339</td>
<td style="text-align: center;">216</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">162</td>
<td style="text-align: center;">237</td>
</tr>
<tr>
<td style="text-align: center;">sisg</td>
<td style="text-align: center;">$\mathbf{3 1 2}$</td>
<td style="text-align: center;">$\mathbf{2 0 6}$</td>
<td style="text-align: center;">$\mathbf{1 4 5}$</td>
<td style="text-align: center;">$\mathbf{1 5 9}$</td>
<td style="text-align: center;">$\mathbf{2 0 6}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Test perplexity on the language modeling task, for 5 different languages. We compare to two state of the art approaches: CLBL refers to the work of Botha and Blunsom (2014) and CANLM refers to the work of Kim et al. (2016).
of the vectors that we propose for morphologically rich languages.</p>
<h2>6 Qualitative analysis</h2>
<h3>6.1 Nearest neighbors.</h3>
<p>We report sample qualitative results in Table 7. For selected words, we show nearest neighbors according to cosine similarity for vectors trained using the proposed approach and for the skipgram baseline. As expected, the nearest neighbors for complex, technical and infrequent words using our approach are better than the ones obtained using the baseline model.</p>
<h3>6.2 Character $n$-grams and morphemes</h3>
<p>We want to qualitatively evaluate whether or not the most important $n$-grams in a word correspond to morphemes. To this end, we take a word vector that we construct as the sum of $n$-grams. As described in Sec. 3.2, each word $w$ is represented as the sum of its $n$-grams: $u_{w}=\sum_{g \in \mathcal{G}<em g="g">{w}} z</em>$ obtained by omitting $g$ :}$. For each $n$-gram $g$, we propose to compute the restricted representation $u_{w \backslash g</p>
<p>$$
u_{w \backslash g}=\sum_{g^{\prime} \in \mathcal{G}-{g}} z_{g^{\prime}}
$$</p>
<p>We then rank $n$-grams by increasing value of cosine between $u_{w}$ and $u_{w \backslash g}$. We show ranked $n$-grams for selected words in three languages in Table 6.</p>
<p>For German, which has a lot of compound nouns, we observe that the most important $n$-grams cor-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">word</th>
<th style="text-align: center;">$n$-grams</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">autofahrer</td>
<td style="text-align: center;">fahr</td>
<td style="text-align: center;">fahrer</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">freundeskreis</td>
<td style="text-align: center;">kreis</td>
<td style="text-align: center;">kreis&gt;</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">grundwort</td>
<td style="text-align: center;">wort</td>
<td style="text-align: center;">wort&gt;</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sprachschule</td>
<td style="text-align: center;">schul</td>
<td style="text-align: center;">hschul</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">tageslicht</td>
<td style="text-align: center;">licht</td>
<td style="text-align: center;">gesl</td>
</tr>
<tr>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">anarchy</td>
<td style="text-align: center;">chy</td>
<td style="text-align: center;">&lt;anar</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">monarchy</td>
<td style="text-align: center;">monarc</td>
<td style="text-align: center;">chy</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">kindness</td>
<td style="text-align: center;">ness&gt;</td>
<td style="text-align: center;">ness</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">politeness</td>
<td style="text-align: center;">polite</td>
<td style="text-align: center;">ness&gt;</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">unlucky</td>
<td style="text-align: center;">&lt;un</td>
<td style="text-align: center;">cky&gt;</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">lifetime</td>
<td style="text-align: center;">life</td>
<td style="text-align: center;">&lt;life</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">starfish</td>
<td style="text-align: center;">fish</td>
<td style="text-align: center;">fish&gt;</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">submarine</td>
<td style="text-align: center;">marine</td>
<td style="text-align: center;">sub</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">transform</td>
<td style="text-align: center;">trans</td>
<td style="text-align: center;">&lt;trans</td>
</tr>
<tr>
<td style="text-align: center;">FR</td>
<td style="text-align: center;">finirais</td>
<td style="text-align: center;">ais&gt;</td>
<td style="text-align: center;">nir</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">finissent</td>
<td style="text-align: center;">ent&gt;</td>
<td style="text-align: center;">finiss</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">finissions</td>
<td style="text-align: center;">ions&gt;</td>
<td style="text-align: center;">finiss</td>
</tr>
</tbody>
</table>
<p>Table 6: Illustration of most important character $n$ grams for selected words in three languages. For each word, we show the $n$-grams that, when removed, result in the most different representation.
respond to valid morphemes. Good examples include Autofahrer (car driver) whose most important $n$-grams are Auto (car) and Fahrer (driver). We also observe the separation of compound nouns into morphemes in English, with words such as lifetime or starfish. However, for English, we also observe that $n$-grams can correspond to affixes in words such as kindness or unlucky. Interestingly, for French we observe the inflections of verbs with endings such as ais&gt;, ent&gt; or ions&gt;.</p>
<h3>6.3 Word similarity for OOV words</h3>
<p>As described in Sec. 3.2, our model is capable of building word vectors for words that do not appear in the training set. For such words, we simply average the vector representation of its $n$-grams. In order to assess the quality of these representations, we analyze which of the $n$-grams match best for OOV words by selecting a few word pairs from the English RW similarity dataset. We select pairs such that one of the two words is not in the training vocabulary and is hence only represented by its $n$ grams. For each pair of words, we display the cosine similarity between each pair of $n$-grams that appear</p>
<table>
<thead>
<tr>
<th style="text-align: center;">query</th>
<th style="text-align: center;">tiling</th>
<th style="text-align: center;">tech-rich</th>
<th style="text-align: center;">english-born</th>
<th style="text-align: center;">micromanaging</th>
<th style="text-align: center;">eateries</th>
<th style="text-align: center;">dendritic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">sisg</td>
<td style="text-align: center;">tile <br> flooring</td>
<td style="text-align: center;">tech-dominated <br> tech-heavy</td>
<td style="text-align: center;">british-born <br> polish-born</td>
<td style="text-align: center;">micromanage <br> micromanaged</td>
<td style="text-align: center;">restaurants <br> eaterie</td>
<td style="text-align: center;">dendrite <br> dendrites</td>
</tr>
<tr>
<td style="text-align: center;">sg</td>
<td style="text-align: center;">bookcases <br> built-ins</td>
<td style="text-align: center;">technology-heavy <br> .ixic</td>
<td style="text-align: center;">most-capped <br> ex-scotland</td>
<td style="text-align: center;">defang <br> internalise</td>
<td style="text-align: center;">restaurants <br> delis</td>
<td style="text-align: center;">epithelial <br> p53</td>
</tr>
</tbody>
</table>
<p>Table 7: Nearest neighbors of rare words using our representations and skipgram. These hand picked examples are for illustration.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the similarity between character $n$-grams in out-of-vocabulary words. For each pair, only one word is OOV, and is shown on the $x$ axis. Red indicates positive cosine, while blue negative.</p>
<p>in the words. In order to simulate a setup with a larger number of OOV words, we use models trained on $1 \%$ of the Wikipedia data as in Sec. 5.4. The results are presented in Fig. 2.</p>
<p>We observe interesting patterns, showing that subwords match correctly. Indeed, for the word chip, we clearly see that there are two groups of $n$-grams in microcircuit that match well. These roughly correspond to micro and circuit, and $n$-grams in between don't match well. Another interesting example is the pair rarity and scarceness. Indeed, scarce roughly matches rarity while the suffix -ness matches -ity very well. Finally, the word preadolescent matches young well thanks to the -adolescsubword. This shows that we build robust word representations where prefixes and suffixes can be ignored if the grammatical form is not found in the dictionary.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we investigate a simple method to learn word representations by taking into account subword information. Our approach, which incorporates character $n$-grams into the skipgram model, is related to an idea that was introduced by Schütze (1993). Because of its simplicity, our model trains fast and does not require any preprocessing or supervision. We show that our model outperforms baselines that do not take into account subword information, as well as methods relying on morphological analysis. We will open source the implementation of our model, in order to facilitate comparison of future work on learning subword representations.</p>
<h2>Acknowledgements</h2>
<p>We thank Marco Baroni, Hinrich Schütze and the anonymous reviewers for their insightful comments.</p>
<h2>References</h2>
<p>Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored neural language models. In Proc. NAACL.
Miguel Ballesteros, Chris Dyer, and Noah A. Smith. 2015. Improved transition-based parsing by modeling characters instead of words with LSTMs. In Proc. EMNLP.
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673721.</p>
<p>Giacomo Berardi, Andrea Esuli, and Diego Marcheggiani. 2015. Word embeddings go to Italy: a comparison of models and training datasets. Italian Information Retrieval Workshop.
Piotr Bojanowski, Armand Joulin, and Tomáš Mikolov. 2015. Alternative structures for character-level RNNs. In Proc. ICLR.
Jan A. Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In Proc. ICML.
Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. 2015. Joint learning of character and word embeddings. In Proc. IJCAI.
Grzegorz Chrupała. 2014. Normalizing tweets with edit scripts and recurrent neural embeddings. In Proc. $A C L$.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proc. ICML.
Ryan Cotterell and Hinrich Schütze. 2015. Morphological word-embeddings. In Proc. NAACL.
Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, Hanjun Dai, and Tie-Yan Liu. 2015. KNET: A general framework for learning word embedding using morphological knowledge. ACM Transactions on Information Systems, 34(1):4:1-4:25.
Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391407.</p>
<p>Cicero Nogueira dos Santos and Maira Gatti. 2014. Deep convolutional neural networks for sentiment analysis of short texts. In Proc. COLING.
Cicero Nogueira dos Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-ofspeech tagging. In Proc. ICML.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proc. WWW.
Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.
Iryna Gurevych. 2005. Using the structure of a conceptual network in computing semantic relatedness. In Proc. IJCNLP.
Zellig S Harris. 1954. Distributional structure. Word, 10(2-3):146-162.
Samer Hassan and Rada Mihalcea. 2009. Cross-lingual semantic relatedness using encyclopedic knowledge. In Proc. EMNLP.</p>
<p>Colette Joubarne and Diana Inkpen. 2011. Comparison of semantic similarity for different languages using the google n-gram corpus and second-order co-occurrence measures. In Proc. Canadian Conference on Artificial Intelligence.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. Character-aware neural language models. In Proc. AAAI.
Maximilian Köper, Christian Scheible, and Sabine Schulte im Walde. 2015. Multilingual reliability and "semantic" structure of continuous word spaces. Proc. IWCS 2015.
Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni. 2013. Compositionally derived representations of morphologically complex words in distributional semantics. In Proc. ACL.
Wang Ling, Chris Dyer, Alan W. Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, Luis Marujo, and Tiago Luis. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In Proc. EMNLP.
Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, \&amp; Computers, 28(2):203-208.
Minh-Thang Luong and Christopher D. Manning. 2016. Achieving open vocabulary neural machine translation with hybrid word-character models. In Proc. ACL.
Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. In Proc. CoNLL.
Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Černocký. 2012. Subword language modeling with neural networks. Technical report, Faculty of Information Technology, Brno University of Technology.
Tomáš Mikolov, Kai Chen, Greg D. Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Tomáš Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Adv. NIPS.
Alexander Panchenko, Dmitry Ustalov, Nikolay Arefyev, Denis Paperno, Natalia Konstantinova, Natalia Loukachevitch, and Chris Biemann. 2016. Human and machine judgements for russian semantic relatedness. In Proc. AIST.
Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Co-learning of word representations and morpheme representations. In Proc. COLING.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In $A d v$. NIPS.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1988. Neurocomputing: Foundations of research. chapter Learning Representations by Backpropagating Errors, pages 696-699. MIT Press.
Haşim Sak, Murat Saraclar, and Tunga Gungör. 2010. Morphology-based and sub-word language modeling for turkish speech recognition. In Proc. ICASSP.
Hinrich Schütze. 1992. Dimensions of meaning. In Proc. IEEE Conference on Supercomputing.
Hinrich Schütze. 1993. Word space. In Adv. NIPS.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proc. ACL.
Cyrus Shaoul and Chris Westbury. 2010. The Westbury lab Wikipedia corpus.
Radu Soricut and Franz Och. 2015. Unsupervised morphology induction using word embeddings. In Proc. NAACL.
Charles Spearman. 1904. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72-101.
Henning Sperr, Jan Niehues, and Alexander Waibel. 2013. Letter n-gram-based input encoding for continuous space language models. In Proc. of the Workshop on Continuous Vector Space Models and their Compositionality at ACL.
Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proc. ICML.
Lukáš Svoboda and Tomáš Brychcin. 2016. New word analogy corpus for exploring embeddings of Czech words. In Proc. CICLING.
Peter D. Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141188.</p>
<p>John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. CHARAGRAM: Embedding words and sentences via character n-grams. In Proc. EMNLP.
Torsten Zesch and Iryna Gurevych. 2006. Automatically creating datasets for measures of semantic relatedness. In Proc. Workshop on Linguistic Distances.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Adv. NIPS.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://code.google.com/archive/p/word2vec
${ }^{3}$ https://github.com/facebookresearch/fastText
${ }^{4}$ https://dumps.wikimedia.org&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ http://mattmahoney.net/dc/textdata&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
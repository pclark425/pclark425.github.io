<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4197 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4197</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4197</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-274141118</p>
                <p><strong>Paper Title:</strong> ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity</p>
                <p><strong>Paper Abstract:</strong> Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a non-profit cloud-based auto finetuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science. The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction. The platform achieves remarkable accuracy with only a small amount of well-annotated articles. This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics. Demo Video</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4197.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4197.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ByteScience</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ByteScience platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloud-based platform that auto-fine-tunes a domain-specific LLM to extract structured scientific data and synthesize relationships/patterns from large scientific corpora using a human-in-the-loop annotation and AWS SageMaker deployment pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ByteScience (DARWIN fine-tuning and extraction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ByteScience ingests PDFs/HTML/XML (PDF conversion via PDFMiner), allows users to define annotation schemas (entity labels and relationships), performs automatic pre-labelling with a base LLM (DARWIN), collects human corrections, converts corrected annotations into an instruction-format dataset, fine-tunes the LLM on AWS SageMaker, deploys the fine-tuned model to a SageMaker Endpoint, and runs large-scale extraction to produce structured JSON stored in MongoDB. The workflow explicitly supports NER, relation extraction (RE), and entity resolution (ER) with iterative retraining (human-in-the-loop) to improve performance and adapt to domain terminology.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (alloy synthesis, batteries, catalysis, photovoltaics) and broadly natural science domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Experimental comparisons used 90 samples across batteries, catalysis, and photovoltaics; training/sample-size experiments referenced 10–20 few-shot examples and ~300 annotated samples; platform claims capability to process millions of papers (operational claim) and mentions cost estimates for 10,000 articles.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical relationships and domain-specific structure–property/process relationships (e.g., Composition–Processing–Structure–Performance (CPSP) empirical relationships), entity-relation patterns and trends extracted from literature (not explicit derivation of closed-form physical laws in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Extraction and structuring of CPSP relationships: linking alloy composition and processing parameters (casting, solution treatment, aging) to microstructure and performance metrics; identification of trends in alloy synthesis that correlate composition/processing variables with structural/performance outcomes. (No explicit mathematical equations provided in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text mining of converted documents (PDFMiner → JSON), schema-driven annotation (user-defined labels), automatic LLM pre-labelling (NER/RE/ER), human correction, fine-tuning on instruction-format datasets, and endpoint-based batch extraction producing structured JSON; primarily extracts textual mentions, numeric parameters, and relations from prose (tables/figures not explicitly described beyond document conversion).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Human-in-the-loop review and correction of auto-labelling; quantitative evaluation against manually annotated samples using precision/recall/F1; iterative retraining when accuracy/recall is insufficient; comparative evaluation against non-LLM baselines (e.g., MatBERT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: precision/recall/F1 in the range 0.8–0.9 for structure extraction tasks with ~300 samples; the platform claims 80%–90% of human extraction accuracy after a few hours of annotation. Annotation-time reduction: 57% reduction using 300 training samples (human-in-the-loop). Processing speed: ~1 second per 10-page document. Cost example: ~$0.023 per paper for 10,000 articles.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Approximately 80%–90% extraction accuracy reported (paper-level claim), and precision/recall/F1 ≈ 0.8–0.9 in cited experiments with ~300 annotated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Domain-specific language and multi-sentence/contextual dependencies (relations spanning sections/pages) make extraction hard; limited existing structured databases to bootstrap learning; traditional entity co-occurrence methods (e.g., MatKG) can oversimplify relationships; some baseline models (e.g., MatBERT) produced irrelevant entities reducing precision; need for more annotations when accuracy is low and for more corpus data when recall is low; no demonstration in the paper of extracting closed-form physical laws or explicit mathematical equations (mostly empirical/relational extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively and quantitatively to non-LLM and transformer baselines (e.g., MatBERT, MatKG-style approaches); ByteScience's LLM pipeline reportedly outperformed traditional methods across NER/RE/ER tasks with fewer labeled samples, producing higher reliability on unstructured scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4197.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4197.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DARWIN Series: Domain Specific Large Language Models for Natural Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, domain-specific large language model series tailored to natural science tasks that serves as the base model used by ByteScience for fine-tuning on domain annotation schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DARWIN Series: Domain Specific Large Language Models for Natural Science.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DARWIN (domain-specific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DARWIN is described as an open-source, state-of-the-art natural-science LLM used as the foundation in ByteScience; within the platform it is fine-tuned on instruction-format datasets derived from corrected annotations to specialize in extracting entities and relations in scientific domains. The paper does not specify DARWIN's architecture details, parameter count, or internal prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural sciences broadly; applied in the paper to materials science subdomains (alloy synthesis, batteries, catalysis, photovoltaics).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified for DARWIN itself in this paper (used as base model for fine-tuning on the platform-specific annotated corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical relationships and structured entity–relation patterns (e.g., CPSP-style relationships) extracted after fine-tuning; DARWIN is the model used to realize these extractions when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Same CPSP relationships described for ByteScience when DARWIN is fine-tuned: linking composition and processing parameters to structure and performance outcomes in alloys; no explicit algebraic laws presented.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Fine-tuning DARWIN on task-specific, instruction-formatted datasets; using it for NER, relation extraction, and entity resolution over converted document text via a deployed endpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Evaluation via human-corrected annotations and standard NER/RE/ER metrics (precision/recall/F1); iterative refinement with additional annotations if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper-level reported metrics for fine-tuned LLMs (including DARWIN when fine-tuned) are precision/recall/F1 ≈ 0.8–0.9 with ~300 annotated samples and claimed 80%–90% of human performance for extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported extraction accuracy after fine-tuning is in the 80%–90% range (paper-level claim when few annotated examples are used to adapt DARWIN).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No model-size or architectural details provided; performance depends on quality/quantity of annotated examples and on handling cross-sentence/contextual dependencies; paper does not demonstrate extraction of closed-form quantitative laws or derivation of new mathematical relationships explicitly using DARWIN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Referenced and compared (in platform experiments) against non-LLM methods such as MatBERT and other traditional extraction approaches; fine-tuned DARWIN-based pipeline reportedly outperforms those baselines on NER/RE/ER with fewer samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4197.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4197.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3/Doping-English</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3/Doping-English model (referenced example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3 variant referenced in the paper as an example where few-shot/few-sample learning enables structural extraction, with reported rapid learning of annotation structure and strong metrics after modest annotation counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3/Doping-English (example of few-shot learning for structure extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an example where 10–20 samples allow the model to learn the correct annotation structure and where precision/recall/F1 reach ~0.8–0.9 with ~300 samples; the paper uses this as background evidence for the sample-efficiency of LLMs in structure extraction tasks rather than reporting new experiments with GPT-3 itself.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3/Doping-English</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Not specific — referenced as a general example for structure extraction across scientific texts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified (example relates to sample counts for training rather than number of processed papers).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Structural extraction (schema/annotation learning) rather than explicit quantitative laws; demonstrates ability to learn extraction schemas with few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No explicit quantitative laws given; the example supports efficient learning of annotation structure enabling downstream extraction of relations and numeric data from texts.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Few-shot/few-sample instruction or fine-tuning to learn annotation schema for NER/RE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Reported evaluation via precision/recall/F1 on held-out or annotated samples (as cited in the paper's summary of LLM behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited performance: precision, recall, and F1 reaching approximately 0.8–0.9 with around 300 samples; 10–20 samples enough to learn the annotation structure in the referenced example.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Implicitly ~80%–90% on the cited extraction tasks after ~300 samples (per the paper's descriptive statement).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Example-level claim; not an experiment presented in this paper. No details on domain transfer, numeric extraction fidelity, or equation parsing; applicability to extracting closed-form quantitative laws not demonstrated in this reference statement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DARWIN Series: Domain Specific Large Language Models for Natural Science. <em>(Rating: 2)</em></li>
                <li>Structured information extraction from complex scientific text with fine-tuned large language models. <em>(Rating: 2)</em></li>
                <li>MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4197",
    "paper_id": "paper-274141118",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "ByteScience",
            "name_full": "ByteScience platform",
            "brief_description": "A cloud-based platform that auto-fine-tunes a domain-specific LLM to extract structured scientific data and synthesize relationships/patterns from large scientific corpora using a human-in-the-loop annotation and AWS SageMaker deployment pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ByteScience (DARWIN fine-tuning and extraction pipeline)",
            "system_description": "ByteScience ingests PDFs/HTML/XML (PDF conversion via PDFMiner), allows users to define annotation schemas (entity labels and relationships), performs automatic pre-labelling with a base LLM (DARWIN), collects human corrections, converts corrected annotations into an instruction-format dataset, fine-tunes the LLM on AWS SageMaker, deploys the fine-tuned model to a SageMaker Endpoint, and runs large-scale extraction to produce structured JSON stored in MongoDB. The workflow explicitly supports NER, relation extraction (RE), and entity resolution (ER) with iterative retraining (human-in-the-loop) to improve performance and adapt to domain terminology.",
            "model_name": "DARWIN",
            "model_size": null,
            "scientific_domain": "Materials science (alloy synthesis, batteries, catalysis, photovoltaics) and broadly natural science domains",
            "number_of_papers": "Experimental comparisons used 90 samples across batteries, catalysis, and photovoltaics; training/sample-size experiments referenced 10–20 few-shot examples and ~300 annotated samples; platform claims capability to process millions of papers (operational claim) and mentions cost estimates for 10,000 articles.",
            "law_type": "Empirical relationships and domain-specific structure–property/process relationships (e.g., Composition–Processing–Structure–Performance (CPSP) empirical relationships), entity-relation patterns and trends extracted from literature (not explicit derivation of closed-form physical laws in the paper).",
            "law_examples": "Extraction and structuring of CPSP relationships: linking alloy composition and processing parameters (casting, solution treatment, aging) to microstructure and performance metrics; identification of trends in alloy synthesis that correlate composition/processing variables with structural/performance outcomes. (No explicit mathematical equations provided in the paper.)",
            "extraction_method": "Text mining of converted documents (PDFMiner → JSON), schema-driven annotation (user-defined labels), automatic LLM pre-labelling (NER/RE/ER), human correction, fine-tuning on instruction-format datasets, and endpoint-based batch extraction producing structured JSON; primarily extracts textual mentions, numeric parameters, and relations from prose (tables/figures not explicitly described beyond document conversion).",
            "validation_approach": "Human-in-the-loop review and correction of auto-labelling; quantitative evaluation against manually annotated samples using precision/recall/F1; iterative retraining when accuracy/recall is insufficient; comparative evaluation against non-LLM baselines (e.g., MatBERT).",
            "performance_metrics": "Reported metrics: precision/recall/F1 in the range 0.8–0.9 for structure extraction tasks with ~300 samples; the platform claims 80%–90% of human extraction accuracy after a few hours of annotation. Annotation-time reduction: 57% reduction using 300 training samples (human-in-the-loop). Processing speed: ~1 second per 10-page document. Cost example: ~$0.023 per paper for 10,000 articles.",
            "success_rate": "Approximately 80%–90% extraction accuracy reported (paper-level claim), and precision/recall/F1 ≈ 0.8–0.9 in cited experiments with ~300 annotated samples.",
            "challenges_limitations": "Domain-specific language and multi-sentence/contextual dependencies (relations spanning sections/pages) make extraction hard; limited existing structured databases to bootstrap learning; traditional entity co-occurrence methods (e.g., MatKG) can oversimplify relationships; some baseline models (e.g., MatBERT) produced irrelevant entities reducing precision; need for more annotations when accuracy is low and for more corpus data when recall is low; no demonstration in the paper of extracting closed-form physical laws or explicit mathematical equations (mostly empirical/relational extraction).",
            "comparison_baseline": "Compared qualitatively and quantitatively to non-LLM and transformer baselines (e.g., MatBERT, MatKG-style approaches); ByteScience's LLM pipeline reportedly outperformed traditional methods across NER/RE/ER tasks with fewer labeled samples, producing higher reliability on unstructured scientific text.",
            "uuid": "e4197.0",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DARWIN",
            "name_full": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
            "brief_description": "An open-source, domain-specific large language model series tailored to natural science tasks that serves as the base model used by ByteScience for fine-tuning on domain annotation schemas.",
            "citation_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science.",
            "mention_or_use": "use",
            "system_name": "DARWIN (domain-specific LLM)",
            "system_description": "DARWIN is described as an open-source, state-of-the-art natural-science LLM used as the foundation in ByteScience; within the platform it is fine-tuned on instruction-format datasets derived from corrected annotations to specialize in extracting entities and relations in scientific domains. The paper does not specify DARWIN's architecture details, parameter count, or internal prompting strategies.",
            "model_name": "DARWIN",
            "model_size": null,
            "scientific_domain": "Natural sciences broadly; applied in the paper to materials science subdomains (alloy synthesis, batteries, catalysis, photovoltaics).",
            "number_of_papers": "Not specified for DARWIN itself in this paper (used as base model for fine-tuning on the platform-specific annotated corpora).",
            "law_type": "Empirical relationships and structured entity–relation patterns (e.g., CPSP-style relationships) extracted after fine-tuning; DARWIN is the model used to realize these extractions when fine-tuned.",
            "law_examples": "Same CPSP relationships described for ByteScience when DARWIN is fine-tuned: linking composition and processing parameters to structure and performance outcomes in alloys; no explicit algebraic laws presented.",
            "extraction_method": "Fine-tuning DARWIN on task-specific, instruction-formatted datasets; using it for NER, relation extraction, and entity resolution over converted document text via a deployed endpoint.",
            "validation_approach": "Evaluation via human-corrected annotations and standard NER/RE/ER metrics (precision/recall/F1); iterative refinement with additional annotations if needed.",
            "performance_metrics": "Paper-level reported metrics for fine-tuned LLMs (including DARWIN when fine-tuned) are precision/recall/F1 ≈ 0.8–0.9 with ~300 annotated samples and claimed 80%–90% of human performance for extraction tasks.",
            "success_rate": "Reported extraction accuracy after fine-tuning is in the 80%–90% range (paper-level claim when few annotated examples are used to adapt DARWIN).",
            "challenges_limitations": "No model-size or architectural details provided; performance depends on quality/quantity of annotated examples and on handling cross-sentence/contextual dependencies; paper does not demonstrate extraction of closed-form quantitative laws or derivation of new mathematical relationships explicitly using DARWIN.",
            "comparison_baseline": "Referenced and compared (in platform experiments) against non-LLM methods such as MatBERT and other traditional extraction approaches; fine-tuned DARWIN-based pipeline reportedly outperforms those baselines on NER/RE/ER with fewer samples.",
            "uuid": "e4197.1",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3/Doping-English",
            "name_full": "GPT-3/Doping-English model (referenced example)",
            "brief_description": "A GPT-3 variant referenced in the paper as an example where few-shot/few-sample learning enables structural extraction, with reported rapid learning of annotation structure and strong metrics after modest annotation counts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "GPT-3/Doping-English (example of few-shot learning for structure extraction)",
            "system_description": "Mentioned as an example where 10–20 samples allow the model to learn the correct annotation structure and where precision/recall/F1 reach ~0.8–0.9 with ~300 samples; the paper uses this as background evidence for the sample-efficiency of LLMs in structure extraction tasks rather than reporting new experiments with GPT-3 itself.",
            "model_name": "GPT-3/Doping-English",
            "model_size": null,
            "scientific_domain": "Not specific — referenced as a general example for structure extraction across scientific texts.",
            "number_of_papers": "Not specified (example relates to sample counts for training rather than number of processed papers).",
            "law_type": "Structural extraction (schema/annotation learning) rather than explicit quantitative laws; demonstrates ability to learn extraction schemas with few examples.",
            "law_examples": "No explicit quantitative laws given; the example supports efficient learning of annotation structure enabling downstream extraction of relations and numeric data from texts.",
            "extraction_method": "Few-shot/few-sample instruction or fine-tuning to learn annotation schema for NER/RE tasks.",
            "validation_approach": "Reported evaluation via precision/recall/F1 on held-out or annotated samples (as cited in the paper's summary of LLM behavior).",
            "performance_metrics": "Cited performance: precision, recall, and F1 reaching approximately 0.8–0.9 with around 300 samples; 10–20 samples enough to learn the annotation structure in the referenced example.",
            "success_rate": "Implicitly ~80%–90% on the cited extraction tasks after ~300 samples (per the paper's descriptive statement).",
            "challenges_limitations": "Example-level claim; not an experiment presented in this paper. No details on domain transfer, numeric extraction fidelity, or equation parsing; applicability to extracting closed-form quantitative laws not demonstrated in this reference statement.",
            "uuid": "e4197.2",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science.",
            "rating": 2,
            "sanitized_title": "darwin_series_domain_specific_large_language_models_for_natural_science"
        },
        {
            "paper_title": "Structured information extraction from complex scientific text with fine-tuned large language models.",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_complex_scientific_text_with_finetuned_large_language_models"
        },
        {
            "paper_title": "MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning.",
            "rating": 1,
            "sanitized_title": "matkg_the_largest_knowledge_graph_in_materials_science_entities_relations_and_link_prediction_through_graph_representation_learning"
        }
    ],
    "cost": 0.011620750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity</p>
<p>Tong Xie 
GreenDynamics
KensingtonAustralia</p>
<p>School of Photovoltaic and Renewable Energy Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Hanzhi Zhang hanzhizhang@my.unt.edu 
Dept. of Computer Science and Engineering
University of North Texas
DentonUnited States</p>
<p>Shaozhou Wang shaozhou@greendynamics.com.au 
GreenDynamics
KensingtonAustralia</p>
<p>Yuwei Wan 
Imran Razzak imran.razzak@unsw.edu.au 
GreenDynamics
KensingtonAustralia</p>
<p>School of Computer Science and Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Chunyu Kit ctckit@cityu.edu.hk 
Dept. of Linguistics and Translation
City University of Hong Kong
Hong KongChina</p>
<p>Wenjie Zhang wenjie.zhang@unsw.edu.au 
School of Computer Science and Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Bram Hoex b.hoex@unsw.edu.au 
School of Photovoltaic and Renewable Energy Engineering
University of New South Wales
KensingtonAustralia</p>
<p>ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity
04E441E4B0A441AF9CBD44781A64541910.1109/ICDMW65004.2024.00126componentformattingstylestylinginsert
Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information.However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information.To address this, we introduce ByteScience, a non-profit cloud-based auto finetuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora.The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science.The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction.The platform achieves remarkable accuracy with only a small amount of wellannotated articles.This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics.Demo Video</p>
<p>I. INTRODUCTION</p>
<p>AI has the potential to revolutionize scientific discovery (AI4Science [1]), but challenges remain.Scientific knowledge is scattered across documents, making it hard to fully leverage past research.LLMs offer a promising solution but require structured texts, including converting PDFs and generating fine-tuning examples for NLP tasks.While machine learning models are used in fields like drug discovery [2], protein design [3], and crystal structure generation [4], limited structured data hinders their effectiveness.Databases like Materials Project [5] and NOMAD [6] cover only a fraction of data, leaving much unstructured information untapped.This gap presents an opportunity for AI to accelerate discovery.Although converting documents to markup is well-studied, extracting complex relationships remains challenging but essential for building knowledge graphs and fine-tuning datasets.</p>
<p>• Contextual Dependency: Relationships in scientific texts often depend heavily on context that may span multiple sentences or sections.For instance, a material's properties might be discussed concerning its synthesis method, as described in paragraphs several pages before.Traditional methods like MatKG [7], which define relationships by entity co-occurrence, often miss the nuances of scientific knowledge.While useful, they risk oversimplifying complex relationships.Advanced techniques are needed to better capture this complexity for improved knowledge extraction in AI-driven scientific discovery.Therefore, we introduce ByteScience, a cloud-based platform featuring an auto-finetuned LLM to extract structured scientific data and synthesize new scientific knowledge from extensive scientific corpora.We conclude as follows:</p>
<p>1) Tailored with DARWIN [8], an open-source state-of-the-art nature-science LLM, to provide research focus utilization; 2) Zero-code user-friendly semi-automated annotation and processing for uploaded science documents; 3) A personalized and domain-specific auto fine-tuning LLM that requires only a single fully annotated piece of literature; 4) Time efficiency high-quality science data extraction from millions of papers for less than a second per article.</p>
<p>II. PLATFORM DESIGN</p>
<p>ByteScience is a robust, scalable cloud-based solution leveraging AWS Sagemaker.This architecture ensures high availability, scalability, and performance for processing large scientific documents.Figure 1 illustrates the extraction pipeline for custom model development and data extraction.The pipeline consists of two primary phases:</p>
<p>• Initial Setup (First-time use for a specific field):</p>
<p>Dataset Construction (Green Pipeline): This phase builds a domain-specific corpus of structured scientific data.LLM Fine-tuning (Blue Pipeline): The system fine-tunes a large language model on the constructed dataset to optimize performance for the target scientific domain.• Operational Phase: Once the initial setup is complete, users can directly utilize the fine-tuned LLM stored in AWS to efficiently generate structured datasets from new scientific documents in the same field.This two-phase approach allows ByteScience to quickly adapt to various scientific domains while maintaining high extraction accuracy.The cloud-based architecture enables seamless scaling and ensures users always have access to the latest fine-tuned models, streamlining the conversion of unstructured scientific literature into structured data.Key steps include: 1) Create Database: Users upload scientific documents in JSON, PDF, HTML, or XML formats.Non-JSON text is extracted and saved as JSON, with HTML/XML markup stripped, and PDF conversion done using PDFMiner [9].2) Define Structure: Users define annotation structures, including entity labels and relationships, using pre-built or custom templates.3) Random Selection: A small text subset is randomly selected for initial annotation on first use.4) Auto Labelling: The LLM applies automatic pre-labeling to the selected texts.</p>
<p>III. ARCHITECTURE OF AWS CLOUD-BASED SERVICES</p>
<p>ByteScience utilizes the robust, scalable infrastructure of Amazon Web Services (AWS) to efficiently handle user requests and data processing.Figure 2 shows the detailed architecture of our platform.</p>
<p>A. General Service(Green Pipeline) Infrastructure</p>
<p>The user interaction layer is built on a series of AWS services that ensure high availability, security, and performance:</p>
<p>• DNS Management: AWS Route 53 routes incoming user requests to the appropriate services within the architecture.• Database Services: Amazon Relational Database Service (RDS) supports complex queries and transactions essential for scientific data management.</p>
<p>B. LLM Service (Blue Pipeline) Architecture</p>
<p>The LLM fine-tuning capability is a core function, implemented through a sophisticated pipeline of AWS services:</p>
<p>C. Workflow Integration</p>
<p>ByteScience workflow seamlessly integrates these components: 1) Users interact with the system through Route 53 and the ALB for initial annotation tasks.</p>
<p>2) The DARWIN LLM processes annotated data on a Sage-Maker Endpoint.This architecture enables ByteScience to offer customized, high-performance language models tailored to specific scientific domains, facilitating accurate and efficient structured data extraction from unstructured scientific literature.</p>
<p>IV. STRUCTURED DATA EXTRACTION PERFORMANCE</p>
<p>LLMs significantly improve human-in-the-loop annotation.Using 300 training samples reduced annotation time by 57% compared to a single sample [10].In the GPT-3/Doping-English model, 10-20 samples were enough to learn the correct structure, with precision, recall, and F1 scores reaching 0.8-0.9 with around 300 samples.</p>
<p>In our experiment, we compared non-LLM and LLM methods for structured data extraction on 90 samples covering batteries, catalysis, and photovoltaics, alongside ByteScience's results.As shown in Table I, we evaluated Named Entity Recognition (NER), Relation Extraction (RE), and Entity Resolution (ER).While models like MatBERT performed well, they often produced irrelevant entities, lowering precision.In contrast, LLMs handled unstructured information more reliably, and our system outperformed traditional methods across all tasks with fewer samples.A. Initial Setup: Schema, Semi-Annotation, Model Fine-Tune Thomas configures ByteScience to meet his research needs by designing a custom annotation schema for alloy synthesis, annotating key details like compositions, casting parameters, solution treatment, and aging variables.ByteScience then initiates semi-automatic annotation, where the DARWIN LLM auto-labels papers from his corpus based on this schema.Thomas reviews and corrects the annotations to refine the model's understanding.Afterward, ByteScience fine-tunes the LLM using AWS SageMaker, optimizing it for alloy synthesis data extraction.The fine-tuned model is deployed to a SageMaker Endpoint for efficient, large-scale processing of complex scientific papers.</p>
<p>B. Data Generation: Document Upload, Endpoint Utilization, and Dataset Creation</p>
<p>With the fine-tuned model, Thomas uploads his entire corpus of scientific papers to ByteScience, which processes various formats for comprehensive coverage.He initiates large-scale data extraction via the SageMaker Endpoint, where the model extracts detailed information on alloy compositions, casting processes, solution treatments, and aging procedures.This automation accelerates his research, completing in days what would have taken months manually.The extracted data is structured and stored in MongoDB, allowing Thomas to easily query, analyze, and identify trends in alloy synthesis, uncovering insights that manual review might have missed.</p>
<p>C. Further Dataset Updates and Refinement</p>
<p>As Thomas advances in his research, he updates his dataset with ByteScience, uploading new papers and processing them through the fine-tuned model to continually enrich his dataset.When discrepancies or improvements are needed, he initiates a re-training cycle, reviewing and correcting a subset of the newly processed papers to further fine-tune the model.This iterative process ensures the model stays accurate and adapts to evolving terminologies or methods in alloy synthesis.Through this dynamic interaction, Thomas maintains an up-to-date, accurate dataset, enhancing his research and keeping him at the forefront of alloy synthesis advancements.</p>
<p>VI. SIGNIFICANCE TO SCIENCE</p>
<p>Constructing databases from scholarly literature is crucial for modern research, but traditional methods are timeconsuming and resource-intensive.ByteScience transforms this process by enabling users to create a customized data extraction tool in hours, achieving 80%-90% human accuracy.It can process a 10-page scientific document in one second, compared to the 20-30 minutes it takes a researcher.With an extraction cost of just $0.023 per paper for 10,000 articles, ByteScience makes large-scale data extraction affordable and accessible.Its versatility across scientific fields democratizes access to advanced data extraction, providing computational power equivalent to hundreds of annotators.This accelerates discovery, enhances research decision-making, and fosters innovation across disciplines.</p>
<p>VII. CONCLUSION</p>
<p>ByteScience is leveraging a powerful approach to handle unstructured text by fine-tuning DARWIN, a pre-trained natural science LLM, using a minimal set of annotated articles.Hosted on the AWS cloud, this platform automates the process of extracting structured data from scientific texts, presenting a zero-code solution that could significantly enhance efficiency in natural science research.The key advantage of ByteScience lies in its ability to train the DARWIN model with few annotations, making it exceptionally adaptive and efficient.This capability ensures that the extracted material data is high-quality and highly accurate.ByteScience exemplifies how cutting-edge technology can be harnessed to propel advancements in science, engineering, and research by integrating advanced NLP techniques with cloud computing.This initiative represents a substantial step forward in making vast scientific corpora more accessible and usable, highlighting the transformative potential of AI in scientific data processing.To optimize resource efficiency, we are developing a slicing version that fine-tunes a low-resource inference model using only partial data from extensive content.APPENDIX Figure 3 shows the label-defining function in our system.Users define the structure for annotations, including entity labels and their relationships (visualized by indent).For each label, there is a definition textbox for filling.</p>
<p>Figure 4 shows the annotation function.Users can use automatic pre-labelling to the selected texts.Different colors will visualize the auto-labeled annotations and users can review and correct them, ensuring accuracy and consistency.</p>
<p>Figure 5 shows the data extraction function on example paper.Users can create a customized data extraction tool that achieves 80%-90% of human extraction accuracy after just a few hours of annotation.</p>
<p>911</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Fig. 1 .
1
Fig.1.ByteScience Pipeline.The initial setup for a specific field involves constructing a domain-specific corpus of structured scientific data (Green Pipeline) and fine-tuning an LLM on this dataset to optimize performance for the target scientific domain (Blue Pipeline).Once this setup is complete, users can efficiently generate structured datasets from new scientific documents in the same field by utilizing the fine-tuned LLM stored in AWS.</p>
<p>5 )
5
Correction: Users review and correct the auto-labeled annotations, ensuring accuracy and consistency.6) Training: Corrected annotations are used to train or finetune an LLM, with training done via Amazon SageMaker.7) Fine-Tuned LLM: The training process results in a finetuned LLM customized for the specific annotation task.8) Structured Data Generation: The fine-tuned LLM processes new documents into structured data stored in Mon-goDB as JSON, allowing flexible use and efficient querying.After uploading the training dataset, it is transformed into the LLM's instruction format for fine-tuning.Users should first test a small text subset and assess accuracy and recall.If accuracy is low, add more annotated data and retrain.For low recall, generate more corpus data and use sequential learning to train a new model.</p>
<p>Fig. 2 .
2
Fig. 2. The architecture of ByteScience creates a structured database on AWS cloud with LLM.</p>
<p>3 )
3
The SageMaker Notebook is used for model development and improvement.4) Training datasets on S3 are used to fine-tune the model via SageMaker Training Jobs.5) The resulting model is deployed to a SageMaker Endpoint.6) Users can then perform data extraction tasks, processing requests by the fine-tuned LLM.</p>
<p>Fig. 3 .
3
Fig. 3. Screenshot of label setup.</p>
<p>Fig. 4 .
4
Fig. 4. Screenshot of labeling page.</p>
<p>Fig. 5 .
5
Fig. 5. Screenshot of extraction results of a paper.</p>
<p>TABLE I RESULT
I
OF STRUCTURED DATA EXTRACTION.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
V. BYTESCIENCE IN ACTION: A USER CASE STUDYTo showcase ByteScience's application, we present Thomas, a materials scientist automating alloy synthesis by analyzing literature to establish "Composition-Processing-Structure-Performance" (CPSP) relationships.He designs alloy compositions, develops processing methods, and predicts microstructures using data on casting, solution treatment, and aging.
R Stevens, V Taylor, J Nichols, A B Maccabe, K Yelick, D Brown, AI for Science: Report on the Department of Energy (DOE) Town Halls on Artificial Intelligence (AI) for Science. Argonne, IL (United StatesFeb. 2020Tech. Rep. ANL-20/17</p>
<p>Applications of machine learning in drug discovery and development. J Vamathevan, D Clark, P Czodrowski, I Dunham, E Ferran, G Lee, B Li, A Madabhushi, P Shah, M Spitzer, S Zhao, Nature Reviews Drug Discovery. 186Jun. 2019Nature Publishing Group</p>
<p>Machine learning for functional protein design | Nature Biotechnology. </p>
<p>Opinion Mining by Convolutional Neural Networks for Maximizing Discoverability of Nanomaterials. T Xie, Y Wan, H Wang, I Østrøm, S Wang, M He, R Deng, X Wu, C Grazian, C Kit, B Hoex, 10.1021/acs.jcim.3c00746Journal of Chemical Information and Modeling. 647Apr. 2024</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, Others, APL materials. 112013AIP Publishing</p>
<p>The NOMAD laboratory: from data sharing to artificial intelligence. C Draxl, M Scheffler, Journal of Physics: Materials. 23360012019IOP Publishing</p>
<p>MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning. V Venugopal, S Pai, E Olivetti, arXiv:2210.17340Oct. 2022cond-mat</p>
<p>DARWIN Series: Domain Specific Large Language Models for Natural Science. T Xie, Y Wan, W Huang, Z Yin, Y Liu, S Wang, Q Linghu, C Kit, C Grazian, W Zhang, Others, arXiv:2308.135652023arXiv preprint</p>
<p>Pdfminer: Python pdf parser and analyzer. Y Shinyama, Retrieved on. 112015</p>
<p>Structured information extraction from complex scientific text with fine-tuned large language models. A Dunn, J Dagdelen, N Walker, S Lee, A S Rosen, G Ceder, K Persson, A Jain, arXiv:2212.052382022</p>            </div>
        </div>

    </div>
</body>
</html>
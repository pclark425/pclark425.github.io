<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-457 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-457</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-457</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-267061230</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.10444v1.pdf" target="_blank">Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?</a></p>
                <p><strong>Paper Abstract:</strong> The paper discusses what is needed to address the limitations of current LLM-centered AI systems. The paper argues that incorporating insights from human cognition and psychology, as embodied by a computational cognitive architecture, can help develop systems that are more capable, more reliable, and more human-like. It emphasizes the importance of the dual-process architecture and the hybrid neuro-symbolic approach in addressing the limitations of current LLMs. In the opposite direction, the paper also highlights the need for an overhaul of computational cognitive architectures to better reflect advances in AI and computing technology. Overall, the paper advocates for a multidisciplinary, mutually beneficial approach towards developing better models both for AI and for understanding the human mind.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e457.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e457.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clarion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clarion cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive dual-process, dual-representation cognitive architecture with four subsystems (action-centered ACS, non-action-centered NACS, motivational MS, and metacognitive MCS) where each subsystem has an explicit symbolic top level and an implicit neural bottom level linked together for interaction, learning, and control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Clarion (original dual-process neuro-symbolic architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Clarion is a psychologically-grounded, mechanistic cognitive architecture that models human cognition via four interacting subsystems (ACS, NACS, MS, MCS). Each subsystem has two levels: a top-level explicit symbolic representation (chunks and rules) for deliberative, rule-based processing, and a bottom-level implicit neural representation for associative, procedural processing. The two levels are linked so symbolic chunks/rules are associated with distributed neural encodings, enabling top-down activation (explicit→implicit), bottom-up activation (implicit→explicit), top-down learning (symbolic→implicit), and bottom-up learning (implicit→symbolic). Clarion supports memory (semantic, procedural, episodic, working), motivation-driven goal selection, metacognitive regulation, and incremental real-time learning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Top-level symbolic 'chunks' (dimension-value pair structures) and explicit rules (if-then style) representing declarative semantic and procedural knowledge, used for symbolic reasoning, planning, and explicit memory.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Bottom-level neural networks / connectionist representations (historically backprop networks; in conception any subsymbolic associative/continuously parameterized model) that store implicit semantic/procedural knowledge and perform fast associative/intuition-like processing.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Dual-representation linking: each symbolic chunk/rule is mapped to corresponding neural representations; interaction via top-down activation (explicit prompts/activations initiate implicit processes), bottom-up activation (implicit patterns activate symbolic structures), and bidirectional learning (top-down assimilation of symbolic knowledge into implicit weights; bottom-up extraction of symbolic rules from implicit experience). Modular subsystems communicate via explicit interfaces (e.g., chunks/rules, episodic records).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Synergistic behaviors including: integration of fast associative (intuition/instinct) processing with slow symbolic deliberation; flexible switching between implicit and explicit control; incremental instance-based and rule-based learning; metacognitive regulation of effort and strategy via motivational subsystem; explicit episodic trace formation supporting later retrieval and learning — capabilities that neither pure symbolic nor pure neural alone fully provides in the architecture's validated psychological simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General cognitive tasks modeled in cognitive science (reasoning, skill learning, planning, action selection, memory, metacognition). (No single numeric benchmark reported in this paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Clarion's hybrid structure supports both procedure-like generalization from implicit associative networks and systematic, compositional generalization from explicit rules; bottom-up rule extraction and top-down assimilation allow learning to generalize across instances while retaining explicit, verifiable procedures. The paper argues this structure supports better human-like generalization (qualitative claim), but provides no quantitative OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability at the symbolic top level (chunks and explicit rules provide inspectable explanations and justifications); implicit bottom-level behavior can be inspected only indirectly (e.g., via extracted rules or episodic traces). The explicit layer enables explanations, verification, and tool use, while the implicit layer supplies intuitive content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Original Clarion (pre-LLM era) limited by older implementation technologies and scale; historically restricted to relatively small laboratory tasks; did not leverage massive pretraining or modern LLM capabilities. Integration complexity and engineering overhead for linking large-scale subsymbolic models to explicit representations are noted as practical challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Dual-process theory (System 1 implicit vs System 2 explicit) and complementary division-of-labor: associative/subsymbolic systems provide fast, context-rich intuition/instinct while symbolic systems provide explicit, rule-based deliberation, metacognition, and precise manipulation; formalized as bidirectional activation and bidirectional learning mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e457.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e457.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clarion+LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clarion cognitive architecture with LLMs incorporated</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed hybrid system that replaces or augments Clarion's bottom implicit levels with large language models (LLMs) and/or multimodal transformers, using LLMs as implicit semantic/procedural/episodic memory and intuition/instinct providers while retaining Clarion's explicit symbolic top levels for reasoning, planning, and metacognitive control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Clarion (LLM-integrated hybrid reasoning system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An engineered hybrid architecture in which multimodal LLMs occupy the bottom implicit levels of Clarion's subsystems (ACS/NACS/MS), serving as implicit semantic/procedural memory, intuition/instinct generators, perceptual-motor processing proxies, and condensed episodic stores; the top-level Clarion symbolic components (chunks, rules, goal selection, metacognition) issue prompts (top-down activations) to LLMs and verify/refine/override LLM outputs (bottom-up activations). Episodic explicit memory retrieval provides context to LLM prompts; LLMs can summarize/condense episodes into implicit representations. The motivational subsystem provides drives and goals that regulate when to invoke LLMs versus symbolic reasoning. The architecture supports prompting, iterative prompting (inner dialogue), extraction of symbolic content from LLM outputs, and continual top-down/bottom-up learning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Clarion's top-level explicit symbolic representations: chunks (dimension-value vectors used as conditions/conclusions) and production-like rules encoding explicit semantic and procedural knowledge, plus explicit episodic and working memory stores.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Large language models / multimodal Transformers (LLMs) used as bottom-level implicit processors for intuition, implicit memory, perceptual and motor description generation, action sequence prediction, and implicit metacognitive reflection; they may be complemented by other neural modules for episodic summarization and embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular coupling via prompts and representational translation: explicit symbolic modules construct prompts (top-down activation) that the LLM consumes; LLM outputs are parsed back into Clarion internal representations (chunks, linguistic descriptions, or embeddings) enabling bottom-up activation. Integration also includes retrieval-augmented prompting (episodic retrieval supplies context), conditional invocation policies governed by the MCS/MS, dual-direction learning (LLM-generated candidate rules can be promoted to symbolic rules; symbolic rules can guide LLM fine-tuning or be used to create synthetic prompts). No end-to-end differentiable linkage is claimed — integration is modular and interaction-driven (prompting, retrieval, rule extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Qualitatively emergent capabilities include: much broader, data-driven implicit knowledge (LLM-provided intuition/commonsense) combined with precise symbolic manipulation for rigorous reasoning; improved planning and multi-step reasoning via symbolic guidance and iterative prompting (e.g., Chain/Tree/Algorithm of Thought templates invoked by symbolic processes); richer episodic/memory behaviors (explicit episodic retrieval + LLM summarization) enabling contextualized responses; motivation-driven autonomy and consistent behavior over time due to MS/MCS regulation; better grounding and communicative abilities by leveraging LLM language fluency together with explicit verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Discussed domains include reasoning (logical and creative problem solving), planning and multi-step action selection, episodic and working memory tasks, grounding and natural language communication; no standard benchmark with numeric results is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Projected qualitative improvement: LLM implicit knowledge gives broad world coverage and statistical generalization, while symbolic top-level offers systematicity and compositional generalization for rule-governed tasks; combination expected to generalize better across open-world and OOD scenarios than either alone, though the paper presents conceptual arguments rather than quantitative OOD tests.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic top-level provides inspectable decision traces, explicit rules, and episodic logs enabling explanations; LLM outputs can be checked and revised by symbolic modules, improving explainability compared to bare LLMs. The architecture enables producing human-interpretable rationales (via explicit rules/episodic retrieval) and auditing of decisions by examining top-level activations and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No quantitative evaluations provided; practical challenges include aligning LLM outputs reliably with explicit internal representations (prompt engineering and parsing), limited LLM working-memory/episodic retention requiring external retrieval structures, potential brittleness from mis-specified prompts, scale/engineering complexity for integrating large subsystems, and unresolved questions about end-to-end learning or catastrophic inconsistencies between layers.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Dual-process neuro-symbolic framework instantiated in Clarion: division of labor where LLMs implement System 1 implicit processing (intuition/instinct/memory) and symbolic top-level implements System 2 explicit reasoning, linked by top-down/bottom-up activations, and governed by motivational/metacognitive principles that allocate effort and select processing modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e457.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e457.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid neuro-symbolic (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid neuro-symbolic / dual-process systems (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A broad class of systems that combine symbolic, declarative rule/logic-based top-level processing with subsymbolic neural, procedural, or LLM-based components to capture both explicit deliberation and implicit intuition/instinct, grounded in dual-process psychological theory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid neuro-symbolic (dual-process) reasoning systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Conceptual class of architectures integrating symbolic declarative methods (logic, rules, chunks, explicit memories) with procedural/neural components (deep networks, LLMs, reinforcement learning) to exploit complementary strengths: symbolic precision, verifiability, and planning; neural coverage, perception, and statistical generalization. Integration can be modular (symbolic controller + neural subroutines), iterative (prompts/callouts), or tightly coupled (rule extraction, symbolic supervision of neural learning). Such systems aim to address shortcomings of purely neural LLM-centered systems (planning, memory, alignment, reliability) while updating cognitive architectures with modern AI components.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic modules: rule systems, logical inference procedures, explicit episodic/semantic/working memory representations, planning/search algorithms (e.g., forward/backward chaining, constraint satisfaction), and knowledge chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural modules: LLMs (Transformers), multimodal transformers for perception, neural episodic summarizers, sequence models for actions (e.g., transformers for action prediction), and reinforcement learning controllers when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Multiple integration paradigms discussed: modular architectures with symbolic top-level invoking neural subroutines via prompts or APIs; retrieval-augmented prompting (explicit episodic memory retrieval passed to neural models); iterative prompting and inner dialogue (top-down activation); bottom-up extraction of symbolic rules or templates from neural outputs; mixed learning regimes (top-down teaching, bottom-up rule induction); selection policies regulated by motivation/metacognition. The paper emphasizes modular, interaction-driven integration (prompting and representational mapping) rather than end-to-end differentiable fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Expected emergent behaviors include improved robustness and trustworthiness (symbolic verification of neural outputs), enhanced planning and multi-step problem solving (symbolic templates guiding neural rollout), richer memory (explicit episodic traces plus implicit condensations), intrinsic motivation and autonomous goal-directed behavior (via motivational modules), and increased human-alignment due to explicit values/constraints encoded symbolically.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned domains: logical reasoning, planning, creative insight tasks, grounded language, episodic recall, action selection; no specific benchmark numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Qualitatively argued to combine neural statistical generalization (broad world knowledge, interpolation/extrapolation) with symbolic systematicity and compositionality, supporting better OOD and compositional generalization in principle; no empirical metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic portion can provide explicit explanations and constraints, improving interpretability and enabling auditability of decisions; neural portion remains less transparent although can be made more interpretable via extracted symbolic rules and explicit episodic/log traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Practical integration issues: aligning representational formats, prompt engineering brittleness, limited working-memory in LLMs, lack of standardized end-to-end training for hybrids, and paucity of quantitative comparative evaluations; also cognitive architectures may need overhaul to exploit modern LLM scale and modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Dual-process theory and complementary-strengths principle: symbolic and neural methods offer orthogonal capabilities (precision vs coverage); the architecture design principle is to allocate tasks to the component that best matches their computational affordances and to mediate interaction via top-down/bottom-up mechanisms, motivational regulation, and episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e457.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e457.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-guided hybrid reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-guided integration (Chain of Thought / Tree of Thought / Algorithm of Thought-style guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integration pattern where explicit symbolic processes (templates, search strategies, planning algorithms) guide LLMs via structured prompts or iterative prompting to produce stepwise reasoning, or where LLM outputs are iteratively refined by symbolic controllers implementing search/control strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prompt-guided hybrid reasoning (explicit→LLM prompting pattern)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A practical integration method in which the symbolic/top-level component creates structured prompts (e.g., Chain of Thought, Tree of Thought, Algorithm of Thought, search templates like depth-first/breadth-first/A*) to trigger desired stepwise reasoning behaviors in LLMs; the symbolic layer may iterate on LLM outputs, perform verification, choose alternative branches, or supply additional constraints; retrieval-augmented prompting and episodic context are also used to ground and steer LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic templates, search/control strategies (depth-first, breadth-first, A*), planning schemas, explicit rules that prescribe the form and constraints of reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>LLMs (Transformers) used as stepwise generators / subroutines that produce candidate steps, inferences, or plan fragments when prompted; may be coupled with neural retrieval or summarization modules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Controller-driven modular integration: symbolic controller composes prompts and interprets/parses LLM outputs; iterative loop where controller checks LLM output, performs symbolic verification or pruning, and re-prompts as needed (implements top-down activation and bottom-up validation). Uses templates and search strategies implemented symbolically to orchestrate multiple LLM calls (multi-turn prompting or tree search).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables LLMs to perform more 'slow', deliberative multi-step reasoning and planning by externalizing control to symbolic modules; can yield improved correctness in constrained reasoning tasks, ability to backtrack and explore alternatives, and emergence of explicit reasoning traces that can be audited — capabilities not reliably present in single-shot LLM generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multi-step reasoning benchmarks, planning problems, insight problems (creative problem solving); specific instances referenced qualitatively (e.g., solving geometric 3D tree-placement insight problem with prompting). No numeric benchmark reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Said to improve structured, discontinuous, and multi-step reasoning (including creative insight) by imposing symbolic templates and search control over an otherwise greedy neural generator; expected to yield better compositional and systematic behavior on complex reasoning tasks relative to bare LLM prompting alone, though no quantitative evidence provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Produces stepwise, human-readable chains/trees of reasoning that can be inspected and verified by symbolic modules, improving explainability compared to unstructured LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Depends on prompt engineering and controller heuristics; brittleness when prompts are poorly specified; heavy computational cost from multiple LLM calls (search/branching); still vulnerable to LLM hallucination and parsing errors; no end-to-end guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Controller-orchestration view within dual-process theory: symbolic control structures orchestrate LLM (System 1) operations to produce System 2-like outputs, leveraging search and verification to compensate for neural model greediness and lack of deliberative backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Integrating Rules and Connectionism for Robust Commonsense Reasoning <em>(Rating: 2)</em></li>
                <li>Synergistic integration of large language models and cognitive architectures for robust AI: An exploratory analysis <em>(Rating: 2)</em></li>
                <li>SwiftSage: a generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Cognitive architectures for language agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-457",
    "paper_id": "paper-267061230",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "Clarion",
            "name_full": "Clarion cognitive architecture",
            "brief_description": "A comprehensive dual-process, dual-representation cognitive architecture with four subsystems (action-centered ACS, non-action-centered NACS, motivational MS, and metacognitive MCS) where each subsystem has an explicit symbolic top level and an implicit neural bottom level linked together for interaction, learning, and control.",
            "citation_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
            "mention_or_use": "use",
            "system_name": "Clarion (original dual-process neuro-symbolic architecture)",
            "system_description": "Clarion is a psychologically-grounded, mechanistic cognitive architecture that models human cognition via four interacting subsystems (ACS, NACS, MS, MCS). Each subsystem has two levels: a top-level explicit symbolic representation (chunks and rules) for deliberative, rule-based processing, and a bottom-level implicit neural representation for associative, procedural processing. The two levels are linked so symbolic chunks/rules are associated with distributed neural encodings, enabling top-down activation (explicit→implicit), bottom-up activation (implicit→explicit), top-down learning (symbolic→implicit), and bottom-up learning (implicit→symbolic). Clarion supports memory (semantic, procedural, episodic, working), motivation-driven goal selection, metacognitive regulation, and incremental real-time learning.",
            "declarative_component": "Top-level symbolic 'chunks' (dimension-value pair structures) and explicit rules (if-then style) representing declarative semantic and procedural knowledge, used for symbolic reasoning, planning, and explicit memory.",
            "imperative_component": "Bottom-level neural networks / connectionist representations (historically backprop networks; in conception any subsymbolic associative/continuously parameterized model) that store implicit semantic/procedural knowledge and perform fast associative/intuition-like processing.",
            "integration_method": "Dual-representation linking: each symbolic chunk/rule is mapped to corresponding neural representations; interaction via top-down activation (explicit prompts/activations initiate implicit processes), bottom-up activation (implicit patterns activate symbolic structures), and bidirectional learning (top-down assimilation of symbolic knowledge into implicit weights; bottom-up extraction of symbolic rules from implicit experience). Modular subsystems communicate via explicit interfaces (e.g., chunks/rules, episodic records).",
            "emergent_properties": "Synergistic behaviors including: integration of fast associative (intuition/instinct) processing with slow symbolic deliberation; flexible switching between implicit and explicit control; incremental instance-based and rule-based learning; metacognitive regulation of effort and strategy via motivational subsystem; explicit episodic trace formation supporting later retrieval and learning — capabilities that neither pure symbolic nor pure neural alone fully provides in the architecture's validated psychological simulations.",
            "task_or_benchmark": "General cognitive tasks modeled in cognitive science (reasoning, skill learning, planning, action selection, memory, metacognition). (No single numeric benchmark reported in this paper.)",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Clarion's hybrid structure supports both procedure-like generalization from implicit associative networks and systematic, compositional generalization from explicit rules; bottom-up rule extraction and top-down assimilation allow learning to generalize across instances while retaining explicit, verifiable procedures. The paper argues this structure supports better human-like generalization (qualitative claim), but provides no quantitative OOD metrics.",
            "interpretability_properties": "High interpretability at the symbolic top level (chunks and explicit rules provide inspectable explanations and justifications); implicit bottom-level behavior can be inspected only indirectly (e.g., via extracted rules or episodic traces). The explicit layer enables explanations, verification, and tool use, while the implicit layer supplies intuitive content.",
            "limitations_or_failures": "Original Clarion (pre-LLM era) limited by older implementation technologies and scale; historically restricted to relatively small laboratory tasks; did not leverage massive pretraining or modern LLM capabilities. Integration complexity and engineering overhead for linking large-scale subsymbolic models to explicit representations are noted as practical challenges.",
            "theoretical_framework": "Dual-process theory (System 1 implicit vs System 2 explicit) and complementary division-of-labor: associative/subsymbolic systems provide fast, context-rich intuition/instinct while symbolic systems provide explicit, rule-based deliberation, metacognition, and precise manipulation; formalized as bidirectional activation and bidirectional learning mechanisms.",
            "uuid": "e457.0",
            "source_info": {
                "paper_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Clarion+LLMs",
            "name_full": "Clarion cognitive architecture with LLMs incorporated",
            "brief_description": "A proposed hybrid system that replaces or augments Clarion's bottom implicit levels with large language models (LLMs) and/or multimodal transformers, using LLMs as implicit semantic/procedural/episodic memory and intuition/instinct providers while retaining Clarion's explicit symbolic top levels for reasoning, planning, and metacognitive control.",
            "citation_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
            "mention_or_use": "use",
            "system_name": "Clarion (LLM-integrated hybrid reasoning system)",
            "system_description": "An engineered hybrid architecture in which multimodal LLMs occupy the bottom implicit levels of Clarion's subsystems (ACS/NACS/MS), serving as implicit semantic/procedural memory, intuition/instinct generators, perceptual-motor processing proxies, and condensed episodic stores; the top-level Clarion symbolic components (chunks, rules, goal selection, metacognition) issue prompts (top-down activations) to LLMs and verify/refine/override LLM outputs (bottom-up activations). Episodic explicit memory retrieval provides context to LLM prompts; LLMs can summarize/condense episodes into implicit representations. The motivational subsystem provides drives and goals that regulate when to invoke LLMs versus symbolic reasoning. The architecture supports prompting, iterative prompting (inner dialogue), extraction of symbolic content from LLM outputs, and continual top-down/bottom-up learning.",
            "declarative_component": "Clarion's top-level explicit symbolic representations: chunks (dimension-value vectors used as conditions/conclusions) and production-like rules encoding explicit semantic and procedural knowledge, plus explicit episodic and working memory stores.",
            "imperative_component": "Large language models / multimodal Transformers (LLMs) used as bottom-level implicit processors for intuition, implicit memory, perceptual and motor description generation, action sequence prediction, and implicit metacognitive reflection; they may be complemented by other neural modules for episodic summarization and embeddings.",
            "integration_method": "Modular coupling via prompts and representational translation: explicit symbolic modules construct prompts (top-down activation) that the LLM consumes; LLM outputs are parsed back into Clarion internal representations (chunks, linguistic descriptions, or embeddings) enabling bottom-up activation. Integration also includes retrieval-augmented prompting (episodic retrieval supplies context), conditional invocation policies governed by the MCS/MS, dual-direction learning (LLM-generated candidate rules can be promoted to symbolic rules; symbolic rules can guide LLM fine-tuning or be used to create synthetic prompts). No end-to-end differentiable linkage is claimed — integration is modular and interaction-driven (prompting, retrieval, rule extraction).",
            "emergent_properties": "Qualitatively emergent capabilities include: much broader, data-driven implicit knowledge (LLM-provided intuition/commonsense) combined with precise symbolic manipulation for rigorous reasoning; improved planning and multi-step reasoning via symbolic guidance and iterative prompting (e.g., Chain/Tree/Algorithm of Thought templates invoked by symbolic processes); richer episodic/memory behaviors (explicit episodic retrieval + LLM summarization) enabling contextualized responses; motivation-driven autonomy and consistent behavior over time due to MS/MCS regulation; better grounding and communicative abilities by leveraging LLM language fluency together with explicit verification.",
            "task_or_benchmark": "Discussed domains include reasoning (logical and creative problem solving), planning and multi-step action selection, episodic and working memory tasks, grounding and natural language communication; no standard benchmark with numeric results is reported in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Projected qualitative improvement: LLM implicit knowledge gives broad world coverage and statistical generalization, while symbolic top-level offers systematicity and compositional generalization for rule-governed tasks; combination expected to generalize better across open-world and OOD scenarios than either alone, though the paper presents conceptual arguments rather than quantitative OOD tests.",
            "interpretability_properties": "Symbolic top-level provides inspectable decision traces, explicit rules, and episodic logs enabling explanations; LLM outputs can be checked and revised by symbolic modules, improving explainability compared to bare LLMs. The architecture enables producing human-interpretable rationales (via explicit rules/episodic retrieval) and auditing of decisions by examining top-level activations and rules.",
            "limitations_or_failures": "No quantitative evaluations provided; practical challenges include aligning LLM outputs reliably with explicit internal representations (prompt engineering and parsing), limited LLM working-memory/episodic retention requiring external retrieval structures, potential brittleness from mis-specified prompts, scale/engineering complexity for integrating large subsystems, and unresolved questions about end-to-end learning or catastrophic inconsistencies between layers.",
            "theoretical_framework": "Dual-process neuro-symbolic framework instantiated in Clarion: division of labor where LLMs implement System 1 implicit processing (intuition/instinct/memory) and symbolic top-level implements System 2 explicit reasoning, linked by top-down/bottom-up activations, and governed by motivational/metacognitive principles that allocate effort and select processing modes.",
            "uuid": "e457.1",
            "source_info": {
                "paper_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Hybrid neuro-symbolic (general)",
            "name_full": "Hybrid neuro-symbolic / dual-process systems (general concept)",
            "brief_description": "A broad class of systems that combine symbolic, declarative rule/logic-based top-level processing with subsymbolic neural, procedural, or LLM-based components to capture both explicit deliberation and implicit intuition/instinct, grounded in dual-process psychological theory.",
            "citation_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
            "mention_or_use": "mention",
            "system_name": "Hybrid neuro-symbolic (dual-process) reasoning systems",
            "system_description": "Conceptual class of architectures integrating symbolic declarative methods (logic, rules, chunks, explicit memories) with procedural/neural components (deep networks, LLMs, reinforcement learning) to exploit complementary strengths: symbolic precision, verifiability, and planning; neural coverage, perception, and statistical generalization. Integration can be modular (symbolic controller + neural subroutines), iterative (prompts/callouts), or tightly coupled (rule extraction, symbolic supervision of neural learning). Such systems aim to address shortcomings of purely neural LLM-centered systems (planning, memory, alignment, reliability) while updating cognitive architectures with modern AI components.",
            "declarative_component": "Symbolic modules: rule systems, logical inference procedures, explicit episodic/semantic/working memory representations, planning/search algorithms (e.g., forward/backward chaining, constraint satisfaction), and knowledge chunks.",
            "imperative_component": "Neural modules: LLMs (Transformers), multimodal transformers for perception, neural episodic summarizers, sequence models for actions (e.g., transformers for action prediction), and reinforcement learning controllers when needed.",
            "integration_method": "Multiple integration paradigms discussed: modular architectures with symbolic top-level invoking neural subroutines via prompts or APIs; retrieval-augmented prompting (explicit episodic memory retrieval passed to neural models); iterative prompting and inner dialogue (top-down activation); bottom-up extraction of symbolic rules or templates from neural outputs; mixed learning regimes (top-down teaching, bottom-up rule induction); selection policies regulated by motivation/metacognition. The paper emphasizes modular, interaction-driven integration (prompting and representational mapping) rather than end-to-end differentiable fusion.",
            "emergent_properties": "Expected emergent behaviors include improved robustness and trustworthiness (symbolic verification of neural outputs), enhanced planning and multi-step problem solving (symbolic templates guiding neural rollout), richer memory (explicit episodic traces plus implicit condensations), intrinsic motivation and autonomous goal-directed behavior (via motivational modules), and increased human-alignment due to explicit values/constraints encoded symbolically.",
            "task_or_benchmark": "Mentioned domains: logical reasoning, planning, creative insight tasks, grounded language, episodic recall, action selection; no specific benchmark numbers provided in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Qualitatively argued to combine neural statistical generalization (broad world knowledge, interpolation/extrapolation) with symbolic systematicity and compositionality, supporting better OOD and compositional generalization in principle; no empirical metrics in this paper.",
            "interpretability_properties": "Symbolic portion can provide explicit explanations and constraints, improving interpretability and enabling auditability of decisions; neural portion remains less transparent although can be made more interpretable via extracted symbolic rules and explicit episodic/log traces.",
            "limitations_or_failures": "Practical integration issues: aligning representational formats, prompt engineering brittleness, limited working-memory in LLMs, lack of standardized end-to-end training for hybrids, and paucity of quantitative comparative evaluations; also cognitive architectures may need overhaul to exploit modern LLM scale and modalities.",
            "theoretical_framework": "Dual-process theory and complementary-strengths principle: symbolic and neural methods offer orthogonal capabilities (precision vs coverage); the architecture design principle is to allocate tasks to the component that best matches their computational affordances and to mediate interaction via top-down/bottom-up mechanisms, motivational regulation, and episodic memory.",
            "uuid": "e457.2",
            "source_info": {
                "paper_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Prompt-guided hybrid reasoning",
            "name_full": "Prompt-guided integration (Chain of Thought / Tree of Thought / Algorithm of Thought-style guidance)",
            "brief_description": "An integration pattern where explicit symbolic processes (templates, search strategies, planning algorithms) guide LLMs via structured prompts or iterative prompting to produce stepwise reasoning, or where LLM outputs are iteratively refined by symbolic controllers implementing search/control strategies.",
            "citation_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
            "mention_or_use": "mention",
            "system_name": "Prompt-guided hybrid reasoning (explicit→LLM prompting pattern)",
            "system_description": "A practical integration method in which the symbolic/top-level component creates structured prompts (e.g., Chain of Thought, Tree of Thought, Algorithm of Thought, search templates like depth-first/breadth-first/A*) to trigger desired stepwise reasoning behaviors in LLMs; the symbolic layer may iterate on LLM outputs, perform verification, choose alternative branches, or supply additional constraints; retrieval-augmented prompting and episodic context are also used to ground and steer LLM responses.",
            "declarative_component": "Symbolic templates, search/control strategies (depth-first, breadth-first, A*), planning schemas, explicit rules that prescribe the form and constraints of reasoning steps.",
            "imperative_component": "LLMs (Transformers) used as stepwise generators / subroutines that produce candidate steps, inferences, or plan fragments when prompted; may be coupled with neural retrieval or summarization modules.",
            "integration_method": "Controller-driven modular integration: symbolic controller composes prompts and interprets/parses LLM outputs; iterative loop where controller checks LLM output, performs symbolic verification or pruning, and re-prompts as needed (implements top-down activation and bottom-up validation). Uses templates and search strategies implemented symbolically to orchestrate multiple LLM calls (multi-turn prompting or tree search).",
            "emergent_properties": "Enables LLMs to perform more 'slow', deliberative multi-step reasoning and planning by externalizing control to symbolic modules; can yield improved correctness in constrained reasoning tasks, ability to backtrack and explore alternatives, and emergence of explicit reasoning traces that can be audited — capabilities not reliably present in single-shot LLM generation.",
            "task_or_benchmark": "Multi-step reasoning benchmarks, planning problems, insight problems (creative problem solving); specific instances referenced qualitatively (e.g., solving geometric 3D tree-placement insight problem with prompting). No numeric benchmark reported in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Said to improve structured, discontinuous, and multi-step reasoning (including creative insight) by imposing symbolic templates and search control over an otherwise greedy neural generator; expected to yield better compositional and systematic behavior on complex reasoning tasks relative to bare LLM prompting alone, though no quantitative evidence provided here.",
            "interpretability_properties": "Produces stepwise, human-readable chains/trees of reasoning that can be inspected and verified by symbolic modules, improving explainability compared to unstructured LLM outputs.",
            "limitations_or_failures": "Depends on prompt engineering and controller heuristics; brittleness when prompts are poorly specified; heavy computational cost from multiple LLM calls (search/branching); still vulnerable to LLM hallucination and parsing errors; no end-to-end guarantees.",
            "theoretical_framework": "Controller-orchestration view within dual-process theory: symbolic control structures orchestrate LLM (System 1) operations to produce System 2-like outputs, leveraging search and verification to compensate for neural model greediness and lack of deliberative backtracking.",
            "uuid": "e457.3",
            "source_info": {
                "paper_title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Integrating Rules and Connectionism for Robust Commonsense Reasoning",
            "rating": 2,
            "sanitized_title": "integrating_rules_and_connectionism_for_robust_commonsense_reasoning"
        },
        {
            "paper_title": "Synergistic integration of large language models and cognitive architectures for robust AI: An exploratory analysis",
            "rating": 2,
            "sanitized_title": "synergistic_integration_of_large_language_models_and_cognitive_architectures_for_robust_ai_an_exploratory_analysis"
        },
        {
            "paper_title": "SwiftSage: a generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 2,
            "sanitized_title": "swiftsage_a_generative_agent_with_fast_and_slow_thinking_for_complex_interactive_tasks"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Cognitive architectures for language agents",
            "rating": 1,
            "sanitized_title": "cognitive_architectures_for_language_agents"
        }
    ],
    "cost": 0.017675749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?</p>
<p>Ron Sun dr.ron.sun@gmail.com 
Department of Cognitive Science
Rensselaer Polytechnic Institute Troy
12180NYUSA</p>
<p>Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?
C07D72AA5B80122811BF7E9A80E9C5D5LLMcognitive architecturedual processpsychologyAI
The paper discusses what is needed to address the limitations of current LLM-centered AI systems.The paper argues that incorporating insights from human cognition and psychology, as embodied by a computational cognitive architecture, can help develop systems that are more capable, more reliable, and more human-like.It emphasizes the importance of the dual-process architecture and the hybrid neuro-symbolic approach in addressing the limitations of current LLMs.In the opposite direction, the paper also highlights the need for an overhaul of computational cognitive architectures to better reflect advances in AI and computing technology.Overall, the paper advocates for a multidisciplinary, mutually beneficial approach towards developing better models both for AI and for understanding the human mind.</p>
<p>Introduction</p>
<p>AI systems, especially LLMs, trained on a massive amount of data and with a huge number of parameters, have achieved spectacular successes in recent years, in terms of, for example, natural language generation or human-level test performance.These systems have found their way into largescale, real-world applications, in domains ranging from creative writing to autonomous driving, dealing with languages, images, voices, and so on.However, at the same time, unlike humans, current LLM-centered systems (as well as other deep learning models) suffer from some fundamental shortcomings, including limited abstract reasoning or planning capabilities, limited memory, lack of autonomy, lack of human-like generalization, limited reliability and trustworthiness, and so on.For instance, such systems are often not reliable ---producing content that is meaningless or simply false.They may be inconsistent with human preferences, deviating from human values or ethics.When faced with complex problems, LLMs may have difficulties in achieving human-level performance, by way of gathering relevant information, reflecting on successes and failures, decomposing and recomposing problems, and using various cognitive strategies that humans seem to possess.</p>
<p>These shortcomings often stem from not taking human psychology into serious enough consideration in LLMs.We thus should explore how the understanding of the human mind (e.g., from cognitive science and psychology) can help in improving them.This can lead to future systems that are more trustworthy and more aligned with human understanding, human values, and so on (beyond current methods such as reinforcement learning from human feedback).Future systems achieving human-level performance should, almost by necessity, draw inspirations from human psychology and models of the human mind (from computational psychology; Sun, 2023).In the ongoing discourse regarding the limits of LLMs, the debate is often between those who think that more data alone can provide the solution and those who favor incorporating additional methods.From the afore-mentioned perspective, the latter position seems more plausible, but nevertheless it needs to be strictly guided by empirically grounded understanding of the human mind.</p>
<p>That is to say, future systems should be more rigorously structured in a cognitively (psychologically) motivated and justified way, based on the structure and content of human mental processes.Computational cognitive architectures that capture (to some significant extent) human mental processes do exist ---They are empirically (psychologically) grounded, comprehensive computational frameworks for understanding and potentially capturing the human mind (e.g., Sun, 2016).In such architectures, dual processes should be considered, as it has become increasingly evident that they are a key feature of the human mind (Kahneman, 2011;Reber, 1989;Sun, 1994Sun, , 2002Sun, , 2015)).Dual-process computational cognitive architectures tap into psychological details of implicit (roughly, unconscious, intuitive) versus explicit (roughly, conscious, deliberative) psychological processes (i.e., System 1 versus 2).I argued a long time ago (e.g., Sun, 1994) as well as very recently that hybrid neuro-symbolic systems seem necessary to address dual processes, which can also remedy many existing shortcomings of LLMs.That is, to build systems that achieve human-level flexibility, reliability, and robustness, one may need both symbolic and subsymbolic (neural) methods, with each side contributing different capabilities.For example, the subsymbolic side may address lower-level sensory-motor processes and unconscious (implicit) processes.The symbolic side can perform high-level "conscious" reasoning, explicit planning, and deliberative reflection.These two sides may be distinct, but work together synergistically (Sun et al., 2005).Such neuro-symbolic models for tackling dual processes have been worked on since the 1990s (e.g., Sun, 1994).More theoretical arguments can also be found in Sun (2002).</p>
<p>Furthermore, another aspect at least as important as dual processes is essential or intrinsic human needs or motives, which are key elements of the human mind (Murray, 1938).They are the basis of directing and regulating behavior.This aspect is well developed in some cognitive architectures.They are important because they are central to alignment with human intentions, values, and ethics (Bretz &amp; Sun, 2018), as well as for dealing with social interaction (Sun, 2006).Of course, many other aspects of the human mind are also important to LLMs, as will be explicated later.</p>
<p>On the other hand, looking at the issues from the opposite direction, we see that the field of computational cognitive modeling, including computational cognitive architectures, has been in development since at least the 1970s.It is concerned with developing models of the human mind in a psychologically realistic way.Although these models are meant to capture human performance in a range of activities, their actual capabilities have not been keeping up with the advances in new technology.For instance, some cognitive architectures were initially conceived in the 1970s and relied on technology available at that time (outdated by now).Despite the fact that some new ideas and implementations have been incorporated, their overall frameworks, as well as some of their essential details, nevertheless seem out of date fundamentally.Therefore, it may be argued that computational cognitive modeling and cognitive architectures need an overhaul to incorporate, more readily and more seamlessly, new advances in AI and in computing technology.LLMs may serve as the key to this endeavor due to their comprehensive capabilities (as will be elaborated later).Note that here we are concerned with psychologically realistic computational cognitive architectures that are meant to be rigorous models of the human mind and have been validated empirically.</p>
<p>In the remainder of this article, I will investigate what LLMs are fundamentally capable of and what else may be needed.I then argue that computational cognitive architectures may help to enhance LLMs, and a detailed example is presented.Several key aspects are discussed and weaved together.</p>
<p>What Can LLMs Capture?</p>
<p>There are many different LLMs, so it may be hard to generalize.Risking being a little simplistic or over-generalizing, one may consider the notions of intuition and instinct (Cosmides &amp;Tooby, 1994;Dreyfus &amp; Dreyfus, 1986;Sun &amp; Wilson, 2014) in characterizing LLMs (besides implicit processes in general; more on these later).Here, intuition refers to understanding without the involvement of explicit reason (but regardless of whether it is immediate or slow in emergence; Sun, 2015), while instinct refers to relatively fixed patterns of responses to environmental stimuli without involving explicit reason (McFarland, 1989;Sun &amp; Wilson, 2014).One may claim that LLMs correspond roughly to human intuition and instinct (setting aside perception, motor control, language, and other peripheral implicit processes for the time being).</p>
<p>Why should LLMs correspond to intuition and instinct?What characteristics of LLMs make them correspond to intuition and instinct?Let us look into intuition first in some detail.</p>
<p>Human intuition results from implicit (unconscious) processes (Evans &amp; Frankish, 2009;Kahneman, 2011;Reber, 1989).Intuition has often been defined as ''the immediate apprehension of an object by the mind without the intervention of any [explicit] reasoning process'' (Oxford English Dictionary), or ''immediate apprehension or cognition'' (Merriam-Webster Dictionary).One may view intuition as a form of reasoning: Reasoning encompasses explicit processes on the one hand, and implicit processes (intuition) on the other (Sun 1994), both guided by one's motives, needs, and goals.In fact, intuition, as well as insight resulting from intuition, is arguably indispensable elements in the overall process of human thinking and reasoning ---They supplement and guide explicit reasoning (Helie &amp; Sun, 2010).Prior empirical work demonstrating the above can be found in Evans andFrankish (2009), Reber (1989), and so on, while theoretical work (with computational modeling) includes Sun (1994Sun ( , 2015)), Helie &amp; Sun (2010), and so on.</p>
<p>Admittedly, intuition might be, though not necessarily always, beyond what language can express; at least, the process leading up to intuition is often beyond language (Dreyfus &amp; Dreyfus, 1986;Sun, 1994).However, intuition is, at a minimum, partially expressible through language (even though it might not necessarily be completely expressed by language).Given an extremely large amount of linguistic data, it is likely that human intuition about a vast array of subjects may be contained in (or embodied by) it.With an extremely large amount of linguistic training data, it may be argued that LLMs resulting from such training can gain a large range of intuition, somewhat comparable to human intuition.That is, although an individual piece of intuition might not be captured by an individual sentence, a vast number of sentences from a vast number of different sources can conceivably capture a fair amount of intuition collectively.Furthermore, human intuition results from and relies on diverse (likely implicit) commonsense knowledge about how the world works (including, e.g., naive physics, folk psychology, folk sociology, and so on), which is what an extremely large amount of linguistic training data from diverse sources can provide.On top of that, the numerical training process with gradual numerical weight adjustments in training neural networks (including in training Transformers, the basis of LLMs) leads to generalization (including both interpolation and extrapolation), which in turn leads to better intuition of an even broader scope.Thus, conceivably, LLMs can embody human intuition as gleaned and generalized from a large volume of data.</p>
<p>Looking at this issue in another way, in humans, intuition is often obtained through experience, especially a large amount of repeated experience.On the other hand, a large amount of repeated experience is exactly what the training of LLMs with a large amount of linguistic data provides.Through such experience, human-like intuition develops within LLMs.Some may claim that there is a fundamental difference between intuition and language expressing intuition, or between intuition gained from the "real" world and whatever gained from linguistic texts.This claim seems to build a wall separating language and the "real" world unnecessarily and unjustifiably.To any individual, the world is made up of sensory-motor, linguistic, and other experiences, which are closely entangled and often mirroring each other (Pavlik, 2023).Word meanings may emerge from use patterns (Durt et al., 2023), so a proximate function such as next-token prediction may give rise to an ultimate function such as intuition (Mollo &amp; Millière, 2023).One may gain intuition from any or all of these modalities (see "grounding" later).</p>
<p>Furthermore, human intuition may result from grasping underlying statistical patterns and structures of the world in various respects in an implicit way, due to repeated experiences (Hasher &amp; Zacks, 1979;Reber, 1989).Likewise, repeated training of LLMs with a large amount of data facilitates the (implicit) capturing of statistical patterns and structures of the data within LLMs (Durt et al., 2023;Mollo &amp; Millière, 2023;Zhang et al., 2023).For example, Dasgupta et al. (2022) and Saparov &amp; He (2022) showed that LLMs demonstrated content effects in reasoning similar to humans.Trott et al. (2023) showed that LLMs could capture human intuition exhibited in false-belief experiments.And so on.</p>
<p>One characteristic of human intuition is that one often makes quick "inferences" without (consciously) considering all possible alternatives.In other words, one often automatically (unconsciously) filters out what is (perceived to be) unlikely or less likely and thus focuses on the most probable inference only (Dreyfus &amp; Dreyfus, 1986).Moreover, when communicating an intuition in language, one may express it one word/phrase at a time on the fly (in a way that resembles "thinking aloud" protocols in psychological experiments), that is, vocalizing thoughts as they unfold, apparently without necessarily a plan for sentence structures or wording in advance.The two aspects above are highly similar to the way in which LLMs generate sentences and paragraphs (although LLMs can be instructed to do otherwise to some limited extent; e.g., Wei et al, 2022).This is probably because, in humans as well as in LLMs, the size of the set of all possibly inferences and/or the size of the set of all possible verbal descriptions of an intuition are both too large for humans and for LLMs to enumerate and examine deliberatively one by one in real time.Thus selectivity becomes necessary for humans and LLMs alike when dealing with real-time, on-the-fly generation of inferences or verbal descriptions.</p>
<p>But, assuming that LLMs can capture intuition, can LLMs capture more than intuition?For one thing, human language involves structured and/or symbolic processes, judging from the output of the human language faculty (as argued by, e.g., Fodor &amp; Pylyshyn, 1988;Pinker &amp; Prince, 1988), which seems to suggest symbolic mental capabilities.LLMs do likewise have some symbolic capabilities, including structures and compositionality to some extent (e.g., Pavlick, 2023), especially when it comes to dealing with language production and comprehension.However, it is worth pointing out that LLMs (at least the currently existing ones) have only limited such capabilities.At the same time, human implicit mental processes have been shown to have likewise limited such capabilities (Macchi et al., 2016;Reber, 1989), when explicit processes are not involved.When explicit mental processes are involved, they can lead to a full range of explicit symbolic capabilities; for example, full explicit symbolic reasoning can be exhibited by humans.Therefore, it seems unnecessary to posit in LLMs anything more than implicit processes to explain limited symbolic capabilities of LLMs.</p>
<p>This view is consistent with some views expressed in the literature.Bubeck et al. (2023) assessed performance of GPT-4 on a variety of tasks and concluded that it could capture "fast" (intuitive, implicit) processes, but not "slow" (reflective, explicit) processes.Mugan (2023) likewise asserted that LLMs captured implicit, unconscious processes.Pavlik (2023) argued that processes within LLMs, although implicit, could perform some apparently symbolic operations and inferences.Now, beside intuition that was just discussed, what about another major type of implicit processes ---instinct?Instinct is the ''tendency of an organism to make a complex and specific response to environmental stimuli without involving [explicit] reason'' (Merriam-Webster Dictionary).One may also link it to the Heideggerian notion of "comportment" (Dreyfus &amp; Dreyfus, 1987).Empirical work has been done to elucidate this concept: For instance, ethologists have studied animal instincts extensively (McFarland, 1989).Some work has also been done to elucidate this notion through computational models: Sun &amp; Wilson (2014) discussed how human motivation and consequent implicit action selection (along with other processes) together capture instinct.It is mostly implicit and centers on the interaction of internally felt needs (motives) and external environmental factors in (implicitly) determining actions, thereby capturing much of characteristic behavioral patterns (e.g., personality; Sun &amp; Wilson 2014).Beyond hardwired (innate) instincts, there are also acquired (learned) instincts, mostly through repeated experiences.To both kinds of instincts, what was argued earlier regarding intuition can be largely applied.In other words, instincts can develop within LLMs through repeated training with a large amount of data (more on this later).Arguments are somewhat similar and thus will not be repeated.</p>
<p>Note that different types of implicit mental processes, such as intuition versus instinct, may be captured by different and separate LLMs, as they have been structured in existing psychological models of intuition and instinct by parallel modules or parallel subsystems (e.g., Sun &amp; Wilson, 2014).</p>
<p>What Else is Needed Beyond LLMs?</p>
<p>In order to achieve human-level intelligence or to understand and capture the full capacity of the human mind, what else do we need beyond the currently existing LLMs?To answer this question, we can take cues from cognitive science, psychology, neuroscience, and so on: Theoretical and computational models and empirical findings from these disciplines can help.In particular, as mentioned before, computational cognitive architectures developed therein can be illuminating on this question.</p>
<p>From a vast amount of research in cognitive science, psychology, and neuroscience, we know that humans are capable of explicit (conscious) cognition involving, among other things, symbol manipulation and explicit rule following (Fodor &amp; Pylyshyn, 1988;Pinker &amp; Prince, 1988;Sun, 1994), beyond mere implicit processes (which may be captured by neural networks, e.g., by LLMs, as argued so far).Therefore, explicit processes that involve symbol manipulation need to be included in a more complete model of intelligence, especially in a psychologically realistic model of human intelligence.</p>
<p>In this regard, dual-process theories, formalizing and codifying what has been stated above, are worth noting.They have been gaining attention in recent decades, in psychology, philosophy, and many other fields.The distinction between implicit processes (also termed System 1 or simply "intuition") and explicit processes (also termed System 2 or "reason") has been argued by many since the 1980s (e.g., Reber, 1989;Sun, 1994Sun, , 2002) ) and popularized later by Kahneman (2011).In general, explicit processes are relatively easily accessible to consciousness, while implicit processes are less so.Explicit processing may be described as symbolic and rule-based to some significant extent, while implicit processing is more ''associative'' and "holistic" (Sun, 1994(Sun, , 2002)).That is, explicit processing may involve the manipulation of symbols, while, in contrast, implicit processing involves more instantiated knowledge that is holistically and/or statistically associated.Empirical evidence and theoretical analysis in support of these points can be found in the literature (e.g., in the work cited above).Work on computational cognitive architectures has also demonstrated computationally the importance of dual processes (e.g., Helie &amp; Sun, 2010;Sun et al., 2005).Furthermore, beyond implicit versus explicit processes, a number of other aspects that have usually been neglected in LLMs are also crucial.For instance, motivation is of fundamental importance to humans and human-like behavior, as has been argued before (e.g., Maslow, 1943;Reiss, 2004;Ryan &amp; Deci, 2000;Sun et al., 2022).Specifically, if one does not have goals (or has only randomly set goals that are randomly changing), one will inevitably act in a disorganized, haphazard way; or one may rely only on fixed reflexes that are rigid and inflexible (namely, much less intelligent).Similarly, for AI systems, without motivational processes, they would be aimlessly; or they would have to rely on prior knowledge coded into them (just like reflexes in humans), in order to accomplish something in only relatively simple circumstances.Alternatively, they may rely on external "feedback" to learn to perform right actions (using, e.g., reinforcement learning).But the requirement of external feedback begs the question of how it should be obtained in the natural world.</p>
<p>Largely absent in current AI systems are essential (or intrinsic) motivation, as well as its correlates: emotion, personality, and other human characteristics (Sun &amp; Wilson, 2014).They are important, not just for attaining goal-orientedness and complex behavior (as discussed so far), but also for addressing the meta-level issues of alignment with human intentions, values, and ethics (Bretz &amp; Sun, 2018), as well as for competency in dealing with social interaction (Sun, 2006).Motivation should be an important part of any human-level system, in order to account for deeper structures in the control of behavior.In this regard, Maslow (1943), Murray (1938), Reiss (2004), and Sun et al. (2022) showed the complexity of human motives and needs, which may serve as roadmaps for enhancing LLMs.In addition, correlated with motivation, personality, emotion, culture, and so on also need to be taken into consideration, both to capture human-like behavior and to understand the human mind.Regarding their respective roles in cognitive architectures, see Sun and Wilson (2014), Sun et al. (2016), and so on.</p>
<p>There are several other general ideas of what current LLMs lack but need (e.g., metacognition, memory, online learning, etc.; more on them later).In addition, beyond general ideas, one may delve into details of existing LLMs for more fine-grained understanding of what is needed.At a more detailed level, one may ascertain the capabilities and shortcomings of existing LLMs (e.g., Bubeck et al, 2023;Chang &amp; Bergen, 2023;Pavlick, 2023); this may be achieved, in part, through applying a variety of psychological and other tests to LLMs (e.g., Binz &amp; Schulz, 2023;Trott et al., 2023).With a more precise understanding, more fine-grained ways of enhancing LLMs may be devised.</p>
<p>Can a Cognitive Architecture Be the Right Framework?</p>
<p>Can a computational cognitive architecture, as mentioned earlier, help in a general way and serve as an overarching framework for the sake of enhancing and strengthening LLMs?The answer so far seems to be yes.For instance, computational cognitive architectures can make it possible for an LLM-based agent to better remember and retrieve information, choose actions in dynamic (physical and social) environments, plan future courses, reason about and solve problems, reflect metacognitively, and interact effectively with other agents (human or otherwise), as directed by one's intrinsic motives and needs.This is not entirely a new idea: Some have already thought of cognitive architectures in a similar way, albeit very recently (and sometimes not very "cognitive").Xie et al. ( 2023) drew on existing cognitive architectures for enhancing LLMs, thereby incorporating attention, memory, reasoning, learning, and decision-making mechanisms.Park et al. (2023) described an architecture that extends an LLM to store an agent's experiences using natural language, distill those (linguistic) memories into higher-level reflections, and retrieve them to plan behavior.Romero et al. (2023)  With a cognitive architecture, an agent takes its current environmental information, past experiences, and internal needs and goals into account when generating behavior.Under the hood, the cognitive architecture may combine a LLM with various mechanisms for organizing and retrieving relevant information as well as for self-regulation and focusing on important objectives, to help the LLM to generate useful outputs and to maintain behavioral consistency over time.Both implicit and explicit processes are involved (according to some cognitive architectures).In particular, the interaction between implicit and explicit processes within a cognitive architecture (which has been extensively studied; e.g., Sun, 2016) can be leveraged for structuring the interaction between LLMs and symbolic processes.</p>
<p>In the other direction, LLMs can serve as an important tool and an underlying technology that elevate existing cognitive architectures beyond laboratory toys towards both theoretical and practical tools for the real world.LLMs better addresses real-world complexity, given the vast amount of data used in training LLMs, as demonstrated by many use cases seen thus far.LLMs can also lead somehow to better psychological realism of cognitive architectures, in the sense that they may generate psychologically plausible behavior in complex environments (beyond typically small laboratory experiments used in validating cognitive models).Psychological realism of LLMs is being actively explored by researchers (Dasgupta, et al., 2022;Trott, et al., 2023;etc.).</p>
<p>An Example of a Dual-process, Dual-representation, and Motivation-focused Cognitive Architecture I will use the Clarion cognitive architecture as an example.Clarion is meant to be a comprehensive, mechanistic, process-based, psychological theory, with detailed computational specifications and implementations (Sun, 2002(Sun, , 2016)).Clarion consists of four major subsystems: the action-centered subsystem (ACS) for dealing with actions involving procedural (i.e., action) knowledge, the non-action-centered subsystem (NACS) for reasoning and memory involving declarative (i.e., factual) knowledge, the motivational subsystem (MS) for dealing with motivation, and the metacognitive subsystem (MCS) for regulating other subsystems (Sun, 2016).</p>
<p>Each of these subsystems consists of two "levels".The top level carries out explicit (roughly, conscious, deliberative) processes; the bottom level carries out implicit (roughly, unconscious, intuitive) processes (Reber, 1989;Sun, 1994Sun, , 2002)).Computationally, one is symbolic and the other neural.The two types of representations are inter-connected: The symbolic representations at the top level, which are in the forms of "chunks" (each representing a concept, defined by a set of dimension-value pairs) and rules connecting chunks, are linked to corresponding neural representations at the bottom level.The two processes thus interact to generate combined outcomes.See Figure 1.</p>
<p>The flow of information among these subsystems is roughly as follows: First, within the MS, situational inputs trigger internal (mostly intrinsic) motives, termed drives, based on both internal propensities and situational inputs.Then, a goal (an intention for action) within the MS is selected (by the MCS) on the basis of activated motives/drives (for the sake of satisfying these).Action/meta-action selection occurs, within the ACS/MCS, on the basis of the goal selected and the situational inputs (to maximize the satisfaction of the motives/drives).</p>
<p>Note that Clarion has been well validated against psychological data, findings, and theories (Sun, 2002(Sun, , 2016)).It constitutes a comprehensive theory of the mind.It is suitable as a framework for enhancing LLMs (as partially explored already, e.g., by Romero et al., 2023).</p>
<p>How Can LLMs be Incorporated into Clarion?</p>
<p>For incorporating LLMs into Clarion, there are multiple possibilities, ranging from LLMs being at the periphery of the cognitive architecture to LLMs being at its very core.Here are five possibilities for LLMs in Clarion (see Romero et al., 2023 for some of these and other alternatives):</p>
<p>• For dealing with perception: Multimodal LLMs handle (implicit early stages of) perception processes for Clarion.Given an input (e.g., a visual scene), LLMs generate linguistic and/or structured descriptions (in a form compatible with the internal representations of Clarion), for processing and then responding by Clarion.</p>
<p>• For dealing with language-based communication: In this case, natural language inputs are directed to LLMs and LLMs (implicitly) process them to provide inputs to Clarion (in a form compatible with the Clarion internal representations).Clarion can then respond to them in its internal forms, and LLMs generate natural language texts on that basis, for the sake of verbal communication (although full natural language understanding would likely require more than that).</p>
<p>• For dealing with motor processes: LLMs (implicitly) generate motor actions, based on directives from Clarion (in an internal form) that provide general specifications for the motor actions.</p>
<p>• For capturing memory: In this case, some modules of Clarion may be based on LLMs, which serve as (implicit) memory of various forms in Clarion (e.g., implicit semantic or procedural memory).</p>
<p>• For capturing all forms of implicit processes: LLMs can capture all implicit processes, ranging from intuition to instinct (as argued before), aside from (implicit processes of) perception, motor control, and natural language communication (as mentioned above).In this case, LLMs play all these roles.</p>
<p>The last approach above evidently confers the most extensive role on LLMs within a cognitive architecture, as it encompasses all other possibilities essentially; for example, perception, motor, and language processes are all largely implicit.Other modules or components within a cognitive architecture therefore must work closely with LLMs.</p>
<p>Among these possibilities, this last approach is preferrable for its generality.This approach is also well justified theoretically (see the discussion earlier on dual processes).Moreover, this approach is readily workable in any cognitive architectures that incorporate dual processes.In other words, LLMs are a natural fit for adding to and enhancing such cognitive architectures and, conversely, such cognitive architectures are well positioned to strengthen LLMs as well.</p>
<p>As will be detailed, it can be argued that the overall architecture of Clarion and many of its technical details are applicable for the sake of structurally and fundamentally enhancing LLMs.Such enhancement is not just adding more parameters to LLMs, not just changing their pre-training methods, and not just fine-tuning them (after pre-training), but much more fundamental as it incorporates well-established elements of the human psychology that are otherwise missing in current LLMs.</p>
<p>Below I investigate this approach in some detail.See Figure 2 for a sketch of the overall architecture, as will be explicated below.Note that, due to the length limit, not all aspects can be covered, but only a few examples of key aspects.</p>
<p>Human-like Dual Processes: Explicit, Symbolic Processes Interacting with LLMs</p>
<p>As argued earlier, dual processes must be taken into serious consideration, in order to enhance and strengthen LLMs: We need to include what correspond to implicit processes (such as those computationally captured by neural networks, including LLMs) and what correspond to explicit processes (such as those computationally captured by symbolic and/or rule-based systems).Concerning LLMs specifically, Mugan (2023) pointed out the need for "tools for deliberate thinking".Bubeck et al. (2023) also noted the missing "slow-thinking" (i.e., explicit) component in LLMs, believing that the "fastthinking" (i.e., implicit) component could serve as its subroutine.</p>
<p>However, each type of process may not be just tools; rather, each is a fundamental part of the human mind (Sun, 1994(Sun, , 2002)).Further, explicit processes may guide (affect in some way) implicit processes, but at the same time, they may also be guided (affected) by implicit processes (Sun, 2002).Thus, complex and dynamic interaction occurs between these two types (in notable contrast to some existing theories, such as the "default-intervention" view of Evans &amp; Frankish, 2009).It should be noted that somewhat similar ideas, although much courser-grained (i.e., not yet addressing finer psychological details), have been proposed by Booch et al. (2021), Lin et al. (2023), and others, with one or more modules consisting of LLMs.</p>
<p>In Clarion, detailed processes of implicit-explicit interaction have been elucidated mechanistically (i.e., computationally; in the ACS and the NACS; Sun, 2016), and validated psychologically (to a significant extent; through modeling and simulation of empirical psychological data).These interaction processes can be mapped, when LLMs are incorporated into Clarion, to those between LLMs (capturing implicit processes) and explicit, symbolic processes.That is, the psychologically validated dual processes within Clarion can be leveraged for structuring the interaction between LLMs and symbolic processes.</p>
<p>Within Clarion, one essential form of interaction is what has been termed top-down activation (initiation of implicit processes by explicit processes), while the other is bottom-up activation (activation of symbolic structures by implicit processes).Thus, for instance, implicit processes may be guided by explicit processes (e.g., by explicit instructions), and results of implicit processes may be confirmed, refined, rectified, or explained by explicit processes that are in turn grounded in implicit processes.These two forms are crucial to Clarion.</p>
<p>In relation to learning, one form of implicit-explicit interaction in Clarion is top-down learning (taking explicit knowledge and assimilating them into implicit forms), and another is bottom-up learning (learning implicit knowledge first and then learning explicit knowledge on that basis).See Sun (2002Sun ( , 2016) ) for technical details of these forms, which enable incremental, continuous, real-time learning specific to situations encountered (more on learning later).</p>
<p>With LLMs capturing implicit processes, top-down and bottom-up activations originally specified in Clarion become (1) prompts to LLMs, and (2) outputs from LLMs in a form that can be linked up with symbolic processes at the top level of the architecture.Note that outputs from LLMs can be in linguistic or Clarion internal representations (depending on prompts to LLMs and other factors).As noted earlier, internal representations in Clarion are in the form of chunks (each defined by a set of dimension-value pairs, serving as a condition or a conclusion in explicit reasoning), or rules (with their conditions and conclusions specified by chunks; acquired through learning, e.g., bottom-up learning).(For now, I ignore utilizing embeddings from LLMs, which would require a much longer treatment.)On the other hand, inputs to LLMs are prompts (in a linguistic or an internal form), along with possibly other information.Thus prompt engineering and other techniques are needed to get desired outcomes from LLMs.It is worth noting that LLMs can translate between linguistic (natural language) representations and internal representations used in Clarion, which facilitates the processing of external natural language inputs (e.g., queries, instructions, or situational descriptions).</p>
<p>In Clarion with LLMs incorporated, there are several roles for explicit, symbolic processes at the top level: (1) providing prompts to the LLMs at the bottom level to initiate their processing; (2) verifying, rectifying, refining, or explaining the results returned from the LLMs (which correspond to intuition or instinct) at the top level, using explicit knowledge at that level; (3) supplementing or even supplanting the results from the LLMs, using explicit knowledge at the top level, when situations call for it.Within the context of Clarion, prompting, in a way, amounts to an inner dialogue within oneself (Vygotsky, 1962).In this case, the dialogue mainly consists of repeated queries to one's inner self (i.e., intuition and so on).Through this dialogue, one's thoughts form, develop, and mature.</p>
<p>In a way, the Clarion approach of dual representations achieve similar outcomes as Instruction Tuning (or Agent Tuning; cf.Zeng et al., 2023) would eventually.But this approach is less time consuming (e.g., no repeated tuning), is more flexible, and can instantaneously change to adapt to new situations.</p>
<p>Below I will examine dual processes in two cognitive functions.</p>
<p>Reasoning</p>
<p>Reasoning is a characteristically human capability.Dasgupta et al. (2022), Saparov &amp; He (2022), Trott et al. (2023), Zhang et al. (2023), and others assessed reasoning by LLMs and found mixed results.Bubeck et al. (2023) pointed out that LLMs suffered from "lack of planning, working memory, ability to backtrack".In other words, although LLMs may perform some (intuitive) reasoning, they are not yet capable of full human-level reasoning.</p>
<p>One may argue that various ideas in terms of "prompt engineering" together show the importance of explicit, symbolic processes in guiding LLMs for reasoning (through one prompt or through iterative prompts).For example, Chain of Thought, Tree of Thought, and Algorithm of Thought prompts (e.g., Wei et al., 2022) provide one-prompt stepwise guidance from explicit to implicit processes.In additional, common search control strategies, such as depth-first, breadth-first, and A* search, can be implemented from explicit processes in Clarion (e.g., through multiple, iterative prompts).Furthermore, prompts to LLMs can involve complex, sophisticated reasoning templates, which again are guidance from explicit to implicit processes: Xie et al. (2023) showed that reasoning in LLMs could be enhanced through templates (given by explicit processes) for performing analogical reasoning, problem decomposition, planning thinking, integrative thinking, and so on.Xi et al. (2023) surveyed additional methods.In the original Clarion, there are a number of other reasoning methods (templates), such as forwarding chaining, backward chaining, constraint satisfaction, and so on (see Sun, 2016), which can be invoked by explicit processes (especially from the MCS; Sun, 2016).So, explicit processes can guide LLMs, in addition to the fact that they can perform explicit, symbolic reasoning with precision, certainty, and other useful characteristics in conjunction with or instead of (supplanting) LLMs.Together, the implicitexplicit interaction in Clarion leads to much enhanced reasoning abilities, combining intuition and explicit reasoning.</p>
<p>Reasoning also relies on various (implicit or explicit) mental models.These models may be formed through (semantic, episodic, and procedural) memory and learning, as will be discussed later.</p>
<p>Action</p>
<p>Transformers or LLMs can capture human-like instincts for actions as discussed earlier (Sun &amp; Wilson, 2004).Chen et al. (2021) proposed that one could use Transformers to capture action sequences, just as they learned natural language sentences.The model, when provided partial action sequences occurred thus far, predicted the next action to be taken.Lin et al. (2023) and others proposed methods that turn outputs of regular LLMs into actions (e.g., through action templates).</p>
<p>Thus, within Clarion, essentially the same kind of model (LLMs based on Transformers) can be used to capture a variety of implicit processes, including both intuition and instinct (as discussed before), in addition to language, perception, and motor functions.Note that implicit processes for reasoning and action are captured by separate but interacting LLMs (cf.Yao et al., 2023), corresponding exactly to the distinction of and the relation between the ACS and the NACS in Clarion (in their bottom levels).</p>
<p>However, in Clarion, on top of implicit action selection processes, explicit, symbolic processes are also present.Besides providing prompts to LLMs (e.g., as guidance), they also provide precision, certainty, and other useful characteristics, in conjunction with or instead of (supplanting) LLMs.</p>
<p>In addition, planning of multi-step action sequences (an essential human capability) can also be carried out in Clarion through implicit-explicit interaction.For instance, Sun and Sessions (2000) showed that explicit, symbolic plans can emerge from neural reinforcement learning.Recent work involving planning through LLMs (through prompts to initiate planning in LLMs) includes Lin et al. (2023), Park et al. (2023), Yao et al. (2023), and so on.Details of planning are omitted, due to space constraints.</p>
<p>Human-like Memory: Mnemonic Aid to LLMs</p>
<p>In Clarion, there are a variety of memory: semantic memory, procedural memory, episodic memory, working memory, and so on, often in both implicit and explicit forms.These types of memory are crucially important for human cognition, as has been demonstrated time and again in empirical psychological research (e.g., Baddeley, 1986;Schacter, 1987;Tulving, 1983).They are thus also important to Clarion in capturing human cognitive processes (Sun, 2012).</p>
<p>LLMs, given their capabilities, serve well as implicit semantic memory and implicit procedural memory within Clarion (at the bottom levels of the NACS and the ACS, respectively), while the top levels of the NACS and the ACS serve as explicit semantic and procedural memory (Schacter, 1987;Sun, 2012).However, episodic memory (Tulving, 1983) and working memory (Baddeley, 1986) are currently lacking in most LLMs.Clarion provides useful insights as to how to incorporate these needed memory systems, so as to lead to enhanced performance overall (Sumers et al., 2023;Xi et al., 2023).</p>
<p>In Clarion, explicit episodic memory consists of an agent's experiences at each time step, in the form of state, action, thought, payoff, next state, and so on (all of which happen at the time step), along with a time stamp (indicating the time step).Here, state refers to whatever was perceived by the agent at the time step (the observed situation, which might include the states of objects and other agents); action is what one did at that time step, including both external (e.g., verbal) and internal (e.g., mental) actions; thought refers to whatever happened in the NACS and the WM, including activated chunks, rules that were applied, and so on; payoff is the reinforcement that one received after action was performed (based on the satisfaction of activated drives within the MS); next state is what one perceived after action was performed.</p>
<p>The same explicit episodic memory is used in Clarion with LLMs incorporated.A retrieval function takes a partial specification of state, action, and so on as the input and returns a subset of the memory that matches the input, in order to pass on to LLMs to generate linguistic descriptions, or to use as the basis for reasoning or learning.Metrics used by the retrieval function include recency, relevance, and significance (e.g., reinforcement resulting from drive satisfaction).</p>
<p>In addition to episodic memory as an explicit list of items, neural networks are used for storing abstracted episodes in a more compact (and more implicit) way.For example, Backpropagation neural networks have been used in the original Clarion (Sun, 2016).LLMs are now used instead.A variety of techniques were developed within LLMs for condensing, summarizing, and integrating memory (e.g., Park et al, 2023;Xi et al., 2023) that can lead to a condensed implicit episodic memory (which in turn forms a part of one's mental model).Such memory can be centered on internal (chunk) or linguistic representations (or even embeddings).</p>
<p>Therefore, there is a dual-representational episodic memory, mirroring the overall structure of Clarion where implicit memory is supplemented by explicit memory (Hasher &amp; Zacks, 1979;Schacter, 1987).Each side serves a different function (e.g., specific episodes versus summarized experiences) and complements the other (as in the original Clarion).</p>
<p>On the other hand, working memory in Clarion is for storing information temporarily, facilitating the processing of the information (especially for the ACS, as part of its input), or for transferring information (especially between the ACS and the NACS, such as information from the ACS to be used as input to the NACS, or vice versa).The same working memory applies to the architecture incorporating LLMs, to supplement the limited memory in LLMs and to transfer information between different subsystems (as in the original Clarion; Sun, 2016).</p>
<p>Human-like Motivation and Regulation for LLMs</p>
<p>The different subsystems of Clarion, for example, the division between the ACS and the NACS or the existence of the MS and the MCS, can be leveraged to enhance LLMs.In particular, the MS and the MCS provide essential (or intrinsic) motives as basis for self-direction and self-regulation of behavior, which are lacking in current LLMs.Incorporating human-like motives that guides behavior enhances autonomy, behavioral consistency, value alignment with humans, mutual intelligibility with humans, and so on.Furthermore, personality and emotion, on the basis of these motives, lead to even more humanlike behavior (Sun &amp; Wilson, 2014).</p>
<p>For any organism, its main "need" (motive) is sustaining its life.Beyond that, another important "need" is the sustaining of the species (through reproduction).However, a number of other "needs" (including socially oriented ones) also exist, depending on species.Cognition has evolved to serve such needs (Sun, 2006).Humans are more evolved and thus have more complex and more sophisticated motivational structures (comprising needs/motives, goals, and so on).Human intrinsic needs/motives have been explored by ethologists and social psychologists and are considered fundamentally important (the lack of which makes human-level intelligence unlikely).From Murray (1938) to Reiss (2004), a number of essential or intrinsic human needs/motives have been identified through empirical and theoretical work.</p>
<p>These motives, termed drives, are an important part of Clarion.They include achievement, affiliation, power, autonomy, deference, and so on (within the MS; for the complete list and definitions, see Sun, 2016).Goals, which direct actions, are set on the basis of drives (together constituting dual processes of motivation).Modeling work based on Clarion shows that these drives are essential to accounting for a broad range of behavior, ranging from work performance to moral judgment (e.g., Bretz &amp; Sun, 2018;Sun et al., 2022).By integrating with Clarion, LLMs acquire human-like motivation and autonomy.</p>
<p>Moreover, personality has been shown to be closely related to human motivation.Deci (1980), for instance, made a case for this point.In Clarion, personality is captured mostly based on drives (Sun &amp; Wilson, 2004).Individual differences are explained (in a large part) by the differences in drive strengths in different situations by different individuals.Differences in drive strengths are consequently reflected in goals and actions.Implicit processes involved in this sequence roughly correspond to the notion of instinct (which can be captured by LLMs, as mentioned before).Furthermore, emotion is also tied to motivation and action, as well as reasoning and metacognition.According to Clarion, emotion is generated based on drive activations and action potentials, with reasoning (i.e., appraisal) and metacognition as secondary factors (for details, see Sun et al., 2016).Motivation is thus fundamental to emotion.Instinct and intuition (both of which can be captured by LLMs) are important to emotion.</p>
<p>Motivation and its correlates are particularly important to self-regulation.The MCS in Clarion monitors and regulates cognition and behavior based on drives, goals, and other factors, involving dual processes (Reder, 1996).Metacognition can occur implicitly through LLMs (Park et al., 2023), when explicit processes initiate what occurs in LLMs.Meanwhile, explicit regulation can also occur at the top level of Clarion.Metacognition is characteristic of humans, so it is important to producing human-like behavior.As a result of the MS and the MCS, Clarion can capture and explain a much larger range of human behavior (e.g., Sun et al., 2022).By integrating with Clarion, LLMs acquire the capabilities.</p>
<p>Learning</p>
<p>Learning has been touched upon earlier.Both top-down and bottom-up learning, as described before, are applicable to LLMs, along with reinforcement learning, which together lead to continuous, real-time learning as in the original Clarion (Sun, 2016).</p>
<p>A further opportunity offered by LLMs is that LLMs themselves may be prompted to generate useful symbolic rules (and other symbolic content) for the top level that can help enhance the top level.This can be done right after pretraining LLMs or during continuous learning mentioned above (thus reflecting continuous learning).The generated symbolic content may contribute to explicit semantic or procedural memory (at the top level of the NACS or the ACS, respectively), supplementing symbolic knowledge obtained from externally provided instructions, external tools and sources, bottom-up learning, and so on (Sun, 2016).</p>
<p>In addition, other forms of learning can also be applied.Learning from mistakes and learning from instructions can both be accomplished through episodic memory in Clarion (following a memory-based approach; Stanfill &amp; Waltz, 1986;Xie et al., 2023).Curriculum learning (Elman, 1993;Mugan, 2023) can be accomplished through guidance by the MCS in Clarion.</p>
<p>Grounding</p>
<p>In Clarion, "grounding" may be achieved through its perceptual and motor modules that are connected to the bottom level of its major subsystems (a form of sensory-motor and referential grounding through causal-informational and historical relations to the world).Symbolic representations at the top level of Clarion are also grounded, through their intrinsic connections to the bottom level (see Sun, 2000 for details).</p>
<p>In the updated architecture, these perceptual and motor modules may instead be connected to LLMs (in conjunction with or instead of multi-modal LLMs).Multimodal learning (touched upon earlier), including linguistic, auditory, visual, and other modalities, is, of course, important to grounding.Furthermore, as Mollo and Millière (2023) argued, even unimodal LLMs themselves are capable of grounding, through their mediated causal-informational and historical relations to worldly entities, which provide a form of referential grounding (see also Pavlick, 2023).</p>
<p>Discussions</p>
<p>Now I will integrate pieces discussed separately so far.Regarding the ACS and the NACS, various roles of explicit and implicit processes can be summarized from the foregoing discussion.On the one hand, the roles of LLMs in Clarion include, among others:</p>
<p>• Natural language processing (in an implicit way)</p>
<p>• Intuition, including Intuitive reasoning, intuitive metacognitive reflection, and so on (possibly guided by explicit, symbolic processes) • Implicit action selection and planning (in an instinctual way, but possibly guided by explicit, symbolic processes) • Implicit semantic, procedural, and episodic memory • Implicit mental models constituted from implicit memory • Implicit metacognitive reflection • Learning from explicit, symbolic processes (e.g., assimilating explicit knowledge into LLMs through reinforcement learning) and helping explicit processes to learn (e.g., by extracting knowledge from implicit processes; Sun et al., 2001;Sun &amp; Sessions, 2000) On the other hand, the roles of explicit, symbolic processes include, among others:</p>
<p>• Directing natural language processing by LLMs • Precise, explicit rule-based and logical reasoning (complementing intuitive reasoning by LLMs; Sun, 1994); reaching new conclusions explicitly based on first principles and foundational methods • Guiding implicit reasoning by LLMs • Explicit action selection and planning (e.g., in unfamiliar situations)</p>
<p>• Guiding implicit action selection and planning by LLMs (e.g., in familiar or well-practiced situations) • Explicit semantic, procedural, episodic, and working memory • Explicit mental models as a result of the memory above • Explicit metacognitive reflection • Learning from implicit processes in LLMs (e.g., extracting explicit knowledge) and helping LLMs to learn (e.g., to assimilate explicit knowledge)</p>
<p>As an example, below I briefly examine reasoning as an applicable domain for this updated cognitive architecture.</p>
<p>LLMs perform a variety of reasoning, but, as people discovered, they tend to (at least sometimes) rely on statistical regularity or world knowledge, not strictly on applicable logical rules (Durt et al., 2023;Saparov &amp; He, 2022;Zhang et al., 2023).Symbolic processes add more rigor to the system when symbolic and neural processes are combined as in Clarion, for example, through applying symbolic rules that perform more rigorous inferences themselves or that guide LLMs to perform more rigorous inferences through prompting, in accordance with some form of logic or template (Sun, 1994;Sun &amp; Zhang, 2006).Moreover, symbolic processes can easily dictate the use of external tools for correctly performing certain types of reasoning, such as mathematical derivation.When additional knowledge is needed, symbolic processes can also dictate the use of external knowledge sources in reasoning.</p>
<p>Memory also plays a significant role in reasoning.Episodic memory helps one to remember when one reaches a certain conclusion and how (Sun, 2016).Thus, memory of past instances of reasoning helps new reasoning cases when they are identical or similar.Memory of past reasoning errors helps to avoid similar errors later (Xie et al., 2023).Moreover, implicit semantic memory as embodied by LLMs provides the source for intuition.Along with other memory stores, it forms a mental model of the world in an implicit way.</p>
<p>Learning is also relevant to reasoning.One form of learning is instance-based learning: Correct and incorrect instances of reasoning in episodic memory serve as reminders.Learning can also involve extraction of heuristic rules in a symbolic form from reasoning by LLMs (i.e., bottom-up learning; Sun et al., 2001).Similarly, explicit, symbolic reasoning rules (e.g., from external sources) can be used to train or instruct LLMs (top-down learning).</p>
<p>Motivational and metacognitive regulation is also relevant to reasoning.Within the MS, activation of drives and consequent selection of goals lead to focus of attention and allocation of effort during reasoning.For example, utility calculation (cost-benefit analysis) is done based on activations of drives (Sun et al., 2022).Effort allocation (e.g., how much time one spends, and so on) is then determined (by the MCS) on the basis of the utility calculation.</p>
<p>Other forms of metacognitive regulation can also be performed, such as determining when one is to reason, what one is to reason about, whether explicit processes or implicit processes (LLMs) should be used in reasoning, when metacognitive reasoning should be involved, when one is to learn from reasoning, what one is to learn, and so on (Sun, 2016).</p>
<p>Ideas above concerning reasoning may be extended to problem solving, for example, creative problem solving.They may also be extended to planning, decision making, moral judgment, and many other areas.For example, Bubeck et al. (2023) pointed out the weakness of LLMs in performing "discontinuous" reasoning tasks, where content generation is not done merely in a gradual or continuous way but involves a discontinuous leap towards a solution, that is, a Eureka (insight) moment, which is often associated with creative problem solving.Such processes may require interaction with explicit reasoning (as shown by Helie &amp; Sun, 2010).Proper prompting by symbolic processes may alleviate the difficulty.For example, presenting the following prompt to an LLM for solving an insight problem: "How do you plant four trees in such a way that they are at the same distance from each other?Assume a 3D space", failure ensued.However, adding the following to the prompt: "This task has a simple solution.Please present its solution with less than 30 words", the LLM produced the correct solution: "To plant four trees at equal distances from each other in 3D space, place them at the vertices of a tetrahedron", which arguably shows the importance of symbolic processes in guiding LLMs in creative problem solving and in other areas.</p>
<p>Concluding Remarks</p>
<p>Given that current LLM-centered AI systems are limited in their ability (1) to truly replicate humanlevel intelligence and (2) to understand and capture human cognition, I argue for a new approach that turns to cognitive science and psychology (especially computational cognitive architectures) for developing hybrid neuro-symbolic models that closely mimic the structures and processes of the human mind (that is, based on mechanistic understanding of the human mind).Such models integrate neural networks and symbolic methods to amplify the strengths of both approaches while mitigating their respective weaknesses.They correspond well to existing dual-process psychological theories.The closer adherence to human psychology allows such models to go above and beyond other approaches that do not take human psychology as seriously.Reverse-engineering the best intelligent system around (i.e., the human mind) has its advantages.</p>
<p>On the other hand, computational cognitive architectures augmented by LLMs have capabilities beyond any previous cognitive architectures before the advent of LLMs, especially in their abilities to communicate (in verbal and other forms) and in capturing a broader scope of human intuition and instinct.As a result, cognitive architectures incorporating LLMs can be used for dealing with real-world situations, as opposed to being limited to modeling small laboratory tasks as before.</p>
<p>This approach could lead to not just improved performance but also better alignment with human values.Furthermore, the approach might lead to replicate the human mind more faithfully, including its dual processes, symbolic capabilities, intrinsic motivation, emotion, personality, and other human characteristics (largely absent in current LLM-centered AI).It might also transform human-machine cooperation into something akin to human-human cooperation.</p>
<p>Overall, there is a case for developing more human-like systems ---better inspired and better guided by human psychology.Integrating cognitive architectures with LLMs is a promising approach for achieving this.This multidisciplinary approach has the potential of leading up to human-aligned, autonomous, human-level systems eventually.</p>
<p>discussed several specific possibilities in this regard.Sumers et al. (2023) surveyed more broadly.</p>
<p>Figure 1 .
1
Figure 1.The original Clarion cognitive architecture with its four major subsystems (from Sun, 2016).</p>
<p>Figure 2 .
2
Figure 2. The Clarion architecture with LLMs incorporated.</p>
<p>AcknowledgementsThis work was carried out while the author was supported (in part) by ARI grant W911NF-17-1-0236 and IARPA HIATUS contract 2022-22072200002.The views and conclusions contained herein are those of the author's and should not be interpreted as necessarily representing the official policies and positions, either expressed or implied, of those agencies.The author also benefited from discussions with colleagues, including Francesca Rossi, Joe Killian, and others.
Working Memory. A Baddeley, 1986Oxford University PressNew York</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, G Booch, F Fabiano, L Horesh, K Kate, J Lenchner, N Linck, A Loreggia, K Murgesan, N Mattei, F Rossi, B Srivastava, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceProceedings of the National Academy of Sciences2023. 2021120Thinking fast and slow in AI</p>
<p>Two models of moral judgment. S Bretz, R Sun, Cognitive Science. 422018</p>
<p>Language model behavior: a comprehensive survey. S Bubeck, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. T Chang, B Bergen, 2023. 2023</p>
<p>. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, </p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Advances in Neural Information Processing Systems. 34</p>
<p>Beyond intuition and instinct blindness: Toward an evolutionarily rigorous cognitive science. L Cosmides, J Tooby, Cognition. 501994</p>
<p>. I Dasgupta, A K Lampinen, S C Chan, A Creswell, D Kumaran, J L Mcclelland, F Hill, </p>
<p>Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer. arXiv:2207.070511986BlackwellDreyfus, Hubert, &amp; Dreyfus; Stuart; Oxford, U.K.Language models show human-like content effects on reasoning</p>
<p>Learning and development in neural networks: the importance of starting small. C Durt, T Froese, T Fuchs, Cognition. 4812023. 1993J.L.</p>
<p>Connectionism and cognitive architecture: A critical analysis. J A Fodor, Z W Pylyshyn, Two Minds: Dual Processes and Beyond. J Evans, K Frankish, Oxford, UKOxford University Press2009. 198828</p>
<p>Automatic and effortful processes in memory. J Flavell, Erlbaum, N J Hillsdale, J Hasher, J Zacks, Journal of Experimental Psychology: General. B. Resnick1081976. 1979Metacognitive aspects of problem solving</p>
<p>Incubation, insight, and creative problem solving: A unified theory and a connectionist model. S Helie, R Sun, Psychological Review. 11732010</p>
<p>Thinking, fast and slow. D Kahneman, 2011Farrar, Straus and GirouxNew York</p>
<p>SwiftSage: a generative agent with fast and slow thinking for complex interactive tasks. B Y Lin, arXiv:2305.17390Psychological Review. Macchi, L., M. Bagassi, &amp; R. Viale502023. 2016. 1943MIT PressA theory of human motivation</p>
<p>Problems of Animal Behaviour. D Mcfarland, 1989LongmanSingapore</p>
<p>D C Mollo, R Millière, The vector grounding problem. 2023</p>
<p>Grounding large language models in a cognitive foundation: how to build someone we can talk to. The Gradient. J Mugan, Explorations in Personality. H Murray, New YorkOxford University Press2023. 1938</p>
<p>On language and connectionism: Analysis of a parallel distributed processing model of language acquisition. J S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, E Pavlick, S Pinker, A Prince, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. L Reder, Erlbaum, Mahwah, NJ2023. 2023. 2251. 1988. 1989. 1996381Implicit Memory and Metacognition</p>
<p>Multifaceted nature of intrinsic motivation: The theory of 16 basic desires. S Reiss, Review of General Psychology. 832004</p>
<p>Synergistic integration of large language models and cognitive architectures for robust AI: An exploratory analysis. O J Romero, J Zimmerman, A Steinfeld, A Tomasic ; Ryan, R M Deci, E L , arXiv:2308.09830American Psychologist. 552023. 2000Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, arXiv:2210.01240Journal of Experimental Psychology: Learning, Memory, and Cognition. Schacter, D.132022. 1987Implicit memory: History and current status</p>
<p>Toward memory-based reasoning. C Stanfill, D Waltz, T Sumers, arXiv:2309.02427v2Cognitive architectures for language agents. 1986. 202329</p>
<p>Integrating Rules and Connectionism for Robust Commonsense Reasoning. R Sun, Duality of the Mind. R Sun, New York; Mahwah, NJErlbaum1994. 2000. 200213Sun, R.</p>
<p>A unified framework for interpreting a range of motivationperformance phenomena. R Sun, R ; Sun, R Sun, S Bugrov, D Dai, Cognition and Multi-Agent Interaction: From Cognitive Modeling to Social Simulation. R , New York; Oxford, UK. Sun; Cambridge, UKCambridge University Press2006. 2012. 2015. 2016. 2023. 202230Cognitive Systems Research</p>
<p>From implicit skills to explicit knowledge: A bottom-up model of skill learning. R Sun, E Merrill, T Peterson, Cognitive Science. 2522001</p>
<p>The interaction of the explicit and the implicit in skill learning: A dual-process approach. R Sun, C Sessions ; Sun, R , P Slusarz, C Terry, Psychological Review. 83/42000. 2000. 2005Adaptive Behavior</p>
<p>Roles of implicit processes: instinct, intuition, and personality. R Sun, N Wilson, 2014Mind and Society13</p>
<p>Emotion: A unified mechanistic interpretation from a cognitive architecture. R Sun, N Wilson, M Lynch, Cognitive Computation. 812016</p>
<p>Accounting for a variety of reasoning data within a cognitive architecture. R Sun, X Zhang ; Trott, S Jones, C Chang, T Michaelov, J Bergen, B , Journal of Experimental and Theoretical Artificial Intelligence. 182472006. 2023Cognitive Science</p>
<p>Thought and Language. E ; Tulving, L Vygotsky, Elements of Episodic Memory. Oxford, UK; Cambridge, MAMIT Press1983. 1962</p>
<p>OlaGPT: Empowering LLMs with human-like problem-solving abilities. J Wei, arXiv:2201.11903arXiv:2310.12823v2Chain of thought prompting elicits reasoning in large language models. H Zhang, L H Li, T Meng, K W Chang, G Van Den Broeck, 2022. 2023. 2023. 2023. 2023. 2023AgentTuning: enabling generalized agent abilities for LLMs</p>            </div>
        </div>

    </div>
</body>
</html>
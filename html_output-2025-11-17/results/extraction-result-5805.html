<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5805 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5805</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5805</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-3f5b31c4f7350dc88002c121aecbdc82f86eb5bb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb" target="_blank">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</p>
                <p><strong>Paper Abstract:</strong> The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5805.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5805.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VQA zero-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot VQA prompt templates used for conditioning LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explicit natural-language prompt templates used at inference for zero-shot Visual Question Answering (VQA); two templates are reported depending on the LLM family (decoder-based OPT vs instruction-tuned FlanT5). The paper evaluates how these prompt formats are used with different LLMs and reports task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT / FlanT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>OPT: 2.7B, 6.7B; FlanT5: XL, XXL</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot Visual Question Answering (VQAv2, OK-VQA, GQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an image and a natural-language question, generate a short textual answer without task-specific finetuning of the LLM (visual conditioning provided via BLIP-2 pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompt templates appended after visual soft-prompts. For OPT: "Question: {question} Answer:". For FlanT5: "Question: {question} Short answer:". During generation beam search (beam=5) and a length-penalty of -1 were used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples from paper (BLIP-2 w/ ViT-g): FlanT5_XXL: VQAv2 val 65.2% (VQA acc.), VQAv2 test-dev 65.0%; OK-VQA test 45.9%; GQA test-dev 44.7%. OPT_6.7B: VQAv2 val 54.3%, test-dev 52.6% (other metrics reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>FlanT5_XXL (instruction-tuned) substantially outperforms OPT_6.7B (unsupervised) on VQAv2 (≈65.2% vs ≈54.3% on val in the reported table).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+~10.9 percentage points VQAv2 val (FlanT5_XXL vs OPT_6.7B) as reported in the paper's table.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (instruction-tuned prompt format / LLM-pretraining style led to higher zero-shot VQA accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors attribute part of the advantage to the LLM family and its pretraining: FlanT5 is instruction-tuned and thus better at following short natural-language prompts, leading to stronger zero-shot VQA performance compared to unsupervised OPT. They also note stronger image encoders improve performance, so both model and prompt/LM pretraining interact.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5805.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5805.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context few-shot VQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Providing in-context (few-shot) VQA examples to the LLM at inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports experiments attempting to improve VQA performance by giving the frozen LLM in-context examples (few-shot prompting) and finds no observed improvement in VQA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP-2 with various frozen LLMs (e.g., OPT, FlanT5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot / few-shot Visual Question Answering (VQAv2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether giving multiple image-question-answer examples in the same input (in-context learning) improves LLM answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>In-context (few-shot) prompting: supplying multiple image-text pairs/examples together in the LLM input sequence as demonstration examples (attempted for VQA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard zero-shot prompt (single image-question) vs providing in-context few-shot VQA examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No measurable improvement in VQA performance when in-context VQA examples were provided (authors report 'do not observe an improved VQA performance').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize the null result is due to the pretraining data format: their pretraining dataset contains only a single image-text pair per sample, so the LLM (in the BLIP-2 pipeline) did not learn correlations among multiple interleaved image-text pairs and thus cannot utilize in-context examples. They cite Flamingo which used an interleaved multi-pair dataset and did observe in-context capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Explicit null result reported: in-context examples did not improve VQA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5805.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5805.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-stage pretraining (representation stage effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage Q-Former pretraining (vision-language representation learning stage followed by vision-to-language generative stage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BLIP-2's two-stage pretraining deliberately controls the presentation/format of visual information (via Q-Former queries and attention masks) before connecting to the frozen LLM; the representation stage is shown to be crucial for good downstream generative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP-2 pipeline (Q-Former + frozen LLMs, tested with OPT and FlanT5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot VQA / vision-to-language generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate impact of including a dedicated vision-language representation learning stage (ITC, ITM, ITG objectives) prior to connecting Q-Former outputs as visual prompts to the frozen LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training format comparison: (A) Full two-stage pretraining: (1) representation-learning stage using ITC/ITM/ITG with controlled attention masks, then (2) generative stage connecting projected queries to frozen LLM; versus (B) skipping the representation-learning stage and relying solely on generative pretraining (vision-to-language loss) to align modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Two-stage (representation + generative) vs generative-only (no representation stage).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that without the representation-learning stage both types of LLMs produced substantially lower zero-shot VQA performance; OPT in particular suffered catastrophic forgetting with performance drastically degrading during training. Exact numeric deltas are shown qualitatively in Figure 5 but no single global percentage is given in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced performance when representation stage is omitted (so including representation stage improves performance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The representation stage forces the Q-Former queries to extract visual features most relevant to language (with ITC/ITM/ITG losses and attention masks), reducing the burden on the frozen LLM to learn vision-language alignment and mitigating catastrophic forgetting during subsequent generative training.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5805.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5805.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft visual prompts (prepended projected queries)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Projected Q-Former outputs prepended as soft visual prompts to frozen LLM input</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Format of presenting visual information to the LLM: the Q-Former's output query embeddings are linearly projected and prepended to the token embeddings of the LLM to condition generation (a soft-prompting input format).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP-2 (Q-Former + OPT/FlanT5 LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Vision-to-language generation (image captioning, zero-shot VQA, instructed image-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Condition LLM generation on visual content by presenting projected visual-query vectors as additional input tokens before the text prompt (soft visual prompt format).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input format to LLM: linear FC projection maps Q-Former Z (32×d) to LLM embedding dimensionality and the projected vectors are prepended to the text token embeddings as soft visual prompts. For decoder LLMs they act as prefix tokens for causal generation; for encoder-decoder LLMs they are concatenated to encoder input as a prefix.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually to methods that either finetune LLM cross-attention layers (Flamingo) or use image encoder outputs directly as soft prompts without a representation stage (Frozen); BLIP-2's Q-Former + projection + prepend is the used format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using this soft-prompt format plus the two-stage pretraining, BLIP-2 reports state-of-the-art zero-shot and finetuned results (examples: VQAv2 zero-shot up to ≈65% with FlanT5_XXL; NoCaps CIDEr ~121.6 for some BLIP-2 configs; see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (this presentation format enables leveraging frozen LLMs effectively and reduces trainable parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prepending projected query embeddings acts as an information bottleneck providing only language-relevant visual features to the LLM; this reduces the LLM's need to learn new cross-modal mappings, mitigating catastrophic forgetting and yielding strong performance while keeping LLMs frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5805.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5805.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decoding hyperparameters (beam & length-penalty)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation decoding settings: beam search and length-penalty adjustments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors report generation hyperparameters used for zero-shot VQA and image captioning and note how changing length-penalty affects answer length and alignment with annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP-2 with OPT / FlanT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot VQA (and generation tasks more generally)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Model generates text answers/captions; decoding hyperparameters determine search behavior and output length preference.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Decoding format: beam search with beam width = 5; length-penalty set to -1 for VQA generation to encourage shorter answers that align better with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative effect reported: length-penalty -1 encourages shorter answers and yields outputs that better match human annotations; no numeric ablation quantification provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved alignment with human annotations (by encouraging shorter answers)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Shorter, more concise outputs match the evaluation protocol/annotation style for VQA, so penalizing length improves metric alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5805.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5805.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Captioning prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Initial textual prompt used for image captioning fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple fixed textual prefix prompt ('a photo of') is provided as the initial input token(s) to the frozen LLM during image-caption fine-tuning, which frames the generation task and produces strong captioning results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP-2 with FlanT5 / OPT (finetuned Q-Former + image encoder, LLM frozen)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>FlanT5_XL, OPT variants (see tables)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Image captioning (COCO finetuning, zero-shot transfer to NoCaps)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a caption for an image during supervised finetuning on COCO; LLM is kept frozen while Q-Former and image encoder are updated.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Finetuning prompt format: use the fixed prefix 'a photo of' appended after the visual soft-prompts as initial conditioning text for caption generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BLIP-2 finetuned with this prompt achieves state-of-the-art captioning performance in their comparisons (example: BLIP-2 ViT-g FlanT5_XL NoCaps overall CIDEr ≈121.6 and SPICE ≈15.8 as reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (simple framing prompt yielded strong performance in finetuning setting)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing a fixed, natural-language prefix frames the generation as a captioning task and likely stabilizes outputs; authors keep LLM frozen and only train Q-Former/image encoder, relying on the prompt to steer the LLM's frozen generation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Flamingo: a visual language model for few-shot learning <em>(Rating: 2)</em></li>
                <li>Multimodal few-shot learning with frozen language models <em>(Rating: 2)</em></li>
                <li>From images to textual prompts: Zero-shot VQA with frozen large language models <em>(Rating: 2)</em></li>
                <li>A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5805",
    "paper_id": "paper-3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "VQA zero-shot prompts",
            "name_full": "Zero-shot VQA prompt templates used for conditioning LLMs",
            "brief_description": "Explicit natural-language prompt templates used at inference for zero-shot Visual Question Answering (VQA); two templates are reported depending on the LLM family (decoder-based OPT vs instruction-tuned FlanT5). The paper evaluates how these prompt formats are used with different LLMs and reports task performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT / FlanT5",
            "model_size": "OPT: 2.7B, 6.7B; FlanT5: XL, XXL",
            "task_name": "Zero-shot Visual Question Answering (VQAv2, OK-VQA, GQA)",
            "task_description": "Given an image and a natural-language question, generate a short textual answer without task-specific finetuning of the LLM (visual conditioning provided via BLIP-2 pipeline).",
            "problem_format": "Zero-shot prompt templates appended after visual soft-prompts. For OPT: \"Question: {question} Answer:\". For FlanT5: \"Question: {question} Short answer:\". During generation beam search (beam=5) and a length-penalty of -1 were used.",
            "comparison_format": null,
            "performance": "Examples from paper (BLIP-2 w/ ViT-g): FlanT5_XXL: VQAv2 val 65.2% (VQA acc.), VQAv2 test-dev 65.0%; OK-VQA test 45.9%; GQA test-dev 44.7%. OPT_6.7B: VQAv2 val 54.3%, test-dev 52.6% (other metrics reported in Table 2).",
            "performance_comparison": "FlanT5_XXL (instruction-tuned) substantially outperforms OPT_6.7B (unsupervised) on VQAv2 (≈65.2% vs ≈54.3% on val in the reported table).",
            "format_effect_size": "+~10.9 percentage points VQAv2 val (FlanT5_XXL vs OPT_6.7B) as reported in the paper's table.",
            "format_effect_direction": "improved (instruction-tuned prompt format / LLM-pretraining style led to higher zero-shot VQA accuracy)",
            "explanation_or_hypothesis": "The authors attribute part of the advantage to the LLM family and its pretraining: FlanT5 is instruction-tuned and thus better at following short natural-language prompts, leading to stronger zero-shot VQA performance compared to unsupervised OPT. They also note stronger image encoders improve performance, so both model and prompt/LM pretraining interact.",
            "counterexample_or_null_result": null,
            "uuid": "e5805.0",
            "source_info": {
                "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "In-context few-shot VQA",
            "name_full": "Providing in-context (few-shot) VQA examples to the LLM at inference",
            "brief_description": "The paper reports experiments attempting to improve VQA performance by giving the frozen LLM in-context examples (few-shot prompting) and finds no observed improvement in VQA accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLIP-2 with various frozen LLMs (e.g., OPT, FlanT5)",
            "model_size": null,
            "task_name": "Zero-shot / few-shot Visual Question Answering (VQAv2)",
            "task_description": "Evaluate whether giving multiple image-question-answer examples in the same input (in-context learning) improves LLM answer generation.",
            "problem_format": "In-context (few-shot) prompting: supplying multiple image-text pairs/examples together in the LLM input sequence as demonstration examples (attempted for VQA).",
            "comparison_format": "Standard zero-shot prompt (single image-question) vs providing in-context few-shot VQA examples",
            "performance": "No measurable improvement in VQA performance when in-context VQA examples were provided (authors report 'do not observe an improved VQA performance').",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "no effect",
            "explanation_or_hypothesis": "Authors hypothesize the null result is due to the pretraining data format: their pretraining dataset contains only a single image-text pair per sample, so the LLM (in the BLIP-2 pipeline) did not learn correlations among multiple interleaved image-text pairs and thus cannot utilize in-context examples. They cite Flamingo which used an interleaved multi-pair dataset and did observe in-context capabilities.",
            "counterexample_or_null_result": "Explicit null result reported: in-context examples did not improve VQA accuracy.",
            "uuid": "e5805.1",
            "source_info": {
                "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Two-stage pretraining (representation stage effect)",
            "name_full": "Two-stage Q-Former pretraining (vision-language representation learning stage followed by vision-to-language generative stage)",
            "brief_description": "BLIP-2's two-stage pretraining deliberately controls the presentation/format of visual information (via Q-Former queries and attention masks) before connecting to the frozen LLM; the representation stage is shown to be crucial for good downstream generative performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLIP-2 pipeline (Q-Former + frozen LLMs, tested with OPT and FlanT5)",
            "model_size": null,
            "task_name": "Zero-shot VQA / vision-to-language generation",
            "task_description": "Evaluate impact of including a dedicated vision-language representation learning stage (ITC, ITM, ITG objectives) prior to connecting Q-Former outputs as visual prompts to the frozen LLM.",
            "problem_format": "Training format comparison: (A) Full two-stage pretraining: (1) representation-learning stage using ITC/ITM/ITG with controlled attention masks, then (2) generative stage connecting projected queries to frozen LLM; versus (B) skipping the representation-learning stage and relying solely on generative pretraining (vision-to-language loss) to align modalities.",
            "comparison_format": "Two-stage (representation + generative) vs generative-only (no representation stage).",
            "performance": "Paper reports that without the representation-learning stage both types of LLMs produced substantially lower zero-shot VQA performance; OPT in particular suffered catastrophic forgetting with performance drastically degrading during training. Exact numeric deltas are shown qualitatively in Figure 5 but no single global percentage is given in the text.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced performance when representation stage is omitted (so including representation stage improves performance)",
            "explanation_or_hypothesis": "The representation stage forces the Q-Former queries to extract visual features most relevant to language (with ITC/ITM/ITG losses and attention masks), reducing the burden on the frozen LLM to learn vision-language alignment and mitigating catastrophic forgetting during subsequent generative training.",
            "counterexample_or_null_result": null,
            "uuid": "e5805.2",
            "source_info": {
                "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Soft visual prompts (prepended projected queries)",
            "name_full": "Projected Q-Former outputs prepended as soft visual prompts to frozen LLM input",
            "brief_description": "Format of presenting visual information to the LLM: the Q-Former's output query embeddings are linearly projected and prepended to the token embeddings of the LLM to condition generation (a soft-prompting input format).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLIP-2 (Q-Former + OPT/FlanT5 LLMs)",
            "model_size": null,
            "task_name": "Vision-to-language generation (image captioning, zero-shot VQA, instructed image-to-text)",
            "task_description": "Condition LLM generation on visual content by presenting projected visual-query vectors as additional input tokens before the text prompt (soft visual prompt format).",
            "problem_format": "Input format to LLM: linear FC projection maps Q-Former Z (32×d) to LLM embedding dimensionality and the projected vectors are prepended to the text token embeddings as soft visual prompts. For decoder LLMs they act as prefix tokens for causal generation; for encoder-decoder LLMs they are concatenated to encoder input as a prefix.",
            "comparison_format": "Compared conceptually to methods that either finetune LLM cross-attention layers (Flamingo) or use image encoder outputs directly as soft prompts without a representation stage (Frozen); BLIP-2's Q-Former + projection + prepend is the used format.",
            "performance": "Using this soft-prompt format plus the two-stage pretraining, BLIP-2 reports state-of-the-art zero-shot and finetuned results (examples: VQAv2 zero-shot up to ≈65% with FlanT5_XXL; NoCaps CIDEr ~121.6 for some BLIP-2 configs; see paper tables).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (this presentation format enables leveraging frozen LLMs effectively and reduces trainable parameters)",
            "explanation_or_hypothesis": "Prepending projected query embeddings acts as an information bottleneck providing only language-relevant visual features to the LLM; this reduces the LLM's need to learn new cross-modal mappings, mitigating catastrophic forgetting and yielding strong performance while keeping LLMs frozen.",
            "counterexample_or_null_result": null,
            "uuid": "e5805.3",
            "source_info": {
                "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Decoding hyperparameters (beam & length-penalty)",
            "name_full": "Generation decoding settings: beam search and length-penalty adjustments",
            "brief_description": "The authors report generation hyperparameters used for zero-shot VQA and image captioning and note how changing length-penalty affects answer length and alignment with annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLIP-2 with OPT / FlanT5",
            "model_size": null,
            "task_name": "Zero-shot VQA (and generation tasks more generally)",
            "task_description": "Model generates text answers/captions; decoding hyperparameters determine search behavior and output length preference.",
            "problem_format": "Decoding format: beam search with beam width = 5; length-penalty set to -1 for VQA generation to encourage shorter answers that align better with human annotations.",
            "comparison_format": null,
            "performance": "Qualitative effect reported: length-penalty -1 encourages shorter answers and yields outputs that better match human annotations; no numeric ablation quantification provided in the paper.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved alignment with human annotations (by encouraging shorter answers)",
            "explanation_or_hypothesis": "Shorter, more concise outputs match the evaluation protocol/annotation style for VQA, so penalizing length improves metric alignment.",
            "counterexample_or_null_result": null,
            "uuid": "e5805.4",
            "source_info": {
                "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Captioning prompt",
            "name_full": "Initial textual prompt used for image captioning fine-tuning",
            "brief_description": "A simple fixed textual prefix prompt ('a photo of') is provided as the initial input token(s) to the frozen LLM during image-caption fine-tuning, which frames the generation task and produces strong captioning results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLIP-2 with FlanT5 / OPT (finetuned Q-Former + image encoder, LLM frozen)",
            "model_size": "FlanT5_XL, OPT variants (see tables)",
            "task_name": "Image captioning (COCO finetuning, zero-shot transfer to NoCaps)",
            "task_description": "Generate a caption for an image during supervised finetuning on COCO; LLM is kept frozen while Q-Former and image encoder are updated.",
            "problem_format": "Finetuning prompt format: use the fixed prefix 'a photo of' appended after the visual soft-prompts as initial conditioning text for caption generation.",
            "comparison_format": null,
            "performance": "BLIP-2 finetuned with this prompt achieves state-of-the-art captioning performance in their comparisons (example: BLIP-2 ViT-g FlanT5_XL NoCaps overall CIDEr ≈121.6 and SPICE ≈15.8 as reported in Table 3).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (simple framing prompt yielded strong performance in finetuning setting)",
            "explanation_or_hypothesis": "Providing a fixed, natural-language prefix frames the generation as a captioning task and likely stabilizes outputs; authors keep LLM frozen and only train Q-Former/image encoder, relying on the prompt to steer the LLM's frozen generation behavior.",
            "counterexample_or_null_result": null,
            "uuid": "e5805.5",
            "source_info": {
                "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Flamingo: a visual language model for few-shot learning",
            "rating": 2
        },
        {
            "paper_title": "Multimodal few-shot learning with frozen language models",
            "rating": 2
        },
        {
            "paper_title": "From images to textual prompts: Zero-shot VQA with frozen large language models",
            "rating": 2
        },
        {
            "paper_title": "A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models",
            "rating": 1
        }
    ],
    "cost": 0.01570425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h1>
<p>Junnan Li Dongxu Li Silvio Savarese Steven Hoi<br>Salesforce Research<br>https://github.com/salesforce/LAVIS/tree/main/projects/blip2</p>
<h4>Abstract</h4>
<p>The cost of vision-and-language pre-training has become increasingly prohibitive due to end-toend training of large-scale models. This paper proposes BLIP-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various visionlanguage tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by $8.7 \%$ on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</p>
<h2>1. Introduction</h2>
<p>Vision-language pre-training (VLP) research has witnessed a rapid advancement in the past few years, where pre-trained models with increasingly larger scale have been developed to continuously push the state-of-the-art on various downstream tasks (Radford et al., 2021; Li et al., 2021; 2022; Wang et al., 2022a; Alayrac et al., 2022; Wang et al., 2022b). However, most state-of-the-art vision-language models incur a high computation cost during pre-training, due to end-to-end training using large-scale models and datasets.</p>
<p>Vision-language research sits at the intersection between vision and language, therefore it is naturally expected that vision-language models can harvest from the readilyavailable unimodal models from the vision and natural language communities. In this paper, we propose a generic and
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of BLIP-2's framework. We pre-train a lightweight Querying Transformer following a two-stage strategy to bridge the modality gap. The first stage bootstraps visionlanguage representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen LLM, which enables zero-shot instructed image-totext generation (see Figure 4 for more examples).
compute-efficient VLP method by bootstrapping from off-the-shelf pre-trained vision models and language models. Pre-trained vision models offer high-quality visual representation. Pre-trained language models, in particular large language models (LLMs), offer strong language generation and zero-shot transfer abilities. To reduce computation cost and counteract the issue of catastrophic forgetting, the unimodal pre-trained models remain frozen during the pre-training.</p>
<p>In order to leverage pre-trained unimodal models for VLP, it is key to facilitate cross-modal alignment. However, since LLMs have not seen images during their unimodal pretraining, freezing them makes vision-language alignment in particular challenging. In this regard, existing methods (e.g. Frozen (Tsimpoukelli et al., 2021), Flamingo (Alayrac et al., 2022)) resort to an image-to-text generation loss, which we show is insufficient to bridge the modality gap.</p>
<p>To achieve effective vision-language alignment with frozen unimodal models, we propose a Querying Transformer (QFormer) pre-trained with a new two-stage pre-training strategy. As shown in Figure 1, Q-Former is a lightweight transformer which employs a set of learnable query vectors to extract visual features from the frozen image encoder. It acts as an information bottleneck between the frozen image encoder and the frozen LLM, where it feeds the most useful</p>
<p>visual feature for the LLM to output the desired text. In the first pre-training stage, we perform vision-language representation learning which enforces the Q-Former to learn visual representation most relevant to the text. In the second pre-training stage, we perform vision-to-language generative learning by connecting the output of the Q-Former to a frozen LLM, and trains the Q-Former such that its output visual representation can be interpreted by the LLM.</p>
<p>We name our VLP framework as BLIP-2: Bootstrapping Language-Image Pre-training with frozen unimodal models. The key advantages of BLIP-2 include:</p>
<ul>
<li>BLIP-2 effectively leverages both frozen pre-trained image models and language models. We bridge the modality gap using a Q-Former pre-trained in two-stages: representation learning stage and generative learning stage. BLIP-2 achieves state-of-the-art performance on various vision-language tasks including visual question answering, image captioning, and image-text retrieval.</li>
<li>Powered by LLMs (e.g. OPT (Zhang et al., 2022), FlanT5 (Chung et al., 2022)), BLIP-2 can be prompted to perform zero-shot image-to-text generation that follows natural language instructions, which enables emerging capabilities such as visual knowledge reasoning, visual conversation, etc. (see Figure 4 for examples).</li>
<li>Due to the use of frozen unimodal models and a lightweight Q-Former, BLIP-2 is more compute-efficient than exisiting state-of-the-arts. For example, BLIP-2 outperforms Flamingo (Alayrac et al., 2022) by $8.7 \%$ on zero-shot VQAv2, while using $54 \times$ fewer trainable parameters. Furthermore, our results show that BLIP-2 is a generic method that can harvest more advanced unimodal models for better VLP performance.</li>
</ul>
<h2>2. Related Work</h2>
<h3>2.1. End-to-end Vision-Language Pre-training</h3>
<p>Vision-language pre-training aims to learn multimodal foundation models with improved performance on various vision-and-language tasks. Depending on the downstream task, different model architectures have been proposed, including the dual-encoder architecture (Radford et al., 2021; Jia et al., 2021), the fusion-encoder architecture (Tan \&amp; Bansal, 2019; Li et al., 2021), the encoder-decoder architecture (Cho et al., 2021; Wang et al., 2021b; Chen et al., 2022b), and more recently, the unified transformer architecture (Li et al., 2022; Wang et al., 2022b). Various pre-training objectives have also been proposed over the years, and have progressively converged to a few time-tested ones: image-text contrastive learning (Radford et al., 2021; Yao et al., 2022; Li et al., 2021; 2022), image-text matching (Li et al., 2021; 2022; Wang et al., 2021a), and (masked) language modeling (Li et al., 2021; 2022; Yu et al., 2022; Wang et al., 2022b).</p>
<p>Most VLP methods perform end-to-end pre-training using large-scale image-text pair datasets. As the model size keeps increasing, the pre-training can incur an extremely high computation cost. Moreover, it is inflexible for end-to-end pre-trained models to leverage readily-available unimodal pre-trained models, such as LLMs (Brown et al., 2020; Zhang et al., 2022; Chung et al., 2022).</p>
<h3>2.2. Modular Vision-Language Pre-training</h3>
<p>More similar to us are methods that leverage off-the-shelf pre-trained models and keep them frozen during VLP. Some methods freeze the image encoder, including the early work which adopts a frozen object detector to extract visual features (Chen et al., 2020; Li et al., 2020; Zhang et al., 2021), and the recent LiT (Zhai et al., 2022) which uses a frozen pre-trained image encoder for CLIP (Radford et al., 2021) pre-training. Some methods freeze the language model to use the knowledge from LLMs for vision-to-language generation tasks (Tsimpoukelli et al., 2021; Alayrac et al., 2022; Chen et al., 2022a; Mañas et al., 2023; Tiong et al., 2022; Guo et al., 2022). The key challenge in using a frozen LLM is to align visual features to the text space. To achieve this, Frozen (Tsimpoukelli et al., 2021) finetunes an image encoder whose outputs are directly used as soft prompts for the LLM. Flamingo (Alayrac et al., 2022) inserts new cross-attention layers into the LLM to inject visual features, and pre-trains the new layers on billions of image-text pairs. Both methods adopt the language modeling loss, where the language model generates texts conditioned on the image.</p>
<p>Different from existing methods, BLIP-2 can effectively and efficiently leverage both frozen image encoders and frozen LLMs for various vision-language tasks, achieving stronger performance at a lower computation cost.</p>
<h2>3. Method</h2>
<p>We propose BLIP-2, a new vision-language pre-training method that bootstraps from frozen pre-trained unimodal models. In order to bridge the modality gap, we propose a Querying Transformer (Q-Former) pre-trained in two stages: (1) vision-language representation learning stage with a frozen image encoder and (2) vision-to-language generative learning stage with a frozen LLM. This section first introduces the model architecture of Q-Former, and then delineates the two-stage pre-training procedures.</p>
<h3>3.1. Model Architecture</h3>
<p>We propose Q-Former as the trainable module to bridge the gap between a frozen image encoder and a frozen LLM. It extracts a fixed number of output features from the image encoder, independent of input image resolution. As shown in Figure 2, Q-Former consists of two transformer submodules that share the same self-attention layers: (1) an image transformer that interacts with the frozen image encoder</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. (Left) Model architecture of Q-Former and BLIP-2's first-stage vision-language representation learning objectives. We jointly optimize three objectives which enforce the queries (a set of learnable embeddings) to extract visual representation most relevant to the text. (Right) The self-attention masking strategy for each objective to control query-text interaction.</p>
<p>For visual feature extraction, (2) a text transformer that can function as both a text encoder and a text decoder. We create a set number of learnable query embeddings as input to the image transformer. The queries interact with each other through self-attention layers, and interact with frozen image features through cross-attention layers (inserted every other transformer block). The queries can additionally interact with the text through the same self-attention layers. Depending on the pre-training task, we apply different self-attention masks to control query-text interaction. We initialize Q-Former with the pre-trained weights of BERT<sub>base</sub> <em>(Devlin et al., 2019)</em>, whereas the cross-attention layers are randomly initialized. In total, Q-Former contains 188M parameters. Note that the queries are considered as model parameters.</p>
<p>In our experiments, we use 32 queries where each query has a dimension of 768 (same as the hidden dimension of the Q-Former). We use <em>Z</em> to denote the output query representation. The size of <em>Z</em> (32 × 768) is much smaller than the size of frozen image features (<em>e.g</em>. 257 × 1024 for ViT-L/14). This bottleneck architecture works together with our pre-training objectives into forcing the queries to extract visual information that is most relevant to the text.</p>
<h3>3.2 Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder</h3>
<p>In the representation learning stage, we connect Q-Former to a frozen image encoder and perform pre-training using image-text pairs. We aim to train the Q-Former such that the queries can learn to extract visual representation that is most informative of the text. Inspired by BLIP <em>(Li et al., 2022)</em>, we jointly optimize three pre-training objectives that share the same input format and model parameters. Each objective employs a different attention masking strategy between queries and text to control their interaction (see Figure 2).</p>
<h4>Image-Text Contrastive Learning (ITC)</h4>
<p>In the representation learning stage, we connect Q-Former to a frozen image encoder and perform pre-training using image-text pairs. We aim to train the Q-Former such that the queries can learn to extract visual representation that is most informative of the text. Inspired by BLIP <em>(Li et al., 2022)</em>, we jointly optimize three pre-training objectives that share the same input format and model parameters. Each objective employs a different attention masking strategy between queries and text to control their interaction (see Figure 2).</p>
<h4>Image-Text Contrastive Learning (ITC)</h4>
<p>Image-grammed text representations (ITG) learn to align image representation and text representation such that their mutual information is maximized. It achieves so by contrasting the image-text similarity of a positive pair against those of negative pairs. We align the output query representation <em>Z</em> from the image transformer with the text representation <em>t</em> from the text transformer, where <em>t</em> is the output embedding of the [CLS] token. Since <em>Z</em> contains multiple output embeddings (one from each query), we first compute the pairwise similarity between each query output and <em>t</em>, and then select the highest one as the image-text similarity. To avoid information leak, we employ a unimodal self-attention mask, where the queries and text are not allowed to see each other. Due to the use of a frozen image encoder, we can fit more samples per GPU compared to end-to-end methods. Therefore, we use in-batch negatives instead of the momentum queue in BLIP.</p>
<h4>Image-grounded Text Generation (ITG)</h4>
<p>Image-grounded text generation (ITG) loss trains the Q-Former to generate texts, given input images as the condition. Since the architecture of Q-Former does not allow direct interactions between the frozen image encoder and the text tokens, the information required for generating the text must be first extracted by the queries, and then passed to the text tokens via self-attention layers. Therefore, the queries are forced to extract visual features that capture all the information about the text. We employ a multimodal causal self-attention mask to control query-text interaction, similar to the one used in UniLM <em>(Dong et al., 2019)</em>. The queries can attend to each other but not the text tokens. Each text token can attend to all queries and its previous text tokens. We also replace the [CLS] token with a new [DEC] token as the first text token to signal the decoding task.</p>
<h4>Image-Text Matching (ITM)</h4>
<p>Image-text matching (ITM) aims to learn fine-grained alignment between image and text representation. It is a binary classification task where the model is asked to predict whether an image-text pair is positive (matched) or negative (unmatched). We use a bi-directional self-attention mask where all queries and texts can attend to each other. The output query embeddings <em>Z</em> thus capture multimodal information. We feed each output query embedding into a two-class linear classifier to obtain a logit, and average the logits across all queries as the output matching score. We adopt the hard negative mining strategy from <em>Li et al. (2021, 2022)</em> to create informative negative pairs.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. BLIP-2's second-stage vision-to-language generative pre-training, which bootstraps from frozen large language models (LLMs). (Top) Bootstrapping a decoder-based LLM (e.g. OPT). (Bottom) Bootstrapping an encoder-decoder-based LLM (e.g. FlanT5). The fully-connected layer adapts from the output dimension of the Q-Former to the input dimension of the chosen LLM.</p>
<h3>3.3. Bootstrap Vision-to-Language Generative Learning from a Frozen LLM</h3>
<p>In the generative pre-training stage, we connect QFormer (with the frozen image encoder attached) to a frozen LLM to harvest the LLM's generative language capability. As shown in Figure 3, we use a fully-connected (FC) layer to linearly project the output query embeddings $Z$ into the same dimension as the text embedding of the LLM. The projected query embeddings are then prepended to the input text embeddings. They function as soft visual prompts that condition the LLM on visual representation extracted by the Q-Former. Since the Q-Former has been pre-trained to extract language-informative visual representation, it effectively functions as an information bottleneck that feeds the most useful information to the LLM while removing irrelevant visual information. This reduces the burden of the LLM to learn vision-language alignment, thus mitigating the catastrophic forgetting problem.</p>
<p>We experiment with two types of LLMs: decoder-based LLMs and encoder-decoder-based LLMs. For decoderbased LLMs, we pre-train with the language modeling loss, where the frozen LLM is tasked to generate the text conditioned on the visual representation from Q-Former. For encoder-decoder-based LLMs, we pre-train with the prefix language modeling loss, where we split a text into two parts. The prefix text is concatenated with the visual representation as input to the LLM's encoder. The suffix text is used as the generation target for the LLM's decoder.</p>
<h3>3.4. Model Pre-training</h3>
<p>Pre-training data. We use the same pre-training dataset as BLIP with 129M images in total, including COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), and 115M images from the LAION400M dataset (Schuhmann et al., 2021). We adopt the CapFilt method (Li et al., 2022) to create synthetic captions for the web images. Specifically, we generate 10
captions using the $\mathrm{BLIP}_{\text {large }}$ captioning model, and rank the synthetic captions along with the original web caption based on the image-text similarity produced by a CLIP ViT-L/14 model. We keep top-two captions per image as training data and randomly sample one at each pre-training step.</p>
<p>Pre-trained image encoder and LLM. For the frozen image encoder, we explore two state-of-the-art pre-trained vision transformer models: (1) ViT-L/14 from CLIP (Radford et al., 2021) and (2) ViT-g/14 from EVA-CLIP (Fang et al., 2022). We remove the last layer of the ViT and uses the second last layer's output features, which leads to slightly better performance. For the frozen language model, we explore the unsupervised-trained OPT model family (Zhang et al., 2022) for decoder-based LLMs, and the instruction-trained FlanT5 model family (Chung et al., 2022) for encoder-decoder-based LLMs.</p>
<p>Pre-training settings. We pre-train for 250k steps in the first stage and 80k steps in the second stage. We use a batch size of 2320/1680 for ViT-L/ViT-g in the first stage and a batch size of 1920/1520 for OPT/FlanT5 in the second stage. During pre-training, we convert the frozen ViTs' and LLMs' parameters into FP16, except for FlanT5 where we use BFloat16. We found no performance degradation compared to using 32-bit models. Due to the use of frozen models, our pre-training is more computational friendly than existing large-scale VLP methods. For example, using a single 16-A100(40G) machine, our largest model with ViT-g and FlanT5-XXL requires less than 6 days for the first stage and less than 3 days for the second stage.</p>
<p>The same set of pre-training hyper-parameters are used for all models. We use the AdamW (Loshchilov \&amp; Hutter, 2017) optimizer with $\beta_{1}=0.9, \beta_{1}=0.98$, and a weight decay of 0.05 . We use a cosine learning rate decay with a peak learning rate of $1 \mathrm{e}-4$ and a linear warmup of 2 k steps. The minimum learning rate at the second stage is $5 \mathrm{e}-5$. We use images of size $224 \times 224$, augmented with random resized cropping and horizontal flipping.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Selected examples of instructed zero-shot image-to-text generation using a BLIP-2 model w/ ViT-g and FlanT5 ${ }_{\mathrm{XXL}}$, where it shows a wide range of capabilities including visual conversation, visual knowledge reasoning, visual commensense reasoning, storytelling, personalized image-to-text generation, etc.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">#Trainable <br> Params</th>
<th style="text-align: center;">Open- <br> sourced?</th>
<th style="text-align: center;">Visual Question Answering VQAv2 (test-dev) <br> VQA acc.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Image Captioning <br> NoCaps (val) <br> CIDEr</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Image-Text Retrieval <br> Flickr (test) <br> IR@1</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BLIP (Li et al., 2022)</td>
<td style="text-align: center;">583 M</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">113.2</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">86.7</td>
</tr>
<tr>
<td style="text-align: center;">SimVLM (Wang et al., 2021b)</td>
<td style="text-align: center;">1.4B</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">112.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BEIT-3 (Wang et al., 2022b)</td>
<td style="text-align: center;">1.9B</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: center;">Flamingo (Alayrac et al., 2022)</td>
<td style="text-align: center;">10.2B</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BLIP-2</td>
<td style="text-align: center;">188 M</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">121.6</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">89.7</td>
</tr>
</tbody>
</table>
<p>Table 1. Overview of BLIP-2 results on various zero-shot vision-language tasks. Compared with previous state-of-the-art models. BLIP-2 achieves the highest zero-shot performance while requiring the least number of trainable parameters during vision-language pre-training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">#Trainable <br> Params</th>
<th style="text-align: left;">#Total <br> Params</th>
<th style="text-align: center;">VQAv2 <br> val</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OK-VQA <br> test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GQA <br> test-dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VL-T5 $5_{\text {no-typ }}$</td>
<td style="text-align: left;">224 M</td>
<td style="text-align: left;">269 M</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FewVLM (Jin et al., 2022)</td>
<td style="text-align: left;">740 M</td>
<td style="text-align: left;">785 M</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Frozen (Tsimpoukelli et al., 2021)</td>
<td style="text-align: left;">40 M</td>
<td style="text-align: left;">7.1 B</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">VLKD (Dai et al., 2022)</td>
<td style="text-align: left;">406 M</td>
<td style="text-align: left;">832 M</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Flamingo3B (Alayrac et al., 2022)</td>
<td style="text-align: left;">1.4B</td>
<td style="text-align: left;">3.2B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Flamingo9B (Alayrac et al., 2022)</td>
<td style="text-align: left;">1.8B</td>
<td style="text-align: left;">9.3B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Flamingo80B (Alayrac et al., 2022)</td>
<td style="text-align: left;">10.2B</td>
<td style="text-align: left;">80B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">$\mathbf{5 0 . 6}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-L OPT ${ }_{2.7 B}$</td>
<td style="text-align: left;">104 M</td>
<td style="text-align: left;">3.1B</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-g OPT ${ }_{2.7 B}$</td>
<td style="text-align: left;">107 M</td>
<td style="text-align: left;">3.8B</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-g OPT ${ }_{6.7 B}$</td>
<td style="text-align: left;">108 M</td>
<td style="text-align: left;">7.8B</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-L FlanT5 ${ }_{\text {XL }}$</td>
<td style="text-align: left;">103 M</td>
<td style="text-align: left;">3.4B</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-g FlanT5 ${ }_{\text {XL }}$</td>
<td style="text-align: left;">107 M</td>
<td style="text-align: left;">4.1B</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-g FlanT5 ${ }_{\text {XXL }}$</td>
<td style="text-align: left;">108 M</td>
<td style="text-align: left;">12.1B</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2. Comparison with state-of-the-art methods on zero-shot visual question answering.</p>
<h2>4. Experiment</h2>
<p>Table 1 provides an overview of the performance of BLIP-2 on various zero-shot vision-language tasks. Compared to previous state-of-the-art models, BLIP-2 achieves improved performance while requiring substantially fewer number of trainable parameters during vision-language pre-training.</p>
<h3>4.1. Instructed Zero-shot Image-to-Text Generation</h3>
<p>BLIP-2 effectively enables a LLM to understand images while preserving its capability in following text prompts, which allows us to control image-to-text generation with instructions. We simply append the text prompt after the visual prompt as input to the LLM. Figure 4 shows examples to demonstrate a wide range of zero-shot image-to-text capabilities including visual knowledge reasoning, visual commensense reasoning, visual conversation, personalized image-to-text generation, etc.</p>
<p>Zero-shot VQA. We perform quantitative evaluation on the zero-shot visual question answering task. For OPT models, we use the prompt "Question: {} Answer:". For FlanT5 models, we use the prompt "Question: {} Short answer:". During generation, we use beam search with a beam width of 5 . We also set the length-penalty to -1 which encourages shorter answers that align better with human annotation.</p>
<p>As shown in Table 2. BLIP-2 achieves state-of-the-art result on the VQAv2 (Goyal et al., 2017) and GQA (Hudson \&amp; Manning, 2019) datasets. It outperforms Flamingo80B by $8.7 \%$ on VQAv2, despite having 54x fewer trainable parameters. On the OK-VQA (Marino et al., 2019) dataset, BLIP-2 comes secondary to Flamingo80B. We hypothesis that this is because OK-VQA focuses more on open-world knowledge than visual understanding, and the 70B Chinchilla (Hoffmann et al., 2022) language model from Flamingo80B possesses more knowledge than the 11B FlanT5 $5_{\text {XXL }}$.</p>
<p>We make a promising observation from Table 2: a stronger image encoder or a stronger LLM both lead to better performance. This observation is supported by several facts: (1) ViT-g outperforms ViT-L for both OPT and FlanT5. (2) Within the same LLM family, larger models outperform smaller ones. (3) FlanT5, an instruction-tuned LLM, outperforms the unsupervised-trained OPT on VQA. This observation validates BLIP-2 as a generic vision-language pre-training method that can efficiently harvest the rapid advances in vision and natural language communities.</p>
<h2>Effect of Vision-Language Representation Learning.</h2>
<p>The first-stage representation learning pre-trains the QFormer to learn visual features relevant to the text, which reduces the burden of the LLM to learn vision-language alignment. Without the representation learning stage, Q-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">#Trainable <br> Params</th>
<th style="text-align: center;">NoCaps Zero-shot (validation set)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">COCO Fine-tuned</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">in-domain</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">near-domain</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">out-domain</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">overall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Karpathy test</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">B@4</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR (Li et al., 2020)</td>
<td style="text-align: center;">345M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">127.8</td>
</tr>
<tr>
<td style="text-align: center;">VinVL (Zhang et al., 2021)</td>
<td style="text-align: center;">345M</td>
<td style="text-align: center;">103.1</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">129.3</td>
</tr>
<tr>
<td style="text-align: center;">BLIP (Li et al., 2022)</td>
<td style="text-align: center;">446M</td>
<td style="text-align: center;">114.9</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">112.1</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">115.3</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">113.2</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">136.7</td>
</tr>
<tr>
<td style="text-align: center;">OFA (Wang et al., 2022a)</td>
<td style="text-align: center;">930M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">145.3</td>
</tr>
<tr>
<td style="text-align: center;">Flamingo (Alayrac et al., 2022)</td>
<td style="text-align: center;">10.6B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">138.1</td>
</tr>
<tr>
<td style="text-align: center;">SimVLM (Wang et al., 2021b)</td>
<td style="text-align: center;">$\sim 1.4 B$</td>
<td style="text-align: center;">113.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">110.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">115.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">112.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">143.3</td>
</tr>
<tr>
<td style="text-align: center;">BLIP-2 ViT-g OPT_{2.7B}</td>
<td style="text-align: center;">1.1B</td>
<td style="text-align: center;">123.0</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">117.8</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">123.4</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">119.7</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">145.8</td>
</tr>
<tr>
<td style="text-align: center;">BLIP-2 ViT-g OPT_{6.7B}</td>
<td style="text-align: center;">1.1B</td>
<td style="text-align: center;">123.7</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">119.2</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">124.4</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">121.0</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">145.2</td>
</tr>
<tr>
<td style="text-align: center;">BLIP-2 ViT-g FlanT5 ${ }_{\mathrm{XL}}$</td>
<td style="text-align: center;">1.1B</td>
<td style="text-align: center;">123.7</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">120.2</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">124.8</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">121.6</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">144.5</td>
</tr>
</tbody>
</table>
<p>Table 3. Comparison with state-of-the-art image captioning methods on NoCaps and COCO Caption. All methods optimize the crossentropy loss during finetuning. C: CIDEr, S: SPICE, B@4: BLEU@4.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Effect of vision-language representation learning on vision-to-language generative learning. Without representation learning, the Q-Former fails the bridge the modality gap, leading to significantly lower performance on zero-shot VQA.</p>
<p>Former relies solely on the vision-to-language generative learning to bridge the modality gap, which is similar to the Perceiver Resampler in Flamingo. Figure 5 shows the effect of representation learning on generative learning. Without representation learning, both types of LLMs give substantially lower performance on zero-shot VQA. In particular, OPT suffers from catastrophic forgetting where performance drastically degrades as training proceeds.</p>
<h3>4.2. Image Captioning</h3>
<p>We finetune BLIP-2 models for the image captioning task, which asks the model to generate a text description for the image's visual content. We use the prompt "a photo of" as an initial input to the LLM and trains the model to generate the caption with the language modeling loss. We keep the LLM frozen during finetuning, and updates the parameters of the Q-Former together with the image encoder. We experiment with ViT-g and various LLMs. Detailed hyperparameters can be found in the appendix. We perform finetuning on COCO, and evaluate on both COCO test set and zero-shot transfer to NoCaps (Agrawal et al., 2019) validation set.</p>
<p>The results are shown in Table 3. BLIP-2 achieves state-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">#Trainable <br> Params</th>
<th style="text-align: left;">VQAv2 <br> test-dev</th>
<th style="text-align: left;">test-std</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Open-ended generation models</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ALBEF (Li et al., 2021)</td>
<td style="text-align: left;">314 M</td>
<td style="text-align: left;">75.84</td>
<td style="text-align: left;">76.04</td>
</tr>
<tr>
<td style="text-align: left;">BLIP (Li et al., 2022)</td>
<td style="text-align: left;">385 M</td>
<td style="text-align: left;">78.25</td>
<td style="text-align: left;">78.32</td>
</tr>
<tr>
<td style="text-align: left;">OFA (Wang et al., 2022a)</td>
<td style="text-align: left;">930 M</td>
<td style="text-align: left;">82.00</td>
<td style="text-align: left;">82.00</td>
</tr>
<tr>
<td style="text-align: left;">Flamingo80B (Alayrac et al., 2022)</td>
<td style="text-align: left;">10.6 B</td>
<td style="text-align: left;">82.00</td>
<td style="text-align: left;">82.10</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-g FlanT5 ${ }_{\mathrm{XL}}$</td>
<td style="text-align: left;">1.2 B</td>
<td style="text-align: left;">81.55</td>
<td style="text-align: left;">81.66</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-g OPT ${ }_{2.7 B}$</td>
<td style="text-align: left;">1.2 B</td>
<td style="text-align: left;">81.59</td>
<td style="text-align: left;">81.74</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2 ViT-g OPT ${ }_{6.7 B}$</td>
<td style="text-align: left;">1.2 B</td>
<td style="text-align: left;">$\mathbf{8 2 . 1 9}$</td>
<td style="text-align: left;">$\mathbf{8 2 . 3 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Closed-ended classification models</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">VinVL</td>
<td style="text-align: left;">345 M</td>
<td style="text-align: left;">76.52</td>
<td style="text-align: left;">76.60</td>
</tr>
<tr>
<td style="text-align: left;">SimVLM (Wang et al., 2021b)</td>
<td style="text-align: left;">$\sim 1.4 \mathrm{~B}$</td>
<td style="text-align: left;">80.03</td>
<td style="text-align: left;">80.34</td>
</tr>
<tr>
<td style="text-align: left;">CoCa (Yu et al., 2022)</td>
<td style="text-align: left;">2.1 B</td>
<td style="text-align: left;">82.30</td>
<td style="text-align: left;">82.30</td>
</tr>
<tr>
<td style="text-align: left;">BEIT-3 (Wang et al., 2022b)</td>
<td style="text-align: left;">1.9 B</td>
<td style="text-align: left;">$\mathbf{8 4 . 1 9}$</td>
<td style="text-align: left;">$\mathbf{8 4 . 0 3}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Comparison with state-of-the-art models fine-tuned for visual question answering.
of-the-art performance with significant improvement on NoCaps over existing methods, demonstrating strong generalization ability to out-domain images.</p>
<h3>4.3. Visual Question Answering</h3>
<p>Given annotated VQA data, we finetune the parameters of the Q-Former and the image encoder while keeping the LLM frozen. We finetune with the open-ended answer generation loss, where the LLM receives Q-Former's output and the question as input, and is asked to generate the answer. In order to extract image features that are more relevant to the question, we additionally condition Q-Former on the question. Specifically, the question tokens are given as input to the Q-Former and interact with the queries via the self-attention layers, which can guide the Q-Former's crossattention layers to focus on more informative image regions.</p>
<p>Following BLIP, our VQA data includes the training and validation splits from VQAv2, as well as training samples from Visual Genome. Table 4 demonstrates the state-of-theart results of BLIP-2 among open-ended generation models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">#Trainable <br> Params</th>
<th style="text-align: center;">Flickr30K Zero-shot (1K test set)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">COCO Fine-tuned (5K test set)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Image $\rightarrow$ Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text $\rightarrow$ Image</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Image $\rightarrow$ Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text $\rightarrow$ Image</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
</tr>
<tr>
<td style="text-align: center;">Dual-encoder models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CLIP (Radford et al., 2021)</td>
<td style="text-align: center;">428 M</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ALIGN (Jia et al., 2021)</td>
<td style="text-align: center;">820 M</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: center;">FILIP (Yao et al., 2022)</td>
<td style="text-align: center;">417 M</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">90.6</td>
</tr>
<tr>
<td style="text-align: center;">Florence (Yuan et al., 2021)</td>
<td style="text-align: center;">893 M</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BEIT-3(Wang et al., 2022b)</td>
<td style="text-align: center;">1.9 B</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">92.8</td>
</tr>
<tr>
<td style="text-align: center;">Fusion-encoder models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UNITER (Chen et al., 2020)</td>
<td style="text-align: center;">303 M</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">88.0</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR (Li et al., 2020)</td>
<td style="text-align: center;">345 M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">88.5</td>
</tr>
<tr>
<td style="text-align: center;">VinVL (Zhang et al., 2021)</td>
<td style="text-align: center;">345 M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">90.3</td>
</tr>
<tr>
<td style="text-align: center;">Dual encoder + Fusion encoder reranking</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ALBEF (Li et al., 2021)</td>
<td style="text-align: center;">233 M</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">90.5</td>
</tr>
<tr>
<td style="text-align: center;">BLIP (Li et al., 2022)</td>
<td style="text-align: center;">446 M</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">91.8</td>
</tr>
<tr>
<td style="text-align: center;">BLIP-2 ViT-L</td>
<td style="text-align: center;">474 M</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">91.8</td>
</tr>
<tr>
<td style="text-align: center;">BLIP-2 ViT-g</td>
<td style="text-align: center;">1.2 B</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">92.6</td>
</tr>
</tbody>
</table>
<p>Table 5. Comparison with state-of-the-art image-text retrieval methods, finetuned on COCO and zero-shot transferred to Flickr30K.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">COCO finetuning <br> objectives</th>
<th style="text-align: center;">Image $\rightarrow$ Text</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Text $\rightarrow$ Image</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
</tr>
<tr>
<td style="text-align: left;">ITC + ITM</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">87.1</td>
</tr>
<tr>
<td style="text-align: left;">ITC + ITM + ITG</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">87.7</td>
</tr>
</tbody>
</table>
<p>Table 6. The image-grounded text generation (ITG) loss improves image-text retrieval performance by enforcing the queries to extract language-relevant visual features.</p>
<h3>4.4. Image-Text Retrieval</h3>
<p>Since image-text retrieval does not involve language generation, we directly finetune the first-stage-pretrained model w/o LLM. Specifically, we finetune the image encoder together with Q-Former on COCO using the same objectives (i.e. ITC, ITM, and ITG) as pre-training. We then evaluate the model for both image-to-text retrieval and text-to-image retrieval on COCO and Flickr30K (Plummer et al., 2015) datasets. During inference, we follow Li et al. (2021; 2022) which first select $k=128$ candidates based on the imagetext feature similarity, followed by a re-ranking based on pairwise ITM scores. We experiment with both ViT-L and ViT-g as the image encoder. Detailed hyperparameters can be found in the appendix.</p>
<p>The results are shown in Table 5. BLIP-2 achieves state-of-the-art performance with significant improvement over existing methods on zero-shot image-text retrieval.</p>
<p>The ITC and ITM losses are essential for image-text retrieval as they directly learn image-text similarity. In Table 6, we show that the ITG (image-grounded text generation) loss is also beneficial for image-text retrieval. This result supports our intuition in designing the representation learning objectives: the ITG loss enforces the queries to extract visual features most relevant to the text, thus improving visionlanguage alignment.</p>
<h2>5. Limitation</h2>
<p>Recent LLMs can perform in-context learning given fewshot examples. However, our experiments with BLIP-2 do not observe an improved VQA performance when providing the LLM with in-context VQA examples. We attribute the lack of in-context learning capability to our pretraining dataset, which only contains a single image-text pair per sample. The LLMs cannot learn from it the correlation among multiple image-text pairs in a single sequence. The same observation is also reported in the Flamingo paper, which uses a close-sourced interleaved image and text dataset (M3W) with multiple image-text pairs per sequence. We aim to create a similar dataset in future work.</p>
<p>BLIP-2's image-to-text generation could have unsatisfactory results due to various reasons including inaccurate knowledge from the LLM, activating the incorrect reasoning path, or not having up-to-date information about new image content (see Figure 7). Furthermore, due to the use of frozen models, BLIP-2 inherits the risks of LLMs, such as outputting offensive language, propagating social bias, or leaking private information. Remediation approaches include using instructions to guide model's generation or training on a filtered dataset with harmful content removed.</p>
<h2>6. Conclusion</h2>
<p>We propose BLIP-2, a generic and compute-efficient method for vision-language pre-training that leverages frozen pretrained image encoders and LLMs. BLIP-2 achieves state-of-the-art performance on various vision-language tasks while having a small amount of trainable parameters during pre-training. BLIP-2 also demonstrates emerging capabilities in zero-shot instructed image-to-text generation. We consider BLIP-2 as an important step towards building a multimodal conversational AI agent.</p>
<h2>References</h2>
<p>Agrawal, H., Anderson, P., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., and Lee, S. nocaps: novel object captioning at scale. In ICCV, pp. 8947-8956, 2019.</p>
<p>Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: a visual language model for fewshot learning. arXiv preprint arXiv:2204.14198, 2022.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), NeurIPS, 2020.</p>
<p>Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.</p>
<p>Chen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. In CVPR, pp. 18009-18019, 2022a.</p>
<p>Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A. J., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., Kolesnikov, A., Puigcerver, J., Ding, N., Rong, K., Akbari, H., Mishra, G., Xue, L., Thapliyal, A., Bradbury, J., Kuo, W., Seyedhosseini, M., Jia, C., Ayan, B. K., Riquelme, C., Steiner, A., Angelova, A., Zhai, X., Houlsby, N., and Soricut, R. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022b.</p>
<p>Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. UNITER: universal image-text representation learning. In ECCV, volume 12375, pp. $104-120,2020$.</p>
<p>Cho, J., Lei, J., Tan, H., and Bansal, M. Unifying vision-and-language tasks via text generation. arXiv preprint arXiv:2102.02779, 2021.</p>
<p>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S.,</p>
<p>Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao, V. Y., Huang, Y., Dai, A. M., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Dai, W., Hou, L., Shang, L., Jiang, X., Liu, Q., and Fung, P. Enabling multimodal generation on CLIP via visionlanguage knowledge distillation. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), ACL Findings, pp. 23832395, 2022.</p>
<p>Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T. (eds.), NAACL, pp. 4171-4186, 2019.</p>
<p>Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. Unified language model pre-training for natural language understanding and generation. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E. B., and Garnett, R. (eds.), NeurIPS, pp. 13042-13054, 2019.</p>
<p>Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint arXiv:2211.07636, 2022.</p>
<p>Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In CVPR, pp. 6325-6334, 2017.</p>
<p>Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., and Hoi, S. C. H. From images to textual prompts: Zero-shot VQA with frozen large language models. In CVPR, 2022.</p>
<p>Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. v. d., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Hudson, D. A. and Manning, C. D. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, pp. 6700-6709, 2019.</p>
<p>Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.</p>
<p>Jin, W., Cheng, Y., Shen, Y., Chen, W., and Ren, X. A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), ACL, pp. 2763-2775, 2022.</p>
<p>Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L., Shamma, D. A., Bernstein, M. S., and Fei-Fei, L. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123(1):32-73, 2017.</p>
<p>Li, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong, C., and Hoi, S. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS, 2021.</p>
<p>Li, J., Li, D., Xiong, C., and Hoi, S. C. H. BLIP: bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In ICML, pp. 12888-12900, 2022.</p>
<p>Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, pp. 121-137, 2020.</p>
<p>Lin, T., Maire, M., Belongie, S. J., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft COCO: common objects in context. In Fleet, D. J., Pajdla, T., Schiele, B., and Tuytelaars, T. (eds.), ECCV, volume 8693, pp. 740-755, 2014.</p>
<p>Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.</p>
<p>Mañas, O., Rodríguez, P., Ahmadi, S., Nematzadeh, A., Goyal, Y., and Agrawal, A. MAPL: parameter-efficient adaptation of unimodal pre-trained models for visionlanguage few-shot prompting. In EACL, 2023.</p>
<p>Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Okvqa: A visual question answering benchmark requiring external knowledge. In CVPR, 2019.</p>
<p>Ordonez, V., Kulkarni, G., and Berg, T. L. Im2text: Describing images using 1 million captioned photographs. In Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F. C. N., and Weinberger, K. Q. (eds.), NIPS, pp. 1143-1151, 2011.</p>
<p>Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV, pp. 2641-2649, 2015.</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.</p>
<p>Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clipfiltered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.</p>
<p>Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Gurevych, I. and Miyao, Y. (eds.), ACL, pp. 2556-2565, 2018.</p>
<p>Tan, H. and Bansal, M. LXMERT: learning cross-modality encoder representations from transformers. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), EMNLP, pp. 50995110, 2019.</p>
<p>Tiong, A. M. H., Li, J., Li, B., Savarese, S., and Hoi, S. C. H. Plug-and-play VQA: zero-shot VQA by conjoining large pretrained models with zero training. In EMNLP Findings, 2022.</p>
<p>Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S. M. A., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), NeurIPS, pp. 200-212, 2021.</p>
<p>Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., and Yang, H. OFA: unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvári, C., Niu, G., and Sabato, S. (eds.), ICML, pp. 23318-23340, 2022a.</p>
<p>Wang, W., Bao, H., Dong, L., and Wei, F. Vlmo: Unified vision-language pre-training with mixture-of-modalityexperts. arXiv preprint arXiv:2111.02358, 2021a.</p>
<p>Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., and Wei, F. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022b.</p>
<p>Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021b.</p>
<p>Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., and Xu, C. FILIP: fine-grained interactive language-image pre-training. In $I C L R, 2022$.</p>
<p>Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are imagetext foundation models. arXiv preprint arXiv:2205.01917, 2022.</p>
<p>Yuan, L., Chen, D., Chen, Y., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., and Zhang, P. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.</p>
<p>Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., and Beyer, L. Lit: Zero-shot transfer with locked-image text tuning. In CVPR, pp. 18102-18112, 2022.</p>
<p>Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., and Gao, J. Vinvl: Making visual representations matter in vision-language models. arXiv preprint arXiv:2101.00529, 2021.</p>
<p>Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M. T., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLM</th>
<th style="text-align: center;">FlanT5 $5_{\mathrm{XL}}$</th>
<th style="text-align: center;">$\mathrm{OPT}_{2.7 \mathrm{~B}}$</th>
<th style="text-align: center;">$\mathrm{OPT}_{6.7 \mathrm{~B}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fine-tuning epochs</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Warmup steps</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">AdamW $\beta$</td>
<td style="text-align: center;">$(0.9,0.999)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Drop path</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Image resolution</td>
<td style="text-align: center;">364</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Prompt</td>
<td style="text-align: center;">"a photo of"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Inference beam size</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Layer-wise learning rate decay for ViT</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.95</td>
</tr>
</tbody>
</table>
<p>Table 7. Hyperparameters for fine-tuning BLIP-2 with ViT-g on COCO captioning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLM</th>
<th style="text-align: center;">FlanT5 $5_{\mathrm{XL}}$</th>
<th style="text-align: center;">$\mathrm{OPT}_{2.7 \mathrm{~B}}$</th>
<th style="text-align: center;">$\mathrm{OPT}_{6.7 \mathrm{~B}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fine-tuning epochs</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Warmup steps</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">AdamW $\beta$</td>
<td style="text-align: center;">$(0.9,0.999)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Drop path</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Image resolution</td>
<td style="text-align: center;">490</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Prompt</td>
<td style="text-align: center;">"Question: $}$ Answer:"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Inference beam size</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Layer-wise learning rate decay for ViT</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.9</td>
</tr>
</tbody>
</table>
<p>Table 8. Hyperparameters for fine-tuning BLIP-2 with ViT-g on VQA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Image Encoder</th>
<th style="text-align: right;">ViT-L/14</th>
<th style="text-align: right;">ViT-g/14</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fine-tuning epochs</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Warmup steps</td>
<td style="text-align: right;">1000</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: right;">$5 \mathrm{e}-6$</td>
<td style="text-align: right;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: right;">224</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">AdamW $\beta$</td>
<td style="text-align: right;">$(0.9,0.98)$</td>
<td style="text-align: right;">$(0.9,0.999)$</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Drop path</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Image resolution</td>
<td style="text-align: right;">364</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Layer-wise learning rate decay for ViT</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.95</td>
</tr>
</tbody>
</table>
<p>Table 9. Hyperparameters for fine-tuning BLIP-2 on COCO image-text retrieval.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Incorrect output examples for instructed zero-shot image-to-text generation using a BLIP-2 model w/ ViT-g and FlanT5 $5_{\mathrm{XXL}}$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Model architecture for VQA finetuning, where the LLM receives Q-Former's output and the question as input, then predicts answers. We also provide the question as a condition to Q-Former, such that the extracted image features are more relevant to the question.</p>            </div>
        </div>

    </div>
</body>
</html>
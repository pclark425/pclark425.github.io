<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5214 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5214</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5214</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-264406178</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.13522v2.pdf" target="_blank">Teaching Language Models to Self-Improve through Interactive Demonstrations</a></p>
                <p><strong>Paper Abstract:</strong> The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve LLaMA-7B’s performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on *its own generations*. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its *own* mistakes is crucial for small models to improve their performance.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5214.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5214.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TRIPOST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TRIPOST (Trajectory Interactive Post-processing Training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative training algorithm that teaches a smaller LM to self-improve by (1) having the small model generate attempts, (2) using expert LLMs or scripts to produce feedback and incremental improvements on those attempts, (3) post-processing and balancing the resulting single-step improvement triplets, and (4) re-training the small model with weighted supervised learning; iterations are repeated (typically t=3).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B (as M_θ)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA family small model; 7 billion parameters; decoder-only transformer used as the target small model to be trained with TRIPOST (experiments also include LLaMA-2 7B in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>TRIPOST (interactive trajectory editing / generate-then-reflect replay)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The small model generates an initial attempt; a separate feedback module (FBK: text-davinci-003 or scripted checker) identifies the first error and emits feedback; an improvement module (IMP: Codex) generates an improved attempt conditioned on the previous attempt and the feedback, but only edits steps from the first error onward. Collected trajectories are split into single-step triplets (x_att_i, x_fb_i, x_att_{i+1}), filtered, re-balanced (controlled by hyperparameter p) and used to finetune the small model with weighted cross-entropy emphasizing feedback/improvement tokens. The process repeats for t iterations (typically t=3).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-Bench Hard: Multistep Arithmetic, Word Sorting, Date Understanding, Logical Deduction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Four challenging math and reasoning tasks from BIG-Bench Hard: (i) multistep arithmetic (scriptable), (ii) word sorting (scriptable), (iii) date understanding (unscriptable), (iv) logical deduction (unscriptable).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>TRIPOST-trained LLaMA-7B improved task performance relative to baselines; paper reports improvements up to 7.13% absolute on math and reasoning tasks for LLaMA-7B (across the evaluated BIG-Bench Hard tasks). TRIPOST(t=3) models achieve the best metrics reported across the baselines (see main tables), and show measurable SI.Freq and SI.Contrib values (the model both attempts self-improvement and those attempts sometimes convert to correct final answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines include: ft_rationale (finetune on gold step-by-step rationales), LMSI (fine-tuning on self-generated self-consistent rationales), and ft_SI_demo (finetune on LLM-generated self-improvement demonstrations). TRIPOST outperforms these baselines; ft_SI_demo often slightly degrades performance versus ft_rationale. Exact baseline accuracies vary by task (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: TRIPOST-trained LLaMA-7B shows higher final-answer accuracy than ft_rationale, LMSI, and ft_SI_demo across the four BIG-Bench Hard tasks; the authors report up to 7.13% absolute improvement for LLaMA-7B. Ablations show that interaction (using the small-model's own attempts), filtering, dataset balancing (p), and weighted supervised learning are each important for the gains; TRIPOST-auto and TRIPOST with fixed p both outperform baselines. Qualitative: TRIPOST produces improving trajectories tailored to the small model's error modes, and SI.Contrib analysis demonstrates that some fraction of final-answer gains come from successful self-improvement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Collecting improving trajectories depends on the ability of FBK/IMP (LLMs or scripts) to generate useful feedback/improvements; as the small model improves, fewer incorrect attempts are available and feedback/improvement generation becomes harder. Using too large a proportion p of improvement triplets causes the small model to attempt self-improvement even when unnecessary, harming accuracy. TRIPOST requires external LLMs/scripts (cost) and was only evaluated on 7B models and on math/reasoning tasks; generalization to other tasks or larger model sizes is not proven in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Language Models to Self-Improve through Interactive Demonstrations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5214.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5214.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TRIPOST-auto</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TRIPOST-auto (automatic rebalancing variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of TRIPOST that omits manual re-balancing of improvement vs directly-correct examples and instead includes all edited improvement tuples and directly correct attempts, resulting in an automatic effective p proportional to current model error rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B (as M_θ)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 7B LLaMA target model as TRIPOST experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>TRIPOST-auto (generate-then-reflect with automatic dataset proportioning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Collect edited trajectories through interaction (FBK+IMP), include all x_imp and x_T without explicit manual balancing; train on the resulting dataset with weighted SL. Because the fraction of x_imp in training data naturally reflects the model's current error rate, the model rarely attempts to self-improve at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-Bench Hard: same four tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: multistep arithmetic, word sorting, date understanding, logical deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>TRIPOST-auto sometimes yields even better overall performance than TRIPOST with fixed re-balancing, especially on unscriptable tasks (paper reports TRIPOST-auto producing higher total accuracy for some tasks and iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to ft_rationale and ft_SI_demo baselines, TRIPOST-auto provides better accuracy; in practice TRIPOST-auto trained models rarely attempt self-improvement at inference (low SI.Freq) yet still outperform baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Empirical tables show TRIPOST-auto achieving higher total accuracy on several tasks and iterations than baselines and sometimes than TRIPOST with fixed p; authors argue automatical balancing results in fewer harmful attempted improvements while preserving gains from learning improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Because TRIPOST-auto ends up with fewer x_imp in training when the model is strong, the model rarely attempts self-improvement (low SI.Freq), so it may not fully learn to produce feedback/improvements even if that capability would be useful. The method still depends on LLM/script FBK and IMP quality and is subject to the same data-collection limitations as TRIPOST.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Language Models to Self-Improve through Interactive Demonstrations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5214.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5214.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ft_SI_demo (finetune on LLM self-improvement demos)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning on LLM-generated self-improvement demonstrations (ft_SI_demo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that finetunes the small model on self-improvement demonstration trajectories produced entirely by a large LLM (FBK+IMP from the LLM), following prior work (Ye et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfee: Iterative self-revising llm empowered by selffeedback generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B (finetuned on LLM demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-7B model finetuned on self-improvement trajectories generated by a larger LLM (e.g., Codex or ChatGPT) used as demonstrator for FBK and IMP.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>FT on LLM self-improvement demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt an LLM to produce an initial attempt, then have the LLM produce feedback and an improved attempt (possibly iteratively) and use these LLM-produced trajectories to finetune the smaller model with supervised learning (unweighted).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-Bench Hard tasks (same four tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multistep arithmetic, word sorting, date understanding, logical deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>The paper reports that ft_SI_demo slightly degraded the small model's performance across tasks compared to ft_rationale; finetuning on LLM demonstrations was less effective or harmful for the 7B model in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline ft_rationale (finetune on gold rationales) performed better than ft_SI_demo; LMSI and TRIPOST outperform ft_SI_demo in most reported settings.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Negative evidence: experiments show that directly distilling LLM-generated self-improvement demonstrations into a small model may be ineffective and can slightly reduce accuracy. Authors attribute this to a capability mismatch: LLMs make different types and fewer errors than small models, so the small model is forced to learn from mistakes unlike its own.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Capability mismatch between demonstrator LLM and learner small model leads to learning from errors that do not match the small model's error profile; can cause degradation. Also requires high-quality LLM demonstrations (cost).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Language Models to Self-Improve through Interactive Demonstrations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5214.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5214.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMSI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMSI (self-generated, self-consistent rationale finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method (from Huang et al., 2023) that generates multiple step-by-step solutions with high-temperature sampling and retains self-consistent solutions (by majority voting) to fine-tune a model, intended to distill self-improvement from self-generated rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models can self-improve</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LMSI applied to LLaMA-7B (after ft_rationale initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMSI produces many candidate rationales from the model itself (high-temperature sampling), filters for self-consistency, and uses the retained rationales to finetune the model (was originally applied to very large LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency / self-generated reflection (LMSI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple candidate step-by-step solutions from the model, keep those whose final answers agree (self-consistent), optionally apply formatting augmentation, and use them as additional training data to improve model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-Bench Hard tasks (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same four tasks; used to compare to TRIPOST and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>The paper reports LMSI is roughly on-par with ft_rationale only when the base model already has reasonably high performance on the seen subtask; for weak base models (e.g., 7B with poor initial accuracy) LMSI provides little benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to ft_rationale, LMSI does not consistently improve performance for small weak models; TRIPOST outperforms LMSI in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Empirical: LMSI sometimes matches ft_rationale when the base model already performs well, but fails to help when base performance is low. Authors include LMSI as a baseline and show TRIPOST yields larger gains for small models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LMSI is designed for LLMs that can generate many near-correct solutions; it struggles when applied to smaller models that produce many incorrect or inconsistent candidates, limiting usefulness as a training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Language Models to Self-Improve through Interactive Demonstrations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5214.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5214.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM self-improvement prompting (e.g., Codex, text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-improvement via prompting of large language models (examples: Codex-175B, text-davinci-003, GPT-family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior and in-paper observations that very large LLMs can be prompted to analyze and revise their own outputs (self-improvement / self-critique), often yielding improved final-answer accuracy; these LLMs were also used within TRIPOST as feedback (text-davinci-003) and improvement (Codex) modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>mention (prior work) and use (in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex-175B, text-davinci-003 (GPT-3), ChatGPT (used in ablations/comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained LLMs (Codex and GPT-3 variants) with hundreds of billions or many billions of parameters used for generating feedback and improved attempts; Codex used as IMP module and text-davinci-003 used as FBK in TRIPOST data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Prompted self-critique / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompting an LLM to produce a first attempt, then generate structured feedback identifying the earliest error and generate an improved attempt conditioned on that feedback (possibly iteratively). In TRIPOST, these LLMs are used as the FBK and IMP modules to produce corrective steps targeted to the small model's mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-Bench Hard tasks (used as FBK/IMP and also in prior self-improvement prompting evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same four tasks; prior studies and this paper demonstrate LLM prompting can yield improvements on math and logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Large LLMs (e.g., Codex) when prompted to self-improve can obtain notable accuracy gains on mathematical/logical tasks (paper references and prior work illustrate positive gains for large models). The paper contrasts this with small models failing to benefit from prompting-based self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Large LLM baselines without explicit self-improvement prompting have lower accuracy than after self-improvement prompting in prior work (quantities task-dependent; in this paper Codex is shown to be able to self-improve whereas LLaMA-7B cannot via prompting alone).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper reports that LLMs (Codex-175B) can be prompted to self-improve and achieve higher final-answer accuracy on the tested tasks (cites prior work and includes a direct contrast: prompting Codex to self-improve helps, while prompting LLaMA-7B to self-improve does not). The authors analyze error types showing LLMs make different error distributions and longer, fewer-error solutions than small models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even LLMs can struggle to generate useful feedback/improvements for some cases; their feedback generation quality constrains TRIPOST data collection. Using LLMs as FBK/IMP implies cost trade-offs (quality vs expense). Also, demonstrations from LLMs may be mismatched to a small-model learner's error modes, making direct distillation ineffective (capability mismatch).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Language Models to Self-Improve through Interactive Demonstrations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models can self-improve <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Selfee: Iterative self-revising llm empowered by selffeedback generation <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 1)</em></li>
                <li>Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5214",
    "paper_id": "paper-264406178",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "TRIPOST",
            "name_full": "TRIPOST (Trajectory Interactive Post-processing Training)",
            "brief_description": "An iterative training algorithm that teaches a smaller LM to self-improve by (1) having the small model generate attempts, (2) using expert LLMs or scripts to produce feedback and incremental improvements on those attempts, (3) post-processing and balancing the resulting single-step improvement triplets, and (4) re-training the small model with weighted supervised learning; iterations are repeated (typically t=3).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B (as M_θ)",
            "model_description": "LLaMA family small model; 7 billion parameters; decoder-only transformer used as the target small model to be trained with TRIPOST (experiments also include LLaMA-2 7B in appendix).",
            "reflection_method_name": "TRIPOST (interactive trajectory editing / generate-then-reflect replay)",
            "reflection_method_description": "The small model generates an initial attempt; a separate feedback module (FBK: text-davinci-003 or scripted checker) identifies the first error and emits feedback; an improvement module (IMP: Codex) generates an improved attempt conditioned on the previous attempt and the feedback, but only edits steps from the first error onward. Collected trajectories are split into single-step triplets (x_att_i, x_fb_i, x_att_{i+1}), filtered, re-balanced (controlled by hyperparameter p) and used to finetune the small model with weighted cross-entropy emphasizing feedback/improvement tokens. The process repeats for t iterations (typically t=3).",
            "num_iterations": 3,
            "task_name": "BIG-Bench Hard: Multistep Arithmetic, Word Sorting, Date Understanding, Logical Deduction",
            "task_description": "Four challenging math and reasoning tasks from BIG-Bench Hard: (i) multistep arithmetic (scriptable), (ii) word sorting (scriptable), (iii) date understanding (unscriptable), (iv) logical deduction (unscriptable).",
            "performance_with_reflection": "TRIPOST-trained LLaMA-7B improved task performance relative to baselines; paper reports improvements up to 7.13% absolute on math and reasoning tasks for LLaMA-7B (across the evaluated BIG-Bench Hard tasks). TRIPOST(t=3) models achieve the best metrics reported across the baselines (see main tables), and show measurable SI.Freq and SI.Contrib values (the model both attempts self-improvement and those attempts sometimes convert to correct final answers).",
            "performance_without_reflection": "Baselines include: ft_rationale (finetune on gold step-by-step rationales), LMSI (fine-tuning on self-generated self-consistent rationales), and ft_SI_demo (finetune on LLM-generated self-improvement demonstrations). TRIPOST outperforms these baselines; ft_SI_demo often slightly degrades performance versus ft_rationale. Exact baseline accuracies vary by task (see paper tables).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: TRIPOST-trained LLaMA-7B shows higher final-answer accuracy than ft_rationale, LMSI, and ft_SI_demo across the four BIG-Bench Hard tasks; the authors report up to 7.13% absolute improvement for LLaMA-7B. Ablations show that interaction (using the small-model's own attempts), filtering, dataset balancing (p), and weighted supervised learning are each important for the gains; TRIPOST-auto and TRIPOST with fixed p both outperform baselines. Qualitative: TRIPOST produces improving trajectories tailored to the small model's error modes, and SI.Contrib analysis demonstrates that some fraction of final-answer gains come from successful self-improvement steps.",
            "limitations_or_failure_cases": "Collecting improving trajectories depends on the ability of FBK/IMP (LLMs or scripts) to generate useful feedback/improvements; as the small model improves, fewer incorrect attempts are available and feedback/improvement generation becomes harder. Using too large a proportion p of improvement triplets causes the small model to attempt self-improvement even when unnecessary, harming accuracy. TRIPOST requires external LLMs/scripts (cost) and was only evaluated on 7B models and on math/reasoning tasks; generalization to other tasks or larger model sizes is not proven in this paper.",
            "uuid": "e5214.0",
            "source_info": {
                "paper_title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "TRIPOST-auto",
            "name_full": "TRIPOST-auto (automatic rebalancing variant)",
            "brief_description": "A variant of TRIPOST that omits manual re-balancing of improvement vs directly-correct examples and instead includes all edited improvement tuples and directly correct attempts, resulting in an automatic effective p proportional to current model error rate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B (as M_θ)",
            "model_description": "Same 7B LLaMA target model as TRIPOST experiments.",
            "reflection_method_name": "TRIPOST-auto (generate-then-reflect with automatic dataset proportioning)",
            "reflection_method_description": "Collect edited trajectories through interaction (FBK+IMP), include all x_imp and x_T without explicit manual balancing; train on the resulting dataset with weighted SL. Because the fraction of x_imp in training data naturally reflects the model's current error rate, the model rarely attempts to self-improve at inference.",
            "num_iterations": 3,
            "task_name": "BIG-Bench Hard: same four tasks",
            "task_description": "As above: multistep arithmetic, word sorting, date understanding, logical deduction.",
            "performance_with_reflection": "TRIPOST-auto sometimes yields even better overall performance than TRIPOST with fixed re-balancing, especially on unscriptable tasks (paper reports TRIPOST-auto producing higher total accuracy for some tasks and iterations).",
            "performance_without_reflection": "Compared to ft_rationale and ft_SI_demo baselines, TRIPOST-auto provides better accuracy; in practice TRIPOST-auto trained models rarely attempt self-improvement at inference (low SI.Freq) yet still outperform baselines.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Empirical tables show TRIPOST-auto achieving higher total accuracy on several tasks and iterations than baselines and sometimes than TRIPOST with fixed p; authors argue automatical balancing results in fewer harmful attempted improvements while preserving gains from learning improvements.",
            "limitations_or_failure_cases": "Because TRIPOST-auto ends up with fewer x_imp in training when the model is strong, the model rarely attempts self-improvement (low SI.Freq), so it may not fully learn to produce feedback/improvements even if that capability would be useful. The method still depends on LLM/script FBK and IMP quality and is subject to the same data-collection limitations as TRIPOST.",
            "uuid": "e5214.1",
            "source_info": {
                "paper_title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ft_SI_demo (finetune on LLM self-improvement demos)",
            "name_full": "Fine-tuning on LLM-generated self-improvement demonstrations (ft_SI_demo)",
            "brief_description": "A baseline that finetunes the small model on self-improvement demonstration trajectories produced entirely by a large LLM (FBK+IMP from the LLM), following prior work (Ye et al., 2023).",
            "citation_title": "Selfee: Iterative self-revising llm empowered by selffeedback generation",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B (finetuned on LLM demonstrations)",
            "model_description": "LLaMA-7B model finetuned on self-improvement trajectories generated by a larger LLM (e.g., Codex or ChatGPT) used as demonstrator for FBK and IMP.",
            "reflection_method_name": "FT on LLM self-improvement demonstrations",
            "reflection_method_description": "Prompt an LLM to produce an initial attempt, then have the LLM produce feedback and an improved attempt (possibly iteratively) and use these LLM-produced trajectories to finetune the smaller model with supervised learning (unweighted).",
            "num_iterations": null,
            "task_name": "BIG-Bench Hard tasks (same four tasks)",
            "task_description": "Multistep arithmetic, word sorting, date understanding, logical deduction.",
            "performance_with_reflection": "The paper reports that ft_SI_demo slightly degraded the small model's performance across tasks compared to ft_rationale; finetuning on LLM demonstrations was less effective or harmful for the 7B model in their experiments.",
            "performance_without_reflection": "Baseline ft_rationale (finetune on gold rationales) performed better than ft_SI_demo; LMSI and TRIPOST outperform ft_SI_demo in most reported settings.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Negative evidence: experiments show that directly distilling LLM-generated self-improvement demonstrations into a small model may be ineffective and can slightly reduce accuracy. Authors attribute this to a capability mismatch: LLMs make different types and fewer errors than small models, so the small model is forced to learn from mistakes unlike its own.",
            "limitations_or_failure_cases": "Capability mismatch between demonstrator LLM and learner small model leads to learning from errors that do not match the small model's error profile; can cause degradation. Also requires high-quality LLM demonstrations (cost).",
            "uuid": "e5214.2",
            "source_info": {
                "paper_title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LMSI",
            "name_full": "LMSI (self-generated, self-consistent rationale finetuning)",
            "brief_description": "A baseline method (from Huang et al., 2023) that generates multiple step-by-step solutions with high-temperature sampling and retains self-consistent solutions (by majority voting) to fine-tune a model, intended to distill self-improvement from self-generated rationales.",
            "citation_title": "Large language models can self-improve",
            "mention_or_use": "use",
            "model_name": "LMSI applied to LLaMA-7B (after ft_rationale initialization)",
            "model_description": "LMSI produces many candidate rationales from the model itself (high-temperature sampling), filters for self-consistency, and uses the retained rationales to finetune the model (was originally applied to very large LMs).",
            "reflection_method_name": "Self-consistency / self-generated reflection (LMSI)",
            "reflection_method_description": "Generate multiple candidate step-by-step solutions from the model, keep those whose final answers agree (self-consistent), optionally apply formatting augmentation, and use them as additional training data to improve model performance.",
            "num_iterations": null,
            "task_name": "BIG-Bench Hard tasks (used as baseline)",
            "task_description": "Same four tasks; used to compare to TRIPOST and other baselines.",
            "performance_with_reflection": "The paper reports LMSI is roughly on-par with ft_rationale only when the base model already has reasonably high performance on the seen subtask; for weak base models (e.g., 7B with poor initial accuracy) LMSI provides little benefit.",
            "performance_without_reflection": "Compared to ft_rationale, LMSI does not consistently improve performance for small weak models; TRIPOST outperforms LMSI in their experiments.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Empirical: LMSI sometimes matches ft_rationale when the base model already performs well, but fails to help when base performance is low. Authors include LMSI as a baseline and show TRIPOST yields larger gains for small models.",
            "limitations_or_failure_cases": "LMSI is designed for LLMs that can generate many near-correct solutions; it struggles when applied to smaller models that produce many incorrect or inconsistent candidates, limiting usefulness as a training signal.",
            "uuid": "e5214.3",
            "source_info": {
                "paper_title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM self-improvement prompting (e.g., Codex, text-davinci-003)",
            "name_full": "Self-improvement via prompting of large language models (examples: Codex-175B, text-davinci-003, GPT-family)",
            "brief_description": "Prior and in-paper observations that very large LLMs can be prompted to analyze and revise their own outputs (self-improvement / self-critique), often yielding improved final-answer accuracy; these LLMs were also used within TRIPOST as feedback (text-davinci-003) and improvement (Codex) modules.",
            "citation_title": "mention (prior work) and use (in this paper)",
            "mention_or_use": "use",
            "model_name": "Codex-175B, text-davinci-003 (GPT-3), ChatGPT (used in ablations/comparisons)",
            "model_description": "Large pretrained LLMs (Codex and GPT-3 variants) with hundreds of billions or many billions of parameters used for generating feedback and improved attempts; Codex used as IMP module and text-davinci-003 used as FBK in TRIPOST data collection.",
            "reflection_method_name": "Prompted self-critique / generate-then-reflect",
            "reflection_method_description": "Prompting an LLM to produce a first attempt, then generate structured feedback identifying the earliest error and generate an improved attempt conditioned on that feedback (possibly iteratively). In TRIPOST, these LLMs are used as the FBK and IMP modules to produce corrective steps targeted to the small model's mistakes.",
            "num_iterations": null,
            "task_name": "BIG-Bench Hard tasks (used as FBK/IMP and also in prior self-improvement prompting evaluations)",
            "task_description": "Same four tasks; prior studies and this paper demonstrate LLM prompting can yield improvements on math and logical reasoning tasks.",
            "performance_with_reflection": "Large LLMs (e.g., Codex) when prompted to self-improve can obtain notable accuracy gains on mathematical/logical tasks (paper references and prior work illustrate positive gains for large models). The paper contrasts this with small models failing to benefit from prompting-based self-improvement.",
            "performance_without_reflection": "Large LLM baselines without explicit self-improvement prompting have lower accuracy than after self-improvement prompting in prior work (quantities task-dependent; in this paper Codex is shown to be able to self-improve whereas LLaMA-7B cannot via prompting alone).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Paper reports that LLMs (Codex-175B) can be prompted to self-improve and achieve higher final-answer accuracy on the tested tasks (cites prior work and includes a direct contrast: prompting Codex to self-improve helps, while prompting LLaMA-7B to self-improve does not). The authors analyze error types showing LLMs make different error distributions and longer, fewer-error solutions than small models.",
            "limitations_or_failure_cases": "Even LLMs can struggle to generate useful feedback/improvements for some cases; their feedback generation quality constrains TRIPOST data collection. Using LLMs as FBK/IMP implies cost trade-offs (quality vs expense). Also, demonstrations from LLMs may be mismatched to a small-model learner's error modes, making direct distillation ineffective (capability mismatch).",
            "uuid": "e5214.4",
            "source_info": {
                "paper_title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models can self-improve",
            "rating": 2,
            "sanitized_title": "large_language_models_can_selfimprove"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Selfee: Iterative self-revising llm empowered by selffeedback generation",
            "rating": 2,
            "sanitized_title": "selfee_iterative_selfrevising_llm_empowered_by_selffeedback_generation"
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 1,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision",
            "rating": 1,
            "sanitized_title": "read_revise_repeat_a_system_demonstration_for_humanintheloop_iterative_text_revision"
        }
    ],
    "cost": 0.01697125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Teaching Language Models to Self-Improve through Interactive Demonstrations
31 Mar 2024</p>
<p>Xiao Yu 
Baolin Peng baolinpeng@global.tencent.com 
Now at Tencent AI</p>
<p>Michel Galley mgalley@microsoft.com 
Jianfeng Gao jfgao@microsoft.com 
Zhou Yu 
Columbia University ‡ Microsoft Research</p>
<p>Yuntao Bai 
Saurav Kadavath 
Sandipan Kundu 
Amanda Askell 
Jackson Kernion 
Andy Jones 
Anna Chen 
Anna Goldie 
Azalia Mirhoseini 
Cameron Mckinnon 
Carol Chen 
Catherine Olsson 
Christo- Pher Olah 
Danny Hernandez 
Dawn Drain 
Deep Ganguli 
Dustin Li 
Eli Tran-Johnson 
Ethan Perez 
Jamie Kerr 
Jared Mueller 
Jeffrey Ladish 
Joshua Landau 
Kamal Ndousse 
Kamile Lukosuite 
Liane Lovitt 
Michael Sellitto 
Nelson Elhage 
Nicholas Schiefer 
Noemi Mercado 
Nova Dassarma 
Robert Lasenby 
Robin Larson 
Sam Ringer 
Scott John- Ston 
Columbia University ‡ Microsoft Research</p>
<p>SheerShauna Kravec 
El Showk 
Stanislav Fort 
Tamera Lanham 
Timothy Telleen-Lawton 
Tom Henighan 
Tristan Hume 
Samuel R Bow- Man 
Zac Hatfield-Dodds 
Benjamin Mann 
Dario 2020 Amodei 
Nicholas Joseph 
Sam Mccandlish 
Tom Brown 
Jared 2022 Kaplan 
Constitutional 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Rewon Child 
Aditya Ramesh 
Daniel Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Ma- Teusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Alec Radford 
Ilya Sutskever 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Greg Brockman 
Alex Ray 
Raul Puri 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Wojciech 2021 Zaremba 
Evaluating 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
HyungPaul Barham 
Won Chung 
Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Yi Tay 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Nan Du 
Ben Hutchinson 
Reiner Pope 
James Bradbury 
Jacob Austin 
Michael Isard 
Guy Gur-Ari 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Sunipa Dev 
Henryk Michalewski 
Xavier Garcia 
Kevin Robinson 
Liam Fedus 
Denny 2023 Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Mark Omernick 
An- Drew M Dai 
Thanumalayan Sankaranarayana 
Marie Pellat 
Aitor Lewkowycz 
Erica Moreira 
Oleksandr Polozov 
Katherine Lee 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Orhan Firat 
Michele Catasta 
Jason Wei 
Kathy Meier-Hellstern 
Douglas Eck 
Jeff Dean 
Slav Petrov 
Karl Cobbe 
Vineet Kosaraju 
Mark Chen 
Jacob Hilton 
Reiichiro Nakano 
Tri Dao 
Daniel Y Fu 
Stefano Ermon 
Atri Rudra 
Rohan Taori 
Ishaan Gulrajani 
Tianyi Zhang 
Yann Dubois 
Xuechen Li 
Carlos Guestrin 
Percy Liang 
Tatsunori B Hashimoto 
Stan 
Hugo Touvron 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timothée Lacroix 
Baptiste Rozière 
Naman Goyal 
Eric Hambro 
Faisal Azhar 
Aurelien Rodriguez 
Armand Joulin 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cristian Canton Ferrer 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi 
Qizhou Wang 
Feng Liu 
Bo Han 
Tongliang Liu 
Chen Gong 
Gang Niu 
Mingyuan Zhou 
Masashi 2022 Sugiyama 
Probabilistic 
Yisen Wang 
Difan Zou 
Jinfeng Yi 
James Bailey 
Xingjun Ma 
Quanquan 2020 Gu 
Dale Schuurmans 
Brian Ichter 
Fei Xia 
Ed Chi 
Quoc V Le 
Johannes Welbl 
Amelia Glaese 
Jonathan Uesato 
Sumanth Dathathri 
John Mellor 
Lisa Anne 
Kirsty Anderson 
Pushmeet Kohli 
Sean Welleck 
Ximing Lu 
Peter West 
Faeze Brah- Man 
Tianxiao Shen 
Daniel Khashabi 
Chengrun Yang 
Yifeng Lu 
Hanxiao Liu 
Xinyun Chen 
Kevin Yang 
Yuandong Tian 
Nanyun Peng 
Dan 2022 Klein </p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Angela Fan, Sharan NarangYuchen Zhang, Melanie Kambadur</p>
<p>Sergey Edunov, and Thomas Scialom. 2023b. Llama
Aurelien Ro-driguez
Robert Stojnic</p>
<p>Teaching Language Models to Self-Improve through Interactive Demonstrations
31 Mar 20247DDF8FD93A7752EC192CF1C2EE0C5F2DarXiv:2310.13522v2[cs.CL]Lei Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep?
The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research.However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more costeffective and faster ones.To reduce this gap, we introduce TRIPOST, a training algorithm that endows smaller models with such selfimprovement ability, and show that our approach can improve LLaMA-7B's performance on math and reasoning tasks by up to 7.13%.In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on its own generations.We then replay this experience to train the small model.Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its own mistakes is crucial for small models to improve their performance.</p>
<p>Introduction</p>
<p>Large language models (OpenAI, 2023;Ouyang et al., 2022) together with techniques such as fewshot prompting (Brown et al., 2020) and Chain-of-Thought (CoT) prompting (Wei et al., 2023;Kojima et al., 2023) have been shown to be effective in achieving strong performance on various downstream language tasks.More recently, a new way to adapt LLMs to downstream tasks has captured the attention of many researchers, namely to further enhance the LLM's downstream task performance by asking the LLM to provide feedback on its own generations and then use the feedback to revise its outputs (Bai et al., 2022;Huang et al., 2023;Peng et al., 2023a;Shinn et al., 2023).This process is often called "self-improvement", and has proven to</p>
<p>Logical Deduction</p>
<p>Codex (175B) LLaMa (7B)</p>
<p>Figure 1: Compared to LLMs, smaller models have difficulty performing self-improvement on math or logical tasks, such as Multistep Arithmetics and Logical Deduction from the Big-Bench.+ft: finetuned on groundtruth rationales; +SI.prompt: prompted to perform self-improvement; +ft SI. demo further finetuned +ft on LLM self-improvement demonstrations.</p>
<p>be an effective technique to make the LLM's generations more diverse, more precise, or more faithful to a given piece of knowledge (Schick et al., 2022;Madaan et al., 2023;Peng et al., 2023a).However, Saunders et al. (2022); Huang et al. (2023) found that the ability to generate critical feedback or to self-improve is hardly evident in smaller models1 .Similarly, Ye et al. (2023) found that fine-tuning smaller models (e.g.7-13B) with self-improvement demonstrations from LLMs can still fail on tasks such as math, reasoning, and factuality.Following these previous works, we performed a similar study on two math and reasoning tasks in Figure 1.We compared the accuracy of the final answer generated by prompting Table 1: Training smaller models using self-improvement demonstrations from LLMs can be ineffective, as models of different sizes make different types and amount of mistakes (highlighted in red).Small models can make simple copying errors, while LLMs can make other arithmetic errors, such as not switching plus or minus signs when adding parentheses.See Appendix B for a more quantitative analysis.a 175B Codex (Chen et al., 2021) to self-improve, with prompting or training a LLaMA-7B model to self-improve using demonstrations from Codex (Ye et al., 2023).In Figure 1, we surprisingly find that smaller models performed worse using prior selfimprovement-related methods than simply training on ground-truth step-by-step rationales (+ft).By comparing the generated solutions from Codex-175B and LLaMA-7B, we find that smaller models, such as LLaMA-7B, not only make more mistakes, but also different types of mistakes compared to an LLM (Table 1 and Appendix B).Due to the smaller model's weaker math and reasoning ability, we believe training on LLM self-improvement demonstrations is less effective, as it forces the smaller model to learn from mistakes not of its own.Motivated by this finding, we propose TRIPOST, a training algorithm that can more effectively train a small model to learn from its mistakes, generate feedback, and improve its performance on math and reasoning tasks.TRIPOST is an iterative algorithm consisting of three stages: Interactive Trajectory Editing, Data Post-processing, and Model Training.Similar to the exploration stage in reinforcement learning, TRIPOST first creates improvement demonstrations using the small model to interact with the expert LLMs or relevant Python scripts.Then, TRIPOST postprocesses the collected data by filtering out failed improvement attempts, and then re-balances the dataset to disincentivize the model from trying to self-"improve" when it is not needed.Finally, TRIPOST replays the post-process dataset (Andrychowicz et al., 2018;Schaul et al., 2016), and trains the smaller model using weighted supervised learning.TRIPOST repeats entire the process several times.We evaluate our approach on four maths and reasoning datasets from the BIG-Bench Hard (Suzgun et al., 2022) collection, and find that TRIPOST-trained models can use its learned selfimprovement ability to improve their task performance.We also find that TRIPOST-trained models achieve better in-domain and out-of-domain performance than models trained using just the ground truth step-by-step rationales and trained using direct LLM demonstrations (Saunders et al., 2022;Ye et al., 2023).This paper makes the following contributions:</p>
<p>• We illustrate how prior work (Saunders et al., 2022;Ye et al., 2023) can be ineffective in training smaller models to self-improve their performance on math and reasoning tasks.</p>
<p>• We propose TRIPOST, an iterative training algorithm that trains a smaller language model to learn to self-improve.</p>
<p>• We show that TRIPOST-trained models achieve better performance than models trained using ground-truth rationales or using LLM demonstrations on four math and reasoning datasets from BIG-Bench Hard.</p>
<p>Approach</p>
<p>TRIPOST is an algorithm that trains a small language model to self-improve by learning from its own mistakes.Each iteration of TRIPOST consists of three stages.On a high level, we first collect a set of improving trajectories by using a smaller model M θ to interact with LLMs.We use M θ to generate initial attempts and then use a feedback module FBK and an improvement module IMP to edit parts of the M θ generated attempts.This creates a trajectory that includes attempts generated by the small model, with feedbacks and improvements tailored to the small model's capability (Figure 2).Next, we post-process the collected trajectories by 1) using scripts and other heuristics to filter out failed "improvement" attempts; and 2) re-balancing the dataset using both directly correct attempts and the improving trajectories.Finally, we use weighted supervised learning to train a smaller model M θ using the post-processed data.</p>
<p>We provide an overview of our algorithm in Figure 2, and detail each of the three stages in Section 2.2, Section 2.3, and Section 2.4, respectively.</p>
<p>Notation</p>
<p>We denote the entire attempt from a language model to solve a given question as a trajectory x:
x = (x att 0 , x fb 1 , x att 1 , x fb 2 , x att 2 , ..., x fb m ),
where x att 0 denotes the initial attempt, and x fb i , x att i denotes the i-th feedback and updated attempt, respectively.Such a trajectory ends when the last feedback x fb m contains the phrase "the final response is correct".Therefore, directly correct trajectories take the form of x ✓ = (x att 0 , x fb 1 ), and self-improving trajectories take the form of x SI = (x att 0 , x fb 1 , x att 1 , ..., x fb m ) where m &gt; 1.</p>
<p>Interactive Trajectory Editing</p>
<p>In our prior study in Figure 1 and Table 1, we find that it is difficult to elicit a 7B model to perform self-improvement due to its significantly weaker math and reasoning capability compared to LLMs.</p>
<p>To address this issue, we use the smaller model M θ to first generate an initial attempt2 , and then apply a feedback module FBK and an improvement module IMP to rewrite parts of the M θ trajectories.Specifically, we first use FBK (prompting text-davinci-003 or using a Python script) to generate a feedback x fb * i based on the first error step it identified for each incorrect attempt.After that, we edit the trajectory by replacing the first feedback that M θ and FBK disagree on with the FBKgenerated feedback, creating an edited trajectory:
(x att 0 , ..., x att i−1 , x fb * i ).
Finally, we use our improvement module IMP (prompting Codex) to generate an improved attempt x att * i conditioned on the previous x att i−1 and feedback x fb * i , and append it to the trajectory:
x edited = (x att 0 , ..., x att i−1 , x fb * i , x att * i ).
As an example, if feedback x fb * i identifies that the first mistake in x att i−1 appears in step 3, then step 1-2 in x att i−1 is kept untouched, and IMP is used to generate an improved solution by only changing steps ≥ 3.This design is to prevent IMP from re-writing the whole attempt from scratch (e.g., generating the gold solution), which would violate our motivation to create trajectories with feedback and improvements that are incremental and tailored to the small model's capability.</p>
<p>We repeat this process, up to a maximum number of iterations, until the last attempt in x edited is correct.Otherwise, we discard x edited that failed to reach the correct answer.</p>
<p>Data Post-processing</p>
<p>After the interactive trajectory editing step, we have three types of data: 1) gold step-by-step demonstrations x gold for the task, 2) directly correct trajectories x ✓ generated by M θ , and 3) edited trajectories x edited created using M θ , FBK, and IMP.</p>
<p>To make training easier, we first split all data into triplets of single-step improvement x imp = (x att i , x fb i , x att i+1 ) if an attempt x att i was incorrect, or into x T = (x att , x fb ) where the attempt is correct and the trajectory ends with x fb containing the phrase "the final response is correct".To learn from expert's correction, x att j and x fb j may be the edited x att * j and x fb * j , respectively (see Section 2.2).Next, we filter out some x imp triplets that contain incorrect feedbacks or improvement steps using some rules (see more in Appendix I).Then, we combine x T and filtered x imp into a single dataset, and balance them using a hyperparameter p specifying the proportion of x imp .We find that this parameter is important for the model to learn to improve its attempt only when necessary.This is because we found that training with too many x imp can cause the model to attempt self-improvement even when the last attempt is already correct, thus damaging its performance (see Section 4.2 for more details).</p>
<p>Model Training</p>
<p>Finally, we use supervised learning (SL) to train a smaller model M θ on the combined dataset.To promote the model to focus on learning the feedback and improvement steps in x imp , we use a weighted cross-entropy loss.We weight the loss for all the tokens in x T with w = 1.0, but with w &gt; 1.0 for the tokens that belong to x fb i or x att i+1 in single-step improvement triplets x imp .We note that we also experimented with masking x att i (Zheng et al., 2023), but found it to be less effective than weighted SL in our case.See Appendix E for more empirical analysis and discussions on related techniques.</p>
<p>TRIPOST</p>
<p>In Figure 2 and Algorithm 1 we summarize our TRIPOST algorithm.For each of the t iterations, we first utilize M θ to generate its own attempts X, and then use FBK and IMP to generate and create a set of edited trajectories as described in Section 2.2.Next, we process the newly collected trajectories and the gold task demonstrations X gold by first splitting them into a unified format of x imp triplet or x T , and then filtering out erroneous x imp data (Section 2.3).Finally, we create a training dataset D by balancing the number of x imp and x T using a hyperparameter p, and finetune M θ on D using weighted SL.Unless otherwise specified, we repeat this procedure for t = 3 iterations, and refer to the model trained using TRIPOST with t iterations as TRIPOST(t).</p>
<p>Algorithm 1 TRIPOST Training Algorithm</p>
<p>Require: Generative language model M θ Require: FBK and IMP modules Require: Gold task demonstrations X gold Require: Data buffer B 1: for t iterations do 2:</p>
<p>// interactive trajectory editing 3:</p>
<p>Gen. trajectories X = {X ✓ , X ✗ } with M θ 4:</p>
<p>Add correct trajectories X ✓ to B 5:</p>
<p>for each incorrect trajectory x ✗ ∈ X ✗ do 6:</p>
<p>Use FBK to generate feedbacks x fb * i 7:</p>
<p>Replace feedback from x ✗ with x fb * Train M θ on D using weighted SL 18: end for 3 Experiments</p>
<p>In this section, we test if our TRIPOST can 1) help distill self-improvement ability into a smaller model M θ , and 2) help M θ improve performance on math and reasoning tasks.</p>
<p>Dataset and Preprocessing</p>
<p>We utilize the BIG-Bench (Srivastava et al., 2023) benchmark to evaluate our approach.BIG-Bench is a collection of more than 200 text-based tasks including categories such as traditional NLP, mathematics, commonsense reasoning, and more.</p>
<p>We perform experiments on four math and reasoning tasks from the challenging BIG-Bench Hard (Suzgun et al., 2022) collection.We consider two scriptable tasks: Multistep Arithmetic and Word Sorting, where a step-by-step solution (rationale) and a feedback can be generated using a script; and two unscriptable tasks: Date Understanding and Logical Deduction, where we prompt an LLM (Codex/text-davinci-003) to generate feedbacks.We prompt Codex as the IMP module for all tasks.</p>
<p>For each task, we first collect a set of gold stepby-step rationales by either scripting a solution for scriptable tasks, or using the CoT prompts from Suzgun et al. (2022) to generate a solution using LLMs.For those LLM-generated rationales, we only keep the correct ones (see Appendix A for more details) for training.Then, to better measure a model's generalization ability, we split each of the 4 tasks further into seen and unseen subtasks.We mainly categorize simpler questions as the seen subtasks to be used for model training.We describe our categorization method in Table 2.</p>
<p>Models and Baselines</p>
<p>Models We use LLaMA-7B as M θ in our main experiments in Table 3. LLaMA (Touvron et al., 2023a) is a collection of foundation language models ranging from 7B to 65B that have shown strong performance compared to GPT-3 (175B) on many benchmarks (Zheng et al., 2023;Taori et al., 2023;Peng et al., 2023b).Due to the cost of training language models, we use the smallest 7B model.For results with LLaMA-2 models, see Appendix D. For training hyperparameters, see Appendix J.</p>
<p>Baselines We compare TRIPOST training with three baselines: fine-tuning using self-generated, self-consistent rationales (LMSI, Huang et al. ( 2023)); fine-tuning using only ground truth rationales (ft rationale); and fine-tuning using selfimprovement demonstrations from LLMs (ft SI. demo, similar to Ye et al. (2023)).For better performance, we initialize with the model trained after ft rationale for all methods.Lastly, for a fair comparison, we restrict iterative algorithms such as TRIPOST to only have access to the same amount of input prompts as used to train baselines such as ft rationale.For more implementation details, see Appendix G and Appendix I.</p>
<p>Metrics</p>
<p>To measure task performance, we follow prior studies on Big-Bench (Ho et al., 2023;Huang et al., 2023) and report the accuracy of the final answer extracted from the model's output.For each task, we report the accuracy on the seen subtasks and unseen subtasks, and its overall performance.To measure the model's self-improvement ability, we mainly consider two metrics: 1) how often the model tries to self-improve (SI.Freq.), and 2) how much those of self-improvement attempts contribute to the model's task performance (SI.Contrib.).We measure SI.Freq.as the number of times the model attempted to self-improve divided by the size of the test set, and SI.Contrib.as the number of times those improvement attempts actually reached the correct final answer.</p>
<p>Main Results</p>
<p>Table 3 summarizes TRIPOST's evaluation results on the four datasets.First, we find LMSI (Huang et al., 2023) to be roughly on-par with ft.rationale only when the performance of the base model (i.e., ft.rationale) is already high on the training questions (the seen subtask).This is understandable, as LMSI was originally designed for LLM (e.g., Table 3: Overall performance of TRIPOST on four BIG-Bench hard datasets.For each dataset, we train our models on the seen tasks, and evaluate their performance on both seen and unseen tasks.For all TRIPOST runs, we use the same hyperparameters (e.g., p = 0.43).PaLM-540B) to improve on tasks where it can already achieve a reasonable performance.Next, we find ft SI. demo to slightly degrade the model's performance across all tasks, which we believe is due to the capability mismatch between the LLM demonstrator and the small LM learner (Section 1).This forces the small LM to learn from "advanced" errors not from its own (Table 1 and Appendix B).Finally, we see that in all tasks, TRIPOST-trained models performs the best in all metrics.In general, we also observe improvement in the performance of TRIPOST-trained models as the number of iterations t increases. 3We believe this is because, during the process of learning to self-improve, the model also learns to better understand the tasks by learning from its own mistakes (Zhang et al., 2023;Andrychowicz et al., 2018;Lightman et al., 2023).This enables the model to not only generate better initial attempts, but also improve its self-improvement ability.</p>
<p>In Table 4, we further explore the contribution of M θ 's self-improvement ability by describing how its overall performance improved.We find that in two out of the four datasets, TRIPOST-trained models generate an more accurate initial attempt than the baselines (denoted as Directly Correct), and in 3 For a comparison against LMSI with more than t = 1 iteration, please see Appendix H. all cases, TRIPOST-trained models had measurable self-improvement contributions in both seen and unseen tasks (cf. Figure 1 and Table A4).This suggests that TRIPOST-training can 1) help the model better understand the tasks and generate better initial attempts, and 2) help distill self-improving ability into the model.We believe that the combination of both factors improve the model's overall performance in Table 3.</p>
<p>TRIPOST-auto</p>
<p>In Table 5, we explore another way of training M θ with TRIPOST.Instead of re-balancing the training dataset using a fixed p as in Section 3.4, we can simply include all the edited improvement tuples x imp and the directly correct attempts x T generated by M θ .We denote this method as TRIPOST-auto, as it automatically "balances" its training data to be proportional to its current performance, because p can be interpreted as how often the model's attempts were incorrect and needed editing.TRI-POST-auto training included no less x imp compared to TRIPOST (but generally more x T , resulting in p &lt; 0.43), and we find that the model now rarely attempts to self-improve.However, this unexpectedly leads to even better overall performance, especially on unscriptable tasks.We believe this indicates that 1) learning to always generate a useful feedback and the corresponding improvement is harder than learning to directly generate a correct attempt, and 2) using LLM-generated feedbacks, which covers more error cases than a Python script, is effective in improving a model's performance.</p>
<p>Analysis</p>
<p>To investigate the factors that can influence how TRIPOST-trained models learned to attempt selfimprovement, we focus our analysis on the Multistep Arithmetic and Logical Deduction datatset.</p>
<p>We also mainly study TRIPOST with p = 0.43, which has both a measurable self-improvement contribution and improvement in its task performance (see Table 3 and Table 4)4 .</p>
<p>Ablation Studies</p>
<p>We perform ablation studies for each of the three stages in TRIPOST to better understand their contribution to model's overall performance.In Table 6, we report the task accuracy when: interaction between M θ and LLM is removed, so that M θ is distilled with purely LLM demonstrations (-interaction); data filtering is removed (-filtering); dataset balancing is changed to using its own performance (+auto-balance); and the weights for SL are changed to be the same for all tokens (weighed SL).We find that all components are important for TRIPOST to work well, and the choice of fixing p presents a trade-off between a model's self-improvement ability and its task performance (notibly, both TRIPOST and TRIPOST-auto improve upon the baselines).</p>
<p>Proportion of SI. Training Data</p>
<p>In Table 7, we investigate how much improvement demonstration (x imp ) is needed to elicit a measurable self-improvement contribution from M θ .We find that when a large proportion (e.g.p = 0.70) of the training data contains x imp , the model often attempts to self-improve but does not always result in an overall better performance.This is because many of the "improvement" attempts result in failures (e.g.changing an already correct attempt to become an incorrect one), and the best performance is achieved typically when p is low.Despite this, we find that for all other cases with p ≤ 0.43, TRI-POST-trained model achieved a better performance than the baseline methods (see Table 4).</p>
<p>Number of TRIPOST Iterations</p>
<p>In most of our experiments, we trained TRIPOST up to t = 3 iterations.This is because we found that LLMs and our Python scripts start to struggle with generating feedback or improving M θ attempts after three iterations.In Figure 3, we present how the number of self-improving trajectories collected (x imp , after filtering) changes as TRIPOST iteration increases.We found that as M θ improves its performance over time, it 1) poses a greater challenge for our FBK module to generate feedback and/or the IMP module to generate improvement, and 2) generates fewer incorrect attempts for TRI-POST to edit.This is especially impactful for Multistep Arithmetic, as our feedback scripts can only consider a fixed number of error types.This also shows that even LLMs can struggle at generating useful feedbacks or correct improvements, which supports our findings in Section 3.5 that learning to generate feedback and improvements may be harder than to directly generate a correct solution.</p>
<p>Lastly, we note that TRIPOST can, in principle, be applied as an online RL algorithm, where one does not restrict the input prompts to be a fixed set as in Section 3. We believe this could be beneficial to improve the model's performance and genearlization ability beyond TRIPOST(t = 3).2023) has attempted to equip a single small model to selfimprove by training on LLM demonstrations, but found that it had little to no effect for small models on math/reasoning tasks.Our work presents analyses of how these previous methods can fail, and proposes TRIPOST that can train a small model to self-improve and achieve better task performance.</p>
<p>Related Work</p>
<p>Knowledge Distillation Learning from experts' demonstrations or reasoning (e.g., from GPT-4) has shown to be successful at improving the performance of smaller models in various tasks (Mukherjee et al., 2023;Laskin et al., 2022;Peng et al., 2023b;Ho et al., 2023;Ye et al., 2023;Huang et al., 2023;Jung et al., 2024).Distillation methods (Hinton et al., 2015;Ba and Caruana, 2014) generally train a target model using expert demonstrations unaware of the target model's capability.While TRI-POST also use LLMs to demonstrate generating a feedback or an improvement, these demonstrations are always conditioned on the output of the smaller model.In this view, our approach combines merits from reinforcement learning with knowledge distillation techniques, where small models are distilled with demonstrations that are created by its own exploration augmented by LLMs' supervision.</p>
<p>Conclusion</p>
<p>We introduce TRIPOST, a training algorithm that distills the ability to self-improve to a small model and help it achieve better task performance.TRI-POST first creates improving trajectories using interactions between a smaller model and an LLM, then post-process the collected trajectories, and finally train the smaller model to self-improve using weighted SL.We evaluated TRIPOST on four math and reasoning tasks from the Big-Bench Hard collection and found that it can help small models achieve better task performance.In our analysis, we find that 1) the interactive process of learning from and correcting its own mistakes is crucial for small models to learn to self-improve and 2) learning to always generate a useful feedback and a corresponding improvement can be much harder than learning to directly generate a correct answer.These findings suggest that other data formats, beyond the traditional (input, answer) pair, could be better suited for training a language model to solve a downstream task.We believe this also opens new possibilities for future work to leverage LLMs to improve the performance of smaller, faster models.</p>
<p>Limitations</p>
<p>Model Sizes In all of our experiments, we used a single A100 and mainly tested TRIPOST on 7B models, the smallest in the LLaMA-1 and LLaMA-2 family (Touvron et al., 2023a,b).However, with the recently introduced flash attention technique (Dao et al., 2022;Dao, 2023) which can be used to reduce memory usage during training, we plan to extend our experiments to use models with more than 7B parameters.</p>
<p>Datasets We focused our experiments on math and reasoning tasks because 1) prior work (Ye et al., 2023) had found it difficult to train a 7-13B to self-improve on those tasks and 2) measuring performance improvement is more well defined (for example, as compared to creative story writing).</p>
<p>However, we note that as TRIPOST is task agnostic, in theory it can be applied to other tasks such as knowledge-grounded dialogue generation (Yoshino et al., 2023) or dialogue safety (Dinan et al., 2019).We intend to leave this for future work.</p>
<p>LLM Usage While attempts for some tasks can be parsed and evaluated using a Python script (e.g., multistep arithmetic and word sorting), it quickly becomes unmanageable for tasks where reasonings mostly take the form of free text (e.g., date understanding and logical deduction).Therefore, we use LLMs such as GPT-3 and Codex (and ChatGPT, see Appendix F), which are highly performant at a reasonable cost.Specifically, we mainly use textdavinci-003 as the feedback module and Codex as the improvement module, as we found this to be the most cost-performant configuration in our experiments.However, since the ability of LLMs to generate feedback or improvements is crucial for TRIPOST to collect training data, this presents a trade-off between the cost of using more performant LLMs (e.g., GPT-4) and the training outcome of TRI-POST, for example on harder tasks such as GSM8k (Cobbe et al., 2021).We hope that with advances in making LLMs more available (Zhang et al., 2022a), such a trade-off would diminish.</p>
<p>Ethical Considerations</p>
<p>Our work describes an algorithm to improve small models' performance on math and reasoning tasks, by distilling them the ability to self-improve using interaction records with LLMs.Generally, while most algorithms are not designed for unethical usage, there is often potential for abuse in their applications.In our experiments, we apply TRIPOST to four math and reasoning tasks from the Big-Bench Hard collection (Suzgun et al., 2022).However, because training algorithms are typically taskagnostic, it is possible to use them for unethical tasks, such as scamming and generating harmful responses (Welbl et al., 2021;Gehman et al., 2020).We do not condone the use of TRIPOST for any unlawful or morally unjust purposes.</p>
<p>A More Details on Datasets and Preprocessing</p>
<p>We use four tasks from the Big-Bench Hard collection (Suzgun et al., 2022) for our experiments: multistep arithmetic, word sorting, date understanding, and logical deduction.Since these tasks do not provide ground truth step-by-step rationale, we either generate them using a script (for multistep arithmetic and word sorting), or prompt Codex (Chen et al., 2021) in a few-shot setting using examples from Suzgun et al. (2022).For rationales generated using prompting, we only keep the ones that reached the correct answer and passed a simple consistency check (e.g. for multiple choice questions, we ensure that the final selected choice in the last step appeared in the second last step).We provide example rationales used for each task in Table A8, Table A9, Table A10, and Table A11.Since Big-Bench (Srivastava et al., 2023) did not provide an official training/validation/test split, we generated our own splits with statistics shown in Table A1.</p>
<p>B Analyzing Errors Made by Codex and LLaMA-7B</p>
<p>To detail the different type and amount of errors made by an LLM (e.g., Codex) and a smaller model (e.g., LLaMA-7B), we manually examine incorrect attempts generated by the two models in the Multistep Arithmetics dataset.We use Codex with few-shot prompting, and LLaMA-7B after supervised finetuning on ground-truth step-by-step solutions (denoted as LLaMA+ft).We randomly sample 50 generated attempts with incorrect answers, and carefully review each step in those attempts.</p>
<p>For each incorrect step, we apply the principle of error-carried-forward and categorize the first error encountered according to Table A2.We present our analysis in Figure A1 and Table A3. Figure A1 shows that calculation errors take up more than 50% of the time for both Codex and the finetuned LLaMA-7B.However, Codex also makes many algebriac errors (such as forgetting to change sign after adding brackets), while LLaMA-7B often hallucinates by adding or deleting terms from previous calculations.Furthermore, Table A3 shows that, compared to the fine-tuned LLaMA-7B, Codex generates longer solutions while producing fewer errors per step.These findings suggest that supervised finetuning a smaller LM (e.g., LLaMA-7B) based on correcting LLM-generated errors may be inefficient, as it forces the smaller model to learn from attempts and mistakes very different from its own (see Section 1 and Appendix C for more details).</p>
<p>C More Details on the Prior Study</p>
<p>In the prior study mentioned in Section 1, we experimented with distilling a smaller model (e.g.LLaMA-7B) with self-improvement demonstration using just the LLMs.We found that not only can the smaller model not self-improve by few-shot prompting, they also still fail to do so after training on the LLM self-improvement demonstrations (also discussed in Section 1).In Figure 1 we presented the performance gap between prompting Codex (175B) and finetuning/prompting LLaMA (7B) with self-improvement demonstrations, and in Table A4 we show the detailed numerical results.</p>
<p>D Additional Results on LLaMA-2</p>
<p>In Table A5 we present the results of using the LLaMA-2 7B model (Touvron et al., 2023b) for TRIPOST training.We used the same procedure as testing with the LLaMA-1 model in our main experiments (Section 3), except that we used p = 0.26 across all settings with LLaMA-2 instead of p = 0.43.This is because we found that the LLaMA-2 baseline (ft rationale) achieves almost twice the performance compared to its LLaMA-1 counterpart.As the LLaMA-2 models make fewer mistakes, we decrease p accordingly to prevent TRIPOST from terminating early due to lack of data.In general, Table A5 shows a similar trend as discussed in Section 3 that 1) fine-tuning on LLM demonstrations of self-improvement did not help improve math/reasoning task performance, and 2) TRIPOST can further improve upon the baselines.loss to emphasize learning the improvement-related tokens (x fb i or x att i+1 ) of each training sample.In Table A6, we find that using a weight too low (w = 1.0) can result in the model rarely attempting to self-improve, while using a weight too high (w = 3.0) does not result in better performance.We believe that this has a similar effect of adjusting p in Section 4.2: some incentive is needed for the model to learn to self-improve, while too much emphasis on trying to self-improve can result in a worse performance.</p>
<p>E Effect of Weighted SL</p>
<p>While we also experimented with alternatives such as masking easier tokens (x att i in a single-step improvement triplet), we believe there is a rich set of techniques that can be used to train the model to focus on harder inputs.This includes boosting algorithms (Schapire, 1999;He et al., 2019), automatic loss reweighing methods (Kanai et al., 2023;Wang et al., 2022Wang et al., , 2020)), as well as importance-sampling based methods (Katharopoulos and Fleuret, 2019).We leave this for future work as it is orthogonal to our main contributions.</p>
<p>F Prompting Details</p>
<p>Besides prompting to generate rationales (e.g. for date understanding), we also use prompting to gen-  A12, and the prompts used to generate feedback or improvements in Table A13, Table A14, Table A15, and Table A16.Note that we used a form-type of prompting for generating feedback because it can more easily ensure that our (formatted) feedback will contain all the elements we need.</p>
<p>When an answer is correct, we manually attach the phrase "Step 1 to step x is correct, and the final response is also correct." as the termination feedback, where "x" is the last step number.This termination condition is also used during inference.</p>
<p>G More Details on Baselines LMSI Huang et al. (2023) proposed LMSI, a method to improve PaLM-540B (Chowdhery et al., 2022) on math and reasoning tasks by training it on self-generated and consistent step-by-step rationales.First, LMSI generates multiple step-bystep solutions using a high temperature (τ = 1.2).</p>
<p>Then, LMSI only keeps the answers that are selfconsistent (by majority voting) in the final answer.Finally, LMSI further augments these solutions with mixed formats, such as removing all the intermediate steps and only keep the final answer.To be comparable with other methods in Table 3 that have access to the ground truth answer, we modify the second step to only keep the answers that are correct.In addition, since small models such as LLaMA-7B performed poorly in these tasks without fine-tuning, we perform LMSI after training the model on the collected silver step-by-step solutions in Appendix A.</p>
<p>ft. SI demo Following Ye et al. (2023), ft.SI demo finetunes a model on LLM-generated selfimprovement demonstrations.For all tasks, we experimented with LLMs ∈ {ChatGPT, Codex} and reported one with better performance (often Codex).In details, we first prompt a LLM (e.g.Codex) to generate an initial attempt, and then reused TRIPOST with the same LLM as the FBK and IMP to generate a feedback and an improvement.For a fair comparison in Table 3, we also balanced the collected data using the same p = 0.43 as with TRIPOST.Finally, train the small LM using (unweighted) SL on the collected data.</p>
<p>H Running LMSI(t &gt; 1)</p>
<p>LMSI described in (Huang et al., 2023) was not applied as an iterative algorithm.However, since LMSI training only relies on self-generated and self-consistent answers, it can be ran iteratively similar to TRIPOST.We present this comparison in Table A7</p>
<p>I Implementation Details</p>
<p>We combine techniques from prompting-based selfimprovement (Madaan et al., 2023;Bai et al., 2022) and active learning (Zhang et al., 2022b;Lightman et al., 2023) to collect a set of self-improving trajectories.Specifically, we first either use a script or few-shot prompting (see Appendix F for more details) to gather feedbacks on a given attempt, and then use prompting to generate improvements conditioned on the previous attempt, the feedback, and all the steps in the previous attempt before the first error step (see Tables A13 to A16 for example).This is to ensure that the improved attempt is making modifications on the previous attempt, rather than creating an entirely new attempt.</p>
<p>To edit the original attempt given the script/LLM-generated feedback, we 1) find the first x fb * i feedback that differs from the M θgenerated feedback x fb i (usually i = 1); 2) replace x fb * i with x fb i ; 3) remove all the attempts, feedback, and improvement after after x fb i from the trajectory.</p>
<p>After this, we prompt an LLM in the improvement module IMP to generate an improvement as described above and in Appendix F.</p>
<p>To filter out some of the unhelpful feedbacks or incorrectly "improved" attempts, we mainly check 1) whether the final attempt reached the correct answer; 2) if there is at least one difference between the previous attempt and the improved attempt; and 3) if the final answer is consistent with the second last step.We only keep the data that have passed all checks.The effect of this filtering is discussed in our ablation studies in Section 4.1.</p>
<p>J Model/Training hyperparameters</p>
<p>In our main experiments, we used a single A100 GPU with DeepSpeed (Rasley et al., 2020) Zero-2 optimization.We used AdamW (Loshchilov and Hutter, 2019) as the optimizer.For each iteration of the TRIPOST algorithm, we train the model for 4 epochs (line 17).We use a linear learning rate schedule with 20% of warmup steps and a peak learning rate of 1e-6.We use a maximum sequence length of 1024 tokens, batch size = 1 and gradient accumulation step = 4. On average, three iterations of TRIPOST take about 12 hours to train.</p>
<p>FeedbackFigure 2 :
2
Figure 2: Overview of TRIPOST algorithm.TRIPOST consists of three stages: interactive trajectory editing where we use our FBK and IMP module to edit trajectories generated by a smaller model M θ ; data post-processing where we filter out erroneous trajectories and create a re-balanced dataset; and model training where we train M θ using weighted supervised learning on the post-processed dataset.</p>
<p>X gold ∪ B into triplets x imp or x T</p>
<p>l = {3, 4} × d = {2} l = {3, 4} × d = {3} and number of operands (l) // l = 3, d = 2 l = {5, 6} × d = {2, 3} Word Sorting number of words to sort (l) Q: orange apple banana pear l = {2, 3, ..., 7} l = {8, 9, ..., 16} // l = 4 Date Understanding number of steps to solve (l) Q: Today is 01/02, what's the l = {1, 2} l ≥ 3 date yesterday?// l = 1 Logical Deduction number of options (l) Q: John runs ... Who runs fastest?l = {3, 5} l = {7} Options: (A).. (B).. (C).. // l = 3 Table 2: Categorization of the datasets into seen and unseen tasks.seen tasks are chosen to be easier and are used for training.Example questions are abbreviated, for complete examples please refer to Appendix A.</p>
<p>Figure 3 :
3
Figure 3: Improvement demonstrations become more difficult to collect as TRIPOST iteration increases.</p>
<p>Besides balancing the training dataset, we also found it important to use a weighted cross-entropy Error NameDefinition ExampleCalculation Error errors in performing basic arithmetic operations (addition, subtraction, multiplication, division) manipulation, such as forgetting to change signs when adding brackets or forgetting the correct order of operations1 − 2 + 3 = 1 − (2 + 3)Copy Error mis-copying an operand or an operator from previous steps 7 + 1 + (...) = 7 − 1 + (...) Hallucation adding or deleting an operand or an operator from previous steps 7 + (...) = 7 − 1 + (...) Other Error errors that do not fall into the above categories</p>
<p>Table 4 :
4
Total accuracy (total) is accuracy weighted based on the number of test samples.† denotes that the task uses scripted rationale/feedback.Results are averaged over three runs.Analyzing how TRIPOST-trained models improved the overall task performance.Total accuracy is first decomposed into attempts that are directly correct (Directly Correct) and attempts with self-improvement (SI.
DatasetSI. Contrib. seen unseen totalDirectly Correct Total Acc.Multistep Arithmetic 1.390.281.6720.8322.50Word Sorting1.850.522.3727.4429.82Date Understanding 1.951.293.2533.7637.01Logical Deduction8.230.638.8639.5648.52
Contrib.).SI.Contrib. is then further decomposed into its accuracy contribution on the seen and unseen subtasks.</p>
<p>Table 5 :
5
Freq SI.Cont.totalSI.Freq SI.Cont.totalSI.Freq SI.Cont.totalSI.Freq SI.Cont.totalOverallperformance of TRIPOST without explicit re-balancing.TRIPOST-auto uses the same training procedure as TRIPOST, except that the proportion of x imp used for training is determined automatically using the model's current task performance.
MethodMultistep Arithmetic  †Word Sorting  †Date UnderstandingLogical DeductionSI. TRIPOST(t = 1) 0.000.0017.171.580.5228.230.000.0027.278.862.8546.52TRIPOST(t = 2)1.331.1120.672.900.5229.551.940.6532.4729.7211.3945.25TRIPOST(t = 3)3.671.6722.504.382.3729.8210.383.2537.0123.428.8648.42TRIPOST-auto(t = 1)0.000.0020.000.000.0030.340.000.0032.471.900.6351.27TRIPOST-auto(t = 2)0.000.0023.330.000.0029.550.000.0056.820.630.0055.06TRIPOST-auto(t = 3)0.000.0024.330.000.0030.340.000.0068.830.630.6356.96MethodMultistep ArithmeticLogical DeductionSI. Contrib. Total Acc. SI. Contrib. Total Acc.TRIPOST1.6722.508.8648.42-interaction0.2811.670.0041.67-filtering0.3320.677.5948.27+auto-balance0.0024.330.6356.96-weighed SL0.0021.331.9043.67</p>
<p>Table 6 :
6
TRIPOST ablation studies.
DatasetpSelf-Improvement Total Acc. Freq. Contrib.0.05 0.000.0023.170.20 0.000.0024.33Multistep Arithmetic0.43 3.671.6722.500.56 8.612.5020.000.70 18.883.6118.670.05 0.000.0049.370.20 0.630.0052.63Logical Deduction0.43 23.428.8648.420.56 20.257.5945.570.70 59.4931.6445.57</p>
<p>Table 7 :
7
Varying the proportion of x SI used during TRIPOST training.</p>
<p>Table A1 :
A1
(Suzgun et al., 2022)alidation, and test samples used for the four tasks from the Big-Bench Hard collection(Suzgun et al., 2022).
DatasetTrain Validation TestMultistep Arithmetics 55050300Word Sorting43340379Date Understanding1912087Logical Deduction36040158</p>
<p>Table A2 :
A2
Categorization of errors commonly made by Codex or LLaMA-7B in the Multistep Arithmetics dataset.LMs of different sizes make different types of errors.In the Multistep Arithmetics dataset, more than half of the errors made by Codex or a finetuned LLaMA-7B belong to Calculation Error.However, the second most common error is Arithmetic Error for Codex, and Copy Error for LLaMA-7B.
51.3%31.6% 9.4% 6.8%55.0%7.6% 9.2% 26.0%Calculation Error Algebraic Error Copy Error Hallucination Other Error(a) Codex(b) LLaMA+ft (7B)Figure A1: Codex LLaMA+ft (7B)Avg. Char per Question113.8102.4Avg. Char per Attempt920.0650.1Percent Steps with Errors31.735.1Table A3: LMs of different sizes make different amountof errors. In the Multistep Arithmetics dataset, Codexmakes less errors per step compared to a finetunedLLaMA-7B, while answering longer questions and gen-erating longer solutions.Dataset MethodSI. Contrib. Total Acc.Codex (175B)-31.33+ SI. prompting2.0033.33 ↑MS.A.LLaMA+ft (7B)-16.78+ SI. prompting0.0011.60 ↓+ ft SI. demo0.2811.67 ↓Codex (175B)-81.01+ SI. prompting4.4385.44 ↑L.D.LLaMA+ft (7B)-45.78+ SI. prompting0.0043.67 ↓+ ft SI. demo0.0041.67 ↓Table A4: Compared to LLMs, smaller models havedifficulty performing self-improvement (SI.) on math-ematical/logical tasks, such as Multistep Arithmetics(MS.A.) and Logical Deduction (L.D.).</p>
<p>Table A5 :
A5
Using TRIPOST with LLaMA-2 7B model.Overall, LLaMA-2 performs better than its LLaMA-1 counterpart, and TRIPOST further improves LLaMA-2's task performance.
MethodMultistep Arithmetics  †Logical Deductionseen unseen totalseen unseen totalLLaMA-1 (7B)ft rationale ft SI. demo TRIPOST(t = 1) 41.67 38.75 29.17 TRIPOST(t = 2) 49.58 TRIPOST(t = 3) 52.501.48 0.00 0.84 1.39 2.5016.78 62.69 11.67 54.63 15.00 41.67 8.67 45.78 17.17 57.88 22.00 46.52 20.67 58.80 18.00 45.25 22.50 63.89 15.00 48.42LLaMA-2 (7B)ft rationale ft SI. demo TRIPOST(t = 1) 71.67 72.50 51.67 TRIPOST(t = 2) 75.00 TRIPOST(t = 3) 72.225.00 2.22 3.89 6.11 5.1932.00 87.04 34.00 70.25 22.00 80.56 42.00 68.35 31.00 83.33 52.00 73.42 33.67 83.33 48.00 72.15 32.00 71.67 50.00 72.78DatasetwSelf-Improvement Total Acc. Freq. Contrib.1.0 0.000.0021.33Arithmetic1.5 3.671.6722.503.0 3.331.3822.001.0 10.131.9043.67Logical Deduction1.5 23.428.8648.423.0 19.629.4946.84Table A6: Varying the SL weights w used during TRI-POST training.erate feedbacks and improvements given the ini-tial attempt. For scriptable tasks such as multisteparithmetic and word sorting, we use a script to gen-erate the feedback by first parsing each step in theattempt, and check their correctness/consistencywith other steps using a set of predefined rules.This is similar to Welleck et al. (2022), but we alsogeneralize this to unscriptable tasks such as dateunderstanding and logical deduction by few-shotprompting GPT-3 (text-davinci-003) (Brown et al.,2020) and Codex (Chen et al., 2021) to generatefeedbacks and improvements. We found that beingable to generate useful feedback is critical for gath-ering successful improvement trajectories, and wediscovered that ChatGPT (OpenAI, 2022) is lesseffective than GPT-3 or Codex in our case. Weprovide examples of the feedbacks generated foreach task in Table</p>
<p>Table A7 :
A7
, and find that LMSI(t ≥ 1) struggles when the base model (ft rationale) has a weak task performance.We believe this is because LMSI is mainly a self-training algorithm designed for LLMs such as PaLM-540B (Chowdhery et al., 2022), which can often generate correct or near-correct solutions.However, TRIPOST is a training algorithm designed for smaller LMs, where models learns to self-improve from its interaction records with expert LLMs.Comparing TRIPOST(t &gt; 1) with LMSI(t &gt; 1).For simplicity, we show total accuracy for each task.
MethodMultistep Arithmetic Date Understandingft rationale16.7829.87LMSI(t = 1)4.3312.99LMSI(t = 2)2.5011.69TRIPOST(t = 1)17.1727.27TRIPOST(t = 2)20.6737.01
The distinction between small and large language models is often context-dependent(Saunders et al.,<br />
). In this work, we denote "small models" to those with a few billion parameters (e.g., LLaMA-7B), and LLMs as those scaled to hundreds of billions of parameters (e.g., ChatGPT).
We also allow M θ to attempt generating feedbacks and improvements, as self-improvement training progresses.
In practice, we implement p by specifying the ratio of the number of "self-improvement samples vs. directly correct samples vs. gold samples". For example, a ratio of 1.5 : 1.0 : 1.0 corresponds to p = 0.43.
ford alpaca: An instruction-following llama model.https://github.com/tatsu-lab/stanford_alpaca.Language models are few-shot learners.In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.Curran Associates, Inc.Example Question and Generated Rationale for Multistep Arithmetic Q: ((7 --1 + -1 + -4) -(-7 + 7 * 3 * -9)) = Answer: Let's think step by step.Recall that the order of operations in mathematics is as follows:(1) Parentheses, (2) exponents, (3) multiplication and division (from left to right), (4) addition and multiplication (from left to right).So, remember to always compute the expressions inside parentheses or brackets first.(1) This equation can be written as "(A -B)", where A = (7 --1 + -1 + -4) and B = (-7 + 7 * 3 * -9).(2) Let's calculate A = (7 --1 + -1 + -4) = (((7 --1) + -1) + -4) = ((8 + -1) + -4) = (7 + -4) = 3.(3) Let's calculate B = (-7 + 7 * 3 * -9) = (-7 + ((7 * 3) * -9)) = (-7 + (21 * -9)) = (-7 + -189) = -196.(4) Then, the final equation is (A -B) = (3 --196) = 199.(Final response) So the answer is 199.TableA8: Examples of generated rationales for Multistep Arithmetic using Python scripts.Question and prompts are written in gray italics, and the generated step-by-step rationale is shown in black.Example Question and Generated Rationale for Word Sorting Q: Sort the following words alphabetically: List: sioux fortescue purloin percept helmsman Attempt: Let's think step by step.(1) The first letter: "sioux"="s", "fortescue"="f" (6), "purloin"="p" (16), "percept"="p" (16), "helmsman"="h" (8) (2) We now have: (6) "fortescue" &lt; (8) "helmsman" &lt; (16) ["purloin" ?"percept"] &lt; (19) "sioux".(3) Now sort this subpart ["purloin" ?"percept"] by looking at their second letters: "purloin"="u" (21), "percept"="e" (5).(3.1)We now have: (5) "percept" &lt; (21) "purloin".(3.2) Hence, we have "percept" &lt; "purloin".(4) Hence, we have "fortescue" &lt; "helmsman" &lt; "percept" &lt; "purloin" &lt; "sioux".(Final response) So the answer is: fortescue helmsman percept purloin sioux.Example Question and Generated Rationale for Logical Deduction Q: The following paragraphs each describe a set of three objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a shelf, there are three books: a white book, a green book, and an orange book.The green book is to the right of the white book.The orange book is the rightmost.Options: (A) The white book is the leftmost.(B) The green book is the leftmost.(C) The orange book is the leftmost.Attempt: Let's think step by step.Let "??" represent 0 or more objects, and "?" represent exactly 1 object.(1) The green book is to the right of the white book: "(left) ?? white ?? green ?? (right)".(2) The orange book is the rightmost: "(left) ?? orange (right)".(3) There are in total three books: a white book, a green book, and an orange book.(4) Combining (1) and (2) we get the following ordering: "(left) ?? white ?? green ?? orange (right)".(5) Combining (3) and (4) we get the following ordering: "(left) white green orange (right)".(6) According to this ordering, the leftmost book is the white book.(Final response) So the answer is (A).Answer: Let's think step by step.Recall that the order of operations in mathematics ...(1) This equation can be written as "(A * B)", where A = (-8 + -2 * 1 * 3) and B = ...(2) Let's calculate A = (-8 + -2 * 1 * 3) = ... = -14.(3) Let's calculate B = (-2 * * 5 -2) = ... = (2 * 5 -2) = 10.Then, the final equation is (A * B) = (-14 * 10) = -140.(Final response) So the answer is -140.Feedback:Step (1) to (2) are correct.In step (3) the part " (2 * 5 -2) = 10 " is incorrect.This is because there is a calculation error, since (2 * 5 -2) is not equal to 10. Word Sorting † Q: Sort the following words alphabetically: List: cole estonia brian redneck coda Answer: Let's think step by step.(1) The first letter: "cole"="c" (3), "estonia"="e" (5), ..., "coda"="c" (3).(2) We now have: (2) "brian" &lt; (3) ["cole" ?"coda"] &lt; (5) "estonia" &lt; (18) "redneck".(3) Now sort this subpart ["cole" ?"coda"] by looking at their second letters: "cole"="o" (15), "coda"="o" (15).(3.1)We now have: (15) ["cole" ?"coda"].(3.2) Now sort this subpart ["cole" ?"coda"] by looking at their third letters: "cole"="l" (12), "coda"="d" (4).(3.2.1)We now have: (12) "cole" &lt; (4) "coda".... (Final response) So the answer is: cole coda estonia brian redneck.Feedback:Step (1) to (3.2) are correct.In step (3.2.1) the part " (12) "cole" &lt; (4) "coda" " is incorrect.This is because words are not sorted in ascending order.Answer: Let's think step by step.Let "??" represent 0 or more objects, and "?" represent exactly 1 object.(1) The motorcycle is the oldest: "(oldest) motorcycle ?? (newest)".(2) The bus is newer than the tractor: "(newest) bus ?? tractor ?? (oldest)".(3) There are in total three vehicles: a motorcycle, a bus, and a tractor.(4) Combining (1) and (2) we get the following ordering: "(newest) bus ?? tractor ?? (oldest)"(5) Combining (3) and (4) we get the following ordering: "(oldest) motorcycle bus (newest) tractor".(6) According to this ordering, the vehicle that is the newest is the tractor.(Final response) So the answer is (C).Feedback:Step (1) to (2) are correct.In step (3) the part "(newest) bus ?? tractor ?? (oldest)" is incorrect.This is because it is missing the motorcycle from step (1).TableA12: Examples of an incorrect attempt generated by a small model, followed by its feedback generated by either an LLM (Codex or text-davinci-003) or a script (indicated by † ) for each task.Input question is written in black, a generated attempt is written in blue, and its feedback is written in cyan.Prompt Type Prompt ExampleGet Improvement Q: (((-9 * 3 + -6) * (-2 + 7 * -6)) -((-4 --6 + 6) * (4 * -4 + 8))) = Answer: Let's think step by step.Recall that the order of operations in mathematics is as follows:(1) Parentheses, (2) exponents, (3) multiplication and division (from left to right), (4) addition and multiplication (from left to right).So, remember to always compute the expressions inside parentheses or brackets first.( TableA13: Prompt used for generating improvement for Multistep Arithmetic.The generated feedback is then formatted as "Step 1 to step {earliest error step -1} is correct.In step {earliest error step} the part '{error segment}' is incorrect.This is because '{error reason}'."In general, we used three-shot prompting.Parts that will be generated are highlighted in blue.Due to limited space, we present one example used for each task.Please refer to our code repository for the full prompt.Prompt Type Prompt ExampleGet Improvement Q: Sort the following words alphabetically: List: cosh inferno every foyer coterie mcfadden halifax Answer: Let's think step by step.(1) The first letter: "cosh"="c" (3), "inferno"="i" (9), "every"="e" (5), "foyer"="f" (6), "coterie"="c" (3), "mcfadden"="m" (13), "halifax"="h" (8).(2) We now have: (3) "cosh" ?"coterie" &lt; (5) "every" &lt; (6) "foyer" &lt; (8) "halifax" &lt; (9) "inferno" &lt; (13) "mcfadden".(3) Now sort this subpart ["cosh" ?"coterie"] by looking at their second letters: "cosh"="o" (15), "coterie"="o" (15).(3.1)We now have: (15) ["cosh" ?"coterie"].(3.2) Now sort this subpart ["cosh" ?"coterie"] by looking at their third letters: "cosh"="s" (19), "coterie"="c" (3).(3.3)We now have: (3) "coterie" &lt; (19) "cosh".(3.4) Hence, we have "coterie" &lt; "cosh".(4) Hence, we have "coterie" &lt; "cosh" &lt; "every" &lt; "foyer" &lt; "halifax" &lt; "inferno" &lt; "mcfadden".(Final response) So the answer is: coterie cosh every foyer halifax inferno mcfadden.[END] Feedback:Step (1) to (3.1) are correct.In step (3.2) the part " "coterie"="c" (3) " is incorrect.This is because the third letter of "coterie" should be "t".Updated Answer: Let's think step by step.(1) The first letter: "cosh"="c" (3), "inferno"="i" (9), "every"="e" (5), "foyer"="f" (6), "coterie"="c" (3), "mcfadden"="m" (13), "halifax"="h" (8).(2) We now have: (3) ["cosh" ?"coterie"] &lt; (5) "every" &lt; (6) "foyer" &lt; (8) "halifax" &lt; (9) "inferno" &lt; (13) "mcfadden".(3) Now sort this subpart ["cosh" ?"coterie"] by looking at their second letters: "cosh"="o" (15), "coterie"="o" (15).(3.1)We now have: (15) ["cosh" ?"coterie"].(3.2) Now sort this subpart ["cosh" ?"coterie"] by looking at their third letters: "cosh"="s" (19), "coterie"="t" (20).(3.3)We now have: (19) "cosh" &lt; (20) "coterie".(3.4) Hence, we have "cosh" &lt; "coterie".(4) Hence, we have "cosh" &lt; "coterie" &lt; "every" &lt; "foyer" &lt; "halifax" &lt; "inferno" &lt; "mcfadden".(Final response) So the answer is: cosh coterie every foyer halifax inferno mcfadden.[END]TableA14: Prompt used for generating improvement for Word Sorting.The generated feedback is then formatted as "Step 1 to step {earliest error step -1} is correct.In step {earliest error step} the part '{error segment}' is incorrect.This is because '{error reason}'."In general, we used three-shot prompting.Parts that will be generated are highlighted in blue.Due to limited space, we present one example used for each task.Please refer to our code repository for the full prompt.TableA15: Prompt used for generating feedback and improvement for Date Understanding.The generated feedback is then formatted as "Step 1 to step {first error step -1} is correct.In step {first error step} the part '{error part}' is incorrect.This is because '{error reason}'."In general, we used three-shot prompting.Parts that will be generated are highlighted in blue.Due to limited space, we present one example used for each task.Please refer to our code repository for the full prompt.Prompt Type Prompt ExampleGet Feedback Q: The following paragraphs each describe a set of three objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a branch, there are three birds: a hummingbird, an owl, and a falcon.The falcon is to the right of the owl.The hummingbird is to the left of the owl.Options: (A) The hummingbird is the second from the left.(B) The owl is the second from the left.(C) The falcon is the second from the left.Answer: Let's think step by step.Let "??" represents 0 or more objects, and "?" represents exactly 1 object.(1) The falcon is to the right of the owl: "(left) ?? owl ?? falcon ?? (right)".(2) The hummingbird is to the left of the owl: "(left) ?? hummingbird ?? owl ?? (right)".(3) There are in total three birds: a hummingbird, an owl, and a falcon.(4) Combining (1) and (2) we get the following ordering: "(left) ?? hummingbird ?? owl ?? falcon ?? (right)".(5) Combining (3) and (4) we get the following ordering: "(left) hummingbird owl falcon (right)".(6) According to this ordering, the bird that is second from the left is the falcon.(Final response) The falcon is the second from the left.So the answer is (C).Earliest error step: (6) Error segment: "the bird that is second from the left is the falcon" Error reason: It is inconsistent with the ordering in step (5), which established that "(left) hummingbird owl falcon (right)".[END] Get Improvement Q: The following paragraphs each describe a set of three objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a branch, there are three birds: a hummingbird, an owl, and a falcon.The falcon is to the right of the owl.The hummingbird is to the left of the owl.Options: (A) The hummingbird is the second from the left.(B) The owl is the second from the left.(C) The falcon is the second from the left.Answer: Let's think step by step.Let "??" represents 0 or more objects, and "?" represents exactly 1 object.(1) The falcon is to the right of the owl: "(left) ?? owl ?? falcon ?? (right)".(2) The hummingbird is to the left of the owl: "(left) ?? hummingbird ?? owl ?? (right)".(3) There are in total three birds: a hummingbird, an owl, and a falcon.(4) Combining (1) and (2) we get the following ordering: "(left) ?? hummingbird ?? owl ?? falcon ?? (right)".(5) Combining (3) and (4) we get the following ordering: "(left) hummingbird owl falcon (right)".(6) According to this ordering, the bird that is second from the left is the falcon.(Final response) The falcon is the second from the left.So the answer is (C).Feedback:Step (1) to (5) are correct.In step (6) the part "the bird that is second from the left is the falcon" is incorrect.This is because it is inconsistent with the ordering in step (5), which established that "(left) hummingbird owl falcon (right)".Updated Answer: Let's think step by step.Let "??" represents 0 or more objects, and "?" represents exactly 1 object.(1) The falcon is to the right of the owl: "(left) ?? owl ?? falcon ?? (right)".(2) The hummingbird is to the left of the owl: "(left) ?? hummingbird ?? owl ?? (right)".(3) There are in total three birds: a hummingbird, an owl, and a falcon.(4) Combining (1) and (2) we get the following ordering: "(left) ?? hummingbird ?? owl ?? falcon ?? (right)".(5) Combining (3) and (4) we get the following ordering: "(left) hummingbird owl falcon (right)".(6) According to this ordering, the bird that is second from the left is the owl.(Final response) The owl is the second from the left.So the answer is (B).[END]TableA16: Prompt used for generating feedback and improvement for Logical Deduction.The generated feedback is then formatted as "Step 1 to step {first error step -1} is correct.In step {first error step} the part '{error part}' is incorrect.This is because '{error reason}'."In general, we used three-shot prompting.Parts that will be generated are highlighted in blue.Due to limited space, we present one example used for each task.Please refer to our code repository for the full prompt.
. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mc-Grew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, 2018Hindsight experience replay</p>
<p>Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. Wanyu Du, Myung Zae, Vipul Kim, Raheja, 10.18653/v1/2022.in2writing-1.14Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants. the First Workshop on Intelligent and Interactive Writing AssistantsDublin, IrelandAssociation for Computational LinguisticsDhruv Kumar, and Dongyeop Kang. 2022. In2Writing 2022</p>
<p>RealToxi-cityPrompts: Evaluating neural toxic degeneration in language models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, 10.18653/v1/2020.findings-emnlp.301Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Gradient boosting machine: A survey. Zhiyuan He, Danchen Lin, Thomas Lau, Mike Wu, 2019</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Distilling the knowledge in a neural network. 2015</p>
<p>Large language models are reasoning teachers. Namgyu Ho, Laura Schmid, Se-Young Yun, 2023</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Impossible distillation: from low-quality model to high-quality dataset &amp; model for summarization and paraphrasing. Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin Choi, 2024</p>
<p>One-vs-the-rest loss to focus on important samples in adversarial training. Sekitoshi Kanai, Masanori Shin'ya Yamaguchi, Hiroshi Yamada, Kentaro Takahashi, Yasutoshi Ohno, Ida, 2023</p>
<p>Not all samples are created equal: Deep learning with importance sampling. Angelos Katharopoulos, François Fleuret, 2019</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Satinder Singh, and Volodymyr Mnih. Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, Steven Strouse, Angelos Hansen, Ethan Filos, Maxime Brooks, Himanshu Gazeau, Sahni, 2022context reinforcement learning with algorithm distillation</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJan</p>
<p>Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization. 2019</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 2023</p>
<p>Think about it! improving defeasible reasoning by first modeling the question scenario. Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, Eduard Hovy, 2021</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, Orca: Progressive learning from complex explanation traces of gpt-4. 2023</p>
<p>OpenAI: Introducing ChatGPT. 2022OpenAI</p>
<p>GPT-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Refiner: Reasoning feedback on intermediate representations. 2023</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, 2023a</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023b. Instruction tuning with gpt-4. </p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, 10.1145/3394486.3406703Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jan Leike. 2022Jonathan Ward</p>
<p>A brief introduction to boosting. Robert E Schapire, IJ- CAI'99Proceedings of the 16th International Joint Conference on Artificial Intelligence -Volume. the 16th International Joint Conference on Artificial Intelligence -VolumeSan Francisco, CA, USAMorgan Kaufmann Publishers Inc19992</p>
<p>. Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, 2016Prioritized experience replay</p>
<p>Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2022. 2023Reflexion: Language agents with verbal reinforcement learning</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2023</p>
<p>Graphbased, self-supervised program repair from diagnostic feedback. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 2022. 2020Challenging Michihiro Yasunaga and Percy Liang</p>
<p>Selfee: Iterative self-revising llm empowered by selffeedback generation. Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Minjoon Seo, 2023Blog post</p>
<p>Koichiro Yoshino, Yun-Nung Chen, Paul Crook, Satwik Kottur, Jinchao Li, Behnam Hedayatnia, Seungwhan Moon, Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, Jie Zhou, Seokhwan Kim, Yang Liu, Di Jin, Alexandros Papangelis, Karthik Gopalakrishnan, Dilek Hakkani-Tur, Babak Damavandi, Alborz Geramifard, Chiori Hori, Ankit Shah, Chen Zhang, Haizhou Li, João Sedoc, Luis F D'haro, Rafael Banchs, Alexander Rudnicky, 10.1109/TASLP.2023.3293030Overview of the tenth dialog system technology challenge: Dstc10. IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2023</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022aOpt: Open pre-trained transformer language models</p>
<p>The wisdom of hindsight makes language models better instruction followers. Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, Joseph E Gonzalez, 2023</p>
<p>A survey of active learning for natural language processing. Zhisong Zhang, Emma Strubell, Eduard Hovy, 10.18653/v1/2022.emnlp-main.414Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022bAssociation for Computational Linguistics</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>            </div>
        </div>

    </div>
</body>
</html>
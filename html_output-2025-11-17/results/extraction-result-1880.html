<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1880 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1880</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1880</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-37.html">extraction-schema-37</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-279392181</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.11343v2.pdf" target="_blank">From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review</a></p>
                <p><strong>Paper Abstract:</strong> The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1880.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1880.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT ranking system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based pairwise-comparison ranking system (GPT ranking system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A peer-review redesign in which GPT-4o-mini agents perform large numbers of randomized pairwise paper comparisons; outcomes are aggregated with the Bradley-Terry model to produce a global ranking and acceptance decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review (LLM-based automated ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>nearest-neighbor distance in embedding space (OpenAI text-embedding-3-small) between a paper's abstract and the closest abstract in the same conference (smaller distance = lower novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>GPT-selected top-tier papers have statistically smaller nearest-neighbor distances than human-selected papers (significant for ICLR 2023 Notable-top-5%, ICLR 2024 Oral, and EMNLP 2023 Main Track; exact mean distances and p-values reported in figures/tables of the paper). Institutional concentration: in ICLR 2023, 43.8% of GPT top-tier papers came from 10 institutions vs 27.0% for humans; ICLR 2024: 37.2% vs 26.7%. Area acceptance disparities: robotics (GPT 0.56 vs human 0.32), societal considerations (0.51 vs 0.30), learning theory (GPT 0.12 vs human 0.50), optimization (0.12 vs 0.31).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>negative association (papers with larger nearest-neighbor distances — i.e., more novel topics — are less likely to be selected by the GPT ranking system; monotonic/associative relationship reported, not a fitted functional form)</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>No multi-year recognition lag for novelty is reported; temporal pattern analyzed relates to number of pairwise comparisons (agents): system performance (average citations of accepted set) improves as agents increase from ~10^3 to >10^5 and plateaus beyond ~10^5 comparisons (average citations converge ~20).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>computer science / machine learning (ICLR 2023, ICLR 2024, NeurIPS 2023, EMNLP 2023, CoRL 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Clear area-specific differences within ML: higher GPT acceptance in application-oriented areas (robotics: 0.56 vs 0.32; societal considerations: 0.51 vs 0.30) and lower acceptance in theoretical areas (learning theory: 0.12 vs 0.50; optimization: 0.12 vs 0.31). Some conferences (CoRL 2023, NeurIPS 2023) did not show significant novelty differences.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>future citation counts (average citations of accepted papers); also acceptance decisions and area-distributions</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>future citation counts used as a proxy for long-term impact (acknowledged as imperfect)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>GPT ranking system converges to human-level average citations: GPT (no area control) = 20.00 avg citations, GPT (area control) = 18.97, Human acceptance = 19.36; GPT rating baseline = 11.41, Random = 8.56. Spearman correlation between BT scores and average human ratings: 27% (ICLR 2023) and 24% (ICLR 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>The paper does not label papers 'incremental' vs 'transformational' explicitly; using embedding-based novelty as a proxy for transformational/novel topics, GPT ranking selects papers with lower novelty (smaller nearest-neighbor distance) than humans — effect statistically significant in several datasets (see bias_magnitude).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Yes — the paper examines multiple proxy failures simultaneously (novelty measured by embedding distance, institutional concentration measured by Gini coefficient, and area acceptance differences). They co-occur (reduced novelty and increased institutional concentration among GPT-selected papers) but no formal multiplicative/ additive decomposition is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Outperforms rating-based LLM baseline and random: at scale GPT ranking (no area control) achieves 20.00 avg citations vs human 19.36, GPT rating baseline 11.41, random 8.56. Performance scales with number of pairwise comparisons (poor at ~10^3, approaches human-level by >10^5 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Hypothesized qualitatively: LLMs trained on web-crawled corpora may favor applied/practical content, explaining higher acceptance in application-oriented fields; paper does not provide a quantitative estimate of effect of training-data composition on bias.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Area-control variant (force accepted set area distribution to match human reviewers) and scaling (increasing #pairwise comparisons). Area control reduced area-distribution differences but yielded slightly lower avg citations (area control: 18.97 vs no-control: 20.00). Increasing comparisons reduced random-like behavior and improved selection quality (plateau beyond ~10^5 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>CoRL 2023 and NeurIPS 2023 did not show statistically significant differences in nearest-neighbor novelty between GPT-selected and human-selected top-tier papers (no significant novelty bias observed in those cases).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Sample size/scale of comparisons (number of agents) strongly moderates performance; area control moderates area-distribution bias; author institutional prestige correlates with higher selection by GPT (higher Gini coefficients). No quantified moderation of novelty bias by author reputation is provided beyond institutional concentration statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Empirical analysis over multiple ML conference corpora (examples: ICLR 2024 pool ~7158 papers mentioned; Table 2 lists counts per conference: ICLR 2023 total ~3, ... specific breakdowns provided). Methods: randomized pairwise comparisons by GPT-4o-mini agents (each agent performs one pairwise comparison), Bradley-Terry maximum-likelihood aggregation to recover BT scores and ranking; novelty measured via OpenAI text-embedding-3-small nearest-neighbor distance; institutional inequality measured with Gini coefficient; comparisons of average future citation counts as proxy for impact; additional experiments with Gemini 2.0 Flash and Claude-3-Haiku at >10^6 comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1880.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1880.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT rating system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based paper-level rating baseline (GPT rating system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where three independent GPT reviewers generate reviews per paper and a meta-reviewer synthesizes ratings (1–10); acceptance decisions made on the synthesized rating, representing a replication of traditional absolute-scoring review workflows with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review (LLM-based rating/score aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not directly used to measure novelty in the study; baseline's behavior on novel work is evaluated indirectly by comparing selected papers' citation counts and topic novelty against GPT ranking and human decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Baseline performs poorly at distinguishing high-impact papers: average citations for GPT rating selected set = 11.41 vs GPT ranking 20.00, humans 19.36, random 8.56. GPT ratings concentrated around 6–7 leading to poor discrimination. No direct numeric novelty-bias measure reported for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>limited discriminative range (scores concentrated near midrange) leading to weak association between rating and future impact; described qualitatively rather than with a fitted function.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>No temporal dynamics reported specifically for the rating baseline (only aggregate performance comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>computer science / machine learning (same conference datasets as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not separately characterized across research areas in detail; baseline's overall lower discriminative power implies weaker signal across fields.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>synthesized paper rating (1–10) and downstream selection; compared via average future citation counts</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>future citation counts used as proxy ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Large gap in discriminative ability: GPT rating selection avg citations 11.41 vs human 19.36; indicates underperformance in selecting high-impact work.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Not explicitly compared, but poor discrimination suggests rating baseline is less effective at surfacing high-impact (potentially transformational) work compared to GPT ranking; no explicit novelty-bias numeric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Examined alongside other proxies (citations, area distributions) — GPT rating baseline underperforms on citation proxy but no multi-proxy compounding analysis done.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Inferior to GPT ranking and humans in selecting high-citation papers; ratings concentrated and insufficiently diverse to separate papers by future impact.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not analyzed for this baseline specifically beyond general discussion of LLM limitations in evaluative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>No intervention beyond the standard meta-review synthesis; paper tests alternative (pairwise) mechanism as an intervention and shows superior outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>None reported in paper where GPT rating outperforms GPT ranking or humans on novelty-based measures.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Not reported beyond general model prompting/ensembling techniques referenced in related work that can moderately improve rating performance in other studies.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Applied at same conference datasets as GPT ranking for head-to-head comparison; three independent GPT reviews per paper + meta-reviewer rating; aggregate acceptance decisions compared by average citations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1880.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1880.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding nearest-neighbor novelty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest-neighbor embedding distance for topic novelty (OpenAI text-embedding-3-small)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operationalization of topic novelty: for each abstract, compute embedding and take Euclidean/cosine distance to the closest other abstract in the same conference; smaller distance = lower novelty (more similar to existing work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>measurement tool for novelty in evaluation-system analyses (used to compare selection biases)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>nearest-neighbor vector distance in embedding space</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>GPT-selected top-tier papers have smaller mean nearest-neighbor distances than human-selected papers; differences are statistically significant (p<0.05) for several comparisons (ICLR 2023 Notable-top-5%, ICLR 2024 Oral, EMNLP 2023 Main Track); exact mean distances and confidence intervals plotted in paper figures.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>negative association between embedding distance (novelty) and probability of GPT selection (lower distance → higher selection probability by GPT relative to humans).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>No longitudinal temporal recognition analysis (e.g., delayed recognition) is performed with this measure; novelty comparisons are cross-sectional within conference cohorts.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>machine learning conference papers (ICLR, EMNLP, NeurIPS, CoRL)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Significant in some conferences/tracks (ICLR 2023 Notable-top-5%, ICLR 2024 Oral, EMNLP 2023 Main), not significant in others (CoRL 2023 Oral, NeurIPS 2023 Oral), indicating variability by venue/track.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>topic novelty measured by embedding distance (not itself a proxy for impact), used to examine selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>No absolute ground truth for novelty; embedding distance used as a continuous proxy for topical novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>The study documents a selection gap (GPT selects lower-embedding-distance papers) but does not quantify an explicit 'truth gap' between embedding novelty and long-term scientific value beyond showing GPT-selected lower-novelty sets have comparable or higher average citations.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Embedding distance used as proxy: GPT selections skew toward less novel (more incremental-looking by embedding similarity) topics relative to human selections; exact % shift in mean distance shown in figures (statistical tests reported for several datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Used together with citation counts and institutional concentration to demonstrate multi-faceted biases; no multiplicative decomposition reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Used to reveal that an automated LLM-ranking mechanism favors lower-embedding-distance (less novel) papers relative to humans; despite that, GPT ranking still identifies high-citation papers at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not directly measured here, but authors hypothesize that LLM training corpora (web, applied content) may explain preference for less novel, application-oriented work.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>No direct intervention to correct embedding-measured novelty bias; area-control preserves human area-proportions but does not directly restore embedding-distance distributions (area-control reduced acceptance-area bias but slightly lowered citation average).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Conferences/tracks (CoRL 2023, NeurIPS 2023) where nearest-neighbor distance differences were not statistically significant serve as counter-examples.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Venue/track and sample sizes moderate whether novelty differences are statistically detectable; smaller sample sizes (oral subsets) can reduce significance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Embeddings computed for all paper abstracts per conference using text-embedding-3-small; nearest-neighbor distances computed within-conference; comparisons made between sets of papers selected by humans vs GPT across multiple conferences/tracks with statistical tests (p-values reported in figures).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1880.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1880.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human peer review baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conventional human peer review decisions (conference accept/reject, track tiers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human reviewers' acceptance and tier decisions used as a baseline to compare selection quality, novelty, area distributions, and institutional concentration against LLM-based systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review (human committees and reviewers)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Compared via the same embedding nearest-neighbor distance metric; human-selected top-tier papers on average had larger nearest-neighbor distances (higher measured novelty) than GPT-selected top-tier papers in several datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Humans selected more novel topics by embedding measure in several datasets (statistically significant differences noted). Institutional concentration among human-selected top-tier papers was lower: ICLR 2023 top-10 institutions produced 27.0% of human top-tier vs 43.8% for GPT; ICLR 2024: 26.7% human vs 37.2% GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>Humans show a weaker negative association between novelty (embedding distance) and acceptance than GPT; humans accept a higher proportion of higher-novelty papers in several datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>No longitudinal novelty-recognition lag analyzed for human review in this paper; human decisions are taken at conference time and compared cross-sectionally to future citations.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>machine learning conferences as above</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Human acceptance rates are higher in theoretical areas (learning theory acceptance: humans 0.50 vs GPT 0.12) and lower in some application areas relative to GPT (robotics humans 0.32 vs GPT 0.56).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>human accept/reject and tier decisions; used to compare with future citation proxy</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>future citation counts as proxy for long-term impact; human-selected papers average 19.36 citations in reported comparison</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Human selections yield average citations 19.36 vs GPT ranking 20.00 (no area control), showing that humans and GPT overlap but differ in the composition of selected papers (novelty and institutional distribution). Spearman correlation between BT scores and average human ratings: 24–27% indicating moderate alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Humans appear comparatively more likely than GPT to select higher-novelty (transformational-looking by embedding proxy) papers in multiple datasets, with statistical significance in several tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Human selection is not immune to biases (visibility effects, institutional advantages) but in this paper human selections show lower institutional concentration than GPT-selected sets; both selection modes are evaluated against citation proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>N/A (this entry is a human baseline for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not applicable for humans; however, the paper notes human processes confer visibility (conference presentation) which can affect downstream citation counts and complicate assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Area-control is an artificial intervention applied to GPT rankings to match human area distribution; no direct interventions on human reviewers are tested in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Human reviewers also show inconsistency (paper cites NeurIPS 2021 consistency experiment where human-human acceptance overlap was 48%), showing that human committees reject some papers later judged impactful.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Human review variability across committees, area-specific evaluation norms, and the visibility boost conferred by acceptance moderate outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Human decision labels taken from OpenReview API for multiple conferences and tracks (see Table 2 for per-conference counts); compared cross-sectionally to LLM outputs and future citation counts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment <em>(Rating: 2)</em></li>
                <li>Bias in peer review <em>(Rating: 2)</em></li>
                <li>Cite-seeing and reviewing: A study on citation bias in peer review <em>(Rating: 2)</em></li>
                <li>Can large language models provide useful feedback on research papers? a large-scale empirical analysis <em>(Rating: 2)</em></li>
                <li>Is peer review broken? submissions are up, reviewers are overtaxed, and authors are lodging complaint after complaint about the process at top-tier journals. what's wrong with peer review? <em>(Rating: 1)</em></li>
                <li>Prior and prejudice: The novice reviewers' bias against resubmissions in conference peer review <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1880",
    "paper_id": "paper-279392181",
    "extraction_schema_id": "extraction-schema-37",
    "extracted_data": [
        {
            "name_short": "GPT ranking system",
            "name_full": "LLM-based pairwise-comparison ranking system (GPT ranking system)",
            "brief_description": "A peer-review redesign in which GPT-4o-mini agents perform large numbers of randomized pairwise paper comparisons; outcomes are aggregated with the Bradley-Terry model to produce a global ranking and acceptance decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "peer review (LLM-based automated ranking)",
            "novelty_measure": "nearest-neighbor distance in embedding space (OpenAI text-embedding-3-small) between a paper's abstract and the closest abstract in the same conference (smaller distance = lower novelty)",
            "bias_magnitude": "GPT-selected top-tier papers have statistically smaller nearest-neighbor distances than human-selected papers (significant for ICLR 2023 Notable-top-5%, ICLR 2024 Oral, and EMNLP 2023 Main Track; exact mean distances and p-values reported in figures/tables of the paper). Institutional concentration: in ICLR 2023, 43.8% of GPT top-tier papers came from 10 institutions vs 27.0% for humans; ICLR 2024: 37.2% vs 26.7%. Area acceptance disparities: robotics (GPT 0.56 vs human 0.32), societal considerations (0.51 vs 0.30), learning theory (GPT 0.12 vs human 0.50), optimization (0.12 vs 0.31).",
            "relationship_type": "negative association (papers with larger nearest-neighbor distances — i.e., more novel topics — are less likely to be selected by the GPT ranking system; monotonic/associative relationship reported, not a fitted functional form)",
            "temporal_pattern": "No multi-year recognition lag for novelty is reported; temporal pattern analyzed relates to number of pairwise comparisons (agents): system performance (average citations of accepted set) improves as agents increase from ~10^3 to &gt;10^5 and plateaus beyond ~10^5 comparisons (average citations converge ~20).",
            "field_studied": "computer science / machine learning (ICLR 2023, ICLR 2024, NeurIPS 2023, EMNLP 2023, CoRL 2023)",
            "field_differences": "Clear area-specific differences within ML: higher GPT acceptance in application-oriented areas (robotics: 0.56 vs 0.32; societal considerations: 0.51 vs 0.30) and lower acceptance in theoretical areas (learning theory: 0.12 vs 0.50; optimization: 0.12 vs 0.31). Some conferences (CoRL 2023, NeurIPS 2023) did not show significant novelty differences.",
            "proxy_metric_studied": "future citation counts (average citations of accepted papers); also acceptance decisions and area-distributions",
            "ground_truth_measure": "future citation counts used as a proxy for long-term impact (acknowledged as imperfect)",
            "proxy_truth_gap": "GPT ranking system converges to human-level average citations: GPT (no area control) = 20.00 avg citations, GPT (area control) = 18.97, Human acceptance = 19.36; GPT rating baseline = 11.41, Random = 8.56. Spearman correlation between BT scores and average human ratings: 27% (ICLR 2023) and 24% (ICLR 2024).",
            "incremental_vs_transformational": "The paper does not label papers 'incremental' vs 'transformational' explicitly; using embedding-based novelty as a proxy for transformational/novel topics, GPT ranking selects papers with lower novelty (smaller nearest-neighbor distance) than humans — effect statistically significant in several datasets (see bias_magnitude).",
            "multiple_proxy_failures": "Yes — the paper examines multiple proxy failures simultaneously (novelty measured by embedding distance, institutional concentration measured by Gini coefficient, and area acceptance differences). They co-occur (reduced novelty and increased institutional concentration among GPT-selected papers) but no formal multiplicative/ additive decomposition is reported.",
            "automated_system_performance": "Outperforms rating-based LLM baseline and random: at scale GPT ranking (no area control) achieves 20.00 avg citations vs human 19.36, GPT rating baseline 11.41, random 8.56. Performance scales with number of pairwise comparisons (poor at ~10^3, approaches human-level by &gt;10^5 comparisons).",
            "training_data_bias": "Hypothesized qualitatively: LLMs trained on web-crawled corpora may favor applied/practical content, explaining higher acceptance in application-oriented fields; paper does not provide a quantitative estimate of effect of training-data composition on bias.",
            "intervention_tested": "Area-control variant (force accepted set area distribution to match human reviewers) and scaling (increasing #pairwise comparisons). Area control reduced area-distribution differences but yielded slightly lower avg citations (area control: 18.97 vs no-control: 20.00). Increasing comparisons reduced random-like behavior and improved selection quality (plateau beyond ~10^5 comparisons).",
            "counter_examples": "CoRL 2023 and NeurIPS 2023 did not show statistically significant differences in nearest-neighbor novelty between GPT-selected and human-selected top-tier papers (no significant novelty bias observed in those cases).",
            "moderating_factors": "Sample size/scale of comparisons (number of agents) strongly moderates performance; area control moderates area-distribution bias; author institutional prestige correlates with higher selection by GPT (higher Gini coefficients). No quantified moderation of novelty bias by author reputation is provided beyond institutional concentration statistics.",
            "sample_size_and_methods": "Empirical analysis over multiple ML conference corpora (examples: ICLR 2024 pool ~7158 papers mentioned; Table 2 lists counts per conference: ICLR 2023 total ~3, ... specific breakdowns provided). Methods: randomized pairwise comparisons by GPT-4o-mini agents (each agent performs one pairwise comparison), Bradley-Terry maximum-likelihood aggregation to recover BT scores and ranking; novelty measured via OpenAI text-embedding-3-small nearest-neighbor distance; institutional inequality measured with Gini coefficient; comparisons of average future citation counts as proxy for impact; additional experiments with Gemini 2.0 Flash and Claude-3-Haiku at &gt;10^6 comparisons.",
            "uuid": "e1880.0"
        },
        {
            "name_short": "GPT rating system",
            "name_full": "LLM-based paper-level rating baseline (GPT rating system)",
            "brief_description": "Baseline where three independent GPT reviewers generate reviews per paper and a meta-reviewer synthesizes ratings (1–10); acceptance decisions made on the synthesized rating, representing a replication of traditional absolute-scoring review workflows with LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "peer review (LLM-based rating/score aggregation)",
            "novelty_measure": "Not directly used to measure novelty in the study; baseline's behavior on novel work is evaluated indirectly by comparing selected papers' citation counts and topic novelty against GPT ranking and human decisions.",
            "bias_magnitude": "Baseline performs poorly at distinguishing high-impact papers: average citations for GPT rating selected set = 11.41 vs GPT ranking 20.00, humans 19.36, random 8.56. GPT ratings concentrated around 6–7 leading to poor discrimination. No direct numeric novelty-bias measure reported for this baseline.",
            "relationship_type": "limited discriminative range (scores concentrated near midrange) leading to weak association between rating and future impact; described qualitatively rather than with a fitted function.",
            "temporal_pattern": "No temporal dynamics reported specifically for the rating baseline (only aggregate performance comparisons).",
            "field_studied": "computer science / machine learning (same conference datasets as main experiments)",
            "field_differences": "Not separately characterized across research areas in detail; baseline's overall lower discriminative power implies weaker signal across fields.",
            "proxy_metric_studied": "synthesized paper rating (1–10) and downstream selection; compared via average future citation counts",
            "ground_truth_measure": "future citation counts used as proxy ground truth",
            "proxy_truth_gap": "Large gap in discriminative ability: GPT rating selection avg citations 11.41 vs human 19.36; indicates underperformance in selecting high-impact work.",
            "incremental_vs_transformational": "Not explicitly compared, but poor discrimination suggests rating baseline is less effective at surfacing high-impact (potentially transformational) work compared to GPT ranking; no explicit novelty-bias numeric reported.",
            "multiple_proxy_failures": "Examined alongside other proxies (citations, area distributions) — GPT rating baseline underperforms on citation proxy but no multi-proxy compounding analysis done.",
            "automated_system_performance": "Inferior to GPT ranking and humans in selecting high-citation papers; ratings concentrated and insufficiently diverse to separate papers by future impact.",
            "training_data_bias": "Not analyzed for this baseline specifically beyond general discussion of LLM limitations in evaluative tasks.",
            "intervention_tested": "No intervention beyond the standard meta-review synthesis; paper tests alternative (pairwise) mechanism as an intervention and shows superior outcomes.",
            "counter_examples": "None reported in paper where GPT rating outperforms GPT ranking or humans on novelty-based measures.",
            "moderating_factors": "Not reported beyond general model prompting/ensembling techniques referenced in related work that can moderately improve rating performance in other studies.",
            "sample_size_and_methods": "Applied at same conference datasets as GPT ranking for head-to-head comparison; three independent GPT reviews per paper + meta-reviewer rating; aggregate acceptance decisions compared by average citations.",
            "uuid": "e1880.1"
        },
        {
            "name_short": "Embedding nearest-neighbor novelty",
            "name_full": "Nearest-neighbor embedding distance for topic novelty (OpenAI text-embedding-3-small)",
            "brief_description": "Operationalization of topic novelty: for each abstract, compute embedding and take Euclidean/cosine distance to the closest other abstract in the same conference; smaller distance = lower novelty (more similar to existing work).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "measurement tool for novelty in evaluation-system analyses (used to compare selection biases)",
            "novelty_measure": "nearest-neighbor vector distance in embedding space",
            "bias_magnitude": "GPT-selected top-tier papers have smaller mean nearest-neighbor distances than human-selected papers; differences are statistically significant (p&lt;0.05) for several comparisons (ICLR 2023 Notable-top-5%, ICLR 2024 Oral, EMNLP 2023 Main Track); exact mean distances and confidence intervals plotted in paper figures.",
            "relationship_type": "negative association between embedding distance (novelty) and probability of GPT selection (lower distance → higher selection probability by GPT relative to humans).",
            "temporal_pattern": "No longitudinal temporal recognition analysis (e.g., delayed recognition) is performed with this measure; novelty comparisons are cross-sectional within conference cohorts.",
            "field_studied": "machine learning conference papers (ICLR, EMNLP, NeurIPS, CoRL)",
            "field_differences": "Significant in some conferences/tracks (ICLR 2023 Notable-top-5%, ICLR 2024 Oral, EMNLP 2023 Main), not significant in others (CoRL 2023 Oral, NeurIPS 2023 Oral), indicating variability by venue/track.",
            "proxy_metric_studied": "topic novelty measured by embedding distance (not itself a proxy for impact), used to examine selection bias.",
            "ground_truth_measure": "No absolute ground truth for novelty; embedding distance used as a continuous proxy for topical novelty.",
            "proxy_truth_gap": "The study documents a selection gap (GPT selects lower-embedding-distance papers) but does not quantify an explicit 'truth gap' between embedding novelty and long-term scientific value beyond showing GPT-selected lower-novelty sets have comparable or higher average citations.",
            "incremental_vs_transformational": "Embedding distance used as proxy: GPT selections skew toward less novel (more incremental-looking by embedding similarity) topics relative to human selections; exact % shift in mean distance shown in figures (statistical tests reported for several datasets).",
            "multiple_proxy_failures": "Used together with citation counts and institutional concentration to demonstrate multi-faceted biases; no multiplicative decomposition reported.",
            "automated_system_performance": "Used to reveal that an automated LLM-ranking mechanism favors lower-embedding-distance (less novel) papers relative to humans; despite that, GPT ranking still identifies high-citation papers at scale.",
            "training_data_bias": "Not directly measured here, but authors hypothesize that LLM training corpora (web, applied content) may explain preference for less novel, application-oriented work.",
            "intervention_tested": "No direct intervention to correct embedding-measured novelty bias; area-control preserves human area-proportions but does not directly restore embedding-distance distributions (area-control reduced acceptance-area bias but slightly lowered citation average).",
            "counter_examples": "Conferences/tracks (CoRL 2023, NeurIPS 2023) where nearest-neighbor distance differences were not statistically significant serve as counter-examples.",
            "moderating_factors": "Venue/track and sample sizes moderate whether novelty differences are statistically detectable; smaller sample sizes (oral subsets) can reduce significance.",
            "sample_size_and_methods": "Embeddings computed for all paper abstracts per conference using text-embedding-3-small; nearest-neighbor distances computed within-conference; comparisons made between sets of papers selected by humans vs GPT across multiple conferences/tracks with statistical tests (p-values reported in figures).",
            "uuid": "e1880.2"
        },
        {
            "name_short": "Human peer review baseline",
            "name_full": "Conventional human peer review decisions (conference accept/reject, track tiers)",
            "brief_description": "Human reviewers' acceptance and tier decisions used as a baseline to compare selection quality, novelty, area distributions, and institutional concentration against LLM-based systems.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_system_type": "peer review (human committees and reviewers)",
            "novelty_measure": "Compared via the same embedding nearest-neighbor distance metric; human-selected top-tier papers on average had larger nearest-neighbor distances (higher measured novelty) than GPT-selected top-tier papers in several datasets.",
            "bias_magnitude": "Humans selected more novel topics by embedding measure in several datasets (statistically significant differences noted). Institutional concentration among human-selected top-tier papers was lower: ICLR 2023 top-10 institutions produced 27.0% of human top-tier vs 43.8% for GPT; ICLR 2024: 26.7% human vs 37.2% GPT.",
            "relationship_type": "Humans show a weaker negative association between novelty (embedding distance) and acceptance than GPT; humans accept a higher proportion of higher-novelty papers in several datasets.",
            "temporal_pattern": "No longitudinal novelty-recognition lag analyzed for human review in this paper; human decisions are taken at conference time and compared cross-sectionally to future citations.",
            "field_studied": "machine learning conferences as above",
            "field_differences": "Human acceptance rates are higher in theoretical areas (learning theory acceptance: humans 0.50 vs GPT 0.12) and lower in some application areas relative to GPT (robotics humans 0.32 vs GPT 0.56).",
            "proxy_metric_studied": "human accept/reject and tier decisions; used to compare with future citation proxy",
            "ground_truth_measure": "future citation counts as proxy for long-term impact; human-selected papers average 19.36 citations in reported comparison",
            "proxy_truth_gap": "Human selections yield average citations 19.36 vs GPT ranking 20.00 (no area control), showing that humans and GPT overlap but differ in the composition of selected papers (novelty and institutional distribution). Spearman correlation between BT scores and average human ratings: 24–27% indicating moderate alignment.",
            "incremental_vs_transformational": "Humans appear comparatively more likely than GPT to select higher-novelty (transformational-looking by embedding proxy) papers in multiple datasets, with statistical significance in several tracks.",
            "multiple_proxy_failures": "Human selection is not immune to biases (visibility effects, institutional advantages) but in this paper human selections show lower institutional concentration than GPT-selected sets; both selection modes are evaluated against citation proxy.",
            "automated_system_performance": "N/A (this entry is a human baseline for comparisons).",
            "training_data_bias": "Not applicable for humans; however, the paper notes human processes confer visibility (conference presentation) which can affect downstream citation counts and complicate assessment.",
            "intervention_tested": "Area-control is an artificial intervention applied to GPT rankings to match human area distribution; no direct interventions on human reviewers are tested in this work.",
            "counter_examples": "Human reviewers also show inconsistency (paper cites NeurIPS 2021 consistency experiment where human-human acceptance overlap was 48%), showing that human committees reject some papers later judged impactful.",
            "moderating_factors": "Human review variability across committees, area-specific evaluation norms, and the visibility boost conferred by acceptance moderate outcomes.",
            "sample_size_and_methods": "Human decision labels taken from OpenReview API for multiple conferences and tracks (see Table 2 for per-conference counts); compared cross-sectionally to LLM outputs and future citation counts.",
            "uuid": "e1880.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment",
            "rating": 2
        },
        {
            "paper_title": "Bias in peer review",
            "rating": 2
        },
        {
            "paper_title": "Cite-seeing and reviewing: A study on citation bias in peer review",
            "rating": 2
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis",
            "rating": 2
        },
        {
            "paper_title": "Is peer review broken? submissions are up, reviewers are overtaxed, and authors are lodging complaint after complaint about the process at top-tier journals. what's wrong with peer review?",
            "rating": 1
        },
        {
            "paper_title": "Prior and prejudice: The novice reviewers' bias against resubmissions in conference peer review",
            "rating": 1
        }
    ],
    "cost": 0.01538525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review</p>
<p>Yaohui Zhang 
Stanford University</p>
<p>Haijing Zhang 
Wenlong Ji 
Stanford University</p>
<p>Tianyu Hua 
Stanford University</p>
<p>Nick Haber 
Stanford University</p>
<p>Hancheng Cao 
Stanford University</p>
<p>Emory University</p>
<p>Weixin Liang 
Stanford University</p>
<p>From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review
35F1C00BA9B6782833FDB39CBA5303CD
The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows.Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process.In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring.By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality.Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers.However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance.These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.</p>
<p>Introduction</p>
<p>The peer review system serves as a cornerstone for validating and disseminating scientific ideas [2].Yet, despite its widespread adoption, the peer review system has become increasingly strained because of the rapid growth of research output and a persistent shortage of qualified reviewers [13,30,40].</p>
<p>Figure 1: Comparison of the conventional peer review process (top) and the proposed pairwise evaluation ranking system.The traditional paradigm (followed by both human reviewers and existing LLM-based review systems) works on paper-level assessment, where each manuscript is first evaluated independently by multiple reviewers who provide detailed comments and scores.These evaluations are integrated by a meta-reviewer to recommend a final decision.Our method adopts pairwise comparisons between papers: each agent is randomly assigned a pair of submissions to evaluate.These comparative judgments are then aggregated using the Bradley-Terry model, where the resulting BT coefficients serve as scoring functions to produce a global ranking of all submissions.Final decisions are derived from the aggregated rankings, enabling a more objective assessment of the relative quality across the entire submission pool.</p>
<p>In this work, we propose a shift from replication to redesign.We focus on the decision-making layer of peer review-specifically, mechanisms that can rank submissions and support acceptance decisions.Our approach is intended to complement, not replace, the current peer review process.Specifically, we introduced and explored a novel mechanism that engages LLM agents in pairwise comparisons for more effective manuscript assessment.Rather than assigning absolute scores to individual papers, our approach directly contrasts pairs of submissions to determine which one better meets the multidimensional evaluation criteria.Each LLM agent is tasked with evaluating two manuscripts at a time, and by integrating the results from numerous pairwise comparisons, we can construct a robust overall assessment that more accurately reflects relative quality.</p>
<p>Our results indicate that with proper scaling, this pairwise mechanism demonstrates potential to identify papers of higher academic impact (as measured by future citation count), significantly outperforming paper-level rating approach.However, our analysis also reveals important limitations: Papers preferred by our LLM-based pairwise ranking system tend to exhibit lower topic novelty and greater concentration among a limited number of high-prestige institutions.While these biases highlight critical challenges for the deployment of LLM-based review systems, we view this work as a first step toward exploring fundamentally new paradigms for scalable, equitable, and robust research evaluation.</p>
<p>It is important to note that while we use proxy metrics-such as future citation counts-to evaluate our proposed mechanism, this represents an exploratory and pragmatic step rather than an ideal.The gold standard would be direct human evaluation of how well a mechanism supports the selection and improvement of high-quality research, but such evaluations are costly and difficult to scale [43].Proxy metrics offer a tractable, though imperfect, way to approximate long-term impact and anticipate how different mechanisms might perform.Ideally, a strong alternative review mechanism should correlate with-but not fully replicate-the outcomes of the current system, offering complementary perspectives and surfacing different strengths.As we show, our proposed pairwise comparison mechanism achieves this balance, aligning with current human judgments while introducing useful differentiation.Crucially, our goal is not to mimic existing outcomes, but to design mechanisms that add value.Moreover, we believe the strength of human-led peer review lies not only in final decisions, but also in the human interactions it enables-feedback, discussion, and iterative refinement-that help authors strengthen their work.Through these interactions, reviewers and authors also engage in a shared evaluative practice that reinforces norms, builds trust, and helps individuals identify with the scholarly community [63,19].Our intention is not to automate or replace this process, but to broaden how we understand and support scholarly evaluation.</p>
<p>Related Work</p>
<p>Challenges in Peer Review Peer review, while essential to scientific progress, faces numerous challenges in today's academic landscape.Reviewers often struggle with overwhelming workloads, further compounded by the absence of meaningful incentives and the inequitable distribution of review requests [7,31].In addition, implicit biases can influence evaluation [46,47,50], with studies showing that factors like author affiliation or gender may affect acceptance rates [59].As research output continues to accelerate globally [5], these systemic challenges demand innovative solutions to preserve the integrity and effectiveness of scientific evaluation.Earlier works [48,57,56] introduce pairwise comparisons by incorporating author-provided rankings to calibrate noisy reviewer scores.In contrast, we fully replace paper-level scores with large-scale pairwise comparisons performed by LLM agents, without relying on human ratings or author input.</p>
<p>LLMs for Peer Review Recent advancements in Large Language Models (LLMs) have sparked considerable interest in their potential to transform academic peer review processes [21,62,24,23].While existing studies [9,22,38] have demonstrated that LLMs can provide valuable feedback for research papers, exhibiting substantial overlap with reviews written by human reviewers, they often struggle with assigning reasonable ratings and making sound decisions, which raises concerns about their reliability for evaluative tasks [26,58].For instance, Niu et al. [35] showed that ChatGPT tends to grant acceptances: out of 1558 full paper evaluations, ChatGPT suggested 1243 acceptances compared to only 315 rejections, resulting in an acceptance rate of approximately 79.8%.Similarly, Zhou et al. [60] highlighted that LLMs could generate reasonable aspect scores (e.g.recommendation, soundness, originality) from human reviews but failed when given only the research paper.Lu et al. [27] improved the base LLM's decision-making process through prompting techniques such as self-reflection [42], few-shot examples [54] and response ensembling [53].CycleReviewer [55] and DeepReview [61] enhanced LLM-based paper review via fine-tuning on high-quality review dataset.</p>
<p>In contrast, Less effort is made to explore new paradigms that fundamentally rethink how LLMs can participate in the academic review process.</p>
<p>Method</p>
<p>Framework Overview</p>
<p>We now describe our peer review mechanism (Figure 1) that transforms the traditional paper-level assessment process into a comparative ranking system through pairwise judgments.This approach builds on a key insight that evaluators are generally more reliable when deciding which of two items is better than when assigning each an absolute score [8].By integrating a large number of pairwise judgments into a global ranking with the Bradley-Terry model, we could potentially mitigate the calibration inconsistencies and personal rating biases that plague conventional peer review systems.</p>
<p>The mechanism operates in three key stages: First, we collect pairwise paper comparisons from multiple LLM agents, where each agent evaluates a randomly sampled pair of papers and produces a binary preference judgment.Second, we apply the Bradley-Terry model to quantify each paper's relative quality through BT coefficients.Finally, we determine the optimal coefficients through maximum likelihood estimation and derive a complete ranking of all papers.</p>
<p>From Pairwise Comparisons to Rankings</p>
<p>Suppose we have N paper candidates indexed by i ∈ [N ] and M LLM agents participating in the evaluation process.We aim to establish a quality-based ranking through pairwise comparisons.The framework consists of the following steps:</p>
<ol>
<li>Pairwise Comparison: Let S = {(i, j) : i ̸ = j and i, j ∈ [N ]} be all possible pairs of papers to compare.For each of M agents, we randomly assign one pair (i, j) ∈ S to evaluate.Each agent then analyzes both papers and produces a binary preference judgment y ij ∈ {0, 1}, where y ij = 1 indicates that the agent prefers paper i over paper j, and y ij = 0 indicates the opposite preference.The full prompt used for pairwise judgment is provided in Supp Figure 7. Denote the collection of all chosen pairs as A, where |A| = M .We collect all comparison results into an observation set O = {(i, j, y ij ) : (i, j) ∈ A}, which contains triplets of the compared papers and their corresponding preference judgments from all agents.Since the size of |S| grows quadratically with n, we can only afford to assess a small fraction of all possible pairs, and hence it is challenging to recover the ranking under this scenario.Later in Section 4.2, we empirically examined how ranking quality scales with the number of LLM agents.The results reveal a clear scaling law, and we are able to recover a high-quality ranking with less than 2% samples of all possible pairs.1 2. Bradley-Terry Model: We employ the Bradley-Terry model [6] to recover the paper ranking from pairwise comparison results.The model provides stable statistical estimation from sparse comparisons, which is particularly suitable for our scenario.Specifically, it assumes each paper i has an underlying quality score β i ∈ R, and the outcome of comparing papers i and j is a Bernoulli random variable, where the probability of paper i beating j is determined by a logistic function of the score difference β i − β j , i.e.,
p ij := P(y ij = 1|(i, j)) = e βi e βi + e βj = 1 1 + e −(βi−βj )(1)
To estimate these scores β based on the outcome of observed pairwise comparisons, we utilize the maximum likelihood estimator on the Bradley-Terry model, which maximizes the log-likelihood function below:
L(β) = (i,j,y)∈O [y ij log(p ij ) + (1 − y ij ) log(1 − p ij )] .(2)</li>
<li>Ranking Inference: Once the scores are estimated, they can be used to rank all candidates in a descending order, with a higher score indicating "stronger" candidates with higher quality.</li>
</ol>
<p>Since all our experiments were conducted using GPT-4o mini [36] for its balance of performance and computational cost, we will refer to our proposed mechanism as the GPT ranking system in the following sections.</p>
<p>Experiments</p>
<p>We now empirically validate whether our proposed pairwise-based ranking framework better identifies high-impact papers compared to conventional rating-based approaches.</p>
<p>Dataset Desciption</p>
<p>We collected papers from major ML conferences publicly available on OpenReview, including ICLR, NeurIPS, CoRL, and EMNLP.Papers were categorized into different decision outcomes (e.g., accepted vs. rejected, main vs. findings track, oral vs. poster, etc.), reflecting the quality assessments by the peer review process.The paper PDFs and corresponding decisions were retrieved using the OpenReview API (https://docs.openreview.net/).We used ScienceBeam [12] to extract the title, abstract, figure and table captions, and the main text.For each conference, we applied our method to the submitted papers and maintained the original distribution of papers across decision categories.Experiment setup details are available in the Appendix B.  Scaling of average citation counts for accepted papers by GPT ranking system with increasing pairwise comparisons.We introduce two variants of our GPT ranking system: a basic version without area control, and one with area control, which ensures that the distribution of accepted papers across primary research areas matches that of human peer review.These two variants are compared against three baselines: (1) human acceptance decisions, (2) random acceptance based on the conference acceptance rate, and (3) GPT rating system (acceptance depends on the rating synthesized by a meta-reviewer from three independent GPT reviewers).As the number of comparisons increases, both variants steadily improve and approach human-level performance.At maximum scale, the GPT ranking system without area control achieves 20.00 average citations, while the version with area control reaches 18.97 average citations.These compare favorably to human acceptance decisions (19.36 average citations), and significantly outperform both random acceptance (8.56 average citations) and the GPT rating system baseline (11.41 average citations).</p>
<p>We investigate the scaling performance of our GPT ranking system with the number of agents, where each agent conducts a single pairwise comparison between randomly sampled papers from ICLR 2024. Figure 2 shows the overall academic impact (measured by their average citation counts) of the accepted papers as the number of agents increases.In particular, the system exhibits a steady increasing trend in average citations as the number of agents scales from around 10 3 to 10 5 .At insufficient sample sizes (e.g., 10 3 agents), the system performance approaches that of random acceptance.Since each paper receives too few comparisons, it fails to establish reliable relative rankings across the entire pool of 7158 papers.As the scale increases beyond 10 5 , the growth rate gradually slows down, with the average citation count eventually plateauing at approximately 20.</p>
<p>Given that the GPT ranking system demonstrates different acceptance patterns across research areas compared to humans (Section 4.5), we include two variants in our evaluation: one with area control, which maintains the same distribution of accepted papers across primary research areas as the human peer review system, and one without such controls.Nevertheless, they all yield comparable results to human-accepted papers at the convergence point.In the subsequent analysis, we focus on the GPT ranking system without area control as it better reflects the true properties of our system without deliberate controls.</p>
<p>We also compare our GPT ranking system with another baseline that follows a similar setup as prior works [27,18].This baseline, called the GPT rating system, involves three GPT reviewers independently generating reviews for each paper.Another agent then acts as a meta-reviewer to synthesize these individual reviews and provide a final rating (on a scale of 1 to 10).Acceptance decisions are made based on the rating provided by the meta-reviewer.However, it revealed a critical limitation: GPT-generated ratings exhibited less diversity, with most ratings concentrated around 6 and 7. Consequently, the system's ability to differentiate papers of varying quality was significantly constrained, leading to only minor improvements over the random baseline.</p>
<p>Discriminative Capability of GPT ranking system in Research Evaluation</p>
<p>With sufficient scale, our GPT ranking system approaches human-level performance in terms of average citation counts for accepted papers.However, beyond raw citation averages, a key question remains: does the GPT ranking system effectively distinguish between highly influential and less impactful papers in a manner comparable to human reviewers?To address this question, we analyzed the system's ability to discriminate papers across the entire citation distribution spectrum.</p>
<p>Across multiple top AI conferences (ICLR 2024, EMNLP 2023, ICLR 2023, and CoRL 2023) and under various decision conditions (accepted vs. rejected, main vs. findings track, oral vs. poster, etc.), papers ranked highly by our system consistently received more citations than those ranked lower (Figure 3, Supp Figures 8, 9).This pattern closely mirrors the citation advantages observed in human-selected papers across the same decision categories.The statistical significance of these differences (indicated by asterisks) further validates that our GPT ranking system can serve as reliable proxies for human peer review when identifying work likely to generate greater impact in the research community.The consistency of this pattern across different conferences and years suggests that as the system employs sufficient agents to conduct comprehensive pairwise comparisons, it could also capture the subtle quality signals that correlate with future scholarly influence.</p>
<p>Consistency Analysis between GPT Ranking System and Human Peer Review</p>
<p>We further examined the decision consistency between the GPT ranking system and conventional human peer review in ICLR 2024 and ICLR 2023.As shown in Table 1, each paper was independently categorized by human reviewers and by the GPT ranking system into one of four decision categories -Oral, Spotlight, Poster, or Reject.Papers withdrawn after review releases were considered as rejected for this analysis.</p>
<p>Notably, 41.0% of the papers accepted by human reviewers in ICLR 2024 were also accepted by the GPT ranking system (42.2% for papers accepted in ICLR 2023; Supp Table 4).The result aligns with findings from the NeurIPS 2021 consistency experiment [3], where 48.0% of papers accepted by the first committee were also accepted by the second committee.This level of agreement between AI and human reviewers is roughly on par with the consistency observed between independent human review committees, suggesting that the GPT ranking system could offer valuable insights for identifying Figure 3: Comparison of mean citation counts across multiple AI conferences under twofold decisions (e.g., accepted vs. rejected, main vs. findings track, oral vs. poster).The results consistently show that higher-tier papers (both GPT-selected and human-selected) receive substantially higher citation counts than lower-tier papers.This implies that our GPT ranking system can effectively distinguish more influential papers, performing comparably to human peer review system in identifying works likely to generate greater impact in the academic community.Error bars represent 95% confidence intervals.<em>P &lt; 0.05, </em><em>P &lt; 0.01, </em><strong>P &lt; 0.001, and </strong>**P &lt; 0.0001.potentially impactful papers that might otherwise be rejected due to the inherent randomness in the traditional peer review process.</p>
<p>Different Acceptance Patterns across Research Areas</p>
<p>We found notable disparities between the GPT ranking system and human peer review in their acceptance rates of several research areas (Figure 4, Supp Figure 10): GPT ranking system shows significantly higher acceptance rates for applied research areas such as robotics/autonomy applications (0.56 vs. 0.32) and societal considerations (0.51 vs. 0.30).In contrast, theoretical areas that received relatively high acceptance rates from human reviewers, such as learning theory (0.50 vs. 0.12) and optimization (0.31 vs. 0.12), had much lower acceptance rates from the GPT ranking system.</p>
<p>There are several ways to interpret these findings.First, LLMs are trained on massive web-crawled corpora rich in practical, application-oriented content.Consequently, these models may exhibit an inherent preference for studies that demonstrate immediate real-world applicability.Alternatively, large language models often struggle with complex mathematical reasoning [33], which could hinder their understanding of papers that require deep theoretical and mathematical rigor beyond memorized We sort areas by the GPT ranking system's acceptance rate from highest to lowest.The GPT ranking system exhibits noticeably higher acceptance rates in application-oriented fields compared to human reviewers, showing the most striking disparities in robotics (0.56 vs. 0.32) and societal considerations (0.51 vs. 0.30).In contrast, for more theoretical or methodologically focused areas, it assigns significantly lower acceptance rates than human reviewers.Learning theory demonstrates the largest gap, with acceptance rate at 0.12 versus humans at 0.50.The categorization is based on the 20 primary areas by ICLR' 24.</p>
<p>patterns.This limitation can result in a preference for research with more concrete implementations than abstract theoretical work.Future interdisciplinary research could explore these hypotheses.</p>
<p>Potential Bias against Novel Research Topics</p>
<p>We explored how our GPT ranking system influences the novelty of topics in selected papers.To measure the novelty of the topics, we embed each paper's abstract with OpenAI's text-embedding-3-small model, generating a vector representation for each abstract.We then compute the distance between each paper's vector and the closest neighbor within the abstracts from the same conference.A smaller distance indicates greater similarity between abstracts, thus lower novelty of the topic.</p>
<p>Through examination of papers categorized into top decision tiers by human reviewers and our GPT ranking system, our analysis revealed a consistent pattern (Figure 5): papers receiving higher rankings from the GPT system exhibited substantially smaller average distances to their nearest neighbors compared to papers selected by human reviewers.This might indicate that GPT exhibit a bias toward papers that cover topics similar to those in the existing literature, potentially undervaluing work that delves into novel research areas.</p>
<p>Impact on Academic Inequality</p>
<p>We also observed that our GPT ranking system tend to favor papers from established research institutions, potentially amplifying existing imbalances in the academic ecosystem.To quantify this imbalance, we track the first authors' affiliation distribution across papers presented in higher decision tiers.Using the Gini coefficient, a statistical measure commonly applied to income inequality, we quantified the degree of publication concentration across institutional affiliations.</p>
<p>We found a concerning pattern of institutional bias: the GPT ranking system consistently exhibited higher institutional inequality compared to humans, with significantly higher Gini coefficients across all conferences studied.The most significant disparity was observed in ICLR 2023 and ICLR 2024.</p>
<p>Gini Coefficient</p>
<p>Human Selected GPT Selected</p>
<p>Figure 6: Comparison of Gini coefficients for papers selected by humans versus GPT ranking system across conferences.Larger Gini coefficients represent greater inequality or imbalance in the distribution.The consistently higher Gini coefficients for GPT-selected papers (purple bars) compared to human-selected papers (green bars) indicate that GPT ranking system exhibit greater institutional concentration, potentially favoring papers from established research institutions.</p>
<p>Conclusion</p>
<p>Existing LLM-assisted peer review efforts largely replicate traditional workflows, framing evaluation as absolute scoring followed by aggregation [18,27,49,55,61].However, we argue that the scalability of LLMs open new opportunities to redesign scholarly evaluation structures rather than merely automate human processes.In this work, we introduce and explore a novel mechanism using LLM agents to evaluate academic papers through pairwise comparisons.Rather than assigning isolated scores, the system constructs a global ranking by aggregating local relative judgments between submissions.Through empirical experiments, we find that with sufficient scale, our system effectively identifies high-impact papers across multiple conferences, significantly exceeding traditional ratingbased methods.The overlap between papers accepted by our system and those accepted by human reviewers aligns with human-human agreement levels observed in previous studies, suggesting potential value as a complementary tool in the review process.</p>
<p>At the same time, our analysis reveals important challenges.The system exhibits area-specific preferences that diverge from human reviewers, shows a measurable decline in topic novelty among selected papers, and concentrates acceptances among a smaller set of prestigious institutions than human-selected papers.These patterns suggest that while scalable LLM-driven evaluation systems hold promise, careful design will be critical to ensure they promote diversity, equity, and innovation rather than reinforcing existing hierarchies.</p>
<p>A Discussion</p>
<p>A.1 Limitations</p>
<p>In this study, we rely on citation counts as a proxy for academic impact.Although citation metrics are widely used and provide a quantifiable measure of influence, they are affected by numerous factors beyond quality, such as the visibility of research communities, prevailing research trends, and established reputations [25,4,51,11].More critically, papers selected by human reviewers inherently receive more visibility through conference presentations and proceedings.The fact that our GPT ranking system identified papers with comparable citation impact despite this disadvantage demonstrates a certain level of effectiveness in our approach.Nevertheless, this circularity problem makes it difficult to establish a truly independent measure of quality.To further verify the robustness of our proposed method, we conducted experiments using alternative measures.Specifically, we measured the Spearman correlation between our system's output and human review scores.In both ICLR 2023 and ICLR 2024, the BT scores produced by our mechanism show moderate correlation with the average human ratings-27% and 24%, respectively.This indicates that our system aligns with signals in the current review process, while not fully replicating its outcomes.We believe this complementary nature is essential for identifying potential improvements to existing peer review systems.</p>
<p>Furthermore, while we exclusively used GPT-4o mini for our large-scale experiments due to its balance of performance and computational cost, we acknowledge that there are other diverse LLMs capable of academic assessment, and they might yield substantially different results.A more comprehensive evaluation across multiple models would provide better insight into the generalizability of our method and findings.In Appendix C.4, we tested two additional models beyond GPT-4o mini: Gemini 2.0 Flash and Claude-3-Haiku-20240307.We scaled the number of comparisons to over 10 6 and found that, in both cases, our pairwise comparison framework performs significantly better than the GPT rating system baseline.Specifically, papers selected by Gemini 2.0 Flash receive an average of 18.3 citations (vs.11.4), while those chosen by Claude-3-Haiku average 16.8 (vs.11.4).These results suggest that the advantage of our approach is robust across different LLMs.</p>
<p>Finally, while our current work constructs a single global ranking, future extensions could explore personalized or multi-objective evaluation systems that explicitly account for epistemic diversity and evolving community goals.</p>
<p>A.2 Expanding Peer Review through Pairwise Evaluation</p>
<p>Beyond the immediate results, our framework and explorations open broader directions for redesigning scholarly review systems.</p>
<p>First, because pairwise comparisons produce local relative judgments that can be incrementally aggregated, our approach naturally lends itself to continuous evaluation.Rather than operating within a fixed submission and decision timeline, conferences could maintain an ongoing review pipeline, where new papers are progressively compared against the existing submission pool.Such a dynamic process could allow for rolling acceptances, faster feedback loops, and better accommodation of late-breaking research.</p>
<p>Second, pairwise evaluation offers distinct advantages for emerging or interdisciplinary fields where traditional scoring rubrics are poorly defined or difficult to establish.Absolute scoring requires consensus on evaluation criteria and careful calibration across reviewers, which can be challenging in fast-moving or nascent research areas.Comparative judgments, by focusing on relative rather than absolute assessments, can surface high-potential work even when shared evaluation norms are still evolving, making the system more adaptable to domains where innovation resists rigid checklist-based quantification.</p>
<p>Third, the flexibility of the aggregation process suggests opportunities for personalized and diversified peer review.Different weighting schemes could prioritize novelty, interdisciplinarity, methodological rigor, or other dimensions based on the goals of specific conferences, tracks, or even reviewer communities.In principle, such mechanisms could allow peer review to better reflect the heterogeneous values of different research communities, moving beyond the one-size-fits-all model [15] currently dominant in scientific publishing.</p>
<p>Finally, an important direction for future exploration lies in integrating human expertise and LLM evaluations [41,29].Rather than viewing LLM-based and human-based assessments as competing alternatives, hybrid models could combine the strengths of both: leveraging LLMs for large-scale, consistent pairwise comparisons while relying on human reviewers to provide deeper qualitative insights, assess boundary cases, and adjudicate particularly novel or interdisciplinary submissions.Designing effective protocols for human-AI collaboration in peer review could further enhance both the scalability and the fairness of the evaluation process.</p>
<p>Together, these directions highlight how reframing peer review around relative comparisons, enriched by human-AI collaboration, could support a more scalable, inclusive, and adaptive scholarly communication ecosystem.</p>
<p>B Experimental Details B.1 Dataset Details</p>
<p>Here we include additional details on the datasets used for our experiments.</p>
<p>B.2 Implementation Details</p>
<p>We use the official OpenAI's Batch API for GPT-4o mini and set temperature as 0 during pairwise comparison.The number of API calls and estimated cost for each simulated conference is as follows:  Your JSON output should look like this : {{ " paper_1_review ": " Your meta -review and reasoning for paper 1" , " paper_2_review ": " Your meta -review and reasoning for paper 2" , " chosen_paper ": " paper_1 or paper_2 " }} """  decisions (e.g., accepted vs. rejected, main vs. findings track, oral vs. poster).The results consistently show that higher-tier papers (both GPT-selected and human-selected) receive overall higher influence than lower-tier papers.</p>
<p>C Additional Results</p>
<p>C.1 Discriminative Capability of GPT ranking system in Research Evaluation</p>
<p>C.2 Consistency Analysis between GPT Ranking System and Human Peer Review</p>
<p>Table 4: Summary of recommendations for ICLR 2023 papers by two review systems: the human peer review (Human) and the GPT ranking system (GPT).Each row represents humans' decision, while each column shows how the GPT ranking system categorized the same papers.with increasing pairwise comparisons.We observe a similar temporal scaling pattern as in the GPT ranking system, with citation counts increasing as the number of pairwise comparisons grows.Scaling of average citation counts for accepted papers by Gemini ranking system with increasing pairwise comparisons.We observe a similar temporal scaling pattern as in the GPT ranking system, with citation counts increasing as the number of pairwise comparisons grows.</p>
<p>Human\GPT</p>
<ol>
<li>2
2
Agent Scaling Boosts the Performance of GPT Ranking System</li>
</ol>
<p>Figure 2 :
2
Figure2: Scaling of average citation counts for accepted papers by GPT ranking system with increasing pairwise comparisons.We introduce two variants of our GPT ranking system: a basic version without area control, and one with area control, which ensures that the distribution of accepted papers across primary research areas matches that of human peer review.These two variants are compared against three baselines: (1) human acceptance decisions, (2) random acceptance based on the conference acceptance rate, and (3) GPT rating system (acceptance depends on the rating synthesized by a meta-reviewer from three independent GPT reviewers).As the number of comparisons increases, both variants steadily improve and approach human-level performance.At maximum scale, the GPT ranking system without area control achieves 20.00 average citations, while the version with area control reaches 18.97 average citations.These compare favorably to human acceptance decisions (19.36 average citations), and significantly outperform both random acceptance (8.56 average citations) and the GPT rating system baseline (11.41 average citations).</p>
<p>Figure 4 :
4
Figure4: Comparative acceptance rates of ICLR' 24 papers by humans and GPT ranking system across research areas.We sort areas by the GPT ranking system's acceptance rate from highest to lowest.The GPT ranking system exhibits noticeably higher acceptance rates in application-oriented fields compared to human reviewers, showing the most striking disparities in robotics (0.56 vs. 0.32) and societal considerations (0.51 vs. 0.30).In contrast, for more theoretical or methodologically focused areas, it assigns significantly lower acceptance rates than human reviewers.Learning theory demonstrates the largest gap, with acceptance rate at 0.12 versus humans at 0.50.The categorization is based on the 20 primary areas by ICLR' 24.</p>
<p>Figures Captions : { f i g u r e _ a n d_ t a b l e _ c a p t io n s } Main : { main_content } '''</p>
<p>Figure 7 :
7
Figure 7: Example prompt for pairwise comparison.</p>
<p>Figure 8 :Figure 9 :
89
Figure8: Comparison of mean citation counts across multiple AI conferences under fine-grained decisions in accepted papers.The results consistently show that higher-tier papers (both GPTselected and human-selected) receive overall higher influence than lower-tier papers.Error bars represent 95% confidence intervals.<em>P &lt; 0.05, </em><em>P &lt; 0.01, </em><strong>P &lt; 0.001, and </strong>**P &lt; 0.0001.</p>
<p>Figure 11 :
11
Figure11: Scaling of average citation counts for accepted papers by Claude ranking system with increasing pairwise comparisons.We observe a similar temporal scaling pattern as in the GPT ranking system, with citation counts increasing as the number of pairwise comparisons grows.</p>
<p>Figure 12 :
12
Figure12: Scaling of average citation counts for accepted papers by Gemini ranking system with increasing pairwise comparisons.We observe a similar temporal scaling pattern as in the GPT ranking system, with citation counts increasing as the number of pairwise comparisons grows.</p>
<p>Table 1 :
1
Summary of recommendations for ICLR 2024 papers by two review systems: the human peer review (Human) and the GPT ranking system (GPT).Each row represents humans' decision, while each column shows how the GPT ranking system categorized the same papers.
Human\GPT Oral Spotlight Poster RejectOral6112841Spotlight1032130191Poster291165561,085Reject412041,0723,606</p>
<p>Nearest neighbor distances of papers selected for top-tier acceptance by GPT ranking system versus human reviewers.Higher distances indicate greater novelty.Human-selected papers (blue) consistently show higher nearest neighbor distances than GPT-selected papers (orange) across all conferences.The differences are statistically significant (p &lt; 0.05) for ICLR 2023 Notable-top-5%, ICLR 2024 Oral, and EMNLP 2023 Main Track.For CoRL 2023 Oral and NeurIPS 2023 Oral, we did not observe statistically significant differences, which may be due to smaller sample sizes.In ICLR 2023, 43.8% of GPT ranking system's top-tier papers came from 10 institutions compared to only 27.0% in human decisions, while in ICLR 2024, the gap persisted with 37.2% of GPT ranking system's top selections coming from 10 institutions versus 26.7% in human evaluations.
1.0GPT Selected Human Selected0.7 0.8 0.9 Nearest Neighbor Distance0.60.5ICLR2023 Notable-top-5%ICLR2024 OralEMNLP2023 Main TrackCoRL2023 OralNeurIPS2023 OralNotable-top-5% Figure 5: ICLR2023 0.1 0.2 0.3 0.4 0.5 0.6 0.7ICLR2024 OralEMNLP2023 Main TrackCoRL2023 OralNeurIPS2023 Oral</p>
<p>Table 2 :
2
Academic paper data from major ML conferences.
ConferenceDecision Types# of PapersICLR 2023• Notable-top-5%89• Notable-top-25%281• Poster1,193• Reject (Withdrawn submissions included)3,303ICLR 2024• Accept (Oral)86• Accept (Spotlight)363• Accept (Poster)1,786• Reject (Withdrawn submissions included)4,923NeurIPS 2023 • Accept (Oral)67• Accept (Spotlight)374• Accept (Poster)2,748EMNLP 2023 • Accept-Main975• Accept-Findings993CoRL 2023• Accept (Oral)32• Accept (Poster)166</p>
<p>Table 3 :
3
API usage and estimated costs for pairwise comparison across each conference.EMNLP 2023 and CoRL 2023 used all available pairs, while others were randomly downsampled to 3M pairs.Please act as an impartial judge and evaluate the quality of the following two papers .As the area chair for a top ML conference , you can only select one paper .Start with a brief meta -review / reasoning of the pros and cons for each paper ( two sentences ) , and then provide your choice in a binary format .Start with a brief meta -review / reasoning of the pros and cons for each paper , focusing on novelty , significance , clarity , methodology , and practical implications .Be very critical and do not be biased by what the author claimed .Finally , provide your choice in a binary format .
Conference# of API Calls Estimated Cost (USD)ICLR 20233,000,000∼ 1, 350ICLR 20243,000,000∼ 1, 350NeurIPS 20233,000,000∼ 1, 350EMNLP 20233,871,056∼ 1, 700CoRL 202339,006∼ 17</p>
<p>Figure 10: Comparative acceptance rates of ICLR' 23 papers by humans and GPT ranking Systems across research areas.We sort areas by the GPT ranking system's acceptance rate from highest to lowest.The GPT ranking system exhibits noticeably higher acceptance rates in applicationoriented fields compared to human reviewers, showing the most striking disparities in robotics (0.53 vs. 0.36) and societal considerations (0.51 vs. 0.25).In contrast, for more theoretical or methodologically focused areas, it assigns significantly lower acceptance rates than human reviewers.Learning theory demonstrates the largest gap, with acceptance rate at 0.10 versus humans at 0.43.
C.4 Using other LLMs as agentsNotable top 5% Notable top 25% Poster Reject 10 9 26 44 11 27 95 148 29 93 360 711 39 152 712 2,400 C.3 Different Acceptance Patterns across Research Areas Notable top 5% Notable top 25% Poster Reject a b 10 3 10 4 10 5 Number of Agents 5 10 15 20 25 Average Number of Citations for accepted papers Claude Ranking System without Area Control Claude Ranking System with Area Control Human Accept Baseline Randomly Accept Baseline GPT Rating System Baseline10 6cd
Theorem 4 in[34] shows that if m &gt; 12n log n pairs are sampled uniformly at random, then with high probability the estimate θ satisfies ∥ θ − θ * ∥ = O n log n m .</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>Reviewing peer review. B Alberts, B Hanson, K L Kelner, 2008</p>
<p>Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment. A Beygelzimer, Y N Dauphin, P Liang, J W Vaughan, arXiv:2306.032622023arXiv preprint</p>
<p>What do citation counts measure? a review of studies on citing behavior. L Bornmann, H.-D Daniel, Journal of documentation. 6412008</p>
<p>Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. L Bornmann, R Mutz, Journal of the association for information science and technology. 66112015</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. R A Bradley, M E Terry, Biometrika. 393/41952</p>
<p>Reviewer fatigue? why scholars decline to review their peers' work. M Breuning, J Backstrom, J Brannon, B I Gross, M Widmeier, PS: Political Science &amp; Politics. 4842015</p>
<p>Here or there: Preference judgments for relevance. B Carterette, P N Bennett, D M Chickering, S T Dumais, European Conference on Information Retrieval. Springer2008</p>
<p>Marg: Multi-agent review generation for scientific papers. M D'arcy, T Hope, L Birnbaum, D Downey, arXiv:2401.042592024arXiv preprint</p>
<p>The international development of open access publishing: A comparative empirical analysis over seven world regions and nine academic disciplines. M Demeter, A Jele, Z B Major, Publishing Research Quarterly. 3732021</p>
<p>Publish (in english) or perish: The effect on citation rate of using languages other than english in scientific publications. M S Di Bitetti, J A Ferreras, Ambio. 462017</p>
<p>Sciencebeam -using computer vision to extract pdf data. D Ecer, G Maciocci, Elife Blog Post. 2017. 26 March 2025sciencebeam-using-computer-vision-to-extract-pdf-data</p>
<p>Recruitment of reviewers is becoming harder at some journals: a test of the influence of reviewer fatigue at six journals in ecology and evolution. C W Fox, A Y Albert, T H Vines, Research Integrity and Peer Review. 22017</p>
<p>Double-blind peer review affects reviewer ratings and editor decisions at an ecology journal. C W Fox, J Meyer, E Aimé, Functional Ecology. 3752023</p>
<p>Jury learning: Integrating dissenting voices into machine learning models. M L Gordon, M S Lam, J S Park, K Patel, J Hancock, T Hashimoto, M S Bernstein, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>The llama 3 herd of models. A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. M Hosseini, S P Horbach, Research integrity and peer review. 8142023</p>
<p>Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, arXiv:2406.12708Agentreview: Exploring peer review dynamics with llm agents. 2024arXiv preprint</p>
<p>How professors think: Inside the curious world of academic judgment. M Lamont, 2009Harvard University Press</p>
<p>Bias in peer review. C J Lee, C R Sugimoto, G Zhang, B Cronin, Journal of the American Society for information Science and Technology. 6412013</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. W Liang, Z Izzo, Y Zhang, H Lepp, H Cao, X Zhao, L Chen, H Ye, S Liu, Z Huang, International Conference on Machine Learning. PMLR2024</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. W Liang, Y Zhang, H Cao, B Wang, D Y Ding, X Yang, K Vodrahalli, S He, D S Smith, Y Yin, D A Mcfarland, J Zou, 10.1056/AIoa2400196NEJM AI. 18AIoa2400196, 2024</p>
<p>Moprd: A multidisciplinary open peer review dataset. J Lin, J Song, Z Zhou, Y Chen, X Shi, 10.1007/s00521-023-08891-5Neural Comput. Appl. 0941-06433534Sept. 2023</p>
<p>Automated scholarly paper review: Concepts, technologies, and challenges. Information Fusion. J Lin, J Song, Z Zhou, Y Chen, X Shi, 10.1016/j.inffus.2023.101830.URLhttps://www.sciencedirect.com/science/article/pii/S156625352300146X202398101830</p>
<p>Using citation counts as a measure of quality in science measuring what's measurable rather than what's valid. D Lindsey, Scientometrics. 151989</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. R Liu, N B Shah, 2023</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Calibrating "cheap signals" in peer review without a prior. Y Lu, Y Kong, Advances in Neural Information Processing Systems. 202336</p>
<p>Towards human-ai deliberation: Design and evaluation of llm-empowered deliberative ai for ai-assisted decisionmaking. S Ma, Q Chen, X Wang, C Zheng, Z Peng, M Yin, X Ma, arXiv:2403.168122024arXiv preprint</p>
<p>Is peer review broken? submissions are up, reviewers are overtaxed, and authors are lodging complaint after complaint about the process at top-tier journals. what's wrong with peer review?. A Mccook, The scientist. 2022006</p>
<p>Rethinking Reviewer Fatigue. EON. B Mehmani, A , nov 15 2024</p>
<p>Peer review as an evolving response to organizational constraint: Evidence from sociology journals. B Merriman, The American Sociologist. 522021</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. I Mirzadeh, K Alizadeh, H Shahrokhi, O Tuzel, S Bengio, M Farajtabar, 2024</p>
<p>Rank centrality: Ranking from pairwise comparisons. S Negahban, S Oh, D Shah, 10.1287/opre.2016.1534Oper. Res. 0030-364X651Feb. 2017</p>
<p>Unveiling the sentinels: Assessing ai performance in cybersecurity peer review. L Niu, N Xue, C Pöpper, arXiv:2309.054572023arXiv preprint</p>
<p>Gpt-4o mini: advancing cost-efficient intelligence. 2024OpenAI</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Gpt4 is slightly helpful for peer-review assistance: A pilot study. Z Robertson, arXiv:2307.054922023arXiv preprint</p>
<p>The emergence of large language models (llm) as a tool in literature reviews: an llm automated systematic review. D Scherbakov, N Hubig, V Jansari, A Bakumenko, L A Lenert, arXiv:2409.046002024arXiv preprint</p>
<p>Challenges, experiments, and computational solutions in peer review. N B Shah, 10.1145/3528086Commun. ACM. 0001-0782656May 2022</p>
<p>Collaborative gym: A framework for enabling and evaluating human-agent collaboration. Y Shao, V Samuel, Y Jiang, J Yang, D Yang, arXiv:2412.157012024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>C Si, D Yang, T Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>Peer review and gender bias: A study on 145 scholarly journals. F Squazzoni, G Bravo, M Farjam, A Marusic, B Mehmani, M Willis, A Birukou, P Dondio, F Grimaldo, 10.1126/sciadv.abd0299Science Advances. 722021</p>
<p>Auctions and peer prediction for academic peer review. S Srinivasan, J Morgenstern, 2023</p>
<p>Prior and prejudice: The novice reviewers' bias against resubmissions in conference peer review. I Stelmakh, N B Shah, A Singh, H Daumé, Proceedings of the ACM on Human-Computer Interaction. 5CSCW12021</p>
<p>Cite-seeing and reviewing: A study on citation bias in peer review. I Stelmakh, C Rastogi, R Liu, S Chawla, F Echenique, N B Shah, 10.1371/journal.pone.0283980PLOS ONE. 1932-6203187e0283980July 2023</p>
<p>You are the best reviewer of your own papers: an owner-assisted scoring mechanism. W J Su, Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS '21. the 35th International Conference on Neural Information Processing Systems, NIPS '21Red Hook, NY, USACurran Associates Inc2021ISBN 9781713845393</p>
<p>Peer review as a multi-turn and long-context dialogue with role-based interactions. C Tan, D Lyu, S Li, Z Gao, J Wei, S Ma, Z Liu, S Z Li, arXiv:2406.056882024arXiv preprint</p>
<p>Chatgpt identifies gender disparities in scientific peer review. J P Verharen, 2023Elife12P90230</p>
<p>Unpacking the matthew effect in citations. J Wang, Journal of Informetrics. 822014</p>
<p>Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. J Wang, N B Shah, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '19. the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '19Richland, SC2019International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450363099</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022ISBN 9781713871088</p>
<p>Y Weng, M Zhu, G Bao, H Zhang, J Wang, Y Zhang, L Yang, arXiv:2411.00816Cycleresearcher: Improving automated research via automated review. 2024arXiv preprint</p>
<p>An isotonic mechanism for overlapping ownership. J Wu, H Xu, Y Guo, W Su, 2025</p>
<p>Isotonic mechanism for exponential family estimation in machine learning peer review. Y Yan, W J Su, J Fan, Journal of the Royal Statistical Society Series B: Statistical Methodology. 252025</p>
<p>Are we there yet? revealing the risks of utilizing large language models in scholarly peer review. R Ye, X Pang, J Chai, J Chen, Z Yin, Z Xiang, X Dong, J Shao, S Chen, 2024</p>
<p>Investigating fairness disparities in peer review: A language model enhanced approach. J Zhang, H Zhang, Z Deng, D Roth, arXiv:2211.063982022arXiv preprint</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. R Zhou, L Chen, K Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). N Calzolari, M.-Y Kan, V Hoste, A Lenci, S Sakti, N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCLMay 2024</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. M Zhu, Y Weng, L Yang, Y Zhang, arXiv:2503.085692025arXiv preprint</p>
<p>Large language models for automated scholarly paper review: A survey. Z Zhuang, J Chen, H Xu, Y Jiang, J Lin, 10.1016/j.inffus.2025.103332.URLhttps://www.sciencedirect.com/science/article/pii/S1566253525004051Information Fusion. 1566-25351241033322025</p>
<p>Patterns of evaluation in science: Institutionalisation, structure and functions of the referee system. H Zuckerman, R K Merton, 1971Minerva</p>            </div>
        </div>

    </div>
</body>
</html>
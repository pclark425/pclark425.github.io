<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4906 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4906</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4906</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-c6ee979c2da4b55a8486abae4cd720422ab09b26</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c6ee979c2da4b55a8486abae4cd720422ab09b26" target="_blank">When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases, and a new method is devised that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.</p>
                <p><strong>Paper Abstract:</strong> Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4906.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4906.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 davinci-003 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large decoder-only LM used as a parametric baseline and as a backbone for retrieval-augmented and GenRead experiments on entity-centric open-domain QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-3 davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A large decoder-only language model evaluated in zero- and few-shot prompting for open-domain QA; used both unaugmented (parametric) and augmented with retrieved or generated context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric (in-weights) / retrieval-augmented when applied</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>By default relies on knowledge encoded in model parameters; when augmented, the top-1 retrieved Wikipedia paragraph (from BM25 or Contriever) or a GenRead-generated passage is concatenated to the question as context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer factual, entity-centric questions (triples S,R,O verbalized) without ground-truth paragraph; evaluated in zero/few-shot on PopQA and EntityQuestions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PopQA, EntityQuestions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Contriever-augmented: reported +7 percentage points over vanilla davinci-003 on PopQA (i.e., ~35% -> ~42%); adaptive combination (GenRead + Contriever, adaptive selection) yields 46.5% accuracy on PopQA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla (parametric-only) davinci-003: 35% accuracy on PopQA (zero-shot baseline reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Retrieval (Contriever/BM25) substantially improves accuracy on less-popular (long-tail) entities; Contriever yields ~+7% for GPT-3 overall; GenRead can give sizeable gains for GPT-3 as well. For popular entities, parametric (vanilla) performance is competitive and retrieved context can sometimes harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieved context can be misleading (10% of questions where retrieval caused a previously-correct vanilla answer to become incorrect; those cases had low recall@1 ≈ 0.14). GenRead is costly and relies on parametric knowledge so performs poorly on long-tail entities for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Parametric memory in GPT-3 is reliable for many popular facts; augmenting with high-quality retrieval helps long-tail facts and yields the biggest gains for smaller/backbone LMs, but retrieval must be used selectively to avoid harming answers for already-memorized popular facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4906.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4906.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contriever-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMs augmented with Contriever dense retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-parametric retrieval augmentation using the Contriever dense retriever to fetch a top-1 Wikipedia paragraph and supply it as context to LMs for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised dense information retrieval with contrastive learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Contriever-augmented LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A language model whose input is concatenated with the top-1 passage returned by Contriever (a pretrained dense retriever); evaluated across multiple LM backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric retrieval-augmented memory (dense retriever + document store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieves the top-1 paragraph from a December 2018 Wikipedia dump using Contriever and concatenates it to the question before generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Entity-centric factual QA on PopQA and EntityQuestions to test memorization and effectiveness of retrieved external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PopQA, EntityQuestions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Varies by backbone. Reported examples: Contriever gives +7 percentage points on GPT-3 davinci-003 (PopQA). Contriever-augmented GPT-Neo 2.7B outperforms vanilla GPT-3 on the 4,000 least-popular PopQA questions (no single absolute percent provided for that comparison in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Corresponding vanilla LM baselines (e.g., GPT-3 davinci-003 35% on PopQA; smaller GPT-Neo variants substantially lower).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Contriever generally provides large gains, especially for less-popular entities and smaller models (can allow a small model to match/exceed a larger vanilla model). Contriever often outperforms BM25 overall on PopQA, but BM25 can be better on the very least-popular items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval errors (low recall@1) can mislead the LM; for ~10% of questions retrieval caused a correct vanilla answer to become incorrect (these cases have low recall@1 ≈ 0.14). Effectiveness depends on retrieval quality and entity disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Dense retrieval is an effective non-parametric memory for long-tail factual QA; retriever choice matters by popularity regime and retrieval quality critically affects improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4906.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4906.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMs augmented with BM25 lexical retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lexical (term-based) retrieval augmentation: top-1 paragraph from BM25 is concatenated to LM input as a non-parametric memory source.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The probabilistic relevance framework: BM25 and beyond</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BM25-augmented LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LM that receives as input the top-1 paragraph returned by BM25 on a Wikipedia dump concatenated with the question.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric retrieval (sparse lexical)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieve top-1 paragraph by BM25 from a Wikipedia dump and append to the prompt as evidence/context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Entity-centric factual QA (PopQA, EntityQuestions) to measure effect of lexical retrieval on memorization and long-tail knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PopQA, EntityQuestions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BM25-based augmentation improves accuracy; on PopQA BM25 performs better than Contriever specifically on the very least-popular entities (no single global number provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla LM baselines as reported (e.g., GPT-3 davinci-003 35% on PopQA).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>BM25 yields clear improvements versus vanilla LMs; overall Contriever tends to outperform BM25 on PopQA, but BM25 shows relative advantage on extreme tail items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lexical matching may fail when relevant context is paraphrased; can still retrieve wrong-entity pages and thus mislead LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Even simple lexical retrievers are competitive on extreme long-tail examples; practitioner should consider retriever choice conditioned on the popularity regime.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4906.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4906.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenRead</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenRead (generate rather than retrieve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parametric augmentation technique that elicits LMs to generate supporting context (a synthetic document) to be used as input context for answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generate rather than retrieve: Large language models are strong context generators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GenRead</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A method that prompts LMs to generate a contextual passage (rather than retrieve) and uses that generated passage as evidence concatenated with the question to answer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric generated context (elicited/context generation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>LM generates a supporting passage using a specialized prompt; the generated text is concatenated to the prompt and used by the LM to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Entity-centric factual QA on PopQA and EntityQuestions to evaluate whether generated context can substitute for retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PopQA, EntityQuestions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GenRead generally outperforms vanilla LMs, especially for large LMs (GPT-3 davinci-003 sees sizeable gains); little-to-no improvement for smaller models; GenRead still has low performance on less-popular entities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla LM baselines (e.g., GPT-3 davinci-003 35% on PopQA).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>GenRead can improve large LMs by surfacing parametric knowledge into context; it does not replace retrieval for long-tail facts and is less effective for small models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High inference cost/latency for generating context (example: GPT-NeoX-20B GenRead run took ~70 seconds per query); generated passages can still be incorrect and do not solve long-tail memorization gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Eliciting LMs to generate context is a viable alternative to retrieval for large models, but is costly and limited on tail facts because it relies on parametric memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4906.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4906.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Retrieval (popularity-thresholded retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method introduced in this paper that decides per-query whether to retrieve external context based on subject-entity popularity and relationship-type thresholds, using retrieval only when necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Method that uses a per-relationship popularity threshold (based on Wikipedia pageviews) to decide whether to augment a query with retrieved (BM25/Contriever) or generated (GenRead) context; otherwise uses parametric-only prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid conditional retrieval (mixing parametric and non-parametric memories)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Compute subject popularity from Wikipedia pageviews; if popularity is below a tuned per-relation threshold, retrieve top-1 passage (or use GenRead) and concatenate it to the prompt; otherwise query the LM without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Entity-centric QA on PopQA and EntityQuestions; objective is to combine parametric and non-parametric memory to maximize accuracy while reducing retrieval cost/latency.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PopQA, EntityQuestions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>For GPT-3 davinci-003 paired with GenRead and Contriever, Adaptive Retrieval achieves 46.5% accuracy on PopQA (≈+5.3 percentage points above the best non-adaptive method reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla GPT-3 35% on PopQA; non-adaptive always-retrieve methods performed worse than the adaptive best.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Adaptive Retrieval outperforms both always-retrieve and always-parametric baselines, especially for large LMs; it reduces amount of retrieval (example: GPT-3 retrieves for ~40% of questions with BM25) and provides accuracy and efficiency benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on Wikipedia pageviews as a proxy for memorization (time-dependent and dataset-specific); per-relationship thresholds are tuned on a dev set and may not generalize; gains are smaller for small LMs that require retrieval on most queries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Selective retrieval based on an input-only signal (entity popularity and relation type) can deliver better accuracy-cost tradeoffs: use retrieval only when the model is unlikely to have memorized the fact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4906.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4906.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Neo 2.7B + Contriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Neo 2.7B augmented with Contriever dense retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medium-sized LM whose retrieval-augmentation with Contriever enables it to outperform a much larger vanilla model on long-tail factual QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-Neo 2.7B (Contriever-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-Neo 2.7B (decoder-only LM) whose input is augmented with the top-1 Contriever passage; evaluated on PopQA long-tail questions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric retrieval-augmented (dense retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Top-1 Contriever-retrieved Wikipedia paragraph concatenated to the question; used to supply external factual memory for answering.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering (long-tail)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer PopQA questions about less-popular entities (long tail). The experiment tests whether retrieval can compensate for smaller parametric capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PopQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Contriever-augmented GPT-Neo 2.7B outperforms vanilla GPT-3 davinci-003 on the 4,000 least-popular PopQA questions (text reports superiority but does not list the exact absolute accuracy for this specific combo).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla GPT-Neo 2.7B lower (exact baseline not reported in text for 2.7B specifically). Example baselines for least-popular: GPT-Neo 6B 15%, GPT-NeoX/20B 16%, GPT-3 davinci-003 19% for the 4,000 least popular questions (vanilla).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Retrieval augmentation enables smaller models to match or exceed larger vanilla models on long-tail facts; demonstrates complementary nature of non-parametric memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No exact per-model absolute numbers provided in text for this specific pairing; performance depends on retriever quality and the nature of the long-tail facts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Using retrieval to supplement medium-sized models is an efficient way to address long-tail memorization gaps without scaling model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised dense information retrieval with contrastive learning <em>(Rating: 2)</em></li>
                <li>Generate rather than retrieve: Large language models are strong context generators <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>The probabilistic relevance framework: BM25 and beyond <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4906",
    "paper_id": "paper-c6ee979c2da4b55a8486abae4cd720422ab09b26",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "GPT-3 davinci-003",
            "name_full": "GPT-3 davinci-003 (OpenAI)",
            "brief_description": "Large decoder-only LM used as a parametric baseline and as a backbone for retrieval-augmented and GenRead experiments on entity-centric open-domain QA.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "agent_name": "GPT-3 davinci-003",
            "agent_description": "A large decoder-only language model evaluated in zero- and few-shot prompting for open-domain QA; used both unaugmented (parametric) and augmented with retrieved or generated context.",
            "memory_type": "parametric (in-weights) / retrieval-augmented when applied",
            "memory_description": "By default relies on knowledge encoded in model parameters; when augmented, the top-1 retrieved Wikipedia paragraph (from BM25 or Contriever) or a GenRead-generated passage is concatenated to the question as context.",
            "task_name": "Open-domain question answering",
            "task_description": "Answer factual, entity-centric questions (triples S,R,O verbalized) without ground-truth paragraph; evaluated in zero/few-shot on PopQA and EntityQuestions.",
            "benchmark_name": "PopQA, EntityQuestions",
            "performance_with_memory": "Contriever-augmented: reported +7 percentage points over vanilla davinci-003 on PopQA (i.e., ~35% -&gt; ~42%); adaptive combination (GenRead + Contriever, adaptive selection) yields 46.5% accuracy on PopQA.",
            "performance_without_memory": "Vanilla (parametric-only) davinci-003: 35% accuracy on PopQA (zero-shot baseline reported).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Retrieval (Contriever/BM25) substantially improves accuracy on less-popular (long-tail) entities; Contriever yields ~+7% for GPT-3 overall; GenRead can give sizeable gains for GPT-3 as well. For popular entities, parametric (vanilla) performance is competitive and retrieved context can sometimes harm performance.",
            "limitations_or_challenges": "Retrieved context can be misleading (10% of questions where retrieval caused a previously-correct vanilla answer to become incorrect; those cases had low recall@1 ≈ 0.14). GenRead is costly and relies on parametric knowledge so performs poorly on long-tail entities for smaller models.",
            "key_insights": "Parametric memory in GPT-3 is reliable for many popular facts; augmenting with high-quality retrieval helps long-tail facts and yields the biggest gains for smaller/backbone LMs, but retrieval must be used selectively to avoid harming answers for already-memorized popular facts.",
            "uuid": "e4906.0",
            "source_info": {
                "paper_title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Contriever-augmented",
            "name_full": "LMs augmented with Contriever dense retriever",
            "brief_description": "Non-parametric retrieval augmentation using the Contriever dense retriever to fetch a top-1 Wikipedia paragraph and supply it as context to LMs for QA.",
            "citation_title": "Unsupervised dense information retrieval with contrastive learning",
            "mention_or_use": "use",
            "agent_name": "Contriever-augmented LM",
            "agent_description": "A language model whose input is concatenated with the top-1 passage returned by Contriever (a pretrained dense retriever); evaluated across multiple LM backbones.",
            "memory_type": "non-parametric retrieval-augmented memory (dense retriever + document store)",
            "memory_description": "Retrieves the top-1 paragraph from a December 2018 Wikipedia dump using Contriever and concatenates it to the question before generation.",
            "task_name": "Open-domain question answering",
            "task_description": "Entity-centric factual QA on PopQA and EntityQuestions to test memorization and effectiveness of retrieved external knowledge.",
            "benchmark_name": "PopQA, EntityQuestions",
            "performance_with_memory": "Varies by backbone. Reported examples: Contriever gives +7 percentage points on GPT-3 davinci-003 (PopQA). Contriever-augmented GPT-Neo 2.7B outperforms vanilla GPT-3 on the 4,000 least-popular PopQA questions (no single absolute percent provided for that comparison in text).",
            "performance_without_memory": "Corresponding vanilla LM baselines (e.g., GPT-3 davinci-003 35% on PopQA; smaller GPT-Neo variants substantially lower).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Contriever generally provides large gains, especially for less-popular entities and smaller models (can allow a small model to match/exceed a larger vanilla model). Contriever often outperforms BM25 overall on PopQA, but BM25 can be better on the very least-popular items.",
            "limitations_or_challenges": "Retrieval errors (low recall@1) can mislead the LM; for ~10% of questions retrieval caused a correct vanilla answer to become incorrect (these cases have low recall@1 ≈ 0.14). Effectiveness depends on retrieval quality and entity disambiguation.",
            "key_insights": "Dense retrieval is an effective non-parametric memory for long-tail factual QA; retriever choice matters by popularity regime and retrieval quality critically affects improvements.",
            "uuid": "e4906.1",
            "source_info": {
                "paper_title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "BM25-augmented",
            "name_full": "LMs augmented with BM25 lexical retriever",
            "brief_description": "Lexical (term-based) retrieval augmentation: top-1 paragraph from BM25 is concatenated to LM input as a non-parametric memory source.",
            "citation_title": "The probabilistic relevance framework: BM25 and beyond",
            "mention_or_use": "use",
            "agent_name": "BM25-augmented LM",
            "agent_description": "An LM that receives as input the top-1 paragraph returned by BM25 on a Wikipedia dump concatenated with the question.",
            "memory_type": "non-parametric retrieval (sparse lexical)",
            "memory_description": "Retrieve top-1 paragraph by BM25 from a Wikipedia dump and append to the prompt as evidence/context.",
            "task_name": "Open-domain question answering",
            "task_description": "Entity-centric factual QA (PopQA, EntityQuestions) to measure effect of lexical retrieval on memorization and long-tail knowledge.",
            "benchmark_name": "PopQA, EntityQuestions",
            "performance_with_memory": "BM25-based augmentation improves accuracy; on PopQA BM25 performs better than Contriever specifically on the very least-popular entities (no single global number provided).",
            "performance_without_memory": "Vanilla LM baselines as reported (e.g., GPT-3 davinci-003 35% on PopQA).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "BM25 yields clear improvements versus vanilla LMs; overall Contriever tends to outperform BM25 on PopQA, but BM25 shows relative advantage on extreme tail items.",
            "limitations_or_challenges": "Lexical matching may fail when relevant context is paraphrased; can still retrieve wrong-entity pages and thus mislead LMs.",
            "key_insights": "Even simple lexical retrievers are competitive on extreme long-tail examples; practitioner should consider retriever choice conditioned on the popularity regime.",
            "uuid": "e4906.2",
            "source_info": {
                "paper_title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GenRead",
            "name_full": "GenRead (generate rather than retrieve)",
            "brief_description": "Parametric augmentation technique that elicits LMs to generate supporting context (a synthetic document) to be used as input context for answering.",
            "citation_title": "Generate rather than retrieve: Large language models are strong context generators",
            "mention_or_use": "use",
            "agent_name": "GenRead",
            "agent_description": "A method that prompts LMs to generate a contextual passage (rather than retrieve) and uses that generated passage as evidence concatenated with the question to answer.",
            "memory_type": "parametric generated context (elicited/context generation)",
            "memory_description": "LM generates a supporting passage using a specialized prompt; the generated text is concatenated to the prompt and used by the LM to produce the final answer.",
            "task_name": "Open-domain question answering",
            "task_description": "Entity-centric factual QA on PopQA and EntityQuestions to evaluate whether generated context can substitute for retrieved context.",
            "benchmark_name": "PopQA, EntityQuestions",
            "performance_with_memory": "GenRead generally outperforms vanilla LMs, especially for large LMs (GPT-3 davinci-003 sees sizeable gains); little-to-no improvement for smaller models; GenRead still has low performance on less-popular entities.",
            "performance_without_memory": "Vanilla LM baselines (e.g., GPT-3 davinci-003 35% on PopQA).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "GenRead can improve large LMs by surfacing parametric knowledge into context; it does not replace retrieval for long-tail facts and is less effective for small models.",
            "limitations_or_challenges": "High inference cost/latency for generating context (example: GPT-NeoX-20B GenRead run took ~70 seconds per query); generated passages can still be incorrect and do not solve long-tail memorization gaps.",
            "key_insights": "Eliciting LMs to generate context is a viable alternative to retrieval for large models, but is costly and limited on tail facts because it relies on parametric memorization.",
            "uuid": "e4906.3",
            "source_info": {
                "paper_title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Adaptive Retrieval",
            "name_full": "Adaptive Retrieval (popularity-thresholded retrieval)",
            "brief_description": "A hybrid method introduced in this paper that decides per-query whether to retrieve external context based on subject-entity popularity and relationship-type thresholds, using retrieval only when necessary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Adaptive Retrieval",
            "agent_description": "Method that uses a per-relationship popularity threshold (based on Wikipedia pageviews) to decide whether to augment a query with retrieved (BM25/Contriever) or generated (GenRead) context; otherwise uses parametric-only prediction.",
            "memory_type": "hybrid conditional retrieval (mixing parametric and non-parametric memories)",
            "memory_description": "Compute subject popularity from Wikipedia pageviews; if popularity is below a tuned per-relation threshold, retrieve top-1 passage (or use GenRead) and concatenate it to the prompt; otherwise query the LM without retrieval.",
            "task_name": "Open-domain question answering",
            "task_description": "Entity-centric QA on PopQA and EntityQuestions; objective is to combine parametric and non-parametric memory to maximize accuracy while reducing retrieval cost/latency.",
            "benchmark_name": "PopQA, EntityQuestions",
            "performance_with_memory": "For GPT-3 davinci-003 paired with GenRead and Contriever, Adaptive Retrieval achieves 46.5% accuracy on PopQA (≈+5.3 percentage points above the best non-adaptive method reported).",
            "performance_without_memory": "Vanilla GPT-3 35% on PopQA; non-adaptive always-retrieve methods performed worse than the adaptive best.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Adaptive Retrieval outperforms both always-retrieve and always-parametric baselines, especially for large LMs; it reduces amount of retrieval (example: GPT-3 retrieves for ~40% of questions with BM25) and provides accuracy and efficiency benefits.",
            "limitations_or_challenges": "Relies on Wikipedia pageviews as a proxy for memorization (time-dependent and dataset-specific); per-relationship thresholds are tuned on a dev set and may not generalize; gains are smaller for small LMs that require retrieval on most queries.",
            "key_insights": "Selective retrieval based on an input-only signal (entity popularity and relation type) can deliver better accuracy-cost tradeoffs: use retrieval only when the model is unlikely to have memorized the fact.",
            "uuid": "e4906.4",
            "source_info": {
                "paper_title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-Neo 2.7B + Contriever",
            "name_full": "GPT-Neo 2.7B augmented with Contriever dense retriever",
            "brief_description": "A medium-sized LM whose retrieval-augmentation with Contriever enables it to outperform a much larger vanilla model on long-tail factual QA.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GPT-Neo 2.7B (Contriever-augmented)",
            "agent_description": "GPT-Neo 2.7B (decoder-only LM) whose input is augmented with the top-1 Contriever passage; evaluated on PopQA long-tail questions.",
            "memory_type": "non-parametric retrieval-augmented (dense retriever)",
            "memory_description": "Top-1 Contriever-retrieved Wikipedia paragraph concatenated to the question; used to supply external factual memory for answering.",
            "task_name": "Open-domain question answering (long-tail)",
            "task_description": "Answer PopQA questions about less-popular entities (long tail). The experiment tests whether retrieval can compensate for smaller parametric capacity.",
            "benchmark_name": "PopQA",
            "performance_with_memory": "Contriever-augmented GPT-Neo 2.7B outperforms vanilla GPT-3 davinci-003 on the 4,000 least-popular PopQA questions (text reports superiority but does not list the exact absolute accuracy for this specific combo).",
            "performance_without_memory": "Vanilla GPT-Neo 2.7B lower (exact baseline not reported in text for 2.7B specifically). Example baselines for least-popular: GPT-Neo 6B 15%, GPT-NeoX/20B 16%, GPT-3 davinci-003 19% for the 4,000 least popular questions (vanilla).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Retrieval augmentation enables smaller models to match or exceed larger vanilla models on long-tail facts; demonstrates complementary nature of non-parametric memory.",
            "limitations_or_challenges": "No exact per-model absolute numbers provided in text for this specific pairing; performance depends on retriever quality and the nature of the long-tail facts.",
            "key_insights": "Using retrieval to supplement medium-sized models is an efficient way to address long-tail memorization gaps without scaling model parameters.",
            "uuid": "e4906.5",
            "source_info": {
                "paper_title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised dense information retrieval with contrastive learning",
            "rating": 2
        },
        {
            "paper_title": "Generate rather than retrieve: Large language models are strong context generators",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "The probabilistic relevance framework: BM25 and beyond",
            "rating": 1
        }
    ],
    "cost": 0.0218625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories</h1>
<p>Alex Mallen ${ }^{\circ}$ Akari Asai ${ }^{\circ}$ Victor Zhong ${ }^{\circ}$ Rajarshi Das ${ }^{\circ}$<br>Daniel Khashabi ${ }^{\text {H }}$ Hannaneh Hajishirzi ${ }^{\circ}$<br>${ }^{\circ}$ University of Washington ${ }^{\text {J }}$ Johns Hopkins University<br>${ }^{\circ}$ Allen Institute for AI<br>{atmallen, akari, vzhong, rajarshi, hannaneh}@cs.washington.edu danielk@jhu.edu</p>
<h4>Abstract</h4>
<p>Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14 k questions about long-tail entities, and EntityQuestions, a widely used opendomain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the long tail. Based on those findings, we devise a new method for retrieval augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs; Brown et al. 2020; Raffel et al. 2020) have been shown to be competitive on diverse NLP tasks, including knowledgeintensive tasks that require fine-grained memorization of factual knowledge (Chowdhery et al., 2022; Yu et al., 2022). Meanwhile, LMs have also been shown to have limited memorization for less frequent entities (Kandpal et al., 2022), are prone to hallucinations (Shuster et al., 2021), and suffer from temporal degradation (Kasai et al., 2022; Jang et al., 2022). Incorporating non-parametric knowledge (i.e., retrieved text chunks) largely helps address those issues stemming from reliance on LMs' parametric knowledge-knowledge stored</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Relationship between subject entity popularity in a question and GPT-3 performance in open-domain QA, with and without retrieved passages. Adaptive Retrieval only retrieves when necessary (orange bars) based on the heuristically-decided threshold (red line).
in their parameters (Izacard et al., 2022b)—but it is unclear whether it is strictly superior or complementary to parametric knowledge. Understanding when we should not trust LMs' outputs is also crucial to safely deploying them in real-world applications (Kadavath et al., 2022).</p>
<p>This work conducts a large-scale knowledge probing of LMs on factual knowledge memorization, to understand when we should and should not rely on LMs' parametric knowledge, and how scaling and non-parametric memories (e.g., retrievalaugmented LMs) can help. In particular, we aim to address the following research questions:
$\left(R Q_{1}\right)$ How much factual knowledge is memorized by LMs and what factors affect the memorization? (Section 4)
$\left(R Q_{2}\right)$ To what extent can non-parametric memories alleviate the shortcomings of parametric memories of LMs? (Section 5)
$\left(R Q_{3}\right)$ Can we build a system to adaptively combine non-parametric and parametric memories? (Section 6)
We hypothesize that factual knowledge frequently discussed on the web is easily memorized</p>
<p>by LMs, while the knowledge that is less discussed may not be well captured and thus they require retrieving external non-parametric memories. We evaluate ten large LMs of three families (i.e., GPTNeo, OPT, and GPT-3) with varying scales on the open-domain question answering (QA) task in a zero- or few-shot prompting manner. We construct a new dataset, PopQA, consisting of 14 k questions to cover factual information in the long tail that might have been missed in popular QA datasets (Kwiatkowski et al., 2019). We use Wikipedia page views as a measure of popularity and convert knowledge triples from Wikidata, with diverse levels of popularity, into natural language questions, anchored to the original entities and relationship types. We also use EntityQuestions (Sciavolino et al., 2021), an open-domain QA dataset with a long-tail distribution.</p>
<p>On both datasets, LMs' memorization $\left(R Q_{1}\right)$ is often limited to the popular factual knowledge and even GPT-3 davinci-003 fails to answer the majority of the long-tail questions. Moreover, on such questions, scaling up models does not significantly improve the performance (e.g., for the 4,000 least popular questions in POPQA, GPT-j 6B has $16 \%$ accuracy and GPT-3 davinci-003 has $19 \%$ accuracy). This also suggests that we can predict if LMs memorize certain knowledge based on the information presented in the input question only.</p>
<p>We next investigate whether a semi-parametric approach that augments LMs with retrieved evidence can mitigate the low performance on questions about less popular entities $\left(R Q_{2}\right)$. Nonparametric memories largely improve performance on long-tail distributions across models. Specifically, we found that retrieval-augmented LMs are particularly competitive when subject entities are not popular: a neural dense retriever (Izacard et al., 2022a)-augmented GPT-neo 2.7B outperforms GPT-3 davinci-003 on the 4,000 least popular questions. Surprisingly, we also find that retrieval augmentation can hurt the performance of large LMs on questions about popular entities as the retrieved context can be misleading.</p>
<p>As a result, we devise a simple-yet-effective retrieval-augmented LM method, Adaptive Retrieval, which adaptively combines parametric and non-parametric memories based on popularity $\left(R Q_{3}\right)$. This method further improves performance on POPQA by up to $10 \%$, while significantly reducing the inference costs, especially with larger</p>
<p>LMs (e.g., reducing GPT-3 API costs by half), indicating the potential for future research in more efficient and powerful retrieval-augmented LMs.</p>
<h2>2 Related Work</h2>
<p>Parametric and non-parametric knowledge. Petroni et al. (2019) demonstrate that large pretrained LMs such as BERT (Devlin et al., 2019) memorize the significant amount of world knowledge in their parameters (parametric knowledge), and Roberts et al. (2020) show that fine-tuned T5 without any reference documents (closedbook QA) can achieve competitive performance on open-domain QA. More recent and powerful LMs (Brown et al., 2020; Chowdhery et al., 2022) further improve performance on diverse knowledge-intensive tasks, leveraging their strong parametric memories (Kandpal et al., 2022; Yu et al., 2022). However, relying solely on their parameters to encode a wealth of world knowledge requires a prohibitively large number of parameters and the knowledge can become obsolete quickly (Kasai et al., 2022; Jang et al., 2022). Recent work shows that augmenting LMs with nonparametric memories (i.e., retrieved text chunks) enables much smaller models to match the performance of larger models (Izacard et al., 2022b; Khandelwal et al., 2020; Min et al., 2022), although Chen et al. (2022) and Longpre et al. (2021) show that even those models can ignore non-parametric knowledge and rely on parametric knowledge.</p>
<p>Understanding memorization. Several prior work establishes a positive relationship between string frequency in pre-training corpora and memorization (Carlini et al., 2022; Razeghi et al., 2022). Concurrent to our work, Kandpal et al. (2022) show that the co-occurrence of the question and answer entities in pretraining corpora has a positive correlation with models' QA accuracy on popular open-domain QA benchmarks such as Natural Questions (Kwiatkowski et al., 2019). This work, instead, attempts to predict memorization using the variables available in the input question only and uses popularity to obtain a proxy for how frequently an entity is likely to be discussed on the web. Importantly, by constructing a new dataset, we can conduct fine-grained controlled experiments across a wide range of popularities, allowing the investigation of hypotheses that might have been missed in prior analysis using existing open QA datasets. We further analyze the effectiveness and limitations of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: POPQA is created by sampling knowledge triples from Wikidata and converting them to natural language questions, followed by popularity calculation.
retrieval-augmented LMs and introduce Adaptive Retrieval. Prior work investigates the effectiveness of deciding when to use non-parametric memories at the token level in $k$-nn LM (He et al., 2021). This work is the first work to study the effectiveness of deciding whether to retrieve for each query and show their effectiveness in retrieval-augmented LM prompting.</p>
<h2>3 Evaluation Setup</h2>
<p>We evaluate LMs' ability to memorize factual knowledge through closed-book QA tasks with fewshot samples. We evaluate LMs on our new dataset, POPQA (Figure 2), and EntityQuestions, both of which have long-tail distributions (Figure 3).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Distribution of subject entity popularity for EntityQuestions, POPQA, and for NQ-open for reference. Details on NQ entities can be found in Appendix A.</p>
<h3>3.1 Focus and Task</h3>
<p>Focus: factual knowledge. Among diverse types of world knowledge, this work focuses on factual knowledge (Adams, 2015) of entities-knowledge about specific details of the target entities. We define factual knowledge as a triplet of (subject, relationship, object) as in Figure 2 left.
Task format: open-domain QA. We formulate the task as open-domain QA (Roberts et al., 2020):
given a question, a model predicts an answer without any pre-given ground-truth paragraph. ${ }^{2}$ As in Kandpal et al. (2022), we study few-shot settings and prompt LMs without any parameter updates, instead of fine-tuning them on QA datasets such as in Roberts et al. (2020).
Metrics: accuracy. We mark a prediction as correct if any substring of the prediction is an exact match of any of the gold answers.</p>
<h3>3.2 Dimensions of Analysis</h3>
<p>We hypothesize that factual knowledge that is less frequently discussed on the web may not be wellmemorized by LMs. Previous research often uses the term frequency of object entities in pretraining corpora to understand memorization (Févry et al., 2020; Kandpal et al., 2022; Razeghi et al., 2022). Instead, we investigate whether it's possible to predict memorization based on the input information only, and then apply the findings for modeling improvements, unlike prior analyses. Therefore, our work focuses on the other two variables in a factual knowledge triple: the subject entity and the relationship type.
Subject entity popularity. We use the popularity of the entities measured by Wikipedia monthly page views as a proxy for how frequently the entities are likely to be discussed on the web, instead of using the occurrence of entities or strings in the pretraining corpus (Carlini et al., 2022; Kandpal et al., 2022; Razeghi et al., 2022). Calculating frequencies over large pretraining corpora requires massive computations to link entities over billions of tokens, or can result in noisy estimations. ${ }^{3}$ Our initial studies show that this is much cheaper ${ }^{4}$ and aligns well with our intuition.
Relationship type. We also consider the relationship types as key factors for factual knowledge memorization. For example, even given the same combinations of the subject and object entities, model performance can depend on the relationship types; relationship types widely discussed can be easier to be memorized, while types that are less discussed may not be memorized much.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.3 Benchmarks</h3>
<p>PopQA. In our preliminary studies, we found that existing common open-domain QA datasets such as Natural Questions (NQ; Kwiatkowski et al. 2019) are often dominated by subject entities with high popularity, and it is often hard to identify relationship types due to diverse question surface forms. To enable a fine-grained analysis of memorization based on the aforementioned analysis dimensions, we construct PopQA, a new large-scale entitycentric open-domain QA dataset about entities with a wide variety of popularity, as shown in Figure 3.</p>
<p>To construct PopQA, we randomly sample knowledge triples of 16 diverse relationship types from Wikidata and convert them into natural language questions, using a natural language template (depicted in Figure 2). We verbalize a knowledge triple $(S, R, O)$ into a question that involves substituting the subject $S$ into a template manually written for the relationship type $R$. The full list of templates is found in Table 2 of the Appendix. The set of acceptable answers to the question is the set of entities $E$ such that $(S, R, E)$ exists in the knowledge graph. We tried various templates and found that the results were fairly robust to the templates. Since PopQA is grounded to a knowledge base, links to Wikidata entities allow for reliable analysis of popularity and relationship types.
EntityQuestions. We test on another popular opendomain QA dataset, EntityQuestions (Sciavolino et al., 2021), which also covers a long-tail entity distribution. They use Wikipedia hyperlink counts as a proxy of the frequency of entities and sample knowledge triples from WikiData, from the frequency distributions. Unlike PopQA, EntityQuestions doesn't provide entity annotations, so we only use $82 \%$ of the questions, where the mention of the subject entity has a unique match with a Wikidata entity.</p>
<h2>4 Memorization Depends on Popularity and Relationship Type</h2>
<p>We evaluate a range of LMs with varying numbers of parameters, to quantify how much factual knowledge they memorize and how different factors affect those memorization behaviors $\left(R Q_{1}\right)$.</p>
<h3>4.1 Experimental Setup</h3>
<p>Models. We evaluate ten models with a varying scale of model size: OPT (Zhang et al. 2022; 1.3, 2.7, 6.7, and 13 billion), GPT-Neo (Black et al.</p>
<p>2022; 1.3, 2.7, 6, and 20 billion), and GPT-3 (Brown et al. 2020; davinci-002, davinci-003) on our benchmark without any fine-tuning. ${ }^{5}$</p>
<p>Instructions and demonstrations. We use a simple template "Q: <question> A:" to format all of our questions for generative prediction. More sophisticated instructions were attempted in preliminary experiments but they did not improve upon the simple template significantly enough to merit using them, especially given that they may overfit to the model. While we use zero-shot prompting for GPT-3 to reduce API costs, ${ }^{6}$ we use 15 -shot prompting for all GPT-neo and OPT models.</p>
<h3>4.2 Results</h3>
<p>Overall model performance. The top left column of Figure 4 illustrates the overall performance on PopQA. As shown, even without using incontext examples, larger LMs exhibit reasonable performance: GPT-3 achieves $35 \%$ accuracy, and GPT-Neo 20B achieves $25 \%$ accuracy. This indicates that large LMs memorize factual knowledge in their parameters to some extent. This section examines which types of knowledge are better memorized and what factors influence memorization.</p>
<p>Subject entity popularity predicts memorization. Figure 4 (bottom) shows that there is a positive correlation between subject entity popularity and models' accuracy for almost all relationship types. This supports our hypothesis that subject entity popularity can be a reliable indicator of LMs' factual knowledge memorization. In general, the correlations between subject entity popularity and accuracy are stronger for larger LMs; GPT-3 003 shows the highest positive correlation (roughly 0.4) while GPT-Neo-1.3B shows relatively weak positive correlations (approximately 0.1 ).</p>
<p>Relationship types affects memorization. We find that models have a higher average performance for some relationship types than for others. While this is evidence that factual knowledge of some relationship types are more easily memorized than others, we also observe that questions of certain relationship types can be easily guessed without memorizing the knowledge triple. Specifically, certain relationship types (e.g., nationalities) allow models</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Per relationship type (n = the number of questions) results on PopQA by model, showing overall accuracy and the correlation between accuracy and log popularity. We uniformly bin the questions by log (popularity), then report the correlation between the bin center and the bin's accuracy. We see that both subject entity popularity and relationship type are strong predictors of memorization across models. The correlation with popularity exists across relationship types and is stronger for larger LMs. We show a representative subset of relationship types and the complete results are in Figures 16 and 17 in Appendix C.1, including results on EntityQuestions.</p>
<p>to exploit surface-level artifacts in subject entity names (Poerner et al., 2020; Cao et al., 2021). Additionally, models often output the most dominant answer entities for questions about relationship types with fewer answer entities (e.g., red for the color relationship type). In Figure 4, relationships with lower correlation (e.g., country, sport) often shows higher accuracy, indicating that on those relationship types, models may exploit surface-level clues. On the other hand, for relationship types with relatively low accuracy (e.g., occupation, author, director), larger LMs often show a high correlation. Further details are in Appendix C.1.</p>
<p><strong>Scaling may not help with tail knowledge.</strong> As seen in the left column of Figure 4, there are clear overall performance improvements with scale on the PopQA dataset. However, Figure 5 shows that on both PopQA and EntityQuestions, most of scaling's positive effect on parametric knowledge comes from questions with high popularity. Specifically, for the questions about the entities whose log10 (popularity) is larger than 4, there is an improvement in accuracy as model size increases (red and yellow lines), while performance on questions with lower popularity remains relatively constant (blue and green lines). For the 4,000 least popular questions, GPT-Neo 6B, 20B, and GPT-3 davinci-003 have 15%, 16%, and 19% accuracy, respectively.</p>
<p>This somewhat dampens prior works' findings that scaling up models significantly improves their factual knowledge memorization (Roberts et al., 2020; Kandpal et al., 2022). We hypothesize that</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: PopQA scaling results, broken down by question popularity level. Scaling mostly improves memorization of more popular factual knowledge. Error bars are 95% confidence intervals.</p>
<p>this is because their evaluations are often conducted on QA datasets with popular entities. In sum, scaling lowers the threshold of popularity for knowledge to be reliably memorized, but is not projected to move the threshold far into the long tail for practical model scales.</p>
<p>Relationship type results breakdown. Figure 6 provides a closer look at the relationship between popularity, accuracy, and relationship type; it shows model accuracy over the popularity distributions for director and country. For the first two types, we can see a clear positive trend between popularity and accuracy across models, and as the model size gets larger, the LMs memorize more. On the other hand, in the "country" relationship type, no models show trends, while overall the accuracy is high, indicating the LMs often exploit artifacts to answer less popular questions. We show example models' predictions in Appendix Section C.3.</p>
<h2>5 Non-parametric Memory Complements Parametric Memory</h2>
<p>Our analysis indicates that even the current state-of-the-art LMs struggle with less popular subjects or certain relationship types, and increasing the model size does not lead to further performance improvements. In light of this, we extend our analysis to non-parametric sources of knowledge, as outlined in Section $\left(R Q_{2}\right)$. Specifically, we investigate the effectiveness of retrieval-augmented LMs (Borgeaud et al., 2022; Lewis et al., 2020), which leverage non-parametric memories (i.e., retrieved text) to improve performance.</p>
<h3>5.1 Experimental Setup</h3>
<p>Augmenting input. In this work, we try a simple retrieval-augmented LM approach, where we run an off-the-shelf retrieval system off-line to retrieve context from Wikipedia relevant to a question, ${ }^{7}$ and then we concatenate the retrieved context with the original question. Although increasing the context size often leads to performance gains (Izacard and Grave, 2021; Asai et al., 2022), we only use the top one retrieved paragraph for simplicity.
Retrieval models. We use two widely-used retrieval systems: BM25 (Robertson et al., 2009)</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Memorization versus popularity for three models and the relationship types with the largest and smallest correlations. Within a relationship type, generally, there is a monotonically increasing link between popularity and performance, except for "country". Error bars show Wilson 95\% confidence intervals.
and Contriever (Izacard et al., 2022a). BM25 is a static term-based retriever without training, while Contriever is pretrained on large unlabeled corpora, followed by fine-tuning on MS MARCO (Bajaj et al., 2016). We also experiment with a parametric augmentation method, GenRead (Yu et al., 2022), which prompts LMs to generate rather than retrieve a contextual document to answer a question. We use the ten LMs in Section 4, resulting in 40 LMs and retrieval-augmented LMs.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: PopQA accuracy of LMs augmented with BM25, Contriever, GenRead, and unassisted (vanilla). Retrieving non-parametric memories significantly improves the performance of smaller models. Complete results on PopQA are found in Figure 13. EntityQuestions results are in Figure 14 of the Appendix.</p>
<h3>5.2 Results</h3>
<p>Retrieval largely improves performance. Figure 7 shows that augmenting LMs with nonparametric memories significantly outperforms unassisted vanilla LMs. A much smaller LM (e.g., GPT-Neo 2.7B) augmented by the Contriever retrieval results outperforms vanilla GPT-3. Large LMs such as GPT-3 also enjoy the benefits of nonparametric memories. Contriever gives $7 \%$ accuracy gains on top of GPT-3 davinci-003. Gen-</p>
<p>Read shows little-to-no performance improvement over vanilla parametric knowledge for smaller models, while the technique shows sizeable gains for GPT-3, especially davinci-003. In addition to its limited effectiveness with smaller LMs, GenRead has potentially prohibitive inference time costs, with GPT-NeoX 20B taking 70 seconds per query.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: GPT-3 davinci-003 accuracy versus relative popularity (how popular a question is relative to other questions of its relationship type). <strong>Retrieval-augmented LMs (dashed) outperform LMs' parametric memory (solid) for less popular entities, while parametric memory is competitive for more popular entities.</strong> Relative popularity is defined as the log-popularity of a question, normalized by the mean and standard deviation of log-popularity for the question's relationship type (smaller for less popular entities). Figure 17 shows per-relationship results.</p>
<h3>Non-parametric memories are effective for less popular facts.</h3>
<p>How does retrieval augmentation lead to such significant improvements? Figure 8 shows the relationship between the entity popularity and models' QA performance. It can be seen that retrieval-augmented LMs guided by Contriever or BM25 have a clear advantage over unassisted vanilla LMs, especially on less popular entities, resulting in a significant performance gain. Overall, Contriever-guided LMs outperform BM25-based ones on PopQA, while the BM25-based models perform better on the least popular entities, consistent with the findings from Sciavolino et al. (2021). On the other hand, for more popular entities, parametric knowledge shows equal or higher accuracy, indicating that the state-of-the-art LMs have all-</p>
<table>
<thead>
<tr>
<th></th>
<th>Contriever-augmented LM succeeded</th>
<th>failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>LM succeeded</td>
<td>0.83 (24%)</td>
<td>0.14 (10%)</td>
</tr>
<tr>
<td>LM failed</td>
<td>0.88 (17%)</td>
<td>0.11 (49%)</td>
</tr>
</tbody>
</table>
<p>Table 1: The recall@1 of Contriever for questions that GPT-3 davinci-003 answered correctly and incorrectly with and without retrieval on PopQA. The percent of questions falling in each category is shown in parentheses. <strong>For 10% of questions, retrieval is harmful due to low-quality retrieved text (0.14 recall@1).</strong></p>
<p>ready memorized the answers, and augmenting input with retrieved-context doesn't help much or even hurts the performance. Interestingly, GenRead generally outperforms vanilla LMs despite relying on LMs' parametric memory. This demonstrates the effectiveness of elicitive prompting (Wei et al., 2022; Sun et al., 2022) as observed in prior work. However, like vanilla LMs, GenRead shows low performance on less popular entities.</p>
<h3>Non-parametric memories can mislead LMs.</h3>
<p>We conduct an in-depth analysis of why retrieval-augmented models suffer in more popular entities. We hypothesize that retrieval results may not always be correct or helpful, and can mislead LMs. To test this hypothesis, we group the questions based on two axes: whether unassisted GPT-3 davinci-003 predict correctly or not, and whether retrieval-augmented predictions are correct or not. For each of the four categories, we calculate recall@1 (whether a gold answer is included in the top 1 document; Karpukhin et al. 2020).</p>
<p>Table 1 shows recall@1 for each group with percentages of the questions falling into each of the categories. For 10% of questions, retrieval-augmentation causes the LM to incorrectly answer a question it could otherwise answer correctly. We found that on those questions, recall@1 is significantly lower than the overall recall@1 (0.14 vs 0.42 overall), indicating that failed retrieval can result in performance drops. Conversely, for the 17% of questions for which retrieval causes the LM to correctly answer a question it would otherwise have failed to answer, the recall@1 is 0.88. We include examples of both cases in Appendix Section C.3.</p>
<h2>6 Adaptive Retrieval: Using Retrieval Only Where It Helps</h2>
<p>While incorporating non-parametric memories helps in long-tail distributions, powerful LMs have</p>
<p>^{8}Error bars show Wilson 95% confidence intervals. Bins with less than 40 samples have been excluded to avoid showing results with exceedingly wide errorbars.</p>
<p>already memorized factual knowledge for popular entities, and retrieval augmentation can be harmful. As outlined in $\left(R Q_{3}\right)$, can we achieve the best of both worlds? We propose a simple-yeteffective method, Adaptive Retrieval, which decides when to retrieve passages only based on input query information and augments the input with retrieved non-parametric memories only when necessary. We show that this is not only more powerful than LMs or retrieval-augmented LMs always retrieving context, but also more efficient than the standard retrieval-augmented setup.</p>
<h3>6.1 Method</h3>
<p>Adaptive Retrieval is based on our findings: as the current best LMs have already memorized more popular knowledge, we can use retrieval only when they do not memorize the factual knowledge and thus need to find external non-parametric knowledge. In particular, we use retrieval for questions whose popularity is lower than a threshold (popularity threshold), and for more popular entities, do not use retrieval at all.</p>
<p>Using a development set, the threshold is chosen to maximize the adaptive accuracy, which we define as the accuracy attained by taking the predictions of the retrieval-augmented system for questions below the popularity threshold and the predictions based on parametric knowledge for the rest. We determine the popularity threshold independently for each relationship type.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: POPQA performance of GPT-neo models and GPT3 davinci-003, with different retrieval methods. Adaptive Retrieval robustly outperforms approaches that always retrieve, especially for larger LMs.</p>
<h3>6.2 Results</h3>
<p>Adaptive Retrieval improves performance. Figure 9 shows the results when we adaptively retrieve non-parametric memories based on the perrelationship type thresholds. We can see that adaptively retrieving non-parametric memories is effective for larger models. The best performance on POPQA is using GPT-3 davinci-003 adaptively
with GenRead and Contriever, yielding $46.5 \%$ accuracy, $5.3 \%$ higher than any non-adaptive method.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: The proportion of questions for which various models use retrieval in the Adaptive Retrieval setup on POPQA. When using Adaptive Retrieval, small models must still rely on non-parametric memory for most questions, while larger models have more reliable parametric memories enabling them to use retrieval less often.</p>
<p>The threshold shifts with LM scale. While Adaptive Retrieval shows performance gains for larger models, smaller models do not realize the same benefits; as shown in Figure 9, the performance gain from Adaptive Retrieval is much smaller when we use models smaller than 10 billion. Why does this happen? Figure 10 shows that smaller LMs almost always retrieve, indicating that there are not many questions for which small LMs' parametric knowledge is more reliable than non-parametric memory. In contrast, large models typically retrieve much less. For example, GPT3 davinci-003 only retrieves for $40 \%$ of questions when paired with BM25, and even the much smaller GPT-neox 20B does not retrieve documents on more than $20 \%$ of the questions. On EntityQuestions (Appendix Figure 15) all of the LMs retrieve much more, as the questions are mostly about less popular entities.</p>
<p>Adaptive Retrieval reduces inference-time costs. We also found that Adaptive Retrieval improves efficiency; if we know we do not need to retrieve documents, we can skip retrieval components and the input length becomes shorter, which improves latency in both retrieval and language model components. Figure 11 shows the inference latency of GPT-J 6B and GPT-neox 20B, and API costs of GPT-3. Especially for larger LMs, concatenating retrieved context results in significantly increased latency (e.g., for GPT-J 6B, the inference time latency almost doubles). Adaptive retrieval enables reducing inference time up to $9 \%$ from standard</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: PopQA latency for large GPT-neo models that were run on our machines, and API costs for GPT3. <strong>Adaptive retrieval reduces latency and API costs.</strong></p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Accuracy and cost savings of Adaptive Retrieval for EntityQuestions. Despite EntityQuestions's lack of popular entities (see Figure 3), Adaptive Retrieval is able to reduce API costs by 15% while maintaining equivalent performance to retrieval only.</p>
<p>retrieval. We also observe cost reduction on EntityQuestions, as shown in Figure 12.</p>
<h2>7 Discussion and Conclusions</h2>
<p>This work conducts large-scale knowledge probing to examine the effectiveness and limitations of relying on LMs' parameters to memorize factual knowledge and to understand what factors affect factual knowledge memorization. Our results show that memorization has a strong correlation with entity popularity and that scaling up models on long-tail distributions may only provide marginal improvements. We also demonstrate that non-parametric memories can greatly aid LMs on these long-tail distributions, but can also mislead LMs on questions about well-known entities, as powerful LMs have already memorized them in their parameters. Based on those findings, we devise simple-yet-effective Adaptive Retrieval, which only retrieves when necessary, using a heuristic based on entity popularity and relationship types. Our experimental results show that this method is not only more powerful than LMs or previous retrieval-augmented LMs but also more efficient.</p>
<h2>Limitations</h2>
<p>This work focuses on entity-centric factual knowledge and demonstrates that LMs' memorization is heavily affected by the popularity of the entities and the aspect of the entities being asked in the questions. It is important to emphasize that for running controlled experiments, we have relied on two synthetic datasets, and the extent to which our results apply to naturally occurring factual knowledge has not been firmly established. While we can be fairly confident about the relationship between scaling, retrieval, popularity, relationship type, and performance for the kinds of knowledge studied here, the effectiveness of Adaptive Retrieval will depend on many details of the question answering pipeline. Moreover, our work depends on a definition of popularity that is time-dependent and may not perfectly reflect how frequently entities are discussed on the web. Wikipedia page views are one possible definition of popularity for which we observe our results, and we invite others to improve upon it in future work. Further research can expand upon this simple approach, perhaps drawing on insights from Kadavath et al. (2022) to improve the effectiveness of Adaptive Retrieval.</p>
<p>It is an open question if the same findings are applicable to other types of world knowledge such as commonsense. We conjecture that the concept of the subject topic (entity), as well as the aspect (relationship type), can be applied with some minor modifications, which future work can quantify memorization following our scheme.</p>
<h2>Ethical Considerations</h2>
<p>Recent work (Huang et al., 2022) shows that LMs memorize personal information available on the web, which has significant security issues. Our evaluation focuses on the memorization of general entity-centric knowledge, but our findings can be applicable to those areas. Our findings suggest that LMs are likely to have less reliable knowledge of minority groups. Parrish et al. (2022) established that models often rely on stereotypes to answer in uncertain cases, so our results indicate that LMs are likely to rely on stereotypes disproportionately for minority groups. Future work could investigate whether retrieval augmentation reduces bias in these cases.</p>
<h2>Acknowledgements</h2>
<p>We thank the UW NLP group members for their helpful discussions, and Joongwon Kim, Wenya Wang, and Sean Welleck for their insightful feedback on this paper. This research was supported by NSF IIS-2044660, ONR N00014-18-1-2826,</p>
<p>ONR MURI N00014- 18-1-2670, and Allen Distinguished Award. AM is funded by a Goldwater Scholarship and AA is funded by the IBM PhD Fellowship.</p>
<h2>References</h2>
<p>Nancy E Adams. 2015. Bloom's taxonomy of cognitive learning objectives. Journal of the Medical Library Association.</p>
<p>Akari Asai, Matt Gardner, and Hannaneh Hajishirzi. 2022. Evidentiality-guided generation for knowledge-intensive NLP tasks. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. MS MARCO: A human generated machine reading comprehension dataset.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing systems.</p>
<p>Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021. Knowledgeable or educated guess? revisiting language models as knowledge bases. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models.</p>
<p>Hung-Ting Chen, Michael JQ Zhang, and Eunsol Choi. 2022. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by wikipedia entities). In Proceedings of the 19th ACM international conference on Information and knowledge management.</p>
<p>Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Junxian He, Graham Neubig, and Taylor BergKirkpatrick. 2021. Efficient nearest neighbor language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. 2022. Are large pre-trained language models leaking your personal information? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard</p>
<p>Grave. 2022b. Few-shot learning with retrieval augmented language models.</p>
<p>Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. 2022. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, and Kentaro Inui. 2022. Realtime QA: What's the answer right now?</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems.</p>
<p>Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7052-7063, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Nonparametric masked language model.</p>
<p>Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.</p>
<p>Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020. E-BERT: Efficient-yet-effective entity embeddings for BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval.</p>
<p>Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric questions challenge dense retrievers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than retrieve: Large language models are strong context generators.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models.</p>
<h2>Appendix</h2>
<h2>A Details of POPQA Constructions</h2>
<p>List of the relationship types and templates. In this work, we use the following 16 relationship types, and the authors of this paper manually annotated templates to verbalize knowledge triple to natural language questions. We show the final list of the templates used to create POPQA in Table 2.</p>
<p>Figure 3 shows the distribution of subject popularity of PoPQAand EntityQuestions versus the popular NQ benchmark. NQ may have multiple entities so the distribution of the least popular entity per question is shown. Subject entities from NQ were extracted using TagMe (Ferragina and Scaiella, 2010) on the NQ-open development set with a score threshold of 0.22 . TagMe returns the title of a Wikidata entity which can be directly used to find popularity.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relationship</th>
<th style="text-align: right;">Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">occupation</td>
<td style="text-align: right;">What is [subj] 's occupation?</td>
</tr>
<tr>
<td style="text-align: left;">place of birth</td>
<td style="text-align: right;">In what city was [subj] born?</td>
</tr>
<tr>
<td style="text-align: left;">genre</td>
<td style="text-align: right;">What genre is [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">father</td>
<td style="text-align: right;">Who is the father of [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">country</td>
<td style="text-align: right;">In what country is [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">producer</td>
<td style="text-align: right;">Who was the producer of [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">director</td>
<td style="text-align: right;">Who was the director of [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">capital of</td>
<td style="text-align: right;">What is [subj] the capital of?</td>
</tr>
<tr>
<td style="text-align: left;">screenwriter</td>
<td style="text-align: right;">Who was the screenwriter for [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">composer</td>
<td style="text-align: right;">Who was the composer of [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">color</td>
<td style="text-align: right;">What color is [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">religion</td>
<td style="text-align: right;">What is the religion of [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">sport</td>
<td style="text-align: right;">What sport does [subj] play?</td>
</tr>
<tr>
<td style="text-align: left;">author</td>
<td style="text-align: right;">Who is the author of [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">mother</td>
<td style="text-align: right;">Who is the mother of [subj]?</td>
</tr>
<tr>
<td style="text-align: left;">capital</td>
<td style="text-align: right;">What is the capital of [subj]?</td>
</tr>
</tbody>
</table>
<p>Table 2: Full list of the manually annotated templated used for POPQAcreations. [subj] denotes a placeholder for subject entities.</p>
<p>Knowledge triples sampling. In the construction of the POPQAdataset, knowledge triples are sampled with higher weight given to more popular entities, otherwise, the distribution would be dominated by the tail and we would not have enough high-popularity entities to complete our analysis. Specifically, when considering whether to sample a particular knowledge triple, we include the knowledge triple if and only if $f&gt;\exp (8 R-6)$, where $R \sim U(0,1)$ is a unit uniform pseudo-random number and $f$ is the exact match term frequency of the subject entity's aliases in an 800 MB random sample of C4. To increase diversity, once 2000 knowledge triples of a particular relation type have been sampled, they are no longer sampled.</p>
<h2>B Experimental Details</h2>
<p>Computational resources and API costs. GPT3 API usage totaled to $\$ 275$. We ran 14,282 questions through two GPT-3 davinci models using four different methods: vanilla experiments cost $\$ 13$ ( $\$ 0.46$ per 1000 questions), Contrieveraugmented experiments cost $\$ 88$ ( $\$ 3.08$ per 1000 questions), BM25-augmented experiments cost $\$ 81$ ( $\$ 2.80$ per 1000 questions), and GenRead experiments cost $\$ 93$ ( $\$ 3.25$ per 1000 questions).</p>
<p>To run experiments using LMs larger than two billion parameters, we use a single V100 Volta GPU with 32GB GPU memories. We use int8bit (Zeng et al., 2022) quantization with OPT 13 billion and GPT-Neo 20 billion models to make them fit our GPUs. In our preliminary experiments using GPT-Neo 6 billion, we did not observe a notable performance drop by using the quantization.</p>
<p>Constructing few-shot contexts. For POPQA, we sample few-shot examples stratified by relationship type to diversify the samples: for each of the 15 relationship types other than the one in the test question, we sample one random question-answer pair to include in the context. For EntityQuestions, we take a simple random sample of 15 questionanswer pairs because there are more than 16 relationship types.</p>
<p>Details of deciding thresholds. We 75\% of POPQAto determine a popularity threshold for each relation type. Using brute force search, we select the threshold to maximize the adaptive accuracy, which we define as the accuracy attained by taking the predictions of the retrieval-augmented system for questions below the popularity threshold and the predictions based on parametric knowledge for the rest.</p>
<p>We then evaluate adaptive accuracy using the learned thresholds on the remaining $25 \%$ of POPQA, and repeat with 100 different random splits and take the mean to obtain the reported adaptive accuracy measurement.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Accuracy by LMs and retrieval-augmented LMs on PopQA. This is an extension of Figure 7
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Accuracy by LMs and retrieval-augmented LMs on EntityQuestions.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: The proportion of questions for which Adaptive Retrieval uses retrieval versus model size for EntityQuestions.</p>
<h2>C Detailed Results</h2>
<h2>C. 1 LM results</h2>
<p>Full results of per-relationship type accuracy and correlation. Figure 16 shows the full result of per-relationship type accuracy for all relationship types in PopQA. Figure 17 shows the correlations for all relation types. Figures 19 and 18 show the same results for the EntityQuestions dataset.</p>
<p>Negative correlations of capital on EntityQuestions. As shown in Figure 19, the capital relationship types on in EntityQuestions, while on PopQA, this relationship shows relatively high correlations. We found that in EntityQuestions, this capital relationship type has many low-popularity questions whose answers are included in subject entity names (e.g., subject="canton of Marseille-Belsunce", object="Marseille"). This causes performance to have a U-shaped relationship with popularity for the capital relationship type, so if most of the questions sampled come from the top half of popularity, the linear correlation will be positive, and vice versa.</p>
<h2>C. 2 Retrieval-augmented LM results</h2>
<p>Overall performance of retrieval-augmented LMs. Figure 13 shows the overall performance of 40 LMs and retrieval-augmented LMs on PopQA. Retrieval-augmentation largely improves performance across different LMs, and much smaller models (GPT-Neo 1.3B) can perform on per with GPT-3. Figure 14 shows the results on EntityQuestions. Due to computational and time constraints, we were only able to run vanilla and Contriever results for most models.</p>
<p>Adaptive Retrieval for EntityQuestions. Figure 15 shows the proportion of questions above the retrieval threshold for various models using Adaptive Retrieval on EntityQuestions. Because EntityQuestions has a large quantity of low-popularity questions, models (especially smaller ones) must rely heavily on retrieval.</p>
<p>Full results on all relationship types. Figure 20 shows the full results on PopQA of the retrievalaugmented LMs and unassisted LMs on 16 relationship types using three different LMs as backbones. Figure 21 shows these results for GPT-3 davinci-003 on EntityQuestions.</p>
<h2>C. 3 Qualitative Results</h2>
<p>Table 3 shows several examples on PopQA, where GPT-3 davinci-003 answers correctly while the Contriever-augmented version fails to answer. Along with the low recall@1 of 0.14 for this group, Table 3 suggests that the most common reason retrieval can be harmful is that it retrieves a document about a mistaken entity, such as a person with the same name as the subject, or an entity that simply is not relevant to the question (as in the case of "Noel Black").</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Accuracy on PopQA for all relationship types and models. This is an extension of Figure 4.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17: Correlations on PopQA for all relationship types and models. This is an extension of Figure 4.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18: Accuracy on EntityQuestions for all relationship types and models.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19: Correlations on EntityQuestions for all relationship types and models.
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20: Accuracy for 3 models on PopQA versus popularity as shown in Figure 8 broken down by relationship type. Popularity bins with less than 5 samples are excluded to avoid cluttering the figures with noisy results that have wide error bars.</p>
<p><img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 21: Accuracy versus popularity for GPT-3 davinci-003 on EntityQuestions broken down by relationship type. Popularity bins with less than 5 samples are excluded.</p>
<p>Table 4 shows several examples on POPQA, where GPT-3 davinci-003 answers correctly only when augmented with Contriever. The recall@1 for this case is 0.88 , which is significantly higher than the overall recall. Note that in the second example, the retrieval caused the LM to answer correctly, but only by coincidence: the subject entity "Pierre" actually refers to the city in South Dakota, not the Basketball player. Otherwise, retrieval appears to be helpful because it provides the relevant information directly.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question (answer)</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">Retrieval-LM</th>
<th style="text-align: center;">Retrieval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Who was the director of Black? (Sanjay Leela Bhansali)</td>
<td style="text-align: center;">The director of Black is Sanjay Leela Bhansali. $\checkmark$</td>
<td style="text-align: center;">Noel Black $\boldsymbol{X}$</td>
<td style="text-align: center;">Noel Black (June 30, 1937 - July 5, 2014) was an American film and television director, screenwriter, and producer. Black was born in Chicago, Illinois. He won awards at the 1966 ... and subsequently concentrated on directing for television, occasionally directing films such as "Private School".</td>
</tr>
<tr>
<td style="text-align: center;">Who was the producer of The Faculty? (Robert Rodriguez)</td>
<td style="text-align: center;">The Faculty was produced by Elizabeth Avellan and Robert Rodriguez. $\checkmark$</td>
<td style="text-align: center;">The producer of The Faculty was Elizabeth Avellan. $\boldsymbol{X}$</td>
<td style="text-align: center;">The Faculty is a 1998 American science fiction teen horror film written by Kevin Williamson, directed by Robert Rodriguez, and starring Elijah Wood,... The film was released on Christmas Day, 1998, by Miramax Films through Dimension Films. ... The film has developed a cult following in the years since its release.</td>
</tr>
<tr>
<td style="text-align: center;">What is Michael Shelley's occupation? (singer-songwriter)</td>
<td style="text-align: center;">Michael Shelley is a singersongwriter and musician. $\checkmark$</td>
<td style="text-align: center;">Michael Shelley is an American applied mathematician. $\boldsymbol{X}$</td>
<td style="text-align: center;">Michael Shelley (mathematician) Michael J. Shelley (born August 17, 1959) is an American applied mathematician who works on the modeling and simulation of complex systems arising in physics and biology. This has included free-boundary problems in fluids and materials science... He is also the co-founder and co-director of the Courant Institute's Applied Mathematics Lab.</td>
</tr>
<tr>
<td style="text-align: center;">In what city was Zijah Sokolović born? (Sarajevo)</td>
<td style="text-align: center;">Zijah Sokolović was born in Sarajevo $\checkmark$</td>
<td style="text-align: center;">Zijah Sokolović was born in Orahovac, Kingdom $\boldsymbol{X}$</td>
<td style="text-align: center;">Ali Sokol ... (born 8 May 1921 in Orahovac, Kingdom of Serbs, Croats and Slovenes, died 23 September 1974) was a Yugoslav pulmonologist. Ali Sokol was born into an agricultural family. He was the third of four children of father and mother Hatixhes Solomon. It is unknown the exact date of birth but the most reliable date is May 8 year in 1921.</td>
</tr>
</tbody>
</table>
<p>Table 3: Qualitative examples of the questions where only retrieval-augmented LMs fail to answer correctly. The blue underlined text indicates the sub-strings matching the gold answers in the retrieved context.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question (answer)</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">Retrieval-LM</th>
<th style="text-align: center;">Retrieval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">What genre is Unknown? (fantasy)</td>
<td style="text-align: center;">Unknown is not a specific genre of music. It could refer to $\mathcal{F}$</td>
<td style="text-align: center;">Unknown is a pulp fantasy fiction magazine. $\checkmark$</td>
<td style="text-align: center;">Unknown (magazine) Unknown (also known as Unknown Worlds) was an American pulp fantasy fiction magazine, published from 1939 to 1943 by Street \&amp; Smith, and edited by John W. Campbell. ... The leading fantasy magazine in the 1930s was "Weird Tales", which focused on shock and horror.</td>
</tr>
<tr>
<td style="text-align: center;">In what country is Pierre? (United States)</td>
<td style="text-align: center;">Pierre is in France. $\mathcal{F}$</td>
<td style="text-align: center;">Pierre is from the United States. $\checkmark$</td>
<td style="text-align: center;">Jonas Farley Pierre (born May 30, 1980) is an American professional basketball player. A center, he has played with three teams in the National Basketball League of Canada (NBL). Pierre last played with the Saint John Mill Rats in Canada. ... He then joined Lincoln University in Missouri for his third year. In the summer of 2003, he signed a letter of intent to play with Nicholls State in the NCAA Division I.</td>
</tr>
<tr>
<td style="text-align: center;">Who was the producer of The Cocoanuts? (Walter Wanger)</td>
<td style="text-align: center;">The Cocoanuts was produced by Florenz Ziegfeld. $\mathcal{F}$</td>
<td style="text-align: center;">The Cocoanuts was produced for Paramount Pictures by Walter Wanger, who $\checkmark$</td>
<td style="text-align: center;">The Cocoanuts is a 1929 musical comedy film starring the Marx Brothers. Produced for Paramount Pictures by Walter Wanger, who is not credited, the film stars the four Marx Brothers, Oscar Shaw, Mary Eaton, and Margaret Dumont. It was the first sound film to credit more than one director (Robert Florey and Joseph Santley), and was adapted to the screen by Morrie Ryskind from the George S. Kaufman Broadway musical play. ...</td>
</tr>
<tr>
<td style="text-align: center;">Who was the director of The White Suit? (Lazar Ristovski)</td>
<td style="text-align: center;">The White Suit was directed by Sachin Kundalkar. $\mathcal{F}$</td>
<td style="text-align: center;">Lazar Ristovski <br> $\checkmark$</td>
<td style="text-align: center;">In 1999 "The White Suit" an auteur film by Ristovski (director, writer, lead actor, and producer) was at the Cannes Film Festival in the Critics Week program. "The White Suit" was the Serbian entry for the 1999 Academy Awards. Lazar Ristovski is the sole owner of Zillion Film Company In 2006, he made a small appearance in the James Bond film "Casino Royale". He played Caruso in the 2004 movie "King of Thieves". He starred as Đorđe in the award-winning 2009 film "St. George Shoots the Dragon".</td>
</tr>
</tbody>
</table>
<p>Table 4: Qualitative examples of the questions where only retrieval-augmented LMs successfully answer correctly. The blue underlined text indicates the sub-strings matching the gold answers in the retrieved context.</p>
<h1>A For every submission:</h1>
<p>A1. Did you describe the limitations of your work?
Section 7: Limitations
A2. Did you discuss any potential risks of your work?
Section 7: Ethical Considerations
A3. Do the abstract and introduction summarize the paper's main claims?
Section 1
\ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<p>Section 3.3
B1. Did you cite the creators of artifacts you used?
Section 3.3
B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
The license is in our GitHub repository, which will be linked to from the abstract in the non-anonymous version.</p>
<p>■ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Our dataset only contains data from Wikidata, which is widely used for NLP experiments and is already publicly available.</p>
<p>■ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Our dataset only contains data from Wikidata, which is widely used for NLP experiments and is already publicly available.</p>
<p>B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Appendix A
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Appendix B
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<h1>C Did you run computational experiments?</h1>
<p>Sections 4, 5, and 6
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Sections 4.1 and 5.1, Appendix B
C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
Sections 4, 5, and 6
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
Sections 4, 5, and 6, Appendix C
C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
Appendix A
D Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6} 30$ PopQA and 26 EntityQuestions questions had popularity less than the smallest popularity bin, and are excluded to avoid showing results for small sample sizes.
${ }^{7}$ We use Wikipedia dump from December 2018.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
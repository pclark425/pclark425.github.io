<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1950 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1950</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1950</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-281496824</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.19012v1.pdf" target="_blank">Pure Vision Language Action (VLA) Models: A Comprehensive Survey</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.</p>
                <p><strong>Cost:</strong> 0.035</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1950.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1950.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1 (Robotics Transformer 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive, multimodal Transformer trained on large-scale real-world robotic demonstrations to map images and language instructions to action tokens for robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robotics transformer for real-world control at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer policy that tokenizes vision, language, and robot state into a unified sequence and decodes action tokens; multimodal fusion via FiLM-style conditioning; trained as a generalist manipulation policy.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>supervised imitation pretraining on robot demonstration datasets (real-world)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale real robot demonstration dataset (cited ~130k demonstrations in survey) containing paired images, low-dimensional states, action trajectories and natural language labels/instructions; primarily tabletop/manipulation skills.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (multi-task real-world)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task manipulation (pick/place, grasping, assembly-type skills) on real physical robot arms; continuous low-level control reconstructed from decoded action tokens; tasks are real-world demonstrations rather than synthetic simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports strong semantic alignment between language instructions and action tokens due to paired demonstrations; pretraining data contains object descriptions and action semantics aligned to manipulation primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey emphasizes large-data pretraining (130k demos) improves generalization and zero/few-shot transfer compared to training from scratch, but no precise sample-count comparisons reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey notes multimodal fusion (FiLM) and tokenization help alignment but does not present attention-visualization analyses for RT-1 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in detail in survey for RT-1; survey mentions that joint tokenization yields shared representations aiding transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Evidence is descriptive: RT-1 is reported to map language to executable action tokens from paired demonstrations, implying grounding between instruction verbs and motor trajectories via supervised signals.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit layer- or hierarchy-level feature analyses reported for RT-1 in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when target tasks share objects/actions with demonstration distribution; scaling data size and diversity improves cross-task transfer according to survey.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not provide quantitative split results for novel vs familiar objects for RT-1, only qualitative claims of improved generalization with large real-data pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey indicates RT-1-style pretraining supports improved few-shot promptability and some zero-shot transfer to related tasks, but no numeric zero-shot metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No detailed ablation/layer-freezing analysis reported in survey for RT-1.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes generalization limits and sim-to-real gaps broadly, but no explicit negative transfer case quantified for RT-1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey contrasts VLM+language-integrated pretraining to vision-only but does not provide direct numeric comparisons for RT-1 versus vision-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit description of representation dynamics during fine-tuning for RT-1 in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported for RT-1 in this survey.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1950.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Robotics Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A follow-up RT model that transfers web-scale vision-language knowledge into robotic control (autoreg. Transformer trained with web-scale pretraining plus robot demos).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive multimodal Transformer that integrates pretrained vision-language knowledge (web-scale) with robot action-token decoding to improve open-vocabulary grasping and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on web-scale image-text plus supervised fine-tuning on robot demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Web-scale image-text (VLM) knowledge plus robot demonstration datasets; contains object labels, captions and likely action-descriptive text from web-scale corpora used to inject semantic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (open-vocabulary grasping and generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world manipulation tasks with varied objects and open-vocabulary language prompts; continuous/parametric action reconstruction from tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey describes RT-2 as leveraging web knowledge to increase semantic grounding (open-vocabulary), implying overlap in object names and affordance descriptions but no quantified overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey claims web-scale pretraining reduces reliance on robot-specific pretraining but does not give precise sample-efficiency numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No specific attention or saliency analyses presented in the survey for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Survey suggests pretrained VLM features provide semantic priors but gives no embedding probes for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Qualitative evidence: RT-2 shows improved open-vocabulary behavior consistent with semantic priors from VLMs; explicit grounding experiments not detailed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works better when language pretraining covers object/action vocabulary relevant to downstream tasks; domain mismatch remains a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey mentions RT-2 improves open-vocabulary handling but provides no quantitative novel/familiar object split.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey indicates stronger zero/few-shot recognition of language instructions relative to vision-only baselines, but no numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer ablation details reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey warns of brittle multimodal alignment and potential failure under noisy inputs, but no RT-2-specific negative transfer quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey frames RT-2 as better than vision-only due to injected language priors, but without numeric comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1950.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E (Embodied PaLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal model that extends large language model knowledge into embodied control by adding action tokens and conditioning on visual/state inputs for robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-augmented embodied model (language backbone extended with perception and action tokens); conditions on images and states to autoregressively generate action sequences; integrates language knowledge into control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>pretrained text-only LLM extended with multimodal finetuning (vision+robot demos)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-text LLM pretraining (language knowledge), then multimodal finetuning on paired image/state-action demonstrations; contains object descriptions and instruction-style data but limited robot-interaction experience prior to finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation and instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world and simulated manipulation tasks with natural language instructions, mapping semantic goals to action sequences; action space continuous and discretized into tokens for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey describes PaLM-E as integrating pretrained language knowledge into embodied policies, increasing semantic alignment between instructions and actions though lacking direct physical interaction experience.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey indicates language priors help reduce required robot-specific data but does not report numeric sample-efficiency figures for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No specific attention-analysis reported in survey for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not detailed in survey; described qualitatively as benefiting from LLM semantic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey discusses PaLM-E's ability to map language to actions via supervised demonstrations; explicit experiments probing grounding mechanisms not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Benefits when linguistic priors are relevant to target tasks; still limited by limited embodied interaction data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey suggests improved few-shot instruction following due to LLM priors but no numerical results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-level analysis reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes general limitations: disconnect between semantic-level knowledge and physical execution (may 'understand' but fail to act) but not quantified for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey contrasts LLM-augmented models to vision-only; claims semantic advantages but no quantified head-to-head numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1950.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gato</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gato (A generalist agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early unified sequence model that tokenizes heterogeneous modalities (vision, language, state, actions) and is trained to perform many tasks including some embodied control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A generalist agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gato</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based unified model that tokenizes multimodal inputs (images, states, text) and outputs actions autoregressively; trained across diverse tasks (games, vision, robotics) as a generalist.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal multi-task supervised pretraining on heterogeneous datasets (including some robot data)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Mixture of datasets spanning control, vision, and language; includes action trajectories, observations, and command-like inputs, though robot-specific coverage limited relative to later robotics-focused datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>embodied tasks and generalist control (multi-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Varied tasks spanning manipulation, navigation, and non-robotic control; action spaces include discrete tokenized actions or low-dimensional continuous commands depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey frames Gato as a proof-of-concept showing modality tokenization can unify perception-language-action, but notes limitations in embodied specialization and real-world reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No direct sample-efficiency comparisons provided in survey for Gato; highlighted as early demonstration of joint training benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No detailed attention/visualization analyses for Gato included in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Survey notes tokenization fosters shared representations across modalities but no in-depth embedding space probes cited here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Gato demonstrates that token-level training can produce executables for embodied tasks, but survey emphasizes this was limited in scale and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer is limited when embodied task distributions differ substantially from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Gato shows some generalization across tasks in original work; survey mentions this qualitatively but gives no embodied zero-shot numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey references generalization limits and brittleness to domain shifts for early unified models like Gato.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared; emphasis is on multimodal joint training advantage over isolated models.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1950.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIMA (General robot manipulation with multimodal prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that tokenizes vision, language, and actions to enable zero-shot and few-shot manipulation via multimodal prompts, primarily evaluated in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vima: General robot manipulation with multimodal prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified tokenization of language, images and actions with a Transformer policy; uses multimodal prompting to specify task variants and supports strong cross-task generalization in simulated settings.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal imitation learning pretraining on large simulated demonstration datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale simulated manipulation demonstrations paired with language instructions; contains object identities, spatial relations, and action trajectories oriented to tabletop manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>simulated robotic manipulation and instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Language-conditioned pick-and-place and compositional manipulation tasks in simulation benchmarks (e.g., VIMA-BENCH), discrete/continuous actions tokenized for autoregressive decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports VIMA demonstrates language-to-action mapping in simulation, with pretraining data containing instruction-action pairs providing direct alignment of verbs, objects and placements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey cites VIMA's strong cross-task generalization in simulation but does not provide explicit comparative sample-efficiency numbers in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not summarize specific attention visualizations for VIMA; research emphasizes prompting effectiveness instead.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not detailed; survey attributes performance to joint tokenization rather than explicit embedding probes.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>VIMA's demonstrated generalization in simulated instruction-conditioned tasks provides indirect evidence that action semantics are grounded via paired demonstrations, though no mechanistic probes reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Best when simulation distribution and language prompts match pretraining; sim-to-real gap noted as a limitation when moving to physical robots.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey mentions zero-shot capabilities in simulation but gives no numerical novel/familiar splits.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey states VIMA supports strong zero-shot generalization (simulation) via multimodal prompts, but no numeric success rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Sim-to-real transfer remains challenging; potential negative effects when domain mismatch large, but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey implies joint vision-language pretraining outperforms vision-only in instruction-conditioned settings but no precise numbers shown.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1950.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-conditioned manipulation model that disentangles 'what' (object identity via CLIP) and 'where' pathways to generate pick-and-place heatmaps from image-language inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-pathway architecture using CLIP-derived visual features for 'what' (object recognition) and a spatial localization pathway for 'where' producing affordance/heatmap outputs for manipulation planning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining via CLIP on image-text pairs for the perception backbone, then task-specific supervised finetuning for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Image-text pairs (CLIP pretraining) providing object and attribute semantics, combined with manipulation datasets (task-specific demos) that contain action outcomes and spatial relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>language-conditioned pick-and-place manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tabletop pick-and-place tasks producing spatial heatmaps for pick and place locations (discrete heatmap outputs), typically evaluated in simulation and some real-robot scenarios with known object types.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Strong: CLIP features provide object-level semantic alignment between language and visual regions, enabling CLIPort to map instruction nouns to affordance heatmaps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey highlights CLIP-based semantic priors improve sample-efficiency for mapping instructions to affordances, but no numeric sample counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey notes CLIP-derived features focus on semantically relevant object regions when used for 'what' pathway, but no explicit attention maps shown in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Indirect evidence: CLIP's joint embedding space aligns language and object appearance which CLIPort leverages; no clustering statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Affordance heatmaps serve as explicit perceptual-to-action grounding, showing how language-specified objects map to spatial action priors.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Architecture explicitly separates semantics ('what') from spatial localization ('where'), implying higher-level semantic features assist affordance prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works well when object categories are covered by CLIP pretraining and when spatial relationships in target tasks match training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not provide numeric comparisons; CLIP's semantic generalization can support novel object descriptions to some extent.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIPort can leverage CLIP's zero-shot object recognition to inform affordance maps, enabling some zero-shot capability for novel object names, but no metrics given.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey warns about brittle alignment under noisy inputs but no CLIPort-specific negative-transfer numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey indicates CLIP-based multimodal pretraining outperforms vision-only features for semantic language-conditioned tasks, but no numerical comparison is included.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1950.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Policy (Visuomotor policy learning via action diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-model framing of policy learning that models action generation as conditional denoising, producing probabilistic distributions over trajectories for visuomotor control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion policy: Visuomotor policy learning via action diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Policy-as-generation: conditional diffusion (or flow-matching) that denoises action trajectories conditioned on visual and language context; models continuous action distributions and multimodal outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>typically supervised on trajectory datasets, sometimes combined with self-supervised video pretraining; framed as generative pretraining on action sequences</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trajectory datasets (robot demonstrations, bimanual datasets) and video-based data; contains dynamic action sequences, geometric/temporal structure and occasionally language-conditioned goals.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation and trajectory generation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Continuous control tasks requiring generation of physically consistent motion trajectories (e.g., bimanual manipulation, grasping, dexterous control) in simulation and real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports diffusion-based approaches can incorporate geometry-aware constraints (SE(3)) and semantic scene graphs, improving alignment between perception and action distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey describes diffusion's probabilistic modeling enabling multi-task generalization and few-shot adaptation in principle; no concrete numeric comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey mentions diffusion combined with Transformer attention (Diffusion Transformer) but does not detail attention visualization findings.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Some diffusion works incorporate semantic intermediates (scene graphs) implying structured embedding spaces, but the survey gives no quantitative embedding analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Diffusion models produce diverse, physically plausible trajectories and incorporate geometry priors (SE(3)), which is presented as evidence of grounding action semantics to physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Cognitive-inspired decompositions (e.g., reasoning module + diffusion policy) discussed, but no explicit layerwise feature breakdown in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Benefits from geometry-aware representations and when pretraining data contains varied physical interactions; temporal coherence can be fragile under dynamic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey suggests diffusion policies can support few-shot adaptation and zero-shot generalization for diverse trajectories in some settings, but without numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>High compute and dataset demands can limit practical transfer; temporal coherence fragility may cause degraded performance in dynamic novel environments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared qualitatively to trajectory regression approaches; diffusion provides probabilistic multi-modal outputs improving generalization, but no numeric head-to-head provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Survey highlights temporal conditioning and velocity fields (RDT-1B) as important for coherence; no training dynamics numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1950.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDT-1B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDT-1B (Trajectory-level diffusion foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale diffusion trajectory model that conditions on temporal and environmental context to enable zero-shot generalization in bimanual manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RDT-1B</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RDT-1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Scaled diffusion model (trajectory-level) with temporal conditioning and velocity-field modeling, designed for bimanual manipulation and temporally coherent action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>diffusion pretraining on large trajectory datasets (supervised), with temporal/environment conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale manipulation trajectories incorporating bimanual actions, temporal structure, and environment/context signals; likely contains diverse objects and motion primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>bimanual manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Complex multi-arm manipulation requiring temporally-coherent, coordinated continuous trajectories in simulation and physical robots.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey claims RDT-1B uses temporal and environmental conditioning for improved generalization; alignment to language is implied when language goals are provided but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes RDT-1B demonstrates zero-shot generalization benefits from trajectory-level diffusion pretraining but provides no sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not report attention analyses for RDT-1B.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Temporal and geometry-aware diffusion design claimed to yield physically consistent grasp/motion behaviors indicating grounding of action semantics to spatiotemporal cues.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer benefits from temporally-rich pretraining and inclusion of environmental context; lack of diversity or dynamics mismatch can hurt coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey asserts zero-shot generalization capability for bimanual manipulation tasks but gives no numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>High compute/dataset needs and fragile temporal coherence under environment shifts noted as limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared numerically; diffusion foundation models are described as more expressive than deterministic regressors.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Explicitly designed to model temporal coherence via velocity fields; survey notes temporal coherence as a central challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1950.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TinyVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TinyVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight diffusion-based VLA designed for data- and compute-efficient robotic manipulation via parameter-efficient tuning and compact backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TinyVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion-based policy with small-footprint backbone and parameter-efficient tuning (e.g., LoRA); aims to preserve manipulation capabilities while reducing training/inference compute.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>pretrained visual backbones plus diffusion policy finetuning with parameter-efficient updates</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Uses pre-trained vision backbones and smaller amounts of manipulation trajectory data; explicit content includes demonstrations and possibly video-derived motion priors.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>dexterous and contact-rich manipulation (efficient deployment)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Contact-rich manipulation tasks requiring real-time control; focus on reducing compute footprint for deployment on resource-constrained hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey claims TinyVLA leverages pretrained backbones for semantic priors but provides no quantitative alignment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey states TinyVLA can cut training costs to single-GPU scales using LoRA and other efficient strategies, implying improved practical sample-to-train-time efficiency but no explicit sample-efficiency numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Architectural design intended to retain grounding via pretrained backbones and diffusion policies, but no mechanistic evidence in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Designed to perform when pretrained semantic features align with downstream tasks; benefits from parameter-efficient tuning when data limited.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey mentions few-shot/fine-tuning practicality due to LoRA but no performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Trade-offs between parameter efficiency and ultimate performance may exist; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared; emphasis is on compute-efficiency rather than purely pretraining modality comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1950.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B-parameter VLA model trained on ~970k trajectories to provide a generalist vision-language-action policy across tasks and platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive multimodal Transformer (7B) trained on nearly a million trajectories combining vision, language and action tokens; aims at cross-platform generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>large-scale supervised pretraining on multimodal trajectory datasets (imitation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>~970k trajectory dataset aggregating diverse manipulation tasks, likely containing object-action pairs, language instructions, and state/action traces.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>generalist robotic manipulation across embodiments</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task manipulation and embodied control spanning various robot platforms; generates discrete/continuous actions depending on downstream reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey states OpenVLA surpasses some prior baselines (RT-2-X) suggesting that large, diverse multimodal pretraining improves semantic grounding to actions, though no numeric overlap analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey credits scale and dataset diversity for improved generalization but provides no explicit sample-efficiency numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not report attention visualization for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported; survey implies large-scale pretraining yields transferable priors.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Training on paired trajectories and instructions provides supervised grounding; practical success claims made qualitatively but not quantitatively here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer improves with dataset diversity and embodiment coverage; domain mismatch still a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No quantitative comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey suggests improved cross-task zero-shot capabilities due to scale, but no numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Scaling does not guarantee safety/robustness; survey flags alignment and safety issues but no OpenVLA-specific negative transfer numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>OpenVLA benefits from language conditioning compared to purely vision-only policies per qualitative survey claims; no numeric head-to-head here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1950.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open X-Embodiment (OXE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open X-Embodiment (OXE) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multi-institutional dataset aggregating 22 robot datasets covering 527 skills and 160,266 tasks to standardize formats and accelerate VLA training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open X-Embodiment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Open X-Embodiment (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model  a large-scale consolidated dataset for embodied robotics containing multimodal annotations (RGB-D, states, actions, language) across many tasks and robot platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>dataset used for supervised pretraining/fine-tuning of VLA models</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Consolidation of 22 robot datasets with demonstrations across diverse manipulation skills; includes multimodal sensor streams, action trajectories and language descriptions/labels; aims to improve cross-domain coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>broad embodied manipulation and skill transfer</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Collection includes tabletop manipulation, object interactions and a wide breadth of tasks intended to train generalist VLA models across multiple robot embodiments (real-world).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High  dataset explicitly includes paired language descriptions and action demonstrations, providing direct semantic-action overlap for pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes that joint training on BridgeData plus a small subset of unseen tasks can double success rates; for OXE the survey suggests aggregation improves transfer but gives no numeric sample-efficiency comparisons specific to OXE.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable (dataset), but survey highlights it enables studies of grounding and cross-domain transfer that could support attention analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not applicable directly; dataset facilitates embedding and representation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>By containing paired language and action demonstrations across many skills, OXE provides the primary supervised signal that supports action-language grounding in downstream VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Dataset supports hierarchical learning experiments but survey does not list concrete hierarchical-feature analyses done with OXE.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Large cross-institutional diversity in OXE helps transfer across embodiments; coverage gaps in long-tail tasks still remain.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Dataset scope includes many objects but survey does not provide explicit novel vs familiar performance splits.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey implies OXE enables better few-shot transfer due to diverse coverage, but no numeric examples provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not directly reported; survey mentions dataset aggregation can help but heterogeneity poses challenges for standardization and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Dataset contains language labels enabling vision-language pretraining; no direct comparative numbers provided by survey.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Dataset includes trajectory sequences enabling temporal modeling; no dynamics analyses reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1950.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BridgeData</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BridgeData</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-domain robot dataset (71 tasks across 10 environments) designed to boost generalization of robotic skills; joint training with small target-domain data can markedly improve success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BridgeData</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BridgeData (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset resource containing RGB-D demonstrations, language annotations and task metadata across multiple domains, intended to be used to pretrain or augment VLA policies for cross-domain transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>dataset used for supervised pretraining/fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>71 tasks across 10 environments with multimodal sensory streams and language instructions; contains object manipulations, action sequences and environment context.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>cross-domain robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Varied manipulation tasks spanning multiple environments; supports generalization studies via training on source tasks and evaluating on held-out target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicit pairing of language instructions with demonstrations fosters semantic alignment; survey cites experiments where joint training doubled success rates versus target-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports: joint training on BridgeData plus a small subset (e.g., 50 tasks) of unseen target-domain tasks can double success rates compared to using target-domain data alone (qualitative/relative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Quantitative claim in survey: combining BridgeData with a small number (~50) of target tasks doubled success vs training on target alone, implying substantial sample-efficiency gains from cross-domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in survey for BridgeData itself; dataset enables such analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paired instructions and demonstrations in BridgeData provide supervised grounding signals enabling learned mapping from language to action.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Survey indicates cross-domain diversity and joint training with a small set of target tasks improves transfer; benefits degrade when target domains are very different.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not quantified beyond generalization claims.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>BridgeData supports few-shot transfer: example given where adding ~50 target tasks yielded 2x success relative to target-only training, implying few-shot effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not specifically reported for BridgeData; potential domain mismatch risks noted broadly in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Dataset enables VLM+language-conditioned training; survey does not provide direct comparisons but implies language labels aid transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Dataset contains trajectories enabling temporal modeling; no explicit temporal analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e1950.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DROID (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DROID (Large in-the-wild robot manipulation dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multimodal in-the-wild robot manipulation dataset containing >150k trajectories across 1k+ objects and tasks, intended to support foundation-scale VLA pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DROID</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DROID (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset resource with RGB-D, language, state/action traces across many tasks and objects for training and benchmarking VLA foundation/policy models.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>dataset used for large-scale supervised pretraining of VLA models</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>150k+ real or in-the-wild trajectories covering over 1,000 objects and tasks; multimodal annotations include RGB-D, language descriptions and low-dimensional states.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>large-scale manipulation foundation model pretraining and transfer</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Supports training of generalist manipulation policies and evaluation on diverse tasks and objects in real or realistic settings.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Dataset includes paired language and manipulation trajectories providing direct semantic-action alignment; survey emphasizes its scale for foundation-model training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes the dataset enables foundation-scale pretraining that improves downstream transfer, but no explicit sample-efficiency numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Large volume of paired language/action data offers grounding signal for downstream VLA models though specific grounding studies not summarized.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Scale and diversity support transfer; sim-to-real and distributional shifts remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Dataset intended to enable improved zero/few-shot transfer for foundation models, but survey provides no numeric examples.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not discussed specifically for DROID.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Dataset's inclusion of language makes it suited for VLM/VLA pretraining; direct comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Contains trajectory sequences enabling temporal modeling; no dynamics analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e1950.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (open-source Vision-Language-Action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B VLA trained on 970k trajectories claimed to surpass RT-2-X, intended as a generalist VLA for multiple embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer trained on large-scale trajectory datasets combining vision, language, and action tokens for cross-platform robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>supervised imitation pretraining on large multimodal trajectory datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>~970k robot trajectories with visual, state, action, and instruction annotations, focused on manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>generalist robotic manipulation and embodied control</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task manipulation across diverse settings and embodiments (real/sim), with autoregressive action decoding reconstructable into continuous controls.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey claims OpenVLA achieves improved alignment via large-scale training, but does not report quantitative alignment measures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey attributes improved cross-task generalization to large dataset scale; explicit comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not discussed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Grounding implied via paired instruction-trajectory pretraining; survey-level claims of better performance vs prior models.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Benefits from dataset diversity and embodiment coverage; heterogeneity remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey suggests improved zero/few-shot transfer but no numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Claimed advantage of vision+language pretraining qualitatively, no numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e1950.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language-Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastively-trained vision-language model mapping images and text into a shared embedding space enabling zero-shot recognition and providing strong semantic priors for downstream VLA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-encoder vision-language model trained with contrastive objectives on image-text pairs; provides transferable semantic embeddings used as perception backbone or promptable representations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language contrastive pretraining on image-text pairs</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Massive image-caption pairs scraped from web; contains object names, attributes, and many forms of object-related language though not explicit robot action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>used as perceptual backbone in language-conditioned manipulation (e.g., CLIPort pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Provides semantic features for mapping language instructions to object-focused affordances in tabletop/robotic tasks; used alongside spatial modules to produce actionable heatmaps.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High for object and attribute semantics; provides robust language-image alignment useful for object grounding in manipulation tasks, though action semantics less directly encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey suggests CLIP features improve downstream sample-efficiency for language-conditioned policies but gives no numeric measures.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in survey; CLIP embeddings are known to focus on semantically-relevant regions but survey does not include visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Survey alludes to CLIP's joint embedding space enabling retrieval and zero-shot labeling, used as semantic priors in VLA approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Provides object-level grounding which downstream modules convert to affordances/actions (e.g., CLIPort heatmaps), but CLIP itself lacks direct action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>CLIP supplies high-level semantic features; lower-level spatial/geom reasoning typically added by downstream modules.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when object vocabulary overlaps with CLIP pretraining; less direct benefit for fine motor primitives absent from captions.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>CLIP supports zero-shot recognition of novel object names to some extent, which downstream VLA systems can leverage; no numeric splits in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIP's zero-shot capabilities are explicitly used in language-conditioned manipulation pipelines for object selection/identification.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>CLIP may not encode action affordances, causing downstream modules to require additional spatial grounding; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP's vision-language pretraining provides superior semantic grounding compared to vision-only backbones for language-conditioned tasks per survey discussion (no numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1950.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e1950.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimpleVLA-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimpleVLA-RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online RL approach that trains a VLA policy from a single trajectory using binary rewards (0/1), demonstrating extreme sample-efficiency in certain online RL regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimpleVLA-RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning based VLA variant that uses minimal reward signal (binary success) and online updating to train from very small numbers of trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>online RL finetuning from small demonstration set (often one trajectory) possibly on top of pretrained VLM/VLA backbones</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>One or very few task-specific demonstration trajectories; language conditioning may be present depending on experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>sample-efficient online RL for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Online robotic tasks where a single demonstration plus sparse success/failure reward is used to train a policy; action space continuous and learned via RL updates.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey presents SimpleVLA-RL as combining SFT stability with RL exploration; alignment relies on starting from pretrained multimodal priors if available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey highlights SimpleVLA-RL trains with one trajectory and binary rewards to reach performance comparable to full supervised fine-tuning, indicating dramatic sample efficiency in certain setups (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Method shows that reinforcement updates from sparse signals can refine action grounding when combined with pretrained priors, but mechanistic probes not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works when environment and task structure permit signal from single trajectory to guide policy and when pretrained priors provide strong initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Explicit few-shot claim: trains from a single trajectory (1-shot) in online RL setting; survey reports performance comparable to supervised fine-tuning in those experiments (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey warns that sparse-reward RL can be unstable and depends strongly on initialization; no quantified negative-transfer reported for SimpleVLA-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey frames SimpleVLA-RL as combining SFT (which may be multimodal) with RL; direct vision-only comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vima: General robot manipulation with multimodal prompts <em>(Rating: 2)</em></li>
                <li>Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion <em>(Rating: 2)</em></li>
                <li>CLIPort <em>(Rating: 2)</em></li>
                <li>Open X-Embodiment <em>(Rating: 2)</em></li>
                <li>BridgeData <em>(Rating: 2)</em></li>
                <li>DROID <em>(Rating: 2)</em></li>
                <li>Gato: A generalist agent <em>(Rating: 1)</em></li>
                <li>RDT-1B <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1950",
    "paper_id": "paper-281496824",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "RT-1",
            "name_full": "RT-1 (Robotics Transformer 1)",
            "brief_description": "An autoregressive, multimodal Transformer trained on large-scale real-world robotic demonstrations to map images and language instructions to action tokens for robot manipulation.",
            "citation_title": "Robotics transformer for real-world control at scale",
            "mention_or_use": "mention",
            "model_name": "RT-1",
            "model_description": "Autoregressive Transformer policy that tokenizes vision, language, and robot state into a unified sequence and decodes action tokens; multimodal fusion via FiLM-style conditioning; trained as a generalist manipulation policy.",
            "pretraining_type": "supervised imitation pretraining on robot demonstration datasets (real-world)",
            "pretraining_data_description": "Large-scale real robot demonstration dataset (cited ~130k demonstrations in survey) containing paired images, low-dimensional states, action trajectories and natural language labels/instructions; primarily tabletop/manipulation skills.",
            "target_task_name": "robotic manipulation (multi-task real-world)",
            "target_task_description": "Multi-task manipulation (pick/place, grasping, assembly-type skills) on real physical robot arms; continuous low-level control reconstructed from decoded action tokens; tasks are real-world demonstrations rather than synthetic simulation.",
            "semantic_alignment": "Survey reports strong semantic alignment between language instructions and action tokens due to paired demonstrations; pretraining data contains object descriptions and action semantics aligned to manipulation primitives.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey emphasizes large-data pretraining (130k demos) improves generalization and zero/few-shot transfer compared to training from scratch, but no precise sample-count comparisons reported in this survey.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey notes multimodal fusion (FiLM) and tokenization help alignment but does not present attention-visualization analyses for RT-1 specifically.",
            "embedding_space_analysis": "Not reported in detail in survey for RT-1; survey mentions that joint tokenization yields shared representations aiding transfer.",
            "action_grounding_evidence": "Evidence is descriptive: RT-1 is reported to map language to executable action tokens from paired demonstrations, implying grounding between instruction verbs and motor trajectories via supervised signals.",
            "hierarchical_features_evidence": "No explicit layer- or hierarchy-level feature analyses reported for RT-1 in the survey.",
            "transfer_conditions": "Works best when target tasks share objects/actions with demonstration distribution; scaling data size and diversity improves cross-task transfer according to survey.",
            "novel_vs_familiar_objects": "Survey does not provide quantitative split results for novel vs familiar objects for RT-1, only qualitative claims of improved generalization with large real-data pretraining.",
            "zero_shot_or_few_shot": "Survey indicates RT-1-style pretraining supports improved few-shot promptability and some zero-shot transfer to related tasks, but no numeric zero-shot metrics provided.",
            "layer_analysis": "No detailed ablation/layer-freezing analysis reported in survey for RT-1.",
            "negative_transfer_evidence": "Survey notes generalization limits and sim-to-real gaps broadly, but no explicit negative transfer case quantified for RT-1.",
            "comparison_to_vision_only": "Survey contrasts VLM+language-integrated pretraining to vision-only but does not provide direct numeric comparisons for RT-1 versus vision-only baselines.",
            "temporal_dynamics": "No explicit description of representation dynamics during fine-tuning for RT-1 in the survey.",
            "dimensionality_analysis": "Not reported for RT-1 in this survey.",
            "uuid": "e1950.0"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Robotics Transformer 2)",
            "brief_description": "A follow-up RT model that transfers web-scale vision-language knowledge into robotic control (autoreg. Transformer trained with web-scale pretraining plus robot demos).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Autoregressive multimodal Transformer that integrates pretrained vision-language knowledge (web-scale) with robot action-token decoding to improve open-vocabulary grasping and manipulation.",
            "pretraining_type": "vision-language pretraining on web-scale image-text plus supervised fine-tuning on robot demonstrations",
            "pretraining_data_description": "Web-scale image-text (VLM) knowledge plus robot demonstration datasets; contains object labels, captions and likely action-descriptive text from web-scale corpora used to inject semantic priors.",
            "target_task_name": "robotic manipulation (open-vocabulary grasping and generalization)",
            "target_task_description": "Real-world manipulation tasks with varied objects and open-vocabulary language prompts; continuous/parametric action reconstruction from tokens.",
            "semantic_alignment": "Survey describes RT-2 as leveraging web knowledge to increase semantic grounding (open-vocabulary), implying overlap in object names and affordance descriptions but no quantified overlap.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey claims web-scale pretraining reduces reliance on robot-specific pretraining but does not give precise sample-efficiency numbers.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No specific attention or saliency analyses presented in the survey for RT-2.",
            "embedding_space_analysis": "Survey suggests pretrained VLM features provide semantic priors but gives no embedding probes for RT-2.",
            "action_grounding_evidence": "Qualitative evidence: RT-2 shows improved open-vocabulary behavior consistent with semantic priors from VLMs; explicit grounding experiments not detailed in survey.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Works better when language pretraining covers object/action vocabulary relevant to downstream tasks; domain mismatch remains a limitation.",
            "novel_vs_familiar_objects": "Survey mentions RT-2 improves open-vocabulary handling but provides no quantitative novel/familiar object split.",
            "zero_shot_or_few_shot": "Survey indicates stronger zero/few-shot recognition of language instructions relative to vision-only baselines, but no numbers.",
            "layer_analysis": "No layer ablation details reported in this survey.",
            "negative_transfer_evidence": "Survey warns of brittle multimodal alignment and potential failure under noisy inputs, but no RT-2-specific negative transfer quantification.",
            "comparison_to_vision_only": "Survey frames RT-2 as better than vision-only due to injected language priors, but without numeric comparison.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.1"
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E (Embodied PaLM)",
            "brief_description": "An embodied multimodal model that extends large language model knowledge into embodied control by adding action tokens and conditioning on visual/state inputs for robotic tasks.",
            "citation_title": "Palm-e: An embodied multimodal language model",
            "mention_or_use": "mention",
            "model_name": "PaLM-E",
            "model_description": "LLM-augmented embodied model (language backbone extended with perception and action tokens); conditions on images and states to autoregressively generate action sequences; integrates language knowledge into control.",
            "pretraining_type": "pretrained text-only LLM extended with multimodal finetuning (vision+robot demos)",
            "pretraining_data_description": "Large-text LLM pretraining (language knowledge), then multimodal finetuning on paired image/state-action demonstrations; contains object descriptions and instruction-style data but limited robot-interaction experience prior to finetuning.",
            "target_task_name": "robotic manipulation and instruction following",
            "target_task_description": "Real-world and simulated manipulation tasks with natural language instructions, mapping semantic goals to action sequences; action space continuous and discretized into tokens for decoding.",
            "semantic_alignment": "Survey describes PaLM-E as integrating pretrained language knowledge into embodied policies, increasing semantic alignment between instructions and actions though lacking direct physical interaction experience.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey indicates language priors help reduce required robot-specific data but does not report numeric sample-efficiency figures for PaLM-E.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No specific attention-analysis reported in survey for PaLM-E.",
            "embedding_space_analysis": "Not detailed in survey; described qualitatively as benefiting from LLM semantic priors.",
            "action_grounding_evidence": "Survey discusses PaLM-E's ability to map language to actions via supervised demonstrations; explicit experiments probing grounding mechanisms not detailed here.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Benefits when linguistic priors are relevant to target tasks; still limited by limited embodied interaction data.",
            "novel_vs_familiar_objects": "Not reported quantitatively.",
            "zero_shot_or_few_shot": "Survey suggests improved few-shot instruction following due to LLM priors but no numerical results provided.",
            "layer_analysis": "No layer-level analysis reported in survey.",
            "negative_transfer_evidence": "Survey notes general limitations: disconnect between semantic-level knowledge and physical execution (may 'understand' but fail to act) but not quantified for PaLM-E.",
            "comparison_to_vision_only": "Survey contrasts LLM-augmented models to vision-only; claims semantic advantages but no quantified head-to-head numbers.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.2"
        },
        {
            "name_short": "Gato",
            "name_full": "Gato (A generalist agent)",
            "brief_description": "An early unified sequence model that tokenizes heterogeneous modalities (vision, language, state, actions) and is trained to perform many tasks including some embodied control tasks.",
            "citation_title": "A generalist agent",
            "mention_or_use": "mention",
            "model_name": "Gato",
            "model_description": "Transformer-based unified model that tokenizes multimodal inputs (images, states, text) and outputs actions autoregressively; trained across diverse tasks (games, vision, robotics) as a generalist.",
            "pretraining_type": "multimodal multi-task supervised pretraining on heterogeneous datasets (including some robot data)",
            "pretraining_data_description": "Mixture of datasets spanning control, vision, and language; includes action trajectories, observations, and command-like inputs, though robot-specific coverage limited relative to later robotics-focused datasets.",
            "target_task_name": "embodied tasks and generalist control (multi-domain)",
            "target_task_description": "Varied tasks spanning manipulation, navigation, and non-robotic control; action spaces include discrete tokenized actions or low-dimensional continuous commands depending on task.",
            "semantic_alignment": "Survey frames Gato as a proof-of-concept showing modality tokenization can unify perception-language-action, but notes limitations in embodied specialization and real-world reliability.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "No direct sample-efficiency comparisons provided in survey for Gato; highlighted as early demonstration of joint training benefits.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No detailed attention/visualization analyses for Gato included in this survey.",
            "embedding_space_analysis": "Survey notes tokenization fosters shared representations across modalities but no in-depth embedding space probes cited here.",
            "action_grounding_evidence": "Gato demonstrates that token-level training can produce executables for embodied tasks, but survey emphasizes this was limited in scale and robustness.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Transfer is limited when embodied task distributions differ substantially from training data.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Gato shows some generalization across tasks in original work; survey mentions this qualitatively but gives no embodied zero-shot numbers.",
            "layer_analysis": "Not reported in survey.",
            "negative_transfer_evidence": "Survey references generalization limits and brittleness to domain shifts for early unified models like Gato.",
            "comparison_to_vision_only": "Not directly compared; emphasis is on multimodal joint training advantage over isolated models.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.3"
        },
        {
            "name_short": "VIMA",
            "name_full": "VIMA (General robot manipulation with multimodal prompts)",
            "brief_description": "A vision-language-action model that tokenizes vision, language, and actions to enable zero-shot and few-shot manipulation via multimodal prompts, primarily evaluated in simulation.",
            "citation_title": "Vima: General robot manipulation with multimodal prompts",
            "mention_or_use": "mention",
            "model_name": "VIMA",
            "model_description": "Unified tokenization of language, images and actions with a Transformer policy; uses multimodal prompting to specify task variants and supports strong cross-task generalization in simulated settings.",
            "pretraining_type": "multimodal imitation learning pretraining on large simulated demonstration datasets",
            "pretraining_data_description": "Large-scale simulated manipulation demonstrations paired with language instructions; contains object identities, spatial relations, and action trajectories oriented to tabletop manipulation.",
            "target_task_name": "simulated robotic manipulation and instruction following",
            "target_task_description": "Language-conditioned pick-and-place and compositional manipulation tasks in simulation benchmarks (e.g., VIMA-BENCH), discrete/continuous actions tokenized for autoregressive decoding.",
            "semantic_alignment": "Survey reports VIMA demonstrates language-to-action mapping in simulation, with pretraining data containing instruction-action pairs providing direct alignment of verbs, objects and placements.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey cites VIMA's strong cross-task generalization in simulation but does not provide explicit comparative sample-efficiency numbers in this text.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey does not summarize specific attention visualizations for VIMA; research emphasizes prompting effectiveness instead.",
            "embedding_space_analysis": "Not detailed; survey attributes performance to joint tokenization rather than explicit embedding probes.",
            "action_grounding_evidence": "VIMA's demonstrated generalization in simulated instruction-conditioned tasks provides indirect evidence that action semantics are grounded via paired demonstrations, though no mechanistic probes reported here.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Best when simulation distribution and language prompts match pretraining; sim-to-real gap noted as a limitation when moving to physical robots.",
            "novel_vs_familiar_objects": "Survey mentions zero-shot capabilities in simulation but gives no numerical novel/familiar splits.",
            "zero_shot_or_few_shot": "Survey states VIMA supports strong zero-shot generalization (simulation) via multimodal prompts, but no numeric success rates provided.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Sim-to-real transfer remains challenging; potential negative effects when domain mismatch large, but not quantified here.",
            "comparison_to_vision_only": "Survey implies joint vision-language pretraining outperforms vision-only in instruction-conditioned settings but no precise numbers shown.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.4"
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort",
            "brief_description": "A language-conditioned manipulation model that disentangles 'what' (object identity via CLIP) and 'where' pathways to generate pick-and-place heatmaps from image-language inputs.",
            "citation_title": "CLIPort",
            "mention_or_use": "mention",
            "model_name": "CLIPort",
            "model_description": "Two-pathway architecture using CLIP-derived visual features for 'what' (object recognition) and a spatial localization pathway for 'where' producing affordance/heatmap outputs for manipulation planning.",
            "pretraining_type": "vision-language pretraining via CLIP on image-text pairs for the perception backbone, then task-specific supervised finetuning for manipulation",
            "pretraining_data_description": "Image-text pairs (CLIP pretraining) providing object and attribute semantics, combined with manipulation datasets (task-specific demos) that contain action outcomes and spatial relationships.",
            "target_task_name": "language-conditioned pick-and-place manipulation",
            "target_task_description": "Tabletop pick-and-place tasks producing spatial heatmaps for pick and place locations (discrete heatmap outputs), typically evaluated in simulation and some real-robot scenarios with known object types.",
            "semantic_alignment": "Strong: CLIP features provide object-level semantic alignment between language and visual regions, enabling CLIPort to map instruction nouns to affordance heatmaps.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey highlights CLIP-based semantic priors improve sample-efficiency for mapping instructions to affordances, but no numeric sample counts provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey notes CLIP-derived features focus on semantically relevant object regions when used for 'what' pathway, but no explicit attention maps shown in the survey.",
            "embedding_space_analysis": "Indirect evidence: CLIP's joint embedding space aligns language and object appearance which CLIPort leverages; no clustering statistics provided.",
            "action_grounding_evidence": "Affordance heatmaps serve as explicit perceptual-to-action grounding, showing how language-specified objects map to spatial action priors.",
            "hierarchical_features_evidence": "Architecture explicitly separates semantics ('what') from spatial localization ('where'), implying higher-level semantic features assist affordance prediction.",
            "transfer_conditions": "Works well when object categories are covered by CLIP pretraining and when spatial relationships in target tasks match training distribution.",
            "novel_vs_familiar_objects": "Survey does not provide numeric comparisons; CLIP's semantic generalization can support novel object descriptions to some extent.",
            "zero_shot_or_few_shot": "CLIPort can leverage CLIP's zero-shot object recognition to inform affordance maps, enabling some zero-shot capability for novel object names, but no metrics given.",
            "layer_analysis": "Not provided in survey.",
            "negative_transfer_evidence": "Survey warns about brittle alignment under noisy inputs but no CLIPort-specific negative-transfer numbers.",
            "comparison_to_vision_only": "Survey indicates CLIP-based multimodal pretraining outperforms vision-only features for semantic language-conditioned tasks, but no numerical comparison is included.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.5"
        },
        {
            "name_short": "Diffusion Policy",
            "name_full": "Diffusion Policy (Visuomotor policy learning via action diffusion)",
            "brief_description": "A diffusion-model framing of policy learning that models action generation as conditional denoising, producing probabilistic distributions over trajectories for visuomotor control.",
            "citation_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "mention_or_use": "mention",
            "model_name": "Diffusion Policy",
            "model_description": "Policy-as-generation: conditional diffusion (or flow-matching) that denoises action trajectories conditioned on visual and language context; models continuous action distributions and multimodal outcomes.",
            "pretraining_type": "typically supervised on trajectory datasets, sometimes combined with self-supervised video pretraining; framed as generative pretraining on action sequences",
            "pretraining_data_description": "Trajectory datasets (robot demonstrations, bimanual datasets) and video-based data; contains dynamic action sequences, geometric/temporal structure and occasionally language-conditioned goals.",
            "target_task_name": "robotic manipulation and trajectory generation",
            "target_task_description": "Continuous control tasks requiring generation of physically consistent motion trajectories (e.g., bimanual manipulation, grasping, dexterous control) in simulation and real robots.",
            "semantic_alignment": "Survey reports diffusion-based approaches can incorporate geometry-aware constraints (SE(3)) and semantic scene graphs, improving alignment between perception and action distributions.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey describes diffusion's probabilistic modeling enabling multi-task generalization and few-shot adaptation in principle; no concrete numeric comparisons provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey mentions diffusion combined with Transformer attention (Diffusion Transformer) but does not detail attention visualization findings.",
            "embedding_space_analysis": "Some diffusion works incorporate semantic intermediates (scene graphs) implying structured embedding spaces, but the survey gives no quantitative embedding analyses.",
            "action_grounding_evidence": "Diffusion models produce diverse, physically plausible trajectories and incorporate geometry priors (SE(3)), which is presented as evidence of grounding action semantics to physical constraints.",
            "hierarchical_features_evidence": "Cognitive-inspired decompositions (e.g., reasoning module + diffusion policy) discussed, but no explicit layerwise feature breakdown in survey.",
            "transfer_conditions": "Benefits from geometry-aware representations and when pretraining data contains varied physical interactions; temporal coherence can be fragile under dynamic changes.",
            "novel_vs_familiar_objects": "Not quantified in survey.",
            "zero_shot_or_few_shot": "Survey suggests diffusion policies can support few-shot adaptation and zero-shot generalization for diverse trajectories in some settings, but without numeric metrics.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "High compute and dataset demands can limit practical transfer; temporal coherence fragility may cause degraded performance in dynamic novel environments.",
            "comparison_to_vision_only": "Compared qualitatively to trajectory regression approaches; diffusion provides probabilistic multi-modal outputs improving generalization, but no numeric head-to-head provided.",
            "temporal_dynamics": "Survey highlights temporal conditioning and velocity fields (RDT-1B) as important for coherence; no training dynamics numbers.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.6"
        },
        {
            "name_short": "RDT-1B",
            "name_full": "RDT-1B (Trajectory-level diffusion foundation model)",
            "brief_description": "A large-scale diffusion trajectory model that conditions on temporal and environmental context to enable zero-shot generalization in bimanual manipulation.",
            "citation_title": "RDT-1B",
            "mention_or_use": "mention",
            "model_name": "RDT-1B",
            "model_description": "Scaled diffusion model (trajectory-level) with temporal conditioning and velocity-field modeling, designed for bimanual manipulation and temporally coherent action generation.",
            "pretraining_type": "diffusion pretraining on large trajectory datasets (supervised), with temporal/environment conditioning",
            "pretraining_data_description": "Large-scale manipulation trajectories incorporating bimanual actions, temporal structure, and environment/context signals; likely contains diverse objects and motion primitives.",
            "target_task_name": "bimanual manipulation",
            "target_task_description": "Complex multi-arm manipulation requiring temporally-coherent, coordinated continuous trajectories in simulation and physical robots.",
            "semantic_alignment": "Survey claims RDT-1B uses temporal and environmental conditioning for improved generalization; alignment to language is implied when language goals are provided but not quantified here.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey notes RDT-1B demonstrates zero-shot generalization benefits from trajectory-level diffusion pretraining but provides no sample counts.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey does not report attention analyses for RDT-1B.",
            "embedding_space_analysis": "Not reported in survey.",
            "action_grounding_evidence": "Temporal and geometry-aware diffusion design claimed to yield physically consistent grasp/motion behaviors indicating grounding of action semantics to spatiotemporal cues.",
            "hierarchical_features_evidence": "Not detailed.",
            "transfer_conditions": "Transfer benefits from temporally-rich pretraining and inclusion of environmental context; lack of diversity or dynamics mismatch can hurt coherence.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Survey asserts zero-shot generalization capability for bimanual manipulation tasks but gives no numeric results.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "High compute/dataset needs and fragile temporal coherence under environment shifts noted as limitations.",
            "comparison_to_vision_only": "Not directly compared numerically; diffusion foundation models are described as more expressive than deterministic regressors.",
            "temporal_dynamics": "Explicitly designed to model temporal coherence via velocity fields; survey notes temporal coherence as a central challenge.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.7"
        },
        {
            "name_short": "TinyVLA",
            "name_full": "TinyVLA",
            "brief_description": "A lightweight diffusion-based VLA designed for data- and compute-efficient robotic manipulation via parameter-efficient tuning and compact backbones.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "TinyVLA",
            "model_description": "Diffusion-based policy with small-footprint backbone and parameter-efficient tuning (e.g., LoRA); aims to preserve manipulation capabilities while reducing training/inference compute.",
            "pretraining_type": "pretrained visual backbones plus diffusion policy finetuning with parameter-efficient updates",
            "pretraining_data_description": "Uses pre-trained vision backbones and smaller amounts of manipulation trajectory data; explicit content includes demonstrations and possibly video-derived motion priors.",
            "target_task_name": "dexterous and contact-rich manipulation (efficient deployment)",
            "target_task_description": "Contact-rich manipulation tasks requiring real-time control; focus on reducing compute footprint for deployment on resource-constrained hardware.",
            "semantic_alignment": "Survey claims TinyVLA leverages pretrained backbones for semantic priors but provides no quantitative alignment metrics.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey states TinyVLA can cut training costs to single-GPU scales using LoRA and other efficient strategies, implying improved practical sample-to-train-time efficiency but no explicit sample-efficiency numbers.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Architectural design intended to retain grounding via pretrained backbones and diffusion policies, but no mechanistic evidence in survey.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Designed to perform when pretrained semantic features align with downstream tasks; benefits from parameter-efficient tuning when data limited.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Survey mentions few-shot/fine-tuning practicality due to LoRA but no performance numbers.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Trade-offs between parameter efficiency and ultimate performance may exist; not quantified.",
            "comparison_to_vision_only": "Not directly compared; emphasis is on compute-efficiency rather than purely pretraining modality comparisons.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.8"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source 7B-parameter VLA model trained on ~970k trajectories to provide a generalist vision-language-action policy across tasks and platforms.",
            "citation_title": "OpenVLA",
            "mention_or_use": "mention",
            "model_name": "OpenVLA",
            "model_description": "Large autoregressive multimodal Transformer (7B) trained on nearly a million trajectories combining vision, language and action tokens; aims at cross-platform generalization.",
            "pretraining_type": "large-scale supervised pretraining on multimodal trajectory datasets (imitation learning)",
            "pretraining_data_description": "~970k trajectory dataset aggregating diverse manipulation tasks, likely containing object-action pairs, language instructions, and state/action traces.",
            "target_task_name": "generalist robotic manipulation across embodiments",
            "target_task_description": "Multi-task manipulation and embodied control spanning various robot platforms; generates discrete/continuous actions depending on downstream reconstruction.",
            "semantic_alignment": "Survey states OpenVLA surpasses some prior baselines (RT-2-X) suggesting that large, diverse multimodal pretraining improves semantic grounding to actions, though no numeric overlap analyses provided.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey credits scale and dataset diversity for improved generalization but provides no explicit sample-efficiency numbers.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey does not report attention visualization for OpenVLA.",
            "embedding_space_analysis": "Not reported; survey implies large-scale pretraining yields transferable priors.",
            "action_grounding_evidence": "Training on paired trajectories and instructions provides supervised grounding; practical success claims made qualitatively but not quantitatively here.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Transfer improves with dataset diversity and embodiment coverage; domain mismatch still a limiting factor.",
            "novel_vs_familiar_objects": "No quantitative comparison provided.",
            "zero_shot_or_few_shot": "Survey suggests improved cross-task zero-shot capabilities due to scale, but no numeric metrics.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Scaling does not guarantee safety/robustness; survey flags alignment and safety issues but no OpenVLA-specific negative transfer numbers.",
            "comparison_to_vision_only": "OpenVLA benefits from language conditioning compared to purely vision-only policies per qualitative survey claims; no numeric head-to-head here.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.9"
        },
        {
            "name_short": "Open X-Embodiment (OXE)",
            "name_full": "Open X-Embodiment (OXE) dataset",
            "brief_description": "A large multi-institutional dataset aggregating 22 robot datasets covering 527 skills and 160,266 tasks to standardize formats and accelerate VLA training and evaluation.",
            "citation_title": "Open X-Embodiment",
            "mention_or_use": "mention",
            "model_name": "Open X-Embodiment (dataset)",
            "model_description": "Not a model  a large-scale consolidated dataset for embodied robotics containing multimodal annotations (RGB-D, states, actions, language) across many tasks and robot platforms.",
            "pretraining_type": "dataset used for supervised pretraining/fine-tuning of VLA models",
            "pretraining_data_description": "Consolidation of 22 robot datasets with demonstrations across diverse manipulation skills; includes multimodal sensor streams, action trajectories and language descriptions/labels; aims to improve cross-domain coverage.",
            "target_task_name": "broad embodied manipulation and skill transfer",
            "target_task_description": "Collection includes tabletop manipulation, object interactions and a wide breadth of tasks intended to train generalist VLA models across multiple robot embodiments (real-world).",
            "semantic_alignment": "High  dataset explicitly includes paired language descriptions and action demonstrations, providing direct semantic-action overlap for pretraining.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey notes that joint training on BridgeData plus a small subset of unseen tasks can double success rates; for OXE the survey suggests aggregation improves transfer but gives no numeric sample-efficiency comparisons specific to OXE.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not applicable (dataset), but survey highlights it enables studies of grounding and cross-domain transfer that could support attention analyses.",
            "embedding_space_analysis": "Not applicable directly; dataset facilitates embedding and representation studies.",
            "action_grounding_evidence": "By containing paired language and action demonstrations across many skills, OXE provides the primary supervised signal that supports action-language grounding in downstream VLA models.",
            "hierarchical_features_evidence": "Dataset supports hierarchical learning experiments but survey does not list concrete hierarchical-feature analyses done with OXE.",
            "transfer_conditions": "Large cross-institutional diversity in OXE helps transfer across embodiments; coverage gaps in long-tail tasks still remain.",
            "novel_vs_familiar_objects": "Dataset scope includes many objects but survey does not provide explicit novel vs familiar performance splits.",
            "zero_shot_or_few_shot": "Survey implies OXE enables better few-shot transfer due to diverse coverage, but no numeric examples provided.",
            "layer_analysis": null,
            "negative_transfer_evidence": "Not directly reported; survey mentions dataset aggregation can help but heterogeneity poses challenges for standardization and benchmarking.",
            "comparison_to_vision_only": "Dataset contains language labels enabling vision-language pretraining; no direct comparative numbers provided by survey.",
            "temporal_dynamics": "Dataset includes trajectory sequences enabling temporal modeling; no dynamics analyses reported in survey.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.10"
        },
        {
            "name_short": "BridgeData",
            "name_full": "BridgeData",
            "brief_description": "A multi-domain robot dataset (71 tasks across 10 environments) designed to boost generalization of robotic skills; joint training with small target-domain data can markedly improve success rates.",
            "citation_title": "BridgeData",
            "mention_or_use": "mention",
            "model_name": "BridgeData (dataset)",
            "model_description": "Dataset resource containing RGB-D demonstrations, language annotations and task metadata across multiple domains, intended to be used to pretrain or augment VLA policies for cross-domain transfer.",
            "pretraining_type": "dataset used for supervised pretraining/fine-tuning",
            "pretraining_data_description": "71 tasks across 10 environments with multimodal sensory streams and language instructions; contains object manipulations, action sequences and environment context.",
            "target_task_name": "cross-domain robotic manipulation",
            "target_task_description": "Varied manipulation tasks spanning multiple environments; supports generalization studies via training on source tasks and evaluating on held-out target tasks.",
            "semantic_alignment": "Explicit pairing of language instructions with demonstrations fosters semantic alignment; survey cites experiments where joint training doubled success rates versus target-only training.",
            "performance_with_language_pretraining": "Survey reports: joint training on BridgeData plus a small subset (e.g., 50 tasks) of unseen target-domain tasks can double success rates compared to using target-domain data alone (qualitative/relative claim).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Quantitative claim in survey: combining BridgeData with a small number (~50) of target tasks doubled success vs training on target alone, implying substantial sample-efficiency gains from cross-domain pretraining.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not reported in survey for BridgeData itself; dataset enables such analyses.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Paired instructions and demonstrations in BridgeData provide supervised grounding signals enabling learned mapping from language to action.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Survey indicates cross-domain diversity and joint training with a small set of target tasks improves transfer; benefits degrade when target domains are very different.",
            "novel_vs_familiar_objects": "Not quantified beyond generalization claims.",
            "zero_shot_or_few_shot": "BridgeData supports few-shot transfer: example given where adding ~50 target tasks yielded 2x success relative to target-only training, implying few-shot effectiveness.",
            "layer_analysis": null,
            "negative_transfer_evidence": "Not specifically reported for BridgeData; potential domain mismatch risks noted broadly in survey.",
            "comparison_to_vision_only": "Dataset enables VLM+language-conditioned training; survey does not provide direct comparisons but implies language labels aid transfer.",
            "temporal_dynamics": "Dataset contains trajectories enabling temporal modeling; no explicit temporal analyses provided.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.11"
        },
        {
            "name_short": "DROID (dataset)",
            "name_full": "DROID (Large in-the-wild robot manipulation dataset)",
            "brief_description": "A large multimodal in-the-wild robot manipulation dataset containing &gt;150k trajectories across 1k+ objects and tasks, intended to support foundation-scale VLA pretraining.",
            "citation_title": "DROID",
            "mention_or_use": "mention",
            "model_name": "DROID (dataset)",
            "model_description": "Dataset resource with RGB-D, language, state/action traces across many tasks and objects for training and benchmarking VLA foundation/policy models.",
            "pretraining_type": "dataset used for large-scale supervised pretraining of VLA models",
            "pretraining_data_description": "150k+ real or in-the-wild trajectories covering over 1,000 objects and tasks; multimodal annotations include RGB-D, language descriptions and low-dimensional states.",
            "target_task_name": "large-scale manipulation foundation model pretraining and transfer",
            "target_task_description": "Supports training of generalist manipulation policies and evaluation on diverse tasks and objects in real or realistic settings.",
            "semantic_alignment": "Dataset includes paired language and manipulation trajectories providing direct semantic-action alignment; survey emphasizes its scale for foundation-model training.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey notes the dataset enables foundation-scale pretraining that improves downstream transfer, but no explicit sample-efficiency numbers provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in survey.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Large volume of paired language/action data offers grounding signal for downstream VLA models though specific grounding studies not summarized.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Scale and diversity support transfer; sim-to-real and distributional shifts remain concerns.",
            "novel_vs_familiar_objects": "Not explicitly quantified in survey.",
            "zero_shot_or_few_shot": "Dataset intended to enable improved zero/few-shot transfer for foundation models, but survey provides no numeric examples.",
            "layer_analysis": null,
            "negative_transfer_evidence": "Not discussed specifically for DROID.",
            "comparison_to_vision_only": "Dataset's inclusion of language makes it suited for VLM/VLA pretraining; direct comparisons not provided.",
            "temporal_dynamics": "Contains trajectory sequences enabling temporal modeling; no dynamics analysis reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.12"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (open-source Vision-Language-Action model)",
            "brief_description": "Open-source 7B VLA trained on 970k trajectories claimed to surpass RT-2-X, intended as a generalist VLA for multiple embodied tasks.",
            "citation_title": "OpenVLA",
            "mention_or_use": "mention",
            "model_name": "OpenVLA",
            "model_description": "Autoregressive Transformer trained on large-scale trajectory datasets combining vision, language, and action tokens for cross-platform robotic control.",
            "pretraining_type": "supervised imitation pretraining on large multimodal trajectory datasets",
            "pretraining_data_description": "~970k robot trajectories with visual, state, action, and instruction annotations, focused on manipulation tasks.",
            "target_task_name": "generalist robotic manipulation and embodied control",
            "target_task_description": "Multi-task manipulation across diverse settings and embodiments (real/sim), with autoregressive action decoding reconstructable into continuous controls.",
            "semantic_alignment": "Survey claims OpenVLA achieves improved alignment via large-scale training, but does not report quantitative alignment measures.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey attributes improved cross-task generalization to large dataset scale; explicit comparisons not provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not discussed in survey.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Grounding implied via paired instruction-trajectory pretraining; survey-level claims of better performance vs prior models.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Benefits from dataset diversity and embodiment coverage; heterogeneity remains a challenge.",
            "novel_vs_familiar_objects": "Not quantified here.",
            "zero_shot_or_few_shot": "Survey suggests improved zero/few-shot transfer but no numbers.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not detailed.",
            "comparison_to_vision_only": "Claimed advantage of vision+language pretraining qualitatively, no numbers.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.13"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language-Image Pre-training)",
            "brief_description": "A contrastively-trained vision-language model mapping images and text into a shared embedding space enabling zero-shot recognition and providing strong semantic priors for downstream VLA tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CLIP",
            "model_description": "Dual-encoder vision-language model trained with contrastive objectives on image-text pairs; provides transferable semantic embeddings used as perception backbone or promptable representations.",
            "pretraining_type": "vision-language contrastive pretraining on image-text pairs",
            "pretraining_data_description": "Massive image-caption pairs scraped from web; contains object names, attributes, and many forms of object-related language though not explicit robot action labels.",
            "target_task_name": "used as perceptual backbone in language-conditioned manipulation (e.g., CLIPort pipelines)",
            "target_task_description": "Provides semantic features for mapping language instructions to object-focused affordances in tabletop/robotic tasks; used alongside spatial modules to produce actionable heatmaps.",
            "semantic_alignment": "High for object and attribute semantics; provides robust language-image alignment useful for object grounding in manipulation tasks, though action semantics less directly encoded.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey suggests CLIP features improve downstream sample-efficiency for language-conditioned policies but gives no numeric measures.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in survey; CLIP embeddings are known to focus on semantically-relevant regions but survey does not include visualizations.",
            "embedding_space_analysis": "Survey alludes to CLIP's joint embedding space enabling retrieval and zero-shot labeling, used as semantic priors in VLA approaches.",
            "action_grounding_evidence": "Provides object-level grounding which downstream modules convert to affordances/actions (e.g., CLIPort heatmaps), but CLIP itself lacks direct action grounding.",
            "hierarchical_features_evidence": "CLIP supplies high-level semantic features; lower-level spatial/geom reasoning typically added by downstream modules.",
            "transfer_conditions": "Works best when object vocabulary overlaps with CLIP pretraining; less direct benefit for fine motor primitives absent from captions.",
            "novel_vs_familiar_objects": "CLIP supports zero-shot recognition of novel object names to some extent, which downstream VLA systems can leverage; no numeric splits in survey.",
            "zero_shot_or_few_shot": "CLIP's zero-shot capabilities are explicitly used in language-conditioned manipulation pipelines for object selection/identification.",
            "layer_analysis": "Not reported in survey.",
            "negative_transfer_evidence": "CLIP may not encode action affordances, causing downstream modules to require additional spatial grounding; not quantified.",
            "comparison_to_vision_only": "CLIP's vision-language pretraining provides superior semantic grounding compared to vision-only backbones for language-conditioned tasks per survey discussion (no numbers).",
            "temporal_dynamics": "Not applicable.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.14"
        },
        {
            "name_short": "SimpleVLA-RL",
            "name_full": "SimpleVLA-RL",
            "brief_description": "An online RL approach that trains a VLA policy from a single trajectory using binary rewards (0/1), demonstrating extreme sample-efficiency in certain online RL regimes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SimpleVLA-RL",
            "model_description": "Reinforcement learning based VLA variant that uses minimal reward signal (binary success) and online updating to train from very small numbers of trajectories.",
            "pretraining_type": "online RL finetuning from small demonstration set (often one trajectory) possibly on top of pretrained VLM/VLA backbones",
            "pretraining_data_description": "One or very few task-specific demonstration trajectories; language conditioning may be present depending on experiment.",
            "target_task_name": "sample-efficient online RL for manipulation",
            "target_task_description": "Online robotic tasks where a single demonstration plus sparse success/failure reward is used to train a policy; action space continuous and learned via RL updates.",
            "semantic_alignment": "Survey presents SimpleVLA-RL as combining SFT stability with RL exploration; alignment relies on starting from pretrained multimodal priors if available.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Survey highlights SimpleVLA-RL trains with one trajectory and binary rewards to reach performance comparable to full supervised fine-tuning, indicating dramatic sample efficiency in certain setups (qualitative claim).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Method shows that reinforcement updates from sparse signals can refine action grounding when combined with pretrained priors, but mechanistic probes not provided.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Works when environment and task structure permit signal from single trajectory to guide policy and when pretrained priors provide strong initialization.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Explicit few-shot claim: trains from a single trajectory (1-shot) in online RL setting; survey reports performance comparable to supervised fine-tuning in those experiments (qualitative).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Survey warns that sparse-reward RL can be unstable and depends strongly on initialization; no quantified negative-transfer reported for SimpleVLA-RL.",
            "comparison_to_vision_only": "Survey frames SimpleVLA-RL as combining SFT (which may be multimodal) with RL; direct vision-only comparisons not provided.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1950.15"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vima: General robot manipulation with multimodal prompts",
            "rating": 2
        },
        {
            "paper_title": "Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "rating": 2
        },
        {
            "paper_title": "CLIPort",
            "rating": 2
        },
        {
            "paper_title": "Open X-Embodiment",
            "rating": 2
        },
        {
            "paper_title": "BridgeData",
            "rating": 2
        },
        {
            "paper_title": "DROID",
            "rating": 2
        },
        {
            "paper_title": "Gato: A generalist agent",
            "rating": 1
        },
        {
            "paper_title": "RDT-1B",
            "rating": 1
        }
    ],
    "cost": 0.034623999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pure Vision Language Action (VLA) Models: A Comprehensive Survey
25 Sep 2025</p>
<p>Dapeng Zhang 
Jing Sun 
Chenghui Hu 
Xiaoyan Wu 
Zhenlong Yuan 
Rui Zhou 
Fei Shen 
Qingguo Zhou 
Pure Vision Language Action (VLA) Models: A Comprehensive Survey
25 Sep 2025B42469136E586BFAE526C002C49D6376arXiv:2509.19012v2[cs.RO]Vision Language ActionVision Language ModelRoboticsEmbodied AI
The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments.This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research.It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail.In addition, foundational datasets, benchmarks, and simulation platforms are introduced.Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics.By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.</p>
<p>INTRODUCTIONS</p>
<p>Robotics has long been a prominent area of scientific research.Historically, robots primarily relied on preprogrammed instructions and engineered control policies to perform task decomposition and execution.These methods were commonly applied to simple, repetitive tasks, such as factory assembly lines and logistics sorting.In recent years, the rapid advancement of artificial intelligence has enabled researchers to exploit the feature extraction and trajectory prediction capabilities of deep learning across diverse modalities, including images, text, and point clouds.By integrating techniques such as perception, detection, tracking, and localization, researchers have decomposed robotic tasks into multiple stages to meet execution requirements, thereby advancing the development of embodied intelligence and autonomous driving.However, most of these robots still operate as isolated agents, designed for specific tasks and lacking effective interaction with humans and external environment.</p>
<p>To address these limitations, researchers have begun exploring the incorporation of large language models (LLMs) and vision language models (VLMs) to enable more accurate and flexible robotic manipulation.Modern robotic manipulation methods [1], [2] typically leverage vision language generative paradigms (e.g., autoregressive models [3], [4], [5], [6] or diffusion models [7]), combined with large-scale  * Equal contributions   Fei Shen and Qingguo Zhou are the corresponding authors  D. Zhang, J. Sun, C. Hu, X. Wu, R. Zhou and Q.</p>
<p>Zhou are with School of Information Science and Engineering, Lanzhou University, China.E-mail: {zhangdp22, sjing2023,huchh2024,wuxiaoyan2024,zr,zhouqg}@lzu.edu.cn Z. Yuan is with the Institute of Computing Technology, Chinese Academy of Sciences, China.E-mail: yuanzhenlong21b@ict.ac.cn  F. Shen is with the NExT++ Research Centre, National University of Singapore, Singapore.E-mail: shenfei29@nus.edu.sgdatasets [8] and advanced fine-tuning strategies.We refer to these as VLA foundation models, which have substantially improved the quality of robotic manipulations.Fine-grained action control over generated content provides users with greater flexibility, unlocking the practical potential of VLA for task execution.Despite their promise, reviews of pure VLA methods remain scarce.Existing surveys either focus on taxonomy over VLM foundational models or provide broad overviews of robotic manipulation as a whole.Firstly, VLA methods represent a nascent field in robotics, with no established methodological landscape or consensus taxonomy, making it challenging to systematically summarize these approaches.Secondly, current reviews either classify VLA approaches based on differences in foundational models or present a comprehensive analysis of robotic applications across the entire history of the field, often emphasizing traditional methods at the expense of emerging techniques.While these reviews offer valuable insights, they provide only cursory examinations of robotic models or concentrate primarily on foundational models, leaving a significant gap in the literature regarding pure VLA methods.</p>
<p>In this paper, we investigate VLA methods and associated resources, providing a focused and comprehensive review of existing approaches.Our goal is to present a clear taxonomy, systematically summarize VLA research, and elucidate the development trajectory of this rapidly evolving field.After a brief overview of LLMs and VLMs, we focus on the policy strategies of VLA models, highlighting the unique contributions and distinctive features of previous studies.We classify VLA approaches into 4 categories: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods, and provide a detailed analysis of their motivations, core strategies, and mechanisms.As shown in Fig. 2, we present a VLA skeleton of these methods.We examine application domains, including robotic arms, quadruped robots, humanoids, and wheeled robots (autonomous vehicles), offering a comprehensive assessment of VLA deployment across diverse scenarios.Given the strong dependence of VLA models on datasets and simulation platforms, we provide a concise overview of these resources.Finally, based on the current VLA landscape, we identify key challenges and outline future research directions-including data limitations, inference speed, and safety-to accelerate the advancement of VLA models and generalizable robotics.</p>
<p>The overall structure of this survey is illustrated in Fig. 1.First, Section 2 provides an overview of the background for VLA research.Section 3 presents the existing VLA approaches in the robotics field.Section 4 introduces the datasets and benchmarks employed by VLA approaches.Sections 5 and 6 discuss simulation platforms and robotic hardware.Section 7 further discusses the challenges and future directions for VLA-based robotic methods.Finally, we summarize the paper and provide our perspective on future developments.</p>
<p>In summary, our contributions are as follows:</p>
<p></p>
<p>We present the well-structured taxonomy of pure VLA methods, classifying approaches based on their action-generation strategies.This facilitates understanding of existing methods and highlights core challenges in the field.</p>
<p></p>
<p>The survey emphasizes the defining characteristics and methodological innovations of each category and technique, providing a clear perspective on current approaches.</p>
<p></p>
<p>We provide a comprehensive overview of associated resources (datasets, benchmarks and simulation platforms) for training and evaluating VLA models.</p>
<p></p>
<p>We investigate the practical impact of VLA in robotics, identify key limitations of existing techniques, and propose potential avenues for further exploration.</p>
<p>BACKGROUNDS</p>
<p>The emergence of Vision Language Action (VLA) models represents a significant advance toward general-purpose embodied intelligence.Traditional robotic systems typically rely on isolated perception pipelines, hand-engineered control strategies, or task-specific reinforcement learning.Although these approaches perform effectively in constrained environments, such as factory floors or laboratories, they generalize poorly to dynamic and unstructured settings.Modern robots can "see" using computer vision models, "understand" language through large language models, and "act" via controllers or learned policies; however, integrating these capabilities into a coherent and unified system remains a key challenge.VLA models address this challenge by providing a unified framework that grounds language in perception and maps it to executable actions.</p>
<p>Early: LLM/VLM fundamental Models</p>
<p>Breakthroughs in unimodal modeling have laid the methodological and engineering foundation for multimodal integration.In computer vision, convolutional neural networks (e.g., AlexNet [9], ResNet [10]) established a representational paradigm transitioning from local convolutions to deep residual learning, which was further advanced by the Vision Transformer (ViT) [11].ViT introduced self-attention to the image domain, markedly improving transferability and generalization.In natural language processing, the Transformer architecture enabled large-scale pretraining and alignment techniques, giving rise to models such as BERT [12], GPT [13], T5 [14], and GPT-4 [15], which exhibit strong reasoning capabilities, instruction-following, and incontext learning.In parallel, reinforcement learning advanced policy optimization and sequential decision-making, progressing from DQN and PPO to Decision Transformer, which highlights a unifying perspective of control through sequence modeling.</p>
<p>Against this backdrop, vision language models (VLMs) emerged as a crucial bridge between unimodal learning and embodied intelligence.Early approaches (e.g., ViLBERT [16], VisualBERT [17]) aligned and fused images and text using dual-stream or single-stream Transformers, while contrastive learning methods (e.g., CLIP) mapped large-scale image-text pairs into a shared embedding space, enabling zero-shot and few-shot recognition and retrieval.More recently, instruction-tuned, dialogue-centric multimodal models (e.g., BLIP-2 [18], Flamingo [19], LLaVA [20]) have substantially strengthened open-ended cross-modal understanding, fine-grained grounding, and multi-turn reasoning, setting the stage for vision language action (VLA) systems.</p>
<p>Present: Development of VLA Models</p>
<p>From LLM/VLM to VLA Models</p>
<p>Following this trajectory, research has naturally advanced toward VLA integration, which unifies visual perception, language understanding, and executable control within a single sequence modeling framework [1], [2], [21], [22].Typical designs encode images and instructions as prefix or context tokens, inject robot states and sensory feedback as state tokens, and autoregressively generate action tokens to produce control sequences, thereby closing the perceptionlanguage-action loop.Compared with traditional perception, planning, and control pipelines, VLA provides endto-end cross-modal alignment and a unified treatment of goals, constraints, and intent.It inherits the semantic and instruction generalization of VLMs, while explicit state coupling and action generation confer robustness to environmental disturbances and long-horizon tasks.This progression-from unimodal to multimodal, and then to multimodal plus executable control-establishes the methodological foundation for systems that can not only see and understand, but also act.</p>
<p>The Supporting Role of Data and Simulation</p>
<p>The development of vision language action (VLA) models in robotics relies heavily on high-quality datasets and realistic simulators capable of capturing the complexity of real-world scenarios.Recent robotic methods are typically deep learning-based and data-driven; consequently, dataset collection and annotation play a critical role in driving advancements in the field.Some datasets are collected in real-world settings, which requires substantial human effort and financial resources.To address these challenges, researchers also leverage large-scale human manipulation videos from the Internet as generalization datasets to provide auxiliary supervision for VLA model training.Despite these efforts, data collection remains costly, annotations are labor-intensive, and long-tail corner cases are often underrepresented.Other datasets are generated through robot simulators, which facilitate the collection of large-scale labeled data.Simulators provide diverse and controllable environments, flexible sensor configurations, realistic kinematic models, and interactive static and dynamic scenes, supporting both data collection and model evaluation.Representative datasets include Open X-Embodiment (OXE) [8], which integrates 22 robot datasets from 21 institutions, encompassing 527 skills and 160,266 tasks, and BridgeData [23], comprising 71 tasks across 10 environments in multiple domains.These resources standardize data formats, thereby promoting rapid development and reproducibility in VLA research.Simulators such as THOR [24], Habitat [25], Mu-JoCo [26], Isaac Gym [27], and CARLA [28] offer extensible virtual environments capable of generating multimodal annotations, including action trajectories, object states, and natural language instructions.Collectively, these datasets and simulation platforms mitigate the scarcity of real-world robot data and accelerate the training and evaluation of VLA models.</p>
<p>Future: Towards General Embodied Intelligence</p>
<p>VLA models occupy the forefront of research where vision, language, and action converge.They build upon breakthroughs in foundational models for perception and reasoning, emphasizing capabilities in human interaction and task execution, and extend these abilities to the physical world.By integrating the representational power of visual encoders, the reasoning capabilities of large language models, and the decision-making abilities of reinforcement learning and control frameworks, VLA models hold significant potential to bridge the "perception-understanding-action" gap.Despite facing challenges related to scalability, generalization, safety, and real-world deployment, VLA is widely recognized as a key frontier in embodied artificial intelligence.Although VLA has achieved notable success in vision language action interactions, and benefits from advances in large-scale language models, it has not yet attained full generality in the domain of embodied intelligence.General embodied intelligence posits that human-like intelligent behavior relies not only on cognitive processing but also on the physical body, environmental perception, and feedback mechanisms, enabling interaction with the external world.To accommodate the demands of diverse tasks, general embodied intelligence can manifest through various types of robots, including humanoid robots for household applications, assembly robots with dexterous manipulators, and bionic robots with specialized capabilities.Clearly, general embodied intelligence has the potential to enable artificial systems to perform a wider array of tasks across diverse environments.VLA is currently evolving toward this vision of general embodied intelligence and holds considerable promise for realizing it.</p>
<p>VISION LANGUAGE ACTION MODELS</p>
<p>In recent years, Vision Language Action (VLA) models have undergone rapid and systematic development, driven by advances in multimodal representation learning, generative modeling, and reinforcement learning.To trace this evolution, this section reviews the major methodological paradigms in VLA, including autoregressive-based modeling, diffusion-based approaches, reinforcement learning strategies, and hybrid or specialized designs.Fig. 3 presents a tree diagram illustrating the progression of these paradigms, with each branch highlighting representative works within the respective taxonomy.The taxonomy is organized chronologically, emphasizing how methodological innovations have progressively expanded the capabilities of VLA models.</p>
<p>Autoregression-Based Models in Vision Language Action Research</p>
<p>Autoregression-based models constitute a classical yet effective paradigm for sequence generation in Vision-Language-Action (VLA) tasks.By treating action sequences as temporally dependent processes, these models generate actions step by step, conditioned on prior context, perceptual inputs, and task prompts.With the rapid advancement of Transformer architectures, recent VLA systems [21], [29], [30] have demonstrated the scalability and robustness of this approach.Representative works in these directions are summarized in Table 1, collectively highlighting the versatility and generality of the autoregressionbased modeling paradigm in VLA research.</p>
<p>Autoregressive Generalist VLA Methodologies</p>
<p>Research on generalist VLA agents unifies perception, task instructions, and action generation within autoregressive sequence modeling.By tokenizing multimodal inputs, these models enable step-by-step action generation across heterogeneous tasks.</p>
<p>Early work like Gato [31] demonstrated tokenizing heterogeneous modalities for joint training.Subsequent scaling efforts, notably RT-1/RT-2 [29], [33] leveraged massive realworld datasets and web-scale pretraining, while PaLM-E [32] integrated pretrained language knowledge into embodied control, establishing autoregressive Transformers as practical unifying models.</p>
<p>To address embodiment fragmentation, frameworks like Octo [35], LEO [34], and UniAct [39] aligned visuallinguistic modalities with universal action abstractions for cross-platform compatibility.Recent advances focus on reasoning integration and efficiency.Models now combine action generation with language reasoning and adaptive prompting for long-horizon planning [21], [38], [42].Lightweight designs like NORA [41] and RoboMM [37] address deployment constraints.</p>
<p>Overall, research on generalist VLA agents has progressed from early unified tokenization to large-scale realworld training and semantic grounding, advancing toward cross-platform universality, reasoning integration, and efficiency-oriented designs.This trajectory reflects a shift from proof-of-concept demonstrations to systems emphasizing scalability, semantic reasoning, and deployability.Table 1 (A) summarizes representative autoregressive generalist agents and their key contributions.However, issues of safety, interpretability, and alignment with human values remain largely unresolved, leaving ample room for future research.</p>
<p>Autoregressive Reasoning and Semantic Planning with LLMs</p>
<p>The integration of LLMs has transformed them from passive input parsers into semantic mediators within VLA systems, enabling reasoning-driven control for long-horizon and compositional tasks.This section reviews LLM-based reasoning evolution from semantic intermediaries to hierarchical planners and platform-level orchestration.</p>
<p>To inject reasoning into VLA models, Inner Monologue [46], [47] introduced self-talk-style reasoning with preaction planning and post-action reflection.Extensions like Prompt-to-Walk, RoboFlamingo, and RoboMM [37], [48], [93] demonstrated linguistic representations across locomotion and manipulation tasks.</p>
<p>Subsequent approaches enhanced adaptability through feedback and hierarchical planning.Interactive Language [94] enabled real-time correction, Open-Ended Instructable Agents [95] utilized episodic memory, and Hi Robot [55] adopted hierarchical planning for long instructions.Mis-sionGPT, Mobility VLA, and NORA [41], [51], [96] emphasized lightweight deployment and dialogue-driven adaptability.</p>
<p>Hierarchical frameworks combined semantic planning with controllers for dexterous manipulation [97], [98], [99], [100].InSpire, From Foresight to Forethought, and CoT-VLA [58], [62], [101] emphasized runtime stability and chain-ofthought mechanisms.</p>
<p>Autoregression-based reasoning architecture commonly patches the input to the sequence and utilizes those tokens for further reasoning.These models can process input of various lengths,and the strong ability of in-context learning enables them to process different modalities under aunified structure [102].UAV-specific systems, such as Cognitive-Drone and UAV-VLA [52], [60], highlighted aerial navigation and satellite-informed planning.Additional contributions, including OneTwoVLA [42], addressed adaptive reasoning-action switching and abstraction of heterogeneous control spaces.</p>
<p>In contrast to the aforementioned methods, efforts toward systematization and platformization have begun to consolidate these advances.Gemini Robotics [59] and Agentic Robot [103] positioned LLMs as central orchestrators of embodied pipelines, while 0.5 and fast [22], [61] targeted open-world scalability and efficient tokenization.Supporting works, including VLA Model-Expert Collaboration and LLaRA [49], [104], explored collaborative mechanisms and auxiliary tasks for improved VLM-to-VLA transfer.LLMbased reasoning in VLA has progressed from semantic intermediaries to interactive and hierarchical planning, crossmodal extensions, and integrated platforms.</p>
<p>Although LLM-based reasoning in VLA has evolved from semantic intermediaries to interactive and hierarchical planners, cross-modal extensions, and integrated platforms.Nonetheless, persistent challenges remain, including hallucination control, multimodal alignment, reasoning stability, and real-time safety.Table 1 (B) summarizes representative studies and their contributions.</p>
<p>Autoregressive Trajectory Generation and Visual Alignment Modeling</p>
<p>Autoregressive trajectory modeling strengthens perceptionaction mapping while ensuring vision-language semantic alignment.These models decode motion trajectories or control tokens conditioned on multimodal observations, offering unified mechanisms for grounded instruction following and action execution [105], [106], [107].</p>
<p>Early work such as LATTE [63] showed the feasibility of mapping language directly to trajectories, inspiring multimodal extensions.With large-scale pretraining, VIMA [64] and InstructRL [65] demonstrated that joint tokenization of language, vision, and action supports strong cross-task generalization, though often only in simulation.Meanwhile, MOO [108] and GPT-based approaches [109] leveraged pretrained vision-language backbones for open-world generalization and lightweight trajectory generation, suggesting semantic priors can reduce reliance on robot-specific pretraining.</p>
<p>A second line of work explores video prediction and world modeling.GR-1/2 [67], [68] transferred video generation pretraining to robotics, while CronusVLA [110] and WorldVLA [30] improved temporal consistency.TraceVLA [77] and Uni-NaVid [111] further introduced long-horizon prompting, collectively shifting from shorthorizon decoding to predictive environment modeling.</p>
<p>Autoregression-based methods have been adapted to diverse robot embodiments, from quadruped locomotion to bimanual manipulation, demonstrating the flexibility of vision-language-action frameworks [69], [112], [113].Largescale efforts such as OpenVLA [21], [114] further highlight cross-platform generalization and efficient adaptation, while latent motion token approaches [71], [115] point toward lightweight pretraining strategies.</p>
<p>Beyond manipulation, autoregressive trajectory generation has extended to autonomous driving, where recent models achieve closed-loop control by aligning vision and language with trajectory prediction, often without HD maps or LiDAR [116], [117], [118].Similar principles have been applied to mobile manipulation and UAV planning [74], [79], underscoring the versatility of these methods across robotic platforms.</p>
<p>Researchers have also extended autoregressive frameworks toward fine-grained sensing and richer modalities.Recent models emphasize precise manipulation with robust pretraining pipelines [73], [119], [120], [121], [122], while tactile-language-action integration [75], [123] enables contactrich interaction.Parallel efforts exploit 3D/4D perception to embed spatial structure into autoregressive decoding [76], [124], [125], further broadening the multimodal landscape.</p>
<p>Autoregression-based trajectory generation has progressed from direct language-to-trajectory mapping toward a broad ecosystem that spans multimodal pretraining, video-driven world modeling, embodiment-specific architectures, and cross-modal sensing (see Table 1 (C)).These advances showcase the scalability and versatility of autoregression as a unifying mechanism for VLA.Nonetheless, challenges remain in long-horizon stability, semantic grounding under noisy inputs, and efficient deployment on physical robots.Future work should prioritize robust closed-loop integration between predictive modeling and low-level control, and explore synergies between autoregressive policies and higher-level reasoning modules such as LLM planners, moving closer to reliable, general-purpose embodied intelligence.</p>
<p>Structural Optimization and Efficient Inference Mechanisms in Autoregressive VLA</p>
<p>In autoregressive VLA research, structural optimization and efficient inference are critical for enabling scalable deployment and real-time control.Beyond accuracy, the central challenge is how to reduce computational redundancy, shorten inference latency, and maintain robustness across diverse robotic settings.</p>
<p>One important direction is hierarchical and modular optimization.Early work such as HiP [80] demonstrated that decomposing tasks into symbolic planning, video prediction, and action execution enables long-horizon reasoning with autoregressive models.Follow-up designs-ranging from efficient observation backbones and action chunking to trajectory-aware attention and frequency separation [81], [126], [127]-further showed that modular structures can significantly reduce computation while preserving generalization.</p>
<p>Another line of work emphasizes dynamic and adaptive inference.Frameworks like DeeR-VLA [82] enable early termination of decoding based on task complexity, while token-efficient designs such as FAST [22] compress long action sequences into variable-length tokens.Together, these approaches show how adaptive computation can improve real-time responsiveness with minimal loss in accuracy.</p>
<p>A third category emphasizes lightweight compression and parallelization.Quantization and layer-skipping methods [86], [88] cut precision and dynamically activate only subsets of layers, significantly reducing computation.In parallel, decoding and redundancy-reduction strategies [87], [128] accelerate inference without retraining, highlighting how architectural compression complements adaptive inference.</p>
<p>Compression and parallelization methods span quantization and layer-skipping [86], [88], which substantially reduce computation, as well as parallel decoding and redundancy-reduction strategies [87], [128], which accelerate inference without retraining.</p>
<p>Efficiency has also been pursued through sensor fusion and temporal reuse.Domain-specific optimizations such as voxelized spatial modeling [83], adaptive key-value caching [84], and perception adaptation [85] reduce redundant computation while improving robustness.</p>
<p>Notably, several works integrate efficiency with multimodal reasoning.OTTER [91] injects language-awareness into vision encoding, while ChatVLA [92] employs staged coupling with mixture-of-experts routing.Other advances-ranging from diffusion-based goal generation [89], quantization [88], to hierarchical feedback for ultra-long horizons [90]-illustrate how architectural refinements can balance efficiency and scalability.</p>
<p>In summary, structural optimization and efficient inference in autoregressive VLA models have evolved from early hierarchical decomposition strategies to adaptive computation, lightweight compression, caching, and multimodalaware integration (see Table 1 (D)).These methods address long-sequence dependency and computational redundancy, delivering notable gains in both benchmarks and real-world deployment.Looking forward, future research should pursue hardware-aware co-optimization, intelligent scheduling, and robust safety mechanisms to ensure scalable and reliable progress toward general-purpose embodied intelligence.</p>
<p>Discussion</p>
<p>Innovations Autoregression-based models have driven major innovations in Vision-Language-Action research by unifying multimodal perception, language reasoning, and sequential action generation within scalable Transformer architectures.They support generalist agents capable of cross-task generalization, enable semantic planning through LLM integration, and extend trajectory generation to longhorizon and multimodal settings, structural optimizations such as token compression, parallel decoding, and quantization improve efficiency for real-world deployment.</p>
<p>Limitations Autoregressive decoding introduces error accumulation and latency, multimodal alignment can be brittle under noisy or incomplete inputs, and scaling large models requires prohibitive computational resources and data.Moreover, reasoning-driven approaches still face challenges of hallucination, stability, and interpretability, while efficiency mechanisms often trade off accuracy or generality.Addressing these issues will require tighter coupling between reasoning and control, robustness under real-world uncertainty, and hardware-aware optimization strategies to balance scalability with practical deployment.</p>
<p>Diffusion-based Models in Vision Language Action Research</p>
<p>Diffusion (including flow matching, VAE, etc.) models have emerged as a transformative paradigm in generative artificial intelligence, showing remarkable potential within Vision Language Action (VLA) frameworks for embodied intelligence.In this subsection, we review the evolution of diffusion models in VLA systems, focusing on three critical dimensions.Representative works are summarized in Table 2.</p>
<p>Diffusion Generalist VLA Methodologies</p>
<p>Diffusion models integration into VLA systems shifts robotic action generation from deterministic regression to probabilistic generative policies.By formulating action generation as conditional denoising, diffusion-based methods naturally model diverse action distributions, enabling multiple valid trajectories from identical observations [7], [135].</p>
<p>A key trajectory has been the incorporation of richer representational structures.Geometry-aware methods embed SE(3) constraints into diffusion, extending beyond Euclidean spaces to jointly optimize grasping and motion in 3D environments [129], [132], thereby ensuring physically consistent actions.In parallel, reinterpreting policy learning as video generation [130], [133] leverages the temporal richness of video for long-horizon planning and cross-modal grounding.</p>
<p>Scaling efforts like RDT-1B [134] demonstrate trajectorylevel diffusion with temporal and environmental conditioning for zero-shot generalization in bimanual manipulation.Temporal coherence is addressed through unified velocity fields across timesteps [135] or historical conditioning with efficient caching for real-time deployment [136].</p>
<p>These advances mark three transitions: deterministic to probabilistic generation, Euclidean to geometry-aware representations, and supervised to self-supervised paradigms.This reframing as generative modeling enables multi-task generalization, few-shot adaptation, and natural language interfaces.Table 2 (A) summarizes architectural choices and training strategies.However, temporal coherence remains fragile under dynamic environmental shifts.</p>
<p>Diffusion-Based Multimodal Architectural Fusion</p>
<p>The integration of Transformers in VLA systems drives unified modeling of vision, language, and action within single frameworks, moving beyond modular pipelines to capture intricate interdependencies in embodied intelligence.</p>
<p>Within this shift, combining Transformers with diffusion models has proven especially transformative, as attention mechanisms naturally complement generative modeling.Large-scale frameworks such as Dita [145] and Diffusion Transformer Policy [171] show that scaling attention-based architectures beyond small action heads substantially improves continuous action modeling, with self-attention inductive biases aligning well with the compositionality of robotic behaviors.</p>
<p>The central challenge lies not in scaling architectures but in fusing heterogeneous modalities while preserving their distinct properties.Vision, language, and proprioception differ in temporal granularity, semantics, and processing needs-creating opportunities for richer context but also risks of diluting modality-specific strengths.To address this, token-space alignment strategies such as M-DiT [140] map diverse signals into unified representations, enabling conditional diffusion Transformers to flexibly support arbitrary combinations of goals and observations, a key step toward general-purpose robotics.Domain-specific designs like ForceVLA [146] treat force sensing as a first-class modality, using force-aware mixtureof-experts to integrate tactile feedback with visual-language embeddings, significantly improving contact-rich manipulation.</p>
<p>Recent advances integrate explicit reasoning within diffusion policies.Diffusion-VLA [1] introduces Self-Generated Reasoning modules producing symbolic representations, while CogACT [141] leverages semantic scene graphs, unifying perception, reasoning, and control.</p>
<p>Pre-trained model leveraging includes repurposing image editing models for zero-shot manipulation [139] and joint fine-tuning strategies like PERIA [142].Structured decomposition through Chain-of-Affordance [143] and flowgraph approaches like 0 [2] outperform end-to-end methods in complex environments.</p>
<p>Taken together, these developments (Table 2(B)) reveal a field in transition-from monolithic architectural adaptations toward cognitively inspired frameworks that integrate structured reasoning, multi-sensory inputs, and explicit knowledge representation.This shift signals a move beyond purely data-driven end-to-end learning toward more interpretable and generalizable designs, though progress remains constrained by high computational demands and limited dataset diversity.</p>
<p>Application Optimization and Deployment in Diffusion-Based VLA</p>
<p>The transition from laboratory prototypes to real-world deployment remains one of the most formidable challenges for diffusion-based Vision Language Action (VLA) systems.Meeting this challenge requires advances across three interconnected fronts: efficiency, adaptability, and robustness.Recent work demonstrates that rather than scaling model size indiscriminately, progress now hinges on principled optimization strategies, cognitive-inspired architectures, and practical deployment mechanisms.</p>
<p>Efficiency optimization has become a central theme.While diffusion models are resource-intensive, lightweight designs such as TinyVLA and SmolVLA [148], [149] show that pre-trained backbones with parameter-efficient tuning (e.g., LoRA) can cut training costs to single-GPU scales without sacrificing performance.Complementary strategies like VQ-VLA [?], which employs vector-quantized action tokenizers to narrow the sim-to-real gap, illustrate how efficiency gains can align with robustness.Collectively, these works reflect a paradigm shift toward "intelligent sparsity," prioritizing performance-per-computation over brute-force scaling.</p>
<p>At the same time, task adaptability has become a defining feature of advanced VLA systems.In dexterous manipulation, large-scale curated datasets such as DexVLG [150] enable strong zero-shot performance, while in mobile manipulation, frameworks like AC-DiT [151] unify perception and actuation through mobility-to-body conditioning.Overall, the trend is toward balancing general-purpose architectures with deep domain specialization, embedding task-specific inductive biases while retaining broad multimodal capabilities.</p>
<p>Architectural innovation represents the next frontier.Dual-and triple-system designs, such as MinD and TriVLA [153], [155], demonstrate how cognitive principles can be operationalized in robotics.MinD integrates low-frequency video prediction for strategic planning with high-frequency diffusion policies for reactive control, while TriVLA explicitly separates vision language reasoning, dynamics perception, and policy learning into coordinated modules.Operating at interactive frequencies (e.g., 36Hz), these cognitively inspired architectures not only improve task performance but also enhance system interpretability and maintainability-key requirements for industrial deployment.</p>
<p>Beyond efficiency and design, runtime robustness has emerged as a decisive factor for real-world adoption.Lightweight intervention strategies like BYOVLA [156] dynamically edit irrelevant visual regions at inference time without fine-tuning, mitigating robustness failures in unpredictable environments.Meanwhile, self-reflective architectures such as DreamVLA [157] introduce hierarchical error handling with reasoning-enhanced modules, error-aware layers, and expert adapters.Together, these strategies illustrate a shift toward "defensive AI," emphasizing resilience and reliability as much as raw task performance.</p>
<p>The application landscape of diffusion-based VLA systems has rapidly expanded.In autonomous driving, Drive-MoE [159] employs scene-and skill-specialized mixtures of experts to achieve state-of-the-art closed-loop control, while in humanoid robotics, DreamGen [160] leverages video world models to generalize from single-task teleoperation to dozens of novel behaviors.EnerVerse [161] and Vid-Bot [162] extend this paradigm by predicting embodied futures through autoregressive video diffusion and affordance learning, underscoring the potential of video-centric world models for planning.These advances highlight a transition from task-specific prototypes to versatile, domain-spanning systems.</p>
<p>Ambitious efforts toward foundation models further underscore the field's trajectory.FP3 [164] introduces a largescale 3D policy model pre-trained on 60,000 trajectories, while GR00T N1 [165] integrates multimodal Transformer architectures into a humanoid foundation system.Like large language models in NLP, these approaches aim to provide general-purpose priors for robotics, though they must also address safety, real-time control, and physical reliability-challenges less pronounced in text-based domains.</p>
<p>Generalization and fine-tuning strategies remain critical for advancing diffusion-based VLA systems toward real-world deployment.Recent research highlights multiple complementary directions: ObjectVLA and SwitchVLA [166], [167] demonstrate the feasibility of open-world object manipulation and execution-aware task switching, emphasizing flexibility in dynamic environments.In parallel, approaches such as LangToMo and Evo-0 [169], [170] introduce novel intermediate representations and geometry-aware plug-in modules, showing that structured perceptual priors can significantly enhance adaptability across tasks.On the optimization front, systematic finetuning frameworks like OFT [114] integrate techniques including parallel decoding, action chunking, and continuous representation learning, moving the field from exploratory proofs-of-concept toward an engineering discipline.</p>
<p>Collectively, these strategies illustrate that achieving robust generalization requires architectural innovation, efficient model design, adaptive task specialization, cognitively inspired architectures, and robust runtime strategies, as summarized in Table 2 (C).Nonetheless, challenges persist: safety-critical scenarios is still underdeveloped.Bridging these gaps is essential to transition from experimental prototypes to reliable, general-purpose robotic systems.</p>
<p>The application of diffusion models in VLA systems is evolving toward greater efficiency, robustness, and universality.From foundational action generation modeling to complex multimodal fusion and practical deployment optimization, a comprehensive technological framework has emerged.There are still issues need to be solved, future development trends will continue to address key challenges including model efficiency enhancement, generalization capability improvement, and practical deployment performance optimization.</p>
<p>Discussion</p>
<p>Innovations Diffusion-based models fundamentally reframing robotic control as a generative modeling problem.They enable probabilistic action generation, multimodal architectural fusion, and cognitively inspired deployment strategies, advancing beyond deterministic and modular pipelines.These approaches improve trajectory diversity, geometric grounding, and reasoning integration.Besides, efficiencyfocused designs such as TinyVLA and SmolVLA make realworld deployment increasingly feasible.</p>
<p>Limitations However, due to the fact that temporal coherence in dynamic environments is still fragile, largescale diffusion models demand prohibitive computational resources and datasets, and safety-critical reliability under adversarial or uncertain conditions is underexplored.Furthermore, while multimodal fusion enriches representation, it risks diluting modality-specific strengths, and domain-specialized adaptations may reduce transferability.Addressing these challenges will require more efficient and robust training paradigms, richer safety-aware evaluation standards, and closer alignment between foundation-scale modeling and practical deployment constraints.</p>
<p>Reinforcement-based Fine-Tune Models in Vision Language Action Research</p>
<p>Reinforcement-based Fine-Tune Strategies in VLA Research</p>
<p>Reinforcement-based Vision Language Action (VLA) methods integrate vision-language foundation models with reinforcement learning to enhance perception, reasoning, and decision-making.By leveraging visual and linguistic inputs, these methods generate context-aware actions within interactive and dynamic environments.They have emerged as a critical research direction for advancing autonomous driving, robotics, and broader embodied AI systems.Recent advances demonstrate that reinforcement-based VLA approaches can incorporate human feedback, adapt to novel tasks, and outperform purely supervised paradigms.The progression of these efforts is summarized in Table 3.</p>
<p>Earlier methods improved robot manipulation skills using large-scale human video datasets or robot manipulation datasets by introducing reinforcement reward strategies [172], [173], [174], [181].These methods aimed to investigate the promptability of pre-trained Vision Language Models (VLMs) in reinforcement learning, showing that even frozen models can support efficient downstream policy training through prompt embedding learning.VIP [172] derives a self-supervised goal-conditioned value function independent of actions, generating smooth embeddings that implicitly evaluate value via embedding distance.</p>
<p>Similar to other reinforcement fine-tuning approaches, some methods use language and images to jointly generate reward proxies and obtain cross-modal state language representations through self-supervised contrastive training.These methods emphasize the transferability of rewardaware representations, enabling applications in robot learning under sparse rewards or complex language instructions [173], [181], [190].</p>
<p>Furthermore, some approaches primarily optimize reward functions or loss functions to improve policy learning [175], [178].These methods use language models as intermediaries for reward function design, learning reward proxies through human demonstrations and VLM semantic mapping.This approach simplifies reward engineering, while generalization and interpretability can be further optimized with reinforcement learning from human feedback (RLHF).For example, Elemental demonstrates the ability to rapidly customize task requirements and efficiently learn from limited samples in complex manipulation tasks.SafeVLA [180] explores VLA from a safety perspective, addressing the risks of deploying VLAs in open environments.It proposes a constrained-learning alignment mechanism to prevent highrisk behaviors while maintaining task performance.The method incorporates a safety critic network into the VLA architecture to estimate risk levels and employs the Constrained Policy Optimization (CPO) framework to maximize policy rewards while ensuring that security loss remains below a predefined threshold.SafeVLA significantly reduces risk events in multitask testing-including manipulation, navigation, and handling-particularly in scenarios where ambiguous natural language instructions increase policy uncertainty, thereby demonstrating superior safety and stability.This work provides an essential safety mechanism for deploying VLA models in real-world applications.</p>
<p>Unlike the aforementioned robot arm VLA models, researchers have also investigated VLA frameworks for quadruped and humanoid robots.Using natural language navigation instructions, these robots emphasize trajectory prediction, target description, obstacle avoidance, and related tasks.For example, NaVILA [179] fine-tunes a VLA model with a single-stage reinforcement learning (RL) policy to output continuous control commands, enabling adaptation to complex terrain and dynamically changing language instructions.In contrast, MoRE [184] integrates multiple low-rank adaptive modules as distinct experts into a dense multimodal large language model (MLLM), forming a sparsely activated hybrid expert model that is subsequently trained as a Q-function using reinforcement learning objectives.LeVERB [186] extends this line of research by proposing a hierarchical VLA framework for whole-body control (WBC) of humanoid robots.Similar to NaVILA, Le-VERB couples vision-language processing with dynamicslevel action processing, where reinforcement learning strategies translate potential vocabularies into high-frequency dynamic control commands, enabling complex whole-body task execution.Generates dense reward functions from visual pretraining for unseen tasks.LIV [173] 2023 Learns joint vision-language rewards from action-free videos with text.PR2L [174] 2024 Leverages VLM world knowledge with RL for robot manipulation.ALGAE [175] 2024 Introduces language-guided abstractions to explain RL-driven behavior.GRAPE [176] 2024 Aligns VLAs at trajectory level, modeling rewards from successes and failures.RLDG [177] 2024 Uses RL to generate training data for generalist policy finetuning.ELEMENTAL [178] 2025 Learns reward proxies from demonstrations via VLM-based semantic mapping.NaVILA [179] 2025 Integrates multimodal VLA with RL for quadruped navigation on complex terrain.SafeVLA [180] 2025 Constrains RL alignment to prevent risky actions while maintaining performance.iRe-VLA [181] 2025 Combines SFT stability with RL exploration to boost VLA capability.ReinboT [182] 2025 Enhances Robot GPT with RL to maximize rewards and data quality use.ConRFT [183] 2025 Blends offline BC, Q-learning, and online consistency for safe efficient RL.MoRE [184] 2025 Scales quadruped VLAs with RL and mixed-quality dataset finetuning.SimpleVLA-RL [185] 2025 Trains from one trajectory using binary (0/1) rewards in online RL.LeVERB [186] 2025 Couples VLA reasoning with RL dynamics for humanoid whole-body control.AutoVLA [187] 2025 Employs CoT reasoning with RL optimization for autonomous driving.RPD [188] 2025 Distills student from VLA teacher using RL-based refinement.RLRC [189] 2025 Compresses VLA via pruning, SFT+RL recovery, and quantization.VLA-RL [190] 2025 Improves robustness with online RL and VLM-based reward modeling.AutoDrive-R 2 [191] 2025 Enhances autonomous driving with CoT reasoning and RL self-reflection.ReWiND [192] 2025 Uses language-conditioned rewards for offline RL from small demos.Embodied-R1 [193] 2025 Trains with reinforced fine-tuning and multi-task reward curriculum.IRL-VLA [194] 2025 Applies inverse RL in a reward world model within a VLA framework.ThinkAct [195] 2025 Bridges reasoning and execution with reinforced visual latent planning.</p>
<p>Offline reinforcement learning has proven effective for deriving robust policy models from mixed-quality datasets.ReinboT [182] exemplifies this approach by applying the principle of maximizing cumulative rewards via RL.It enhances understanding of data quality distribution by predicting dense rewards that capture subtle differences in operational tasks, thereby enabling robots to generate more robust decision actions guided by long-term benefits.Online reinforcement learning methods have also been extensively explored in the VLA domain.For instance, SimpleVLA-RL [185] employs only a single trajectory and a binary outcomelevel reward (0/1) to train a VLA model.This method avoids reliance on dense supervision or large-scale behavior cloning datasets, but achieves performance comparable to full trajectory supervised fine-tuning (SFT) with simulating rule-based reward signals in the environment.Recognizing the limitations of using only offline or online strategies, ConRFT [183] introduces a hybrid strategy that combines both.Its offline policy incorporates behavior cloning with Qlearning to extract policies from limited demonstrations and stabilize value estimation, while its online policy introduces consistency goals and artificial intervention mechanisms to steadily improve policy performance, ensuring safe exploration and sample efficiency throughout training.</p>
<p>In the autonomous driving domain, VLA models also leverage reinforcement learning to enhance driving performance in previously unseen scenarios [191].AutoVLA [187] exemplifies this direction by introducing an autoregressive generation model equipped with reasoning and action capabilities.It first processes visual inputs and language instructions, then applies reasoning fine-tuning to produce discrete, feasible actions that can be reconstructed into continuous trajectories.This model employs two finetuning steps-Chain-of-Thought Reasoning and Group Relative Policy Optimization-achieving state-of-the-art performance.</p>
<p>Notably, different from existing models that require enormous numbers of parameters, resulting in high computational and memory demands, some researchers have investigated efficiency strategies such as quantization, pruning, and knowledge distillation within reinforcement learning-based VLAs, often combined with algorithms such as Proximal Policy Optimization (PPO) [196].For instance, RPD [188] distills a student model from a VLA teacher model to increase inference speed, while RLRC [189] introduces a novel compression framework composed of structured pruning, SFT-and RL-based performance recovery, and quantization.These approaches reduce memory usage and improve inference throughput while preserving the task success rate of the original VLA.</p>
<p>Discussion</p>
<p>Innovations reinforcement-based VLA fine-tune strategies using visual and language signals to generate dense, transferable reward proxies, and combining offline behavior cloning with online reinforcement learning stabilizes policy optimization and enhances generalization.Safety-focused approaches also represent an important advancement by integrating constrained optimization to reduce high-risk actions in open-world deployment.Furthermore, extensions to quadruped, humanoid, and autonomous driving tasks highlight the versatility of reinforcement-driven VLA across diverse robotic embodiments.Limitations Despite these advances, reward of reinforcement-based VLA engineering often remains indirect or noisy, leading to suboptimal learning; training stability can be hindered by the interplay between supervised fine-tuning and exploration; and scaling to highdimensional, real-world environments is computationally expensive, requiring substantial hardware and data resources.Additionally, while safety-aware strategies have been proposed, ensuring reliable generalization under ambiguous or adversarial instructions remains an open challenge.Addressing these issues will require more VRB [197] 2023 Learns affordances from human videos to enable complex robot interactions.YAY Robot [198] 2024 Incorporates real-time natural language corrections into robot training.HybridVLA [199] 2025 Combines diffusion-based trajectories with autoregressive reasoning in one framework.RationalVLA [200] 2025 Links high-level reasoning with low-level policies via latent embeddings.OpenHelix [201] 2025 Provides standardized hybrid VLA designs through large-scale empirical studies.EgoVLA [202] 2025 Transfers human wrist/hand actions from video into robot actions.ACTLLM [203] 2025 Enforces action consistency to align perception with executable actions.TUDP [135] 2025 Builds a time-unified diffusion policy with velocity and discrimination signals.</p>
<p>(B) Advances in Multi-Modal Fusion and Spatial Understanding in VLA</p>
<p>CLIPort [204] 2021</p>
<p>Separates "what" and "where" pathways with CLIP to generate action heatmaps.3D-VLA [205] 2024 Integrates perception, language, and action via generative 3D world modeling.ReKep [206] 2024 Leverages relational keypoint graphs for precise spatio-temporal reasoning.RoboPoint [207] 2024 Predicts affordance maps as priors for downstream robotic planning.GMLLMA [208] 2024 Adapts MLLMs across embodiments and action spaces for grounded reasoning.BridgeVLA [209] 2025 Aligns 3D observations to 2D heatmaps for sample-efficient action prediction.GeoManip [210] 2025 Embeds geometric constraints to generalize actions without retraining.TTF-VLA [211] 2025 Fuses past and current visual tokens for training-free inference improvement.MemoryVLA [212] 2025 Builds working memory with perceptual and cognitive tokens plus a memory bank.</p>
<p>(C) Specialized Domain Adaptations and Applications in VLA</p>
<p>GenAug [213] 2023 Uses semantic data augmentation to retarget behaviors with minimal real data.ROSIE [214] 2023 Applies diffusion-based augmentation to enrich manipulation datasets.CoVLA [215] 2024 Builds a large VLA dataset for AD with paired instructions and trajectories.LeVERB [186] 2025 Designs hierarchical policies for humanoid body control with robust sim-to-real transfer.Helix [216] 2024 Unifies humanoid control for manipulation, locomotion, and collaboration.AutoRT [217] 2024 Orchestrates robot fleets via observe-reason-execute with foundation models.MoManipVLA [218] 2025 Adapts fixed-base VLAs to mobile robots through waypoint and motion optimization.CubeRobot [219] 2025 Solves Rubik's Cube with dual-loop VisionCoT and memory stream for high success.EAV-VLA [220] 2025 Develops adversarial patch attacks to destabilize or redirect robot actions.</p>
<p>(D) Foundation Models and Large-Scale Training in VLA</p>
<p>R3M [221] 2022 Learns compact visual representations via time-contrastive, video-language alignment.CACTI [222] 2023 Establishes a scalable four-stage pipeline for multi-task robot learning in simulation and real-world.VC-1 [223] 2024 Studies pre-training data scale using 4,000+ hours of video with MAE-trained transformers.DROID [224] 2025 Provides a large multimodal dataset of 150k+ trajectories across 1k+ objects and tasks.ViSA-Flow [225] 2025 Pre-trains generative models on action flows from large-scale human-object interaction videos.Fine-tuned GMPs [226] 2024 Benchmarks fine-tuning strategies for generalist policies across action spaces and signals.Robot CoT [227] 2025 Introduces lightweight chain-of-thought reasoning for embodied policies with 3 faster inference.CAST [228] 2025 Enhances dataset diversity via counterfactual language and action generation.RoboBrain [229] 2025 Proposes a unified embodied foundation model for perception, reasoning, and planning.</p>
<p>(E) Deployment of VLA Models: Efficiency, Safety, and Human-Robot Collaboration</p>
<p>EdgeVLA [230] 2024 Achieves 6 faster inference by removing dependencies and using compact LLMs.DeeR-VLA [231] 2024 Cuts control cost with confidence-based early exit.Dual Process VLA [232] 2024 Splits reasoning and motor control across dual models for efficiency.CEED-VLA [233] 2025 Speeds up inference 4 with consistency distillation and early exit.RoboMamba [234] 2024 Uses lightweight fusion for resource-limited deployment.ReVLA [235] 2025 Adapts across visual domains to improve robustness.SAFE [236] 2025 Proactively detects failures from internal VLA signals.DyWA [237] 2025 Models dynamics for adaptive control in uncertain environments.CrayonRobo [238] 2025 Provides interpretable grounding via object-centric prompts.cVLA [239] 2025 Enhances sim-to-real transfer with 2D waypoint prediction.RTC [240] 2025 Supports smooth asynchronous execution of chunked policies.</p>
<p>efficient reward representation, robust sample-efficient training paradigms, and richer evaluation benchmarks that capture both safety and reasoning capabilities.</p>
<p>Other Advanced Researches</p>
<p>While autoregressive, diffusion, and reinforcement learning remain the foundational paradigms of VLA model design, the growing complexity and diversity of embodied tasks have spurred the development of approaches that transcend these boundaries.Current research progress can be organized into five key directions: hybrid architectures that integrate multiple generation paradigms, advanced multi-modal fusion for enhanced cross-modal and spatial understanding, specialized domain adaptations addressing task-specific challenges, foundation models and large-scale training paradigms that unify perception-reasoning-control at scale, and practical deployment strategies emphasizing efficiency, safety, and human-robot collaboration.The representative works are summarized in Table 4.</p>
<p>Hybrid Architectures and Multi-Paradigm Integration</p>
<p>As embodied manipulation tasks continue to grow in diversity and complexity, relying on a single generation paradigm (whether autoregressive, diffusion, or reinforcement learning) often proves inadequate.Hybrid architectures have thus emerged as a promising solution, strategically combining multiple paradigms to exploit their complementary strengths.The central objective of this approach is to integrate the smoothness and physical consistency of continuous action generation, the precision of discrete reasoning, and the adaptability needed for dynamic, real-world environments.In doing so, hybrid systems lay the groundwork for more capable and versatile VLA models.</p>
<p>A representative example is HybridVLA [199], which unifies diffusion-based continuous trajectory generation with autoregressive token-level reasoning in a single 7Bparameter framework.This design leverages diffusion processes to produce smooth and physically coherent motion, while retaining the contextual inference capacity inherent in autoregressive models.The dual-system philosophy, inspired by cognitive science, has also been embraced in recent works.Fast-in-Slow [241] operationalizes Kahneman's dual-process theory by embedding a low-latency execution module within a slower yet cognitively richer VLM backbone.This enables real-time responsiveness while preserving high-level reasoning.Similarly, RationalVLA [200] integrates vision-language reasoning with low-level manipulation policies through learnable latent embeddings, allowing the model to filter out infeasible commands and plan executable actions.</p>
<p>Scaling hybrid designs has also shown considerable promise.Transformer-based Diffusion Policy [242] demonstrates that billion-parameter architectures can effectively combine diffusion processes with attention mechanisms, surpassing conventional U-Net designs by capturing richer contextual dependencies for trajectory modeling.This trend points toward the next generation of VLA systems that embed autoregressive Transformers within diffusion-based planners, achieving greater context-awareness alongside higher-quality motion generation.</p>
<p>Beyond individual innovations, initiatives such as Open-Helix [201] are moving toward the systematization of hybrid VLA design.Through large-scale empirical evaluation, OpenHelix benchmarks alternative reasoning-execution integration strategies and provides open-source implementations alongside design guidelines.This shift signals a maturation of the field, fostering reproducibility and standardization in hybrid VLA development.The progression of these efforts is summarized in Table 4 (A), which outlines the key innovations driving hybrid VLA architectures.</p>
<p>Advanced Multi-Modal Fusion and Spatial Understanding</p>
<p>Achieving robust manipulation in complex environments requires more than straightforward cross-modal alignment; it calls for structured, task-aware fusion mechanisms capable of capturing fine-grained semantics and spatial relationships.Recent progress reflects a decisive shift away from early feature concatenation toward architectures that explicitly model geometry, affordances, and spatial constraints.These advances are propelling VLA models toward richer spatial grounding and more reliable action generation in unstructured, 3D-aware settings.</p>
<p>Early efforts such as CLIPort [204] laid the foundation by disentangling visual processing into a "what" pathway for object identification and a "where" pathway for action localization.Leveraging CLIP-based representations, CLIPort generates pick-and-place heatmaps from paired image-language inputs, demonstrating the advantages of structured visual reasoning in language-conditioned manipulation.Building upon this foundation, subsequent research has emphasized 3D spatial understanding as a core competency.VoxPoser [243] introduces composable 3D value maps guided by large language models, dividing instruction interpretation into target understanding and action planning over voxelized scene representations.This modular design enhances generalization by cleanly separating semantic parsing from spatial reasoning.Similarly, 3D-VLA [205] integrates autoregressive language modeling with diffusionbased action prediction in a generative 3D world model, achieving coherent unification of perception, language, and action modalities.</p>
<p>The challenge of multi-view perception has been tackled through unified representation learning.RoboUniView [244] employs multi-view Transformer modules to fuse temporal and spatial cues, substantially improving 3D scene geometry understanding compared to single-view baselines.In contrast, BridgeVLA [209] projects 3D observations into multiple 2D views and predicts actions within a unified 2D heatmap space, to highlight the efficiency of compact yet spatially grounded representations.To handle more demanding scenarios, specialized spatial reasoning methods have emerged.ReKep [206] models spatio-temporal dependencies via relational keypoint graphs, excelling in precision-critical tasks.RoboPoint [207] predicts affordance maps that highlight feasible interaction regions, providing essential perception priors for downstream planning.GeoManip [210] integrates symbolic geometric constraints to guide action generation without requiring task-specific retraining, thereby achieving strong out-of-distribution generalization.</p>
<p>Taken together, these works trace a clear trajectory: from early pathway-based 2D fusion to modular, 3D-aware architectures that unify spatial grounding, semantic reasoning, and action generation.As VLA systems increasingly operate in unconstrained, real-world environments, the capacity to reason explicitly about geometry and affordances will remain a decisive factor for achieving robust and generalizable manipulation.Table 4 (B) summarizes this progression.</p>
<p>Specialized Domain Adaptations and Applications</p>
<p>The versatility of the VLA framework has enabled its expansion into specialized embodied domains that pose unique perceptual, reasoning, and control challenges.Such adaptations not only validate the generality of VLA principles but also reveal the architectural and algorithmic modifications necessary for domain-specific success.From safetycritical robotics to fully digital interaction, these innovations demonstrate the adaptability of VLA pipelines to varied operational contexts.</p>
<p>In safety-critical contexts such as autonomous driving, CoVLA [215] presents the first large-scale VLA dataset tailored for this domain, comprising approximately 50,000 paired language instructions and driving trajectory videos across diverse urban scenarios.This work illustrates how vision-language reasoning can be coupled with continuous control policies for navigation and hazard avoidance.</p>
<p>The VLA paradigm has also been extended to graphical user interface (GUI) interaction, where perception-action loops operate in fully digital spaces.ShowUI [245] adopts a vision-language-action pipeline for processing on-screen elements and generating control sequences for actions such as clicking, dragging, and form filling. Its strong performance on GUI-Bench underscores the applicability of VLA principles to non-physical manipulation tasks.</p>
<p>Humanoid whole-body control has emerged as another challenging domain.LeVERB [186] proposes a hierarchical architecture in which a vision-language policy learns latent action vocabularies from kinematic demonstrations, while a reinforcement-learned control layer produces low-level dynamics commands.This two-level design bridges the semantic-control gap, enabling robust sim-to-real transfer across more than 150 tasks.Similarly, Helix [216] demonstrates that a single unified policy network can acquire diverse humanoid behaviors, from object manipulation to cross-robot collaboration, without task-specific retraining.</p>
<p>Specialized adaptations also target large-scale robot orchestration and mobile manipulation.AutoRT [217] coordinates heterogeneous robot fleets via an observe-reason-execute framework that delegates strategic planning to VLMs such as PaLM-E and RT-2, while MoMa-nipVLA [218] transfers fixed-base VLA models to mobile manipulation settings through waypoint-based trajectory generation and dual-layer motion optimization.</p>
<p>Other domain-specific innovations incorporate physical reasoning or task-specialized cognitive structures.Physically grounded VLA [246] embed modules for estimating stability and contact points, improving manipulation under complex physical constraints.CubeRobot [219] applies a dual-loop VisionCoT and Memory Stream design to Rubik's Cube solving, achieving near-perfect success rates in lowand medium-complexity tasks, and strong performance in high-difficulty scenarios.</p>
<p>Overall, these domain-driven adaptations demonstrate the versatility of VLA architectures, as well as the importance of tailoring perception-reasoning-control pipelines to meet the specific demands of different operational contexts.They also reinforce the potential of VLA models as a unifying embodied intelligence framework spanning physical, digital, and hybrid environments.These specialized adaptations are summarized in Table 4 (C), which highlights the architectural and algorithmic innovations enabling VLA systems to succeed across diverse embodied domains.</p>
<p>Foundation Models and Large-Scale Training</p>
<p>The rise of foundation models and large-scale training has reshaped the trajectory of VLA research, enabling unified perception-reasoning-control frameworks that generalize across tasks, embodiments, and environments.By leveraging massive multi-modal datasets and scalable architectures, this direction seeks to build generalist embodied agents with broad capabilities and efficient adaptation.Large-scale pretraining is increasingly becoming the backbone for nextgeneration VLA systems.Recent foundation models provide systematic researches in robotics, covering vision-language models, policy models, and cross-modal alignment techniques for manipulation, navigation, and planning, specifically focus on VLA architectures, organizing them into perception-aligned, policy-generative, and world-modelbased categories, while identifying a unifying trend toward tightly integrated multi-modal interfaces [247].</p>
<p>Large-scale datasets have been pivotal in enabling foundation-scale training.DROID [224] contributes over 150,000 trajectories spanning more than 1,000 objects and task scenarios, with multimodal annotations including RGB-D, language, low-dimensional states, and environment labels.The General Flow framework [248] uses 3D point trajectories as transferable affordance representations, enabling cross-domain skill transfer from humans to robots.Similarly, ViSA-Flow [225] pre-trains generative models on semantic action flows extracted from large-scale human-object interaction videos, requiring minimal adaptation for downstream robot learning.</p>
<p>Training strategies have also been extensively studied to improve efficiency and adaptability.Zhang et al. [226] analyze fine-tuning factors-including action space, policy head design, and supervision signals-through 2,500 rollout experiments, offering practical guidelines for adapting foundation-scale VLA models.Chen et al. [227] investigate the integration of chain-of-thought reasoning into embodied policy learning, demonstrating that lightweight reasoning mechanisms can yield significant performance gains with a 3 inference speedup compared to standard approaches.</p>
<p>Together, these efforts indicate a converging trajectory toward generalist embodied agents trained on massive, diverse datasets and equipped with modular reasoning capabilities.The combination of large-scale pre-training, efficient adaptation, and transferable affordance representations is positioning foundation-scale VLA models as the backbone for next-generation robotic intelligence.The representative works in this direction are summarized in Table 4 (D), highlighting both data-centric and algorithmic advances driving foundation-scale VLA research.</p>
<p>Practical Deployment over Efficiency, Safety, and Human-Robot Collaboration</p>
<p>As VLA models transition from research to real-world applications, practical deployment demands a holistic focus on efficiency, robustness, and human-robot interaction.Realtime inference, resilience to adversarial conditions, and seamless collaborative workflows are critical for reliable operation in dynamic, unpredictable environments.This direction integrates system optimization with safety and adaptability, ensuring that high-capacity models remain both effective and trustworthy in practice.</p>
<p>Efficiency-Oriented Designs have focused on reducing inference latency, lowering computational demands, and improving adaptability to resource-constrained platforms.For real-time execution, RTC (Real-Time Chunking) [240] predicts upcoming action segments while executing current ones, enabling continuous high-frequency control.EdgeVLA [230] eliminates autoregressive dependencies in end-effector prediction and incorporates compact language models, achieving a 6 speedup with minimal performance degradation.Similarly, DeeR-VLA [231] employs dynamic Maintaining knowledge integrity during adaptation has become another priority.Knowledge-insulating VLA models [249] address semantic degradation when integrating specialized modules into pretrained VLMs, using insulation strategies to retain cross-task generalization.Consistency-based acceleration strategies, such as CEED-VLA [233], apply consistency distillation and early-exit decoding to achieve over 4 inference acceleration while mitigating error accumulation through mixed-label supervision.Lightweight multi-modal fusion approaches like Robo-Mamba [234] and cross-domain adaptation methods such as ReVLA [235] further contribute to deployable efficiency.</p>
<p>Safety and Robustness have emerged as equally critical pillars for deployment readiness.SAFE [236] leverages internal VLA feature representations to detect failures across multiple tasks, generalizing to unseen scenarios and enabling proactive intervention.Security assessments by Cheng et al. [250] via the Physical Vulnerability Evaluation Procedures (PVEP) reveal vulnerabilities to adversarial patches, typography-based prompts, and distributional shifts, motivating the development of adversarially robust perception-control pipelines.Interpretability-focused work, such as Lu et al. [251], uncovers symbolic encodings of objects, relations, and actions in VLA hidden layers, laying the groundwork for more transparent decision-making.Adaptive control frameworks like DyWA [237] further enhance robustness by jointly modeling geometry, state, physics, and action to respond to dynamic, partially observable conditions.</p>
<p>Human-Robot Collaboration research has explored interactive learning loops where humans and VLA models refine each other's performance.Xiang et al. [252] propose collaborative frameworks that integrate limited expert interventions into VLA decision-making, reducing operator workload while enriching model training data.Closed-loop strategies like those in Zhi et al. [253] combine GPT-4V perception with real-time feedback control to adapt to environmental changes on the fly.History-aware policy learning [254] and object-centric visual prompting approaches such as CrayonRobo [238] enhance task grounding and transparency, while skill library construction [255] and grounding mask methods [256] enable scalable, reusable task decomposition.Camera-space policy designs like cVLA [239] improve sim-to-real transfer by predicting trajectory waypoints directly in 2D image coordinates, making policies more embodiment-agnostic.The representative methods for practical deployment are summarized in Table 4(E), highlighting key innovations across efficiency, safety, and human-robot collaboration.</p>
<p>In summary, practical deployment of VLA systems demands a multi-faceted design philosophy that simultaneously addresses efficiency, safety, and collaborative adaptability.The integration of real-time inference optimization, robustness against failures and adversarial conditions, and human-in-the-loop refinement strategies is paving the way for persistent, reliable, and interactive robotic systems in real-world environments.</p>
<p>Discussion</p>
<p>Innovations The surveyed other advanced VLA highlight several innovations that collectively extend VLA research beyond the confines of previous section.Hybrid architectures that combine complementary paradigms for both reasoning and action generation, advanced multimodal fusion for 3D-aware spatial grounding, and domain adaptations that extend VLA principles to areas such as autonomous driving, humanoid control, and GUI interaction.Foundation-scale models leverage massive multimodal datasets to build increasingly generalist agents, while deployment-oriented methods emphasize efficiency, safety, and human-robot collaboration for real-world applicability.</p>
<p>Limitations However, these hybrid systems remain computationally costly and complex to scale, and multi-modal fusion still struggles with noisy or incomplete real-world inputs.Domain-specific adaptations risk overfitting to narrow contexts, while foundation models demand prohibitive data and resource investments.Deployment efforts, though promising, continue to face challenges in robustness, interpretability, and reliability under adversarial or dynamic conditions.Addressing these limitations will require more efficient training strategies, broader evaluation standards, and tighter integration between research design and practical deployment.</p>
<p>DATASETS AND BENCHMARKS</p>
<p>Like other imitation learning approaches, Visual Language Action (VLA) models rely on high-quality labeled datasets.These datasets are either collected from real-world scenarios or generated using simulation environments, the dataset</p>
<p>Dataset Name Year Sensors Episodes Tasks (A) Real-world Datasets and Benchmarks</p>
<p>MIME [257] 2018 RGBD 8300 20 EPIC-KITCHENS [258] 2018 RGB 10000 32</p>
<p>RoboNet [259] 2019 RGB 162000 -MT-Opt [260] 2021 RGB 800000 12 BridgeData [23] 2021 RGBD 60100 24 Bc-z [261] 2022 RGB 25877 100 RT-1 [262] 2022 RGB 13000 700 MOO [263] 2023 RGB 59100 -RoboHive [264] 2023 RGBD 98500 38 OBRH [265] 2023 RGBD 5600 109 RH20T [266] 2024 RGBD 110000 147 DROID [267] 2024 RGBD 76000 -AutoRT [268] 2024 RGB 77000 -UMI [269] 2024 RGB 1400 7 OXE [8] 2025 RGBD &gt;1000000 160266</p>
<p>(B) Simulation Datasets and Benchmarks</p>
<p>ROBOTURK [270] 2018 RGB &gt;7868 2200 Meta-World [271] 2019 RGB -50 ALFRED [272] 2019 RGBD 8055 120 RLBench [273] 2019 RGBD -100 VIMA 0.5 [274] 2022 RGB &gt;600000 &gt;1000 CALVIN 0.5 [275] 2022 RGBD -34 LIBERO [276] 2023 RGB 5000 100 Lota-Bench [277] 2024 RGB 611 12 Mobile ALOHA [278] 2024 RGB 825 7 RoboCasa [279] 2024 RGBD &gt;100000 100 RoboGen [280] 2024 RGB -106</p>
<p>samples are illustrated in Fig. 4. Typically, they contain multimodal observations-such as images, LiDAR point clouds, and inertial measurement unit (IMU) readings-along with corresponding ground-truth labels and language instructions.To facilitate a systematic understanding, we analyze existing datasets and benchmarks, and propose a taxonomy that organizes datasets according to their complexity, modality, and task diversity.This taxonomy provides a clear framework for evaluating the suitability of different datasets for VLA research and highlights potential gaps in existing resources, the representative works are summarized in Table 5.</p>
<p>Real-World Datasets and Benchmarks</p>
<p>High-quality real-world datasets are fundamental for the development of reliable VLA algorithms.In recent years, numerous high-quality and diverse real-world robotics datasets have been collected.Researchers have gathered datasets using different sensor modalities, across a variety of tasks and environmental settings.</p>
<p>Real-World Datasets and Benchmarks for Embodied Robotics</p>
<p>A real-world embodied robotics dataset refers to a collection of multimodal data acquired from robots that interact with their environments through perception and action.Embodied robotics datasets are specifically designed to capture the complex interactions between visual, auditory, proprioceptive, and tactile sensory inputs and the corresponding motor actions, intentions, and environmental contexts.They are essential for training and evaluating models in embodied AI, where the objective is to enable robots to perform tasks through closed-loop, adaptive behaviors in dynamic environments.By providing rich, temporally aligned observations and actions, these datasets serve as foundational resources for developing and benchmarking algorithms in imitation learning, reinforcement learning, visual-language action, and robotic planning.Current embodied robot datasets faces significant data costs issues because real-world robot data is not largely collected.Collecting real-world robot datasets poses many challenges.It not only requires hardware equipments, but also requires precise manipulation.Among them, MIME [257], RoboNet [259], and MT-Opt [260] have collected largescale robot demonstration datasets spanning a range of tasks, from simple object pushing to complex household object stacking.Different from prior datasets that often assume a single optimal trajectory per task, these datasets include multiple demonstrations for the same task, using the minimum distance among test trajectories as an evaluation metric.This approach has significantly advanced research in manipulation and VLA tasks.BridgeData [23] offers a large-scale, multi-domain robot dataset comprising 71 tasks across 10 environments.Experiments demonstrate that jointly training on this dataset along with a small subset (e.g., 50 tasks) of unseen tasks in a new domain can double success rates compared to using target domain data alone.As a result, many contemporary VLA methods adopt BridgeData for model training.In the embodied AI field, model generalization is often limited by the difficulty of collecting diverse real-world robotic data.RT-1 [262] provides a broad dataset of real-world robotic tasks to improve both task performance and generalization to novel scenarios.Similarly, Bc-z [261] includes previously unseen manipulation tasks involving novel combinations of objects within the same scene, supporting research in generalizable policy learning.Several datasets also provide comprehensive software platforms and ecosystems for Embodied AI, covering environments such as hand manipulation, locomotion, multi-tasking, multi-agent scenarios, and musclebased control [263].Compared to earlier works, RoboHive [264] bridges the gap between current robot learning capabilities and potential growth, supporting diverse learning paradigms including reinforcement, imitation, and transfer learning.Distinctively, RH20T [266] offers 147 tasks encompassing 110K manipulation episodes, including multimodal visual, force, audio, and action data.Each episode is accompanied by a human demonstration and language description, making this dataset particularly suitable for one-shot imitation learning and policy transfer to new tasks based on previously trained episodes.</p>
<p>To advance the development of more generalizable manipulation policies, the robotics community must prioritize the collection of large-scale, diverse datasets spanning a wide range of tasks and environmental settings.Several datasets have been collaboratively gathered by multiple robots across different regions, making them among the most geographically and contextually diverse embodied robot datasets to date [267], [268], [269], [281].In addition, Open X-Embodiment (OXE) [8] consolidates 22 robot datasets collected through collaboration among 21 insti-tutions, covering 527 skills and 160,266 tasks.OXE provides standardized data formats to facilitate easy use by researchers.An overview of these datasets is provided in Table 5 (A).</p>
<p>For benchmark evaluation, researchers typically use Success Rate-the proportion of tasks successfully completed relative to the total number of tasks.Some studies additionally employ Language Following Rate to assess the model's ability to interpret and execute language instructions.Furthermore, recent VLA models are often evaluated by transferring trained policies to previously unseen environments to measure robustness and generalization performance [2], [21].</p>
<p>Real-World Datasets and Benchmarks for Autonomous Driving</p>
<p>Autonomous driving dataset is different from embodied robot dataset.It has emerged as one of the most transformative applications of artificial intelligence, relying heavily on large-scale datasets to train and evaluate perception, planning, and control algorithms.High-quality datasets are foundational for the development of robust and generalizable autonomous driving systems, as they enable supervised learning, benchmarking, and simulation of rare or safety-critical scenarios.Over the past decade, numerous datasets such as, [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], have been introduced, offering multi-modal sensor data including camera images, LiDAR point clouds, radar signals, and high-definition maps.These datasets vary significantly in geographic coverage, sensor configuration, driving behavior diversity, and annotation richness, making them complementary resources for research and development.</p>
<p>However, most public datasets are collected in open-loop settings and primarily represent normal driving behavior, which limits their ability to cover long-tail corner cases.To address this gap, recent efforts have focused on generating synthetic data, simulating closed-loop interactions, and curating datasets tailored to rare or safety-critical events.Continued innovation in dataset design remains vital for advancing safe, scalable, and generalizable autonomous driving systems.</p>
<p>For evaluation, autonomous driving VLA models commonly rely on metrics such as L2 distance, which measures deviation from reference trajectories-and completion rate, which quantifies the proportion of successfully completed driving tasks.</p>
<p>Simulation Datasets and Benchmarks</p>
<p>Collecting large-scale real-world data for continuous control tasks poses significant challenges, as these tasks require real-time interaction and continuous feedback from human annotators.Moreover, acquiring such data is often costly and time-consuming, limiting its scalability.This enables a scalable mechanism for diverse human supervision on an extensive set of problem instances.To investigate the performance of embodied robot or autonomous driving models in large-scale and high-quality data, researchers utilize the simulated data from virtualization engines for training and evaluation.</p>
<p>Simulation Datasets and Benchmarks for Embodied Robotics</p>
<p>Simulation datasets for embodied AI typically include synthetic scenes, physics-based interactions, annotations for navigation, object manipulation, task execution, and agentenvironment dynamics.These datasets allow for benchmarking and training across a wide range of tasks, from visual navigation and semantic exploration to complex multistep object manipulation.Prominent examples include [271], [273], [280], [292], [293], each offering different trade-offs in realism, task diversity, and control fidelity.By enabling safe experimentation and massive-scale data collection, simulation datasets are foundational to the development of robust, generalizable embodied agents.As the field matures, the design of richer, more realistic simulation datasets, covering diverse embodiments, tasks, and environments, continues to drive progress toward real-world deployment.</p>
<p>ROBOTURK [270] is a simulated dataset for high quality 6-DoF manipulation states and actions, collected through teleoperation using mobile devices.Unlike traditional approaches that rely on remote users to demonstrate actions within a virtual engine, ROBOTURK leverages policy learning to generate multi-step robot tasks with varying rewards.By aggregating large quantities of demonstrations, the dataset provides precise and reliable data for both training and evaluation.iGibson0.5[294] introduces a benchmark for training and evaluating interactive navigation solutions.This work not only provides a novel experimental simulation environment but also proposes a dedicated metric to assess the interplay between navigation and physical interaction along navigation paths.The benchmark introduces the Interactive Navigation Score, composed of two submetrics: Path Efficiency and Effort Efficiency.Path Efficiency is defined as the ratio between the length of the shortest successful path and the actual path length traversed by the robot, weighted by a success indicator function.Effort Efficiency captures the excess kinematic and dynamic effort required during navigation, reflecting the cost of physical interactions.VIMA [274] introduces a new benchmark, VIMA-BENCH, which establishes a four-level evaluation protocol to assess progressively stronger generalization capabilities, ranging from randomized object placement to entirely novel tasks.Similarly, CALVIN and LOTA-Bench [275], [277] focus on learning long-horizon, language-conditioned tasks across diverse manipulation environments using multimodal robot sensor data.These benchmarks are particularly suited for evaluating methods that aim to generalize to unseen entities by training on large-scale interaction datasets and testing on novel scenes.Performance in these benchmarks is typically measured using task success rates.An overview of these simulation datasets is provided in Table 5 (B).</p>
<p>Simulation Datasets and Benchmarks for Autonomous Driving</p>
<p>Closed-loop simulation plays a critical role in ensuring the safety of autonomous driving systems, as it enables the generation of safety-critical scenarios that are difficult or dangerous to capture in the real world.While previously recorded driving logs provide valuable resources for constructing new scenarios, closed-loop evaluation demands modifications to the original sensor data in order to reflect updated scene configurations.For example, actors may need to be added or removed, and the trajectories of both existing actors and the ego vehicle may differ from those in the original recordings [295], [296].UniSim [297] is a neural sensor simulator that extends single recorded trajectories into multi-sensor closed-loop simulations.It constructs neural feature grids to reconstruct both static backgrounds and dynamic actors, compositing them to simulate LiDAR and camera data from novel viewpoints.This allows for the addition, removal, or repositioning of actors.To better accommodate unseen viewpoints, UniSim further employs a convolutional network to complete regions not visible in the original data.</p>
<p>Unlike real-world autonomous driving datasets, closedloop simulation benchmarks require specialized evaluation metrics tailored to interactive driving tasks.Commonly used metrics include Driving Route (measuring adherence to planned trajectories), Infraction Score (penalizing traffic rule violations), and Completion Score (assessing task completion).Together, these metrics provide a more comprehensive assessment of VLA model performance in realistic, safety-critical driving scenarios.</p>
<p>Discussion</p>
<p>Innovations This paper introduce systematic taxonomies, standardized evaluation metrics, and large-scale collaborative efforts such as Open X-Embodiment (OXE), which unifies datasets from multiple institutions to promote reproducibility and generalization.These contributions enable broader coverage of tasks, richer modality combinations, and improved policy transfer across domains, advancing the scalability of embodied AI research.</p>
<p>Limitations However, real-world datasets are expensive and logistically challenging to collect, often restricted to controlled laboratory environments with limited scene diversity; simulation datasets, while scalable and safe, still struggle to fully capture the complexity, noise, and unpredictability of real-world interactions.Moreover, benchmark metrics such as success rate and trajectory deviation may insufficiently reflect nuanced capabilities like language grounding, long-horizon reasoning, or safe deployment in unstructured environments.Addressing these limitations requires not only expanding dataset diversity and realism but also designing richer evaluation protocols that better capture the demands of real-world autonomy.</p>
<p>SIMULATORS</p>
<p>Robot simulators have become indispensable tools for developing and evaluating intelligent robotic systems in diverse and interactive environments.These platforms typically integrate physics engines, sensor models (e.g., RGB-D, IMU, LiDAR), and task logic to support a wide range of tasks such as navigation, manipulation, and multimodal instruction following.State-of-the-art simulators provide scalable, photorealistic, and physically plausible environments for training embodied agents using reinforcement learning, imitation learning, or large pre-trained models.By offering safe, controllable, and reproducible settings, embodied simulators accelerate the development of generalizable robotic intelligence while significantly reducing the cost and risks associated with real-world experimentation [292], [298].</p>
<p>THOR [24] is a simulator featuring near photo-realistic 3D indoor scenes, where AI agents can navigate environments and interact with objects to complete tasks.It supports a variety of research areas, including imitation learning, reinforcement learning, manipulation planning, visual question answering, unsupervised representation learning, object detection, and semantic segmentation.In contrast, some simulators are based on virtualized real spaces rather than artificially designed environments, encompassing thousands of full-scale buildings equipped with embodied agents that are subject to realistic physical and spatial constraints [299], [300].Habitat [25] and Habitat 2.0 [301] further extend this paradigm by offering scalable simulation platforms for training embodied agents in complex 3D environments with interactive, physics-enabled scenarios.ALFRED [272] introduces a benchmark consisting of long-horizon, compositional tasks with non-reversible state changes, aiming to bridge the gap between simulation benchmarks and real-world applications.ALFRED includes both high-level goals and low-level language instructions, making tasks significantly more complex in terms of sequence length, action space, and linguistic variability compared to existing vision-and-language datasets.</p>
<p>Earlier simulation environments that combined physics and robotic tasks often focused on a narrow set of scenarios and featured only small-scale, simplified scenes.In contrast, iGibson 1.0 [302] and iGibson 2.0 [303] are open-source simulation platforms that support a more diverse set of household tasks within large-scale, realistic environments.Their scenes are replicas of real-world homes, with object distributions and layouts closely aligned with physical spaces, thereby enhancing ecological validity and bridging the gap between simulated and real-world robotic learning.</p>
<p>Advanced simulators not only allow multiple agents to interact within the same environment but also provide a wide range of sensor and physics outputs.Ideally, such simulators should combine a universal physics engine, a flexible robotics simulation platform, and a high-fidelity rendering system.These features make them powerful tools for both robotic simulation and generative model evaluation [304], [305], [306], [307], [308], [309].</p>
<p>MuJoCo [26] is a widely adopted, open-source physics engine designed to facilitate research and development in robotics and related domains that require accurate simulation.More recently, GPU-based simulation engines have gained popularity. Notably, NVIDIA Isaac Gym [27], built on the Omniverse platform, enables large-scale development, simulation, and testing of AI-driven robots in physically realistic virtual environments.Isaac Gym has become increasingly popular in both academia and industry for accelerating the creation of new robotics tools and enhancing existing systems.</p>
<p>Similar challenges exist in autonomous driving, where large-scale real-world data collection and annotation are both costly and time-consuming.Collecting sufficient data to cover the multitude of rare corner cases is particularly difficult.To address this, researchers have developed simulators that include both static road elements (e.g., intersections, traffic lights, and buildings) and dynamic agents (e.g., vehicles and pedestrians).CARLA [28] and LGSVL [310] leverage game engines to render realistic driving scenarios, supporting flexible sensor configurations and generating signals suitable for training and evaluating driving strategies.These platforms have become critical for advancing autonomous driving research by providing controllable, reproducible, and cost-effective testing environments.</p>
<p>ROBOT HARDWARE</p>
<p>The physical structure of a robot provides the foundation for perception, locomotion, manipulation, and interaction with its environment.Its core components typically include sensors, actuators, power systems, and a control unit.Sensors-such as cameras, LiDAR, inertial measurement units, and tactile arrays-supply essential information about both the external environment and the robot's internal state.Actuators, including motors, servos, or hydraulic systems, convert control signals into physical actions, enabling tasks such as locomotion and object manipulation.The control unit, generally based on embedded processors or microcontrollers, functions as the computational core by integrating sensor inputs and issuing commands to actuators.Power systems, typically in the form of batteries or external energy sources, sustain continuous operation.Hardware design must balance performance, energy efficiency, weight, and durability to satisfy task-specific requirements across diverse application domains, including industrial automation, service robotics, and autonomous vehicles [311], [312], [313].</p>
<p>CHALLENGES AND FUTURE DIRECTIONS</p>
<p>Challenges of Vision Language Action Models</p>
<p>This section summarizes the open challenges and future directions in advancing Vision Language Action (VLA) models.Despite remarkable progress in recent years, the development of VLA models has gradually revealed critical bottlenecks.The most fundamental issue lies in the fact that current VLA systems are largely built upon the transfer of large-scale LLMs or VLMs.Although these models excel in semantic understanding and cross-modal alignment, they lack direct training and experience in interacting with the physical world.As a result, VLA systems often demonstrate the gap of "understanding the instruction but failing to execute the task" in real environments.This reflects a fundamental contradiction: the disconnection between semanticlevel generalization and embodied capabilities in the physical world.How to achieve the transformation from nonembodied knowledge to embodied intelligence, and to truly bridge the gap between semantic reasoning and physical execution, remains the central challenge.Specifically, this contradiction is manifested in the following aspects.</p>
<p>Scarcity of Robotic Data</p>
<p>Robotic interaction data is a critical resource that determines the performance of VLA models; however, existing datasets remain insufficient in both scale and diversity.Collecting large-scale demonstrations across a wide range of tasks and environments in the real world is constrained by hardware costs, experimental efficiency, and safety concerns.Existing open-source datasets, such as Open X-Embodiment, have advanced robot learning but are primarily focused on tabletop manipulation and object grasping.This lack of task and environmental diversity significantly limits generalization to novel contexts and complex tasks.Simulation platforms, such as RLBench, offer a cost-effective means of generating large-scale trajectories but are constrained by rendering fidelity, physics engine accuracy, and task modeling limitations.Even with techniques such as domain randomization or style transfer, the sim-to-real gap persists, with many models performing well in simulation yet failing when deployed on physical robots.Therefore, enhancing both the diversity and realism of robotic data at scale remains a primary challenge in mitigating generalization deficits.</p>
<p>Architectural Heterogeneity</p>
<p>Most VLA models attempt end-to-end modeling across vision, language, and action, but their implementations reveal strong heterogeneity.On one hand, different works adopt different backbone networks: vision encoders may rely on ViT, DINOv2, or SigLIP; language backbones on PaLM, LLaMA, or Qwen; and action heads on discrete tokenization, continuous control vectors, or even diffusion-based generation.Such architectural diversity hinders comparison and reuse across models, slowing the emergence of unified standards.On the other hand, perception, reasoning, and control are often loosely coupled internally, leading to fragmented feature spaces and weak portability across platforms or task domains.Some models excel at crosstask language understanding but require heavy adaptation when interfacing with low-level controllers.This architectural heterogeneity increases integration complexity and significantly constrains generalization and scalability.</p>
<p>Real-Time Inference Constraints and Cost</p>
<p>Current VLA models are heavily dependent on large-scale Transformer architectures with autoregressive decoding, which severely limits inference speed and execution efficiency on real robots.Since each action token depends on the previous one, latency accumulates, while highfrequency tasks such as dynamic grasping or mobile navigation demand millisecond-level responses.Moreover, highdimensional visual inputs and massive parameter counts impose prohibitive computation and memory costs.Many state-of-the-art VLAs require GPU memory far beyond the capacity of typical embedded platforms.Even with quantization, compression, or edge-cloud co-inference, it remains difficult to balance accuracy, real-time performance, and low cost.This combination of inference constraints and hardware bottlenecks leaves VLA deployment trapped between being too slow and too expensive.</p>
<p>Pseudo-Interaction in Human-Robot Interaction</p>
<p>Systems generate actions based on prior knowledge or static training patterns rather than engaging in genuine interaction grounded in environmental dynamics and causal reasoning.When encountering unfamiliar settings or state changes, models often rely on statistical co-occurrence learned from data, rather than probing the environment or using sensor feedback to refine actions.This lack of causal reasoning means that VLAs may appear to follow instructions but fail to establish genuine causal chains between environment states and action outcomes.As a result, robots often fail to adapt in dynamic environments.This pseudointeraction highlights VLA's deficiency in causal modeling and feedback utilization, and remains a key obstacle to embodied intelligence.</p>
<p>Evaluation and Benchmarking Limitations</p>
<p>The evaluation of VLA models is also limited.Current benchmarks are mainly set in laboratory or highly structured simulated environments, focusing on tabletop manipulation or object grasping.While such tasks measure performance on narrow distributions, they do not capture generalization or robustness in open-world scenarios.Once deployed in outdoor, industrial, or complex home settings, performance often degrades drastically, exposing the gap between evaluation and real-world applicability.This narrow evaluation scope hinders comprehensive assessment of VLA feasibility and limits horizontal comparisons across models.The lack of unified, authoritative, and diverse benchmarks is becoming a major bottleneck for real-world progress.</p>
<p>While these five aspects highlight key shortcomings in data, architecture, interaction, and evaluation, they do not exhaust the challenges faced by VLA research.More fundamentally, long-term is whether VLA systems can truly achieve controllability, trustworthiness, and safety.In other words, the future of VLA requires not only solving performance and generalization issues but also addressing the deeper concerns of deploying intelligent agents responsibly.This transition implies that researchers must move beyond model optimization toward systemic paradigm shifts in order to meet long-term challenges.</p>
<p>Opportunities of Vision Language Action Models</p>
<p>Despite formidable challenges, the future of VLA is also full of opportunities.As the crucial bridge connecting language, perception, and action, VLA has the potential to transcend the semantic-physical gap and become a central pathway for embodied intelligence.Overcoming current bottlenecks could reshape the paradigm of robotics research and position VLA at the forefront of real-world deployment.</p>
<p>World Modeling and Cross-Modal Unification</p>
<p>At present, language, vision, and action remain loosely coupled in VLA systems, confining them to instruction "generation" rather than holistic world understanding.Achieving true cross-modal unification would enable VLA to model environments, reasoning, and interaction jointly within a single token stream.This unified structure would allow VLA to evolve into a proto-world model, enabling robots to close the loop from semantic understanding to physical execution.Beyond a technical advance, this would mark a crucial step toward artificial general intelligence.</p>
<p>Breakthroughs in Causal Reasoning and Genuine Interaction</p>
<p>Most existing VLAs rely on static data distributions and surface-level correlations, lacking the ability to interact based on causal laws.They simulate interaction by guessing from prior patterns, rather than probing the environment and updating strategies with feedback.If future VLAs can incorporate causal modeling and interactive reasoning, robots will learn to probe, validate, and adapt-enabling genuine dialogue with dynamic environments.Such a breakthrough would overcome pseudointeraction and mark the transition from data-driven intelligence to deeply interactive intelligence.</p>
<p>Virtual-Real Integration and Large-Scale Data Generation</p>
<p>While data scarcity is a critical limitation, it also represents a massive opportunity.If virtual and real data ecosystems can be integrated-through high-fidelity simulation, synthetic data generation, and multi-robot sharing-it will be possible to construct datasets containing trillions of trajectories across diverse tasks.Just as GPT leveraged internet-scale corpora to trigger a leap in language intelligence, such data ecosystems could trigger a leap in embodied generalization, enabling VLAs to operate robustly in open-world scenarios.</p>
<p>Societal Embedding and Trustworthy Ecosystems</p>
<p>The ultimate value of VLA lies not only in technical capability but also in societal integration.As VLA enters public and domestic spaces, safety, trustworthiness, and ethical alignment will determine its adoption.Establishing standardized frameworks for risk assessment, explainability, and accountability would transform VLAs from laboratory artifacts into trusted partners.Once embedded in society, VLAs could serve as the next-generation human-AI interface, reshaping domains such as healthcare, industry, education, and services.This societal embedding marks a milestone opportunity for translating frontier research into real-world transformation.</p>
<p>CONCLUSION</p>
<p>Recent advances in Vision Language Action (VLA) models have extended the generalizable capabilities of vision language models to robotic applications, including embodied AI, autonomous driving, and diverse manipulation tasks.This survey systematically charts the emergence of VLA approaches by examining their motivations, methodologies, and applications.It provides a unified taxonomy of architectural structures and analyzes over 300 articles, along with supporting materials.We first categorize VLA architectural innovations based on autoregression-based models, diffusion-based models, reinforcement-based learning, hybrid structures, and efficiency optimization techniques.Subsequently, we explore the datasets, benchmarks, and simulation platforms that facilitate VLA training and evaluation.Building on this comprehensive review, we analyze the strengths and limitations of current approaches and highlight potential directions for future research.Collectively, these insights offer a consolidated reference and a forwardlooking roadmap for developing trustworthy, continually evolving VLAs capable of advancing general artificial intelligence in robotic systems.</p>
<p>Fig. 1 :
1
Fig. 1: Organization and Structure of the VLA Survey.</p>
<p>Fig. 2 :
2
Fig. 2: Illustration of various VLA skeleton.</p>
<p>Fig. 3 :
3
Fig. 3: Vision Language Action Taxonomy: From Autoregression-based, Diffusion-based, to Reinforcement-based and Hybrid/Specialized methods, multi-paradigm advances and practical adaptations in VLA.The taxonomy is organized along a chronological timeline.</p>
<p>Fig. 4 :
4
Fig. 4: Sample data of various datasets.early-exit mechanisms to terminate inference once confidence thresholds are met, reducing online control costs.</p>
<p>TABLE 3 :
3
Evolution of Reinforcement-based Models in VLA Research and Key Innovations.
MethodYearVLA Innovation with Reinforcement LearningVIP [172]2023</p>
<p>TABLE 4 :
4
Evolution of Hybrid Architectures and Specialized Approacher in Vision Language Action Research.
MethodYearKey Innovation</p>
<p>in Hybrid and Specialized Architecture (A) Hybrid Architectures and Multi-Paradigm Integration in VLA</p>
<p>TABLE 5 :
5
Representative Datasets and Benchmarks for Robots.</p>
<p>Unified multimodal Transformer with tokenized vision, language, states, and actions. RT-1 [29] 2022 Real-world robotic Transformer trained on 130k demos with FiLM-based multimodal fusion. Autoregressive Generalist, Vla Methodologies Gato, 2022. 2023. 2023Embodied multimodal LLM combining PaLM and ViT for VQA, navigation, and manipulation. RT-2 [33</p>
<p>Extended PaLM-E with action tokens and web-scale VLM knowledge for open-vocabulary grasping. </p>
<p>Two-stage training for 3D vision-language alignment and VLA fine-tuning. </p>
<p>. Octo, 202435</p>
<p>Open-source cross-robot policy trained on 1.5M video instances with reward-free imitation. RUM [36] 2024 Utility-based scoring enabling robust zero-shot deployment and new benchmarks. RoboMM [37] 2024 Multimodal fusion with modality masking achieving SOTA on RoboData. UP-VLA [38] 2025 Unified prompting framework merging tasks, visuals, and actions for better few-shot performance. UniAct [39] 2025 Defined universal atomic actions to solve cross-embodiment heterogeneity. 2025Humanoid-VLA [40</p>
<p>Applied VLA to humanoid control with multi-view RGB and language prompts. NORA [41] 2025 Lightweight open-source VLA using FAST+ tokenizer and 970k demonstrations. OneTwoVLA. 422025</p>
<p>Adaptive System 1 &amp; 2 reasoning for long-horizon planning and error recovery. 202543</p>
<p>Voting-based ensemble reduces action tokens for faster inference and training. UniVLA [44] 2025 Learned task-centric latent action representations from diverse videos. OE-VLA [45] 2025</p>
<p>Extended VLAs for open-ended multimodal instruction following. </p>
<p>Autoregressive Reasoning and Semantic Planning with LLMs Inner Monologue [46] 2022 Introduced inner language-driven reasoning loop, improving embodied task success. Instruct2Act [47] 2023 Proposed vision-language-task script-action pipeline with semantic intermediaries. 2023RoboFlamingo [48</p>
<p>LLaRA [49] 2024 Augmented trajectories with dialogue tasks to enhance data and transfer. ECoT [50] 2024 Formalized embodied chain-of-thought reasoning to boost OpenVLA. Mobility VLA [51] 2024 Integrated long-context VLM with navigation for multimodal instruction-following. UAV-VLA [52] 2025Adapted OpenFlamingo to robotics, enabling efficient VLM-to-VLA transfer</p>
<p>Shake-VLA [54] 2025 Introduced multimodal bimanual system with speech, vision, and haptics. Hi Robot [55] 2025 Adopted hierarchical coarse-to-fine planning for long-instruction following. DexGraspVLA. 562025Tactile-VLA [53] 2025 Combined tactile feedback with reasoning for precise, adaptive control</p>
<p>Combined VLM planning with diffusion for robust dexterous grasping. HAMSTER [57] 2025 Hierarchical VLA leveraging out-of-domain data for broad generalization. CoT-VLA [58] 2025 Implemented visual chain-of-thought reasoning with predictive goals. </p>
<p>. Gemini Robotics, 202559</p>
<p>Built Gemini 2.0-based platform for robust multi-task embodied reasoning. CognitiveDrone [60] 2025 Developed cognitive UAV VLA achieving high success on aerial benchmark. 0.5 [61] 2025 Trained on heterogeneous robots for open-world generalization. InSpire [62] 2025 Introduced spatial reasoning prompts to reduce spurious correlations</p>
<p>Autoregressive Trajectory Generation and Visual Alignment Modeling LATTE [63] 2022 Mapped natural language to motion trajectories with a Transformer decoder. VIMA. 642023</p>
<p>Unified multimodal tokens for language, vision, and actions with strong zero-shot generalization. InstructRL [65] 2023 Used joint vision-language encoders with policy Transformer for better alignment. CrossFormer [66] 2024 Scalable Transformer policy handling multi-embodiment data for generalization. GR-1 [67] 2024 Unified video and action tokens for temporal modeling via video pretraining transfer. GR-2 [68] 2024</p>
<p>Extended GR-1 with web-scale video-language supervision for zero-shot manipulation. 2024OpenVLA [21</p>
<p>Released 7B open-source VLA trained on 970k trajectories, surpassing RT-2-X. </p>
<p>RoboNurse-VLA [70] 2024 Achieved high-precision surgical grasping with VLA policies in medical settings. Moto [71] 2025 Bridged video pretraining and execution with motion language tokens. - Bi, Vla, 2024. 202569Employed dual-arm predictors for coordinated bimanual manipulation. OpenDriveVLA [72</p>
<p>Aligned 2D/3D perception into a unified semantic space for driving trajectories. </p>
<p>. Graspvla, 202573</p>
<p>Generated vision-language trajectories for dynamic drone racing control. VTLA [75] 2025 Fused vision-tactile input with preference optimization for &gt;90% unseen success. PointVLA [76] 2025 Injected point clouds into pretrained VLAs for lightweight 3D reasoning. WorldVLA [30] 2025 Mitigated autoregressive error propagation with joint vision-action modeling. TraceVLA [77] 2025 Used visual trace prompting to capture cues in long-horizon tasks. Interleave-VLA [78] 2025 Supported interleaved image-text inputs to boost zero-shot manipulation. 2025Pretrained GPT-style decoder with GraspVerse for real-world grasp transfer. MoManipVLA [79] 2025 Unified base-arm trajectory alignment for mobile manipulation transfer</p>
<p>Structural Optimization and Efficient Inference Mechanisms in Autoregressive VLA HiP [80] 2023 Introduced hierarchical planning with token prediction for long-horizon manipulation. Actra. 812024</p>
<p>Optimized Transformer with trajectory attention and learnable queries to cut overhead. DeeR-VLA. 202482</p>
<p>Multi-exit architecture enabling early exits to reduce latency. FAST [22] 2025 Generated variable-length action tokens for efficient long-horizon execution. SpatialVLA [83] 2025 Enhanced geometric perception using voxel grids and spatial attention. 202586MoLe-VLA</p>
<p>Applied parallel fixed-point decoding to accelerate inference without retraining. BitVLA [88] 2025 Used 1-bit quantization, cutting memory to 30% while preserving performance. GR-MG [89] 2025 Leveraged diffusion-generated goals for semi-supervised autoregressive learning. LoHoVLA [90] 2025 Unified hierarchical control for ultra-long-horizon closed-loop tasks. OTTER [91] 2025 Injected language into vision encoding with TAVE for stronger alignment. PD-VLA [87Mixture-of-experts router enabled dynamic layer skipping, reducing cost 40%. 2025ChatVLA [92] 2025 Unified architecture with expert routing and phased alignment for scalability</p>
<p>Extended diffusion to SE(3) poses, learning smooth costs for grasp and motion planning. UPDP [130] 2023 Framed decision-making as video generation with images as interfaces and language guidance. StructDiffusion [131] 2023 Combined diffusion and object-centric transformers for language-guided 3D structure creation. Se, 2023Diffusion Policy [7] 2024 Modeled actions as conditional diffusion, outperforming behavioral cloning</p>
<p>. Diffuser Actor, 2024132</p>
<p>Embedded 3D scenes with conditional diffusion for trajectory generation. AVDC [133] 2024 Learned visuomotor policies from video via optical flow and motion reconstruction. 2025RDT-1B [134</p>
<p>Unified diffusion across time with velocity fields and action discrimination. CDP [136] 2025 Improved coherence via historical action conditioning and caching mechanisms. DD VLA [137] 2025 Modeled discretized action chunks with discrete diffusion and cross-entropy training. 2025. 2025Large-scale diffusion model for bimanual manipulation with temporal conditioning. Recombined task behaviors by averaging hidden states across demonstrated trajectories</p>
<p>Diffusion-based Multimodal Architectural Fusion SuSIE. 2023139</p>
<p>Unified multimodal tokens enabling flexible language-image-position goal conditioning. CogACT [141] 2024 Introduced cognition modules with semantic graphs linking perception and behavior. PERIA [142] 2024 Jointly fine-tuned MLLMs and image editing models for reasoning and sub-goal planning. Chain-of-Affordance [143] 2024 Parsed tasks into sequential affordance sub-goals with explicit perception-action pairs. 0 [2] 2024 Encoded video and language as latent tokens in observe-understand-execute loops. 2024. 2024. 2025Leveraged web videos to predict point tracks and map them to robot actions. Dita [145] 2025 Scalable diffusion Transformer denoising continuous actions directly. Diffusion-VLA [1] 2025 Integrated self-generated reasoning with diffusion policies via symbolic intermediates. ForceVLA [146. Incorporated 6-axis force sensing with force-aware MoE fusion for real-time control</p>
<p>Application Optimization and Deployment in Diffusion-Based VLA NoMaD [147] 2023 Unifies goal-directed navigation and oal-agnostic exploration in diffusion policy. TinyVLA. 1482025</p>
<p>VQ-VLA [?] 2025 Employs vector-quantized tokenizers to reduce sim-to-real gaps. DexVLG [150] 2025 Trains large-scale grasp model for dexterous hand grasping on DexGraspNet for zero-shot dexterity. AC-DiT [151] 2025 Adapts diffusion Transformer with multimodal mobility conditioning. DexVLA [152] 2025 Applies morphology curriculum learning for cross-robot adaptation. MinD [153] 2025 Combines video imagination with high-frequency diffusion policies. Hume [154] 2025 Integrates dual-system value-guided reasoning and fast denoising. TriVLA [155] 2025 Operates triple-system VLA with 36Hz dynamics and reasoning layers. BYOVLA [156] 2025 Adds runtime interventions to enhance robustness without retraining. 2025Uses LoRA fine-tuning with only 5% trainable parameters, effectively reducing training costs. DreamVLA [157] 2025 Employs dynamic self-reflective loops with CoT, error tokens, and expert layers. GEVRM [158</p>
<p>Deploys scene/action-specialized Mixture-of-experts architectures for autonomous driving. DreamGen [160] 2025 Generates neural trajectories enabling humanoids to learn novel tasks. EnerVerse [161] 2025 Predicts embodied futures with multi-view autoregressive video diffusion. VidBot [162] 2025 Reconstructs 3D affordances from monocular videos with depth priors. OFT [114] 2025 Optimizes fine-tuning via parallel decoding and chunked actions. TrackVLA [163] 2025 Uses shared LLM heads and diffusion for embodied visual tracking. FP3 [164] 2025 Builds large-scale 3D foundation diffusion policy for manipulation. GR00T N1 [165] 2025 Provides humanoid foundation model with multimodal Transformer control. ObjectVLA [166] 2025 Enables open-world object manipulation without human demos. 2025Uses IMC principles with text-guided video generation for robustness. SwitchVLA [167] 2025 Models execution-aware task switching from state-context signals</p>
<p>An Affordance-aware model decomposes tasks into spatial affordances and low-level execution. LangToMo [169] 2025 Forecasts pixel motion as intermediate reasoning for actions. 2025170</p>
<p>Injects 3D geometry features via visual geometry foundation modules. </p>
<p>Diffusion-vla: Generalizable and interpretable robot foundation model via self-generated reasoning. J Wen, M Zhu, Y Zhu, Z Tang, J Li, Z Zhou, C Li, X Liu, Y Peng, C Shen, F Feng, 2025</p>
<p> 0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, S Jakubczak, T Jones, L Ke, S Levine, A Li-Bell, M Mothukuri, S Nair, K Pertsch, L X Shi, J Tanner, Q Vuong, A Walling, H Wang, U Zhilinsky, 2024</p>
<p>Clips: An enhanced clip framework for learning with synthetic captions. Y Liu, X Li, Z Wang, B Zhao, C Xie, 2024</p>
<p>Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, 2022</p>
<p>A family of open large multimodal models. L Xue, M Shu, A Awadalla, J Wang, A Yan, S Purushwalkam, H Zhou, V Prabhu, Y Dai, M S Ryoo, S Kendre, J Zhang, S Tseng, G A Lujan-Moreno, M L Olson, M Hinck, D Cobbley, V Lal, C Qin, S Zhang, C.-C Chen, N Yu, J Tan, T M Awalgaonkar, S Heinecke, H Wang, Y Choi, L Schmidt, Z Chen, S Savarese, J C Niebles, C Xiong, R Xu, 2025xgen-mm (blip-3</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, H Zhong, Y Zhu, M Yang, Z Li, J Wan, P Wang, W Ding, Z Fu, Y Xu, J Ye, X Zhang, T Xie, Z Cheng, H Zhang, Z Yang, H Xu, J Lin, Qwen2.5-vl technical report. 2025</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, 2024</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 201225</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of machine learning research. 211402020</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Advances in neural information processing systems. 201932</p>
<p>Visualbert: A simple and performant baseline for vision and language. L H Li, M Yatskar, D Yin, C.-J Hsieh, K.-W Chang, arXiv:1908.035572019arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, International conference on machine learning. PMLR202319742</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in neural information processing systems. 202235</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202336</p>
<p>Openvla: An open-source visionlanguage-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, Q Vuong, T Kollar, B Burchfiel, R Tedrake, D Sadigh, S Levine, P Liang, C Finn, 2024</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, 2025</p>
<p>Bridge data: Boosting generalization of robotic skills with cross-domain datasets. F Ebert, Y Yang, K Schmeckpeper, B Bucher, G Georgakis, K Daniilidis, C Finn, S Levine, 20212109arXiv e-prints</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, arXiv:1712.054742017arXiv preprint</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012. 2012</p>
<p>Isaac gym: High performance gpu-based physics simulation for robot learning. V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, G State, 2021</p>
<p>Carla: An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, 2017</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, 2023</p>
<p>J Cen, C Yu, H Yuan, Y Jiang, S Huang, J Guo, X Li, Y Song, H Luo, F Wang, arXiv:2506.21539Worldvla: Towards autoregressive action world model. 2025arXiv preprint</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, T Eccles, J Bruce, A Razavi, A Edwards, N Heess, Y Chen, R Hadsell, O Vinyals, M Bordbar, N De Freitas, 2022</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S M Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, Y Chebotar, P Sermanet, D Duckworth, S Levine, V Vanhoucke, K Hausman, M Toussaint, K Greff, A Zeng, I Mordatch, P Florence, 2023</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, P Florence, C Fu, M G Arenas, K Gopalakrishnan, K Han, K Hausman, A Herzog, J Hsu, B Ichter, A Irpan, N Joshi, R Julian, D Kalashnikov, Y Kuang, I Leal, L Lee, T.-W E Lee, S Levine, Y Lu, H Michalewski, I Mordatch, K Pertsch, K Rao, K Reymann, M Ryoo, G Salazar, P Sanketi, P Sermanet, J Singh, A Singh, R Soricut, H Tran, V Vanhoucke, Q Vuong, A Wahid, S Welker, P Wohlhart, J Wu, F Xia, T Xiao, P Xu, S Xu, T Yu, B Zitkovich, 2023</p>
<p>An embodied generalist agent in 3d world. J Huang, S Yong, X Ma, X Linghu, P Li, Y Wang, Q Li, S.-C Zhu, B Jia, S Huang, 2024</p>
<p>Octo: An open-source generalist robot policy. O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, J Luo, Y L Tan, L Y Chen, P Sanketi, Q Vuong, T Xiao, D Sadigh, C Finn, S Levine, 2024</p>
<p>Robot utility models: General policies for zero-shot deployment in new environments. H Etukuru, N Naka, Z Hu, S Lee, J Mehu, A Edsinger, C Paxton, S Chintala, L Pinto, N M M Shafiullah, 2024</p>
<p>Robomm: All-in-one multimodal large model for robotic manipulation. F Yan, F Liu, L Zheng, Y Zhong, Y Huang, Z Guan, C Feng, L Ma, 2024</p>
<p>Up-vla: A unified understanding and prediction model for embodied agent. J Zhang, Y Guo, Y Hu, X Chen, X Zhu, J Chen, 2025</p>
<p>Universal actions for enhanced embodied foundation models. J Zheng, J Li, D Liu, Y Zheng, Z Wang, Z Ou, Y Liu, J Liu, Y.-Q Zhang, X Zhan, 2025</p>
<p>Humanoid-vla: Towards universal humanoid control with visual integration. P Ding, J Ma, X Tong, B Zou, X Luo, Y Fan, T Wang, H Lu, P Mo, J Liu, Y Wang, H Zhou, W Feng, J Liu, S Huang, D Wang, 2025</p>
<p>Nora: A small open-sourced generalist vision language action model for embodied tasks. C.-Y Hung, Q Sun, P Hong, A Zadeh, C Li, U.-X Tan, N Majumder, S Poria, 2025</p>
<p>Onetwovla: A unified vision-language-action model with adaptive reasoning. F Lin, R Nai, Y Hu, J You, J Zhao, Y Gao, 2025</p>
<p>Vote: Vision-language-action optimization with trajectory ensemble voting. J Lin, A Taherin, A Akbari, A Akbari, L Lu, G Chen, T Padir, X Yang, W Chen, Y Li, X Lin, D Kaeli, P Zhao, Y Wang, 2025</p>
<p>Univla: Learning to act anywhere with task-centric latent actions. Q Bu, Y Yang, J Cai, S Gao, G Ren, M Yao, P Luo, H Li, 2025</p>
<p>Unveiling the potential of vision-language-action models with open-ended multimodal instructions. W Zhao, G Li, Z Gong, P Ding, H Zhao, D Wang, 2025</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, N Brown, T Jackson, L Luu, S Levine, K Hausman, B Ichter, 2022</p>
<p>Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. S Huang, Z Jiang, H Dong, Y Qiao, P Gao, H Li, 2023</p>
<p>Vision-language foundation models as effective robot imitators. X Li, M Liu, H Zhang, C Yu, J Xu, H Wu, C Cheang, Y Jing, W Zhang, H Liu, H Li, T Kong, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Llara: Supercharging robot learning data for vision-language policy. X Li, C Mata, J Park, K Kahatapitiya, Y S Jang, J Shang, K Ranasinghe, R Burgert, M Cai, Y J Lee, M S Ryoo, International Conference on Learning Representations. 2025</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, 2025</p>
<p>Mobility VLA: Multimodal instruction navigation with long-context VLMs and topological graphs. Z Xu, H.-T L Chiang, Z Fu, M G Jacob, T Zhang, T.-W E Lee, W Yu, C Schenck, D Rendleman, D Shah, F Xia, J Hsu, J Hoech, P Florence, S Kirmani, S Singh, V Sindhwani, C Parada, C Finn, P Xu, S Levine, J Tan, 8th Annual Conference on Robot Learning. 2024</p>
<p>Uav-vla: Vision-language-action system for large scale aerial mission generation. O Sautenkov, Y Yaqoot, A Lykov, M A Mustafa, G Tadevosyan, A Akhmetkazy, M Cabrera, M Martynov, S Karaf, D Tsetserukou, Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI '25. the 2025 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI '25IEEE Press2025</p>
<p>Tactile-vla: Unlocking vision-language-action model's physical knowledge for tactile generalization. J Huang, S Wang, F Lin, Y Hu, C Wen, Y Gao, 2025</p>
<p>Shake-vla: Visionlanguage-action model-based system for bimanual robotic manipulations and liquid mixing. M H Khan, S Asfaw, D Iarchuk, M A Cabrera, L Moreno, I Tokmurziyev, D Tsetserukou, 2025</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, A Li-Bell, D Driess, L Groom, S Levine, C Finn, 2025</p>
<p>Dexgraspvla: A vision-language-action framework towards general dexterous grasping. Y Zhong, X Huang, R Li, C Zhang, Y Liang, Y Yang, Y Chen, 2025</p>
<p>Hamster: Hierarchical action models for open-world robot manipulation. Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, A Gupta, A Goyal, 2025</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, A Handa, M.-Y Liu, D Xiang, G Wetzstein, T.-Y Lin, 2025</p>
<p>Gemini robotics: Bringing ai into the physical world. G R Team, S Abeyruwan, J Ainslie, J.-B Alayrac, M G Arenas, T Armstrong, 2025</p>
<p>A Lykov, V Serpiva, M H Khan, O Sautenkov, A Myshlyaev, G Tadevosyan, Y Yaqoot, D Tsetserukou, arXiv:2503.01378Cognitivedrone: A vla model and evaluation benchmark for real-time cognitive task solving and reasoning in uavs. 2025arXiv preprint</p>
<p>a vision-language-action model with open-world generalization. P Intelligence, K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, M Y Galliker, D Ghosh, L Groom, K Hausman, B Ichter, S Jakubczak, T Jones, L Ke, D Leblanc, S Levine, A Li-Bell, M Mothukuri, S Nair, K Pertsch, A Z Ren, L X Shi, L Smith, J T Springenberg, K Stachowicz, J Tanner, Q Vuong, H Walke, A Walling, H Wang, L Yu, U Zhilinsky, 20255</p>
<p>Inspire: Vision-language-action models with intrinsic spatial reasoning. J Zhang, S Wu, X Luo, H Wu, L Gao, H T Shen, J Song, arXiv:2505.138882025arXiv preprint</p>
<p>Latte: Language trajectory transformer. A Bucker, L Figueredo, S Haddadin, A Kapoor, S Ma, S Vemprala, R Bonatti, 2022</p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, 2023</p>
<p>Instruction-following agents with multimodal transformer. H Liu, L Lee, K Lee, P Abbeel, 2023</p>
<p>Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. R Doshi, H Walke, O Mees, S Dasari, S Levine, 2024</p>
<p>Unleashing large-scale video generative pre-training for visual robot manipulation. H Wu, Y Jing, C Cheang, G Chen, J Xu, X Li, M Liu, H Li, T Kong, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Gr-2: A generative videolanguage-action model with web-scale knowledge for robot manipulation. C.-L Cheang, G Chen, Y Jing, T Kong, H Li, Y Li, Y Liu, H Wu, J Xu, Y Yang, H Zhang, M Zhu, arXiv:2410.061582024arXiv preprint</p>
<p>Bi-vla: Vision-language-action model-based system for bimanual robotic dexterous manipulations. K F Gbagbe, M A Cabrera, A Alabbas, O Alyunes, A Lykov, D Tsetserukou, 2024</p>
<p>Robonurse-vla: Robotic scrub nurse system based on vision-language-action model. S Li, J Wang, R Dai, W Ma, W Y Ng, Y Hu, Z Li, 2024</p>
<p>Moto: Latent motion token as the bridging language for learning robot manipulation from videos. Y Chen, Y Ge, W Tang, Y Li, Y Ge, M Ding, Y Shan, X Liu, 2025</p>
<p>Opendrivevla: Towards end-to-end autonomous driving with large vision language action model. X Zhou, X Han, F Yang, Y Ma, A C Knoll, 2025</p>
<p>Graspvla: a grasping foundation model pre-trained on billion-scale synthetic action data. S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang, T Yang, X Zhang, H Cui, Z Zhang, H Wang, 2025</p>
<p>Racevla: Vla-based racing drone navigation with human-like behaviour. V Serpiva, A Lykov, A Myshlyaev, M H Khan, A A Abdulkarim, O Sautenkov, D Tsetserukou, 2025</p>
<p>Vtla: Vision-tactile-language-action model with preference learning for insertion manipulation. C Zhang, P Hao, X Cao, X Hao, S Cui, S Wang, 2025</p>
<p>Pointvla: Injecting the 3d world into vision-language-action models. C Li, J Wen, Y Peng, Y Peng, F Feng, Y Zhu, 2025</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. R Zheng, Y Liang, S Huang, J Gao, H D Iii, A Kolobov, F Huang, J Yang, 2025</p>
<p>Interleave-vla: Enhancing robot manipulation with interleaved image-text instructions. C Fan, X Jia, Y Sun, Y Wang, J Wei, Z Gong, X Zhao, M Tomizuka, X Yang, J Yan, M Ding, 2025</p>
<p>Momanipvla: Transferring vision-language-action models for general mobile manipulation. Z Wu, Y Zhou, X Xu, Z Wang, H Yan, Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR). the Computer Vision and Pattern Recognition Conference (CVPR)June 2025</p>
<p>Compositional foundation models for hierarchical planning. A Ajay, S Han, Y Du, S Li, A Gupta, T Jaakkola, J Tenenbaum, L Kaelbling, A Srivastava, P , 2023</p>
<p>Actra: Optimized transformer architecture for vision-languageaction models in robot learning. Y Ma, D Chi, S Wu, Y Liu, Y Zhuang, J Hao, I King, 2024</p>
<p>Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution. Y Yue, Y Wang, B Kang, Y Han, S Wang, S Song, J Feng, G Huang, Advances in Neural Information Processing Systems. A Globerson, L Mackey, D Belgrave, A Fan, U Paquet, J Tomczak, C Zhang, Curran Associates, Inc202437643</p>
<p>Spatialvla: Exploring spatial representations for visual-language-action model. D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, X Li, 2025</p>
<p>Vla-cache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation. S Xu, Y Wang, C Xia, D Zhu, T Huang, C Xu, 2025</p>
<p>Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding. J Jones, O Mees, C Sferrazza, K Stachowicz, P Abbeel, S Levine, 2025</p>
<p>Mole-vla: Dynamic layerskipping vision language action model via mixture-of-layers for efficient robot manipulation. R Zhang, M Dong, Y Zhang, L Heng, X Chi, G Dai, L Du, Y Du, S Zhang, 2025</p>
<p>Accelerating vision-language-action model integrated with action chunking via parallel decoding. W Song, J Chen, P Ding, H Zhao, W Zhao, Z Zhong, Z Ge, J Ma, H Li, 2025</p>
<p>Bitvla: 1-bit vision-language-action models for robotics manipulation. H Wang, C Xiong, R Wang, X Chen, 2025</p>
<p>Grmg: Leveraging partially-annotated data via multi-modal goalconditioned policy. P Li, H Wu, Y Huang, C Cheang, L Wang, T Kong, IEEE Robotics and Automation Letters. 2025</p>
<p>Lohovla: A unified vision-language-action model for long-horizon embodied tasks. Y Yang, J Sun, S Kou, Y Wang, Z Deng, 2025</p>
<p>Otter: A vision-language-action model with text-aware visual feature extraction. H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik, K Goldberg, P Abbeel, 2025</p>
<p>Chatvla: Unified multimodal understanding and robot control with vision-language-action model. Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng, R Cheng, Y Peng, C Shen, F Feng, 2025</p>
<p>Prompt a robot to walk with large language models. Y.-J Wang, B Zhang, J Chen, K Sreenath, 2024</p>
<p>Interactive language: Talking to robots in real time. C Lynch, A Wahid, J Tompson, T Ding, J Betker, R Baruch, T Armstrong, P Florence, 2022</p>
<p>Openended instructable embodied agents with memory-augmented large language models. G Sarch, Y Wu, M J Tarr, K Fragkiadaki, 2023</p>
<p>Missiongpt: Mission planner for mobile robot based on robotics transformer model. V Berman, A Bazhenov, S Satsevich, D Tsetserukou, 2024 2nd International Conference on Foundation and Large Language Models (FLLM). 2024</p>
<p>Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. H Bharadhwaj, J Vakil, M Sharma, A Gupta, S Tulsiani, V Kumar, 2023</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Y Hu, F Lin, T Zhang, L Yi, Y Gao, 2023</p>
<p>Language reasoning in vision-language-action model for robotic grasping. L Fan, K Chen, Z Xu, M Yuan, P Huang, W Huang, 2024 China Automation Congress (CAC). 2024</p>
<p>Rt-affordance: Affordances are versatile intermediate representations for robot manipulation. S Nasiriany, S Kirmani, T Ding, L Smith, Y Zhu, D Driess, D Sadigh, T Xiao, 2024</p>
<p>From foresight to forethought: Vlm-in-the-loop policy steering via latent alignment. Y Wu, R Tian, G Swamy, A Bajcsy, 2025</p>
<p>Vlas: Vision-language-action model with speech instructions for customized robot manipulation. W Zhao, P Ding, M Zhang, Z Gong, S Bai, H Zhao, D Wang, 2025</p>
<p>Agentic robot: A brain-inspired framework for vision-language-action models in embodied agents. Z Yang, Y Chen, X Zhou, J Yan, D Song, Y Liu, Y Li, Y Zhang, P Zhou, H Chen, L Sun, 2025</p>
<p>Vla model-expert collaboration for bidirectional manipulation learning. T.-Y Xiang, A.-Q Jin, X.-H Zhou, M.-J Gui, X.-L Xie, S.-Q Liu, S.-Y Wang, S.-B Duang, S.-C Wang, Z Lei, Z.-G Hou, 2025</p>
<p>Occllama: An occupancy-language-action generative world model for autonomous driving. J Wei, S Yuan, P Li, Q Hu, Z Gan, W Ding, 2024</p>
<p>Combatvla: An efficient vision-language-action model for combat tasks in 3d action role-playing games. P Chen, P Bu, Y Wang, X Wang, Z Wang, J Guo, Y Zhao, Q Zhu, J Song, S Yang, J Wang, B Zheng, 2025</p>
<p>Jarvis-vla: Post-training large-scale vision language models to play visual games with keyboards and mouse. M Li, Z Wang, K He, X Ma, Y Liang, 2025</p>
<p>Open-world object manipulation using pre-trained vision-language models. A Stone, T Xiao, Y Lu, K Gopalakrishnan, K.-H Lee, Q Vuong, P Wohlhart, S Kirmani, B Zitkovich, F Xia, C Finn, K Hausman, 2023</p>
<p>Language models as zero-shot trajectory generators. T Kwon, N D Palo, E Johns, 10.1109/LRA.2024.3410155IEEE Robotics and Automation Letters. 97Jul. 2024</p>
<p>Cronusvla: Transferring latent motion across time for multi-frame prediction in manipulation. H Li, S Yang, Y Chen, Y Tian, X Yang, X Chen, H Wang, T Wang, F Zhao, D Lin, J Pang, 2025</p>
<p>Uni-navid: A video-based visionlanguage-action model for unifying embodied navigation tasks. J Zhang, K Wang, S Wang, M Li, H Liu, S Wei, Z Wang, Z Zhang, H Wang, 2025</p>
<p>Quar-vla: Vision-language-action model for quadruped robots. P Ding, H Zhao, W Zhang, W Song, M Zhang, S Huang, N Yang, D Wang, 10.1007/978-3-031-72652-1_21Computer Vision -ECCV 2024: 18th European Conference. Milan, Italy; Berlin, HeidelbergSpringer-VerlagSeptember 29-October 4, 2024. 2024Proceedings, Part V</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. T Z Zhao, V Kumar, S Levine, C Finn, 2023</p>
<p>Fine-tuning vision-languageaction models: Optimizing speed and success. M J Kim, C Finn, P Liang, 2025</p>
<p>Latent action pretraining from videos. S Ye, J Jang, B Jeon, S J Joo, J Yang, B Peng, A Mandlekar, R Tan, Y.-W Chao, B Y Lin, L Liden, K Lee, J Gao, L Zettlemoyer, D Fox, M Seo, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Vavim and vavam: Autonomous driving through video generative modeling. F Bartoccioni, E Ramzi, V Besnier, S Venkataramanan, T.-H Vu, Y Xu, L Chambon, S Gidaris, S Odabas, D Hurych, R Marlet, A Boulch, M Chen, loi Zablocki, A Bursuc, E Valle, M Cord, 2025</p>
<p>Simlingo: Visiononly closed-loop autonomous driving with language-action alignment. K Renz, L Chen, E Arani, O Sinavski, 2025</p>
<p>Orion: A holistic end-to-end autonomous driving framework by visionlanguage instructed action generation. H Fu, D Zhang, Z Zhao, J Cui, D Liang, C Zhang, D Zhang, H Xie, B Wang, X Bai, 2025</p>
<p>Rvt-2: Learning precise manipulation from few demonstrations. A Goyal, V Blukis, J Xu, Y Guo, Y.-W Chao, D Fox, 2024</p>
<p>Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. J Lu, C Clark, S Lee, Z Zhang, S Khosla, R Marten, D Hoiem, A Kembhavi, 2023</p>
<p>In-context imitation learning via next-token prediction. L Fu, H Huang, G Datta, L Y Chen, W C .-H. Panitch, F Liu, H Li, K Goldberg, 2024</p>
<p>Pre-training auto-regressive robotic models with 4d representations. D Niu, Y Sharma, H Xue, G Biamby, J Zhang, Z Ji, T Darrell, R Herzig, 2025</p>
<p>Tla: Tactile-language-action model for contact-rich manipulation. P Hao, C Zhang, D Li, X Cao, X Hao, S Cui, S Wang, 2025</p>
<p>Og-vla: 3d-aware vision language action model via orthographic image generation. I Singh, A Goyal, S Birchfield, D Fox, A Garg, V Blukis, 2025</p>
<p>4d-vla: Spatiotemporal vision-language-action pretraining with cross-scene calibration. J Zhang, Y Chen, Y Xu, Z Huang, Y Zhou, Y Yuan, X Cai, G Huang, X Quan, H Xu, L Zhang, arXiv:2506.222422025arXiv preprint</p>
<p>Baku: An efficient transformer for multi-task policy learning. S Haldar, Z Peng, L Pinto, 2024</p>
<p>HiRT: Enhancing robotic control with hierarchical robot transformers. J Zhang, Y Guo, X Chen, Y.-J Wang, Y Hu, C Shi, J Chen, 8th Annual Conference on Robot Learning. 2024</p>
<p>Fast ecot: Efficient embodied chain-of-thought via thoughts reuse. Z Duan, Y Zhang, S Geng, G Liu, J Boedecker, C X Lu, 2025</p>
<p>Se(3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion. J Urain, N Funk, J Peters, G Chalvatzaki, 2023</p>
<p>Learning universal policies via text-guided video generation. Y Du, M Yang, B Dai, H Dai, O Nachum, J B Tenenbaum, D Schuurmans, P Abbeel, 2023</p>
<p>Structdiffusion: Language-guided creation of physically-valid structures using unseen objects. W Liu, Y Du, T Hermans, S Chernova, C Paxton, 2023</p>
<p>3d diffuser actor: Policy diffusion with 3d scene representations. T.-W Ke, N Gkanatsios, K Fragkiadaki, 2024</p>
<p>Learning to act from actionless videos through dense correspondences. P.-C Ko, J Mao, Y Du, S.-H Sun, J B Tenenbaum, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. S Liu, L Wu, B Li, H Tan, H Chen, Z Wang, K Xu, H Su, J Zhu, International Conference on Representation Learning. A Yue, N Garg, F Peng, R Sha, Yu, 20252025</p>
<p>Timeunified diffusion policy with action discrimination for robotic manipulation. Y Niu, S Zhou, Y Li, Y Den, L Wang, 2025</p>
<p>Cdp: Towards robust autoregressive visuomotor policy learning via causal diffusion. J Ma, Y Qin, Y Li, X Liao, Y Guo, R Zhang, 2025</p>
<p>Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies. Z Liang, Y Li, T Yang, C Wu, S Mao, L Pei, X Yang, J Pang, Y Mu, P Luo, 2025</p>
<p>Task reconstruction and extrapolation for  0 using text latent. Q Li, 2025</p>
<p>Zero-shot robotic manipulation with pretrained image-editing diffusion models. K Black, M Nakamoto, P Atreya, H Walke, C Finn, A Kumar, S Levine, 2023</p>
<p>Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. M Reuss, mer Erdinc ya Gmurlu, F Wenzel, R Lioutikov, 2024</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, X Wang, B Liu, J Fu, J Bao, D Chen, Y Shi, J Yang, B Guo, 2024</p>
<p>Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation. F Ni, J Hao, S Wu, L Kou, Y Yuan, Z Dong, J Liu, M Li, Y Zhuang, Y Zheng, Advances in Neural Information Processing Systems. A Globerson, L Mackey, D Belgrave, A Fan, U Paquet, J Tomczak, C Zhang, Curran Associates, Inc202437</p>
<p>Improving vision-languageaction models via chain-of-affordance. J Li, Y Zhu, Z Tang, J Wen, M Zhu, X Liu, C Li, R Cheng, Y Peng, F Feng, 2024</p>
<p>Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation. H Bharadhwaj, R Mottaghi, A Gupta, S Tulsiani, 2024</p>
<p>Dita: Scaling diffusion transformer for generalist vision-language-action policy. Z Hou, T Zhang, Y Xiong, H Duan, H Pu, R Tong, C Zhao, X Zhu, Y Qiao, J Dai, arXiv:2503.197572025arXiv preprint</p>
<p>Forcevla: Enhancing vla models with a force-aware moe for contact-rich manipulation. J Yu, H Liu, Q Yu, J Ren, C Hao, H Ding, G Huang, G Huang, Y Song, P Cai, arXiv:2505.221592025arXiv preprint</p>
<p>Nomad: Goal masked diffusion policies for navigation and exploration. A Sridhar, D Shah, C Glossop, S Levine, 2023</p>
<p>Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. J Wen, Y Zhu, J Li, M Zhu, K Wu, Z Xu, N Liu, R Cheng, C Shen, Y Peng, F Feng, J Tang, 2025</p>
<p>Smolvla: A visionlanguage-action model for affordable and efficient robotics. M Shukor, D Aubakirova, F Capuano, P Kooijmans, S Palma, A Zouitine, M Aractingi, C Pascal, M Russi, A Marafioti, S Alibert, M Cord, T Wolf, R Cadene, 2025</p>
<p>Dexvlg: Dexterous visionlanguage-grasp model at scale. J He, D Li, X Yu, Z Qi, W Zhang, J Chen, Z Zhang, Z Zhang, L Yi, H Wang, 2025</p>
<p>Ac-dit: Adaptive coordination diffusion transformer for mobile manipulation. S Chen, J Liu, S Qian, H Jiang, L Li, R Zhang, Z Liu, C Gu, C Hou, P Wang, arXiv:2507.019612025arXiv preprint</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. J Wen, Y Zhu, J Li, Z Tang, C Shen, F Feng, 2025</p>
<p>Mind: Unified visual imagination and control via hierarchical world models. X Chi, K Ge, J Liu, S Zhou, P Jia, Z He, Y Liu, T Li, L Han, S Han, S Zhang, Y Guo, 2025</p>
<p>Hume: Introducing system-2 thinking in visual-language-action model. H Song, D Qu, Y Yao, Q Chen, Q Lv, Y Tang, M Shi, G Ren, M Yao, B Zhao, D Wang, X Li, 2025</p>
<p>Trivla: A triple-system-based unified vision-language-action model for general robot control. Z Liu, Y Gu, S Zheng, X Xue, Y Fu, 2025</p>
<p>Run-time observation interventions make vision-language-action models more visually robust. A J Hancock, A Z Ren, A Majumdar, 1st Workshop on Safely Leveraging Vision-Language Foundation Models in Robotics: Challenges and Opportunities. 2025</p>
<p>Dreamvla: A vision-language-action model dreamed with comprehensive world knowledge. W Zhang, H Liu, Z Qi, Y Wang, X Yu, J Zhang, R Dong, J He, H Wang, Z Zhang, L Yi, W Zeng, X Jin, 10.48550/arXiv.2507.04447abs/2507.04447CoRR. 2025</p>
<p>Gevrm: Goal-expressive video generation model for robust visual manipulation. H Zhang, P Ding, S Lyu, Y Peng, D Wang, 2025</p>
<p>Drivemoe: Mixture-of-experts for vision-language-action model in end-to-end autonomous driving. Z Yang, Y Chai, X Jia, Q Li, Y Shao, X Zhu, H Su, J Yan, 2025</p>
<p>Dreamgen: Unlocking generalization in robot learning through video world models. J Jang, S Ye, Z Lin, J Xiang, J Bjorck, Y Fang, F Hu, S Huang, K Kundalia, Y.-C Lin, L Magne, A Mandlekar, A Narayan, Y L Tan, G Wang, J Wang, Q Wang, Y Xu, X Zeng, K Zheng, R Zheng, M.-Y Liu, L Zettlemoyer, D Fox, J Kautz, S Reed, Y Zhu, L Fan, 2025</p>
<p>Enerverse: Envisioning embodied future space for robotics manipulation. S Huang, L Chen, P Zhou, S Chen, Z Jiang, Y Hu, Y Liao, P Gao, H Li, M Yao, arXiv:2501.018952025arXiv preprint</p>
<p>Vidbot: Learning generalizable 3d actions from in-the-wild 2d human videos for zero-shot robotic manipulation. H Chen, B Sun, A Zhang, M Pollefeys, S Leutenegger, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference202527672</p>
<p>Trackvla: Embodied visual tracking in the wild. S Wang, J Zhang, M Li, J Liu, A Li, K Wu, F Zhong, J Yu, Z Zhang, H Wang, 2025</p>
<p>Fp3: A 3d foundation policy for robotic manipulation. R Yang, G Chen, C Wen, Y Gao, 2025</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. : Nvidia, J Bjorck, F Casta eda, N Cherniadev, X Da, R Ding, L J Fan, Y Fang, D Fox, F Hu, S Huang, J Jang, Z Jiang, J Kautz, K Kundalia, L Lao, Z Li, Z Lin, K Lin, G Liu, E Llontop, L Magne, A Mandlekar, A Narayan, S Nasiriany, S Reed, Y L Tan, G Wang, Z Wang, J Wang, Q Wang, J Xiang, Y Xie, Y Xu, Z Xu, S Ye, Z Yu, A Zhang, H Zhang, Y Zhao, R Zheng, Y Zhu, 2025</p>
<p>Objectvla: End-to-end open-world object manipulation without demonstration. M Zhu, Y Zhu, J Li, Z Zhou, J Wen, X Liu, C Shen, Y Peng, F Feng, 2025</p>
<p>Switchvla: Execution-aware task switching for vision-language-action models. M Li, Z Zhao, Z Che, F Liao, K Wu, Z Xu, P Ren, Z Jin, N Liu, J Tang, 2025</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. R Xu, J Zhang, M Guo, Y Wen, H Yang, M Lin, J Huang, Z Li, K Zhang, L Wang, arXiv:2504.126362025arXiv preprint</p>
<p>Pixel motion as universal representation for robot control. K Ranasinghe, X Li, C Mata, J Park, M S Ryoo, 2025</p>
<p>Evo-0: Visionlanguage-action model with implicit spatial understanding. T Lin, G Li, Y Zhong, Y Zou, B Zhao, arXiv:2507.004162025arXiv preprint</p>
<p>Diffusion transformer policy. Z Hou, T Zhang, Y Xiong, H Pu, C Zhao, R Tong, Y Qiao, J Dai, Y Chen, 2025</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Y J Ma, S Sodhani, D Jayaraman, O Bastani, V Kumar, A Zhang, 2023</p>
<p>Liv: Language-image representations and rewards for robotic control. Y J Ma, W Liang, V Som, V Kumar, A Zhang, O Bastani, D Jayaraman, 2023</p>
<p>Visionlanguage models provide promptable representations for reinforcement learning. W Chen, O Mees, A Kumar, S Levine, 2024</p>
<p>Adaptive language-guided abstraction from contrastive explanations. A Peng, B Z Li, I Sucholutsky, N Kumar, J A Shah, J Andreas, A Bobu, 2024</p>
<p>Grape: Generalizing robot policy via preference alignment. Z Zhang, K Zheng, Z Chen, J Jang, Y Li, C Wang, M Ding, D Fox, H Yao, 2024</p>
<p>Rldg: Robotic generalist policy distillation via reinforcement learning. C Xu, Q Li, J Luo, S Levine, 2024</p>
<p>Elemental: Interactive learning from demonstrations and vision-language models for reward design in robotics. L Chen, N Moorman, M Gombolay, 2025</p>
<p>Navila: Legged robot vision-language-action model for navigation. A.-C Cheng, Y Ji, Z Yang, Z Gongye, X Zou, J Kautz, E Byk, H Yin, S Liu, X Wang, 2025</p>
<p>Safevla: Towards safety alignment of vision-language-action model via constrained learning. B Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, 2025</p>
<p>Improving vision-language-action model with online reinforcement learning. Y Guo, J Zhang, X Chen, X Ji, Y.-J Wang, Y Hu, J Chen, 2025</p>
<p>Reinbot: Amplifying robot visual-language manipulation with reinforcement learning. H Zhang, Z Zhuang, H Zhao, P Ding, H Lu, D Wang, 2025</p>
<p>Conrft: A reinforced fine-tuning method for vla models via consistency policy. Y Chen, S Tian, S Liu, Y Zhou, H Li, D Zhao, arXiv:2502.054502025arXiv preprint</p>
<p>More: Unlocking scalability in reinforcement learning for quadruped vision-language-action models. H Zhao, W Song, D Wang, X Tong, P Ding, X Cheng, Z Ge, 2025</p>
<p>Simplevla-rl: Online rl with simple reward enables training vla models with only one trajectory. Simplevla-Rl Team, 2025gitHub repository</p>
<p>Leverb: Humanoid whole-body control with latent vision-language instruction. H Xue, X Huang, D Niu, Q Liao, T Kragerud, J T Gravdahl, X B Peng, G Shi, T Darrell, K Screenath, S Sastry, 2025</p>
<p>Autovla: A vision-language-action model for end-toend autonomous driving with adaptive reasoning and reinforcement fine-tuning. Z Zhou, T Cai, Y Zhao, Seth Z Zhang, Z Huang, B Zhou, J Ma, arXiv:2506.137572025arXiv preprint</p>
<p>Refined policy distillation: From vla generalists to rl experts. T lg, W Burgard, F Walter, arXiv:2503.058332025arXiv preprint</p>
<p>Rlrc: Reinforcement learning-based recovery for compressed vision-language-action models. Y Chen, X Li, 2025</p>
<p>Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. G Lu, W Guo, C Zhang, Y Zhou, H Jiang, Z Gao, Y Tang, Z Wang, 2025</p>
<p>Autodrive-r 2 : Incentivizing reasoning and self-reflection capacity for vla model in autonomous driving. Z Yuan, J Tang, J Luo, R Chen, C Qian, L Sun, X Chu, Y Cai, D Zhang, S Li, 2025</p>
<p>Rewind: Language-guided rewards teach robot policies without new demonstrations. J Zhang, Y Luo, A Anwar, S A Sontakke, J J Lim, J Thomason, E Biyik, J Zhang, 2025</p>
<p>Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. Y Yuan, H Cui, Y Huang, Y Chen, F Ni, Z Dong, P Li, Y Zheng, J Hao, 2025</p>
<p>Irl-vla: Training an vision-language-action policy via reward world model. A Jiang, Y Gao, Y Wang, Z Sun, S Wang, Y Heng, H Sun, S Tang, L Zhu, J Chai, J Wang, Z Gu, H Jiang, L Sun, 2025</p>
<p>Thinkact: Vision-language-action reasoning via reinforced visual latent planning. C.-P Huang, Y.-H Wu, M.-H Chen, Y.-C F Wang, F.-E Yang, 2025</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, 2017</p>
<p>Affordances from human videos as a versatile representation for robotics. S Bahl, R Mendonca, L Chen, U Jain, D Pathak, 2023</p>
<p>Yell at your robot: Improving on-the-fly from language corrections. L X Shi, Z Hu, T Z Zhao, A Sharma, K Pertsch, J Luo, S Levine, C Finn, 2024</p>
<p>Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. J Liu, H Chen, P An, Z Liu, R Zhang, C Gu, X Li, Z Guo, S Chen, M Liu, C Hou, M Zhao, K Zhou, P.-A Heng, S Zhang, 2025</p>
<p>Rationalvla: A rational vision-languageaction model with dual system. W Song, J Chen, W Li, X He, H Zhao, C Cui, P D S Su, F Tang, X Cheng, D Wang, Z Ge, X Zheng, Z Liu, H Wang, H Li, 2025</p>
<p>Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. C Cui, P Ding, W Song, S Bai, X Tong, Z Ge, R Suo, W Zhou, Y Liu, B Jia, arXiv:2505.039122025arXiv preprint</p>
<p>Egovla: Learning vision-language-action models from egocentric human videos. R Yang, Q Yu, Y Wu, R Yan, B Li, A.-C Cheng, X Zou, Y Fang, X Cheng, R.-Z Qiu, H Yin, S Liu, S Han, Y Lu, X Wang, 2025</p>
<p>Actllm: Action consistency tuned large language model. J Bi, L B Wen, Z Liu, C Xu, 2025</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, 2021</p>
<p>3d-vla: A 3d vision-languageaction generative world model. H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, 2024</p>
<p>Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. W Huang, C Wang, Y Li, R Zhang, L Fei-Fei, 2024</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, 2024</p>
<p>Grounding multimodal large language models in actions. A Szot, B Mazoure, H Agrawal, D Hjelm, Z Kira, A Toshev, 2024</p>
<p>Bridgevla: Input-output alignment for efficient 3d manipulation learning with vision-language models. P Li, Y Chen, H Wu, X Ma, X Wu, Y Huang, L Wang, T Kong, T Tan, 2025</p>
<p>Geomanip: Geometric constraints as general interfaces for robot manipulation. W Tang, J.-H Pan, Y.-H Liu, M Tomizuka, L E Li, C.-W Fu, M Ding, 2025</p>
<p>Ttf-vla: Temporal token fusion via pixel-attention integration for vision-language-action models. C Liu, J Zhang, C Li, Z Zhou, S Wu, S Huang, H Duan, 2025</p>
<p>Memoryvla: Perceptualcognitive memory in vision-language-action models for robotic manipulation. H Shi, B Xie, Y Liu, L Sun, F Liu, T Wang, E Zhou, H Fan, X Zhang, G Huang, 2025</p>
<p>Genaug: Retargeting behaviors to unseen situations via generative augmentation. Z Chen, S Kiami, A Gupta, V Kumar, 2023</p>
<p>Scaling robot learning with semantically imagined experience. T Yu, T Xiao, A Stone, J Tompson, A Brohan, S Wang, J Singh, C Tan, D M , J Peralta, B Ichter, K Hausman, F Xia, 2023</p>
<p>Covla: Comprehensive vision-languageaction dataset for autonomous driving. H Arai, K Miwa, K Sasaki, Y Yamaguchi, K Watanabe, S Aoki, I Yamamoto, 2024</p>
<p>Helix: A vision-language-action model for generalist humanoid control. A Figure, Figure AI News. 2024</p>
<p>Autort: Embodied foundation models for large scale orchestration of robotic agents. M Ahn, D Dwibedi, C Finn, M G Arenas, K Gopalakrishnan, K Hausman, B Ichter, A Irpan, N Joshi, R Julian, S Kirmani, I Leal, E Lee, S Levine, Y Lu, I Leal, S Maddineni, K Rao, D Sadigh, P Sanketi, P Sermanet, Q Vuong, S Welker, F Xia, T Xiao, P Xu, S Xu, Z Xu, 2024</p>
<p>Momanipvla: Transferring vision-language-action models for general mobile manipulation. Z Wu, Y Zhou, X Xu, Z Wang, H Yan, 2025</p>
<p>Cuberobot: Grounding language in rubik's cube manipulation via vision-language model. F Wang, X Yu, W Wu, Companion Proceedings of the ACM on Web Conference 2025. 2025</p>
<p>Exploring the adversarial vulnerabilities of vision-language-action models in robotics. T Wang, C Han, J C Liang, W Yang, D Liu, L X Zhang, Q Wang, J Luo, R Tang, 2025</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, 2022</p>
<p>Cacti: A framework for scalable multi-task multi-scene visual imitation learning. Z Mandi, H Bharadhwaj, V Moens, S Song, A Rajeswaran, V Kumar, 2023</p>
<p>Where are we in the search for an artificial visual cortex for embodied intelligence?. A Majumdar, K Yadav, S Arnaud, Y J Ma, C Chen, S Silwal, A Jain, V.-P Berges, P Abbeel, J Malik, D Batra, Y Lin, O Maksymets, A Rajeswaran, F Meier, 2024</p>
<p>. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, P D Fagan, J Hejna, M Itkina, M Lepert, Y J Ma, P T Miller, J Wu, S Belkhale, S Dass, H Ha, A Jain, A Lee, Y Lee, M Memmel, S Park, I Radosavovic, K Wang, A Zhan, K Black, C Chi, K B Hatch, S Lin, J Lu, J Mercat, A Rehman, P R Sanketi, A Sharma, C Simpson, Q Vuong, H R Walke, B Wulfe, T Xiao, J H Yang, A Yavary, T Z Zhao, C Agia, R Baijal, M G Castro, D Chen, Q Chen, T Chung, J Drake, E P Foster, J Gao, V Guizilini, D A Herrera, M Heo, K Hsu, J Hu, M Z Irshad, D Jackson, C Le, Y Li, K Lin, R Lin, Z Ma, A Maddukuri, S Mirchandani, D Morton, T Nguyen, A O'neill, R Scalise, D Seale, V Son, S Tian, E Tran, A E Wang, Y Wu, A Xie, J Yang, P Yin, Y Zhang, O Bastani, G Berseth, J Bohg, K Goldberg, A Gupta, A Gupta, D Jayaraman, J J Lim, J Malik, R Martn-Martn, S Ramamoorthy, D Sadigh, S Song, J Wu, M C Yip, Y Zhu, T Kollar, S Levine, C Finn, Droid: A large-scale in-the-wild robot manipulation dataset," 2025. [Online</p>
<p>Visa-flow: Accelerating robot skill learning via large-scale video semantic action flow. C Chen, Q Yang, X Xu, N Fazeli, O Andersson, arXiv:2505.012882025arXiv preprint</p>
<p>Effective tuning strategies for generalist robot manipulation policies. W Zhang, Y Li, Y Qiao, S Huang, J Liu, F Dayoub, X Ma, L Liu, 2024</p>
<p>Training strategies for efficient embodied reasoning. W Chen, S Belkhale, S Mirchandani, O Mees, D Driess, K Pertsch, S Levine, arXiv:2505.082432025arXiv preprint</p>
<p>Cast: Counterfactual labels improve instruction following in vision-language-action models. C Glossop, W Chen, A Bhorkar, D Shah, S Levine, 2025</p>
<p>Robobrain: A unified brain model for robotic manipulation from abstract to concrete. Y Ji, H Tan, J Shi, X Hao, Y Zhang, H Zhang, P Wang, M Zhao, Y Mu, P An, X Xue, Q Su, H Lyu, X Zheng, J Liu, Z Wang, S Zhang, 2025</p>
<p>Edgevla: Efficient vision-language-action models. P Budzianowski, W Maa, M Freed, J Mo, W Hsiao, A Xie, T Moduchowski, V Tipnis, B Bolte, arXiv:2507.140492025arXiv preprint</p>
<p>Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution. Y Yue, Y Wang, B Kang, Y Han, S Wang, S Song, J Feng, G Huang, 2024</p>
<p>A dual process vla: Efficient robotic manipulation leveraging vlm. B Han, J Kim, J Jang, 2024</p>
<p>Ceed-vla: Consistency vision-language-action model with early-exit decoding. W Song, J Chen, P Ding, Y Huang, H Zhao, D Wang, H Li, 2025</p>
<p>Robomamba: Efficient vision-languageaction model for robotic reasoning and manipulation. J Liu, M Liu, Z Wang, P An, X Li, K Zhou, S Yang, R Zhang, Y Guo, S Zhang, 2024</p>
<p>Revla: Reverting visual domain limitation of robotic foundation models. S Dey, J.-N Zaech, N Nikolov, L V Gool, D P Paudel, 2025</p>
<p>Safe: Multitask failure detection for vision-language-action models. Q Gu, Y Ju, S Sun, I Gilitschenski, H Nishimura, M Itkina, F Shkurti, 2025</p>
<p>Dywa: Dynamics-adaptive world action model for generalizable nonprehensile manipulation. J Lyu, Z Li, X Shi, C Xu, Y Wang, H Wang, 2025</p>
<p>Crayonrobo: Object-centric prompt-driven vision-languageaction model for robotic manipulation. X Li, L Xu, M Zhang, J Liu, Y Shen, I Ponomarenko, J Xu, L Heng, S Huang, S Zhang, H Dong, 2025</p>
<p>M Argus, J Bratulic, H Masnavi, M Velikanov, N Heppert, A Valada, T Brox, arXiv:2507.02190cvla: Towards efficient camera-space vlas. 2025arXiv preprint</p>
<p>Real-time execution of action chunking flow policies. K Black, M Y Galliker, S Levine, 2025</p>
<p>Fast-in-slow: A dual-system foundation model unifying fast manipulation within slow reasoning. H Chen, J Liu, C Gu, Z Liu, R Zhang, X Li, X He, Y Guo, C.-W Fu, S Zhang, P.-A Heng, 2025</p>
<p>Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation. M Zhu, Y Zhu, J Li, J Wen, Z Xu, N Liu, R Cheng, C Shen, Y Peng, F Feng, J Tang, 2024</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, 2023</p>
<p>Robouniview: Visual-language model with unified view representation for robotic manipulation. F Liu, F Yan, L Zheng, C Feng, Y Huang, L Ma, 2024</p>
<p>Showui: One vision-languageaction model for gui visual agent. K Q Lin, L Li, D Gao, Z Yang, S Wu, Z Bai, W Lei, L Wang, M Z Shou, 2024</p>
<p>Physically grounded visionlanguage models for robotic manipulation. J Gao, B Sarkar, F Xia, T Xiao, J Wu, B Ichter, A Majumdar, D Sadigh, 2024</p>
<p>A survey on robotics with foundation models: toward embodied ai. Z Xu, K Wu, J Wen, J Li, N Liu, Z Che, J Tang, 2024</p>
<p>General flow as foundation affordance for scalable robot learning. C Yuan, C Wen, T Zhang, Y Gao, 8th Annual Conference on Robot Learning. 2024</p>
<p>Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. D Driess, J T Springenberg, B Ichter, L Yu, A Li-Bell, K Pertsch, A Z Ren, H Walke, Q Vuong, L X Shi, S Levine, 2025</p>
<p>Manipulation facing threats: Evaluating physical vulnerabilities in end-to-end vision language action models. H Cheng, E Xiao, C Yu, Z Yao, J Cao, Q Zhang, J Wang, M Sun, K Xu, J Gu, R Xu, 2024</p>
<p>Probing a vision-language-action model for symbolic states and integration into a cognitive architecture. H Lu, H Li, P S Shahani, S Herbers, M Scheutz, 2025</p>
<p>Vla modelexpert collaboration for bi-directional manipulation learning. T.-Y Xiang, A.-Q Jin, X.-H Zhou, M.-J Gui, X.-L Xie, S.-Q Liu, S.-Y Wang, S.-B Duang, S.-C Wang, Z Lei, arXiv:2503.041632025arXiv preprint</p>
<p>Closed-loop open-vocabulary mobile manipulation with gpt-4v. P Zhi, Z Zhang, Y Zhao, M Han, Z Zhang, Z Li, Z Jiao, B Jia, S Huang, 2025</p>
<p>Instruction-driven history-aware policies for robotic manipulations. P.-L Guhur, S Chen, R Garcia, M Tapaswi, I Laptev, C Schmid, 2022</p>
<p>An atomic skill library construction method for data-efficient embodied manipulation. D Li, B Peng, C Li, N Qiao, Q Zheng, L Sun, Y Qin, B Li, Y Luan, B Wu, Y Zhan, M Sun, T Xu, L Li, H Shen, X He, 2025</p>
<p>Roboground: Robotic manipulation with grounded vision-language priors. H Huang, X Chen, Y Chen, H Li, X Han, Z Wang, T Wang, J Pang, Z Zhao, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference202522550</p>
<p>Multiple interactions made easy (mime): Large scale demonstrations data for imitation. P Sharma, L Mohan, L Pinto, A Gupta, publisher Copyright:  CoRL 2018. All rights reserved.; 2nd Conference on Robot Learning. 2018. 2018. 2018 Through 31-10-201887Proceedings of Machine Learning Research. Conference date</p>
<p>Scaling egocentric vision: The epic-kitchens dataset. D Damen, H Doughty, G M Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray, 2018</p>
<p>Robonet: Large-scale multirobot learning. S Dasari, F Ebert, S Tian, S Nair, B Bucher, K Schmeckpeper, S Singh, S Levine, C Finn, Machine Learning Research. 1002019. 2019</p>
<p>Mt-opt: Continuous multi-task robotic reinforcement learning at scale. D Kalashnikov, J Varley, Y Chebotar, B Swanson, R Jonschkowski, C Finn, S Levine, K Hausman, arXiv:2104.082122021arXiv preprint</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, Conference on Robot Learning. PMLR2022</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Open-world object manipulation using pre-trained vision-language models. A Stone, T Xiao, Y Lu, K Gopalakrishnan, K.-H Lee, Q Vuong, P Wohlhart, S Kirmani, B Zitkovich, F Xia, Conference on Robot Learning. PMLR2023</p>
<p>Robohive: A unified framework for robot learning. V Kumar, R Shah, G Zhou, V Moens, V Caggiano, A Gupta, A Rajeswaran, Advances in Neural Information Processing Systems. 202336340</p>
<p>On bringing robots home. N M M Shafiullah, A Rai, H Etukuru, Y Liu, I Misra, S Chintala, L Pinto, 2023</p>
<p>Rh20t: A comprehensive robotic dataset for learning diverse skills in one-shot. H.-S Fang, H Fang, Z Tang, J Liu, C Wang, J Wang, H Zhu, C Lu, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Droid: A large-scale in-the-wild robot manipulation dataset. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, 2024CoRR</p>
<p>Autort: Embodied foundation models for large scale orchestration of robotic agents. M Ahn, D Dwibedi, C Finn, M G Arenas, K Gopalakrishnan, K Hausman, B Ichter, A Irpan, N Joshi, R Julian, arXiv:2401.129632024arXiv preprint</p>
<p>Universal manipulation interface: Inthe-wild robot teaching without in-the-wild robots. C Chi, Z Xu, C Pan, E Cousineau, B Burchfiel, S Feng, R Tedrake, S Song, 2024CoRR</p>
<p>Roboturk: A crowdsourcing platform for robotic skill learning through imitation. A Mandlekar, Y Zhu, A Garg, J Booher, M Spero, A Tung, J Gao, J Emmons, A Gupta, E Orbay, S Savarese, L Fei-Fei, Proceedings of The 2nd Conference on Robot Learning, ser. Proceedings of Machine Learning Research. A Billard, J Dragan, J Peters, Morimoto, The 2nd Conference on Robot Learning, ser. Machine Learning ResearchPMLROct 201887</p>
<p>Meta-world: A benchmark and evaluation for multitask and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Conference on robot learning. PMLR2020</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202010749</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, IEEE Robotics and Automation Letters. 522020</p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, arXiv:2210.03094202226arXiv preprint</p>
<p>Calvin: A benchmark for language-conditioned policy learning for longhorizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, IEEE Robotics and Automation Letters. 732022</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, 2023</p>
<p>Lota-bench: Benchmarking language-oriented task planners for embodied agents. J.-W Choi, Y Yoon, H Ong, J Kim, M Jang, 2024</p>
<p>Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. Z Fu, T Z Zhao, C Finn, 2024</p>
<p>Robocasa: Large-scale simulation of everyday tasks for generalist robots. S Nasiriany, A Maddukuri, L Zhang, A Parikh, A Lo, A Joshi, A Mandlekar, Y Zhu, 2024</p>
<p>Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. Y Wang, Z Xian, F Chen, T.-H Wang, Y Wang, K Fragkiadaki, Z Erickson, D Held, C Gan, International Conference on Machine Learning. PMLR2024983</p>
<p>Benchmarking vision, language, &amp; action models on robotic learning tasks. P Guruprasad, H Sikka, J Song, Y Wang, P P Liang, 2024</p>
<p>Persformer: 3d lane detection via perspective transformer and the openlane benchmark. L Chen, C Sima, Y Li, Z Zheng, J Xu, X Geng, H Li, C He, J Shi, Y Qiao, J Yan, 2022</p>
<p>The cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, 2016</p>
<p>Bdd100k: A diverse driving dataset for heterogeneous multitask learning. F Yu, H Chen, X Wang, W Xian, Y Chen, F Liu, V Madhavan, T Darrell, 2020</p>
<p>The apolloscape open dataset for autonomous driving and its application. X Huang, P Wang, X Cheng, D Zhou, Q Geng, R Yang, 10.1109/TPAMI.2019.2926463IEEE Transactions on Pattern Analysis and Machine Intelligence. 4210Oct. 2020</p>
<p>Argoverse: 3d tracking and forecasting with rich maps. M.-F Chang, J Lambert, P Sangkloy, J Singh, S Bak, A Hartnett, D Wang, P Carr, S Lucey, D Ramanan, J Hays, 2019</p>
<p>Navsim: Data-driven non-reactive autonomous vehicle simulation and benchmarking. D Dauner, M Hallgarten, T Li, X Weng, Z Huang, Z Yang, H Li, I Gilitschenski, B Ivanovic, M Pavone, A Geiger, K Chitta, 2024</p>
<p>One million scenes for autonomous driving: Once dataset. J Mao, M Niu, C Jiang, H Liang, J Chen, X Liang, Y Li, C Ye, W Zhang, Z Li, J Yu, H Xu, C Xu, 2021</p>
<p>. P Sun, H Kretzschmar, X Dotiwalla, A Chouard, V Patnaik, P Tsui, J Guo, Y Zhou, Y Chai, B Caine, V Vasudevan, W Han, J Ngiam, H Zhao, A Timofeev, S Ettinger, M Krivokon, A Gao, A Joshi, S Zhao, S Cheng, Y Zhang, J Shlens, Z Chen, D Anguelov, Scalability in perception for autonomous driving: Waymo open dataset," 2020. [Online</p>
<p>nuscenes: A multimodal dataset for autonomous driving. H Caesar, V Bankiti, A H Lang, S Vora, V E Liong, Q Xu, A Krishnan, Y Pan, G Baldan, O Beijbom, 2020</p>
<p>Are we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, 2012 IEEE Conference on Computer Vision and Pattern Recognition. 2012</p>
<p>Virtualhome: Simulating household activities via programs. X Puig, K Ra, M Boben, J Li, T Wang, S Fidler, A Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, Conference on Robot Learning. PMLR2020</p>
<p>Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments. F Xia, W B Shen, C Li, P Kasimbeg, M E Tchapmi, A Toshev, R Martn-Martn, S Savarese, IEEE Robotics and Automation Letters. 522020</p>
<p>Bench2drive: Towards multi-ability benchmarking of closed-loop end-to-end autonomous driving. X Jia, Z Yang, Q Li, Z Zhang, J Yan, Advances in Neural Information Processing Systems. A Globerson, L Mackey, D Belgrave, A Fan, U Paquet, J Tomczak, C Zhang, Curran Associates, Inc202437</p>
<p>Lmdrive: Closed-loop end-to-end driving with large language models. H Shao, Y Hu, L Wang, G Song, S L Waslander, Y Liu, H Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2024130</p>
<p>Unisim: A neural closed-loop sensor simulator. Z Yang, Y Chen, J Wang, S Manivasagam, W.-C Ma, A J Yang, R Urtasun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2023</p>
<p>Deepmind control suite. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.006902018arXiv preprint</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Sapien: A simulated part-based interactive environment. F Xiang, Y Qin, K Mo, Y Xia, H Zhu, F Liu, M Liu, H Jiang, Y Yuan, H Wang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020107</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D S Chaplot, O Maksymets, Advances in neural information processing systems. 202134</p>
<p>igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Martn-Martn, L Fan, G Wang, C Prez-D'arpino, S Buch, S Srivastava, L Tchapmi, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. C Li, F Xia, R Martn-Martn, M Lingelbach, S Srivastava, B Shen, K E Vainio, C Gokmen, G Dharan, T Jain, Conference on Robot Learning. PMLR2022</p>
<p>Threedworld: A platform for interactive multi-modal physical simulation. C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, J De Freitas, J Kubilius, A Bhandwaldar, N Haber, M Sano, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Close the optical sensing domain gap by physics-grounded active stereo sensor simulation. X Zhang, R Chen, A Li, F Xiang, Y Qin, J Gu, Z Ling, M Liu, P Zeng, S Han, Z Huang, T Mu, J Xu, H Su, IEEE Transactions on Robotics. 2023</p>
<p>Bullet physics sdk. </p>
<p>Genesis: A universal and generative physics engine for robotics and beyond. G Authors, december 2024</p>
<p>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. Q Bu, J Cai, L Chen, X Cui, Y Ding, S Feng, S Gao, X He, X Huang, S Jiang, arXiv:2503.066692025arXiv preprint</p>
<p>Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. S Tao, F Xiang, A Shukla, Y Qin, X Hinrichsen, X Yuan, C Bao, X Lin, Y Liu, T Chan, Y Gao, X Li, T Mu, N Xiao, A Gurha, V N Rajesh, Y W Choi, Y -R. Chen, Z Huang, R Calandra, R Chen, S Luo, H Su, 2025</p>
<p>Lgsvl simulator: A high fidelity simulator for autonomous driving. G Rong, B H Shin, H Tabatabaee, Q Lu, S Lemke, M Moeiko, E Boise, G Uhm, M Gerow, S Mehta, E Agafonov, T H Kim, E Sterner, K Ushiroda, M Reyes, D Zelenkovsky, S Kim, 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). 2020</p>
<p>Franka robotics. </p>
<p>A1 robot arm. </p>
<p>Lerobot: State-of-the-art machine learning for realworld robotics in pytorch. R Cadene, S Alibert, A Soare, Q Gallouedec, A Zouitine, S Palma, P Kooijmans, M Aractingi, M Shukor, D Aubakirova, M Russi, F Capuano, C Pascale, J Choghari, J Moss, T Wolf, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
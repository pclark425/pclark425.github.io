<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2610 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2610</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2610</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-c3f1fae241a3c2449e675ab750873d800f95513c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c3f1fae241a3c2449e675ab750873d800f95513c" target="_blank">SimPO: Simple Preference Optimization with a Reference-Free Reward</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> SimPO is proposed, a simpler yet more effective approach to DPO that consistently and significantly outperforms existing approaches without substantially increasing response length and introduces a target reward margin to the Bradley-Terry objective.</p>
                <p><strong>Paper Abstract:</strong> Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a 72.4% length-controlled win rate on AlpacaEval 2, a 59.1% win rate on Arena-Hard, and ranks 1st on Chatbot Arena among<10B models with real user votes.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2610.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2610.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple Preference Optimization (SimPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline preference-optimization algorithm that uses the average (length-normalized) token log-probability of a response as an implicit reward and a target reward margin in a Bradley–Terry objective; eliminates the need for a reference model and is more compute- and memory-efficient than reference-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SimPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SimPO is an offline preference-optimization algorithm for aligning large language models with human preferences. Key capabilities: (1) defines an implicit reward r(x,y) = (β/|y|) log π_θ(y|x) (average token log-probability, length-normalized), (2) plugs this reward into a Bradley–Terry pairwise preference objective with an added target margin γ, and (3) directly optimizes the policy model (no explicit reward model or reference model required). It is applied in training loops (single epoch, batch 128) to update model weights; it does not itself generate new experimental designs beyond producing updated model parameters and associated scalar rewards and statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Preference-Optimization Algorithm (LLM alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — large language model alignment / instruction-following (LLMs such as Mistral, Llama-3, Gemma)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Optimize LLM policies from offline human preference data (pairs of prompt, winning response y_w, losing response y_l) to improve helpfulness/quality of generated responses across open-ended instruction-following and chat-style tasks evaluated by pairwise comparisons and automatic judges.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional nonconvex optimization over billions of model parameters (7B–70B models used in experiments), large discrete sequence output spaces, pairwise preference supervision; multi-objective trade-offs (quality vs. length, KL drift vs. improvement); search space implicitly huge (all possible sequences up to 2048 tokens). Quantitative aspects: training on SFT models trained on UltraChat-200k; preference dataset UltraFeedback (size not precisely enumerated in text, but used at scale); models trained on 8× H100 GPUs; hyperparameter ranges β∈[2.0,2.5], γ∈[0.3..1.6].</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses pre-existing offline preference datasets (UltraFeedback) and SFT checkpoints (UltraChat-200k SFT or off-the-shelf Instruct checkpoints). Evaluation datasets used: AlpacaEval 2 (805 examples), Arena-Hard (500 queries), MT-Bench (500 examples). Preference pairs were sometimes generated by the SFT model (Instruct setup) by sampling 5 responses per prompt and selecting highest/lowest via PairRM; data generation was a single pass in this work (not iterative).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Experiments performed on 8× H100 GPUs; typical preference-optimization runs use batch size 128 and a single epoch with max sequence length 2048. The paper reports SimPO reduced overall run time by ~20% and per-GPU peak memory by ~10% compared to a vanilla DPO implementation (Llama-3-Base, 8× H100). Exact wall-clock hours or dollar costs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-specified supervised pairwise preference learning problem (discrete sequence outputs); stochastic generation; clear evaluation metrics available (pairwise win rates, length-controlled win rate, MT-Bench score, reward accuracy, KL divergence to a reference model). Relies on substantial domain knowledge for prompt formatting and decoding hyperparameters; offline (non-iterative) training in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Length-controlled and raw pairwise win rate on AlpacaEval 2 and Arena-Hard, MT-Bench score (GPT-4 judge), reward accuracy (percentage of training/held-out pairs where r(y_w)>r(y_l)), KL divergence to reference models, generation length statistics, and judge-based rankings (e.g., Chatbot Arena placement).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Consistently outperforms baseline preference objectives across multiple setups: reported improvements up to +6.4 percentage points on AlpacaEval 2 and up to +7.5 percentage points on Arena-Hard versus DPO; best reported model (Gemma-2-9B-it-SimPO) achieved 72.4% length-controlled win rate on AlpacaEval 2 and 59.1% win rate on Arena-Hard; in benchmark tables SimPO LC win rates range (examples) Mistral-Base 21.5% (LC), Mistral-Instruct 32.1% (LC), Llama-3-Base 22.0% (LC), Llama-3-Instruct 44.7% (LC). Also reports higher 'reward accuracy' than DPO on held-out data (quantitatively improved but exact percent by setting in Figure 4c).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Potential for reward hacking or degeneration if not properly tuned (no explicit KL regularization in SimPO); can reduce average log-likelihood of winning sequences if target margin γ is too large, leading to degeneration; occasional drops in math/reasoning-heavy downstream tasks (GSM8K) observed after preference optimization; requires careful hyperparameter tuning (β, γ, learning rate).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key design elements: (1) length-normalized average log-likelihood reward aligns training objective with inference generation metric and prevents length exploitation; (2) a target margin γ improves reward accuracy and generalization when properly tuned; (3) starting from high-quality SFT/Instruct models and high-quality preference data improves outcomes; thorough hyperparameter tuning; elimination of reference model reduces variance and resource cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Across multiple base and instruct settings, SimPO outperforms DPO and a suite of baselines (RRHF, SLiC-HF, IPO, CPO, KTO, ORPO, R-DPO) on AlpacaEval 2 LC and Arena-Hard WR; SimPO yields consistent gains (e.g., +3.6 to +4.8 LC points over best baseline on AlpacaEval 2 across settings) and is also ~20% faster and ~10% less memory-intensive than naive DPO. CPO sometimes surpasses SimPO on Arena-Hard but generates much longer outputs (~50% longer on average).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2610.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2610.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline preference optimization algorithm that reparameterizes the implicit reward as a scaled log-ratio between the policy and a reference (typically SFT) model and optimizes a Bradley–Terry pairwise objective without training an explicit reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DPO expresses the reward r(x,y) = β log (π_θ(y|x)/π_ref(y|x)) (plus a partition term) and optimizes pairwise preferences via a logistic (Bradley–Terry) objective; requires a reference model (π_ref) during training to compute the ratio, which increases compute and memory overhead. DPO has been used as a baseline and starting point in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Preference-Optimization Algorithm (LLM alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — LLM alignment / preference learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Optimize LLM policies from offline pairwise human preferences by using the policy-to-reference log-likelihood ratio as an implicit reward in a pairwise ranking objective.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same high-dimensional optimization as SimPO; requires maintaining and forward-passing a reference model in addition to the policy model, increasing memory and compute. Sensitive to length bias because formulation does not explicitly normalize for response length.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained on the same offline preference datasets (UltraFeedback) and initialized from SFT or instruct checkpoints; evaluation on AlpacaEval 2, Arena-Hard, MT-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Requires forward passes through both policy and reference models during naive implementations, which increases per-step compute and memory; in experiments used 8× H100 GPUs. Paper reports that SimPO (reference-free) cuts run time by ~20% and per-GPU peak memory by ~10% compared to a vanilla DPO implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Pairwise preference ranking objective; discrete, stochastic generation; evaluation via pairwise judges; risk of mismatch between training reward and inference ranking because DPO reward uses log-ratio while inference uses average log-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Same evaluation suite: AlpacaEval 2 LC & raw win rate, Arena-Hard win rate, MT-Bench score, reward accuracy (how often r(y_w)>r(y_l)), KL divergence to reference.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Improves over SFT baselines but generally underperforms SimPO in reported experiments. Example numbers from paper: Llama-3-Instruct DPO LC = 40.3% vs SimPO 44.7% (LC); DPO often yields lower reward-accuracy alignment with average log-likelihood (paper reports ~50% of training triples satisfy likelihood ranking under DPO, indicating near-random alignment for that metric).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Mismatch between DPO implicit reward and average log-likelihood used at generation causes poor correspondence between trained reward ranks and actual likelihood ranking—only ~50% of training triples had matching likelihood ranking. Potential length bias remains (though log-ratio can implicitly attenuate length effects). Additional memory/compute overhead from reference model; sometimes inferior reward-accuracy generalization compared to SimPO.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Constrain policy relative to a reference model can reduce KL drift; for strong references higher β reduces KL divergence and can help; widely adopted due to simplicity and stability compared to multi-stage RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>DPO is a strong baseline among offline preference objectives but in these experiments is consistently outperformed by SimPO across multiple architectures and settings. Some DPO variants (R‑DPO, IPO) aim to address length/regulation issues but with mixed empirical gains.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2610.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2610.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ORPO (reference-free odd-ratio preference optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent reference-model-free preference optimization method that introduces an odd-ratio term which contrasts winning and losing responses under the policy model and is trained jointly with an SFT objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ORPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ORPO is a monolithic, reference-free objective that contrasts winning and losing responses using quantities derived from the policy model (an odd-ratio term) and optionally includes a supervised fine-tuning loss as regularization. In this paper ORPO is included as a baseline and compared empirically to SimPO and other objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Preference-Optimization Algorithm (reference-free)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — LLM alignment and preference optimization</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Train a policy directly on preference data without a separate reference model, using a probabilistic odd-ratio term and (in the original formulation) additional regularization via an SFT objective.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same high-dimensional discrete sequence optimization; needs careful tuning of joint terms (odd-ratio weighting and SFT regularizer) to avoid degeneration or failure to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applicable to offline preference datasets; in this paper started from SFT checkpoints for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Reference-free so similar compute profile to SimPO; however ORPO includes additional joint losses which may affect stability and tuning complexity. Exact compute numbers not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Offline pairwise preference learning, discrete generation, stochastic; includes supervised regularization to avoid degeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AlpacaEval 2 LC/WR, Arena-Hard WR, MT-Bench scores.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Per Table 4, ORPO improved over SFT but in these experiments SimPO still outperforms ORPO in most settings (examples: Mistral-Base ORPO LC 14.7% vs SimPO 21.5%; Llama-3-Instruct ORPO LC 28.5% vs SimPO 44.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Needs supervised regularization to avoid degeneration; in this paper ORPO was less effective than SimPO in many settings despite being reference-free.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Reference-free formulation and joint SFT regularization can stabilize training and retain supervised signal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Performed worse than SimPO across most settings in the paper; ORPO benefits from SFT joint loss but did not match SimPO's alignment of reward with generation likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2610.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2610.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Preference Optimization (CPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preference optimization method that uses sequence likelihood as a reward and includes a supervised fine-tuning term; competitive on some tasks (e.g., machine translation) but can favor longer outputs in some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CPO uses a contrastive objective that maximizes the likelihood of winning sequences relative to losing ones and includes a supervised fine-tuning regularizer to stabilize training. It treats sequence likelihood as the reward and optimizes a logistic pairwise loss augmented with an SFT penalty term.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Preference-Optimization Algorithm (contrastive + SFT regularizer)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — LLM alignment; has demonstrated strong performance in machine translation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve quality of generated sequences from LLMs using contrastive likelihood objectives while preserving supervised behavior via an SFT penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same large-scale model tuning complexity; objective trades off reward maximization and supervised fidelity; can encourage verbosity if not controlled (paper reports CPO generates longer outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on offline preference pairs and SFT checkpoints; used the same UltraFeedback/UltraChat datasets in comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Comparable to other single-model objectives when SFT term is computed; experiment details use 8× H100 but exact per-method runtime not enumerated beyond SimPO vs. DPO comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Offline pairwise sequence ranking with explicit supervised regularization; clear metrics available for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AlpacaEval 2 LC/WR, Arena-Hard WR, downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>CPO improves over SFT in many settings; in some Arena-Hard runs CPO occasionally surpasses SimPO but tends to produce much longer outputs (paper notes ~50% longer average outputs than SimPO), suggesting length exploitation depending on evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tendency to produce much longer responses (verbosity) when optimizing raw sequence likelihood without length normalization; may exploit length if SFT regularization is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Combining contrastive reward with SFT regularizer can balance quality and supervised fidelity; strong when tasks benefit from more verbose outputs (some Arena-Hard items).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>CPO sometimes beats SimPO on Arena-Hard but at the cost of substantially longer generations; SimPO generally better balanced for length-controlled metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2610.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2610.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RRHF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ranked Responses with Human Feedback (RRHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ranking-based loss that uses length-normalized log-likelihood to prefer winning responses and includes an optional penalty promoting high-probability winning sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RRHF</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RRHF is a ranking objective which directly contrasts length-normalized log-likelihoods of winning and losing responses (max(0, -p(y_w)+p(y_l))) and optionally penalizes low-probability winning sequences; used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Preference-Optimization Algorithm (ranking loss)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — preference-based LLM alignment</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Rank pairs of responses to increase probability of winning responses while optionally applying a penalty to avoid low-probability outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Relatively simpler objective among preference methods; still operates on large models and sequence outputs; requires careful λ regularization tuning to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Same offline preference datasets as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Single-model objective (reference-free) so similar compute to SimPO; specific compute numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Pairwise ranking with length normalization option; discrete, stochastic outputs with standard evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AlpacaEval 2 LC/WR, Arena-Hard WR.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Improves over SFT but in these experiments underperforms SimPO and some other baselines. Example: Mistral-Base RRHF LC 11.6% vs SimPO 21.5% (LC).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance sensitive to λ regularizer; may not generalize as well as margin-based or likelihood-aligned objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Using length-normalized log-likelihood helps mitigate length bias; simple ranking losses are stable and easy to tune.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Better than SFT but generally below SimPO and some other modern baselines in the reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2610.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2610.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IPO (Implicit Preference Optimization / related variant cited as IPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretically-motivated preference optimization approach that avoids DPO's pointwise reward replacement with a pairwise-aware objective; includes a margin-like term; evaluated as a baseline here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>IPO formulates a pairwise-aware squared-objective based on log ratios to the reference model (similar in spirit to adding an explicit margin), aiming to honor pairwise preference structure more faithfully than pointwise approximations. Included in empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Preference-Optimization Algorithm (pairwise-aware)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — preference learning for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Optimize policy parameters to match pairwise preference data while reducing approximations inherent in other reparameterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Comparable to DPO in complexity; requires reference model and careful hyperparameter τ settings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses same offline preference datasets and SFT initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Reference-based objective so similar memory/compute footprint to naive DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Pairwise quadratic objective; discrete outputs; requires hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Win rates (AlpacaEval 2, Arena-Hard), MT-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Improves over SFT but in reported experiments IPO underperforms SimPO and sometimes DPO depending on setting (e.g., Mistral-Base IPO LC 11.8% vs SimPO 21.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Less effective than SimPO empirically in these experiments; sensitive to τ hyperparameter selection.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Theoretical motivation to better match pairwise preference structure may help in some regimes; requires strong reference and tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>IPO did not outperform SimPO in the paper's reported empirical suite.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2610.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2610.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KTO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KTO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that learns from non-paired preference data by optimizing prospect-theoretic objectives and KL-related reference statistics; included as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KTO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KTO is a preference-optimization approach capable of learning from non-paired preference data by leveraging reference-model-calibrated thresholds and KL-based statistics (z_ref) to decide winners/losers; tested as a baseline in the experimental comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Preference-Optimization Algorithm (non-pairwise-aware / prospect-theoretic)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — LLM alignment from preference-like signals</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn to align a policy model using weaker/non-paired preference signals and reference-model-derived thresholds to handle non-paired data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handles more ambiguous supervision formats (non-paired), requiring estimation of reference-model expectations and KL terms; complexity increases with reference estimations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Works with preference or approximate-preference datasets; compared here on UltraFeedback-derived data.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses reference-model computations and KL expectations; similar to other reference-based methods in compute profile.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Less structured supervision (non-paired), stochastic outputs, needs extra calibration statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Win rates and MT-Bench scores.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Per Table 4, KTO improves over SFT and performs competitively in some settings (e.g., Mistral-Instruct KTO LC 24.5% vs SimPO 32.1%), but generally below SimPO in most reported setups.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Complexity from estimating z_ref and dependence on reference-model fidelity; may be sensitive to optimizer choices and reference quality.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Can leverage weaker supervision formats and KL-based calibration to learn from non-paired data; benefits from strong reference models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Competitive baseline in some setups but below SimPO overall in the paper's benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2610.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2610.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLHF / PPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from Human Feedback (classical RLHF) and PPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical RLHF is a multi-stage pipeline (train reward model from human preferences, then run RL policy optimization e.g., PPO to maximize reward) widely used for LLM alignment; PPO is a common RL optimizer used in the RLHF policy optimization stage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RLHF (classical pipeline) and PPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RLHF comprises: (1) supervised fine-tuning, (2) reward model training from human preference labels, (3) policy optimization (commonly with PPO) to maximize the learned reward. PPO (Proximal Policy Optimization) is an on-policy RL algorithm used to update policies under reward signals while constraining update steps. In this paper RLHF/PPO are discussed as background and compared conceptually to offline preference objectives (DPO/SimPO), but PPO was not directly run in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Policy Optimization Framework (reinforcement learning pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning — LLM alignment via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Use human preference labels to train a reward model and then optimize a language model policy with RL to produce preferred outputs; challenges include complex multi-stage optimization, instability, and resource intensity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Multi-stage pipeline complexity: training a reward model (model capacity and overfitting risks), then RL policy optimization across huge action space (token sequences). On-policy sampling and iterative data collection further increase complexity. PPO/online RL requires many environment evaluations/samples; known to be resource intensive and difficult to tune.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires paired human preference data for reward model training and potentially further online sampling; can be expensive to collect human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>High: reward-model training plus RL optimization with many environment rollouts; historically large compute (OpenAI/DeepMind-scale runs) but exact numbers vary. Paper cites that offline alternatives like DPO and SimPO are simpler and more stable.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, stochastic, and often on-policy; clear evaluation metrics but RL optimization can overfit reward model or produce degenerate policies if not regularized.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Human-preference improvement, win rates against baselines, RL reward maximization, and downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Classical RLHF (with PPO) has yielded strong results in prior work but is noted in this paper to have optimization challenges; direct numerical comparisons to PPO are deferred to future work and not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Multi-stage complexity leads to instability, reward-model overoptimization, and many practical challenges (tuning, sample inefficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large, high-quality human preference datasets; careful regularization (KL penalties), good reward-model generalization, and tuned RL hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper positions offline methods (DPO, SimPO) as simpler and more stable alternatives to classical RLHF+PPO pipeline; direct empirical PPO-vs-SimPO comparisons were left to future work.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimPO: Simple Preference Optimization with a Reference-Free Reward', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>ORPO: Monolithic preference optimization without reference model <em>(Rating: 2)</em></li>
                <li>RRHF: Rank responses to align language models with human feedback <em>(Rating: 2)</em></li>
                <li>Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation <em>(Rating: 2)</em></li>
                <li>A general theoretical paradigm to understand learning from human preferences <em>(Rating: 2)</em></li>
                <li>KTO: Model alignment as prospect theoretic optimization <em>(Rating: 2)</em></li>
                <li>Iterative DPO alignment <em>(Rating: 1)</em></li>
                <li>Self-rewarding language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2610",
    "paper_id": "paper-c3f1fae241a3c2449e675ab750873d800f95513c",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "SimPO",
            "name_full": "Simple Preference Optimization (SimPO)",
            "brief_description": "An offline preference-optimization algorithm that uses the average (length-normalized) token log-probability of a response as an implicit reward and a target reward margin in a Bradley–Terry objective; eliminates the need for a reference model and is more compute- and memory-efficient than reference-based methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SimPO",
            "system_description": "SimPO is an offline preference-optimization algorithm for aligning large language models with human preferences. Key capabilities: (1) defines an implicit reward r(x,y) = (β/|y|) log π_θ(y|x) (average token log-probability, length-normalized), (2) plugs this reward into a Bradley–Terry pairwise preference objective with an added target margin γ, and (3) directly optimizes the policy model (no explicit reward model or reference model required). It is applied in training loops (single epoch, batch 128) to update model weights; it does not itself generate new experimental designs beyond producing updated model parameters and associated scalar rewards and statistics.",
            "system_type": "Automated Preference-Optimization Algorithm (LLM alignment)",
            "problem_domain": "Machine learning — large language model alignment / instruction-following (LLMs such as Mistral, Llama-3, Gemma)",
            "problem_description": "Optimize LLM policies from offline human preference data (pairs of prompt, winning response y_w, losing response y_l) to improve helpfulness/quality of generated responses across open-ended instruction-following and chat-style tasks evaluated by pairwise comparisons and automatic judges.",
            "problem_complexity": "High-dimensional nonconvex optimization over billions of model parameters (7B–70B models used in experiments), large discrete sequence output spaces, pairwise preference supervision; multi-objective trade-offs (quality vs. length, KL drift vs. improvement); search space implicitly huge (all possible sequences up to 2048 tokens). Quantitative aspects: training on SFT models trained on UltraChat-200k; preference dataset UltraFeedback (size not precisely enumerated in text, but used at scale); models trained on 8× H100 GPUs; hyperparameter ranges β∈[2.0,2.5], γ∈[0.3..1.6].",
            "data_availability": "Uses pre-existing offline preference datasets (UltraFeedback) and SFT checkpoints (UltraChat-200k SFT or off-the-shelf Instruct checkpoints). Evaluation datasets used: AlpacaEval 2 (805 examples), Arena-Hard (500 queries), MT-Bench (500 examples). Preference pairs were sometimes generated by the SFT model (Instruct setup) by sampling 5 responses per prompt and selecting highest/lowest via PairRM; data generation was a single pass in this work (not iterative).",
            "computational_requirements": "Experiments performed on 8× H100 GPUs; typical preference-optimization runs use batch size 128 and a single epoch with max sequence length 2048. The paper reports SimPO reduced overall run time by ~20% and per-GPU peak memory by ~10% compared to a vanilla DPO implementation (Llama-3-Base, 8× H100). Exact wall-clock hours or dollar costs not provided.",
            "problem_structure": "Well-specified supervised pairwise preference learning problem (discrete sequence outputs); stochastic generation; clear evaluation metrics available (pairwise win rates, length-controlled win rate, MT-Bench score, reward accuracy, KL divergence to a reference model). Relies on substantial domain knowledge for prompt formatting and decoding hyperparameters; offline (non-iterative) training in this work.",
            "success_metric": "Length-controlled and raw pairwise win rate on AlpacaEval 2 and Arena-Hard, MT-Bench score (GPT-4 judge), reward accuracy (percentage of training/held-out pairs where r(y_w)&gt;r(y_l)), KL divergence to reference models, generation length statistics, and judge-based rankings (e.g., Chatbot Arena placement).",
            "success_rate": "Consistently outperforms baseline preference objectives across multiple setups: reported improvements up to +6.4 percentage points on AlpacaEval 2 and up to +7.5 percentage points on Arena-Hard versus DPO; best reported model (Gemma-2-9B-it-SimPO) achieved 72.4% length-controlled win rate on AlpacaEval 2 and 59.1% win rate on Arena-Hard; in benchmark tables SimPO LC win rates range (examples) Mistral-Base 21.5% (LC), Mistral-Instruct 32.1% (LC), Llama-3-Base 22.0% (LC), Llama-3-Instruct 44.7% (LC). Also reports higher 'reward accuracy' than DPO on held-out data (quantitatively improved but exact percent by setting in Figure 4c).",
            "failure_modes": "Potential for reward hacking or degeneration if not properly tuned (no explicit KL regularization in SimPO); can reduce average log-likelihood of winning sequences if target margin γ is too large, leading to degeneration; occasional drops in math/reasoning-heavy downstream tasks (GSM8K) observed after preference optimization; requires careful hyperparameter tuning (β, γ, learning rate).",
            "success_factors": "Key design elements: (1) length-normalized average log-likelihood reward aligns training objective with inference generation metric and prevents length exploitation; (2) a target margin γ improves reward accuracy and generalization when properly tuned; (3) starting from high-quality SFT/Instruct models and high-quality preference data improves outcomes; thorough hyperparameter tuning; elimination of reference model reduces variance and resource cost.",
            "comparative_results": "Across multiple base and instruct settings, SimPO outperforms DPO and a suite of baselines (RRHF, SLiC-HF, IPO, CPO, KTO, ORPO, R-DPO) on AlpacaEval 2 LC and Arena-Hard WR; SimPO yields consistent gains (e.g., +3.6 to +4.8 LC points over best baseline on AlpacaEval 2 across settings) and is also ~20% faster and ~10% less memory-intensive than naive DPO. CPO sometimes surpasses SimPO on Arena-Hard but generates much longer outputs (~50% longer on average).",
            "human_baseline": null,
            "uuid": "e2610.0",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DPO",
            "name_full": "Direct Preference Optimization (DPO)",
            "brief_description": "An offline preference optimization algorithm that reparameterizes the implicit reward as a scaled log-ratio between the policy and a reference (typically SFT) model and optimizes a Bradley–Terry pairwise objective without training an explicit reward model.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Direct Preference Optimization (DPO)",
            "system_description": "DPO expresses the reward r(x,y) = β log (π_θ(y|x)/π_ref(y|x)) (plus a partition term) and optimizes pairwise preferences via a logistic (Bradley–Terry) objective; requires a reference model (π_ref) during training to compute the ratio, which increases compute and memory overhead. DPO has been used as a baseline and starting point in this paper's experiments.",
            "system_type": "Automated Preference-Optimization Algorithm (LLM alignment)",
            "problem_domain": "Machine learning — LLM alignment / preference learning",
            "problem_description": "Optimize LLM policies from offline pairwise human preferences by using the policy-to-reference log-likelihood ratio as an implicit reward in a pairwise ranking objective.",
            "problem_complexity": "Same high-dimensional optimization as SimPO; requires maintaining and forward-passing a reference model in addition to the policy model, increasing memory and compute. Sensitive to length bias because formulation does not explicitly normalize for response length.",
            "data_availability": "Trained on the same offline preference datasets (UltraFeedback) and initialized from SFT or instruct checkpoints; evaluation on AlpacaEval 2, Arena-Hard, MT-Bench.",
            "computational_requirements": "Requires forward passes through both policy and reference models during naive implementations, which increases per-step compute and memory; in experiments used 8× H100 GPUs. Paper reports that SimPO (reference-free) cuts run time by ~20% and per-GPU peak memory by ~10% compared to a vanilla DPO implementation.",
            "problem_structure": "Pairwise preference ranking objective; discrete, stochastic generation; evaluation via pairwise judges; risk of mismatch between training reward and inference ranking because DPO reward uses log-ratio while inference uses average log-likelihood.",
            "success_metric": "Same evaluation suite: AlpacaEval 2 LC & raw win rate, Arena-Hard win rate, MT-Bench score, reward accuracy (how often r(y_w)&gt;r(y_l)), KL divergence to reference.",
            "success_rate": "Improves over SFT baselines but generally underperforms SimPO in reported experiments. Example numbers from paper: Llama-3-Instruct DPO LC = 40.3% vs SimPO 44.7% (LC); DPO often yields lower reward-accuracy alignment with average log-likelihood (paper reports ~50% of training triples satisfy likelihood ranking under DPO, indicating near-random alignment for that metric).",
            "failure_modes": "Mismatch between DPO implicit reward and average log-likelihood used at generation causes poor correspondence between trained reward ranks and actual likelihood ranking—only ~50% of training triples had matching likelihood ranking. Potential length bias remains (though log-ratio can implicitly attenuate length effects). Additional memory/compute overhead from reference model; sometimes inferior reward-accuracy generalization compared to SimPO.",
            "success_factors": "Constrain policy relative to a reference model can reduce KL drift; for strong references higher β reduces KL divergence and can help; widely adopted due to simplicity and stability compared to multi-stage RLHF.",
            "comparative_results": "DPO is a strong baseline among offline preference objectives but in these experiments is consistently outperformed by SimPO across multiple architectures and settings. Some DPO variants (R‑DPO, IPO) aim to address length/regulation issues but with mixed empirical gains.",
            "human_baseline": null,
            "uuid": "e2610.1",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ORPO",
            "name_full": "ORPO (reference-free odd-ratio preference optimization)",
            "brief_description": "A recent reference-model-free preference optimization method that introduces an odd-ratio term which contrasts winning and losing responses under the policy model and is trained jointly with an SFT objective.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ORPO",
            "system_description": "ORPO is a monolithic, reference-free objective that contrasts winning and losing responses using quantities derived from the policy model (an odd-ratio term) and optionally includes a supervised fine-tuning loss as regularization. In this paper ORPO is included as a baseline and compared empirically to SimPO and other objectives.",
            "system_type": "Automated Preference-Optimization Algorithm (reference-free)",
            "problem_domain": "Machine learning — LLM alignment and preference optimization",
            "problem_description": "Train a policy directly on preference data without a separate reference model, using a probabilistic odd-ratio term and (in the original formulation) additional regularization via an SFT objective.",
            "problem_complexity": "Same high-dimensional discrete sequence optimization; needs careful tuning of joint terms (odd-ratio weighting and SFT regularizer) to avoid degeneration or failure to learn.",
            "data_availability": "Applicable to offline preference datasets; in this paper started from SFT checkpoints for fair comparison.",
            "computational_requirements": "Reference-free so similar compute profile to SimPO; however ORPO includes additional joint losses which may affect stability and tuning complexity. Exact compute numbers not provided here.",
            "problem_structure": "Offline pairwise preference learning, discrete generation, stochastic; includes supervised regularization to avoid degeneration.",
            "success_metric": "AlpacaEval 2 LC/WR, Arena-Hard WR, MT-Bench scores.",
            "success_rate": "Per Table 4, ORPO improved over SFT but in these experiments SimPO still outperforms ORPO in most settings (examples: Mistral-Base ORPO LC 14.7% vs SimPO 21.5%; Llama-3-Instruct ORPO LC 28.5% vs SimPO 44.7%).",
            "failure_modes": "Needs supervised regularization to avoid degeneration; in this paper ORPO was less effective than SimPO in many settings despite being reference-free.",
            "success_factors": "Reference-free formulation and joint SFT regularization can stabilize training and retain supervised signal.",
            "comparative_results": "Performed worse than SimPO across most settings in the paper; ORPO benefits from SFT joint loss but did not match SimPO's alignment of reward with generation likelihood.",
            "human_baseline": null,
            "uuid": "e2610.2",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CPO",
            "name_full": "Contrastive Preference Optimization (CPO)",
            "brief_description": "A preference optimization method that uses sequence likelihood as a reward and includes a supervised fine-tuning term; competitive on some tasks (e.g., machine translation) but can favor longer outputs in some benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "CPO",
            "system_description": "CPO uses a contrastive objective that maximizes the likelihood of winning sequences relative to losing ones and includes a supervised fine-tuning regularizer to stabilize training. It treats sequence likelihood as the reward and optimizes a logistic pairwise loss augmented with an SFT penalty term.",
            "system_type": "Automated Preference-Optimization Algorithm (contrastive + SFT regularizer)",
            "problem_domain": "Machine learning — LLM alignment; has demonstrated strong performance in machine translation settings.",
            "problem_description": "Improve quality of generated sequences from LLMs using contrastive likelihood objectives while preserving supervised behavior via an SFT penalty.",
            "problem_complexity": "Same large-scale model tuning complexity; objective trades off reward maximization and supervised fidelity; can encourage verbosity if not controlled (paper reports CPO generates longer outputs).",
            "data_availability": "Operates on offline preference pairs and SFT checkpoints; used the same UltraFeedback/UltraChat datasets in comparisons here.",
            "computational_requirements": "Comparable to other single-model objectives when SFT term is computed; experiment details use 8× H100 but exact per-method runtime not enumerated beyond SimPO vs. DPO comparisons.",
            "problem_structure": "Offline pairwise sequence ranking with explicit supervised regularization; clear metrics available for evaluation.",
            "success_metric": "AlpacaEval 2 LC/WR, Arena-Hard WR, downstream tasks.",
            "success_rate": "CPO improves over SFT in many settings; in some Arena-Hard runs CPO occasionally surpasses SimPO but tends to produce much longer outputs (paper notes ~50% longer average outputs than SimPO), suggesting length exploitation depending on evaluation.",
            "failure_modes": "Tendency to produce much longer responses (verbosity) when optimizing raw sequence likelihood without length normalization; may exploit length if SFT regularization is weak.",
            "success_factors": "Combining contrastive reward with SFT regularizer can balance quality and supervised fidelity; strong when tasks benefit from more verbose outputs (some Arena-Hard items).",
            "comparative_results": "CPO sometimes beats SimPO on Arena-Hard but at the cost of substantially longer generations; SimPO generally better balanced for length-controlled metrics.",
            "human_baseline": null,
            "uuid": "e2610.3",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RRHF",
            "name_full": "Ranked Responses with Human Feedback (RRHF)",
            "brief_description": "A ranking-based loss that uses length-normalized log-likelihood to prefer winning responses and includes an optional penalty promoting high-probability winning sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "RRHF",
            "system_description": "RRHF is a ranking objective which directly contrasts length-normalized log-likelihoods of winning and losing responses (max(0, -p(y_w)+p(y_l))) and optionally penalizes low-probability winning sequences; used as a baseline in experiments.",
            "system_type": "Automated Preference-Optimization Algorithm (ranking loss)",
            "problem_domain": "Machine learning — preference-based LLM alignment",
            "problem_description": "Rank pairs of responses to increase probability of winning responses while optionally applying a penalty to avoid low-probability outputs.",
            "problem_complexity": "Relatively simpler objective among preference methods; still operates on large models and sequence outputs; requires careful λ regularization tuning to avoid overfitting.",
            "data_availability": "Same offline preference datasets as other baselines.",
            "computational_requirements": "Single-model objective (reference-free) so similar compute to SimPO; specific compute numbers not provided.",
            "problem_structure": "Pairwise ranking with length normalization option; discrete, stochastic outputs with standard evaluation metrics.",
            "success_metric": "AlpacaEval 2 LC/WR, Arena-Hard WR.",
            "success_rate": "Improves over SFT but in these experiments underperforms SimPO and some other baselines. Example: Mistral-Base RRHF LC 11.6% vs SimPO 21.5% (LC).",
            "failure_modes": "Performance sensitive to λ regularizer; may not generalize as well as margin-based or likelihood-aligned objectives.",
            "success_factors": "Using length-normalized log-likelihood helps mitigate length bias; simple ranking losses are stable and easy to tune.",
            "comparative_results": "Better than SFT but generally below SimPO and some other modern baselines in the reported results.",
            "human_baseline": null,
            "uuid": "e2610.4",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "IPO",
            "name_full": "IPO (Implicit Preference Optimization / related variant cited as IPO)",
            "brief_description": "A theoretically-motivated preference optimization approach that avoids DPO's pointwise reward replacement with a pairwise-aware objective; includes a margin-like term; evaluated as a baseline here.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "IPO",
            "system_description": "IPO formulates a pairwise-aware squared-objective based on log ratios to the reference model (similar in spirit to adding an explicit margin), aiming to honor pairwise preference structure more faithfully than pointwise approximations. Included in empirical comparisons.",
            "system_type": "Automated Preference-Optimization Algorithm (pairwise-aware)",
            "problem_domain": "Machine learning — preference learning for LLMs",
            "problem_description": "Optimize policy parameters to match pairwise preference data while reducing approximations inherent in other reparameterizations.",
            "problem_complexity": "Comparable to DPO in complexity; requires reference model and careful hyperparameter τ settings.",
            "data_availability": "Uses same offline preference datasets and SFT initialization.",
            "computational_requirements": "Reference-based objective so similar memory/compute footprint to naive DPO.",
            "problem_structure": "Pairwise quadratic objective; discrete outputs; requires hyperparameter tuning.",
            "success_metric": "Win rates (AlpacaEval 2, Arena-Hard), MT-Bench.",
            "success_rate": "Improves over SFT but in reported experiments IPO underperforms SimPO and sometimes DPO depending on setting (e.g., Mistral-Base IPO LC 11.8% vs SimPO 21.5%).",
            "failure_modes": "Less effective than SimPO empirically in these experiments; sensitive to τ hyperparameter selection.",
            "success_factors": "Theoretical motivation to better match pairwise preference structure may help in some regimes; requires strong reference and tuning.",
            "comparative_results": "IPO did not outperform SimPO in the paper's reported empirical suite.",
            "human_baseline": null,
            "uuid": "e2610.5",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "KTO",
            "name_full": "KTO",
            "brief_description": "A method that learns from non-paired preference data by optimizing prospect-theoretic objectives and KL-related reference statistics; included as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "KTO",
            "system_description": "KTO is a preference-optimization approach capable of learning from non-paired preference data by leveraging reference-model-calibrated thresholds and KL-based statistics (z_ref) to decide winners/losers; tested as a baseline in the experimental comparisons.",
            "system_type": "Automated Preference-Optimization Algorithm (non-pairwise-aware / prospect-theoretic)",
            "problem_domain": "Machine learning — LLM alignment from preference-like signals",
            "problem_description": "Learn to align a policy model using weaker/non-paired preference signals and reference-model-derived thresholds to handle non-paired data.",
            "problem_complexity": "Handles more ambiguous supervision formats (non-paired), requiring estimation of reference-model expectations and KL terms; complexity increases with reference estimations.",
            "data_availability": "Works with preference or approximate-preference datasets; compared here on UltraFeedback-derived data.",
            "computational_requirements": "Uses reference-model computations and KL expectations; similar to other reference-based methods in compute profile.",
            "problem_structure": "Less structured supervision (non-paired), stochastic outputs, needs extra calibration statistics.",
            "success_metric": "Win rates and MT-Bench scores.",
            "success_rate": "Per Table 4, KTO improves over SFT and performs competitively in some settings (e.g., Mistral-Instruct KTO LC 24.5% vs SimPO 32.1%), but generally below SimPO in most reported setups.",
            "failure_modes": "Complexity from estimating z_ref and dependence on reference-model fidelity; may be sensitive to optimizer choices and reference quality.",
            "success_factors": "Can leverage weaker supervision formats and KL-based calibration to learn from non-paired data; benefits from strong reference models.",
            "comparative_results": "Competitive baseline in some setups but below SimPO overall in the paper's benchmarks.",
            "human_baseline": null,
            "uuid": "e2610.6",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RLHF / PPO",
            "name_full": "Reinforcement Learning from Human Feedback (classical RLHF) and PPO",
            "brief_description": "Classical RLHF is a multi-stage pipeline (train reward model from human preferences, then run RL policy optimization e.g., PPO to maximize reward) widely used for LLM alignment; PPO is a common RL optimizer used in the RLHF policy optimization stage.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "RLHF (classical pipeline) and PPO",
            "system_description": "RLHF comprises: (1) supervised fine-tuning, (2) reward model training from human preference labels, (3) policy optimization (commonly with PPO) to maximize the learned reward. PPO (Proximal Policy Optimization) is an on-policy RL algorithm used to update policies under reward signals while constraining update steps. In this paper RLHF/PPO are discussed as background and compared conceptually to offline preference objectives (DPO/SimPO), but PPO was not directly run in the reported experiments.",
            "system_type": "Automated Policy Optimization Framework (reinforcement learning pipeline)",
            "problem_domain": "Machine learning — LLM alignment via reinforcement learning",
            "problem_description": "Use human preference labels to train a reward model and then optimize a language model policy with RL to produce preferred outputs; challenges include complex multi-stage optimization, instability, and resource intensity.",
            "problem_complexity": "Multi-stage pipeline complexity: training a reward model (model capacity and overfitting risks), then RL policy optimization across huge action space (token sequences). On-policy sampling and iterative data collection further increase complexity. PPO/online RL requires many environment evaluations/samples; known to be resource intensive and difficult to tune.",
            "data_availability": "Requires paired human preference data for reward model training and potentially further online sampling; can be expensive to collect human labels.",
            "computational_requirements": "High: reward-model training plus RL optimization with many environment rollouts; historically large compute (OpenAI/DeepMind-scale runs) but exact numbers vary. Paper cites that offline alternatives like DPO and SimPO are simpler and more stable.",
            "problem_structure": "Open-ended, stochastic, and often on-policy; clear evaluation metrics but RL optimization can overfit reward model or produce degenerate policies if not regularized.",
            "success_metric": "Human-preference improvement, win rates against baselines, RL reward maximization, and downstream task performance.",
            "success_rate": "Classical RLHF (with PPO) has yielded strong results in prior work but is noted in this paper to have optimization challenges; direct numerical comparisons to PPO are deferred to future work and not provided here.",
            "failure_modes": "Multi-stage complexity leads to instability, reward-model overoptimization, and many practical challenges (tuning, sample inefficiency).",
            "success_factors": "Large, high-quality human preference datasets; careful regularization (KL penalties), good reward-model generalization, and tuned RL hyperparameters.",
            "comparative_results": "Paper positions offline methods (DPO, SimPO) as simpler and more stable alternatives to classical RLHF+PPO pipeline; direct empirical PPO-vs-SimPO comparisons were left to future work.",
            "human_baseline": null,
            "uuid": "e2610.7",
            "source_info": {
                "paper_title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2
        },
        {
            "paper_title": "ORPO: Monolithic preference optimization without reference model",
            "rating": 2
        },
        {
            "paper_title": "RRHF: Rank responses to align language models with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation",
            "rating": 2
        },
        {
            "paper_title": "A general theoretical paradigm to understand learning from human preferences",
            "rating": 2
        },
        {
            "paper_title": "KTO: Model alignment as prospect theoretic optimization",
            "rating": 2
        },
        {
            "paper_title": "Iterative DPO alignment",
            "rating": 1
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 1
        }
    ],
    "cost": 0.023504,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SimPO: Simple Preference Optimization with a Reference-Free Reward</h1>
<p>Yu Meng ${ }^{1 <em>}$ Mengzhou Xia ${ }^{2 </em>}$ Danqi Chen ${ }^{2}$<br>${ }^{1}$ Computer Science Department, University of Virginia<br>${ }^{2}$ Princeton Language and Intelligence (PLI), Princeton University yumeng5@virginia.edu<br>{mengzhou, danqic}@cs.princeton.edu</p>
<h4>Abstract</h4>
<p>Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its recent variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a $72.4 \%$ length-controlled win rate on AlpacaEval 2, a $59.1 \%$ win rate on Arena-Hard, and ranks 1st on Chatbot Arena among $&lt;10$ B models with real user votes. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Learning from human feedback is crucial in aligning large language models (LLMs) with human values and intentions [51], ensuring they are helpful, honest, and harmless [5]. Reinforcement learning from human feedback (RLHF) [18, 62, 73] is a popular method for fine-tuning language models to achieve effective alignment. While the classical RLHF approach [62, 70] has shown impressive results, it presents optimization challenges due to its multi-stage procedure, which involves training a reward model and then optimizing a policy model to maximize that reward [13].
Recently, researchers have been exploring simpler offline algorithms. Direct Preference Optimization (DPO) [66] is one such approach. DPO reparameterizes the reward function in RLHF to directly learn a policy model from preference data, eliminating the need for an explicit reward model. It has gained widespread practical adoption due to its simplicity and stability. In DPO, the implicit reward is formulated using the log ratio of the likelihood of a response between the current policy model and the supervised fine-tuned (SFT) model. However, this reward formulation is not directly aligned with</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SimPO and DPO mainly differ in their reward formulation, as indicated in the shaded box. SimPO outperforms DPO significantly across a range of settings on AlpacaEval 2 and Arena-Hard.</p>
<p>The metric used to guide generation, which is approximately the average log likelihood of a response generated by the policy model. We hypothesize that this discrepancy between training and inference may lead to suboptimal performance.</p>
<p>In this work, we propose SimPO, a simple yet effective offline preference optimization algorithm (Figure 1). The core of our algorithm aligns the reward function in the preference optimization objective with the generation metric. SimPO consists of two major components: (1) a length-normalized reward, calculated as the <em>average</em> log probability of all tokens in a response using the policy model, and (2) a target reward margin to ensure the reward difference between winning and losing responses exceeds this margin. In summary, SimPO has the following properties:</p>
<p>Table 1: Length-controlled (LC) and raw win rate (WR), and generation lengths of top models on the AlpacaEval 2 Leaderboard. Bold are the models we trained.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>LC (%)</th>
<th>WR (%)</th>
<th>Len.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gemma-2-9B-it-SimPO</td>
<td>72.4</td>
<td>65.9</td>
<td>1833</td>
</tr>
<tr>
<td>GPT-4 Turbo (04/09)</td>
<td>55.0</td>
<td>46.1</td>
<td>1802</td>
</tr>
<tr>
<td>Gemma-2-9B-it</td>
<td>51.1</td>
<td>38.1</td>
<td>1571</td>
</tr>
<tr>
<td>Llama-3-8B-Instruct-SimPO</td>
<td>44.7</td>
<td>40.5</td>
<td>1825</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>40.5</td>
<td>29.1</td>
<td>1388</td>
</tr>
<tr>
<td>Llama-3-8B-Instruct-DPO</td>
<td>40.3</td>
<td>37.9</td>
<td>1837</td>
</tr>
<tr>
<td>Llama-3-70B-Instruct</td>
<td>34.4</td>
<td>33.2</td>
<td>1919</td>
</tr>
<tr>
<td>Llama-3-8B-Instruct</td>
<td>26.0</td>
<td>25.3</td>
<td>1899</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Simplicity</strong>: SimPO does not require a reference model, making it more lightweight and easier to implement compared to DPO and other reference-based methods.</li>
<li><strong>Significant performance advantage</strong>: Despite its simplicity, SimPO significantly outperforms DPO and its latest variants (<em>e.g.</em>, a recent reference-free objective ORPO [42]). The performance advantage is consistent across various training setups and extensive chat-based evaluations, including AlpacaEval 2 [55, 28] and the challenging Arena-Hard [54] benchmark. It achieves up to a 6.4 point improvement on AlpacaEval 2 and a 7.5 point improvement on Arena-Hard compared to DPO (Figure 1).</li>
<li><strong>Minimal length exploitation</strong>: SimPO does not significantly increase response length compared to the SFT or DPO models (Table 1), indicating minimal length exploitation [28, 71, 85].</li>
</ul>
<p>Extensive analysis shows that SimPO utilizes preference data more effectively, leading to a more accurate likelihood ranking of winning and losing responses on a held-out validation set, which in turn translates to a better policy model. As shown in Table 1, our Gemma-2-9B-it-SimPO model achieves state-of-the-art performance, with a 72.4% length-controlled win rate on AlpacaEval 2 and a 59.1% win rate on Arena-Hard, establishing it as the strongest open-source model under 10B parameters. Most notably, when evaluated on Chatbot Arena [17] with real user votes, our model significantly improved upon the initial Gemma-2-9B-it model, advancing from 36th to 25th place and ranking first among all &lt;10B models on the leaderboard.^{2}</p>
<h2>2 SimPO: Simple Preference Optimization</h2>
<p>In this section, we first introduce the background of DPO (§2.1). Then we identify the discrepancy between DPO's reward and the likelihood metric used for generation, and propose an alternative reference-free reward formulation that mitigates this issue (§2.2). Finally, we derive the SimPO objective by incorporating a target reward margin term into the Bradley-Terry model (§2.3).</p>
<p>^{2}As of September 16th, 2024.</p>
<h1>2.1 Background: Direct Preference Optimization (DPO)</h1>
<p>DPO [66] is one of the most popular preference optimization methods. Instead of learning an explicit reward model [62], DPO reparameterizes the reward function $r$ using a closed-form expression with the optimal policy:</p>
<p>$$
r(x, y)=\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text {ref }}(y \mid x)}+\beta \log Z(x)
$$</p>
<p>where $\pi_{\theta}$ is the policy model, $\pi_{\text {ref }}$ is the reference policy, typically the supervised fine-tuned (SFT) model, and $Z(x)$ is the partition function. By incorporating this reward formulation into the BradleyTerry (BT) ranking objective [11], $p\left(y_{w} \succ y_{l} \mid x\right)=\sigma\left(r\left(x, y_{w}\right)-r\left(x, y_{l}\right)\right)$, DPO expresses the probability of preference data with the policy model rather than the reward model, yielding the following objective:</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\mathrm{DPO}}\left(\pi</em>} ; \pi_{\mathrm{ref}}\right)=-\mathbb{E<em w="w">{\left(x, y</em>\right)\right]
$$}, y_{l}\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\text {ref }}\left(y_{l} \mid x\right)</p>
<p>where $\left(x, y_{w}, y_{l}\right)$ are preference pairs consisting of the prompt, the winning response, and the losing response from the preference dataset $\mathcal{D}$.</p>
<h3>2.2 A Simple Reference-Free Reward Aligned with Generation</h3>
<p>Discrepancy between reward and generation for DPO. Using Eq. (1) as the implicit reward has the following drawbacks: (1) it requires a reference model $\pi_{\text {ref }}$ during training, which incurs additional memory and computational costs; and (2) it creates a mismatch between the reward optimized in training and the log-likelihood optimized during inference, where no reference model is involved. This means that in DPO, for any triple $\left(x, y_{w}, y_{l}\right)$, satisfying the reward ranking $r\left(x, y_{w}\right)&gt;r\left(x, y_{l}\right)$ does not necessarily mean that the likelihood ranking $p_{\theta}\left(y_{w} \mid x\right)&gt;p_{\theta}\left(y_{l} \mid x\right)$ is met (here $p_{\theta}$ is the average log-likelihood in Eq. (3)). In our experiments, we observed that only $\sim 50 \%$ of the triples from the training set satisfy this condition when trained with DPO (Figure 4b). This observation aligns with a concurrent work [14], which finds that existing models trained with DPO exhibit random ranking accuracy in terms of average log-likelihood, even after extensive preference optimization.</p>
<p>Length-normalized reward formulation. One solution is to use the summed token log probability as the reward, but this suffers from length bias-longer sequences tend to have lower log probabilities. Consequently, when $y_{w}$ is longer than $y_{l}$, optimizing the summed log probability as a reward forces the model to artificially inflate probabilities for longer sequences to ensure $y_{w}$ receives a higher reward than $y_{l}$. This overcompensation increases the risk of degeneration. To address this issue, we consider using the average log-likelihood as the implicit reward:</p>
<p>$$
p_{\theta}(y \mid x)=\frac{1}{|y|} \log \pi_{\theta}(y \mid x)=\frac{1}{|y|} \sum_{i=1}^{|y|} \log \pi_{\theta}\left(y_{i} \mid x, y_{&lt;i}\right)
$$</p>
<p>This metric is commonly used for ranking options in beam search [35, 53] and multiple-choice tasks within language models [12, 41, 62]. Naturally, we consider replacing the reward formulation in DPO with $p_{\theta}$ in Eq. (3), so that it aligns with the likelihood metric that guides generation. This results in a length-normalized reward:</p>
<p>$$
r_{\text {SimPO }}(x, y)=\frac{\beta}{|y|} \log \pi_{\theta}(y \mid x)=\frac{\beta}{|y|} \sum_{i=1}^{|y|} \log \pi_{\theta}\left(y_{i} \mid x, y_{&lt;i}\right)
$$</p>
<p>where $\beta$ is a constant that controls the scaling of the reward difference. We find that normalizing the reward with response lengths is crucial; removing the length normalization term from the reward formulation results in a bias toward generating longer but lower-quality sequences (see Section 4.4 for more details). Consequently, this reward formulation eliminates the need for a reference model, enhancing memory and computational efficiency compared to reference-dependent algorithms.</p>
<h3>2.3 The SimPO Objective</h3>
<p>Target reward margin. Additionally, we introduce a target reward margin term, $\gamma&gt;0$, to the Bradley-Terry objective to ensure that the reward for the winning response, $r\left(x, y_{w}\right)$, exceeds the</p>
<p>reward for the losing response, $r(x, y_{l})$, by at least $\gamma$ :</p>
<p>$p(y_{w} \succ y_{l} \mid x)=\sigma\left(r\left(x, y_{w}\right)-r\left(x, y_{l}\right)-\gamma\right)$.</p>
<p>The margin between two classes is known to influence the generalization capabilities of classifiers [1, $10,22,31] .{ }^{3}$ In standard training settings with random model initialization, increasing the target margin typically improves generalization. In preference optimization, the two classes are the winning and losing responses for a single input. In practice, we observe that generation quality initially improves with an increasing target margin but degrades when the margin becomes too large (§4.3). One of DPO's variants, IPO [6], also formulates a target reward margin similar to SimPO. However, its full objective is not as effective as SimPO (§4.1).</p>
<p>Objective. Finally, we obtain the SimPO objective by plugging Eq. (4) into Eq. (5):</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\text {SimPO }}\left(\pi</em>}\right)=-\mathbb{E<em w="w">{\left(x, y</em> \mid x\right)-\gamma\right)\right]
$$}, y_{l}\right) \sim \mathcal{D}}\left[\log \sigma\left(\frac{\beta}{\left|y_{w}\right|} \log \pi_{\theta}\left(y_{w} \mid x\right)-\frac{\beta}{\left|y_{l}\right|} \log \pi_{\theta}\left(y_{l</p>
<p>In summary, SimPO employs an implicit reward formulation that directly aligns with the generation metric, eliminating the need for a reference model. Additionally, it introduces a target reward margin $\gamma$ to help separating the winning and losing responses. In Appendix F, we provide a gradient analysis of SimPO and DPO to further understand the differences between the two methods.</p>
<p>Preventing catastrophic forgetting without KL regularization. Although SimPO does not impose KL regularization, we find that a combination of practical factors ensures effective learning from preference data while maintaining generalization, leading to an empirically low KL divergence from the reference model. These factors are: (1) a small learning rate, (2) a preference dataset that covers diverse domains and tasks, and (3) the intrinsic robustness of LLMs to learn from new data without forgetting prior knowledge. We present KL divergence experiments in Section 4.4.</p>
<h1>3 Experimental Setup</h1>
<p>Models and training settings. We perform preference optimization with two families of models, Llama-3-8B [2] and Mistral-7B [44], under two setups: Base and Instruct. In this section, our goal is to understand the performance of SimPO vs. other preference optimization methods in different experimental setups. Our strongest model is based on Gemma-2-9B (Instruct setup) with a stronger reward model, RLHFlow/ArmoRM-Llama3-8B-v0.1 [84] (Table 1). We will present and discuss these results in Appendix J.</p>
<p>For the Base setup, we follow the training pipeline of Zephyr [80]. First, we train a base model (i.e., mistralai/Mistral-7B-v0.1, or meta-llama/Meta-Llama-3-8B) on the UltraChat-200k dataset [25] to obtain an SFT model. Then, we perform preference optimization on the UltraFeedback dataset [23] using the SFT model as the starting point. This setup provides a high level of transparency, as the SFT models are trained on open-source data.</p>
<p>For the Instruct setup, we use an off-the-shelf instruction-tuned model (i.e., meta-llama/Meta-Llama-3-8B-Instruct, or mistralai/Mistral-7B-Instruct-v0.2) as the SFT models. ${ }^{4}$ These models have undergone extensive instruction-tuning processes, making them more powerful and robust than the SFT models in the Base setup. However, they are also more opaque because their RLHF procedure is not publicly disclosed. To mitigate the distribution shift between SFT models and the preference optimization process, we generate the preference dataset using the SFT models following [79]. This makes our Instruct setup closer to an on-policy setting. Specifically, we use prompts from the UltraFeedback dataset and regenerate the chosen and rejected response pairs $\left(y_{w}, y_{l}\right)$ with the SFT models. For each prompt $x$, we generate 5 responses using the SFT model with a sampling temperature of 0.8 . We then use llm-blender/PairRM [45] to score the 5 responses, selecting the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Evaluation details for AlpacaEval 2 [55], Arena-Hard [54], and MT-Bench [99]. The baseline model refers to the model compared against. GPT-4 Turbo corresponds to GPT-4-Preview-1106.</p>
<table>
<thead>
<tr>
<th></th>
<th># Exs.</th>
<th>Baseline Model</th>
<th>Judge Model</th>
<th>Scoring Type</th>
<th>Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A</td>
<td>805</td>
<td>GPT-4 Turbo</td>
<td>GPT-4 Turbo</td>
<td>Pairwise comparison</td>
<td>LC \&amp; raw win rate</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MT-Bench</td>
<td>500</td>
<td>GPT-4-0314</td>
<td>GPT-4 Turbo</td>
<td>Pairwise comparison</td>
<td>Win rate</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>highest-scoring one as $y_{w}$ and the lowest-scoring one as $y_{l}$. We only generated data in a single pass instead of iteratively as in [79]. ${ }^{5}$
In summary, we have four setups: Llama-3-Base, Llama-3-Instruct, Mistral-Base, and Mistral-Instruct. We believe these configurations represent the state-of-the-art, placing our models among the top performers on various leaderboards. We encourage future research to adopt these settings for better and fairer comparisons of different algorithms. Additionally, we find that tuning hyperparameters is crucial for achieving optimal performance with all the offline preference optimization algorithms, including DPO and SimPO. Generally, for SimPO, setting $\beta$ between 2.0 and 2.5 and $\gamma$ between 0.5 and 1.5 leads to good performance across all setups. For more details, please refer to Appendix B.</p>
<p>Evaluation benchmarks. We primarily assess our models using three of the most popular openended instruction-following benchmarks: MT-Bench [99], AlpacaEval 2 [55], and Arena-Hard v0.1 [54]. These benchmarks evaluate the models' versatile conversational abilities across a diverse set of queries and have been widely adopted by the community (details in Table 2). AlpacaEval 2 consists of 805 questions from 5 datasets, and MT-Bench covers 8 categories with 80 questions. The most recently released Arena-Hard is an enhanced version of an MT-Bench, incorporating 500 welldefined technical problem-solving queries. We report scores following each benchmark's evaluation protocol. For AlpacaEval 2, we report both the raw win rate (WR) and the length-controlled win rate (LC) [28]. The LC metric is specifically designed to be robust against model verbosity. For Arena-Hard, we report the win rate (WR) against the baseline model. For MT-Bench, we report the average MT-Bench score with GPT-4 and GPT-4-Preview-1106 as the judge model. ${ }^{6}$ For decoding details, please refer to Appendix B. We also evaluate on downstream tasks from the Huggingface Open Leaderboard benchmarks [9], with additional details in in Appendix C.</p>
<p>Baselines. We compare SimPO with other offline preference optimization methods listed in Table 3. ${ }^{7}$ RRHF [91] and SLiC-HF [96] are ranking losses. RRHF uses length-normalized log-likelihood, similar to SimPO's reward function, while SLiCHF uses log-likelihood directly and includes an SFT objective. IPO [6] is a theoretically grounded approach method that avoids DPO's assumption that pairwise preferences can be replaced with pointwise rewards. CPO [88] uses sequence likelihood as a reward and trains alongside an SFT objective. KTO [29] learns from non-paired preference data.</p>
<p>Table 3: Various preference optimization objectives given preference data $\mathcal{D}=\left(x, y_{w}, y_{l}\right)$, where $x$ is an input, and $y_{w}$ and $y_{l}$ are the winning and losing responses.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Objective</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RRHF [91]</td>
<td style="text-align: center;">$\max \left(0,-\frac{1}{</td>
</tr>
<tr>
<td style="text-align: center;">SLiC-HF [96]</td>
<td style="text-align: center;">$\max \left(0, \delta-\log \pi_{\theta}\left(y_{w} \mid x\right)+\log \pi_{\theta}(y_{l} \mid x)\right)-\lambda \log \pi_{\theta}\left(y_{w} \mid x\right)$</td>
</tr>
<tr>
<td style="text-align: center;">DPO [66]</td>
<td style="text-align: center;">$-\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text {ref }(y \mid x)}}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">IPO [6]</td>
<td style="text-align: center;">$\left(\log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-\log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text {ref }(y \mid x)}}-\frac{1}{2 \delta}\right)^{2}$</td>
</tr>
<tr>
<td style="text-align: center;">CPO [88]</td>
<td style="text-align: center;">$-\log \sigma\left(\beta \log \pi_{\theta}\left(y_{w} \mid x\right)-\beta \log \pi_{\theta}(y \mid x)\right)-\lambda \log \pi_{\theta}\left(y_{w} \mid x\right)$</td>
</tr>
<tr>
<td style="text-align: center;">KTO [29]</td>
<td style="text-align: center;">$-\lambda_{w} \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{l} \mid x\right)}-z_{\text {ref }}\right)+\lambda_{l} \sigma\left(z_{\text {ref }}-\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text {ref }(y \mid x)}}\right)$, <br> where $z_{\text {ref }}=\mathbb{E}<em _theta="\theta">{(x, y) \sim \mathcal{D}}\left[\beta \mathrm{KL}\left(\pi</em>(y \mid x)\right)\right]$}(y \mid x)\right) \mid \pi_{\text {ref }</td>
</tr>
<tr>
<td style="text-align: center;">ORPO [42]</td>
<td style="text-align: center;">$-\log p_{\theta}\left(y_{w} \mid x\right)-\lambda \log \sigma\left(\log \frac{p_{\theta}\left(y_{w} \mid x\right)}{\Gamma \cdot p_{\theta}\left(y_{w} \mid x\right)}-\log \frac{p_{\theta}(y \mid x)}{\Gamma \cdot p_{\theta}(y \mid x)}\right)$, <br> where $p_{\theta}(y \mid x)=\exp \left(\frac{1}{</td>
</tr>
<tr>
<td style="text-align: center;">R-DPO [64]</td>
<td style="text-align: center;">$-\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text {ref }}(y \mid x)}+\left(\alpha\left</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">$-\log \sigma\left(\frac{\beta}{\left</td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: AlpacaEval 2 [55], Arena-Hard [54], and MT-Bench [99] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Mistral-Base (7B)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mistral-Instruct (7B)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlpacaEval 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arena-Hard</td>
<td style="text-align: center;">MT-Bench</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlpacaEval 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arena-Hard</td>
<td style="text-align: center;">MT-Bench</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LC (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">LC (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">GPT-4</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: center;">RRHF [91]</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: center;">SLiC-HF [96]</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: center;">DPO [66]</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: center;">IPO [6]</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: center;">CPO [88]</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: center;">KTO [29]</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: center;">ORPO [42]</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: center;">R-DPO [64]</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Llama-3-Base (8B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-3-Instruct (8B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlpacaEval 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arena-Hard</td>
<td style="text-align: center;">MT-Bench</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlpacaEval 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arena-Hard</td>
<td style="text-align: center;">MT-Bench</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LC (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">LC (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">GPT-4</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">8.1</td>
</tr>
<tr>
<td style="text-align: center;">RRHF [91]</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">7.9</td>
</tr>
<tr>
<td style="text-align: center;">SLiC-HF [96]</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">8.1</td>
</tr>
<tr>
<td style="text-align: center;">DPO [66]</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">IPO [6]</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8.3</td>
</tr>
<tr>
<td style="text-align: center;">CPO [88]</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">KTO [29]</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">8.2</td>
</tr>
<tr>
<td style="text-align: center;">ORPO [42]</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">R-DPO [64]</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8.0</td>
</tr>
</tbody>
</table>
<p>ORPO [42] ${ }^{8}$ introduces a reference-model-free odd ratio term to directly contrast winning and losing responses with the policy model and jointly trains with the SFT objective. R-DPO [64] is a modified version of DPO that includes an additional regularization term to prevent exploitation of length. We thoroughly tune the hyperparameters for each baseline and report the best performance. We find that many variants of DPO do not empirically present an advantage over standard DPO. Further details can be found in Appendix B.</p>
<h1>4 Experimental Results</h1>
<p>In this section, we present main results of our experiments, highlighting the superior performance of SimPO on various benchmarks and ablation studies (§4.1). We provide an in-depth understanding of the following components: (1) length normalization (§4.2), (2) the margin term $\gamma$ (§4.3), and (3) why SimPO outperforms DPO (§4.4). Unless otherwise specified, the ablation studies are conducted using the Mistral-Base setting.</p>
<h3>4.1 Main Results and Ablations</h3>
<p>SimPO consistently and significantly outperforms existing preference optimization methods. As shown in Table 4, while all preference optimization algorithms enhance performance over the SFT model, SimPO, despite its simplicity, achieves the best overall performance across all benchmarks and settings. These consistent and significant improvements highlight the robustness and effectiveness of SimPO. Notably, SimPO outperforms the best baseline by 3.6 to 4.8 points on the AlpacaEval 2 LC win rate across various settings. On Arena-Hard, SimPO consistently achieves superior performance,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Ablation studies under Mistral-Base and Mistral-Instruct settings. We ablate each key design of SimPO: (1) removing length normalization in Eq. (4) (i.e., w/o LN); (2) setting target reward margin $\gamma$ to be 0 in Eq. (6) (i.e., $\gamma=0$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Mistral-Base (7B) Setting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mistral-Instruct (7B) Setting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlpacaEval 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arena-Hard</td>
<td style="text-align: center;">MT-Bench</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlpacaEval 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Arena-Hard</td>
<td style="text-align: center;">MT-Bench</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LC (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">LC (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">WR (\%)</td>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">GPT-4</td>
</tr>
<tr>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: center;">w/o LN</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma=0$</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">7.7</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Effect of length normalization (LN). (a) Relationship between reward margin and length difference between winning and losing responses. (b) Spearman correlation between average log probability and response length for SimPO. (c) Spearman correlation for SimPO without LN.
though it is occasionally surpassed by CPO [88]. We find that CPO generates responses that are, on average, $50 \%$ longer than those generated by SimPO (See Table 10). Arena-Hard might favor longer generations due to the absence of a length penalty in its evaluation.</p>
<p>Benchmark quality varies. Although all three benchmarks are widely adopted, we find that MTBench exhibits poor separability across different methods. Minor differences between methods on MT-Bench may be attributed to randomness, likely due to the limited scale of its evaluation data and its single-instance scoring protocol. This finding aligns with observations reported in [54]. In contrast, AlpacaEval 2 and Arena-Hard provide more meaningful distinctions between different methods. We observe that the win rate on Arena-Hard is significantly lower than on AlpacaEval 2, indicating that Arena-Hard is a more challenging benchmark. ${ }^{9}$</p>
<p>The Instruct setting introduces significant performance gains. Across all benchmarks, we observe that the Instruct setting consistently outperforms the Base setting. This improvement is likely due to the higher quality of SFT models used for initialization and the generation of more high-quality preference data by these models.</p>
<p>Both key designs in SimPO are crucial. In Table 5, we demonstrate results from ablating each key design of SimPO: (1) removing length normalization in Eq. (4) (i.e., w/o LN); (2) setting the target reward margin to be 0 in Eq. (6) (i.e., $\gamma=0$ ). Removing the length normalization has the most negative impact on the results. Our examination reveals that this leads to the generation of long and repetitive patterns, substantially degrading the overall quality of the output (See Appendix E). Setting $\gamma$ to 0 yields also leads to a performance degradation compared to SimPO, indicating that it is not the optimal target reward margin. In the following subsections, we conduct in-depth analyses to better understand both design choices.</p>
<h1>4.2 Length Normalization (LN) Prevents Length Exploitation</h1>
<p>LN leads to an increase in the reward difference for all preference pairs, regardless of their length. The Bradley-Terry objective in Eq. (5) essentially aims to optimize the reward difference</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Study of the margin $\gamma$. (a) Reward accuracy and AlpacaEval2 LC win rate under different $\gamma$ values. (b) Reward difference distribution under different $\gamma$ values. (c) Log likelihood distribution on chosen responses under different $\gamma$ values.
$\Delta r=r\left(x, y_{w}\right)-r\left(x, y_{l}\right)$ to exceed the target margin $\gamma$. We investigate the relationship between the learned reward differences and the length difference $\Delta l=\left|y_{w}\right|-\left|y_{l}\right|$ between the winning and losing responses from the training set of UltraFeedback. We measure the difference of reward ( $r_{\text {SimPO }}$; Eq. (4)) using the SFT model, the SimPO model, and a model trained with SimPO but without length normalization. We present the results in Figure 2a and observe that SimPO with LN consistently achieves a positive reward margin for all response pairs, regardless of their length difference, and consistently improves the margin over the SFT model. In contrast, SimPO without LN results in a negative reward difference for preference pairs when the winning response is shorter than the losing response, indicating that the model learns poorly for these instances.</p>
<p>Removing LN results in a strong positive correlation between the reward and response length, leading to length exploitation. Figures 2b and 2c illustrate the average log likelihood ( $p_{\theta}$ in Eq. (3)) versus response length on a held-out set for models trained with SimPO and SimPO without LN. The model trained without LN exhibits a much stronger positive Spearman correlation between likelihood and response length compared to SimPO, indicating a tendency to exploit length bias and generate longer sequences (see Appendix E). In contrast, SimPO results in a Spearman correlation coefficient similar to the SFT model (see Figure 6a).</p>
<h1>4.3 The Impact of Target Reward Margin in SimPO</h1>
<p>Influence of $\gamma$ on reward accuracy and win rate. We investigate how the target reward margin $\gamma$ in SimPO affects the reward accuracy on a held-out set and win rate on AlpacaEval 2, presenting the results in Figure 3a. Reward accuracy is measured as the percentage of preference pairs where the winning response ends up having a higher reward for the winning response than the losing response (i.e., $r\left(x, y_{w}\right)&gt;r\left(x, y_{l}\right)$ ). We observe that reward accuracy increases with $\gamma$ on both benchmarks, indicating that enforcing a larger target reward margin effectively improves reward accuracy. However, the win rate on AlpacaEval 2 first increases and then decreases with $\gamma$, suggesting that generation quality is not solely determined by the reward margin.</p>
<p>Impact of $\gamma$ on the reward distribution. We visualize the distribution of the learned reward margin $r\left(x, y_{w}\right)-r\left(x, y_{l}\right)$ and the reward of winning responses $r\left(x, y_{w}\right)$ under varying $\gamma$ values in Figure 2b and Figure 2c. Notably, increasing $\gamma$ tends to flatten both distributions and reduce the average log likelihood of winning sequences. This initially improves performance but can eventually lead to model degeneration. We hypothesize that there is a trade-off between accurately approximating the true reward distribution and maintaining a well-calibrated likelihood when setting the $\gamma$ value. Further exploration of this balance is deferred to future work.</p>
<h3>4.4 In-Depth Analysis of DPO vs. SimPO</h3>
<p>In this section, we compare SimPO to DPO in terms of (1) likelihood-length correlation, (2) reward formulation, (3) reward accuracy, and (4) algorithm efficiency. We demonstrate that SimPO outperforms DPO in terms of reward accuracy and efficiency.</p>
<p>DPO reward implicitly facilitates length normalization. Although the DPO reward expression $r(x, y)=\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\text {oI }}(y \mid x)}$ (with the partition function excluded) lacks an explicit term for length normalization, the logarithmic ratio between the policy model and the reference model can serve to</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison between SimPO and DPO, measured on UltraFeedback. (a) Spearman correlation between average log probability and response length for DPO. (b) Contingency table of rankings based on DPO rewards and the average log likelihood (measured on the training set). (c) Reward accuracy of DPO and SimPO.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison between SimPO and DPO (continued). (a) With different β in DPO and SimPO, KL divergence from the policy model to the reference model on yw. (b) AlpacaEval2 LC win rate of DPO and SimPO with different β. (c) Runtime and memory usage for DPO and SimPO.</p>
<p>implicitly counteract length bias. As shown in Table 6 and Figure 4a, employing DPO reduces the Spearman correlation coefficient between average log likelihood and response length compared to the approach without any length normalization (referred to as "SimPO w/o LN"). However, it still exhibits a stronger positive correlation when compared to SimPO.10</p>
<p><strong>DPO reward mismatches generation likelihood.</strong> There is a divergence between DPO's reward formulation, rθ(x, y) = β logπθ′(y|x), and the average log likelihood metric, pθ′(y | x) = 1/|y| logπθ′(y | x), which directly impacts generation. As shown in Figure 4b, among the instances on the UltraFeedback training set where rθ(x, yw) &gt; rθ(x, yl), almost half of the pairs have pθ′(yw | x) &gt; pθ′(y | x). In contrast, SimPO directly employs the average log likelihood (scaled by β) as the reward expression, thereby eliminating the discrepancy completely, as demonstrated in Figure 6b.</p>
<p><strong>Table 6:</strong> Spearman correlation ρ between average log likelihood of different models and response length on a held-out set.</p>
<table>
<thead>
<tr>
<th></th>
<th>SimPO w/o LN</th>
<th>DPO</th>
<th>SimPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>ρ</td>
<td>0.82</td>
<td>0.59</td>
<td>0.34</td>
</tr>
</tbody>
</table>
<p><strong>DPO lags behind SimPO in terms of reward accuracy.</strong> In Figure 4c, we compare the reward accuracy of SimPO and DPO, assessing how well their final learned rewards align with preference labels on a held-out set. SimPO consistently achieves higher reward accuracy than DPO, suggesting that our reward design facilitates better generalization and leads to higher quality generations.</p>
<p><strong>KL divergence of SimPO and DPO.</strong> In Figure 5a, we present the KL divergence between the policy model trained with DPO and SimPO and the reference model with different β, measured on the winning responses from a held-out set during training. Figure 5b shows the corresponding AlpacaEval 2 LC win rate. Although SimPO does not apply any form of regularization against the reference model, the KL divergence of SimPO is reasonably small. Increasing β reduces the KL divergence for both DPO and SimPO, with DPO exhibiting a more pronounced reduction at higher β values. In this particular setting (Mistral-base), Figure 5b demonstrates that a smaller β can</p>
<p>10Note that this correlation does not fully reflect the generation length. Despite DPO showing a stronger correlation, the length of its generated responses is comparable to or even slightly shorter than those of the SimPO models. Please find more details in Appendix E.</p>
<p>improve AlpacaEval 2 performance, despite the higher KL divergence. ${ }^{11}$ We hypothesize that when the reference model is weak, strictly constraining the policy model to the reference model may not be beneficial. As a caveat, while we did not observe any training collapse or degeneration with proper tuning, in principle, SimPO could potentially lead to reward hacking without explicit regularization against the reference model. In such a scenario, the model might achieve a low loss but degenerate.</p>
<p>SimPO is more memory and compute-efficient than DPO. Another benefit of SimPO is its efficiency as it does not use a reference model. Figure 5c illustrates the overall run time and per-GPU peak memory usage of SimPO and DPO in the Llama-3-Base setting using $8 \times \mathrm{H} 100$ GPUs. Compared to a vanilla DPO implementation, ${ }^{12}$ SimPO cuts run time by roughly $20 \%$ and reduces GPU memory usage by about $10 \%$, thanks to eliminating forward passes with the reference model.</p>
<h1>5 Related Work</h1>
<p>Reinforcement learning from human feedback. RLHF is a technique that aligns large language models with human preferences and values [18, 102, 62, 7]. The classical RLHF pipeline typically comprises three phases: supervised fine-tuning [101, 76, 33, 21, 48, 25, 82, 15, 86], reward model training [32, 60, 16, 56, 37, 50], and policy optimization [70, 4]. Proximal Policy Optimization (PPO) [70] is a widely used algorithm in the third stage of RLHF. The RLHF framework is also widely applied to various applications, such as mitigating toxicity [3, 49, 97], ensuring safety [24], enhancing helpfulness [78, 83], searching and navigating the web [61], and improving model reasoning abilities [36]. Recently, [13] has highlighted challenges across the whole RLHF pipeline from preference data collection to model training. Further research has also demonstrated that RLHF can lead to biased outcomes, such as verbose outputs from the model [28, 71, 85].</p>
<p>Offline vs. iterative preference optimization. Given that online preference optimization algorithms are complex and difficult to optimize [100, 69], researchers have been exploring more efficient and simpler alternative offline algorithms. Direct Preference Optimization (DPO) [66] is a notable example. However, the absence of an explicit reward model in DPO limits its ability to sample preference pairs from the optimal policy. To address this, researchers have explored augmenting preference data using a trained SFT policy [96] or a refined SFT policy with rejection sampling [59], enabling the policy to learn from data generated by the optimal policy. Further studies have extended this approach to an iterative training setup, by continuously updating the reference model with the most recent policy model or generating new preference pairs at each iteration [27, 46, 67, 87, 92]. In this work, we focus exclusively on offline settings, avoiding any iterative training processes.</p>
<p>Preference optimization objectives. A variety of preference optimization objectives have been proposed besides DPO. Ranking objectives allow for comparisons among more than two instances [26, 58, 72, 91]. Another line of work explores simpler preference optimization objectives that do not rely on a reference model [42, 89], similar to SimPO. [8] proposes a method to jointly optimize instructions and responses, finding it effectively improves DPO. [98] focuses on post-training extrapolation between the SFT and the aligned model to further enhance model performance. In this work, we compare SimPO to a series of offline algorithms, including RRHF [91], SLiC-HF [96], DPO [66], IPO [6], CPO [88], KTO [29], ORPO [42], and R-DPO [64], and find that SimPO can outperform them in both efficiency and performance. Recently, [75] proposed a generalized preference optimization framework unifying different offline algorithms, and SimPO can be seen as a special case.</p>
<h2>6 Conclusion</h2>
<p>In this work, we propose SimPO, a simple and effective preference optimization algorithm that consistently outperforms existing approaches across various training setups. By aligning the reward function with the generation likelihood and introducing a target reward margin, SimPO eliminates the need for a reference model and achieves strong performance without exploiting the length bias. Extensive analysis demonstrates that the key designs in SimPO are crucial and validates the efficiency and effectiveness of SimPO. A detailed discussion of the limitations can be found in Appendix A.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Acknowledgments</h1>
<p>The authors would like to thank Li Dong, Tianyu Gao, Tanya Goyal, Di Jin, Yuchen Lin, Kaifeng Lyu, Sadhika Malladi, Eric Mitchell, Lewis Tunstall, Haoxiang Wang, Wei Xiong, Zhen Xu, Libing Yang, Zhiyu Zhao, and members of the Princeton NLP group for their valuable feedback and discussions. We thank Niklas Muennighoff for his advice on training and reproducing training KTO models. We thank Haoran Xu for helping verify our CPO runs. Mengzhou Xia is supported by an Apple Scholars in AIML Fellowship. This research is also funded by the National Science Foundation (IIS-2211779) and a Sloan Research Fellowship.</p>
<h2>References</h2>
<p>[1] Alan Agresti. Categorical data analysis, volume 792. John Wiley \&amp; Sons, 2012.
[2] AI@Meta. Llama 3 model card. 2024.
[3] Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset. arXiv preprint arXiv:2402.10571, 2024.
[4] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017.
[5] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, John Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. ArXiv, abs/2112.00861, 2021.
[6] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences. ArXiv, abs/2310.12036, 2023.
[7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
[8] Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, and Aditya Grover. Comparing bad apples to good oranges: Aligning large language models via joint preference optimization. arXiv preprint arXiv:2404.00530, 2024.
[9] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open LLM leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_ leaderboard, 2023.
[10] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory, pages 144-152, 1992.
[11] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39:324, 1952.
[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.
[13] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.</p>
<p>[14] Angelica Chen, Sadhika Malladi, Lily H Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, and Kyunghyun Cho. Preference learning algorithms do not learn preference rankings. NeurIPS, 2024.
[15] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. AlpaGasus: Training a better Alpaca with fewer data. In ICLR, 2024.
[16] Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. ODIN: Disentangled reward mitigates hacking in RLHF. arXiv preprint arXiv:2402.07319, 2024.
[17] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.
[18] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.
[19] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. ArXiv, abs/1803.05457, 2018.
[20] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[21] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned LLM, 2023.
[22] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20:273-297, 1995.
[23] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. In ICML, 2024.
[24] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023.
[25] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP, 2023.
[26] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, SHUM KaShun, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023.
[27] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024.
[28] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled AlpacaEval: A simple way to debias automatic evaluators. ArXiv, abs/2404.04475, 2024.
[29] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: Model alignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024.</p>
<p>[30] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, 2018.
[31] David Firth and Heather Turner. Bradley-terry models in R: the BradleyTerry2 package. Journal of Statistical Software, 48(9), 2012.
[32] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835-10866. PMLR, 2023.
[33] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April, 1:6, 2023.
[34] Ulrich Germann. Greedy decoding for statistical machine translation in almost linear time. In NAACL, 2003.
[35] Alex Graves. Sequence transduction with recurrent neural networks. ArXiv, abs/1211.3711, 2012.
[36] Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane DwivediYu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.
[37] Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Railneau. GLoRe: When, where, and how to improve LLM reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024.
[38] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.
[39] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2019.
[40] Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. Learning to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1638-1649, 2018.
[41] Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038-7051, 2021.
[42] Jiwoo Hong, Noah Lee, and James Thorne. ORPO: Monolithic preference optimization without reference model. ArXiv, abs/2403.07691, 2024.
[43] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. BeaverTails: Towards improved safety alignment of LLM via a human-preference dataset. ArXiv, abs/2307.04657, 2023.
[44] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B. ArXiv, abs/2310.06825, 2023.
[45] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LLM-Blender: Ensembling large language models with pairwise ranking and generative fusion. In $A C L, 2023$.
[46] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. sDPO: Don’t use your data all at once. ArXiv, abs/2403.19270, 2024.</p>
<p>[47] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[48] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Minh Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.
[49] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506-17533. PMLR, 2023.
[50] Nathan Lambert, Valentina Pyatkin, Jacob Daniel Morrison, Lester James Validad Miranda, Bill Yuchen Lin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hanna Hajishirzi. RewardBench: Evaluating reward models for language modeling. ArXiv, abs/2403.13787, 2024.
[51] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.
[52] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.
[53] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192-1202, Austin, Texas, November 2016. Association for Computational Linguistics.
[54] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The Arena-Hard pipeline, April 2024.
[55] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instructionfollowing models. https://github.com/tatsu-lab/alpaca_eval, 2023.
[56] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.
[57] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In ACL, pages 3214-3252, 2022.
[58] Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, et al. LiPO: Listwise preference optimization through learning-to-rank. arXiv preprint arXiv:2402.01878, 2024.
[59] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations, 2024.
[60] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.
[61] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>[62] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022.
[63] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024.
[64] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. ArXiv, abs/2403.19159, 2024.
[65] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[66] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In NeurIPS, 2023.
[67] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. ArXiv, abs/2404.03715, 2024.
[68] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.
[69] Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. Efficient RLHF: Reducing the memory usage of PPO. arXiv preprint arXiv:2309.00754, 2023.
[70] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[71] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in RLHF. arXiv preprint arXiv:2310.03716, 2023.
[72] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. In AAAI, 2024.
[73] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.
[74] Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Rémi Munos, Bernardo Ávila Pires, Michal Valko, Yong Cheng, et al. Understanding the performance gap between online and offline alignment algorithms. arXiv preprint arXiv:2405.08448, 2024.
[75] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024.
[76] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
[77] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.</p>
<p>[78] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. Finetuning language models for factuality. In The Twelfth International Conference on Learning Representations, 2024.
[79] Hoang Tran, Chris Glaze, and Braden Hancock. Iterative DPO alignment. Technical report, Snorkel AI, 2023.
[80] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of LM alignment. ArXiv, abs/2310.16944, 2023.
[81] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zi-Han Lin, Yuk-Kit Cheng, Sanmi Koyejo, Dawn Xiaodong Song, and Bo Li. DecodingTrust: A comprehensive assessment of trustworthiness in GPT models. In NeurIPS, 2023.
[82] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. OpenChat: Advancing open-source language models with mixed-quality data. In ICLR, 2024.
[83] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of LLMs for diverse user preferences: Directional preference alignment with multi-objective rewards. ArXiv, abs/2402.18571, 2024.
[84] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024.
[85] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.
[86] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting influential data for targeted instruction tuning. In ICML, 2024.
[87] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International Conference on Machine Learning, 2024.
[88] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. ArXiv, abs/2401.08417, 2024.
[89] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023.
[90] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is DPO superior to PPO for LLM alignment? a comprehensive study. arXiv preprint arXiv:2404.10719, 2024.
[91] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to align language models with human feedback. In NeurIPS, 2023.
[92] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.
[93] Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. WildBench: Benchmarking LLMs with challenging tasks from real users in the wild. arXiv e-prints, pages arXiv-2406, 2024.</p>
<p>[94] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics.
[95] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1 m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024.
[96] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. SLiC-HF: Sequence likelihood calibration with human feedback. ArXiv, abs/2305.10425, 2023.
[97] Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of ACL, 2023.
[98] Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, and Nanyun Peng. Weak-to-strong extrapolation expedites alignment. arXiv preprint arXiv:2404.16792, 2024.
[99] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS Datasets and Benchmarks Track, 2023.
[100] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of RLHF in large language models part I: PPO. arXiv preprint arXiv:2307.04964, 2023.
[101] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. NeurIPS, 2023.
[102] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>A Limitations</h1>
<p>More in-depth theoretical analysis. Despite the empirical success and intuitive motivation of SimPO, a more rigorous theoretical analysis is necessary to fully understand the factors contributing to its effectiveness. Additionally, we introduce an additional hyperparameter, the target reward margin, which requires manual tuning. Future work could explore how to determine the optimal margin automatically and provide a more theoretical understanding of SimPO.
Safety and honesty. SimPO is designed to optimize the generation quality of language models by pushing the margin between the average log likelihood of the winning response and the losing response to exceed a target reward margin. However, it does not explicitly consider safety and honesty aspects, which are crucial for real-world applications. Future work should explore integrating safety and honesty constraints into SimPO to ensure that the generated responses are not only high-quality but also safe and honest. The dataset used in this work, UltraFeedback [23], primarily focuses on helpfulness, and future research may consider a more comprehensive study utilizing larger-scale preference datasets [43, 95] and evaluation benchmarks [81] that place a strong emphasis on safety aspects. Nonetheless, we observe that this method consistently achieves high TruthfulQA [57] performance compared to other objectives in Table 9, suggesting its potential for safety alignment.
Performance drop on math. We observed that preference optimization algorithms generally decrease downstream task performance, particularly on reasoning-heavy tasks like GSM8k, as shown in Table 9. SimPO occasionally results in performance comparable to or worse than DPO. We hypothesize that this may be related to the choice of training datasets, hyperparameters used for training, or a mismatch of chat templates used for downstream task evaluations. One explanation is that the preference optimization objective may not be effectively increasing the likelihood of preferred sequences despite increasing the reward margin. [63] first observed this phenomenon and point out that this can hinder learning from math preference pairs where changing one token can flip the label (e.g., changing $2+2=4$ to $2+2=5$ ). They propose a simple regularization strategy to add back a reference-model calibrated supervised fine-tuning loss to the preference optimization objective, and effectively mitigate this issue. Future work may consider integrating this regularization strategy into SimPO to improve performance on reasoning-heavy tasks.</p>
<h2>B Implementation Details</h2>
<p>We find that hyperparameter tuning is crucial for achieving optimal performance of preference optimization methods. However, the importance of careful hyperparameter tuning may have been underestimated in prior research, potentially leading to suboptimal baseline results. To ensure a fair comparison, we conduct thorough hyperparameter tuning for all methods compared in our experiments.</p>
<p>General training hyperparameters. For the Base training setups, we train SFT models using the UltraChat-200k dataset [25] with the following hyperparameters: a learning rate of $2 \mathrm{e}-5$, a batch size of 128, a max sequence length of 2048, and a cosine learning rate schedule with $10 \%$ warmup steps for 1 epoch. All the models are trained with an Adam optimizer [47].</p>
<p>Table 8: The hyperparameter values in SimPO used for each training setting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: center;">$\beta$</th>
<th style="text-align: center;">$\gamma$</th>
<th style="text-align: center;">Learning rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mistral-Base</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">$3 \mathrm{e}-7$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-Instruct</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">$5 \mathrm{e}-7$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3-Base</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">$6 \mathrm{e}-7$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3-Instruct</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
</tr>
</tbody>
</table>
<p>For the preference optimization stage, we conduct preliminary experiments to search for batch sizes in [32, 64, 128] and training epochs in [1, 2, 3]. We find that a batch size of 128 and a single training epoch generally yield the best results across all methods. Therefore, we fix these values for all preference optimization experiments. Additionally, we set the max sequence length to be 2048 and apply a cosine learning rate schedule with $10 \%$ warmup steps on the preference optimization dataset.</p>
<p>Method-specific training hyperparameters. We have noticed that the optimal learning rate varies for different preference optimization methods and greatly influences the benchmark performance. Therefore, we individually search the learning rates in the range of [3e-7, 5e-7, 6e-7, 1e-6] for each</p>
<p>Table 7: Various preference optimization objectives and hyperparameter search range.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Objective</th>
<th>Hyperparameter</th>
</tr>
</thead>
<tbody>
<tr>
<td>RRHF [91]</td>
<td>$\max \left(0,-\frac{1}{\left|y_{w}\right|} \log \pi_{\theta}\left(y_{w} \mid x\right)+\frac{1}{\left|y_{l}\right|} \log \pi_{\theta}\left(y_{l} \mid x\right)\right)-\lambda \log \pi_{\theta}\left(y_{w} \mid x\right)$</td>
<td>$\lambda \in[0.1,0.5,1.0,10.0]$</td>
</tr>
<tr>
<td>SLiC-HF [96]</td>
<td>$\max \left(0, \delta-\log \pi_{\theta}\left(y_{w} \mid x\right)+\log \pi_{\theta}\left(y_{l} \mid x\right)\right)-\lambda \log \pi_{\theta}\left(y_{w} \mid x\right)$</td>
<td>$\begin{aligned} &amp; \lambda \in[0.1,0.5,1.0,10.0] \ &amp; \beta \in[0.1,0.5,1.0,2.0] \end{aligned}$</td>
</tr>
<tr>
<td>DPO [66]</td>
<td>$-\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\text {ref }}\left(y_{l} \mid x\right)}\right)$</td>
<td>$\beta \in[0.01,0.05,0.1]$</td>
</tr>
<tr>
<td>IPO [6]</td>
<td>$\left(\log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-\log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\text {ref }}\left(y_{l} \mid x\right)}-\frac{1}{2 \tau}\right)^{2}$</td>
<td>$\tau \in[0.01,0.1,0.5,1.0]$</td>
</tr>
<tr>
<td>CPO [88]</td>
<td>$-\log \sigma\left(\beta \log \pi_{\theta}\left(y_{w} \mid x\right)-\beta \log \pi_{\theta}\left(y_{l} \mid x\right)\right)-\lambda \log \pi_{\theta}\left(y_{w} \mid x\right)$</td>
<td>$\lambda=1.0, \beta \in[0.01,0.05,0.1]$</td>
</tr>
<tr>
<td>KTO [29]</td>
<td>$-\lambda_{w} \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-z_{\text {ref }}\right)+\lambda_{l} \sigma\left(z_{\text {ref }}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\text {ref }}\left(y_{l} \mid x\right)}\right)$, <br> where $z_{\text {ref }}=\mathbb{E}<em _theta="\theta">{(x, y) \sim \mathcal{D}}\left[\beta \mathrm{KL}\left(\pi</em>(y \mid x)\right)\right]$}(y \mid x) \mid \mid \pi_{\text {ref }</td>
<td>$\lambda_{l}=\lambda_{w}=1.0$ <br> $\beta \in[0.01,0.05,0.1]$</td>
</tr>
<tr>
<td>ORPO [42]</td>
<td>$-\log p_{\theta}\left(y_{w} \mid x\right)-\lambda \log \sigma\left(\log \frac{p_{\theta}\left(y_{w} \mid x\right)}{1-p_{\theta}\left(y_{w} \mid x\right)}-\log \frac{p_{\theta}\left(y_{l} \mid x\right)}{1-p_{\theta}\left(y_{l} \mid x\right)}\right)$, <br> where $p_{\theta}(y \mid x)=\exp \left(\frac{1}{\left|y_{l}\right|} \log \pi_{\theta}(y \mid x)\right)$</td>
<td>$\lambda \in[0.1,0.5,1.0,2.0]$</td>
</tr>
<tr>
<td>R-DPO [64]</td>
<td>$-\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\text {ref }}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\text {ref }}\left(y_{l} \mid x\right)}+\left(\alpha\left</td>
<td>y_{w}\right</td>
</tr>
<tr>
<td>SimPO</td>
<td>$-\log \sigma\left(\frac{\beta}{\left|y_{w}\right|} \log \pi_{\theta}\left(y_{w} \mid x\right)-\frac{\beta}{\left|y_{l}\right|} \log \pi_{\theta}\left(y_{l} \mid x\right)-\gamma\right)$</td>
<td>$\begin{aligned} &amp; \beta \in[2.0,2.5] \ &amp; \gamma \in[0.3,0.5,1.0,1.2,1.4,1.6] \end{aligned}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (a) Likelihood-length correlation plot for Mistral-SFT fine-tuned on UltraChat-200k. (a) Contingency table rankings based on SimPO rewards and the average log likelihood (measured on the training set).
method. Table 7 shows the detailed information on method-specific hyperparameters search ranges for baselines. ${ }^{13}$ Table 8 shows SimPO's hyperparameters used under each setting.</p>
<p>Decoding hyperparameters. For AlpacaEval 2, we use a sampling decoding strategy to generate responses, with a temperature of 0.7 for the Mistral-Base setting following zephyr-7b-beta, ${ }^{14}$ a temperature of 0.5 for the Mistral-Instruct setting following Snorkel-Mistral-PairRM-DPO, and a temperature of 0.9 for both Llama 3 settings. ${ }^{15}$ For Arena-Hard, we use the default greedy decoding for all settings and methods. For MT-Bench, we follow the official decoding configuration which defines different sampling temperatures for different categories.</p>
<p>Computation environment. All the training experiments in this paper were conducted on $8 \times \mathrm{H} 100$ GPUs based on the alignment-handbook repo. ${ }^{16}$</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C Downstream Task Evaluation</h1>
<p>To examine how preference optimization methods affect downstream task performance, we evaluate models trained with different methods on various tasks listed on the Huggingface Open Leaderboard [9]. These tasks include MMLU [38], ARC [19], HellaSwag [94], TruthfulQA [57], Winograd [52], and GSM8K [20]. We follow the established evaluation protocols and present the results for all models in Table 9. Generally, we find that preference optimization's effect varies across tasks.</p>
<p>Knowledge is largely retained with a small loss. Compared to the SFT checkpoint, we find that all preference optimization methods generally maintain MMLU performance with minimal decline. In this aspect, SimPO is largely comparable to DPO.</p>
<p>Reading comprehension and commonsense reasoning improves. For ARC and HellaSwag, preference optimization methods generally improve performance compared to the SFT checkpoint. One hypothesis is that the preference optimization dataset contains similar prompts to these tasks, which helps the model better understand the context and improve reading comprehension and commonsense reasoning abilities.</p>
<p>Truthfulness improves. Surprisingly, we find that preference optimization methods consistently improve TruthfulQA performance compared to the SFT checkpoint, and the improvement could be as high as over $10 \%$ in some cases. Similarly, we hypothesize that the preference dataset contains instances that emphasize truthfulness, which helps the model better understand the context and generate more truthful responses.</p>
<p>Math performance drops. GSM8K is the benchmark that shows the most volatility across methods. Notably, except for ORPO, almost all approaches lead to consistent drops in one or more settings. We hypothesize that ORPO retains performance largely due to its supervised fine-tuning loss for regulation. [63] adds a reference-model calibrated supervised fine-tuning loss to the preference optimization objective, and find that it effectively solves the issue and maintains performance on math tasks as well.</p>
<p>Overall, identifying a pattern in downstream performance is challenging. Comprehensive analysis is difficult due to using different pretrained models, preference optimization datasets, and objectives. Recent works indicate that gradient-based approaches could be effective in finding relevant data for downstream tasks [86], and could possibly extended to understand the effect of preference optimization. We believe a thorough study on how preference optimization affects downstream performance would be valuable and call for a rigorous and more comprehensive analysis in future work.</p>
<h2>D Standard Deviation of AlpacaEval 2 and Arena-Hard</h2>
<p>We present the standard deviation of AlpacaEval 2 and the $95 \%$ confidence interval of Arena-Hard in Table 10. All these metrics are reasonable and do not exhibit any significant outliers or instability.</p>
<h2>E Generation Length Analysis</h2>
<p>Length normalization decreases generation length and improves generation quality. Removing length normalization from the SimPO objective results in an approach similar to Contrastive Preference Optimization (CPO) [88], which interpolates reward maximization with a supervised fine-tuning loss and has demonstrated strong performance in machine translation. However, without the supervised fine-tuning loss, the reward maximization objective without length normalization is suboptimal in preference optimization.
We analyze the generation length of models trained with or without length normalization on AlpacaEval 2 and Arena-Hard. As shown in Figure 6, length normalization significantly decrease the generation length by up to $25 \%$ compared to when it is not used in most cases. However, even though the generation length is shorter, the models with length normalization consistently achieve much higher win rates on both benchmarks. This suggests that length normalization can effectively control the verbosity of the generated responses, and meanwhile improve the generation quality.</p>
<p>Table 9: Downstream task evaluation results of tasks on the huggingface open leaderboard.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MMLU (5)</th>
<th style="text-align: center;">ARC (25)</th>
<th style="text-align: center;">HellaSwag (10)</th>
<th style="text-align: center;">TruthfulQA (0)</th>
<th style="text-align: center;">Winograd (5)</th>
<th style="text-align: center;">GSM8K (5)</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Mistral-Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">60.10</td>
<td style="text-align: center;">58.28</td>
<td style="text-align: center;">80.76</td>
<td style="text-align: center;">40.35</td>
<td style="text-align: center;">76.40</td>
<td style="text-align: center;">28.13</td>
<td style="text-align: center;">57.34</td>
</tr>
<tr>
<td style="text-align: center;">RRHF</td>
<td style="text-align: center;">57.41</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">80.16</td>
<td style="text-align: center;">43.73</td>
<td style="text-align: center;">76.64</td>
<td style="text-align: center;">4.78</td>
<td style="text-align: center;">52.48</td>
</tr>
<tr>
<td style="text-align: center;">SLiC-HF</td>
<td style="text-align: center;">59.24</td>
<td style="text-align: center;">55.38</td>
<td style="text-align: center;">81.15</td>
<td style="text-align: center;">48.36</td>
<td style="text-align: center;">77.35</td>
<td style="text-align: center;">33.74</td>
<td style="text-align: center;">59.20</td>
</tr>
<tr>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">58.48</td>
<td style="text-align: center;">61.26</td>
<td style="text-align: center;">83.59</td>
<td style="text-align: center;">53.06</td>
<td style="text-align: center;">76.80</td>
<td style="text-align: center;">21.76</td>
<td style="text-align: center;">59.16</td>
</tr>
<tr>
<td style="text-align: center;">IPO</td>
<td style="text-align: center;">60.23</td>
<td style="text-align: center;">60.84</td>
<td style="text-align: center;">83.30</td>
<td style="text-align: center;">45.44</td>
<td style="text-align: center;">77.58</td>
<td style="text-align: center;">27.14</td>
<td style="text-align: center;">59.09</td>
</tr>
<tr>
<td style="text-align: center;">CPO</td>
<td style="text-align: center;">59.39</td>
<td style="text-align: center;">57.00</td>
<td style="text-align: center;">80.75</td>
<td style="text-align: center;">47.07</td>
<td style="text-align: center;">76.48</td>
<td style="text-align: center;">33.06</td>
<td style="text-align: center;">58.96</td>
</tr>
<tr>
<td style="text-align: center;">KTO</td>
<td style="text-align: center;">60.90</td>
<td style="text-align: center;">62.37</td>
<td style="text-align: center;">84.88</td>
<td style="text-align: center;">56.60</td>
<td style="text-align: center;">77.27</td>
<td style="text-align: center;">38.51</td>
<td style="text-align: center;">63.42</td>
</tr>
<tr>
<td style="text-align: center;">ORPO</td>
<td style="text-align: center;">63.20</td>
<td style="text-align: center;">61.01</td>
<td style="text-align: center;">84.09</td>
<td style="text-align: center;">47.91</td>
<td style="text-align: center;">78.61</td>
<td style="text-align: center;">42.15</td>
<td style="text-align: center;">62.83</td>
</tr>
<tr>
<td style="text-align: center;">R-DPO</td>
<td style="text-align: center;">59.58</td>
<td style="text-align: center;">61.35</td>
<td style="text-align: center;">84.29</td>
<td style="text-align: center;">46.12</td>
<td style="text-align: center;">76.56</td>
<td style="text-align: center;">18.12</td>
<td style="text-align: center;">57.67</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">59.21</td>
<td style="text-align: center;">62.63</td>
<td style="text-align: center;">83.60</td>
<td style="text-align: center;">50.68</td>
<td style="text-align: center;">77.27</td>
<td style="text-align: center;">22.21</td>
<td style="text-align: center;">59.27</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">60.40</td>
<td style="text-align: center;">63.57</td>
<td style="text-align: center;">84.79</td>
<td style="text-align: center;">66.81</td>
<td style="text-align: center;">76.64</td>
<td style="text-align: center;">40.49</td>
<td style="text-align: center;">65.45</td>
</tr>
<tr>
<td style="text-align: center;">RRHF</td>
<td style="text-align: center;">59.75</td>
<td style="text-align: center;">64.42</td>
<td style="text-align: center;">85.54</td>
<td style="text-align: center;">67.98</td>
<td style="text-align: center;">76.64</td>
<td style="text-align: center;">37.76</td>
<td style="text-align: center;">65.35</td>
</tr>
<tr>
<td style="text-align: center;">SLiC-HF</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">59.90</td>
<td style="text-align: center;">84.05</td>
<td style="text-align: center;">65.30</td>
<td style="text-align: center;">76.32</td>
<td style="text-align: center;">39.65</td>
<td style="text-align: center;">64.30</td>
</tr>
<tr>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">60.53</td>
<td style="text-align: center;">65.36</td>
<td style="text-align: center;">85.86</td>
<td style="text-align: center;">66.71</td>
<td style="text-align: center;">76.80</td>
<td style="text-align: center;">40.33</td>
<td style="text-align: center;">65.93</td>
</tr>
<tr>
<td style="text-align: center;">IPO</td>
<td style="text-align: center;">60.20</td>
<td style="text-align: center;">63.31</td>
<td style="text-align: center;">84.88</td>
<td style="text-align: center;">67.36</td>
<td style="text-align: center;">75.85</td>
<td style="text-align: center;">39.42</td>
<td style="text-align: center;">65.17</td>
</tr>
<tr>
<td style="text-align: center;">CPO</td>
<td style="text-align: center;">60.36</td>
<td style="text-align: center;">63.23</td>
<td style="text-align: center;">84.47</td>
<td style="text-align: center;">67.38</td>
<td style="text-align: center;">76.80</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">65.16</td>
</tr>
<tr>
<td style="text-align: center;">KTO</td>
<td style="text-align: center;">60.52</td>
<td style="text-align: center;">65.78</td>
<td style="text-align: center;">85.49</td>
<td style="text-align: center;">68.45</td>
<td style="text-align: center;">75.93</td>
<td style="text-align: center;">38.82</td>
<td style="text-align: center;">65.83</td>
</tr>
<tr>
<td style="text-align: center;">ORPO</td>
<td style="text-align: center;">60.43</td>
<td style="text-align: center;">61.43</td>
<td style="text-align: center;">84.32</td>
<td style="text-align: center;">66.33</td>
<td style="text-align: center;">76.80</td>
<td style="text-align: center;">36.85</td>
<td style="text-align: center;">64.36</td>
</tr>
<tr>
<td style="text-align: center;">R-DPO</td>
<td style="text-align: center;">60.71</td>
<td style="text-align: center;">66.30</td>
<td style="text-align: center;">86.01</td>
<td style="text-align: center;">68.22</td>
<td style="text-align: center;">76.72</td>
<td style="text-align: center;">37.00</td>
<td style="text-align: center;">65.82</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">60.53</td>
<td style="text-align: center;">66.89</td>
<td style="text-align: center;">85.95</td>
<td style="text-align: center;">68.40</td>
<td style="text-align: center;">76.32</td>
<td style="text-align: center;">35.25</td>
<td style="text-align: center;">65.56</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">64.88</td>
<td style="text-align: center;">60.15</td>
<td style="text-align: center;">81.37</td>
<td style="text-align: center;">45.33</td>
<td style="text-align: center;">75.77</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">62.30</td>
</tr>
<tr>
<td style="text-align: center;">RRHF</td>
<td style="text-align: center;">64.71</td>
<td style="text-align: center;">62.12</td>
<td style="text-align: center;">82.03</td>
<td style="text-align: center;">55.01</td>
<td style="text-align: center;">77.51</td>
<td style="text-align: center;">44.28</td>
<td style="text-align: center;">64.27</td>
</tr>
<tr>
<td style="text-align: center;">SLiC-HF</td>
<td style="text-align: center;">64.36</td>
<td style="text-align: center;">61.43</td>
<td style="text-align: center;">81.88</td>
<td style="text-align: center;">54.95</td>
<td style="text-align: center;">77.27</td>
<td style="text-align: center;">48.82</td>
<td style="text-align: center;">64.79</td>
</tr>
<tr>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">64.31</td>
<td style="text-align: center;">64.42</td>
<td style="text-align: center;">83.87</td>
<td style="text-align: center;">53.48</td>
<td style="text-align: center;">76.32</td>
<td style="text-align: center;">38.67</td>
<td style="text-align: center;">63.51</td>
</tr>
<tr>
<td style="text-align: center;">IPO</td>
<td style="text-align: center;">64.40</td>
<td style="text-align: center;">62.88</td>
<td style="text-align: center;">80.46</td>
<td style="text-align: center;">54.20</td>
<td style="text-align: center;">72.22</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">59.47</td>
</tr>
<tr>
<td style="text-align: center;">CPO</td>
<td style="text-align: center;">64.98</td>
<td style="text-align: center;">61.69</td>
<td style="text-align: center;">82.03</td>
<td style="text-align: center;">54.29</td>
<td style="text-align: center;">76.16</td>
<td style="text-align: center;">46.93</td>
<td style="text-align: center;">64.35</td>
</tr>
<tr>
<td style="text-align: center;">KTO</td>
<td style="text-align: center;">64.42</td>
<td style="text-align: center;">63.14</td>
<td style="text-align: center;">83.55</td>
<td style="text-align: center;">55.76</td>
<td style="text-align: center;">76.09</td>
<td style="text-align: center;">38.97</td>
<td style="text-align: center;">63.65</td>
</tr>
<tr>
<td style="text-align: center;">ORPO</td>
<td style="text-align: center;">64.44</td>
<td style="text-align: center;">61.69</td>
<td style="text-align: center;">82.24</td>
<td style="text-align: center;">56.11</td>
<td style="text-align: center;">77.51</td>
<td style="text-align: center;">50.04</td>
<td style="text-align: center;">65.34</td>
</tr>
<tr>
<td style="text-align: center;">R-DPO</td>
<td style="text-align: center;">64.19</td>
<td style="text-align: center;">64.59</td>
<td style="text-align: center;">83.90</td>
<td style="text-align: center;">53.41</td>
<td style="text-align: center;">75.93</td>
<td style="text-align: center;">39.27</td>
<td style="text-align: center;">63.55</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">64.00</td>
<td style="text-align: center;">65.19</td>
<td style="text-align: center;">83.09</td>
<td style="text-align: center;">59.46</td>
<td style="text-align: center;">77.19</td>
<td style="text-align: center;">31.54</td>
<td style="text-align: center;">63.41</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">67.06</td>
<td style="text-align: center;">61.01</td>
<td style="text-align: center;">78.57</td>
<td style="text-align: center;">51.66</td>
<td style="text-align: center;">74.35</td>
<td style="text-align: center;">68.69</td>
<td style="text-align: center;">66.89</td>
</tr>
<tr>
<td style="text-align: center;">RRHF</td>
<td style="text-align: center;">67.20</td>
<td style="text-align: center;">61.52</td>
<td style="text-align: center;">79.54</td>
<td style="text-align: center;">53.76</td>
<td style="text-align: center;">74.19</td>
<td style="text-align: center;">66.11</td>
<td style="text-align: center;">67.05</td>
</tr>
<tr>
<td style="text-align: center;">SLiC-HF</td>
<td style="text-align: center;">66.41</td>
<td style="text-align: center;">61.26</td>
<td style="text-align: center;">78.80</td>
<td style="text-align: center;">53.23</td>
<td style="text-align: center;">76.16</td>
<td style="text-align: center;">66.57</td>
<td style="text-align: center;">67.07</td>
</tr>
<tr>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">66.88</td>
<td style="text-align: center;">63.99</td>
<td style="text-align: center;">80.78</td>
<td style="text-align: center;">59.01</td>
<td style="text-align: center;">74.66</td>
<td style="text-align: center;">49.81</td>
<td style="text-align: center;">65.86</td>
</tr>
<tr>
<td style="text-align: center;">IPO</td>
<td style="text-align: center;">66.52</td>
<td style="text-align: center;">61.95</td>
<td style="text-align: center;">77.90</td>
<td style="text-align: center;">54.64</td>
<td style="text-align: center;">73.09</td>
<td style="text-align: center;">58.23</td>
<td style="text-align: center;">65.39</td>
</tr>
<tr>
<td style="text-align: center;">CPO</td>
<td style="text-align: center;">67.05</td>
<td style="text-align: center;">62.29</td>
<td style="text-align: center;">78.73</td>
<td style="text-align: center;">54.01</td>
<td style="text-align: center;">73.72</td>
<td style="text-align: center;">67.40</td>
<td style="text-align: center;">67.20</td>
</tr>
<tr>
<td style="text-align: center;">KTO</td>
<td style="text-align: center;">66.38</td>
<td style="text-align: center;">63.57</td>
<td style="text-align: center;">79.51</td>
<td style="text-align: center;">58.15</td>
<td style="text-align: center;">73.40</td>
<td style="text-align: center;">57.01</td>
<td style="text-align: center;">66.34</td>
</tr>
<tr>
<td style="text-align: center;">ORPO</td>
<td style="text-align: center;">66.41</td>
<td style="text-align: center;">61.01</td>
<td style="text-align: center;">79.38</td>
<td style="text-align: center;">54.37</td>
<td style="text-align: center;">75.77</td>
<td style="text-align: center;">64.59</td>
<td style="text-align: center;">66.92</td>
</tr>
<tr>
<td style="text-align: center;">R-DPO</td>
<td style="text-align: center;">66.74</td>
<td style="text-align: center;">64.33</td>
<td style="text-align: center;">80.97</td>
<td style="text-align: center;">60.32</td>
<td style="text-align: center;">74.82</td>
<td style="text-align: center;">43.90</td>
<td style="text-align: center;">65.18</td>
</tr>
<tr>
<td style="text-align: center;">SimPO</td>
<td style="text-align: center;">65.63</td>
<td style="text-align: center;">62.80</td>
<td style="text-align: center;">78.33</td>
<td style="text-align: center;">60.70</td>
<td style="text-align: center;">73.32</td>
<td style="text-align: center;">50.72</td>
<td style="text-align: center;">65.25</td>
</tr>
</tbody>
</table>
<p>Length is not a reliable indicator of generation quality. We further analyze the generation length of models trained with different methods on AlpacaEval 2 and Arena-Hard, as shown in Table 10. Generally, we find that no single method consistently generates longer or shorter responses across all settings. Additionally, even though some methods may generate longer responses, they do not necessarily achieve better win rates on the benchmarks. This indicates that the length of the generated responses is not a reliable indicator of generation quality.</p>
<p>SimPO demonstrates minimal exploitation of response length. We observe that SimPO has a shorter generation length compared to DPO in the Llama-3-Instruct case but exhibits a higher generation length in other settings, with up to $26 \%$ longer responses on AlpacaEval 2. Conversely, SimPO only increases length by only around $5 \%$ on Arena-Hard compared to DPO. It is fair to say that the generation length heavily depends on the evaluation benchmark. A stronger indicator is that SimPO consistently achieves a higher length-controlled win rate on AlpacaEval 2 compared to the raw win rate, demonstrating minimal exploitation of response length.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ There is a discrepancy between the KTO runs in their original paper, where the original runs use a RMSProp optimizer [68]. We use an Adam optimizer [47] for all the experiments.
${ }^{14}$ https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/ models_configs/zephyr-7b-beta/configs.yaml
${ }^{15}$ We grid search the temperature hyperparameter for the Llama-3-Base setting with DPO over $0.1,0.3,0.5,0.7,0.9$, and fix it for all different methods.
${ }^{16}$ https://github.com/huggingface/alignment-handbook&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3868 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3868</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3868</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-76427fe94e4564fd5df2177bb259d93527fddca5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76427fe94e4564fd5df2177bb259d93527fddca5" target="_blank">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training and achieves new state-of-the-art results on the BEIR benchmark.</p>
                <p><strong>Paper Abstract:</strong> Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3868.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3868.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InPars-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InPars version 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based dataset generator and reranker-training pipeline that uses an open-source LLM (GPT-J-6B) to synthesize queries from documents, filters generated query-document pairs with a monoT5 reranker, and uses the synthetic pairs to finetune rerankers for improved retrieval on BEIR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>InPars-v2</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Generate one synthetic query per document using an open-source LLM (GPT-J-6B) prompted with 3 MS MARCO examples; score generated query-document pairs with a pretrained monoT5-3B reranker and keep top-scoring pairs; create negatives via BM25 top-1000 sampling; train/fine-tune monoT5 rerankers on the synthetic positives/negatives, then use BM25 retrieval + fine-tuned monoT5 reranking for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Per BEIR dataset: sample up to 100k documents from each dataset corpus in BEIR (if corpus <100k, generate for entire corpus); BEIR includes diverse domains (news, biomedical, QA, argumentation, scientific datasets like SciDocs and SciFact); language and exact sizes depend on each BEIR dataset (paper reports sampling 100k per dataset when available).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Queries are generated by prompting GPT-J-6B with a 3-shot prompt template (the 'gbq' template from InPars-v1) using 3 MS MARCO examples; one synthetic query is generated per document using greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot LLM generation of synthetic queries (prompt engineering using 3-shot examples) followed by reranker-based filtering: score each generated query-document pair with monoT5-3B (pre-finetuned on MS MARCO) and keep top 10k pairs; negatives sampled randomly from BM25 top-1000 for each synthetic query; finetune reranker on these synthetic positives/negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>For each dataset: a set of synthetic query-document pairs (10k positives and 10k negatives per synthetic dataset) and a fine-tuned monoT5-3B reranker model. The immediate outputs are the synthetic dataset files and finetuned model checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Evaluate retrieval effectiveness on the BEIR benchmark using a pipeline: BM25 retrieving top-1000 documents per query (Pyserini flat index) followed by reranking with the fine-tuned monoT5-3B; primary metric reported is nDCG@10; comparisons made against BM25, monoT5 pretrained on MS MARCO, InPars-v1, Promptagator, and RankT5.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>InPars-v2 achieves state-of-the-art average effectiveness on BEIR (reported Avg nDCG@10 = 0.545), improving over InPars-v1 (Avg 0.539) and substantially improving on several datasets (e.g., TREC-News, Climate-FEVER, Robust, and Touche). Per-dataset nDCG@10 scores are provided in the paper's Table 1. Generation cost: ~30 hours on an A100 for 100k queries; scoring 100k pairs with monoT5 on a TPU v3-8 takes ~1.5 hours; finetuning per synthetic dataset takes <10 minutes on a TPU v3-8.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations include compute cost for generating and scoring large synthetic datasets (e.g., ~30 GPU hours for 100k queries), dataset-dependent effectiveness (argument-retrieval datasets like ArguAna and Touche benefit from dataset-specific prompts and can be better served by other approaches), and dependence on the quality of synthetic queries and the reranker filtering step; no human-expert validation or hallucination analysis is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Direct comparisons to BM25, a monoT5-3B model pretrained on MS MARCO, InPars-v1, Promptagator, and RankT5 are provided; InPars-v2 improves average BEIR nDCG@10 over InPars-v1 and monoT5-MARCO, and is reported as achieving a new state of the art on BEIR average; Promptagator and RankT5 outperform InPars variants on certain argument-retrieval datasets due to dataset-specific prompting and model choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3868.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3868.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InPars-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InPars (original)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier method that uses LLMs to generate synthetic queries from documents and uses those synthetic query-document pairs to train retrieval models; originally relied on a proprietary LLM and used generation log-probabilities for filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inpars: Data augmentation for information retrieval using large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>InPars (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Use an LLM (original work used OpenAI's curie model) to generate queries from documents (few-shot prompting), then filter generated query-document pairs by the LLM's log-probability of generating the query given the document and 3-shot examples; use top-scoring synthetic pairs to train retrievers/rerankers.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to corpora in BEIR (per the paper's comparison); InPars-v1 produced synthetic datasets per BEIR dataset (sampling details in original work), used MS MARCO 3-shot examples for prompting in the InPars pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Queries produced via few-shot prompting of a proprietary LLM (curie) using 3 MS MARCO examples and the 'gbq' style prompts (as referenced by InPars-v2).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based synthetic query generation with filtering based on the generation log-probabilities (rank generated pairs by model log-prob and keep top pairs), then use the resulting synthetic dataset to train retrieval/reranking models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Synthetic query-document pairs used to finetune retrieval and reranking models (e.g., monoT5 variants); results reported as improved retrieval metrics on BEIR.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Evaluated on BEIR and compared to BM25 and other baselines (reported in this paper's comparisons and in the original InPars paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>InPars-v1 improved retrieval performance relative to baselines and served as the basis for InPars-v2; in the paper's Table 1 the InPars-v1 column shows an average nDCG@10 of 0.539 across BEIR datasets, which InPars-v2 improves upon slightly (0.545 avg).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relied on a proprietary LLM (OpenAI curie) for query generation and used log-probability filtering which InPars-v2 replaced with reranker-based filtering; potential reproducibility and access limitations due to proprietary model dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared against BM25 and monoT5-MARCO baselines and reported improvements; later work (InPars-v2) showed further gains by replacing proprietary LLM and improving filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3868.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3868.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Promptagator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Promptagator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses a (proprietary) large finetuned LLM (FLAN) with dataset-specific prompts to generate alternative queries for a corpus in an unsupervised manner and then uses smaller T5 models in a fully trainable retrieval pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Promptagator: Few-shot dense retrieval from 8 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Promptagator</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Feeds dataset-specific prompts to a large LLM (FLAN) to generate synthetic queries; differs from InPars by using dataset-specific prompts, a larger LLM for generation, and smaller (110M-parameter) T5 models for retrieval and reranking in a fully trainable pipeline; does not use supervised MS MARCO data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to the same heterogeneous BEIR-style corpora (paper compares Promptagator results on BEIR datasets); specifics depend on Promptagator original paper (not fully detailed here), and it targets diverse datasets including argument retrieval corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Uses dataset-specific prompt templates (few-shot prompting tailored to each dataset) to instruct FLAN to generate alternative queries for documents; Promptagator emphasizes few-shot prompts (title suggests 8 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering with dataset-specific prompts on a large finetuned LLM (FLAN) to synthesize queries, followed by training smaller retrieval/reranking T5 models end-to-end on the generated data (no MS MARCO supervision used).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Synthetic query sets tailored per dataset and trained retrieval/reranker models (using smaller T5 architectures for retrieval and reranking).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Evaluated on BEIR benchmark in the referenced work; compared against BM25, monoT5, InPars variants, and RankT5 in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Promptagator performs competitively on BEIR and particularly well on argument-retrieval datasets (e.g., ArguAna and Touche) where dataset-specific prompts yield large gains; it achieves stronger performance on some datasets where InPars variants cannot surpass BM25, attributable to its dataset-specific prompting strategy and trainable pipeline using smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on a proprietary LLM (FLAN) for generation and uses dataset-specific prompts that may require per-dataset engineering; does not use supervised MS MARCO data which can be an advantage but also a limitation depending on available supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared in the paper's Table 1 against BM25, monoT5-MARCO, InPars-v1/v2, and RankT5; Promptagator outperforms on certain argument-retrieval datasets but is not uniformly better across all BEIR datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inpars: Data augmentation for information retrieval using large language models. <em>(Rating: 2)</em></li>
                <li>Promptagator: Few-shot dense retrieval from 8 examples. <em>(Rating: 2)</em></li>
                <li>Rankt5: Fine-tuning t5 for text ranking with ranking losses. <em>(Rating: 2)</em></li>
                <li>Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. <em>(Rating: 1)</em></li>
                <li>Document ranking with a pretrained sequence-to-sequence model. <em>(Rating: 2)</em></li>
                <li>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. <em>(Rating: 1)</em></li>
                <li>Finetuned language models are zero-shot learners. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3868",
    "paper_id": "paper-76427fe94e4564fd5df2177bb259d93527fddca5",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "InPars-v2",
            "name_full": "InPars version 2",
            "brief_description": "An LLM-based dataset generator and reranker-training pipeline that uses an open-source LLM (GPT-J-6B) to synthesize queries from documents, filters generated query-document pairs with a monoT5 reranker, and uses the synthetic pairs to finetune rerankers for improved retrieval on BEIR.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "InPars-v2",
            "system_or_method_description": "Generate one synthetic query per document using an open-source LLM (GPT-J-6B) prompted with 3 MS MARCO examples; score generated query-document pairs with a pretrained monoT5-3B reranker and keep top-scoring pairs; create negatives via BM25 top-1000 sampling; train/fine-tune monoT5 rerankers on the synthetic positives/negatives, then use BM25 retrieval + fine-tuned monoT5 reranking for evaluation.",
            "input_corpus_description": "Per BEIR dataset: sample up to 100k documents from each dataset corpus in BEIR (if corpus &lt;100k, generate for entire corpus); BEIR includes diverse domains (news, biomedical, QA, argumentation, scientific datasets like SciDocs and SciFact); language and exact sizes depend on each BEIR dataset (paper reports sampling 100k per dataset when available).",
            "topic_or_query_specification": "Queries are generated by prompting GPT-J-6B with a 3-shot prompt template (the 'gbq' template from InPars-v1) using 3 MS MARCO examples; one synthetic query is generated per document using greedy decoding.",
            "distillation_method": "Few-shot LLM generation of synthetic queries (prompt engineering using 3-shot examples) followed by reranker-based filtering: score each generated query-document pair with monoT5-3B (pre-finetuned on MS MARCO) and keep top 10k pairs; negatives sampled randomly from BM25 top-1000 for each synthetic query; finetune reranker on these synthetic positives/negatives.",
            "output_type_and_format": "For each dataset: a set of synthetic query-document pairs (10k positives and 10k negatives per synthetic dataset) and a fine-tuned monoT5-3B reranker model. The immediate outputs are the synthetic dataset files and finetuned model checkpoints.",
            "evaluation_or_validation_method": "Evaluate retrieval effectiveness on the BEIR benchmark using a pipeline: BM25 retrieving top-1000 documents per query (Pyserini flat index) followed by reranking with the fine-tuned monoT5-3B; primary metric reported is nDCG@10; comparisons made against BM25, monoT5 pretrained on MS MARCO, InPars-v1, Promptagator, and RankT5.",
            "results_summary": "InPars-v2 achieves state-of-the-art average effectiveness on BEIR (reported Avg nDCG@10 = 0.545), improving over InPars-v1 (Avg 0.539) and substantially improving on several datasets (e.g., TREC-News, Climate-FEVER, Robust, and Touche). Per-dataset nDCG@10 scores are provided in the paper's Table 1. Generation cost: ~30 hours on an A100 for 100k queries; scoring 100k pairs with monoT5 on a TPU v3-8 takes ~1.5 hours; finetuning per synthetic dataset takes &lt;10 minutes on a TPU v3-8.",
            "limitations_or_challenges": "Reported limitations include compute cost for generating and scoring large synthetic datasets (e.g., ~30 GPU hours for 100k queries), dataset-dependent effectiveness (argument-retrieval datasets like ArguAna and Touche benefit from dataset-specific prompts and can be better served by other approaches), and dependence on the quality of synthetic queries and the reranker filtering step; no human-expert validation or hallucination analysis is reported in this paper.",
            "comparison_to_baselines_or_humans": "Direct comparisons to BM25, a monoT5-3B model pretrained on MS MARCO, InPars-v1, Promptagator, and RankT5 are provided; InPars-v2 improves average BEIR nDCG@10 over InPars-v1 and monoT5-MARCO, and is reported as achieving a new state of the art on BEIR average; Promptagator and RankT5 outperform InPars variants on certain argument-retrieval datasets due to dataset-specific prompting and model choices.",
            "uuid": "e3868.0",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "InPars-v1",
            "name_full": "InPars (original)",
            "brief_description": "An earlier method that uses LLMs to generate synthetic queries from documents and uses those synthetic query-document pairs to train retrieval models; originally relied on a proprietary LLM and used generation log-probabilities for filtering.",
            "citation_title": "Inpars: Data augmentation for information retrieval using large language models.",
            "mention_or_use": "mention",
            "system_or_method_name": "InPars (v1)",
            "system_or_method_description": "Use an LLM (original work used OpenAI's curie model) to generate queries from documents (few-shot prompting), then filter generated query-document pairs by the LLM's log-probability of generating the query given the document and 3-shot examples; use top-scoring synthetic pairs to train retrievers/rerankers.",
            "input_corpus_description": "Applied to corpora in BEIR (per the paper's comparison); InPars-v1 produced synthetic datasets per BEIR dataset (sampling details in original work), used MS MARCO 3-shot examples for prompting in the InPars pipeline.",
            "topic_or_query_specification": "Queries produced via few-shot prompting of a proprietary LLM (curie) using 3 MS MARCO examples and the 'gbq' style prompts (as referenced by InPars-v2).",
            "distillation_method": "Prompt-based synthetic query generation with filtering based on the generation log-probabilities (rank generated pairs by model log-prob and keep top pairs), then use the resulting synthetic dataset to train retrieval/reranking models.",
            "output_type_and_format": "Synthetic query-document pairs used to finetune retrieval and reranking models (e.g., monoT5 variants); results reported as improved retrieval metrics on BEIR.",
            "evaluation_or_validation_method": "Evaluated on BEIR and compared to BM25 and other baselines (reported in this paper's comparisons and in the original InPars paper).",
            "results_summary": "InPars-v1 improved retrieval performance relative to baselines and served as the basis for InPars-v2; in the paper's Table 1 the InPars-v1 column shows an average nDCG@10 of 0.539 across BEIR datasets, which InPars-v2 improves upon slightly (0.545 avg).",
            "limitations_or_challenges": "Relied on a proprietary LLM (OpenAI curie) for query generation and used log-probability filtering which InPars-v2 replaced with reranker-based filtering; potential reproducibility and access limitations due to proprietary model dependence.",
            "comparison_to_baselines_or_humans": "Compared against BM25 and monoT5-MARCO baselines and reported improvements; later work (InPars-v2) showed further gains by replacing proprietary LLM and improving filtering.",
            "uuid": "e3868.1",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Promptagator",
            "name_full": "Promptagator",
            "brief_description": "A method that uses a (proprietary) large finetuned LLM (FLAN) with dataset-specific prompts to generate alternative queries for a corpus in an unsupervised manner and then uses smaller T5 models in a fully trainable retrieval pipeline.",
            "citation_title": "Promptagator: Few-shot dense retrieval from 8 examples.",
            "mention_or_use": "mention",
            "system_or_method_name": "Promptagator",
            "system_or_method_description": "Feeds dataset-specific prompts to a large LLM (FLAN) to generate synthetic queries; differs from InPars by using dataset-specific prompts, a larger LLM for generation, and smaller (110M-parameter) T5 models for retrieval and reranking in a fully trainable pipeline; does not use supervised MS MARCO data.",
            "input_corpus_description": "Applied to the same heterogeneous BEIR-style corpora (paper compares Promptagator results on BEIR datasets); specifics depend on Promptagator original paper (not fully detailed here), and it targets diverse datasets including argument retrieval corpora.",
            "topic_or_query_specification": "Uses dataset-specific prompt templates (few-shot prompting tailored to each dataset) to instruct FLAN to generate alternative queries for documents; Promptagator emphasizes few-shot prompts (title suggests 8 examples).",
            "distillation_method": "Prompt engineering with dataset-specific prompts on a large finetuned LLM (FLAN) to synthesize queries, followed by training smaller retrieval/reranking T5 models end-to-end on the generated data (no MS MARCO supervision used).",
            "output_type_and_format": "Synthetic query sets tailored per dataset and trained retrieval/reranker models (using smaller T5 architectures for retrieval and reranking).",
            "evaluation_or_validation_method": "Evaluated on BEIR benchmark in the referenced work; compared against BM25, monoT5, InPars variants, and RankT5 in this paper's comparisons.",
            "results_summary": "Promptagator performs competitively on BEIR and particularly well on argument-retrieval datasets (e.g., ArguAna and Touche) where dataset-specific prompts yield large gains; it achieves stronger performance on some datasets where InPars variants cannot surpass BM25, attributable to its dataset-specific prompting strategy and trainable pipeline using smaller models.",
            "limitations_or_challenges": "Relies on a proprietary LLM (FLAN) for generation and uses dataset-specific prompts that may require per-dataset engineering; does not use supervised MS MARCO data which can be an advantage but also a limitation depending on available supervision.",
            "comparison_to_baselines_or_humans": "Compared in the paper's Table 1 against BM25, monoT5-MARCO, InPars-v1/v2, and RankT5; Promptagator outperforms on certain argument-retrieval datasets but is not uniformly better across all BEIR datasets.",
            "uuid": "e3868.2",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inpars: Data augmentation for information retrieval using large language models.",
            "rating": 2
        },
        {
            "paper_title": "Promptagator: Few-shot dense retrieval from 8 examples.",
            "rating": 2
        },
        {
            "paper_title": "Rankt5: Fine-tuning t5 for text ranking with ranking losses.",
            "rating": 2
        },
        {
            "paper_title": "Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.",
            "rating": 1
        },
        {
            "paper_title": "Document ranking with a pretrained sequence-to-sequence model.",
            "rating": 2
        },
        {
            "paper_title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.",
            "rating": 1
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners.",
            "rating": 1
        }
    ],
    "cost": 0.0139475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</h1>
<p>Vitor Jeronymo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Marzieh Fadaee<br>Zeta Alpha, Netherlands</p>
<p>Luiz Bonifacio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Roberto Lotufo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Rodrigo Nogueira<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Zeta Alpha, Netherlands</p>
<p>Hugo Abonizio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Jakub Zavrel<br>Zeta Alpha, Netherlands</p>
<h4>Abstract</h4>
<p>Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/legacy/inpars-v2</p>
<h2>1 Introduction and Background</h2>
<p>Data augmentation has been a reliable tool to improve the effectiveness of AI models in the face of the scarcity of high-quality in-domain training data, which is a common problem in practical applications. Previous work by Bonifacio et al. [1] and Dai et al. [2] successfully leveraged the few-shot capabilities of LLMs to generate reliable synthetic training data for information retrieval models. These training data helped their models achieve state-of-the-art (SOTA) results on the BEIR benchmark [6].
Bonifacio et al. [1] propose InPars where they generate queries from documents in the corpus using LLMs. Similarly to Bonifacio et al. [1], the recently published Promptagator [2] model also feeds prompts to LLMs in order to generate alternative queries for a given document in an unsupervised manner. It differs primarily from InPars in that it uses dataset-specific prompts, a larger LLM to generate queries, and a fully trainable retrieval pipeline with smaller models.
This work extends the method of Bonifacio et al. [1] by using a reranker as a filtering mechanism to select the best synthetically generated examples and further improving retrieval effectiveness</p>
<p>on BEIR. We also use an open-source query generator as opposed to the proprietary one used by Bonifacio et al. and provide the source code and data to reproduce our results on TPUs. We refer to Bonifacio et al. [1] model as Inpars-v1 and the model presented in this paper as Inpars-v2.</p>
<h1>2 Methodology</h1>
<p>In this section, we explain the experiments we performed and how they differ from InPars-v1 [1].
To generate synthetic queries, we use the open-source GPT-J [8] with 6B parameters to replace OpenAI's curie model used in InPars-v1. For each dataset in the BEIR benchmark, we sample 100k documents from its corpus and generate one synthetic query per document using GPT-J prompted with 3 examples from MS MARCO. We use greedy decoding and the "gbq" prompt template from InPars-v1. Some corpora in BEIR such as ArguAna [7] have less than 100k documents. In these cases, we generate as many synthetic queries as there are documents in the corpus. It takes on average 30 hours on an A100 GPU to generate 100k queries.
Once the synthetic queries are generated, we apply a filtering step to select query-document pairs that are more likely to be relevant to each other. In InPars-v1, this filtering step consisted of selecting the top 10 k query-document pairs with the highest log probabilities of generating a query given the 3 -shot examples and the document as input. In InPars-v2, we use monoT5-3B [4] already finetuned on MS MARCO for one epoch ${ }^{1}$ to estimate a relevancy score for each of the 100k query-document pairs. Then, we keep only the top 10 k pairs with the highest scores as our positive query-document pairs for training. It takes approximately 1.5 hours to score 100 k query-document pairs on a TPU v3-8. It should take twice as much on a A100.
To obtain negatives (i.e., non-relevant) query-document pairs, we randomly sample one document from the top 1000 retrieved by BM25 when issued the synthetic query. Thus, our training set consists of 10 k positive query-document pairs and 10 k negative query-document pairs.
The rerankers are finetuned in the same manner as in InPars-v1: monoT5-3B is finetuned on MS MARCO for one epoch and then further finetuned for one epoch on the synthetic data. We use the Adafactor optimizer [5] with a constant learning rate of 1e-3. Each batch has 64 positive and 64 negative query-document pairs randomly sampled from the training dataset. We finetune one model on each synthetic dataset from BEIR, that is, we end up with 18 different rerankers, one per dataset, which are then evaluated on the corresponding test sets. Finetuning on each synthetic dataset takes less than 10 minutes on a TPU v3-8.
Evaluation is performed using the following pipeline: first we use Pyserini's [3] flat indexes ${ }^{2}$ to retrieve a thousand documents for each query using BM25 with default parameters ( $\mathrm{k} 1=0.9, \mathrm{~b}=0.4$ ), for each dataset. Then we use the finetuned monoT5-3B models to rerank these documents.</p>
<h2>3 Results</h2>
<p>Table 1 presents results for BM25 (2nd column), monoT5-3B finetuned on MS MARCO (3rd column), monoT5-3b finetuned on MS MARCO and further finetuned on InPars-v1 (4th column), and monoT5-3B finetuned on MS MARCO and then finetuned on InPars-v2 data (5th column). Compared to InPars-v1, our approach is substantially better on TREC-News, Climate-FEVER, Robust and Touche. Additionally, we compare our method with Promptagator [2] and RankT5 [10]. Taking into account the average of all BEIR datasets, these results represent a new state of the art on BEIR.
Promptagator and RankT5 strive on datasets that monoT5 and InPars-v2 cannot even surpass BM25, such as Touche and ArguAna. Note that these datasets focus on argument retrieval, which is slightly different from other datasets in the BEIR benchmark. As a result, they benefit from using custom prompts. ${ }^{3}$ Promptagator does this without using supervised data from MS MARCO and using smaller T5 models with 110M parameters for the retrieval and reranking steps.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">monoT5-3B <br> MARCO</th>
<th style="text-align: center;">+InPars-v1</th>
<th style="text-align: center;">+InPars-v2</th>
<th style="text-align: center;">PrGator</th>
<th style="text-align: center;">RankT5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TREC-Covid</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.823</td>
</tr>
<tr>
<td style="text-align: left;">Robust</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FiQA</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.493</td>
</tr>
<tr>
<td style="text-align: left;">DBPedia</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.459</td>
</tr>
<tr>
<td style="text-align: left;">SciDocs</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.191</td>
</tr>
<tr>
<td style="text-align: left;">SciFact</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.760</td>
</tr>
<tr>
<td style="text-align: left;">NFCorpus</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.399</td>
</tr>
<tr>
<td style="text-align: left;">BioASQ</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.579</td>
</tr>
<tr>
<td style="text-align: left;">Natural Questions</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: left;">HotpotQA</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.753</td>
</tr>
<tr>
<td style="text-align: left;">TREC-News</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Quora</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.819</td>
</tr>
<tr>
<td style="text-align: left;">FEVER</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.848</td>
</tr>
<tr>
<td style="text-align: left;">Climate-FEVER</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.275</td>
</tr>
<tr>
<td style="text-align: left;">Signal</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.319</td>
</tr>
<tr>
<td style="text-align: left;">ArguAna</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.406</td>
</tr>
<tr>
<td style="text-align: left;">Touche</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.486</td>
</tr>
<tr>
<td style="text-align: left;">CQADupstack</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg PrGator</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.536</td>
</tr>
</tbody>
</table>
<p>Table 1: nDCG@10 on BEIR. "Avg PrGator" is the average of datasets reported by Promptagator.</p>
<p>Promptagator uses a proprietary model, FLAN [9], to generate synthetic queries. The RankT5 model is a modified version of the monoT5 reranker, but its checkpoint and code are not published. In this work, we make the code, models, and data open-source and publicly available.</p>
<h1>4 Conclusion</h1>
<p>In this work, we presented InPars-v2, an improved version of InPars [1] that uses a publicly available language model to generate queries and a better query-document pair selection process. Our results show that we achieve effectiveness on par with the state of the art on BEIR. The synthetic data and finetuned models were publicly released.</p>
<h2>Acknowledgments</h2>
<p>This research was partially supported by Fundao de Amparo  Pesquisa do Estado de So Paulo (FAPESP) (project id 2022/01640-2). We also thank Centro Nacional de Processamento de Alto Desempenho (CENAPAD-SP) and Google Cloud for computing credits.</p>
<h2>References</h2>
<p>[1] L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144, 2022.
[2] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, and M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.
[3] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations. arXiv preprint arXiv:2102.10073, 2021.
[4] R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document ranking with a pretrained sequence-to-sequence model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 708-718, 2020.</p>
<p>[5] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[6] N. Thakur, N. Reimers, A. Rckl, A. Srivastava, and I. Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.
[7] H. Wachsmuth, S. Syed, and B. Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241-251, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[8] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[9] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[10] H. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, and M. Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. arXiv preprint arXiv:2210.10634, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/castorini/monot5-3b-msmarco-10k
${ }^{2}$ As opposed to the multifield index.
${ }^{3}$ In preliminary experiments, we also observed an improvement of more than 10 nDCG@10 points on ArguAna by using a dataset-specific prompt to generate synthetic queries. More details and results on the full BEIR benchmark will appear in an upcoming paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
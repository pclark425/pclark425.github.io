<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1835 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1835</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1835</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-6d8952147b5e9502baf062df83a82cb84d224ea6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6d8952147b5e9502baf062df83a82cb84d224ea6" target="_blank">Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> This work develops an efficient, freely-accessible FEM model of the BioTac and comprises one of the first efforts to combine self-supervision, cross-modal transfer, and sim-to-real transfer for tactile sensors.</p>
                <p><strong>Paper Abstract:</strong> Tactile sensing is critical for robotic grasping and manipulation of objects under visual occlusion. However, in contrast to simulations of robot arms and cameras, current simulations of tactile sensors have limited accuracy, speed, and utility. In this work, we develop an efficient 3D finite element method (FEM) model of the SynTouch BioTac sensor using an open-access, GPU-based robotics simulator. Our simulations closely reproduce results from an experimentally-validated model in an industry-standard, CPU-based simulator, but at 75x the speed. We then learn latent representations for simulated BioTac deformations and real-world electrical output through self-supervision, as well as projections between the latent spaces using a small supervised dataset. Using these learned latent projections, we accurately synthesize real-world BioTac electrical output and estimate contact patches, both for unseen contact interactions. This work contributes an efficient, freely-accessible FEM model of the BioTac and comprises one of the first efforts to combine self-supervision, cross-modal transfer, and sim-to-real transfer for tactile sensors.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1835.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1835.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioTac-LP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real for SynTouch BioTac via FEM + Latent Projections</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper develops a GPU-based 3D FEM model of the SynTouch BioTac in Isaac Gym, learns modality-specific VAEs for FEM deformations and real electrode signals, and trains small supervised latent-space projection networks to perform cross-modal sim-to-real transfer: synthesizing real BioTac electrical outputs from simulated deformations and reconstructing contact patches from real signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>SynTouch BioTac tactile sensor (single physical sensor in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A biomimetic tactile fingertip sensor that transduces skin deformation to electrical (fluidic impedance) signals across 19 electrodes; used here instrumented on a robotic finger for indentation/contact experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / tactile sensing</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym (NVIDIA GPU-based robotics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>GPU-based simulator performing 3D finite element method (FEM) with co-rotational linear-elastic constitutive model, isotropic Coulomb contact, implicit time integration and a GPU Newton solver; returns nodal positions, nodal contact forces, and element stresses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics-style FEM within the constraints of Isaac Gym's linear-elastic deformable model (accurate deformation/contact but approximated physics relative to full ANSYS Neo-Hookean + fluid model)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>contact dynamics (Coulomb frictional contact), nodal deformation fields, nodal contact forces, element stress tensors (via 3D FEM), geometric fidelity of BioTac skin mesh (4000 nodes), multiple contact scenarios across 17 objects.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>skin material modeled as compressible linear-elastic (not Neo-Hookean), internal fluid not modeled, collisions resolved with boundary-layer expanded meshes rather than normal-Lagrange method, linear tetrahedral elements only (limitations of Isaac Gym).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical experiments with a single SynTouch BioTac sensor performing kinematically-randomized indentations and controlled indentations matching simulation sequences over 17 objects and table features; collected 19-electrode impedance signals for training/validation/testing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Synthesis of BioTac electrical signals from simulated FEM deformations (forward mapping) and reconstruction/estimation of contact patch/deformation fields from real electrode signals (inverse mapping); generalization to unseen contact interactions and unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Self-supervised representation learning (VAEs) on large unlabeled datasets for each modality, followed by supervised training of small MLP/FCN projection networks on a small paired (simulated FEM, real electrode) labeled dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>RMS error over all electrodes (normalized to [-1,1] and reported also in raw digital units) and ℓ1/coverage plots; also ℓ2-norm force-deflection errors used for FEM↔ANSYS validation; visual/qualitative contact patch agreement for free-form interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>FEM vs ANSYS: mean ℓ2-norm force error across indenters = 0.153 N; deformation field mean errors: mean error between maximum nodal displacement norms = 1.41e-4 m and between mean norms = 1.81e-5 m. (These quantify simulation fidelity relative to ANSYS.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Electrode prediction median RMS errors (normalized): Unseen Trajectory: Latent Projection 0.010 (≈20 raw units); Fully Supervised 0.012 (≈25 raw units). Unseen Object: Latent Projection 0.015 (≈31 units); Fully Supervised 0.016 (≈32 units). (Coverage plots and qualitative contact-patch matches reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Not performed beyond the learned latent-space projection; the paper explicitly states 'we do not perform domain adaptation beyond projection.'</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Approximations in simulator physics (linear-elastic skin, missing internal fluid), differences in material/friction parameters (optimized to compensate), manufacturing variability across sensors (only one BioTac evaluated), and limits of the mesh VAE latent bottleneck (affecting multimodal deformation distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-fidelity geometric FEM meshes and optimization of material/friction parameters to reproduce validated ANSYS force-deflection profiles; use of large unlabeled simulation and real datasets to learn modality-specific VAEs (self-supervision) and a small paired labeled set to learn latent projections; GPU-based fast simulation enabling large-scale data generation (75x speedup vs ANSYS).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Contact dynamics and accurate nodal deformation fields are critical: reproducing ANSYS force-deflection profiles and deformation fields (mean ℓ2 errors reported) enabled downstream transfer. The paper notes that compensating for missing physics (linear elasticity and no fluid) required adjusting elastic modulus, compressibility, and friction; thus adequate expressivity in simulator plus calibration is required.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No online fine-tuning reported; a small supervised paired dataset (359 resimulated indentations matched to experimental data) was used to train projection networks, but no additional real-world fine-tuning/adaptation beyond that pairing was described.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared Isaac Gym (linear-elastic, GPU) to ANSYS (Neo-Hookean, CPU): Isaac Gym reproduced ANSYS results with mean ℓ2 force error across indenters = 0.153 N and nodal deformation field errors (max and mean norms) on the order of 1e-4–1e-5 m; simulation speed: ANSYS ~7.08 min/indentation on 6 CPUs vs Isaac Gym ~5.57 s/indentation using 8 parallel environments on 1 GPU (≈75x speedup).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An optimized, faster (GPU) FEM approximation combined with modality-specific self-supervised latent representations and small supervised latent-space projections can achieve successful sim-to-real transfer for tactile sensing: synthesized BioTac electrode signals and reconstructed contact patches generalize to unseen interactions and objects with lower electrode prediction error than a fully-supervised baseline. Crucially, accurate contact/deformation modeling (even when approximated) plus calibration to higher-fidelity reference (ANSYS) enable transfer; missing physics (internal fluid, non-linear materials) can be partly compensated by parameter optimization and learned mappings, but limitations (e.g., VAE bottleneck, single-sensor variability) remain and may hinder transfer in broader settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1835.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1835.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GelSight-Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GelSight simulation in Gazebo for sim-to-real learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work mentioned: GelSight visual tactile output was simulated using a depth camera in Gazebo with a calibrated reflectance model and blurring to approximate gel contact; evaluation was limited per the paper's related-work summary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GelSight simulation for sim2real learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>GelSight vision-based tactile sensor (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>High-resolution vision-based tactile sensor producing images of gel deformation for estimating geometry and force; simulated in Gazebo with optics/reflection approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>vision-based tactile sensing / robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Robot simulator used with a depth camera, calibrated reflectance model and blurring to approximate gel contact imagery; primarily models geometry and approximate optical rendering of tactile images.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate photometric/optical simulation (not full multiphysics FEM of gel), limited quantitative evaluation reported</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>visual output via depth-camera-based rendering and reflectance model, approximate gel contact appearance via blurring</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No accurate FEM of gel deformation or fluid/elastic multiphysics; approximations introduced via blurring and reflectance calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Not detailed in this paper; referenced work reported limited quantitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Perception/regression tasks on vision-based tactile images (general sim-to-real for tactile imagery), specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Photometric/appearance mismatches and lack of accurate gel deformation modeling implied as limitations (paper states quantitative evaluation limited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an early attempt to simulate vision-based tactile sensors using approximate rendering in Gazebo; quantitative evaluation limited, implying limited transfer validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1835.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1835.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TacTip-Unity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TacTip simulation in Unity with approximate deformation model and domain randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work mentioned: TacTip pin locations were simulated in Unity with an approximate deformation model and domain randomization; a network trained on synthetic data regressed contact location/angle with moderate accuracy for a limited sensor/contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer for optical tactile sensing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>TacTip optical tactile sensor (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An optical tactile sensor using internal pins whose movements are tracked visually to infer contact; simulated with approximate deformation in Unity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>vision-based tactile sensing / robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Unity</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Game-engine environment used with an approximate deformation model and rendering of pin positions; domain randomization applied.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate deformation and rendering with domain randomization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>pin location rendering and approximate deformation effects</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simplified deformation physics (approximate model), limited contact orientations tested, tuned parameters for plausibility rather than physics fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Experiments on a large TacTip sensor with limited orientations of contact; tested on limited dataset (not fully detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Regression to contact location and angle from simulated tactile images</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning on synthetic data (U-net/FCN style architectures implied in related work summary)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Position error and angular error (reported in paper summary: min error 0.5–0.7 mm and 0.25 rad for their setup)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported minimum errors in cited work: ~0.5–0.7 mm and 0.25 rad (from related-work summary)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Domain randomization applied (details not provided in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Simplified physics and limited coverage of contact orientations; sensor scale and limited contact modes noted.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Domain randomization to improve robustness, parameter tuning for plausible outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Approximate simulation with domain randomization can produce reasonable contact location/angle estimates for limited TacTip scenarios, but generalization is constrained by sensor size and limited contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1835.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1835.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BarrettHand-PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BarrettHand capacitive sensor simulation in PyBullet with RL policy transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work mentioned: BarrettHand capacitive tactile array electrical output was simulated in PyBullet; reinforcement learning policies for stable grasps were trained in simulation and transferred to the real robot, though tactile signals were binarized limiting precision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MAT: Multi-fingered adaptive tactile grasping via deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>BarrettHand capacitive tactile array on Barrett robotic hand (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Multi-fingered robotic hand equipped with capacitive tactile sensor array used for grasping; tactile signals thresholded to binary for control.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic grasping / tactile sensing</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics simulator modeling soft contact used to simulate tactile sensor array outputs for training RL grasp policies.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate soft contact simulation sufficient for RL policy learning; tactile outputs simplified (binary thresholding applied).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>soft contact interactions approximated in PyBullet, generating tactile-like signals for control policies.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Electrical tactile signals binarized via thresholding (loss of precision); likely simplified contact models relative to high-fidelity FEM.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Policies transferred to real BarrettHand for stable grasping tasks (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Stable grasping control policies learned via RL in simulation and transferred to real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (trained in PyBullet simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Limited tactile signal fidelity due to binary thresholding and approximate contact simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Sufficient policy robustness and simulated tactile feedback (even binarized) to enable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simulated tactile feedback, even if coarsely represented (binary), can support RL policy transfer for grasping, but precision is limited by signal processing choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1835.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1835.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERT-sensor-FEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Electrical Resistance Tomography (ERT) tactile sensor simulated by simplified FEM with multiphysics model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work mentioned: An ERT tactile sensor was simulated using a simplified FEM deformation model, an empirical piezoresistive model, and an FEM conductivity model; FCNs trained on synthetic data predicted contact location and force with moderate success (82% contact detection, mean force error 0.51 N; error rose on unseen scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Calibrating a soft ERT-based tactile sensor with a multiphysics model and sim-to-real transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Electrical Resistance Tomography (ERT) tactile sensor (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Soft tactile sensor whose internal conductivity distribution is used to infer contact; simulated via simplified FEM plus empirical conductivity/piezoresistive modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>tactile sensing / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simplified FEM deformation + empirical piezoresistive model + FEM conductivity modeling to simulate sensor outputs for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>multiphysics approximate (coupled deformation and conductivity models but simplified compared to full physics), used for synthetic data generation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>deformation (FEM), piezoresistive response (empirical), conductivity distribution (FEM-based).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simplified deformation model and empirical approximations; performance degraded on unseen contact scenarios (force error increased to 5.0 N).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real ERT tactile sensor evaluated for contact detection and force estimation in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Predict discrete contact location and resultant force from synthetic data to real sensor.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning (FCNs trained on synthetic data).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Contact detection success rate and mean force error (reported: 82% contact detection, mean force error 0.51 N; increased to 5.0 N for unseen scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Mean force error 0.51 N on seen scenarios, increased to 5.0 N on unseen contact scenarios (as reported in paper summary).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Simplified physics models and empirical approximations causing large errors on unseen scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of multiphysics modeling and FCN regression provided reasonable performance for seen scenarios but poor generalization to novel contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multiphysics-informed simulation can yield useful synthetic data for contact detection and force estimation, but simplified models may not generalize well to unseen contacts without further adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1835.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1835.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ruppel-BioTac</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation of the SynTouch BioTac sensor (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work mentioned: prior BioTac simulations include an approximate contact model in Gazebo and other approaches; one cited simulation used estimated force and real-world contact location to predict electrode outputs for a simple single-indenter case.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simulation of the SynTouch BioTac sensor</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>SynTouch BioTac (prior simulation work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Prior attempts to simulate BioTac electrical outputs using approximate contact models and learned regressors; limited to simple contact cases in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>tactile sensing / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo (as used in cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Approximate contact modeling environment used to estimate forces and contact locations, with a learned regressor to produce electrode outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate contact model (not full FEM); evaluated on single spherical indenter case</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>estimated force and contact location to predict electrodes</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No full FEM deformation or multiphysics modeling; limited variability in training/testing (single indenter).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real BioTac electrode data used in cited evaluation; mean difference reported for a location estimator of 0.83 mm in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Regression from estimated force + real contact location to electrode outputs (forward synthesis) for limited cases.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning using regressors trained on synthetic/estimated inputs (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Mean difference in contact location estimation (0.83 mm reported), limited quantitative reporting otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Simplified contact model and limited contact scenarios restrict generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior BioTac simulation efforts achieved limited forward synthesis for simple contact cases; contrast underscores the contribution of the current paper in combining FEM, self-supervision, and latent projection for broader generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1835.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1835.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zapata-Impata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation of tactile data from 3D vision and target robotic grasps / Prediction of tactile perception from vision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related works mentioned that generate tactile data from vision and grasp parameters or explore semi-supervised learning for electrode prediction; these works reported relatively high electrode prediction errors for unseen samples and objects compared to the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prediction of tactile perception from vision on deformable objects</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Vision-to-tactile models (prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Approaches that regress from RGB-D images and grasp parameters to tactile readings or predict tactile perception from vision; some used PointNet or semi-supervised methods.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>vision and tactile cross-modal prediction / robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Not specified in detail in this paper; work involves mapping vision (RGB-D) and grasp parameters to tactile outputs, sometimes using synthetic or real datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>varied; not fully detailed here; results reported as having relatively high electrode prediction errors on unseen samples/objects.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>RGB-D perception and grasp parameters mapped to tactile readings.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>High electrode prediction errors suggest limitations in modeling or data coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real tactile datasets used in evaluation (details in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Predicting electrode signals from vision/grasp inputs; semi-supervised learning approaches explored.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised and semi-supervised learning approaches reported in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Electrode prediction errors; cited comparison: prior work errors were high (e.g., 195–305 raw units in a different referenced baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>High prediction errors for unseen cases indicate sim-to-real/generalization challenges linked to modeling choices or dataset coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior vision-to-tactile prediction approaches showed limited fidelity for unseen objects/samples; the present paper reports substantially lower electrode errors using FEM + latent projection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Interpreting and predicting tactile signals via a physics-based and data-driven framework <em>(Rating: 2)</em></li>
                <li>Interpreting and predicting tactile signals for the SynTouch Biotac <em>(Rating: 2)</em></li>
                <li>GelSight simulation for sim2real learning <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer for optical tactile sensing <em>(Rating: 2)</em></li>
                <li>MAT: Multi-fingered adaptive tactile grasping via deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Calibrating a soft ERT-based tactile sensor with a multiphysics model and sim-to-real transfer learning <em>(Rating: 2)</em></li>
                <li>Simulation of the SynTouch BioTac sensor <em>(Rating: 2)</em></li>
                <li>Prediction of tactile perception from vision on deformable objects <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1835",
    "paper_id": "paper-6d8952147b5e9502baf062df83a82cb84d224ea6",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "BioTac-LP",
            "name_full": "Sim-to-Real for SynTouch BioTac via FEM + Latent Projections",
            "brief_description": "This paper develops a GPU-based 3D FEM model of the SynTouch BioTac in Isaac Gym, learns modality-specific VAEs for FEM deformations and real electrode signals, and trains small supervised latent-space projection networks to perform cross-modal sim-to-real transfer: synthesizing real BioTac electrical outputs from simulated deformations and reconstructing contact patches from real signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "SynTouch BioTac tactile sensor (single physical sensor in experiments)",
            "agent_system_description": "A biomimetic tactile fingertip sensor that transduces skin deformation to electrical (fluidic impedance) signals across 19 electrodes; used here instrumented on a robotic finger for indentation/contact experiments.",
            "domain": "general robotics manipulation / tactile sensing",
            "virtual_environment_name": "Isaac Gym (NVIDIA GPU-based robotics simulator)",
            "virtual_environment_description": "GPU-based simulator performing 3D finite element method (FEM) with co-rotational linear-elastic constitutive model, isotropic Coulomb contact, implicit time integration and a GPU Newton solver; returns nodal positions, nodal contact forces, and element stresses.",
            "simulation_fidelity_level": "high-fidelity physics-style FEM within the constraints of Isaac Gym's linear-elastic deformable model (accurate deformation/contact but approximated physics relative to full ANSYS Neo-Hookean + fluid model)",
            "fidelity_aspects_modeled": "contact dynamics (Coulomb frictional contact), nodal deformation fields, nodal contact forces, element stress tensors (via 3D FEM), geometric fidelity of BioTac skin mesh (4000 nodes), multiple contact scenarios across 17 objects.",
            "fidelity_aspects_simplified": "skin material modeled as compressible linear-elastic (not Neo-Hookean), internal fluid not modeled, collisions resolved with boundary-layer expanded meshes rather than normal-Lagrange method, linear tetrahedral elements only (limitations of Isaac Gym).",
            "real_environment_description": "Physical experiments with a single SynTouch BioTac sensor performing kinematically-randomized indentations and controlled indentations matching simulation sequences over 17 objects and table features; collected 19-electrode impedance signals for training/validation/testing.",
            "task_or_skill_transferred": "Synthesis of BioTac electrical signals from simulated FEM deformations (forward mapping) and reconstruction/estimation of contact patch/deformation fields from real electrode signals (inverse mapping); generalization to unseen contact interactions and unseen objects.",
            "training_method": "Self-supervised representation learning (VAEs) on large unlabeled datasets for each modality, followed by supervised training of small MLP/FCN projection networks on a small paired (simulated FEM, real electrode) labeled dataset.",
            "transfer_success_metric": "RMS error over all electrodes (normalized to [-1,1] and reported also in raw digital units) and ℓ1/coverage plots; also ℓ2-norm force-deflection errors used for FEM↔ANSYS validation; visual/qualitative contact patch agreement for free-form interactions.",
            "transfer_performance_sim": "FEM vs ANSYS: mean ℓ2-norm force error across indenters = 0.153 N; deformation field mean errors: mean error between maximum nodal displacement norms = 1.41e-4 m and between mean norms = 1.81e-5 m. (These quantify simulation fidelity relative to ANSYS.)",
            "transfer_performance_real": "Electrode prediction median RMS errors (normalized): Unseen Trajectory: Latent Projection 0.010 (≈20 raw units); Fully Supervised 0.012 (≈25 raw units). Unseen Object: Latent Projection 0.015 (≈31 units); Fully Supervised 0.016 (≈32 units). (Coverage plots and qualitative contact-patch matches reported.)",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": "Not performed beyond the learned latent-space projection; the paper explicitly states 'we do not perform domain adaptation beyond projection.'",
            "sim_to_real_gap_factors": "Approximations in simulator physics (linear-elastic skin, missing internal fluid), differences in material/friction parameters (optimized to compensate), manufacturing variability across sensors (only one BioTac evaluated), and limits of the mesh VAE latent bottleneck (affecting multimodal deformation distributions).",
            "transfer_enabling_conditions": "High-fidelity geometric FEM meshes and optimization of material/friction parameters to reproduce validated ANSYS force-deflection profiles; use of large unlabeled simulation and real datasets to learn modality-specific VAEs (self-supervision) and a small paired labeled set to learn latent projections; GPU-based fast simulation enabling large-scale data generation (75x speedup vs ANSYS).",
            "fidelity_requirements_identified": "Contact dynamics and accurate nodal deformation fields are critical: reproducing ANSYS force-deflection profiles and deformation fields (mean ℓ2 errors reported) enabled downstream transfer. The paper notes that compensating for missing physics (linear elasticity and no fluid) required adjusting elastic modulus, compressibility, and friction; thus adequate expressivity in simulator plus calibration is required.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "No online fine-tuning reported; a small supervised paired dataset (359 resimulated indentations matched to experimental data) was used to train projection networks, but no additional real-world fine-tuning/adaptation beyond that pairing was described.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared Isaac Gym (linear-elastic, GPU) to ANSYS (Neo-Hookean, CPU): Isaac Gym reproduced ANSYS results with mean ℓ2 force error across indenters = 0.153 N and nodal deformation field errors (max and mean norms) on the order of 1e-4–1e-5 m; simulation speed: ANSYS ~7.08 min/indentation on 6 CPUs vs Isaac Gym ~5.57 s/indentation using 8 parallel environments on 1 GPU (≈75x speedup).",
            "key_findings": "An optimized, faster (GPU) FEM approximation combined with modality-specific self-supervised latent representations and small supervised latent-space projections can achieve successful sim-to-real transfer for tactile sensing: synthesized BioTac electrode signals and reconstructed contact patches generalize to unseen interactions and objects with lower electrode prediction error than a fully-supervised baseline. Crucially, accurate contact/deformation modeling (even when approximated) plus calibration to higher-fidelity reference (ANSYS) enable transfer; missing physics (internal fluid, non-linear materials) can be partly compensated by parameter optimization and learned mappings, but limitations (e.g., VAE bottleneck, single-sensor variability) remain and may hinder transfer in broader settings.",
            "uuid": "e1835.0",
            "source_info": {
                "paper_title": "Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "GelSight-Gazebo",
            "name_full": "GelSight simulation in Gazebo for sim-to-real learning",
            "brief_description": "Related work mentioned: GelSight visual tactile output was simulated using a depth camera in Gazebo with a calibrated reflectance model and blurring to approximate gel contact; evaluation was limited per the paper's related-work summary.",
            "citation_title": "GelSight simulation for sim2real learning",
            "mention_or_use": "mention",
            "agent_system_name": "GelSight vision-based tactile sensor (mentioned in related work)",
            "agent_system_description": "High-resolution vision-based tactile sensor producing images of gel deformation for estimating geometry and force; simulated in Gazebo with optics/reflection approximations.",
            "domain": "vision-based tactile sensing / robotics manipulation",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "Robot simulator used with a depth camera, calibrated reflectance model and blurring to approximate gel contact imagery; primarily models geometry and approximate optical rendering of tactile images.",
            "simulation_fidelity_level": "approximate photometric/optical simulation (not full multiphysics FEM of gel), limited quantitative evaluation reported",
            "fidelity_aspects_modeled": "visual output via depth-camera-based rendering and reflectance model, approximate gel contact appearance via blurring",
            "fidelity_aspects_simplified": "No accurate FEM of gel deformation or fluid/elastic multiphysics; approximations introduced via blurring and reflectance calibration.",
            "real_environment_description": "Not detailed in this paper; referenced work reported limited quantitative evaluation.",
            "task_or_skill_transferred": "Perception/regression tasks on vision-based tactile images (general sim-to-real for tactile imagery), specifics not provided in this paper.",
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Photometric/appearance mismatches and lack of accurate gel deformation modeling implied as limitations (paper states quantitative evaluation limited).",
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Mentioned as an early attempt to simulate vision-based tactile sensors using approximate rendering in Gazebo; quantitative evaluation limited, implying limited transfer validation.",
            "uuid": "e1835.1",
            "source_info": {
                "paper_title": "Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "TacTip-Unity",
            "name_full": "TacTip simulation in Unity with approximate deformation model and domain randomization",
            "brief_description": "Related work mentioned: TacTip pin locations were simulated in Unity with an approximate deformation model and domain randomization; a network trained on synthetic data regressed contact location/angle with moderate accuracy for a limited sensor/contacts.",
            "citation_title": "Sim-to-real transfer for optical tactile sensing",
            "mention_or_use": "mention",
            "agent_system_name": "TacTip optical tactile sensor (mentioned in related work)",
            "agent_system_description": "An optical tactile sensor using internal pins whose movements are tracked visually to infer contact; simulated with approximate deformation in Unity.",
            "domain": "vision-based tactile sensing / robotics manipulation",
            "virtual_environment_name": "Unity",
            "virtual_environment_description": "Game-engine environment used with an approximate deformation model and rendering of pin positions; domain randomization applied.",
            "simulation_fidelity_level": "approximate deformation and rendering with domain randomization",
            "fidelity_aspects_modeled": "pin location rendering and approximate deformation effects",
            "fidelity_aspects_simplified": "Simplified deformation physics (approximate model), limited contact orientations tested, tuned parameters for plausibility rather than physics fidelity.",
            "real_environment_description": "Experiments on a large TacTip sensor with limited orientations of contact; tested on limited dataset (not fully detailed here).",
            "task_or_skill_transferred": "Regression to contact location and angle from simulated tactile images",
            "training_method": "Supervised learning on synthetic data (U-net/FCN style architectures implied in related work summary)",
            "transfer_success_metric": "Position error and angular error (reported in paper summary: min error 0.5–0.7 mm and 0.25 rad for their setup)",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Reported minimum errors in cited work: ~0.5–0.7 mm and 0.25 rad (from related-work summary)",
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Domain randomization applied (details not provided in this paper's summary).",
            "sim_to_real_gap_factors": "Simplified physics and limited coverage of contact orientations; sensor scale and limited contact modes noted.",
            "transfer_enabling_conditions": "Domain randomization to improve robustness, parameter tuning for plausible outputs.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Approximate simulation with domain randomization can produce reasonable contact location/angle estimates for limited TacTip scenarios, but generalization is constrained by sensor size and limited contacts.",
            "uuid": "e1835.2",
            "source_info": {
                "paper_title": "Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "BarrettHand-PyBullet",
            "name_full": "BarrettHand capacitive sensor simulation in PyBullet with RL policy transfer",
            "brief_description": "Related work mentioned: BarrettHand capacitive tactile array electrical output was simulated in PyBullet; reinforcement learning policies for stable grasps were trained in simulation and transferred to the real robot, though tactile signals were binarized limiting precision.",
            "citation_title": "MAT: Multi-fingered adaptive tactile grasping via deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_system_name": "BarrettHand capacitive tactile array on Barrett robotic hand (mentioned in related work)",
            "agent_system_description": "Multi-fingered robotic hand equipped with capacitive tactile sensor array used for grasping; tactile signals thresholded to binary for control.",
            "domain": "robotic grasping / tactile sensing",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "Physics simulator modeling soft contact used to simulate tactile sensor array outputs for training RL grasp policies.",
            "simulation_fidelity_level": "approximate soft contact simulation sufficient for RL policy learning; tactile outputs simplified (binary thresholding applied).",
            "fidelity_aspects_modeled": "soft contact interactions approximated in PyBullet, generating tactile-like signals for control policies.",
            "fidelity_aspects_simplified": "Electrical tactile signals binarized via thresholding (loss of precision); likely simplified contact models relative to high-fidelity FEM.",
            "real_environment_description": "Policies transferred to real BarrettHand for stable grasping tasks (details in cited work).",
            "task_or_skill_transferred": "Stable grasping control policies learned via RL in simulation and transferred to real robot.",
            "training_method": "Reinforcement learning (trained in PyBullet simulation).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Limited tactile signal fidelity due to binary thresholding and approximate contact simulation.",
            "transfer_enabling_conditions": "Sufficient policy robustness and simulated tactile feedback (even binarized) to enable transfer.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Simulated tactile feedback, even if coarsely represented (binary), can support RL policy transfer for grasping, but precision is limited by signal processing choices.",
            "uuid": "e1835.3",
            "source_info": {
                "paper_title": "Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "ERT-sensor-FEM",
            "name_full": "Electrical Resistance Tomography (ERT) tactile sensor simulated by simplified FEM with multiphysics model",
            "brief_description": "Related work mentioned: An ERT tactile sensor was simulated using a simplified FEM deformation model, an empirical piezoresistive model, and an FEM conductivity model; FCNs trained on synthetic data predicted contact location and force with moderate success (82% contact detection, mean force error 0.51 N; error rose on unseen scenarios).",
            "citation_title": "Calibrating a soft ERT-based tactile sensor with a multiphysics model and sim-to-real transfer learning",
            "mention_or_use": "mention",
            "agent_system_name": "Electrical Resistance Tomography (ERT) tactile sensor (mentioned in related work)",
            "agent_system_description": "Soft tactile sensor whose internal conductivity distribution is used to infer contact; simulated via simplified FEM plus empirical conductivity/piezoresistive modeling.",
            "domain": "tactile sensing / robotics",
            "virtual_environment_name": null,
            "virtual_environment_description": "Simplified FEM deformation + empirical piezoresistive model + FEM conductivity modeling to simulate sensor outputs for learning.",
            "simulation_fidelity_level": "multiphysics approximate (coupled deformation and conductivity models but simplified compared to full physics), used for synthetic data generation.",
            "fidelity_aspects_modeled": "deformation (FEM), piezoresistive response (empirical), conductivity distribution (FEM-based).",
            "fidelity_aspects_simplified": "Simplified deformation model and empirical approximations; performance degraded on unseen contact scenarios (force error increased to 5.0 N).",
            "real_environment_description": "Real ERT tactile sensor evaluated for contact detection and force estimation in cited work.",
            "task_or_skill_transferred": "Predict discrete contact location and resultant force from synthetic data to real sensor.",
            "training_method": "Supervised learning (FCNs trained on synthetic data).",
            "transfer_success_metric": "Contact detection success rate and mean force error (reported: 82% contact detection, mean force error 0.51 N; increased to 5.0 N for unseen scenarios).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Mean force error 0.51 N on seen scenarios, increased to 5.0 N on unseen contact scenarios (as reported in paper summary).",
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Simplified physics models and empirical approximations causing large errors on unseen scenarios.",
            "transfer_enabling_conditions": "Use of multiphysics modeling and FCN regression provided reasonable performance for seen scenarios but poor generalization to novel contacts.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Multiphysics-informed simulation can yield useful synthetic data for contact detection and force estimation, but simplified models may not generalize well to unseen contacts without further adaptation.",
            "uuid": "e1835.4",
            "source_info": {
                "paper_title": "Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Ruppel-BioTac",
            "name_full": "Simulation of the SynTouch BioTac sensor (prior work)",
            "brief_description": "Related work mentioned: prior BioTac simulations include an approximate contact model in Gazebo and other approaches; one cited simulation used estimated force and real-world contact location to predict electrode outputs for a simple single-indenter case.",
            "citation_title": "Simulation of the SynTouch BioTac sensor",
            "mention_or_use": "mention",
            "agent_system_name": "SynTouch BioTac (prior simulation work)",
            "agent_system_description": "Prior attempts to simulate BioTac electrical outputs using approximate contact models and learned regressors; limited to simple contact cases in cited work.",
            "domain": "tactile sensing / robotics",
            "virtual_environment_name": "Gazebo (as used in cited prior work)",
            "virtual_environment_description": "Approximate contact modeling environment used to estimate forces and contact locations, with a learned regressor to produce electrode outputs.",
            "simulation_fidelity_level": "approximate contact model (not full FEM); evaluated on single spherical indenter case",
            "fidelity_aspects_modeled": "estimated force and contact location to predict electrodes",
            "fidelity_aspects_simplified": "No full FEM deformation or multiphysics modeling; limited variability in training/testing (single indenter).",
            "real_environment_description": "Real BioTac electrode data used in cited evaluation; mean difference reported for a location estimator of 0.83 mm in cited work.",
            "task_or_skill_transferred": "Regression from estimated force + real contact location to electrode outputs (forward synthesis) for limited cases.",
            "training_method": "Supervised learning using regressors trained on synthetic/estimated inputs (details in cited work).",
            "transfer_success_metric": "Mean difference in contact location estimation (0.83 mm reported), limited quantitative reporting otherwise.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Simplified contact model and limited contact scenarios restrict generalization.",
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Prior BioTac simulation efforts achieved limited forward synthesis for simple contact cases; contrast underscores the contribution of the current paper in combining FEM, self-supervision, and latent projection for broader generalization.",
            "uuid": "e1835.5",
            "source_info": {
                "paper_title": "Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Zapata-Impata",
            "name_full": "Generation of tactile data from 3D vision and target robotic grasps / Prediction of tactile perception from vision",
            "brief_description": "Related works mentioned that generate tactile data from vision and grasp parameters or explore semi-supervised learning for electrode prediction; these works reported relatively high electrode prediction errors for unseen samples and objects compared to the present paper.",
            "citation_title": "Prediction of tactile perception from vision on deformable objects",
            "mention_or_use": "mention",
            "agent_system_name": "Vision-to-tactile models (prior works)",
            "agent_system_description": "Approaches that regress from RGB-D images and grasp parameters to tactile readings or predict tactile perception from vision; some used PointNet or semi-supervised methods.",
            "domain": "vision and tactile cross-modal prediction / robotics manipulation",
            "virtual_environment_name": null,
            "virtual_environment_description": "Not specified in detail in this paper; work involves mapping vision (RGB-D) and grasp parameters to tactile outputs, sometimes using synthetic or real datasets.",
            "simulation_fidelity_level": "varied; not fully detailed here; results reported as having relatively high electrode prediction errors on unseen samples/objects.",
            "fidelity_aspects_modeled": "RGB-D perception and grasp parameters mapped to tactile readings.",
            "fidelity_aspects_simplified": "High electrode prediction errors suggest limitations in modeling or data coverage.",
            "real_environment_description": "Real tactile datasets used in evaluation (details in referenced work).",
            "task_or_skill_transferred": "Predicting electrode signals from vision/grasp inputs; semi-supervised learning approaches explored.",
            "training_method": "Supervised and semi-supervised learning approaches reported in cited works.",
            "transfer_success_metric": "Electrode prediction errors; cited comparison: prior work errors were high (e.g., 195–305 raw units in a different referenced baseline).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "High prediction errors for unseen cases indicate sim-to-real/generalization challenges linked to modeling choices or dataset coverage.",
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Prior vision-to-tactile prediction approaches showed limited fidelity for unseen objects/samples; the present paper reports substantially lower electrode errors using FEM + latent projection.",
            "uuid": "e1835.6",
            "source_info": {
                "paper_title": "Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Interpreting and predicting tactile signals via a physics-based and data-driven framework",
            "rating": 2
        },
        {
            "paper_title": "Interpreting and predicting tactile signals for the SynTouch Biotac",
            "rating": 2
        },
        {
            "paper_title": "GelSight simulation for sim2real learning",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer for optical tactile sensing",
            "rating": 2
        },
        {
            "paper_title": "MAT: Multi-fingered adaptive tactile grasping via deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Calibrating a soft ERT-based tactile sensor with a multiphysics model and sim-to-real transfer learning",
            "rating": 2
        },
        {
            "paper_title": "Simulation of the SynTouch BioTac sensor",
            "rating": 2
        },
        {
            "paper_title": "Prediction of tactile perception from vision on deformable objects",
            "rating": 2
        }
    ],
    "cost": 0.0161495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections</h1>
<p>Yashraj Narang<em>1, Balakumar Sundaralingam</em>1, Miles Macklin ${ }^{1}$, Arsalan Mousavian ${ }^{1}$, Dieter Fox ${ }^{1,2}$</p>
<h4>Abstract</h4>
<ul>
<li>These authors contributed equally. ${ }^{1}$ NVIDIA Corporation, Seattle, USA. ${ }^{2}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington, Seattle, USA.</li>
</ul>
<p>Fig. 1: Overview. We develop an efficient 3D FEM model of a SynTouch BioTac sensor to simulate contact interactions, and we conduct similar real-world experiments. In a learning phase, we train autoencoders to reconstruct unlabeled FEM deformations and real-world electrical signals. With a small amount of labeled data, we subsequently train MLPs to project between the FEM and electrical latent spaces. At test time, we use these learned latent projections to perform cross-modal transfer between FEM and electrical data for unseen contact interactions. During downstream application, we 1) accurately synthesize BioTac electrical signals, and 2) estimate the shape and location of contact patches, facilitating real-world task execution.</p>
<p>Abstract-Tactile sensing is critical for robotic grasping and manipulation of objects under visual occlusion. However, in contrast to simulations of robot arms and cameras, current simulations of tactile sensors have limited accuracy, speed, and utility. In this work, we develop an efficient 3D finite element method (FEM) model of the SynTouch BioTac sensor using an open-access, GPU-based robotics simulator. Our simulations closely reproduce results from an experimentallyvalidated model in an industry-standard, CPU-based simulator, but at 75x the speed. We then learn latent representations for simulated BioTac deformations and real-world electrical output through self-supervision, as well as projections between the latent spaces using a small supervised dataset. Using these learned latent projections, we accurately synthesize real-world BioTac electrical output and estimate contact patches, both for unseen contact interactions. This work contributes an efficient, freely-accessible FEM model of the BioTac and comprises one of the first efforts to combine self-supervision, cross-modal transfer, and sim-to-real transfer for tactile sensors.</p>
<h2>I. INTRODUCTION</h2>
<p>Tactile sensing is critical for grasping and manipulation under visual occlusion, as well as for handling delicate objects [1]. For example, humans leverage tactile sensing when retrieving keys, striking a match, holding a wine glass, and grasping fresh fruit without damage. In robotics, researchers are actively developing a wide variety of tactile sensors (e.g., [2]-[13]). These sensors have been used for tasks such as slip detection [14]-[16], object classification [17], [18], parameter estimation [19], force estimation [20][23], contour following [24], and reacting to humans [25].</p>
<p>For other aspects of robotics, such as robot kinematics, dynamics, and cameras, accurate and efficient simulators</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" />
have advanced the state-of-the-art in task performance. For example, simulators have enabled accurate testing of algorithms for perception, localization, planning, and control [26]; generation of synthetic datasets for learning such algorithms [27]-[29]; efficient training of control policies via reinforcement learning (RL) [30]-[32]; and execution of online algorithms, with the simulator as a model [33]. These capabilities have in turn reduced the need for costly, timeconsuming, dangerous, or intractable experiments.</p>
<p>However, simulators for tactile sensing are still nascent. For the SynTouch BioTac sensor [2], [3], as well as visionbased tactile sensors, most simulation studies have focused on the inverse problem of interpreting sensor output in terms of physical quantities (e.g., [20], [22], [23], [34]). Far fewer efforts have addressed the forward problem of synthesizing sensor output, and perhaps none have accurately generalized to diverse contact scenarios. Forward simulation is invaluable for simulation-based training, which coupled with domain adaptation, can enable effective policy generation.</p>
<p>The dearth of tactile simulation capabilities is a result of its inherent challenges. Accurate tactile sensor simulators must model numerous contacts, complex geometries, and elastic deformation, which can be computationally prohibitive [35]. Simulators must also capture multiphysics behavior, as tactile sensors are cross-modal: for instance, the BioTac transduces skin deformations to fluidic impedances. Furthermore, whereas a small parameter set (e.g., camera intrinsics) can describe variation among visual sensors, no equivalent exists for tactile sensors. Due to manufacturing variability, even sensors of the same type can behave disparately [22], [36].</p>
<p>In this work, we address forward simulation and sim-to-real transfer for the BioTac (Fig. 1). We first develop 3D finite element method (FEM) simulations of the BioTac</p>
<p>using a GPU-based robotics simulator [37]; the FEM simulations predict contact forces and deformation fields for the sensor for arbitrary contact interactions. These simulations are designed to reproduce our previous results [22], [23], which utilized an industry-standard, commercial, CPU-based simulator and were carefully validated against real-world experiments. However, the new simulator is freely accessible, and the simulations execute 75x faster.</p>
<p>We then map FEM output to real-world BioTac electrical signals by leveraging recent methods in self-supervised representation learning. Specifically, we collect a large unlabeled dataset of sensor deformation fields from simulation, as well as a smaller dataset of electrical signals from real-world experiments; we then learn latent representations for each modality using variational autoencoders (VAE) [38], [39]. Next, we learn a cross-modal projection between the latent spaces using a small amount of supervised data. We demonstrate that this learned latent projection allows us to accurately predict BioTac electrical signals from simulated deformation fields for unseen contact interactions, including unseen objects. We can also execute the inverse mapping (from signals to deformations) with higher fidelity than in [22], [23], illustrated via a contact-patch estimator.</p>
<p>To summarize, our key contributions are the following:</p>
<ol>
<li>An accurate, efficient, and freely-accessible 3D FEM-based simulation model for the BioTac</li>
<li>A novel application of self-supervision and learned latent-space projections for facilitating cross-modal transfer between FEM and electrical data</li>
<li>Demonstrations of sim-to-real transfer through accurate synthesis of BioTac electrical data and estimation of contact patches, both for unseen contact interactions</li>
</ol>
<p>The simulation model, as well as additional implementation details and analyses, will be posted on our website.</p>
<h2>II. RELATED WORKS</h2>
<p>In this section, we review research efforts in sim-to-real transfer, self-supervision, and cross-modal transfer involving tactile sensors that are widely used in current robotics projects. For a recent comprehensive review, see [40].</p>
<h3>A. Sim-to-Real for Vision-Based Tactile Sensors</h3>
<p>In [41], visual output of the GelSight [4] was simulated using a depth camera in Gazebo [42], a calibrated reflectance model, and blurring to approximate gel contact. Quantitative evaluation was limited. In [43], a custom marker-based tactile sensor [9] was simulated using FEM, optics models, and synthetic noise. A U-net was trained on the synthetic data to regress to contact force fields with a resultant error of $0.14N$; almost all training and testing was conducted on 1 normally-oriented indenter. In [44], the pin locations of the TacTip [5] were simulated using an approximate deformation model in Unity [45]. Parameters were tuned for a plausible baseline, and domain randomization was performed. A fully connected network (FCN) was trained on synthetic data to regress to contact location and angle with a minimum error of $0.5-0.7mm$ and $0.25rad$. The sensor examined was large, and contact was made over limited orientations.</p>
<p>In comparison to these studies, we use a compact sensor, accurately simulate contact and deformation, do not perform domain adaptation beyond projection, and conduct simulations and experiments on 17 objects over diverse kinematics.</p>
<h3>B. Sim-to-Real for Non-Vision-Based Tactile Sensors</h3>
<p>In [46], the electrical output of the BarrettHand [47] capacitive sensor array was simulated using soft contact in PyBullet [48]. RL policies for stable grasps were trained on the simulator and transferred to the real world. Binary thresholding was applied to tactile signals, limiting precision. In [49], an electrical resistance tomography sensor was simulated using a simplified FEM deformation model, an empirical piezoresistive model, and an FEM conductivity model. FCNs were trained on synthetic data to predict discrete contact location and resultant force, with an $82\%$ success rate for contact and a mean force error of $0.51N$. For unseen contact scenarios, the error increased to $5.0N$.</p>
<p>Finally, for the BioTac sensor, multiple studies have addressed the inverse problem of converting electrical output to physical quantities, such as contact location [22], [23], [50], force [14], [21]–[23], [50], and deformation [22], [23]. In particular, our previous work [22] presented a 3D FEM model of the BioTac, which was built with the industrystandard, CPU-based simulator ANSYS [51] and carefully validated against experimental data. Electrode signals were then mapped to simulator outputs via PointNet++ [52]. The simulations were slow (7min each on 6 CPUs). In addition, the forward problem of synthesizing electrical output was not addressed; some progress was made in our extension [23].</p>
<p>The forward problem has been further explored in [36], [53], [54]. In [53], the BioTac was simulated with an approximate contact model in Gazebo. A deep network was trained to regress from estimated force and real-world contact location to electrical outputs. An existing location estimator was tested on synthetic and real-world data with a mean difference of $0.83mm$. Training and testing was conducted only on 1 spherical indenter. In [36], PointNet was used to regress from RGB-D images and grasp parameters to tactile readings, and in [54], semi-supervised learning was applied. Electrode prediction errors were relatively high for both the simple case of unseen samples, as well as the difficult case of unseen objects; numerical comparison is provided later.</p>
<p>In contrast to the preceding works, we develop an efficient FEM model, conduct simulations and experiments with 17 objects, regress to continuous electrical signals, and demonstrate accurate predictions for unseen objects.</p>
<h3>C. Tactile Self-Supervision and Cross-Modal Transfer</h3>
<p>Numerous studies in robotics have established the utility of multimodal data in task-specific learning. For example, in [55], [56], vision and tactile sensing were combined to predict grasp outcomes and select adjustments, and in [57], vision and force/torque (F/T) sensing were combined to</p>
<p>perform manipulation tasks. In addition, in [58], vision, F/T sensing, and proprioception were used to learn a joint latent space via self-supervision with autoencoders; the output served as perception input to an RL agent for peg-in-hole.</p>
<p>Simultaneously, recent works both outside and within robotics have investigated cross-modal transfer. In [59], audio-video transfer was performed by learning a shared representation via a bimodal autoencoder; networks trained on one modality were then able to classify the other. In [60], audio-image transfer was achieved via generative adversial networks (GAN). In [61], [62], cross-modal transfer was performed between data from cameras and vision-based tactile sensors, and in [36], [54], transfer was achieved between cameras and output from the BioTac SP.</p>
<p>Finally, previous efforts have applied distinct network architectures to encode BioTac-specific data. In our previous work [21]–[23], a 3D voxel-grid network, PointNet++, and an FCN were implemented to encode BioTac electrode data and regress to physical quantities such as forces and deformations. In [63], a 2D CNN was used to predict tactile directionality, and in [64], a graph convolutional network (GCN) was used to predict grasp stability.</p>
<p>We draw upon the preceding works with some distinctions. Analogous to [59], we learn latent representations of FEM and BioTac electrical data via self-supervision with autoencoders for cross-modal transfer. Like [58], we learn modality-specific representations, reducing training time and eliminating zero-inputs for non-present modalities. Unlike both, we never formulate a joint representation, but instead learn a projection between the latent spaces using a small amount of supervised data. To encode BioTac electrical data, we use VAEs, as for vision-based tactile sensors in [6].</p>
<h2>III. METHODS</h2>
<p>In this section, we first discuss our 3D FEM model, which predicts BioTac contact forces and deformation fields for arbitrary contact interactions. We then discuss our implementation of self-supervision and latent-space projection, which can synthesize BioTac electrical output from unlabeled FEM output and predict contact patches from electrical input. Finally, we describe the simulations and experiments used to collect the FEM and electrical data used in this paper.</p>
<h3>III-A. Finite Element Modeling</h3>
<p>FEM is a variational numerical formulation that 1) divides complex global geometries into simple subdomains, 2) solves the weak form of the governing PDEs in each subdomain, and 3) assembles the solutions into a global one. In 3D FEM, objects are represented as volumetric meshes, which consist of 3D elements (e.g., tetrahedrons or hexahedrons) and their associated nodes. With careful model design, high-quality meshes, and small timesteps, FEM predictions for deformable bodies can be exceptionally accurate [65], [66].</p>
<p>In this work, 3D FEM was performed using NVIDIA’s GPU-based Isaac Gym simulator [37]. Isaac Gym models the internal dynamics of deformable bodies using a co-rotational linear-elastic constitutive model; these bodies interact with external rigid objects via an isotropic Coulomb contact model [67]. The resulting nonlinear complementarity problem is solved via implicit time integration using a GPU-based Newton solver [68]. At each timestep, Isaac Gym returns nodal positions (i.e., deformation fields), nodal contact forces (used to compute resultant forces), and element stress tensors.</p>
<p>To create the FEM model for the BioTac, high-resolution triangular meshes for the external and internal surfaces of the BioTac skin were first extracted from the ANSYS model in [22], [23] and simplified via quadric edge collapse decimation in MeshLab [69]. A volumetric mesh was then generated with fTetWild [70]; similar to [22], [23], the mesh had $\approx 4000$ nodes. Fixed boundary conditions (BCs) were applied to 2 sides of the skin to model the BioTac nail and clamp, respectively; these BCs were implemented by introducing thin rigid bodies at the corresponding locations (visible in Fig. 2A-D), which were attached to adjacent nodes on the skin. External rigid objects (e.g., indenters) were driven into the BioTac via a stiff proportional controller.</p>
<p>Relative to the experimentally-validated ANSYS model in [22], [23], the Isaac Gym model makes 3 critical approximations: 1) collisions are resolved via boundary-layer expanded meshes [71], rather than a normal-Lagrange method, 2) a compressible linear-elastic model is used for the skin, rather than a Neo-Hookean model [72], 3) the internal fluid is not modeled. The effects of the first approximation are mitigated by using small collision thicknesses and timesteps (1e-4s). However, the second and third approximations are mitigated by endowing the Isaac Gym model with sufficient expressivity and optimizing it to reproduce ANSYS results.</p>
<p>Specifically, 4 distinct geometric configurations of the BioTac were considered in Isaac Gym (Fig. 2A-D). The material properties (elastic modulus $E$, Poisson’s ratio $\nu$, and friction $\mu$ of the BioTac skin) were designated as free parameters. In [22], [23], 1 shear-rich indentation of the BioTac was used to calibrate the ANSYS model against real-world data; in this work, the same indentation was resimulated in Isaac Gym and used to calibrate the Gym model against ANSYS data (Fig. 2E). For each configuration, the material properties were optimized via sequential least-squares programming to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. Learning structure. To map between FEM deformations and BioTac electrode signals, modality-specific latent representations were learned via self-supervision. Specifically, graph convolutional networks (GCN) compressed deformed meshes with 4000 nodes to a 128-dim. latent space, and fully-connected networks (FCN) compressed the 19 electrode signals to an 8-dim. latent space. Next, FCNs were used on a small supervised dataset to learn forward and inverse projections between the latent spaces.</p>
<p>reproduce the force-deflection profile from ANSYS for the indentation. The cost was defined as the RMS $$ \ell^2 $$-norm of the force error vector over time. Subsequently, each optimized configuration was evaluated by resimulating 358 additional indentations from 8 indenters in [22], [23], and comparing the results to the force-deflection profiles from ANSYS.</p>
<p>Among the 4 configurations, the <em>deformable solid</em> (Fig. 2D) produced the lowest cost during optimization; however, the <em>deformable shell with rigid core</em> (Fig. 2B) produced the lowest cost during evaluation and was thus selected. For this representation, the optimal values of E, $$ \nu $$, and $$ \mu $$ were 1.55e6Pa, 0.316, and 0.783, respectively; the mean $$ \ell^2 $$-norm of the force error vector was 0.125N (Fig. 2E). In comparison, the optimal values for the ANSYS model in [22] were 2.80e5Pa, 0.5, and 0.186, indicating that the Isaac Gym model compensated for its linearity and lack of fluid by increasing elastic modulus, compressibility, and friction.</p>
<h3><em>B. Learning Latent Space Projections</em></h3>
<p>Although FEM captures the effects of contact on BioTac deformations, the BioTac then transduces deformations to fluidic impedances measured at electrodes. Simulating the mapping between deformations and impedances is complex; thus, this mapping was learned. Specifically, modality-specific latent representations were learned using self-supervision, facilitating compression, mitigating noise, and reducing overfitting. Projections were then learned between the latent spaces, enabling cross-modal transfer (Fig. 3).</p>
<p>To learn a latent representation for the FEM deformations, convolutional mesh autoencoders from [39] were trained, which applied graph convolutional networks (GCN) and reduced the mesh data from 4000 nodal positions ($$ R^{4k \times 3} $$) to 128 units. To learn a latent representation for real-world BioTac electrode signals, an autoencoder composed of FCNs was trained, which reduced the electrode data from 19 impedances to 8 units. Latent dimensions were chosen via hyperparameter search. Both networks were trained as VAEs to generate smooth mappings to the latent space [38].</p>
<p>To learn the projections between the latent spaces, 2 FCNs were trained. The first network projected forward from the FEM mesh latent space $$ z_m $$ to the BioTac electrode latent space $$ z_e $$, whereas the second network projected inversely</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. Data collection. A) 17 objects and table features were used to generate data in simulation and the real world. The objects were designed to mimic diverse physical features (e.g., corners, edges, buttons, holes). B-C) The BioTac was constrained when applying kinematically-randomized indentations with the pegs, but unconstrained when contacting table features. from $$ z_e $$ to $$ z_m $$. During training, the previously-described VAEs were frozen and provided with supervised data from [22], generating latent pairs of $$ z_m $$ and $$ z_e $$. These pairs were used to train the projection networks with an RMS loss. Without freezing the VAEs, the networks overfit.</p>
<p>In the FEM deformation VAE, the encoder consisted of an initial convolution with filter size 128, 4 "convolve+downsample" layers with sizes [128, 128, 256, 64] and downsampling factors [4, 4, 4, 2], a convolution with size 64, and 2 fully-connected layers with dimensions [512, 128]. The decoder was symmetric with the encoder, using "upsample+convolve" instead of "convolve+downsample". In the BioTac electrode VAE, the encoder consisted of 4 fully connected layers with [256, 128, 64, 8] neurons, respectively. The decoder was again symmetric with the encoder. The forward and inverse projection networks consisted of 3 fully connected layers with [256, 128, 128] and [128, 128, 256] neurons, respectively, and dropout of 0.3. Exponential linear unit (ELU) activations were applied, and the Adam optimizer was used with initial learning rate of 1e-3 and decay of 0.95.</p>
<h3><em>C. Dataset Collection</em></h3>
<p>For the preceding learning steps, data was collected in both simulation and the real world. For learning the latent representations, unlabeled mesh data was collected by simulating kinematically-randomized interactions on the optimized BioTac model with 6 pegs and 3 table features (surfaces, edges, and corners) in Isaac Gym. Unlabeled experimental electrode data was collected by manually indenting these objects in the real world. For learning the latent projections, labeled data was collected by exactly resimulating 359 indentations from [22], [23] on the optimized BioTac model in Isaac Gym (as stated in Sec. III-A); these were directly matched to corresponding experimental electrode data in the dataset from [22], [23]. 72% of contact interactions were allocated for training, 18% for validation, and 10% for testing.</p>
<p>In total, 2.6k unique contact interactions were executed and 50k timesteps of FEM data were sampled. All objects used in simulation and experiments are shown in Fig. 4. Data collection visualizations are in the supplementary video.</p>
<h3>IV. EXPERIMENTS &amp; RESULTS</h3>
<p>In this section, we present our results on FEM validation, synthesis of BioTac electrode signals from FEM deformations, and contact patch estimation from electrode signals.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Validation of FEM model. The optimized model from Isaac Gym was compared to an experimentally-validated model from ANSYS. Forcedeflection profiles, as well as nodal deformation fields at maximum indentation depth, are illustrated for 6 randomly-selected indentations with 6 different indenters. Nodal deformation fields are colored according to corresponding displacements (i.e., change in nodal positions relative to the no-contact state). The 2 not-pictured indenters are a medium-sized spherical indenter and a small cylindrical indenter. Larger, higher-contrast graphics are on our website.</p>
<h3><em>A. FEM Validation</em></h3>
<p>As described earlier, the force-deflection profiles produced by the optimized model in Isaac Gym were compared to those produced by ANSYS over 358 indentations distributed across 8 indenters (Fig. 5). The mean $$ \ell^2 $$-norm of the force error vectors over all indentations ranged from 0.0876<em>N</em> for a medium-sized cylindrical indenter (<em>less</em> than the training error of 0.125<em>N</em>) to 0.259<em>N</em> for a medium-sized spherical indenter, with a mean of 0.153<em>N</em> across all indenters. Thus, despite being optimized using force-deflection data from only a single indentation, the Isaac Gym model strongly generalized across a diverse range of objects and indentations.</p>
<p>The corresponding FEM deformation fields (i.e., the nodal positions of the deformed FEM meshes) were also compared between Isaac Gym and ANSYS (Fig. 5). For each dataset, the maximum and mean $$ \ell^2 $$-norms of the nodal displacement vectors were computed over all indentations. The mean error between the maximum norms across all indenters was 1.41e−4<em>m</em>, and the mean error between the mean norms was 1.81e−5<em>m</em>. Thus, again, the Isaac Gym model was shown to strongly generalize. These low errors were particularly important, as the nodal deformation fields from Isaac Gym were used as input for subsequent learning.</p>
<p>Finally, simulation speed was compared between the Isaac Gym and ANSYS models. The total simulation time for all 359 indentations was approximately 42 hours (7.08 minutes per sim) on 6 CPUs in ANSYS, but 33 minutes (5.57 seconds per sim) using 8 parallel environments (1 per indenter) on 1 GPU in Isaac Gym. For clarity, Isaac Gym can only currently simulate deformable solids with a linear-elastic model and</p>
<p>linear tetrahedral elements; such a model comprises only a small fraction of those that can be simulated within state-of-the-art FEM software such as ANSYS. However, for the current application, Isaac Gym is highly favorable.</p>
<h3><em>B. Learning-Based Regression and Estimation</em></h3>
<p>For regression from FEM deformations to BioTac electrode signals, 2 learning methods were evaluated: 1) the method proposed in this paper, denoted <em>Latent Projection</em>, which used unlabeled data for latent representation and labeled data for projection, and 2) a PointNet++ baseline [52], denoted <em>Fully Supervised</em>, which used only labeled data, with 128 nodes sampled from the FEM mesh as in [22], [23]. For reference, output is also shown for the <em>Electrode VAE</em>, which is used when training <em>Latent Projection</em>.</p>
<p>When evaluating generalization to novel objects, networks were trained on all objects <em>except</em> the ring (see Fig. 5) and tested on this indenter; the ring has the most unique (thus, challenging) geometry in the dataset. These experiments are denoted "Unseen Object." (Results for other unseen objects are on our website.) When evaluating generalization to novel contact interactions with seen objects, the trained networks are tested on unseen interactions with the other indenters. These experiments are denoted "Unseen Trajectory."</p>
<p>A qualitative comparison of regression results between the learning methods is depicted in Fig. 6A for 2 high-signal, high-variation electrodes over numerous interactions. Raw electrode values were between [0, 4095] digital output units and were tared and normalized to [−1, 1]. For the challenging "Unseen Object" case, <em>Latent Projection</em> could predict several signal peaks over multiple electrodes that <em>Fully</em></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6. Electrode prediction results. A) Visual comparisons for 2 high-magnitude, high-variation electrodes during contact interactions with an unseen object. Electrode range corresponds to a force range of 0.5-19.8<em>N</em>. Each peak corresponds to a distinct interaction. Green and red lines indicate ground-truth and prediction, respectively. Our latent projection (LP) approach can predict peaks more accurately than a fully-supervised (FS) baseline, and often outperforms the VAE used in training. B) RMS error over all electrodes and interactions, for unseen trajectories and objects. Our approach has lower median errors and interquartile ranges. C) Coverage plot, with $$ \ell^1 $$ distance to ground-truth. Our approach has lower errors over nearly the full data distribution.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7. Predicted FEM deformations for unseen trajectories and objects. The columns show the raw FEM output, the output of the mesh VAE, and the prediction from real-world electrode signals through our latent projection. The colorbar shows the Euclidean distance from the undeformed state. Top) "Unseen Trajectory" case, for a randomly-selected indentation. Predictions are consistently accurate. Bottom) "Unseen Object" case, for a randomly-selected indentation with the most distinct indenter, the ring. Predictions do not capture the bimodal deformation distribution due to limitations of the VAE, which has not seen any examples of such distributions in training.</p>
<p><em>Supervised</em> could not capture. Additionally, <em>Latent Projection</em> predictions were consistently noise-free, whereas <em>Fully Supervised</em> ones exhibited low-magnitude, high-frequency noise. Finally, <em>Latent Projection</em> often outperformed <em>Electrode VAE</em>, showing the importance of mesh information in electrode signal synthesis. Predictions for the easier "Unseen Trajectory" case have higher fidelity and are thus not shown.</p>
<p>Quantitative comparisons between the learning methods are illustrated in Fig. 6B-C. RMS errors over all electrodes and interactions are compared in Fig. 6B. The <em>Latent Projection</em> approach performs better than <em>Fully Supervised</em> for both "Unseen Trajectory" and "Unseen Object," in terms of both median error and interquartile range. Median errors for "Unseen Trajectory" were <em>Fully Supervised</em>: 0.012 (25 raw units) and <em>Latent Projection</em>: 0.010 (20 units), and those for "Unseen Object" were 0.016 (32 units) and 0.015 (31 units), respectively. Our errors are substantially lower than the errors from [54], which were 195 units for an easier case of unseen <em>samples</em>, and 305 units for unseen objects. A coverage plot is shown in Fig. 6C, where the ℓ³ distance to the ground-truth electrode signals is depicted. For both "Unseen Trajectory" and "Unseen Object," <em>Latent Projection</em> outperforms <em>Fully Supervised</em> for nearly the entire distribution of data.</p>
<p>For regression from BioTac electrode signals to FEM deformations, we again evaluate <em>Latent Projection</em>. For reference, we also show output of the FEM <em>Mesh VAE</em>, which is used when training <em>Latent Projection</em>. A visual comparison is shown against ground-truth FEM output in Fig. 7 for random indentations. For "Unseen Trajectory", <em>Latent Projection</em> consistently predicted ground-truth, capturing deformation magnitudes and distributions. For "Unseen Object", <em>Latent Projection</em> consistently captured magnitudes and distributions, but not bimodality. As seen from <em>Mesh VAE</em>, this limit is due to the mesh autoencoder (specifically, bottleneck size) rather than the projection. As before, we only show results for the most challenging unseen object, the ring; strong performance on other objects is shown in the video.</p>
<p>As a final demonstration, we also conducted free-form interactions of the BioTac with unseen objects and visualized the estimated contact patches (Fig. 8). For diverse pegs and table features, predicted deformations were visually accurate. For instance, contact patch locations were accurately</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8. Contact patch estimation for free-form interactions. The colorbar shows the Euclidean distance from the undeformed state. Top) Real-world contact with various unseen objects and table features. Bottom) Predicted contact patches using our learned latent projections. Contact patches match contacting features; for example, interactions with the corners of the flat peg and table produce highly-localized, high-magnitude deformation.</p>
<p>predicted across the full spatial limits of the BioTac, and interactions with the corners of a cuboid peg and table showed high-magnitude, highly-localized patch deformations.</p>
<h2>V. DISCUSSION</h2>
<p>In this paper, we present a framework for synthesizing BioTac electrical signals and estimating contact patches for novel contact interactions. The framework consists of 1) a 3D FEM model, which simulates contact between the BioTac and objects and outputs BioTac deformation fields, 2) VAEs that output compressed representations of the deformation fields and electrode signals, and 3) projection networks that perform cross-modal transfer between the representations to facilitate regression of electrode signals or contact patches.</p>
<p>This work has several key contributions. First, compared to our previously-presented, experimentally-validated FEM model [22], the current model is nearly equivalent, available in an open-access robotics simulator, and 75x faster. Second, our work presents one of the first applications of cross-modal self-supervision for tactile sensing; we show that this approach outperforms supervised-only methods for regressing to BioTac electrical signals. Third, for the first time, we accurately predict these signals for unseen interactions, including unseen objects. Finally, we can reconstruct BioTac deformations from real electrical signals with high fidelity.</p>
<p>The present study also has limitations. First, although our FEM model is substantially faster than previous efforts, it currently takes approximately 5.6s to simulate 6mm of indentation, which prohibits dynamic model-predictive control applications. Furthermore, although our networks accurately predicted electrode signals for unseen trajectories and objects, evaluation was performed for 1 BioTac; to compensate for manufacturing variation, unlabeled and labeled data from more BioTacs may be necessary to fine-tune the VAEs and projection networks. Future work will focus on applying our simulation and learning framework to non-BioTac sensors. In the process, we aim to develop powerful, generalized representations of tactile data that can serve as the foundation for transfer learning across sensors of entirely different modalities, such as the BioTac and GelSight.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>We thank V. Makoviychuk, K. Guo, and A. Bakshi for their collaboration with Isaac Gym, as well as K. Van Wyk, A. Handa, and T. Hermans for their feedback.</p>
<h2>REFERENCES</h2>
<p>[1] A. Billard and D. Kragic, "Trends and challenges in robot manipulation," Science, Jun. 2019.
[2] SynTouch. [Online]. Available: https://syntouchinc.com/
[3] N. Wettels, V. J. Santos, R. S. Johansson, and G. E. Loeb, "Biomimetic tactile sensor array," Advanced Robotics, 2008.
[4] W. Yuan, S. Dong, and E. H. Adelson, "GelSight: High-resolution robot tactile sensors for estimating geometry and force," Sensors, Nov. 2017.
[5] B. Ward-Cherrier, N. Pestell, L. Cramphorn, B. Winstone, M. E. Giannaccini, J. Rossiter, and N. F. Lepora, "The TacTip family: Soft optical tactile sensors with 3D-printed biomimetic morphologies," Soft Robotics, Apr. 2018.
[6] M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Maloon, V. R. Most, D. Stroud, R. Santos, A. Byagowi, G. Kammerer, D. Jayaraman, and R. Calandra, "DIGIT: A novel design for a low-cost compact highresolution tactile sensor with application to in-hand manipulation," IEEE Robotics and Automation Letters, Jul. 2020.
[7] A. Alsbach, K. Hashimoto, N. Kuppuswamy, and R. Tedrake, "Softbubble: A highly compliant dense geometry tactile sensor for robot manipulation," in IEEE International Conference on Soft Robotics (RoboSoft), Apr. 2019.
[8] A. Padmanabha, F. Ebert, S. Tian, R. Calandra, C. Finn, and S. Levine, "OmniTact: A multi-directional high resolution touch sensor," in International Conference on Robotics and Automation (ICRA), May 2020.
[9] C. Sferrazza and R. D'Andrea, "Design, motivation and evaluation of a full-resolution optical tactile sensor," Sensors, Feb. 2019.
[10] A. Yamaguchi and C. G. Atkeson, "Combining finger vision and optical tactile sensing: Reducing and handling errors while cutting vegetables," in IEEE-RAS International Conference on Humanoid Robots (Humanoids), Nov. 2016.
[11] I. Huang, J. Liu, and R. Bajcsy, "A depth camera-based soft fingertip device for contact region estimation and perception-action coupling," in IEEE International Conference on Robotics and Automation (ICRA), May 2019.
[12] B. W. McInroe, C. L. Chen, K. Y. Goldberg, R. Bajcsy, and R. S. Fearing, "Towards a soft fingertip with integrated sensing and actuation," in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Oct. 2018.
[13] P. Piacenza, K. Behrman, B. Schifferer, I. Kymissis, and M. Ciocarlie, "A sensorized multicurved robot finger with data-driven touch sensing via overlapping light signals," IEEE/ASME Transactions on Mechatronics, Feb. 2020.
[14] Z. Su, K. Hausman, Y. Chebotar, A. Molchanov, G. E. Loeb, G. S. Sukhatme, and S. Schaal, "Force estimation and slip detection/classification for grip control using a biomimetic tactile sensor," in IEEE-RAS International Conference on Humanoid Robots (Humanoids), Nov. 2015.
[15] F. Veiga, J. Peters, and T. Hermans, "Grip stabilization of novel objects using slip prediction," IEEE Transactions on Haptics, vol. 11, no. 4, 2018.
[16] J. W. James, N. Pestell, and N. F. Lepora, "Slip detection with a biomimetic tactile sensor," IEEE Robotics and Automation Letters, Jul. 2018.
[17] J. Hoelscher, J. Peters, and T. Hermans, "Evaluation of tactile feature extraction for interactive object recognition," in IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2015.
[18] W. Yuan, Y. Mo, S. Wang, and E. H. Adelson, "Active clothing material perception using tactile sensing and deep learning," in IEEE International Conference on Robotics and Automation (ICRA), May 2018.
[19] B. Sundaralingam and T. Hermans, "In-hand object-dynamics inference using tactile fingertips," arXiv preprint arXiv:2003.13165, Mar. 2020.
[20] D. Ma, E. Donlon, S. Dong, and A. Rodriguez, "Dense tactile force estimation using GelSlim and inverse FEM," in IEEE International Conference on Robotics and Automation (ICRA), May 2019.
[21] B. Sundaralingam, A. Lambert, A. Handa, B. Boots, T. Hermans, S. Birchfield, N. Ratliff, and D. Fox, "Robust learning of tactile force estimation through robot interaction," in IEEE International Conference on Robotics and Automation (ICRA), May 2019.
[22] Y. Narang, K. V. Wyk, A. Mousavian, and D. Fox, "Interpreting and predicting tactile signals via a physics-based and data-driven framework," in Robotics: Science and Systems (RSS), Jul. 2020.
[23] Y. Narang, B. Sundaralingam, K. V. Wyk, A. Mousavian, and D. Fox, "Interpreting and predicting tactile signals for the SynTouch Biotac," arXiv preprint arXiv:2101.05452, Jan. 2021.
[24] N. F. Lepora, A. Church, C. de Kerckhove, R. Hadsell, and J. Lloyd, "From pixels to percepts: Highly robust edge perception and contour following using deep learning and an optical biomimetic tactile sensor," IEEE Robotics and Automation Letters, Apr. 2019.
[25] I. Huang and R. Bajcsy, "High resolution soft tactile interface for physical human-robot interaction," in IEEE International Conference on Robotics and Automation (ICRA), May 2020.
[26] A. Afzal, D. S. Katz, C. L. Goues, and C. S. Timperley, "A study on the challenges of using robotics simulators for testing," arXiv, Apr. 2020.
[27] J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and S. Birchfield, "Deep object pose estimation for semantic robotic grasping of household objects," in Conference on Robot Learning (CoRL), Oct. 2018.
[28] C. Matl, Y. Narang, R. Bajcsy, F. Ramos, and D. Fox, "Inferring the material properties of granular media for robotic tasks," in International Conference on Robotics and Automation (ICRA), May 2020.
[29] C. Matl, Y. Narang, D. Fox, R. Bajcsy, and F. Ramos, "STReSSD: Sim-to-real from sound for stochastic dynamics," in Conference on Robot Learning (CoRL), Nov. 2020.
[30] W. Yu, J. Tan, C. K. Liu, and G. Turk, "Preparing for the unknown: Learning a universal policy with online system identification," in Robotics: Science and Systems (RSS), July 2017.
[31] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the sim-to-real loop: Adapting simulation randomization with real world experience," in IEEE International Conference on Robotics and Automation (ICRA), May 2019.
[32] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang, "Solving Rubik's cube with a robot hand," arXiv preprint arXiv:1910.07113, Oct. 2019.
[33] K. Lowrey, S. Kolev, J. Dao, A. Rajeswaran, and E. Todorov, "Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system," in IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR), 2018.
[34] C. Sferrazza, A. Wahlsten, C. Trueeb, and R. D'Andrea, "Ground truth force distribution for learning-based tactile sensing: A finite element approach," IEEE Access, Nov. 2019.
[35] M. M. Zhang, "Necessity for more realistic contact simulation," in Robotics: Science and Systems (RSS) Workshop on Visuotactile Sensors for Robust Manipulation, Jul. 2020.
[36] B. S. Zapata-Impata, P. Gil, Y. Mezouar, and F. Torres, "Generation of tactile data from 3D vision and target robotic grasps," IEEE Transactions on Haptics, 2020.
[37] NVIDIA, "Isaac Gym," https://developer.nvidia.com/isaac-gym, 2020.
[38] D. P. Kingma and M. Welling, "Auto-encoding variational Bayes," in International Conference on Learning Representations (ICLR), Apr. 2014.
[39] A. Ranjan, T. Bolkart, S. Sanyal, and M. J. Black, "Generating 3D faces using convolutional mesh autoencoders," in European Conference on Computer Vision (ECCV), 2018.
[40] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, "A review of tactile information: Perception and action through touch," IEEE Transactions on Robotics, 2020.
[41] D. F. Gomes, A. Wilson, and S. Luo, "GelSight simulation for sim2real learning," in IEEE International Conference on Robotics and Automation (ICRA) Workshop on Integrating Vision and Touch for Multimodal and Cross-modal Perception (ViTac), May 2019.
[42] N. Koenig and A. Howard, "Design and use paradigms for Gazebo, an open-source multi-robot simulator," in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Sep. 2004.
[43] C. Sferrazza, T. Bi, and R. D'Andrea, "Learning the sense of touch in simulation: A sim-to-real strategy for vision-based tactile sensing," in IEEE/RSJ International Conference on Intelligent Robots and Systems, Oct. 2020.
[44] Z. Ding, N. F. Lepora, and E. Johns, "Sim-to-real transfer for optical tactile sensing," in IEEE International Conference on Robotics and Automation (ICRA), May 2020.
[45] A. Juliani, V.-P. Berges, E. Teng, A. Cohen, J. Harper, C. Elion, C. Goy, H. H. Yuan Gao, M. Mattar, and D. Lange, "Unity: A general</p>
<p>platform for intelligent agents," arXiv preprint arXiv:1809.02627v2, May 2020.
[46] B. Wu, I. Akinola, J. Varley, and P. Allen, "MAT: Multi-fingered adaptive tactile grasping via deep reinforcement learning," in Conference on Robot Learning (CoRL), Oct. 2019.
[47] Barrett Technology, "BarrettHand," https://advanced.barrett.com/ barretthand, 2020.
[48] E. Coumans and Y. Bai, "PyBullet: A Python module for physics simulation for games, robotics and machine learning," http://pybullet. org, 2016-2020.
[49] H. Lee, H. Park, G. Serhat, H. Sun, and K. J. Kuchenbecker, "Calibrating a soft ERT-based tactile sensor with a multiphysics model and sim-to-real transfer learning," in IEEE International Conference on Robotics and Automation (ICRA), May 2020.
[50] C.-H. Lin, J. A. Fishel, and G. E. Loeb, "Estimating point of contact, force and torque in a biomimetic tactile sensor with deformable skin," SynTouch LLC, Tech. Rep., 2013.
[51] ANSYS, Inc., "Ansys Mechanical: Finite element analysis (FEA) software," https://www.ansys.com/products/structures/ansys-mechanical, 2020.
[52] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, "PointNet++: Deep hierarchical feature learning on point sets in a metric space," arXiv preprint arXiv:1706.02413, Jun. 2017.
[53] P. Ruppel, Y. Jonetzko, M. Gorner, N. Hendrich, and J. Zhang, "Simulation of the SynTouch BioTac sensor," in International Conference on Intelligent Autonomous Systems (IAS), Jun. 2018.
[54] B. S. Zapata-Impata and P. Gil, "Prediction of tactile perception from vision on deformable objects," in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Workshop on Robotic Manipulation of Deformable Objects (ROMADO), 2020.
[55] R. Calandra, A. Owens, M. Upadhyaya, W. Yuan, J. Lin, E. H. Adelson, and S. Levine, "More than a feeling: Learning to grasp and regrasp using vision and touch," Conference on Robot Learning, Nov. 2017.
[56] R. Calandra, D. J. Andrew Owens, J. Lin, W. Yuan, J. Malik, E. H. Adelson, and S. Levine, "More than a feeling: Learning to grasp and regrasp using vision and touch," IEEE Robotics and Automation Letters, Oct. 2018.
[57] N. Fazeli, M. Oller, J. Wu, Z. Wu, J. B. Tenenbaum, and A. Rodriguez, "See, feel, act: Hierarchical learning for complex manipulation skills with multisensory fusion," Science Robotics, Jan. 2019.
[68] M. Macklin, K. Erleben, M. Müller, N. Chentanez, S. Jeschke, and V. Makoviychuk, "Non-smooth Newton methods for deformable multibody dynamics," ACM Transactions on Graphics, Nov. 2019.
[58] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. FeiFei, A. Garg, and J. Bohg, "Making sense of vision and touch: Self-supervised learning of multimodal representations for contactrich tasks," in International Conference on Robotics and Automation (ICRA), 2019.
[59] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, "Multimodal deep learning," in International Conference on Machine Learning (ICML), 2011.
[60] L. Chen, S. Srivastava, Z. Duan, and C. Xu, "Deep cross-modal audiovisual generation," in Thematic Workshops of ACM Multimedia, Oct. 2019.
[61] J.-T. Lee, D. Bollegala, and S. Luo, ""Touching to see" and "seeing to feel": Robotic cross-modal sensory data generation for visualtactile perception," in IEEE International Conference on Robotics and Automation (ICRA), May 2019.
[62] Y. Li, J.-Y. Zhu, R. Tedrake, and A. Torralba, "Connecting touch and vision via cross-modal prediction," in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2019.
[63] K. Gutierrez and V. J. Santos, "Perception of tactile directionality via artificial fingerpad deformation and convolutional neural networks," IEEE Transactions on Haptics, 2020.
[64] A. Garcia-Garcia, B. S. Zapata-Impata, S. Orts-Escolano, P. Gil, and J. Garcia-Rodriguez, "TactileGCN: A graph convolutional network for predicting grasp stability with tactile sensors," in International Joint Conference on Neural Networks (IJCNN), 2019.
[65] J. N. Reddy, Introduction to the Finite Element Method. McGraw Hill, 2019.
[66] Y. S. Narang, J. J. Vlassak, and R. D. Howe, "Mechanically versatile soft machines through laminar jamming," Advanced Functional Materials, 2018.
[67] D. E. Stewart, "Rigid-body dynamics with friction and impact," SIAM Review, 2000.
[69] P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganovelli, and G. Ranzuglia, "MeshLab: An open-source mesh processing tool," in Eurographics Italian Chapter Conference, 2008.
[70] Y. Hu, T. Schneider, B. Wang, D. Zorin, and D. Panozzo, "Fast tetrahedral meshing in the wild," ACM Transactions on Graphics, Jul. 2020.
[71] K. Hauser, "Robust contact generation for robot simulation with unstructured meshes," Springer Tracts in Advanced Robotics, 2016.
[72] R. W. Ogden, Non-Linear Elastic Deformations. Courier Corporation, 2013.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*These authors contributed equally.
${ }^{1}$ NVIDIA Corporation, Seattle, USA. ${ }^{2}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington, Seattle, USA.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5966 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5966</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5966</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-260351308</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.00245v3.pdf" target="_blank">The Hitchhikerâ€™s Guide to Program Analysis: A Journey with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Model s (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM. By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50%) and appears to have no missing bug. It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5966",
    "paper_id": "paper-260351308",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.006904749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models
15 Nov 2023</p>
<p>Haonan Li 
Yu Hao 
Zhiyun Qian zhiyunq@cs.ucr.edu </p>
<p>UC Riverside Riverside
CaliforniaUSA</p>
<p>UC Riverside Riverside
CaliforniaUSA</p>
<p>UC Riverside Riverside
CaliforniaUSA</p>
<p>UC Riverside Riverside
CaliforniaUSA</p>
<p>The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models
15 Nov 20232B1D675C5F77B5100FF46EF116CB97AFarXiv:2308.00245v3[cs.SE]
Static analysis is a widely used technique in software engineering for identifying and mitigating bugs.However, a significant hurdle lies in achieving a delicate balance between precision and scalability.Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code.Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions.Therefore, at this point, LLMs are better used in an assistive role to complement static analysis.In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study.To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM.By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50%) and appears to have no missing bug.It even identified 13 previously unknown UBI bugs in the Linux kernel.This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.</p>
<p>INTRODUCTION</p>
<p>Static analysis is a popular technique in software engineering, particularly in the area of bug discovery, that can improve code quality, reliability, and security.However, the effectiveness of these techniques is influenced by the fundamental trade-off between precision and scalability, especially when dealing with extensive and complex programs [9,24].On the one hand, static analysis solutions with lower precision tend to generate numerous false positives.On the other hand, expensive static analysis or symbolic execution solutions with higher precision often struggle to complete the analysis.Consequently, achieving comprehensive and accurate static program analysis for sizable programs like the Linux kernel poses a significant challenge.</p>
<p>UBITect [40], a powerful static analysis solution illustrates these inherent limitations thoroughly.Targeting Use-Before-Initialization (UBI) bugs in the Linux kernel, it packages a pipeline of (1) a scalable bottom-up summary-based static analysis with limited precision, and (2) a precise symbolic execution with limited scalability.The solution illuminates the need for alternative strategies to navigate the complex trade-offs between precision and scalability effectively.Despite this strategic combination of analysis techniques, nearly 40% of the potential bugs reported from the static analysis phase experience a timeout or memory exhaustion during the static symbolic execution phase, preventing any conclusive results on such cases.This limitation hinders the overall effectiveness of the tool, leading to the potential of two distinct outcomes: missed bugs if these potential bug reports are ignored (what UBITect performs), or false positives if they are sent to developers for inspection.</p>
<p>In this paper, we investigate the possibility of leveraging Large Language Models (LLMs) as an alternative to handle such "difficult cases".This is because recent LLMs have exhibited strong potential in understanding, generating, and even debugging code [4,8,13].Nevertheless, navigating the intricacies of utilizing LLMs for bug discovery proves to be a complex feat.The technical report on GPT-4 underscores this challenge, admitting that when it comes to discovering new vulnerabilities, it may not be the best solution standalone [21]: "... is less effective than existing tools for complex and high-level activities like novel vulnerability identification".In the same vein, prior research demonstrates the competence of LLMs mostly in simpler tasks or programs [1,25,26].This is because LLMs are far from perfect.For instance, they suffer from hallucination [11] where instead of identifying the bugs in faulty code, LLMs may create non-existent facts in an attempt to rationalize the original intention behind the problematic code [17,31].Another issue is the stochasticity of LLMs which can result in inconsistent or outright incorrect results, thus throwing another wrench into the gears of bug discovery [41].Finally, LLMs have limited context windows, meaning they can only scrutinize a relatively small codebase.</p>
<p>In response, we propose LLift, a fully automated framework that bridges static analysis with LLMs in analyzing UBI bugs.Our solution packages several novel components.First, LLift performs post-constraint guided path analysis, which helps verify the path feasibility of the "use" of an initialized variable, a difficult task for static analysis and symbolic execution.Second, to efficiently interact with LLMs, we employ task decomposition to break down the analysis into more than a single step.Third, we employ progressive prompting by providing information incrementally only when necessary, instead of providing an enormous scope of code at once.Finally, we propose self-validation by requesting LLMs to  we propose self-validation by requesting LLMs to review responses at various stages to obtain accurate and reliable responses.</p>
<p>We implement a prototype of LL and test it in real-world scenarios.Focusing on the inconclusive cases of UBITect caused by time or memory limitation, LL successfully identies 13 previously unknown UBI bugs in the Linux kernel that we conrmed with the Linux community.With 26 positive reports out of nearly 1,000 cases, LL reaches a high precision of 50%.We also test LL against all previously known bugs found by UBITect, and observe a recall of 100%.</p>
<p>We summarize our contributions as follows:</p>
<p>â€¢ New Opportunities.We introduce a novel approach to static analysis that enhances its precision and scalability at the same time by harnessing the capabilities of LLMs.To the best of our knowledge, we are the rst to use LLMs to assist static analysis in bug-nding tasks with large-scale and real-world datasets.</p>
<p>â€¢ New Methodologies.We develop LL, an innovative and fully automated agent that arms static analysis with LLMs.LL employs several prompt strategies to engage with LLMs, eliciting accurate and reliable responses.</p>
<p>â€¢ Results.We rigorously investigate LL by conducting an indepth analysis of nearly 1000 cases, resulting in a high precision rate (50%) and recall rate (100%).Additionally, our examination led to the discovery of 13 previously unknown bugs.</p>
<p>â€¢ Open source.Committed to open research, we will publicly release all of our code and data, fostering further exploration of the new space of LLM-assisted program analysis.</p>
<p>BACKGROUND &amp; MOTIVATION 2.1 UBITect and Motivating Example</p>
<p>UBITect is a state-of-the-art static analysis solution aiming at nding Use Before Initialization (UBI) bugs in the Linux kernel [41].</p>
<p>It employs a two-stage pipeline where the rst stage employs a bottom-up summary-based static analysis of the Linux kernel.By design, this stage aims for scalability and sacrices precision, producing a signicant number of potential bugs (i.e., â‡ 140k), most of which are false alarms.The static analysis is imprecise partly due to its lack of path sensitivity (often needed to discover UBI bugs).</p>
<p>It is complemented by a second stage of static symbolic execution that lters as many false alarms as possible by verifying their path feasibility.However, 40% of the reported bugs are discarded due to timeout (10 minutes) or memory limitations (2 GB) during the symbolic execution, potentially missing genuine bugs.</p>
<p>Figure 1 shows a case where UBITect's static analysis stage considers it a potential UBI bug (a false alarm) and the subsequent symbolic execution stage times out and fails to generate a denitive conclusion.In other words, UBITect failed to rule out this case as a false alarm.As Table 1 presents, the static analysis stage generates a summary of sscanf() as "may not initialize parameters a, b, c, and d" but does use them at Line 3. Consequently, the static analysis stage reports two locations of use-before-initialization at Line 3 and Line 4, respectively.There are two reasons for the static analysis stage to consider the case a potential bug: 1) inability to recognize special functions: For soundness, UBITect assumed the va_start() is a normal function.However, since it cannot nd its denition, it has to conservatively assume that the arguments passed to it will be used inside.Unfortunately, in reality, va_start is a compiler built-in function that simply "prepares' the arguments without any uses.2) insensitivity of postconditions: It fails to recognize the check of its return value, i.e., if(sscanf(...)&gt;=4), which ensures its arguments a to d must be initialized before use.</p>
<p>Practical Challenges of Static Analysis</p>
<p>In light of our motivating example of the sscanf() case, we can summarize the reasons for UBITect's failure as follows:</p>
<p>Inherent Knowledge Boundaries.Developers need to model specic functions or language features.Otherwise, they inuence the correctness of the results.For compiler built-in functions, e.g., va_start(), their denitions are simply not available.Beyond this example, there exists an array of other scenarios, which are particularly prevalent in the Linux kernel.These situations include assembly code, hardware behaviors, callback functions, concurrency, and compiler built-in functions.However, in practical terms, it is often time-consuming to discover and model all these cases, because they can be highly dependent on the analysis target and evolve over time.This limitation often compromises the eectiveness of static analysis, leaving it less precise and comprehensive than desired.</p>
<p>Exhaustive Path Exploration.Correctly handling cases like sscanf() requires it to consider the check: sscanf(...)&gt;=4.Unfortunately, existing path-sensitive static analysis (and symbolic execution) techniques operate under a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase.While this approach is theoretically comprehensive, it often leads to a combinatorial explosion.The vast array of execution paths necessitates the exploration of myriad functions, many of which ultimately prove irrelevant to the specic analysis task at hand.In the sscanf() case, its return value is computed inside an unbounded loop when iterating over an unknown string variable buf.This causes UBITect's symbolic execution to time out exactly due to this problem.We implement a prototype of LLift and test it in real-world scenarios.Focusing on the inconclusive cases of UBITect caused by time or memory limitation, LLift successfully identifies 13 previously unknown UBI bugs in the Linux kernel that we confirmed with the Linux community.With 26 positive reports out of nearly 1,000 cases, LLift reaches a high precision of 50%.We also test LLift against all previously known bugs found by UBITect, and observe a recall of 100%.</p>
<p>We summarize our contributions as follows:</p>
<p>â€¢ New Opportunities.We introduce a novel approach to static analysis that enhances its precision and scalability at the same time by harnessing the capabilities of LLMs.To the best of our knowledge, we are the first to use LLMs to assist static analysis in bug-finding tasks with large-scale and real-world datasets.</p>
<p>â€¢ New Methodologies.We develop LLift, an innovative and fully automated framework that arms static analysis with LLMs.LLift employs several prompt strategies to engage with LLMs, eliciting accurate and reliable responses.</p>
<p>â€¢ Results.We rigorously investigate LLift by conducting an indepth analysis of nearly 1000 cases, resulting in a reasonable precision rate (50%).Additionally, our examination led to the discovery of 13 previously unknown bugs.</p>
<p>â€¢ Open source.Committed to open research, we will publicly release all of our code and data, fostering further exploration of the new space of LLM-assisted program analysis.</p>
<p>BACKGROUND &amp; MOTIVATION 2.1 UBITect and Motivating Example</p>
<p>UBITect is a state-of-the-art static analysis solution aiming at finding Use Before Initialization (UBI) bugs in the Linux kernel [40].</p>
<p>It employs a two-stage pipeline where the first stage employs a bottom-up summary-based static analysis of the Linux kernel.By design, this stage aims for scalability and sacrifices precision, producing a significant number of potential bugs (i.e., âˆ¼140k), most of which are false alarms.The static analysis is imprecise partly due to its lack of path sensitivity (often needed to discover UBI bugs).</p>
<p>It is complemented by a second stage of static symbolic execution that filters as many false alarms as possible by verifying their path feasibility.However, 40% of the reported bugs are discarded due to timeout (10 minutes) or memory limitations (2 GB) during the symbolic execution, potentially missing genuine bugs.</p>
<p>Figure 1 shows a case where UBITect's static analysis stage considers it a potential UBI bug (a false alarm) and the subsequent symbolic execution stage times out and fails to generate a definitive conclusion.In other words, UBITect failed to rule out this case as a false alarm.As Table 1 presents, the static analysis stage generates a summary of sscanf() as "may not initialize parameters a, b, c, and d" but does use them at Line 3. Consequently, the static analysis stage reports two locations of use-before-initialization at Line 3 and Line 4, respectively.There are two reasons for the static analysis stage to consider the case a potential bug: 1) inability to recognize special functions: For soundness, UBITect assumed the va_start() is a normal function.However, since it cannot find its definition, it has to conservatively assume that the arguments passed to it will be used inside.Unfortunately, in reality, va_start is a compiler built-in function that simply "prepares' the arguments without any uses.2) insensitivity of path constraints: It fails to recognize the path constraint, i.e., if(sscanf(...)&gt;=4), which ensures its arguments a to d must be initialized before use.</p>
<p>Practical Challenges of Static Analysis</p>
<p>In light of our motivating example of the sscanf() case, we can summarize the reasons for UBITect's failure as follows:</p>
<p>Inherent Knowledge Boundaries.Developers need to model specific functions or language features.Otherwise, they influence the correctness of the results.For compiler built-in functions, e.g., va_start(), their definitions are simply not available.Beyond this example, there exists an array of other scenarios, which are particularly prevalent in the Linux kernel.These situations include assembly code, hardware behaviors, callback functions, concurrency, and compiler built-in functions.However, in practical terms, it is often time-consuming to discover and model all these cases, because they can be highly dependent on the analysis target and evolve over time.This limitation often compromises the effectiveness of static analysis, leaving it less precise and comprehensive than desired.</p>
<p>Exhaustive Path Exploration.Correctly handling cases like sscanf() requires it to consider the check: sscanf(...)&gt;=4.Unfortunately, existing path-sensitive static analysis (and symbolic execution) techniques operate under a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase.While this approach is theoretically comprehensive, it often leads to a combinatorial explosion.The vast array of execution paths necessitates the exploration of myriad functions, many of which ultimately prove irrelevant to the specific analysis task at hand.In the sscanf() case, its return value is computed inside an unbounded loop when iterating over an unknown string variable buf.This causes UBITect's symbolic execution to time out exactly due to this problem.For each suspicious variable -, we expect it to 1) have an initializer function that probably initializesand 2) use -.</p>
<p>Capability of LLMs</p>
<p>Fortunately, LLMs [21] oers a promising alternative to summarizing code behaviors [22] in a exible way and bypassing the aforementioned challenges.This is because LLMs are trained and aligned with extensive datasets that include both natural language and programs.Specically, we observe that LLMs possess fundamental abilities that assist in addressing each challenge: 1) domainspecic code recognition and 2) smart code summarization.</p>
<p>Domain-specic Programming Constructs Recognition.This prociency is showcased in three key areas: 1) Function Recognition: LLMs can identify frequently used interfaces in the Linux kernel from its semantics, such as sscanf(), kzalloc(), kstrtoul(), and 'list for each', simplifying the analysis and making the analysis more scalable.2) Function pointers and callbacks: LLMs can accurately interpret complex uses of function pointers as callbacks, which often require manual modeling.We will show an interesting case in Â§6.7.</p>
<p>Smart Code Summarization.LLMs can work with complicated functions; for example, that they can summarize loop invariants [26], which is an inherently dicult task in program analysis.This is likely because it has been trained on various functions with loops and their semantics.In contrast, traditional static analysis follows explicitly dened rules without a limited ability to generalize.</p>
<p>PROBLEM FORMULATION</p>
<p>Based on our observation, we propose LL, an automated agent that interfaces with UBITect and LLMs (e.g., ChatGPT) to help identify and reason about UBI bugs in the Linux kernel.</p>
<p>Scope.Our goal of developing LL is not to replace UBITect and static analysis in general.Instead, we aim to use LL to analyze the dicult cases for UBITect, i.e., 40% of cases that are considered inconclusive.In other words, we aim to use LL primarily as a complementary solution to UBITect.This relationship is depicted in Figure 2.</p>
<p>Assumptions.As a rst exploratory study of assisting static analysis with LLMs, we restrict our scope to commonly observed patterns that exceed the time and memory limits set for UBITect's symbolic execution stage.Specically, depicted in Figure 3, we observe that it is often the case that the variable is declared in one function and it is passed to a callee function to be initialized (typically conditionally) before its use.We assume there can be multiple layers of indirection where the initialization actually occurs.Note that there are other less common cases, i.e., the variable is initialized directly in the function where the variable is declared.We choose not to account for them, as most such cases are easier cases and more likely solved successfully by UBITect already.</p>
<p>Conceptual Workow.Figure 4 demonstrates the workow of LL.LL takes bug reports from UBITect as inputs, which contain the suspicious variable that may be used before initialization, and the function that it sits in.We follow the following three key subtasks when interacting with LLMs: Â¨Identify potential initializers of the suspicious variable.In our motivating example, sscanf() is the initializer.</p>
<p>â‰  Extract the path constraints from the initializer return to the use, i.e., the constraints that ensure the use is reachable.In our example, the constraint is sscanf(...)&gt;=4.</p>
<p>AE Summarize the behavior of the initializer, capturing the variable initialization status, i.e., either must_init or may_init, given the path constraints learned in â‰ .In our example, a correct response should be must_init: a,b,c,d.</p>
<p>If the nal output is must_init, we can safely lter out such cases as non-bugs; if the output is may_init, we will consider it a potential bug.Note that this simple policy may still lead to false alarms (which we evaluate in Â§6).For example, our workow by design does not take into account preconditions,</p>
<p>DESIGN</p>
<p>We have illustrated the conceptual workow previously in Figure 4 that works together logically to solve our formulated problem.However, to achieve better results, we have to overcome a number of challenges in interacting with LLMs.This means that will need to map the conceptual workow into an instantiated workow which will follow a number of principles described in this section.</p>
<p>Design Challenges</p>
<p>It is non-trivial to prompt LLMs eectively [28,42].We meet the following challenges and propose solutions correspondingly in designing LL.</p>
<p>â€¢ C1.Limited Understanding of Postconditions.Despite LLMs (e.g., GPT-4) are able to comprehend the denition of postconditions and apply them in simple scenarios, we found their capacity to utilize this knowledge in actual program analysis-such as Figure 3: A typical type of potential UBI bug.For each suspicious variable  , we expect it to 1) have an initializer function that probably initializes  and 2) use  .</p>
<p>Capability of LLMs</p>
<p>Fortunately, LLMs [21] offers a promising alternative to summarizing code behaviors [22] in a flexible way and bypassing the aforementioned challenges.This is because LLMs are trained and aligned with extensive datasets that include both natural language and programs.Specifically, we observe that LLMs possess fundamental abilities that assist in addressing each challenge: 1) domainspecific code recognition and 2) smart code summarization.</p>
<p>Domain-specific Programming Constructs</p>
<p>Recognition.This proficiency is showcased in three key areas: 1) Function Recognition: LLMs can identify frequently used interfaces in the Linux kernel from its semantics, such as sscanf(), kzalloc(), kstrtoul(), and 'list for each', simplifying the analysis and making the analysis more scalable.2) Function pointers and callbacks: LLMs can accurately interpret complex uses of function pointers as callbacks, which often require manual modeling.We will show an interesting case in Â§6.6.</p>
<p>Smart Code Summarization.LLMs can work with complicated functions; for example, that they can summarize loop invariants [26], which is an inherently difficult task in program analysis.This is likely because it has been trained on various functions with loops and their semantics.In contrast, traditional static analysis follows explicitly defined rules without a limited ability to generalize.</p>
<p>PROBLEM FORMULATION 3.1 Definitions and Scope</p>
<p>3.1.1Use-Before-Initialization. A Use Before Initialization (UBI) bug refers to the erroneous scenario where a variable  is accessed or involved in any operation prior to its correct initialization.Let:</p>
<p>â€¢  () represent the declaration of .</p>
<p>â€¢  () signify a use operation involving .</p>
<p>â€¢  () denote the initialization operation of . if there exists  () and  (), then  is used before initialization if:
âˆƒğ‘£ : (ğ‘‘ (ğ‘£) &lt; ğ‘¢ (ğ‘£)) âˆ§ Â¬(âˆƒğ‘– (ğ‘£) : ğ‘‘ (ğ‘£) &lt; ğ‘– (ğ‘£) &lt; ğ‘¢ (ğ‘£))(1)
where &lt; indicates a temporal sequence in the program execution.</p>
<p>Postcondition.</p>
<p>Postconditions encapsulate the expected state or behavior of a system upon the conclusion of a routine [18].</p>
<p>Specifically, they detail the guarantees a routine offers based on its observable outcomes.For a routine , consider its set of outcomes as O.These outcomes are defined as updates to its parameters (and return value) for a path of .Particularly, O does not include initialization for variables for convenience.In the study of UBI bug, for a routine  that can yield a set of outcomes O, the postcondition P can be defined as:
P ğ‘… : S(ğ‘…) â†’ O Ã— must_init (2)
Here, S() signifies all possible execution paths through the routine , O describes all updates of  on its variables, and must_init is a set of variables that must be initialized.</p>
<p>Motivating Example.Consider the sscanf() function in our motivating example.Based on these return values, the postconditions assure the initialization of certain variables:
P (ğ‘ğ‘ğ‘¡â„ 1 ) : {ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 0, must_init â†¦ â†’ âˆ…} P (ğ‘ğ‘ğ‘¡â„ 2 ) : {ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 1, must_init â†¦ â†’ {ğ‘}} P (ğ‘ğ‘ğ‘¡â„ 3 ) : {ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 2, must_init â†¦ â†’ {ğ‘, ğ‘}} P (ğ‘ğ‘ğ‘¡â„ 4 ) : {ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 3, must_init â†¦ â†’ {ğ‘, ğ‘, ğ‘}} P (ğ‘ğ‘ğ‘¡â„ 5 ) : {ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 4, must_init â†¦ â†’ {ğ‘, ğ‘, ğ‘, ğ‘‘ }} P (ğ‘ğ‘ğ‘¡â„ 6 ) : {ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 5, must_init â†¦ â†’ {ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘›}}
Here, the â„ 1 âˆ’ â„ 6 represent different possible paths in the sscanf() and each path corresponds with a different postcondition.</p>
<p>For UBI detection, not every associated postcondition is relevant; instead, only the outcomes making the  () reachable are critical.The constraints of the use are post-constraints C  [? ].The qualified postcondition, P  , is a subset of P refined by C  :
P ğ‘ğ‘¢ğ‘ğ‘™ = P | C ğ‘ğ‘œğ‘ ğ‘¡ For the sscanf() case, if the post-constraint is C ğ‘ğ‘œğ‘ ğ‘¡ = ret â‰¥ 4,
the qualified postcondition would be P (â„ 5 ) âˆ§ P (â„ 6 ), which ensures that variables a, b, c, and d must be initialized; therefore, all variables used subsequently are initialized, and no UBI happens.</p>
<p>In subsequent discussions, unless otherwise specified, the term 'postcondition' shall denote 'qualified postcondition'.</p>
<p>Post-Constraint Guided Path Analysis</p>
<p>When analyzing a routine or function in a path-sensitive manner, the number of paths to explore can grow rapidly.Fortunately, if we have information about what the function is expected to achieve (given by C  ), we can prune paths that inherently don't meet those expectations.We categorize two scenarios, direct application and outcome conflicts, in applying this optimization.</p>
<p>Let  be the routine or function under analysis and S() be its path set.Let â„ âˆˆ S() refer to a specific path in .Besides, Each path â„ has an associated path constraint  that dictates its feasibility.These two optimizations can be formed with: Direct Application.For direct application, the post-constraint C  can be directly applied as a path constraint.A path can be discarded if:
Â¬(ğ‘ (ğ‘ğ‘ğ‘¡â„) âˆ§ C ğ‘ğ‘œğ‘ ğ‘¡ )
This implies that if a â„ inherently contradicts the post-constraint, it can be removed from consideration.</p>
<p>Outcome Conflicts.Let O () denote the set of all outcomes or effects produced by path .A path can be pruned if any of its outcomes conflict with the post-constraint:
âˆƒğ‘œ âˆˆ O (ğ‘ğ‘ğ‘¡â„) : Â¬(ğ‘œ âˆ§ C ğ‘ğ‘œğ‘ ğ‘¡ )
This stipulates that if an outcome from â„ inherently contradicts the post-constraint, that path can be disregarded in the analysis.</p>
<p>Correctness.The validity of these optimization methods can be proved by contradiction.Consider an instance where one of these paths is executed.If this path conflicts with the C post , it would render  () unreachable.Thus, it becomes evident that such paths can be pruned without sacrificing the correctness of the analysis.We provide a concrete example of how we perform these optimizations in Â§4.3.3.</p>
<p>Conceptual Workflow</p>
<p>Given a bug report containing a suspicious variable  and its residing function  , the workflow Î¦ is as follows:</p>
<p>(1) Î¦ 1 (, ) â†’ { ()}: Identify potential initializers for  from the bug report.(2) Î¦ 2 (,  ()) â†’ C  : Extract the C  from the bug report for each  ().(3) Î¦ 3 (, { (), C  }) â†’ InitStatus(): Summarize the initialization status for variable  after all possible initializers completion (merge multiple initializers).</p>
<p>Decision Policy.The decision policy Î” is defined as:</p>
<p>Î”(InitStatus() = must_init) : non-bug Î”(InitStatus() â‰  must_init) : potential bug In this policy, we adopt a conservative approach by treating all variables not explicitly marked as must_init as potential vulnerabilities.And it is worth noting that this policy may introduce some false positives.For example, it might over-approximate preconditions.</p>
<p>Conceptually, LLift will not miss more bugs.The post-constraint guided path optimizations and decision policies are safe.</p>
<p>Turns and Conversations in LLMs</p>
<p>We define two key concepts in interacting with LLMs: turn and conversation.</p>
<p>â€¢ Turn: A turn encapsulates a singular interaction with the LLM.</p>
<p>Formally, it's defined as a tuple, (,  ), where  represents the problem or question, and  denotes the LLM's response.</p>
<p>â€¢ Conversation: Leveraging the capabilities of LLMs often necessitates a series of interactions, especially for complex problem-solving.A conversation is an ordered sequence of turns.A conversation comprising  turns can be expressed as
[(ğ‘ 1 , ğ‘Ÿ 1 ), (ğ‘ 2 , ğ‘Ÿ 2 ), . . . , (ğ‘ ğ‘› , ğ‘Ÿ ğ‘› )].</p>
<p>DESIGN</p>
<p>In Section Â§3.3, we introduced a conceptual workflow.Elaborating on that foundation, Figure 4 showcases a compelling illustration of our methodological approach.Yet, translating this workflow into practice presents its challenges.Even with the advanced knowledge and analytical capabilities of cutting-edge LLMs, achieving optimal results remains a challenge.Throughout the development of LLift, we identified several obstacles and subsequently introduced four distinct design components to effectively address these challenges.</p>
<p>Design Challenges</p>
<p>It is non-trivial to prompt LLMs effectively [28,41].We meet the following challenges and propose solutions correspondingly in designing LLift.</p>
<p>â€¢ C1.Limited Understanding of Post-constraint.Despite LLMs (e.g., GPT-4) are able to comprehend the definition of post-constraint and apply them in simple scenarios, we found their capacity to utilize this knowledge in actual program analysis-such as summarizing function behavior in line with specific post-constraint -to be limited.This critical limitation often results in unpredictable and inconsistent outcomes.</p>
<p>â€¢ C2.Token Limitations.It is known that LLMs have token limitations.For example, GPT-3.5 supports 16k tokens and GPT-4 supports 32k tokens [20].This means that we do not want to copy a large number of function bodies in our prompts to LLMs.</p>
<p>â€¢ C3.Unreliable and Inconsistent Response.LLMs are known to result in unreliable and inconsistent responses due to hallucination and stochasticity [41].Stochasticity refers to the inherent unpredictability in the model's outputs [32]; and the hallucination refers to LLMs generating nonsensical or unfaithful responses [11,42].By design, the stochasticity can be mitigated with lower temperature, a hyperparameter controlling the degree of randomness in outputs [27]; however, reducing temperature may impair the model's exploring ability [37] and therefore may miss corner cases that result in vulnerabilities.</p>
<p>Design Overview</p>
<p>We will discuss our design strategies to address the above challenges in the rest of the section.Before that, we provide a high-level overview of our solution.â€¢ To tackle challenge C2 (Token Limitation), We employ two strategies: (D#2) Progressive Prompt.Instead of copying a large number of function bodies (i.e., subroutines), we only provide function details on demand, i.e., when LLMs are not able to conduct a result immediately.(D#3) Task Decomposition.We break down the problem into sub-problems that can be solved in independent conversations, i.e., a sequence of prompt and response pairs.</p>
<p>â€¢ To tackle challenge C3 (Unreliable Response), we employ the following strategies: (D#4) Self-Validation.We ask LLMs to review and correct their previous responses.This helps improve the consistency and accuracy based on our observation.Besides, (D#2) Progressive Prompt and (D#3) Task Decomposition also help to deal with this challenge.Additionally, we implement majority voting by running each case multiple times and use majority voting to combat stochasticity.We elaborate the design of (D#1 -#4) Post Constraint Guided Path Analysis, Progressive Prompts, Task Decomposition, and Self-Validation detailed in the rest of this section.The effectiveness and efficiency of these design strategies are rigorously evaluated in Â§6.4,revealing a substantial enhancement in bug detection within the Linux kernel.</p>
<p>Design #1: Post-Constraint Guided Path Analysis</p>
<p>The Linux kernel frequently employs return value checks as illustrated in Table 2. Through our detailed examination of non-bug instances, we found that a path-sensitivity analysis can effectively eliminate over 70% of these negative cases.However, path-sensitive static analysis usually suffers from path explosion, especially in large-scale codebases like the Linux kernel.Fortunately, we can prompt the LLM to collect C  and summarize the function with respective to the C  .It is worth noting error conditions cause the use to become unreachable, as illustrated in Type B, the post-constraint is 4AA 7 !0. Type B' depicts a variant where the initializer keeps retrying til success, and therefore with expected output A4C 7 !0, which indicates its rst successful execution to break the endless loop.</p>
<p>Function Behavior Summarization.</p>
<p>Once we obtain the postcontraints in Convo.1, we feed them to the LLM to obtain the behavior summary in Convo.2 .For example, we provide the following: Note that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution.Consider the sscanf() example, even if the analysis is aware that the qualied postcondition should be limited to those where A4C 4, it would still need to enumerate the paths inside of sscanf(), which involves loops and can easily lead to timeouts as explained in Â§2.1.</p>
<p>Design #2: Progressive Prompt</p>
<p>The Linux kernel has an extremely large codebase.Summarizing an initializer using LLMs without providing any supplementary function denitions can result in incomplete or erroneous responses.On the other hand, ooding the LLM with every relevant function denition upfront risks exceeding their context window limitations.</p>
<p>To address this dilemma, we choose to progressively provide function denitions as needed.Illustrated in Figure 5, this approach, which we refer to as Progressive Prompt, fosters a dynamic interaction with the LLM rather than expecting a response in one shot.Throughout this iterative exchange, we consistently prompt the LLM: "If you encounter uncertainty due to a lack of function denitions, please signal your need, and I'll supply them".Should the LLM need more information, LL will promptly extract the relevant</p>
<p>De</p>
<p>At times, iors, parti constructs dition mus to be may_ A4C 7 !0. 5 that current LLMs (e.g., GPT-4) are not natively sensitive to the sensitivity; without any additional instructions, LLMs usually overlook the post-constraints.Therefore, we teach the LLM to be sensitive to post-constraints rules through few-shots in-context learning.We describe the design details as follows:</p>
<p>4.3.1 Post-Constraints Extraction.To extract the qualified postcondition, we first determine the post-constraints that lead to the use of suspicious variables.We incorporate few-shot in-context learning to teach LLMs how to extract such constraints from the caller context.Table 2 demonstrates how we teach LLM with in-context learning.We focus primarily on two types of code patterns:</p>
<p>â€¢ Check Before Use.Type A is our motivating example; by looking at its check, the post-constraint should be  â‰¥ 4. Type A' describes a similar case with switch-cases, with expected output  â†¦ â†’ crticial_case.</p>
<p>â€¢ Failure Check.This pattern captures the opposite of the first pattern.They commonly occur in the Linux kernel where the error conditions cause the use to become unreachable, as illustrated in Type B, the post-constraint is  â†¦ â†’ 0. Type B' depicts a variant where the initializer keeps retrying til success, and therefore with expected output  â†¦ â†’ 0, which indicates its first successful execution to break the endless loop.</p>
<p>Function Behavior Summarization.</p>
<p>Once we obtain the postcontraints in Convo.1, we feed them to the LLM to obtain the behavior summary in Convo.2 .For example, we provide the following:</p>
<p>{ "initializer": "ret = sscanf(str,'%u.%u.%u.%u%n',&amp;a,&amp;b,&amp;c,&amp;d,&amp;n)", "suspicious": ["a", "b", "c", "d"], "postconstraint": "ret &gt;= 4" }</p>
<p>The LLM may respond with { "ret": "success", "response": { "must_init": ["a", "b", "c", "d"], "may_init": [{"name":"n", "condition": "ret &gt; 4"}] } }</p>
<p>The response succinctly encapsulates the function behavior, where variables a,b,c,d are classified as must_init, and n is categorized as may_init.This is due to the initialization of n only occurring when  &gt; 4, and not when  â†¦ â†’ 4.  Note that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution.Consider the sscanf() example, even if the analysis is aware that the qualified postcondition should be limited to those where  â‰¥ 4, it would still need to enumerate the paths inside of sscanf(), which involves loops and can easily lead to timeouts as explained in Â§2.1.
C ğ‘ğ‘œğ‘ ğ‘¡ = âŠ¤ or âˆ€ğ‘ğ‘  âˆˆ {Â¬some_condi} : ğ‘ğ‘  âŠ¥ C ğ‘ğ‘œğ‘ ğ‘¡ âˆ§ âˆ€ğ‘œ âˆˆ {ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 0} : ğ‘œ âŠ¥ C ğ‘ğ‘œğ‘ ğ‘¡ must_init = {ğ‘} if: (Â¬some_condi) âˆ§ C ğ‘ğ‘œğ‘ ğ‘¡ or (ğ‘Ÿğ‘’ğ‘¡ â†¦ â†’ 0) âˆ§ C ğ‘ğ‘œğ‘ ğ‘¡</p>
<p>Apply Path</p>
<p>Analysis.Following Â§3.2, Figure 6 presents a concert example of post-constraint guided path analysis.This case shows a simple initializer  () of the variable .Given an early return, the initialization in line 4 may not be executed.As such, the qualified postconditions become contingent on the post-constraints C  .There are:</p>
<p>â€¢ If the use of variable a is unconditional, i.e., C  = âŠ¤.In this case, the variable  is labeled as may_init given that the initialization may not be reached.</p>
<p>In general, if all path constraints and outcomes of must_init are disjoint from C  , no path can be pruned out.We could also conclude  as may_init.</p>
<p>â€¢ If the use of variable  is conditional with constraints, i.e., C  â‰  âŠ¤, two cases emerge:</p>
<p>(1) C  clashes with the constraints of the path (e.g., some_condi), or (2) C  conflicts with the path outcome (e.g., return -1).In these instances, C  could be some_condi or func(...)==0 and we can designate *a as must_init.</p>
<p>Design #2: Progressive Prompt</p>
<p>The Linux kernel has an extremely large codebase.Summarizing an initializer using LLMs without providing any supplementary function definitions can result in incomplete or erroneous responses.On the other hand, flooding the LLM with every relevant function definition upfront risks exceeding their context window limitations.</p>
<p>To address this dilemma, we choose to progressively provide function definitions as needed.Illustrated in Figure 5, this approach, which we refer to as Progressive Prompt, fosters a dynamic interaction with the LLM rather than expecting a response in one shot.Throughout this iterative exchange, we consistently prompt the LLM: "If you encounter uncertainty due to a lack of function definitions, please signal your need, and I'll supply them".Should the LLM need more information, LLift will promptly extract the relevant details on demand from the source code and provide it to the LLM automatically, enabling it to reassess and generate a more accurate response.</p>
<p>Specifically, We teach the LLM to ask for more information with a specific format:</p>
<p>[{"type":"function_def", "name":"some_func" }] Subsequently, LLift scans this format in the LLM's response.For each requested function definition, LLift supplies its corresponding code along with comments extracted from the Linux source code.Though GPT-4 may seek other types of information beyond function definitions (e.g., struct definitions), we currently limit our support to requests pertaining to function definitions.</p>
<p>The iterative process continues until either the LLM no longer requests additional information, or LLift cannot supply the requested details.In certain situations where LLift is unable to provide more information (e.g., the definition of an indirect call), LLift will still prompt the LLM to proceed with the analysis.In these instances, the LLM is encouraged to infer the behavior based on the available data and its inherent knowledge, thereby facilitating continued analysis even when not all information is directly accessible.</p>
<p>Design #3: Task Decomposition</p>
<p>We systematically apply the principle of task decomposition, a vital element of our design process.This concept is incorporated primarily in two distinct ways.</p>
<p>Multistage Problem Solving.As illustrated in Figure 5, we employ a two-conversation approach to complete the task.Each conversation, essentially consists of multiple iterations of prompts and responses.The first conversation (Convo.1) is dedicated to extracting the initializer and its associated post-constraints (subtasks 1 and 2), while the second conversation (Convo.2) focuses on summarizing the function (subtask 3) based on the previously identified post-constraints.This division allows a more manageable and effective way of achieving the task, compared to combining all three subtasks into a single conversation.The efficacy of this task decomposition approach is further evaluated in Â§6.5.</p>
<p>Thinking in English.Our workflow necessitates a structured output, such as a JSON format, for automation.However, we observe that LLMs often produce suboptimal results when directly prompted to output in this format.As LLMs build responses incrementally, word-by-word, based on preceding outputs [32], direct prompts to output JSON may interrupt their thought progression.This emphasizes the importance of initially soliciting responses in natural language to ensure comprehensive and effective reasoning.Consequently, we instruct the LLM to first articulate their thought processes in English, followed by a subsequent prompt to transform their response into a JSON summary.</p>
<p>Design #4: Self-Validation</p>
<p>At times, LLMs can display unpredictable or inconsistent behaviors, particularly in complex scenarios involving detailed logical constructs.Consider a case where an initializer carries the postcondition must_init if  â†¦ â†’ 0. LLMs may still mistakenly assume it to be may_init, despite the explicit presence of the post-constraint  â†¦ â†’ 0.</p>
<p>Conversely, an LLM might erroneously interpret a non-existent post-constraint and incorrectly infer a may_init case as must_init.This phenomenon is known as hallucination.Essentially, the hallucination can lead to both false positives and false negatives in bug detection, thereby affecting accuracy and reliability.</p>
<p>In addition to task decomposition, we also introduce the concept of self-validation to enhance reliability.Before the LLM reaches its final conclusion, this method reinforces specific rules, allowing the LLM to reassess their previous responses for adherence and make necessary corrections.We observed that this practice yields better results.We evaluate the effect of self-validation in Â§6.4.</p>
<p>As seen in Figure 5, we employ self-validation in both conversations.By prompting a list of correct properties that we expect, LLMs can verify and correct their results by themselves automatically.</p>
<p>Additional Prompting Strategies</p>
<p>In order to further optimize the efficacy of our model, we have incorporated several additional strategies into our prompt design:</p>
<p>â€¢ Chain-of-Thought.Leveraging the Chain-of-Thought (CoT) approach, we encourage the LLMs to engage in stepwise reasoning, using the phrase "think step by step".This not only helps generate longer, comprehensive responses, but it also provides intermediate results at each juncture of the thought process.Previous studies suggest the CoT approach considerably enhances the LLMs' reasoning capabilities [3].We incorporate the CoT strategy into every prompt.</p>
<p>â€¢ Source Code Analysis.Rather than analyzing abstract representations, we opt to focus our attention directly on the functions within the source code.This approach not only economizes on token use compared to LLVM IR, but also allows the model to leverage the semantic richness of variable names and other programming constructs to conduct a more nuanced analysis.</p>
<p>There are still some interesting details in designing an effective prompt but due to space constraints and without changing the overall strategy, we will not list them all.Readers intrigued can delve into the intricacies of our open-sourced prompt 1 design and experimental implementations to gain a deeper understanding.</p>
<p>IMPLEMENTATION</p>
<p>We implement the prototype of LLift based on OpenAI's API [19] (i.e., gpt-4-0613).We describe some implementation details in the following aspects:</p>
<p>Interaction with LLMs.LLift's interaction with LLMs is managed by a simple agent developed in Python, containing roughly 1,000 lines of code.In addition, it uses seven prompts, which altogether constitute about 2,000 tokens in two conversations.All interactions are fully automated via APIs of OpenAI.Besides sending prompts and waiting for responses, our agent also 1) interacts with LLMs according to the progressive prompt design, 2) locates function definitions within the Linux source code, and 3) processes responses from LLMs, then receives and stores to a database.</p>
<p>Hyper-Parameters.There are several hyper-parameters in calling the APIs provided by OpenAI.We choose max_token and temperature to 1,024 and 1.0, respectively.max_token controls the output length; since LLMs always predict the next words by the previous output, the longer output can benefit and allow its reasoning.However, too many tokens will exhaust the context window quickly, so we pick 1024 as a reasonable balance.The temperature controls the randomness and also the ability to reason.Intuitively, we want the analysis to be as non-random as possible and reduce the temperature (it can take a value between 0 1 https://sites.google.com/view/llift-open/promptand 2 for GPT models); however, an overly low temperature can result in repetitive or overly simplistic responses.We set it to 1.0 (also the default of gpt-4-0613), which allows for higher-quality responses, and use strategies such as self-validation and majority voting to improve the consistency of responses.</p>
<p>EVALUATION</p>
<p>Our evaluation aims to address the following research questions.</p>
<p>â€¢ RQ1 (Precision): How accurately is LLift able to identify bugs?â€¢ RQ2 (Recall): Is there a possibility for LLift to miss real bugs?â€¢ RQ3 (Comparison): How does the performance of individual components within LLift compare to that of the final design?</p>
<p>â€¢ RQ4 (Model Versatility): How does LLift perform when applied to LLMs other than GPT-4?</p>
<p>We evaluate RQ1 to RQ3 in GPT-4, under API from OpenAI with version gpt4-0613.For RQ4, we also test GPT-3.5 with version gpt-3.5-turbo-0613and Claude 2 additionally for comparison.</p>
<p>Dataset</p>
<p>Our experiment data, sourced from UBITect, includes all potential bugs labeled by its static analysis stage but experienced timeout or memory exhaustion during its symbolic execution stage.Overall, UBITect's static analysis stage produced 140,000 potential bugs, with symbolic execution able to process only 60%, leaving 53,000 cases unattended, which means that these cases are generally difficult for static analysis or symbolic execution to decide We craft the following dataset from 53,000 cases to evaluate LLift:</p>
<p>(1) Random-1000.We randomly chose 1,000 from the 53,000 cases for testing.However, there are 182 cases where there are no initializers, which are automatically recognized and filtered (see Â§3).The remaining 818 cases are used in evaluating precision, i.e., the ratio of true positives to false positives.</p>
<p>(2) Bug-50.This dataset comprises the 52 confirmed UBI bugs previously identified by UBITect.It is used as ground truth for assessing recall by verifying if any true bugs were overlooked.</p>
<p>(3) Cmp-40.This dataset comprises 27 negative and 13 positive cases selected from the Random-1000.We utilize this dataset to illustrate which of our design strategies contributed most to the outcome of our solution.</p>
<p>Turns and Conversations.Due to the progressive prompt, each case may require different turns (pairs of a prompt and a response).In Random-1000, the average number of turns is 2.78, with a max of 8 and a variance of 1.20.</p>
<p>Cost.On average, it costs 7,000 tokens in GPT-4 to analyze each potential bug.</p>
<p>RQ1: Precision</p>
<p>LLift reports 26 positives among the Random-1000 dataset, where half of them are true bugs based on our manual inspection.This represents a precision of 50%.In keeping with UBITect and we focus on the analysis of Linux v4.14, 12 of the bugs still exist in the latest Linux kernel.We are in the process of reporting the 12 bugs to the Linux community.So far, we have submitted patches for 4 bugs and received confirmation that they are true bugs.</p>
<p>RQ2: Recall Estimate</p>
<p>Conceptually, the core optimization (post-constraint guided path analysis) of LLift is sound, and we also prompt a series of rules to let LLMs tend to respond "may_init when uncertain.We expect LLift would not reject true bugs or with a high recall.We sample 300 negative cases from Random-1000 in an effort to see whether we will miss any true bugs.We confirm that all are true negatives.Despite the limited data sampled, this result indicates that integrating GPT-4 into our implementation does not introduce apparent unsoundness.</p>
<p>Further, we test LLift on the Bug-50 dataset to see whether it will miss any bugs discovered by UBITect.LLift has demonstrated full effectiveness in identifying all real bugs from Bug-50.This result, while encouraging, does not imply that LLift is flawless.Detailed data analysis reveals that: 1) There remain some inconsistencies in 3âˆ¼5 cases occasionally, though they are mitigated by majority voting; and 2) all the bugs found by UBITect have trivial postconstraints (C  = âŠ¤) and postcondition of may_init (P  : must_init â†¦ â†’ âˆ…).Hence, LLift could identify them easily.It is noteworthy that these cases are already those cases detectable by UBITect.Such cases tend to be simpler in nature and can be verified by symbolic execution in UBITect.</p>
<p>LLift has proven effective in identifying UBI bugs, consistently detecting all known instances.</p>
<p>RQ3: Contributions of Design Strategies</p>
<p>In our effort to delineate the contributions of distinct design strategies to the final results, we undertook an evaluative exercise against the Cmp-40 dataset, employing varying configurations of our solution, each entailing a unique combination of our proposed strategies.As illustrated in Table 4, the strategies under consideration encompass Post-constraint Analysis (PCA), Progressive Prompt (PP), Self-Validation (SV ), and Task Decomposition (TD).The findings underscore an overall trend of enhanced performance with the integration of additional design strategies.In this study, the Baseline corresponds to a straightforward prompt, "check this code to determine if there are any UBI bugs", a strategy that has been found to be rather insufficient for discovering new vulnerabilities, as corroborated by past studies [17,21,31], reflecting a modest recall rate of 0.15 and a precision of 0.12.</p>
<p>Incorporating PCA offers a notable enhancement, enabling the LLM to uncover a wider array of vulnerabilities.As shown in Table 4, there is a substantial improvement in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution.However, solely relying on this strategy still leaves a lot of room for optimization.</p>
<p>The influence of Progressive Prompt (PP) on the results is quite intriguing.While its impact appears to lower precision initially, the introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance.Without
âœ“ âœ“ âœ“ âœ— ctrl_cx2341x_getv4lflags âœ“ âœ“ âœ— âœ— axi_clkgen_recalc_rate âœ“ âœ“ âœ“ âœ“ max8907_regulator_probe âœ“ âœ“ âœ“ âœ“ ov5693_detect âœ“ âœ“ âœ— âœ“ iommu_unmap_page âœ“ âœ— âœ“ âœ— mt9m114_detect âœ“ âœ“ âœ“ âœ“ ec_read_u8 âœ“ âœ“ âœ“ âœ“ compress_sliced_buf âœ“ âœ“ âœ— âœ“
PP, the LLM is restricted to deducing the function behavior merely based on the function context's semantics without further code analysis.Even though this approach can be effective in a range of situations, it confines the reasoning ability to the information available in its training data.By checking the detailed conversation, we notice the omission of TD or SV tends to result in the LLM neglecting the post-constraints, subsequently leading to errors.Beyond influencing precision and recall, Task Decomposition (TD) and Self-Validation (SV ) also play a crucial role in enhancing consistency.In this context, a result is deemed consistent if the LLM yields the same outcome across its initial two runs.A comparison between our comprehensive final design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results, and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring reliable and consistent outcomes.</p>
<p>Finally, TD also holds significance in terms of conserving tokens.In our evaluation phase, we identified two instances within the PCA+PP and PCA+PP+SV configurations where the token count surpassed the limitations set by GPT-4.However, this constraint was not breached in any case when TD was incorporated.Takeaway 3.All of LLift's design strategies contributed to the positive results.</p>
<p>RQ4: Alternative Models</p>
<p>Table 5 provides a comprehensive view of the performance of our solution, LLift, when implemented across an array of LLMs including GPT-4.0,GPT-3.5, Claude 2 [2], and Bard [12].GPT-4 passes all tests, while GPT-3.5,Claude 2, and Bard exhibit recall rates of 89%, 67%, and 67%, respectively.Despite the unparalleled performance of GPT-4, the other LLMs still produce substantial and competitive results, thereby indicating the wide applicability of our approaches.</p>
<p>It is imperative to note that not all design strategies in our toolbox are universally applicable across all language models.Bard and GPT-3.5, in particular, exhibit limited adaptability towards the progressive prompt and task decomposition strategies.Bard's interaction patterns suggest a preference for immediate response generation, leveraging its internal knowledge base rather than requesting additional function definitions, thereby hindering the effectiveness of the progressive prompt approach.Similarly, when task Haonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution.However, solely relying on this strategy still leaves a lot of room for optimization.The inuence of Progressive Prompt (PP) on the results is quite intriguing.While its impact appears to lower precision initially, the introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance.Without PP, the LLM is restricted to deducing the function behavior merely based on the function context's semantics without further code analysis.Even though this approach can be eective in a range of situations, it connes the reasoning ability to the information available in its training data.By checking the detailed conversation, we notice the omission of TD or SV tends to result in the LLM neglecting the postcondition, subsequently leading to errors.</p>
<p>Beyond inuencing precision and recall, Task Decomposition (TD) and Self-Validation (SV ) also play a crucial role in enhancing consistency.In this context, a result is deemed consistent if the LLM yields the same outcome across its initial two runs.A comparison between our comprehensive nal design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results, and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring reliable and consistent outcomes.</p>
<p>Finally, TD also holds signicance in terms of conserving tokens.In our evaluation phase, we identied two instances within the PCA+PP and PCA+PP+SV congurations where the token count surpassed the limitations set by GPT-4.However, this constraint was not breached in any case when TD was incorporated.</p>
<p>Takeaway.All of LL's design strategies contributed to the positive results.</p>
<p>RQ4: Alternative Models</p>
<p>Table 5 provides a comprehensive view of the performance of our solution, LL, when implemented across an array of LLMs including GPT-4.0,GPT-3.5, Claude 2 [2], and Bard [12].GPT-4 passes all tests, while GPT-3.5,Claude 2, and Bard exhibit recall rates of 89%, 67%, and 67%, respectively.Despite the unparalleled performance of GPT-4, the other LLMs still produce substantial and competitive results, thereby indicating the wide applicability of our approaches.It is imperative to note that not all design strategies in our toolbox are universally applicable across all language models.Bard and GPT-3.5, in particular, exhibit limited adaptability towards the progressive prompt and task decomposition strategies.Bard's interaction patterns suggest a preference for immediate response generation, leveraging its internal knowledge base rather than requesting additional function denitions, thereby hindering the eectiveness of the progressive prompt approach.Similarly, when task decomposition is implemented, these models often misinterpret or inaccurately collect post-constraints, subsequently compromising the results.To harness their maximum potential, we apply a postcondition-aware design specically (i.e., without other design strategies) for GPT-3.5 and Bard.</p>
<p>Contrasting the GPT series, Bard and Claude 2 demonstrate less familiarity with the Linux kernel and are more prone to failures due to their unawareness of the may_init possibility of initializers.</p>
<p>Takeaway.GPT-4 remains at the pinnacle of performance for LL, yet other LLMs can achieve promising results.</p>
<p>Case Study</p>
<p>In this case study, we pick three interesting cases demonstrating the eectiveness of LL in analyzing function behaviors and detecting uninitialized variables.All these cases are undecided for the previous static analyzer, UBITect.We put the complete conversations on an anonymous online page for reference 2 .</p>
<p>Loop and Index.Figure 6 presents an intriguing case involving the variable pages[j], which is reported by UBITect as used in Line 17 potentially without being initialized.Unfortunately, this case is a false positive which is hard to prune due to loops.Specically, the initializer function get_user_pages_unlocked(), which is responsible for mapping user space pages into the kernel space, initializes the pages array allocated in Line 3. If get_user_pages_unlocked() is successfully executed, pages[0] through pages[res-1] pointers will be initialized to point to struct page instances.</p>
<p>To summarize the behavior, i.e., must_init facts under conditions where the use is reachable, we must rst extract the postconstraints that lead to the use of pages.Through interacting with ChatGPT, LL successfully extracts it: decomposition is implemented, these models often misinterpret or inaccurately collect post-constraints, subsequently compromising the results.To harness their maximum potential, we only apply the PCA design specifically (i.e., without other design strategies) for GPT-3.5 and Bard.Contrasting the GPT series, Bard and Claude 2 demonstrate less familiarity with the Linux kernel and are more prone to failures due to their unawareness of the may_init possibility of initializers.Takeaway 4. GPT-4 remains at the pinnacle of performance for LLift, yet other LLMs can achieve promising results.</p>
<p>Case Study</p>
<p>In this case study, we pick three interesting cases demonstrating the effectiveness of LLift in analyzing function behaviors and detecting uninitialized variables.All these cases are undecided for the previous static analyzer, UBITect.We put the complete conversations on an anonymous online page for reference 2 .</p>
<p>Loop and Index.Figure 7 presents an intriguing case involving the variable pages[j], which is reported by UBITect as used in Line 17 potentially without being initialized.Unfortunately, this case is a false positive which is hard to prune due to loops.Specifically, the initializer function get_user_pages_unlocked(), which is responsible for mapping user space pages into the kernel space, initializes the pages array allocated in Line 3. If get_user_pages_unlocked() is successfully executed, pages[0] through pages[res-1] pointers will be initialized to point to struct page instances.</p>
<p>To summarize the behavior, i.e., must_init facts under conditions where the use is reachable, we must first extract the postconstraints that lead to the use of pages.Through interacting with ChatGPT, LLift successfully extracts it: { "initializer": "res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ?FOLL_WRITE : 0)", "suspicious": ["pages[j]"], "postconstraint": "res &lt; nr_pages &amp;&amp; res &gt; 0 &amp;&amp; j &lt; res", } After feeding the post-constraints to LLM, LLift then successfully obtains the result:    As we can see, GPT-4 exhibits impressive comprehension of this complex function.It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1.This insight leads GPT-4 to correctly deduce that all elements in the pages array must be initialized, i.e., they are must_init.This example underscores GPT-4's prociency in handling loop and even index sensitivity.</p>
<p>Concurrency and Callback.Consider the case illustrated in Figure 7.At rst glance, UBITect ags Line 10 for potentially using the variable comp_pkt.completion_statusbefore initialization.The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug.However, the mystery unravels when we examine hv_pci_generic_compl(), the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread.Here wait_for_completion() is a synchronization primitive that pauses the current thread and waits for the new thread (i.e., hv_pci_generic_compl()) to complete.Despite this complexity, GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result.</p>
<p>It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives.LL prompts LLMs with "The 'initializer' must be the 'actual' function that initializes the variable." and then LLMs can automatically identify the function hv_pci_generic_compl() as the initializer of comp_pkt.completion_status.Unfamiliar Function.As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics (e.g., postconditions) of common functions like sscanf().However, the notion that "the LLM simply learns everything from the internet and acts merely as a blurry search engine" [6], is misguided according to our experiments.This viewpoint is robustly contradicted by the case illustrated in Figure 8.</p>
<p>The case presents an intriguing real-world bug.The function p9pdu_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu_read() returns non-zero in line 19 (thus "break" early).Notably, unlike sscanf(), where GPT-4 can provide a precise summary of the function without asking for its denition, it does request the function denition of p9pdu_readf(), as it is not as ubiquitous as sscanf().Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that ecode could be initialized when p9pdu_readf() returns 0, demonstrating the ecacy of LL for unfamiliar cases.The result is as follow:</p>
<p>Reason for Imprecision</p>
<p>Despite LL achieving a precision of 50% in real-world applications, the precision can still be improved in the future.Some can be solved with better prompts or better integration with static analysis.</p>
<p>Challenges in Constraint Extraction.Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints.For instance, during error handling, the checks for failures may involve another function or macro.This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis (e.g., path exploration in symbolic execution to collect the post-constraints).</p>
<p>Information Gaps in UBITect.For instance, UBITect does not provide explicit eld names within a structure when a specic  As we can see, GPT-4 exhibits impressive comprehension of this complex function.It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1.This insight leads GPT-4 to correctly deduce that all elements in the pages array must be initialized, i.e., they are must_init.This example underscores GPT-4's proficiency in handling loop and even index sensitivity.</p>
<p>Concurrency and Callback.Consider the case illustrated in Figure 8.At first glance, UBITect flags Line 10 for potentially using the variable comp_pkt.completion_statusbefore initialization.The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug.However, the mystery unravels when we examine hv_pci_generic_compl(), the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread.Here wait_for_completion() is a synchronization primitive that pauses the current thread and waits for the new thread (i.e., hv_pci_generic_compl()) to complete.Despite this complexity, GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result.</p>
<p>It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives.LLift prompts LLMs with "The 'initializer' must be the 'actual' function that initializes the variable." and then LLMs can automatically identify the function hv_pci_generic_compl() as the initializer of comp_pkt.completion_status.Unfamiliar Function.As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics (e.g., postconditions) of common functions like sscanf().However, some argue that "the LLM simply learns everything from the internet and acts merely as a search engine" [6].This viewpoint is challenged by the case illustrated in Figure 9.As we can see, GPT-4 exhibits impressive comprehension of this complex function.It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1.This insight leads GPT-4 to correctly deduce that all elements in the pages array must be initialized, i.e., they are must_init.This example underscores GPT-4's prociency in handling loop and even index sensitivity.</p>
<p>Concurrency and Callback.Consider the case illustrated in Figure 7.At rst glance, UBITect ags Line 10 for potentially using the variable comp_pkt.completion_statusbefore initialization.The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug.However, the mystery unravels when we examine hv_pci_generic_compl(), the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread.Here wait_for_completion() is a synchronization primitive that pauses the current thread and waits for the new thread (i.e., hv_pci_generic_compl()) to complete.Despite this complexity, GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result.</p>
<p>It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives.LL prompts LLMs with "The 'initializer' must be the 'actual' function that initializes the variable." and then LLMs can automatically identify the function hv_pci_generic_compl() as the initializer of comp_pkt.completion_status.Unfamiliar Function.As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics (e.g., postconditions) of common functions like sscanf().However, the notion that "the LLM simply learns everything from the internet and acts   merely as a blurry search engine" [6], is misguided according to our experiments.This viewpoint is robustly contradicted by the case illustrated in Figure 8.</p>
<p>The case presents an intriguing real-world bug.The function p9pdu_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu_read() returns non-zero in line 19 (thus "break" early).Notably, unlike sscanf(), where GPT-4 can provide a precise summary of the function without asking for its denition, it does request the function denition of p9pdu_readf(), as it is not as ubiquitous as sscanf().Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that ecode could be initialized when p9pdu_readf() returns 0, demonstrating the ecacy of LL for unfamiliar cases.The result is as follow:</p>
<p>Reason for Imprecision</p>
<p>Despite LL achieving a precision of 50% in real-world applications, the precision can still be improved in the future.Some can be solved with better prompts or better integration with static analysis.</p>
<p>Challenges in Constraint Extraction.Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints.For instance, during error handling, the checks for failures may involve another function or macro.This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis (e.g., path exploration in symbolic execution to collect the post-constraints).</p>
<p>Information Gaps in UBITect.For instance, UBITect does not provide explicit eld names within a structure when a specic  The case presents an intriguing real-world bug.The function p9pdu_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu_read() returns non-zero in line 19 (thus "break" early).Notably, unlike sscanf(), where GPT-4 can provide a precise summary of the function without asking for its definition, it does request the function definition of p9pdu_readf(), as it is not as ubiquitous as sscanf().</p>
<p>Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that ecode could be initialized when p9pdu_readf() returns 0, demonstrating the efficacy of LLift for unfamiliar cases.The result is as follows:</p>
<p>{ "initializer": "err = p9pdu_readf(req-&gt;rc, c-&gt;proto_version, 'd', &amp;ecode)", "suspicious": ["ecode"], "postconstraint": null, "response": { "must_init": [], "may_init": [{ "name": "ecode", "condition": "p9pdu_readf returns 0" }] } }</p>
<p>Reason for Imprecision</p>
<p>Despite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future.Some can be solved with better prompts or better integration with static analysis.</p>
<p>Challenges in Constraint Extraction.Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints.For instance, during error handling, the checks for failures may involve another function or macro.This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis (e.g., path exploration in symbolic execution to collect the post-constraints).</p>
<p>Information Gaps in UBITect.For instance, UBITect does not provide explicit field names within a structure when a specific software engineering.LLift complements these efforts by demonstrating the efficacy of LLMs in bug finding in the real world.</p>
<p>CONCLUSION</p>
<p>This work presents a novel approach that utilizes LLMs to aid static analysis using a completely automated agent.By carefully considering the scope and designing the interactions with LLMs, our solution has yielded promising results.We believe our effort only scratched the surface of the vast design space, and hope our work will inspire future research in this exciting direction.</p>
<p>Figure 1 :
1
Figure 1: Code snippet of sscanf and its usecase</p>
<p>2
2</p>
<p>Figure 1 :
1
Figure 1: Code snippet of sscanf and its usecase</p>
<p>Figure 2 :Figure 2 :Figure 3 :
223
Figure 2: The overview of LLift.Start with the discarded cases by UBITect and determine whether these potential bugs are true or false.</p>
<p>2 3Figure 4 :
24
Figure 4: Example run of LL.For each potential bug, we Â¨identify its initializer, â‰  extract the post-constraints of the initializer, and AE analyze the behavior of the initializer with the post-constraints via LLM.</p>
<p>2 3Figure 4 :
24
Figure 4: Example run of LLift.For each potential bug, LLift â‘  (Î¦ 1 ) identifies its initializer, â‘¡ (Î¦ 2 ) extracts the post-constraints of the initializer, and â‘¢ (Î¦ 3 ) analyzes the behavior of the initializer with the post-constraints via LLM.</p>
<p>{ 4 }
4
initializer : ret = sscanf(str, %u.%u.%u.%u%n ,&amp;a,&amp;b,&amp;c,&amp;d,&amp;n) , suspicious : [ a , b , c , d ], postconstraint : ret &gt;= The LLM may respond with { ret : success , response : { must_init : [ a , b , c , d ], may_init : [{ name : n , condition : ret &gt; 4 }] } } The response succinctly encapsulates the postcondition, where variables a,b,c,d are classied as must_init, and n is categorized as may_init.This is due to the initialization of n only occurring when A4C &gt; 4, and not when A4C 7 !4.</p>
<p>Figure 6 :
6
Figure 6: A sample case of initializer func, *a is may_init or must_init under different post-constraints.</p>
<p>1 static 3 if 6 / 7 if
1367
((pages = kmalloc(..., GFP_KERNEL)) == NULL) 4 return -ENOMEM; 5 res = get_user_pages_unlocked(..., pages, ...); * Errors and no page mapped should return here */</p>
<p>17 }Figure 6 :
176
Figure 6: Case Study I (Loop and Index).Derived from drivers/scsi/st.c</p>
<p>{ 8 Figure 7 :
87
Figure 7: Case Study I (Loop and Index).Derived from drivers/scsi/st.c</p>
<p>{</p>
<p>12 } 13 14 static 23 }Figure 7 :
121314237
Figure 7: Case Study II (Concurrency and Indirect Call).Derived from drivers/pci/host/pci-hyperv.c suspicious : [ pages[j] ], postconstraint : res &lt; nr_pages &amp;&amp; res &gt; 0 &amp;&amp; j &lt; res , } After feeding the post-constraints to LLM, LL then successfully obtains the result: { ret : success , response : { must_init : [ pages[j] ], may_init : [], } }</p>
<p>(req-&gt;rc, c-&gt;proto_version, d , &amp;ecode) , suspicious : [ ecode ], postconstraint : null, response : { must_init : [], may_init : [{ name : ecode , condition : p9pdu_readf returns 0 }] } }</p>
<p>9
9</p>
<p>Figure 8 :
8
Figure 8: Case Study II (Concurrency and Indirect Call).Derived from drivers/pci/host/pci-hyperv.c</p>
<p>The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models 1 static int hv_pci_enter_d0(struct hv_device *hdev){ 2</p>
<p>23 }Figure 7 :
237
Figure 7: Case Study II (Concurrency and Indirect Call).Derived from drivers/pci/host/pci-hyperv.c suspicious : [ pages[j] ], postconstraint : res &lt; nr_pages &amp;&amp; res &gt; 0 &amp;&amp; j &lt; res , } After feeding the post-constraints to LLM, LL then successfully obtains the result: { ret : success , response : { must_init : [ pages[j] ], may_init : [], } }</p>
<p>3int</p>
<p>err = p9pdu_readf(req-&gt;rc, c-&gt;proto_version, d , &amp;ecode); p9pdu_readf(struct p9_fcall <em>pdu, int proto_version, const char </em>fmt, ...)</p>
<p>26 }Figure 8 :
268
Figure 8: Case Study III (Unfamiliar Function), derived from net/9p</p>
<p>(req-&gt;rc, c-&gt;proto_version, d , &amp;ecode) , suspicious : [ ecode ], postconstraint : null, response : { must_init : [], may_init : [{ name : ecode , condition : p9pdu_readf returns 0 }] } }</p>
<p>9
9</p>
<p>Figure 9 :
9
Figure 9: Case Study III (Unfamiliar Function), derived from net/9p</p>
<p>Table 1 :
1
UBITect's summary for sscanf.Both use and initialization for va_args are incorrect.3 and 7 stand for whether this parameter will be used/initialized after its call."..." represents all other parameters of va_args.
buf fmt ... <em>buf </em>fmtUse Initialize3 73 3 7 73 73 7</p>
<p>Table 1 :
1
UBITect's summary for sscanf.Both use and initialization for va_args are incorrect.âœ“ and âœ— stand for whether this parameter will be used/initialized after its call."..." represents all other parameters of va_args.
buf fmt ... <em>buf </em>fmtUse Initializeâœ“ âœ—âœ“ âœ“ âœ— âœ—âœ“ âœ—âœ“ âœ—review responses at various stages to obtain accurate and reliable responses.</p>
<p>Table 2 :
2
Two types of post-constraints and their variants.The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models</p>
<p>Table 2 :
2
Two types of post-constraints and their variants.
Check Before UseFailure CheckType A:Type B:if (sscanf(...) &gt;= 4) {err = func(&amp;a);use(a, b, c, d);if (err) { return/break/goto; }}use(a)Type A':Type B':switch(ret=func(&amp;a)){while(func(&amp;a)){case some_irrelevant_case:do_something(...);do_something(...);}break;use(a);case critical_case:use(a);}</p>
<p>Table 3 :
3
14ue bugs identified by LLift from Random-1000, analyzing in Linux v4.14Imprecise and Failed Cases.Despite the effectiveness of LLift, there are instances where it does not yield precise results, resulting in 13 false positives by mistakenly classifying must_init cases as may_init.Upon a careful examination of these cases, we attribute the imprecision to a variety of factors, which we discuss in detail in Â§6.7.Briefly, we give a breakdown of them here: Incomplete con-
InitializerCallerFile PathVariableLineread_reg regmap_read ep0_read_setup regmap_read bcm3510_do_hab_cmd bcm3510_check_firmware_version drivers/media/dvb-frontends/bcm3510.c get_signal_parameters drivers/media/dvb-frontends/stv0910.c isc_update_profile drivers/media/platform/atmel/atmel-isc.c ep0_handle_setup drivers/usb/mtu3/mtu3_gadget_ep0.c mdio_sc_cfg_reg_write drivers/net/ethernet/hisilicon/hns_mdio.c readCapabilityRid airo_get_range drivers/net/wireless/cisco/airo.c e1e_rphy __e1000_resume drivers/net/ethernet/intel/e1000e/netdev.c pci_read_config_dword adm8211_probe drivers/net/wireless/admtek/adm8211.c lan78xx_read_reg lan78xx_write_raw_otp drivers/net/usb/lan78xx.c t1_tpi_read my3126_phy_reset drivers/net/ethernet/chelsio/cxgb/my3126.c val tmp sr setup.bRequestType 637 504 664 reg_value 169 ver.demod_version 666 cap_rid.softCap 6936 phy_data 6580 reg 1814 buf 873 193 pci_read_config_dword quirk_intel_purley_xeon_ras_cap arch/x86/kernel/quirks.c capid0 562 ata_timing_compute opti82c46x_set_piomode drivers/ata/pata_legacy.c &amp;tp 564 pt_completion pt_req_sense drivers/block/paride/pt.c buf 368
straint extraction (4 cases), Information gaps in UBITect (5 cases), Variable reuse (1 case), Indirect call (1 case), and Additional constraints (1 case).Additionally, there is one false positive caused by inconsistent output (i.e., two false positives in three runs).Four cases exceed the maximum context length while exploring deeper functions in the progressive prompt.Takeaway 1. LLift Can effectively summarize initializer behavior and discover new bugs with high precision (50%).</p>
<p>Table 4 :
4
Performance evaluation of bug detection tool with progressive addition of design components: Post-Constraint Guided Path Analysis (PCA), Progressive Prompt (PP), Self-Validation (SV), and Task Decomposition (TD).(C) indicates the number of Consistent cases.
CombinationTN(C) TP(C) Precision Recall Accuracy F1 ScoreSimple Prompt PCA PCA+PP PCA+PP+SV PCA+PP+TD PCA+PP+SV+TD 25(17) 13(12) 12(9) 2(1) 13(9) 5(1) 5(3) 6(1) 5(2) 11(8) 22(14) 6(4)0.12 0.26 0.21 0.33 0.55 0.870.15 0.38 0.46 0.85 0.46 1.000.35 0.45 0.28 0.40 0.70 0.950.13 0.31 0.29 0.48 0.50 0.93Oracle27</p>
<p>Table 5 :
5
Comparison of different LLMs on real bugs, from a subset of Bug-50
CallerGPT 4 3.5Claude2 Bardhpet_msi_resume</p>
<p>Table 5 :
5
Comparison of dierent LLMs on real bugs, from a subset of Bug-50
CallerGPT 4 3.5Claude2 Bardhpet_msi_resume ctrl_cx2341x_getv4lags axi_clkgen_recalc_rate max8907_regulator_probe 3 3 3 3 3 3 3 3 ov5693_detect 3 3 iommu_unmap_page 3 7 mt9m114_detect 3 3 ec_read_u8 3 3 compress_sliced_buf 3 33 7 3 3 7 3 3 3 77 7 3 3 3 7 3 3 3
https://sites.google.com/view/llift-open/case-studies
field is in use.This information gap can result in LLift lacking precision in its analysis.Additionally, UBITect only reports the variable utilized, not necessarily the same variable passed to an initializer.For example, consider an uninitialized variable a passed to an initializer, which is then assigned to variable b for usage.In such a scenario, LLift may fail to identify the initializer due to this incomplete information correctly.These challenges, primarily due to the interface design in UBITect, can be addressed with focused engineering efforts to enrich the output information from UBITect.Variable Reuse.Varaible reuse is an interesting problem of LLM.In general, LLM usually confuses different variables in different scopes (e.g., different function calls).For example, if the suspicious variable is ret and passed as a argument to its initializer (say, func(&amp;ret)) and there is another stack variable defined in func also called ret, LLM will confuse them.Explicitly prompting and teaching LLM to note the difference does not appear to work.One solution is to leverage a simple static analysis to normalize the source code to ensure each variable has a unique name.Indirect Call.As mentioned Â§4.4,LLift follows a simple but imprecise strategy to handle indirect calls.Theoretically, existing static analysis tools, such as MLTA[16], can give possible targets for indirect calls.However, each indirect call may have multiple possible targets and dramatically increase the token usage.We leave the exploration of such an exhaustive strategy for future work.LLift may benefit from a more precise indirect call resolution.Additional Constraints.There are many variables whose values are determined outside of the function we analyze, e.g., preconditions capturing constraints from the outer caller.Since our analysis is fundamentally under-constrained, this can lead LLift to incorrectly determine a must_init case to be may_init.Mitigating this imprecision relies on further analysis to provide more information.DISCUSSION AND FUTURE WORKPost-Constraint Analysis.Our approach prioritizes postconstraints over other constraints, such as preconditions.By focusing on the post-constraints, we enhance the precision and scalability significantly.Importantly, our utilization of large language models in program analysis suggests strong abilities in summarizing complex function behaviors involving loops, a classic hurdle in program analysis.Better Integration with Static Analysis.Our work presents opportunities for greater integration and synergy with static analysis methods.Currently, our proposed solution operates largely independently of the static analysis methods, taking only inputs from static analysis initially.Looking into the future, we can consider integrating static analysis and LLMs in a holistic workflow.For example, this could involve selectively utilizing LLM as an assistant to overcome certain hurdles encountered by static analysis, e.g., difficulty in scaling up the analysis or summarizing loop invariants.In turn, further static analysis based on these findings can provide insights to refine the queries to the LLM.This iterative process could enable a more thorough and accurate analysis of complex cases.We believe such a more integrated approach is a very promising future direction.Deploying on Open-sourced LLMs.The reproducibility of LLift could be potentially challenged, considering its dependency on GPT-4, a closed-source API subject to frequent updates.At the time of writing, Meta introduced Llama 2, an open-source language model with capabilities rivaling GPT-3.5.Our initial assessments suggest that Llama 2 can understand our instructions and appears well-suited to support LLift.The open-source nature of Llama 2 provides us with opportunities to deploy and refine the model further.We plan to leverage these prospects in future studies.RELATED WORKTechniques of Utilizing LLMs.Wang et al.[33]propose an embodied lifelong learning agent based on LLMs.Pallagani et al.[23]explores the capabilities of LLMs for automated planning.Weng[35]summarizes recent work in building an autonomous agent based on LLMs and proposes two important components for planning: Task Decomposition and Self-reflection, which are similar to the design of LLift.Beyond dividing tasks into small pieces, task decomposition techniques also include some universal strategies such as Chain-of-thought[34]and Tree-of-thought[38].The general strategy of self-reflection has been used in several flavors: ReAct[39], Reflexion[29]and Chain of Hindsight[15].Despite the similarity in name, self-reflection is fundamentally different from self-validation in LLift where the former focuses on using external sources to provide feedback to their models.Huang et al.[10]let an LLM self-improve its reasoning without supervised data by asking the LLM to lay out different possible results.LLMs for Program Analysis.Ma et al.[17]and Sun et al.[30]explore the capabilities of LLMs when performing various program analysis tasks such as control flow graph construction, call graph analysis, and code summarization.They conclude that while LLMs can comprehend basic code syntax, they are somewhat limited in performing more sophisticated analyses such as pointer analysis and code behavior summarization.In contrast to their findings, our research with LLift has yielded encouraging results.We conjecture that this might be due to several reasons: (1) benchmark selection, i.e., Linux kernel vs. others.(2) Prompt designs.(3) GPT-3.5 vs. GPT-4.0-prior work only evaluated the results using only GPT-3.5.Pei et al.[26]use LLMs to reason about loop invariants with decent performance.In contrast, LLift leverages LLMs for a variety of tasks (including program behavior summarization) and integrates them successfully into a static analysis pipeline.LLMs for Software Engineering.Xia et al.[36]propose an automated conversation-driven program repair tool using ChatGPT, achieving nearly 50% success rate.Pearce et al.[25]examine zeroshot vulnerability repair using LLMs and found promise in synthetic and hand-crafted scenarios but faced challenges in real-world examples.Chen et al.[5]teach LLMs to debug its own predicted program to increase its correctness, but only performs on relatively simple programs.Lemieux et al.[14]leverages LLM to generate tests for uncovered functions when the search-based approach got coverage stalled.Feng and Chen[7]use LLM to replay Android bug automatedly.Recently, LangChain proposed LangSimith[13], a LLM-powered platform for debugging, testing, and evaluating.These diverse applications underline the vast potential of LLMs in
Improving Few-Shot Prompts with Relevant Static Analysis Products. Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, Earl T Barr, arXiv:2304.068152023</p>
<p>. Anthropic, 20232023Claude 2.</p>
<p>When do you need Chain-of-Thought Prompting for ChatGPT?. Jiuhai Chen, Lichang Chen, Heng Huang, Tianyi Zhou, arXiv:2304.032622023</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Teaching Large Language Models to Self-Debug. Xinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, Denny Zhou, 2023</p>
<p>ChatGPT Is a Blurry JPEG of the Web. The New Yorker. Ted Chiang, of-the-web Section: annals of artificial intelligence. 2023. Feb. 2023</p>
<p>Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models. Sidong Feng, Chunyang Chen, 10.48550/arXiv.2306.01987arXiv:2306.019872023</p>
<p>Github, GitHub Copilot documentation. 2023</p>
<p>Anjana Gosain, Ganga Sharma, Intelligent Computing and Applications (Advances in Intelligent Systems and Computing). Rajib Kar, Swagatam Das, Bijaya Ketan, Panigrahi , India; New DelhiSpringer2015Static Analysis: A Survey of Techniques and Tools</p>
<p>Large Language Models Can Self-Improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022</p>
<p>Survey of Hallucination in Natural Language Generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730Comput. Surveys. 552023. Dec. 2023</p>
<p>Bard's latest update: more features, languages and countries. Jack Krawczyk, Amarnag Subramanya, 10.1145/35717302023</p>
<p>Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications. Langchain, 2023. 2023</p>
<p>CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pretrained Large Language Models. Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, Siddhartha Sen, 2023. 2023</p>
<p>Chain of Hindsight Aligns Language Models with Feedback. Hao Liu, Carmelo Sferrazza, Pieter Abbeel, arXiv:2302.026762023</p>
<p>Where Does It Go?: Refining Indirect-Call Targets with Multi-Layer Type Analysis. Kangjie Lu, Hong Hu, 10.1145/3319535.3354244Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. the 2019 ACM SIGSAC Conference on Computer and Communications SecurityLondon United KingdomACM2019</p>
<p>Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Yang Liu, 10.1145/3319535.3354244arXiv:2305.12138The Scope of ChatGPT in Software Engineering: A Thorough Investigation. 2023</p>
<p>Object-Oriented Software Construction. Bertrand Meyer, 1997Prentice-Hall2nd Edition</p>
<p>Openai, 2022. Introducing ChatGPT. 2022</p>
<ol>
<li>Function calling and other API updates. Openai, 2023</li>
</ol>
<p>Openai, arXiv:2303.087742023. GPT-4 Technical Report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, arXiv:2203.02155Jan Leike, and Ryan Lowe. 2022</p>
<p>Understanding the Capabilities of Large Language Models for Automated Planning. Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, Andrea Loreggia, arXiv:2305.161512023</p>
<p>A Survey of Parametric Static Analysis. Jihyeok Park, Hongki Lee, Sukyoung Ryu, 10.1145/3464457ACM Comput. Surv. 54372022. 2022</p>
<p>Examining Zero-Shot Vulnerability Repair with Large Language Models. Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt, 10.1109/SP46215.2023.000012023 IEEE Symposium on Security and Privacy (S&amp;P). Los Alamitos, CA, USAIEEE Computer Society2023</p>
<p>Can Large Language Models Reason about Program Invariants?. Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, Pengcheng Yin, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>What is Temperature in NLP?. Luke Salamone, 2021</p>
<p>Best practices for prompt engineering with OpenAI API | OpenAI Help Center. Jessica Shieh, 2023</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.113662023</p>
<p>Automatic Code Summarization via ChatGPT: How Far Are We?. Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, Hanwei Qian, Yang Liu, Zhenyu Chen, arXiv:2305.128652023</p>
<p>Is ChatGPT the Ultimate Programming Assistant -How far is it?. Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, TegawendÃ© F BissyandÃ©, arXiv:2304.119382023</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023</p>
<p>LLM-powered Autonomous Agents. Lilian Weng, 2023. Jun 2023</p>
<p>Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. Chunqiu Steven, Xia , Lingming Zhang, 2023</p>
<p>A systematic evaluation of large language models of code. F Frank, Uri Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, 10.1145/3520312.3534862Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine ProgrammingSan Diego CA USAACM2022</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 10.1145/3520312.3534862arXiv:2305.106012023</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ReAct: Synergizing Reasoning and Acting in Language Models. International Conference on Learning Representations (ICLR). 2023. 2023</p>
<p>UBITect: A Precise and Scalable Method to Detect Use-before-Initialization Bugs in Linux Kernel. Yizhuo Zhai, Yu Hao, Hang Zhang, Daimeng Wang, Chengyu Song, Zhiyun Qian, Mohsen Lesani, V Srikanth, Paul Krishnamurthy, Yu, Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering2020. 2020</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Jiang, arXiv:2303.18223[cs.CL]A Survey of Large Language Models. Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, Ruiyang Ren, Yifan Li, Xinyu Tang2023</p>
<p>Why Does ChatGPT Fall Short in Providing Truthful Answers?. Shen Zheng, Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2304.105132023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-795 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-795</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-795</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-2872916</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/1705.05637v1.pdf" target="_blank">Text-based Adventures of the Golovin AI Agent</a></p>
                <p><strong>Paper Abstract:</strong> The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments. In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map. We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e795.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e795.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Golovin AI Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-tailored agent for playing text-based interactive fiction that combines large, domain-specific command pattern databases with word embeddings and neural language models, plus specialized behaviors (battle mode, inventory handling) and an internal map graph for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Golovin is an architecture combining (1) large curated command-pattern databases (from tutorials, walkthroughs, decompiled games), (2) word2vec embeddings (trained on 3000 fantasy books) for synonym/affordance extraction, (3) an LSTM neural language model with attention to score words' importance in scene descriptions, and (4) modular command generators (battle mode, item gathering, inventory, general actions, movement). Command candidates are generated by matching nouns found in the current textual description (and inventory) to command patterns (with synonym replacement), scoring each candidate by popularity, embedding cosine similarities, word rarity, neural attention weights, and overlap with description words, and then selected stochastically (roulette-wheel). Specialized subsystems: a battle mode that biases toward repeating fight commands, an inventory manager that proactively tries to take high-weight nouns, a blacklist of failed commands per location, a restart-and-replay mechanism that remembers best sequences, and an internal map graph builder/merger used for planning navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive Fiction games (Z-machine text-based adventure games; Text-Based Adventure AI Competition benchmark of 50 games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Classic single-player interactive fiction: a partially-observable, often non-deterministic textual environment presented as a sequence of natural-language room/area descriptions (rooms can be relabeled or change text over time), with actions given as textual commands. Challenges include partial observability, ambiguous/multi-mapping descriptions, a large unknown action space, puzzles requiring multi-step plans, and delayed or sparse scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>word2vec word embeddings (TensorFlow implementation) trained on 3000 fantasy books; an LSTM + attention neural language model (trained on same fantasy corpus) to weight words in descriptions; a pre-extracted command-pattern database (≈250k patterns) obtained by parsing tutorials, walkthroughs, and decompiled games; NLTK PCFG parser used during preprocessing; the agent's internal map-builder (graph data structure) used as a planning tool.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>word2vec -> dense word vectors and cosine-similarity scores for synonym/affordance matching; LSTM+attention -> per-word importance weights (attention scores) and language-model-based weights for commands; command-pattern DB -> textual command templates and frequency counts; internal map-builder -> structured graph (nodes labeled by first sentence, edges labeled by movement commands), merge/consistency diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A structured, mixed representation consisting of an internal labeled graph (map) of visited areas (nodes labeled by first sentence of descriptions), an inventory representation (list of carried items), per-location blacklists of commands that previously had no effect, command-frequency statistics, short play-history (path that led to best score) and the current scene textual description. Neural outputs (attention weights) and embedding-based synonym matches are used transiently to score and generate actions but also inform 'curiosity' estimates for nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When the textual description changes, command lists are regenerated using the LSTM attention scores and word2vec synonyms. Executed movement commands append/update edges in the map graph; nodes are labeled by the first sentence and MergeNodes attempts to merge nodes with identical labels by recursively joining outgoing edges when consistent. If a command leaves the description unchanged, that command is blacklisted for that location. Inventory changes trigger regeneration of commands that use newly acquired items and clearing of all blacklists. Map merges are applied only when heuristic MergeNodes succeeds; otherwise tentative changes are rolled back.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Primarily reactive, score-driven action selection (ranking/generation of textual commands using neural and embedding signals, then stochastic sampling). For navigation, Golovin uses graph-based planning over its internal map: it scores candidate destination nodes by a heuristic combining shortest-path distance and a curiosity metric (number/score of untested commands and untried movement commands) and then follows the shortest path to the best-scored destination. Battle mode enforces repeated execution of fight commands (a behavior-policy bias).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based planning: internal map with nodes labeled by first sentence and edges labeled by movement commands (assumed deterministic). Destination evaluation minimizes distance + (tested_commands / node_curiosity). Once a destination is chosen, the agent follows a shortest path in the graph (shortest in number of moves) to that node.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Golovin demonstrates that combining language-model tools (word2vec for synonyms/affordances; LSTM+attention for word importance) with a lightweight internal mapping and domain-specific behaviors (battle, inventory heuristics) yields effective behavior in partially observable text games. The paper reports ablations (map on/off, battle mode on/off) and finds battle mode strongly beneficial and the map helpful to a limited extent. Tool outputs are explicitly incorporated: embeddings drive command generation, LSTM attention weights inform object importance, and the map is the belief/plan representation for navigation. The agent uses a hybrid reactive + graph-planning approach rather than full model-based symbolic planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e795.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e795.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BYU-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BYU-Agent (Perception, Control and Cognition lab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A competing agent that uses Q-learning over hashed textual states and affordance extraction from word embeddings to construct plausible verb-noun actions; used as the 2016 Text-Based Adventure AI Competition winner baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BYU-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>BYU-Agent represents game states as hashes of textual descriptions and applies Q-learning to learn action utilities. Its affordance detection module uses word2vec embeddings (trained on Wikipedia in their work) to generate verb-noun affordances (plausible actions for in-game objects), thereby reducing and shaping the action space available to Q-learning. The agent learns across many epochs (paper reports training for many epochs) and stores Q-values for (hashed-state, action) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive Fiction games (Z-machine text-based adventure games; same 50-game benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based adventure games with partial observability, large natural-language state descriptions, and a huge implicit action space; the agent receives only text descriptions and must infer available actions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>word2vec embeddings (Wikipedia-based corpus used by BYU-Agent) for affordance extraction; Q-learning algorithm used to learn policies (not an external 'tool' but the principal learning method).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>word vectors -> similarity scores and nearest neighbors; affordance lists -> candidate verb-noun action pairs; hashed textual state -> discrete state identifier.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>State representation is a hash of the textual description (single discrete identifier per observed text). There is no explicit long-term map; the hash functions as the 'state' for Q-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>State is recomputed each step as the hash of the current textual description; Q-values are updated via Q-learning using observed rewards. Affordance outputs are used to generate candidate actions at each state but are not merged into a separate structured belief beyond the current hashed state.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free reinforcement learning (Q-learning) over hashed textual states with an affordance-guided action space reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported winner of initial competition achieved 18/100 on the competition benchmark (as stated in the paper); Table II in the paper shows per-game maximum achieved scores across training runs for BYU-Agent on the 50-game set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Affordance extraction via word embeddings is effective at pruning/generating reasonable action sets from raw text, enabling Q-learning to learn useful policies in some games, but the learned approach requires many training epochs and is complementary to domain-heuristic methods like Golovin's.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e795.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e795.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM+DQN agent (Narasimhan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-understanding agent using LSTM for state encoding and Deep Q-Networks for policy learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-module agent that encodes textual observations with an LSTM into a state vector and uses a DQN to learn action-value estimates over that learned state representation for text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Understanding for Text-based Games using Deep Reinforcement Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM + DQN agent (Narasimhan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in the related work, this agent first maps textual scene descriptions to learned vector states using an LSTM encoder, then uses a Deep Q-Network to approximate Q-values for candidate actions. The pipeline converts raw text to a compact learned representation that is then consumed by a DQN-based learner to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based / MUD-style games (small-to-medium quests as evaluated in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environments where raw descriptions must be mapped into suitable state representations for RL; challenges include sparse rewards and large action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>LSTM neural encoder for textual observations; Deep Q-Network (DQN) for policy/value learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>LSTM -> dense continuous state vectors encoding recent textual context; DQN -> scalar Q-values per action (numerical estimates).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit learned continuous state vector produced by the LSTM that summarizes recent textual observations; this functions as the agent's belief/state for the DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep the LSTM encodes the new text observation into a new vector state; this vector is used by the DQN to estimate action utilities and is updated stepwise as new observations arrive.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free deep reinforcement learning (DQN) operating over learned LSTM state encodings; planning is via learned Q-values rather than explicit search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported to be able to successfully complete quests in small and some medium-size games (qualitative claim in the paper); no per-task numeric breakdown provided in this paper's text beyond that statement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a compact representation of textual observations via an LSTM and coupling it to a DQN enables end-to-end learning in text environments; the LSTM encoder functions as the belief/state summarizer for the downstream RL module.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e795.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e795.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-based planner (first-order)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First-order logic state-tracking and planner (logic-based agent from related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic approach that translates textual observations into first-order logic, maintains a logical belief state updated from observations and actions, and interleaves planning and execution to solve puzzles in partially observable domains (under deterministic-action assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Logic-based planner / state tracker</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Summarized from related work: the system extracts logical predicates from textual observations, represents the world state in first-order logic, updates this belief state after each action/observation, and uses a symbolic planner that interleaves planning and execution to generate plans that achieve goals. The extension provably handles partially observable, initially unknown domains assuming deterministic actions without conditional effects.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based adventure / partially observable planning domains (as discussed in the cited related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable domains where actions need planning based on an evolving, logically represented belief about objects, locations, and preconditions; challenges include unknown domain models and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>First-order logic representation and classical planning/search over symbolic representations (tools: logic translators and a symbolic planner in the referenced works).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured logical predicates and formulas representing world state; symbolic plans (sequences of actions).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Explicit first-order logical belief state (set of predicates) that is updated deterministically from sequences of actions and observations.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Observations are translated into logical assertions; actions update the belief via logical consequence/transition rules. The extended system interleaves planning and execution, updating the logic-based belief after each executed action and observation.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Symbolic, model-based planning interleaved with execution; correctness formally proved under assumptions (deterministic actions, no conditional effects).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic belief tracking in first-order logic enables principled planning in partially observable text domains and can generate correct plans under strict assumptions, but relies on deterministic-action models and does not directly address noisy or language-ambiguous observations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Goal Achievement in Partially Known, Partially Observable Domains <em>(Rating: 2)</em></li>
                <li>Knowledge-gathering agents in adventure games <em>(Rating: 2)</em></li>
                <li>Language Understanding for Text-based Games using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Adventure games: A challenge for cognitive robotics <em>(Rating: 1)</em></li>
                <li>What can you do with a rock? Affordance extraction via word embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-795",
    "paper_id": "paper-2872916",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "Golovin",
            "name_full": "Golovin AI Agent",
            "brief_description": "A domain-tailored agent for playing text-based interactive fiction that combines large, domain-specific command pattern databases with word embeddings and neural language models, plus specialized behaviors (battle mode, inventory handling) and an internal map graph for navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Golovin",
            "agent_description": "Golovin is an architecture combining (1) large curated command-pattern databases (from tutorials, walkthroughs, decompiled games), (2) word2vec embeddings (trained on 3000 fantasy books) for synonym/affordance extraction, (3) an LSTM neural language model with attention to score words' importance in scene descriptions, and (4) modular command generators (battle mode, item gathering, inventory, general actions, movement). Command candidates are generated by matching nouns found in the current textual description (and inventory) to command patterns (with synonym replacement), scoring each candidate by popularity, embedding cosine similarities, word rarity, neural attention weights, and overlap with description words, and then selected stochastically (roulette-wheel). Specialized subsystems: a battle mode that biases toward repeating fight commands, an inventory manager that proactively tries to take high-weight nouns, a blacklist of failed commands per location, a restart-and-replay mechanism that remembers best sequences, and an internal map graph builder/merger used for planning navigation.",
            "environment_name": "Interactive Fiction games (Z-machine text-based adventure games; Text-Based Adventure AI Competition benchmark of 50 games)",
            "environment_description": "Classic single-player interactive fiction: a partially-observable, often non-deterministic textual environment presented as a sequence of natural-language room/area descriptions (rooms can be relabeled or change text over time), with actions given as textual commands. Challenges include partial observability, ambiguous/multi-mapping descriptions, a large unknown action space, puzzles requiring multi-step plans, and delayed or sparse scoring.",
            "is_partially_observable": true,
            "external_tools_used": "word2vec word embeddings (TensorFlow implementation) trained on 3000 fantasy books; an LSTM + attention neural language model (trained on same fantasy corpus) to weight words in descriptions; a pre-extracted command-pattern database (≈250k patterns) obtained by parsing tutorials, walkthroughs, and decompiled games; NLTK PCFG parser used during preprocessing; the agent's internal map-builder (graph data structure) used as a planning tool.",
            "tool_output_types": "word2vec -&gt; dense word vectors and cosine-similarity scores for synonym/affordance matching; LSTM+attention -&gt; per-word importance weights (attention scores) and language-model-based weights for commands; command-pattern DB -&gt; textual command templates and frequency counts; internal map-builder -&gt; structured graph (nodes labeled by first sentence, edges labeled by movement commands), merge/consistency diagnostics.",
            "belief_state_mechanism": "A structured, mixed representation consisting of an internal labeled graph (map) of visited areas (nodes labeled by first sentence of descriptions), an inventory representation (list of carried items), per-location blacklists of commands that previously had no effect, command-frequency statistics, short play-history (path that led to best score) and the current scene textual description. Neural outputs (attention weights) and embedding-based synonym matches are used transiently to score and generate actions but also inform 'curiosity' estimates for nodes.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When the textual description changes, command lists are regenerated using the LSTM attention scores and word2vec synonyms. Executed movement commands append/update edges in the map graph; nodes are labeled by the first sentence and MergeNodes attempts to merge nodes with identical labels by recursively joining outgoing edges when consistent. If a command leaves the description unchanged, that command is blacklisted for that location. Inventory changes trigger regeneration of commands that use newly acquired items and clearing of all blacklists. Map merges are applied only when heuristic MergeNodes succeeds; otherwise tentative changes are rolled back.",
            "planning_approach": "Primarily reactive, score-driven action selection (ranking/generation of textual commands using neural and embedding signals, then stochastic sampling). For navigation, Golovin uses graph-based planning over its internal map: it scores candidate destination nodes by a heuristic combining shortest-path distance and a curiosity metric (number/score of untested commands and untried movement commands) and then follows the shortest path to the best-scored destination. Battle mode enforces repeated execution of fight commands (a behavior-policy bias).",
            "uses_shortest_path_planning": true,
            "navigation_method": "Graph-based planning: internal map with nodes labeled by first sentence and edges labeled by movement commands (assumed deterministic). Destination evaluation minimizes distance + (tested_commands / node_curiosity). Once a destination is chosen, the agent follows a shortest path in the graph (shortest in number of moves) to that node.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Golovin demonstrates that combining language-model tools (word2vec for synonyms/affordances; LSTM+attention for word importance) with a lightweight internal mapping and domain-specific behaviors (battle, inventory heuristics) yields effective behavior in partially observable text games. The paper reports ablations (map on/off, battle mode on/off) and finds battle mode strongly beneficial and the map helpful to a limited extent. Tool outputs are explicitly incorporated: embeddings drive command generation, LSTM attention weights inform object importance, and the map is the belief/plan representation for navigation. The agent uses a hybrid reactive + graph-planning approach rather than full model-based symbolic planning.",
            "uuid": "e795.0"
        },
        {
            "name_short": "BYU-Agent",
            "name_full": "BYU-Agent (Perception, Control and Cognition lab)",
            "brief_description": "A competing agent that uses Q-learning over hashed textual states and affordance extraction from word embeddings to construct plausible verb-noun actions; used as the 2016 Text-Based Adventure AI Competition winner baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "BYU-Agent",
            "agent_description": "BYU-Agent represents game states as hashes of textual descriptions and applies Q-learning to learn action utilities. Its affordance detection module uses word2vec embeddings (trained on Wikipedia in their work) to generate verb-noun affordances (plausible actions for in-game objects), thereby reducing and shaping the action space available to Q-learning. The agent learns across many epochs (paper reports training for many epochs) and stores Q-values for (hashed-state, action) pairs.",
            "environment_name": "Interactive Fiction games (Z-machine text-based adventure games; same 50-game benchmark)",
            "environment_description": "Text-based adventure games with partial observability, large natural-language state descriptions, and a huge implicit action space; the agent receives only text descriptions and must infer available actions.",
            "is_partially_observable": true,
            "external_tools_used": "word2vec embeddings (Wikipedia-based corpus used by BYU-Agent) for affordance extraction; Q-learning algorithm used to learn policies (not an external 'tool' but the principal learning method).",
            "tool_output_types": "word vectors -&gt; similarity scores and nearest neighbors; affordance lists -&gt; candidate verb-noun action pairs; hashed textual state -&gt; discrete state identifier.",
            "belief_state_mechanism": "State representation is a hash of the textual description (single discrete identifier per observed text). There is no explicit long-term map; the hash functions as the 'state' for Q-learning.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "State is recomputed each step as the hash of the current textual description; Q-values are updated via Q-learning using observed rewards. Affordance outputs are used to generate candidate actions at each state but are not merged into a separate structured belief beyond the current hashed state.",
            "planning_approach": "Model-free reinforcement learning (Q-learning) over hashed textual states with an affordance-guided action space reduction.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "Reported winner of initial competition achieved 18/100 on the competition benchmark (as stated in the paper); Table II in the paper shows per-game maximum achieved scores across training runs for BYU-Agent on the 50-game set.",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Affordance extraction via word embeddings is effective at pruning/generating reasonable action sets from raw text, enabling Q-learning to learn useful policies in some games, but the learned approach requires many training epochs and is complementary to domain-heuristic methods like Golovin's.",
            "uuid": "e795.1"
        },
        {
            "name_short": "LSTM+DQN agent (Narasimhan et al.)",
            "name_full": "Language-understanding agent using LSTM for state encoding and Deep Q-Networks for policy learning",
            "brief_description": "A two-module agent that encodes textual observations with an LSTM into a state vector and uses a DQN to learn action-value estimates over that learned state representation for text-based games.",
            "citation_title": "Language Understanding for Text-based Games using Deep Reinforcement Learning.",
            "mention_or_use": "mention",
            "agent_name": "LSTM + DQN agent (Narasimhan et al.)",
            "agent_description": "Described in the related work, this agent first maps textual scene descriptions to learned vector states using an LSTM encoder, then uses a Deep Q-Network to approximate Q-values for candidate actions. The pipeline converts raw text to a compact learned representation that is then consumed by a DQN-based learner to select actions.",
            "environment_name": "Text-based / MUD-style games (small-to-medium quests as evaluated in the cited work)",
            "environment_description": "Partially observable text environments where raw descriptions must be mapped into suitable state representations for RL; challenges include sparse rewards and large action spaces.",
            "is_partially_observable": true,
            "external_tools_used": "LSTM neural encoder for textual observations; Deep Q-Network (DQN) for policy/value learning.",
            "tool_output_types": "LSTM -&gt; dense continuous state vectors encoding recent textual context; DQN -&gt; scalar Q-values per action (numerical estimates).",
            "belief_state_mechanism": "Implicit learned continuous state vector produced by the LSTM that summarizes recent textual observations; this functions as the agent's belief/state for the DQN.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep the LSTM encodes the new text observation into a new vector state; this vector is used by the DQN to estimate action utilities and is updated stepwise as new observations arrive.",
            "planning_approach": "Model-free deep reinforcement learning (DQN) operating over learned LSTM state encodings; planning is via learned Q-values rather than explicit search.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": "Reported to be able to successfully complete quests in small and some medium-size games (qualitative claim in the paper); no per-task numeric breakdown provided in this paper's text beyond that statement.",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Learning a compact representation of textual observations via an LSTM and coupling it to a DQN enables end-to-end learning in text environments; the LSTM encoder functions as the belief/state summarizer for the downstream RL module.",
            "uuid": "e795.2"
        },
        {
            "name_short": "Logic-based planner (first-order)",
            "name_full": "First-order logic state-tracking and planner (logic-based agent from related work)",
            "brief_description": "A symbolic approach that translates textual observations into first-order logic, maintains a logical belief state updated from observations and actions, and interleaves planning and execution to solve puzzles in partially observable domains (under deterministic-action assumptions).",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Logic-based planner / state tracker",
            "agent_description": "Summarized from related work: the system extracts logical predicates from textual observations, represents the world state in first-order logic, updates this belief state after each action/observation, and uses a symbolic planner that interleaves planning and execution to generate plans that achieve goals. The extension provably handles partially observable, initially unknown domains assuming deterministic actions without conditional effects.",
            "environment_name": "Text-based adventure / partially observable planning domains (as discussed in the cited related work)",
            "environment_description": "Partially observable domains where actions need planning based on an evolving, logically represented belief about objects, locations, and preconditions; challenges include unknown domain models and partial observability.",
            "is_partially_observable": true,
            "external_tools_used": "First-order logic representation and classical planning/search over symbolic representations (tools: logic translators and a symbolic planner in the referenced works).",
            "tool_output_types": "Structured logical predicates and formulas representing world state; symbolic plans (sequences of actions).",
            "belief_state_mechanism": "Explicit first-order logical belief state (set of predicates) that is updated deterministically from sequences of actions and observations.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Observations are translated into logical assertions; actions update the belief via logical consequence/transition rules. The extended system interleaves planning and execution, updating the logic-based belief after each executed action and observation.",
            "planning_approach": "Symbolic, model-based planning interleaved with execution; correctness formally proved under assumptions (deterministic actions, no conditional effects).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Symbolic belief tracking in first-order logic enables principled planning in partially observable text domains and can generate correct plans under strict assumptions, but relies on deterministic-action models and does not directly address noisy or language-ambiguous observations.",
            "uuid": "e795.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Goal Achievement in Partially Known, Partially Observable Domains",
            "rating": 2,
            "sanitized_title": "goal_achievement_in_partially_known_partially_observable_domains"
        },
        {
            "paper_title": "Knowledge-gathering agents in adventure games",
            "rating": 2,
            "sanitized_title": "knowledgegathering_agents_in_adventure_games"
        },
        {
            "paper_title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Adventure games: A challenge for cognitive robotics",
            "rating": 1,
            "sanitized_title": "adventure_games_a_challenge_for_cognitive_robotics"
        },
        {
            "paper_title": "What can you do with a rock? Affordance extraction via word embeddings",
            "rating": 1,
            "sanitized_title": "what_can_you_do_with_a_rock_affordance_extraction_via_word_embeddings"
        }
    ],
    "cost": 0.018031,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Text-based Adventures of the Golovin AI Agent</p>
<p>Bartosz Kostka 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Jarosław Kwiecień jaroslaw.kwiecien@stud.cs.uni.wroc.pl 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Jakub Kowalski 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Paweł Rychlikowski 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Text-based Adventures of the Golovin AI Agent</p>
<p>The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments.In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map.We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.</p>
<p>I. INTRODUCTION</p>
<p>The standard approach to develop an agent playing a given game is to analyze the game rules, choose an appropriate AI technique, and incrementally increase the agent's performance by exploiting these rules, utilizing domain-dependent features and fixing unwanted behaviors. This strategy allowed to beat the single games which were set as the milestones for the AI development: Chess [1] and Go [2].</p>
<p>An alternative approach called General Game Playing (GGP), operating on a higher level of abstraction has recently gained in popularity. Its goal is to develop an agent that can play any previously unseen game without human intervention. Equivalently, we can say that the game is one of the agent's inputs [3].</p>
<p>Currently, there are two main, well-established GGP domains providing their own game specification languages and competitions [4]. The first one is the Stanford's GGP, emerged in 2005 and it is based on the Game Description Language (GDL), which can describe all finite, turn-based, deterministic games with full information [5], and its extensions (GDL-II [6] and rtGDL [7]).</p>
<p>The second one is the General Video Game AI framework (GVGAI) from 2014, which focuses on arcade video games [8]. In contrast to Stanford's GGP agents are provided with the forward game model instead of the game rules. The domain is more restrictive but the associated competition provides multiple tracks, including procedural content generation challenges [9], [10].</p>
<p>What the above-mentioned approaches have in common is usually a well-defined game state the agent is dealing with. It contains the available information about the state (which may be partially-observable), legal moves, and some kind of scoring function (at least for the endgame states). Even in a GGP case, the set of available moves is known to the agent, and the state is provided using some higher-level structure (logic predicates or state observations).</p>
<p>In contrast, the recently proposed Text-Based Adventure AI Competition, held during the IEEE Conference on Computational Intelligence and Games (CIG) in 2016, provides a new kind of challenge by putting more emphasis on interaction with the game environment. The agent has access only to the natural language description about his surroundings and effects of his actions.</p>
<p>Thus, to play successfully, it has to analyze the given descriptions and extract high-level features of the game state by itself. Moreover, the set of possible actions, which are also expected to be in the natural language, is not available and an agent has to deduct it from his knowledge about the game's world and the current state.</p>
<p>In some sense, this approach is coherent with the experiments on learning Atari 2600 games using the Arcade Learning Environment (ALE), where the agent's inputs were only raw screen capture and a score counter [11], [12]. Although in that scenario the set of possible commands is known in advance.</p>
<p>The bar for text-based adventure games challenge is set high -agents should be able to play any interactive fiction (IF) game, developed by humans for the humans. Such environment, at least in theory, requires to actually understand the text in order to act, so completing this task in its full spectrum, means building a strong AI.</p>
<p>Although some approaches tackling similar problems exist since early 2000s ( [13], [14]), we are still at the entry point for this kind of problems, which are closely related to the general problem solving. However, recent successes of the machine learning techniques combined with the power of modern computers, give hope that some vital progress in the domain can be achieved.</p>
<p>We pick up the gauntlet, and in this work we present our autonomous agent that can successfully play many interactive fiction games. We took advantage of the specific game domain, and trained agent using matching sources: fantasy books and texts from decompiled IF games. Moreover, we embed some rpg-game-based mechanisms, that allow us to improve fighting opponents, managing hero's inventory, and navigating in the maze of games' locations.</p>
<p>We evaluated our agent on a set of 50 games, testing the influence of each specific component on the final score. Also, we tested our agent against the winner of the last year competition [15]. The achieved results are comparable. Our agent scored better in 12 games and worse in 11 games.</p>
<p>The paper is organized as follows. Section II provides background for the domain of interactive fiction, Natural Language Processing (NLP), Text-Based Adventure AI Competition, and the related work. In Section III, we presented detailed description of our agent. Section IV contains the results of the performed experiments. Finally, in Section V, we conclude and give perspective of the future research.</p>
<p>II. BACKGROUND</p>
<p>A. Interactive Fiction</p>
<p>Interactive Fiction (IF), emerged in 1970s, is a domain of text-based adventure or role playing games, where the player uses text commands to control characters and influence the environment. One of the most famous example is the Zork series developed by the Infocom company. From the formal point of view, they are single player, non-deterministic games with imperfect information. IF genre is closely related to MUDs (Multi-User Dungeons), but (being single-player) more focused on plot and puzzles than fighting and interacting with other players. IF was popular up to late 1980s, where the graphical interfaces become available and, as much user friendlier, more popular. Nevertheless, new IF games are still created, and there are annual competitions for game authors (such as The Interactive Fiction Competition).</p>
<p>IF games usually (but not always) take place in some fantasy worlds. The space the character is traversing has a form of labyrinth consisting of so called rooms (which despite the name can be open areas like forest). Entering the room, the game displays its description, and the player can interact with the objects and game characters it contains, or try to leave the room moving to some direction. However, reversing a movement direction (e.g. go south ↔ go north) not necessarily returns the character to the previous room.</p>
<p>As a standard, the player character can collect objects from the world, store them in his equipment, and combine with other objects to achieve some effects on the environment (e.g. put the lamp and sword in the case). Thus, many games require solving some kind of logical puzzle to push the action forward. After performing an action, the game describes its effect. Many available actions are viable, i.e. game engine understands them, but they are not required to solve the game, or even serve only for the player amusement.</p>
<p>Some of the games provide score to evaluate the player's progress, however the change in the score is often the result of a complex series of moves rather than quick "frame to frame" decisions, or the score is given only after the game's end. Other games do not contain any scoring function and the only output is win or lose.</p>
<p>B. Playing Text-Based Games</p>
<p>Although the challenge of playing text-based games was not take on often, there are several attempts described in the literature, mostly based on the MUD games rather than the classic IF games.</p>
<p>Adventure games has been carefully revised as the field of study for the cognitive robotics in [14]. First, the authors identify the features of "tradition adventure game environment" to point-out the specifics of the domain. Second, they enumerate and discuss existing challenges, including e.g. the need for commonsense knowledge (its learning, revising, organization, and using), gradually revealing state space and action space, vague goal specification and reward specification.</p>
<p>In [13], the agent able to live and survive in an existing MUD game have been described. The authors used layered architecture: high level planning system consisting of reasoning engine based on hand-crafted logic trees, and a low level system responsible for sensing the environment, executing commands to fulfill the global plan, detecting and reacting in emergency situations.</p>
<p>While not directly-related to playing algorithms, it is worth to note the usage of computational linguistics and theorem proving to build an engine for playing text-based adventure games [16]. Some of the challenges are similar for both tasks, as generating engine requires e.g. object identification (given user input and a state description) and understanding dependencies between the objects.</p>
<p>The approach focused on tracking the state of the world in text-based games, and translating it into the first-order logic, has been presented in [17]. Proposed solution was able to efficiently update agent's belief state from a sequence of actions and observations.</p>
<p>The extension of the above approach, presents the agent that can solve puzzle-like tasks in partially observable domain that is not known in advance, assuming actions are deterministic and without conditional effects [18]. It generates solutions by interleaving planning (based on the traditional logic-based approach) and execution phases. The correctness of the algorithm is formally proved.</p>
<p>Recently, an advanced MUD playing agent has been described in [19]. Its architecture consists of two modules. First, responsible for converting textual descriptions to state representation is based on the Long Short-term Memory (LSTM) networks [20]. Second, uses Deep Q-Networks [11] to learn approximated evaluations for each action in a given state. Provided results show that the agent is able to to successfully complete quests in small, and even medium size, games.</p>
<p>C. Natural Language Processing</p>
<p>Natural Language Processing is present in the history of computers almost from the very beginning. Alan Turing in his famous paper [21] state (approximately) that "exhibit intelligent behavior" means "understand natural language and use it properly in conversations with human being". So, since 1950 Turing test is the way of checking whether computer has reached strong AI capability.</p>
<p>First natural language processing systems were rule based. Thanks to the growing amount of text data and increase of the computer power, during last decades one can observe the shift towards the data driven approaches (statistical or machine learning). Nowadays, NLP very often is done "almost from scratch", as it was done if [22] where the authors have used neural network in order to solve many NLP tasks, including part-of-speech tagging, named entity recognition and semantic role labeling. The base for this was the neural language model. Moreover, this systems produced (as a side effect) for every word in a vocabulary a dense vector which reflected word properties. This vectors are called word embeddings and can be obtained in many ways. One of the most popular is the one proposed in [23] that uses very simple, linear language model and is suitable to large collections of texts.</p>
<p>Language models allow to compute probability of the sentence treated as a sequence of items (characters, morphemes or words). This task was traditionally done using Markov models (with some smoothing procedures, see [24]). Since predicting current words is often dependent on the long part of history, Markov models (which, by definition, looks only small numbers of words behind) are outperformed by the modern methods that can model long distance dependencies. This methods use recursive (deep) neural networks, often augmented with some memory.</p>
<p>We will use both word embeddings (to model words similarity) and LSTM neural networks [25] with attention mechanism (see [26] and [27]). We are particularly interested in the information given from the attention mechanism, which allows us to estimate how important is each word, when we try to predict the next word in the text.</p>
<p>D. The Text-Based Adventure AI Competition</p>
<p>The first Text-Based Adventure AI Competition 1 , organized by the group from the University of York, has been announced in May 2016 and took place at the 2016 IEEE CIG conference in September. The second, will be held this year, also colocated with CIG.</p>
<p>The purpose of the competition is to stimulate research towards the transcendent goal of creating General Problem Solver, the task stated nearly six decades ago [28]. The organizers plan to gradually increase the level of given challenges, with time providing more complex games that require more sophisticated approaches from the competitors. Thus, finally force them to develop agents that can autonomously acquire knowledge required to solve given tasks from the restricted domain of text-based games.</p>
<p>The domain of the competition is specified as any game that can be described by the Z-machine, the classic text adventuring engine. Interactive Fiction games are distributed as compiled byte code, requiring the special interpreter to run them. The first Z-machine has been developed in 1979 by Infocom, and supports games developed using a LISP-like programming language named Infocom's ZIL (Zork Implementation 1 http://atkrye.github.io/IEEE-CIG-Text-Adventurer-Competition/. Language). The Text-based AI Competition uses Frotz 2 , the modern version of the Z-machine, compatible with the original interpreter.</p>
<p>The competition organizers provide a Java package managing the communication between a game file and an agent process. Also, example random agents in Java and Python 3 are available. The interpreter is extended by the three additional commends, allowing players to quit the game, restart it with a new instance of the agent, and restart without modifying the agent. Given that, the text-based AI framework supports learning and simulation-based approaches.</p>
<p>Little details about the competition insides are available. In particular, the number of participants is unknown, and the test environment game used to evaluate agents remained hidden, as it is likely to be used again this year. The game has been developed especially for the purpose of the competition and supports graduated scale of scoring points, depending on the quality of agent's solution.</p>
<p>The winner of the first edition was the BYU-Agent 3 from the Perception Control and Cognition lab at Brigham Young University, which achieved a score 18 out of 100. The idea behind the agent has been described in [15]. It uses Q-learning [29] to estimate the utility of an action in a given game state, identified as the hash of its textual description.</p>
<p>The main contribution concerns affordance detection, used to generating reasonable set of actions. Based on the word2vec [30], an algorithm mapping words into a vector representations based on their contextual similarities, and the Wikipedia as the word corpus, the verb-noun affordances are generated. Thus, the algorithm is able to detect, for an in-game object, words with a similar meaning, and provide a set of actions that are possible to undertake with that object.</p>
<p>Provided results, based on the IF games compatible with Zmachine interpreter, shows the overall ability of the algorithm to successfully play text-based games. Usually, the learning process results in increasing score, and requires playing a game at least 200 times to reach the peek. However, there are some games that achieve that point much slower, or even the score drops as the learning continues.</p>
<p>III. THE GAME PLAYING AGENT</p>
<p>A. Overview</p>
<p>Our agent is characterized by the following features.:</p>
<p>• it uses a huge set of predefined command patterns, obtained by analyzing various domain-related sources; the actual commands are obtained by suitable replacements; • it uses language models based on selection of fantasy books; • it takes advantage of the game-specific behaviors, natural for adventure games, like fight mode, equipment management, movement strategy;</p>
<p>• it memorizes and uses some aspects of the current play history; • it tries to imitate human behavior: after playing several games and exploring the game universe it repeats the most promising sequence of commands. We treat the result reached in this final trial as the agent's result in this game. The agent was named "Golovin", as one of the first answers it gives after asking Hey bot, what is your name?, was your name is Golovin, a phrase from the game Doomlords.</p>
<p>B. Preprocessing 1) Language Models: We used language models for two purposes. First, they allow us to define words similarity (which in turns gives us opportunity to replace some words in commands with their synonyms). For this task we use word2vec [30] (and its implementation in TensorFlow [31]). Secondly, we use neural network language models to determine which words in the scene description plays more important role than other (and so are better candidates to be a part of the next command). We use the LSTM neural networks operating on words [25], augmented by the attention mechanism ( [26] and [27]). This combination was previously tested in [32].</p>
<p>Since the action of many games is situated in fantasy universe, we decided to train our models on the collection of 3000 fantasy books from bookrix.com (instead of using Wikipedia, as in [15]).</p>
<p>2) Commands: In order to secure out agent against overfitting, we fix the set of games used in tests (the same 50 games as in [15]). No data somehow related to this games were used in any stage of preprocessing.</p>
<p>We considered three methods to gather commands:</p>
<p>• walkthroughs -for several games, sequence of commands from winning path can be found in the Internet. This source provides raw valid commands, that are useful in some games. • tutorials -on the other hand, some games have tutorials written in natural language. Analyzing such tutorials 4 seemed to be a promising way of getting valid command. Concept of reading manuals has been successfully used to learn how to play Civilization II game [33]. • games -at the end, there are many games that don't have tutorials nor walkthroughs. We downloaded a big collection of games, decompiled their codes, and extracted all texts from them. The last two sources required slightly more complicated preprocessing. After splitting texts into sentences, we parsed them using PCFG parser from NLTK package [34]. Since commands are (in general) verb phrases, we found all minimal VP phrases from parse trees. After reviewing some of them, we decided not to take every found phrase, but manually create the list of conditions which characterizes 'verb phrases useful in games'. In this way we obtained the collections of approximately 250,000 commands (or, to be more precisely, command patterns). We also remember the count of every command (i.e. the number of parse tree it occurs in).</p>
<p>Some of the commands have special tag: "useful in the battle". We have manually chosen five verbs, as the most commonly related to fighting: attack, kill, fight, shoot, and punch. Approximately 70 most frequent commands containing one of these verbs received this tag.</p>
<p>The commands used by our agent were created from these patterns by replacing (some) nouns by nouns taken from the game texts.</p>
<p>C. Playing Algorithm</p>
<p>The algorithm uses 5 types of command generators: battle mode, gathering items, inventory commands, general actions (interacting with environment), and movement. The generators are fired in the given order, until a non-empty set of commands is proposed.</p>
<p>There are multiple reasons why some generator may not produce any results, e.g. the agent reaches the predefined limit of making actions of that type, all the candidates are blacklisted, or simply we cannot find any appropriate command in the database. We will describe other special cases later.</p>
<p>When the description of the area changes, all the command lists are regenerated.</p>
<p>1) Generating Commands: Our general method to compute available commands and choose the one which is carried out, looks as follows:</p>
<p>1) Find all nouns in the state description and agent's equipment. (We accept all type of nouns classified by the nltk.pos_tag function.) 2) Determine their synonyms, based on the cosine similarity between word vectors. (We use n-best approach with n being subject to Spearmint optimization; see IV-A.) 3) Find the commands containing nouns from the above described set. If a command contains a synonym, it is replaced by the word originally found in the description. 4) Score each command taking into account:</p>
<p>• cosine similarity between used synonyms and the original words • uniqueness of the words in command, computed as the inverse of number of occurrences in the corpora. • the weight given by the neural network model • the number of words occurring both in the description and in the command The score is computed as the popularity of the command (number of occurrences in the command database) multiplied by the product of the above. The formula uses some additional constants influencing the weights of the components. 5) Then, using the score as the command's weight, randomly pick one command using the roulette wheel selection. 2) Battle Mode: The battle mode has been introduced to improve the agent's ability to survive. It prevents from what has been the main cause of agent's death before -careless walking into an enemy or spending too much time searching for the proper, battle-oriented and life-saving, action.</p>
<p>The agent starts working in battle mode after choosing one of the "fight commands". Being in this mode, the agent strongly prefers using battle command, moreover it repeats the same command several times (even if it fails), because in many games the opponent has to be attacked multiple times to be defeated. Therefore, between the consecutive fighting actions we prevent using standard commands (like look, examine), as wasting precious turns usually gives the opponent an advantage.</p>
<p>3) Inventory Management (gathering and using items): In every new area, the algorithm searches its description for interesting objects. It creates a list of nouns ordered by the weight given by the neural network model and their rarity. Then, the agent tries take them.</p>
<p>If it succeeds (the content of the inventory has changed), a new list of commands using only the newly acquired item is generated (using the general method). The constant number of highest scored commands is immediately executed.</p>
<p>4) Exploration:</p>
<p>The task of building an IF game map is difficult for two reasons. One, because a single area can be presented using multiple descriptions and they may change as the game proceeds. Two, because there may be different areas described by the same text. Our map building algorithm tries to handle these problems.</p>
<p>We have found that usually the first sentence of the area description remains unchanged, so we use it to label the nodes of the graph (we have tried other heuristics as well but they performed worse). This heuristic divides all visited nodes into the classes suggesting that corresponding areas may be equivalent. The edges of the graph are labeled by the move commands used to translocate between the nodes (we assume that movement is deterministic).</p>
<p>We initialize the map graph using the paths corresponding to the past movements of the agent. Then, the algorithm takes all pairs of nodes with the same label and considers them in a specific, heuristic-based, order. For every pair, the MergeNodes procedure (Listing 1) is fired. The procedure merges two states joining their outcoming edges and recursively proceeds to the pairs of states that are reachable using the same move command. If the procedure succeeds, we replaces current map graph with the minimized one, otherwise the changes are withdrawn.</p>
<p>We use a small fixed set of movement commands (south, northwest, up, left, etc.) to reveal new areas and improve the knowledge about the game layout. When the agent decides to leave the area, it tries a random direction, unless it already discovered all outgoing edges -then it uses map to find a promising destination. We evaluate destination nodes minimizing the distance to that node plus the number of tested commands divided by the node's curiosity (depending on scores of available commands and untested movement commands). Then, the agent follows the shortest path to the best scored destination. if ¬ MergeNodes(A , B ) then return False end if 12: end for 13: return True 5) Failing Commands: When, after executing a command, the game state (description) remains unchanged, we assume the command failed. Thus, the algorithm puts it on a blacklist assigned to the current location. The command on the location's black list is skipped by the command generators.</p>
<p>After any change in the agent's inventory, all blacklists are cleared.</p>
<p>6) Restarts: The Frotz environment used for the contest allows to restart the game, i.e. abandon current play and start again from the beginning.</p>
<p>Me make use of this possibility in a commonsense imitating of the human behavior. When the agent dies, it restarts the game and, to minimize the chance of the further deaths, it avoids repeating the last commands of his previous lives. The agent also remembers the sequence of moves that lead to the best score and eventually repeats it. The final trial's result is used as the agent's result in the game.</p>
<p>IV. EXPERIMENTS</p>
<p>Our experiments had two main objectives: creating the most effective agent, and analyze how some parameters influence the agents performance.</p>
<p>The most natural way to measure the agent performance is to use the score given by the game (divided by the maximum score, when we want to compare different games). However, there are many games in which our agent (as well as BYU-Agent) has problems with receiving non zero points. So, we have decided to reward any positive score and add to the positive agent result arbitrarily chosen constant 0.2. Therefore, optimal (hypothetical) agent would get 1.2 in every game.</p>
<p>We selected 20 games for the training purposes, for all of them the maximum score is known. The performance of the agent is an averaged (modified) score computed on these games.</p>
<p>A. Creating The Best Agent</p>
<p>The agent's play is determined by some parameters, for instance:</p>
<p>• the set of command patterns, • the number of synonyms taken from word2vec, • the number of items, we are trying to gather, after visiting new place, • the number of standard command, tried after gathering phase, • how to reward the commands containing many words from description (the actual reward is b k , where k is the number of common words, and b is a positive constant), • how to punish the commands containing words with no good reason to be used (neither in state description nor in generated synonyms), the score is divided by p k , where k is the number of such words, and p is a constant, • how many command should be done before trying movement command. Furthermore we wanted to check, whether using battle mode or a map has an observable effect on agent performance. The number of parameter combinations was too large for grid search, so we decided to use Spearmint 5 .</p>
<p>We started this optimization process with (total) score equal to 0.02, and after some hours of computation we end with 0.08 (which means that the score has been multiplied 4 times). From now all parameters (if not stated otherwise) will be taken from the final Spearmint result.</p>
<p>B. Evaluation of Domain-based Mechanisms</p>
<p>We wanted to check whether some more advanced features of our agent give observable influence on agent performance. We checked the following 4 configurations with battle-mode and map turned on or off. The results are presented in Figure  1. One can see that map is useful (but only to some extent), and battle mode is undoubtedly useful. </p>
<p>C. Evaluation of Language Model Sources</p>
<p>Commands were taken from 3 sources: tutorials (T), walkthroughs (W), and state description from games (G). We compared the agents used command from all combination of these sources. The results are presented in Figure 2. The optimal configuration uses only two sources: T and W 6 . We, however, still believe that decompiled games can be a useful source for game commands. But they cannot be found in descriptions, but in command interpreter -which requires more advanced automated code analysis. We left it as a future work. </p>
<p>D. Gameplay Examples</p>
<p>While playing detective, our agent finds himself in a closet. We get the following state description:</p>
<p>Game: You are in a closet. There is a gun on the floor. Better get it. To exit, go east. Our agent determines items: closet, gun, floor, exit. Our agent is choosing from the commands listed in Table I. We see that what is the most important for the agent is gun and how to take it, which is reasonable and even suggested by the game. Moreover, the agent also tries to search using synonyms of the word gun in order to find proper commands (for instance: we can see that he recognizes gun as some kind of weapon, and some weapons, as for example knife, can be sharpened). Fortunately, after using a command get gun, Golovin obtains a black little pistol. Another example comes from zork3. We start with the following location description:</p>
<p>Game: You are in a shadowy land of low, rolling hills stretching out to the west and south. The land is bordered to the north by a massive stone wall. Ancient and weathered, the wall has crumbled enough at one point to permit passage. A cloaked and hooded person, carrying a sword not unlike your own, is standing blocking the way to the northeast. The hooded figure has a great deal of strength, perhaps matching your own. Golovin: examine figure Game: There is nothing special to be seen. Your quick reflexes cannot stop the hooded figure's stroke! You are hit! In this very moment, in the agent's inventory are: a sword, a lamp. In the command list we can find for instance kill man with sword. As the word man is close enough to figure, and Golovin also have sword, he chooses to use command kill figure with sword. Because the fighting command was chosen, the battle mode is turned on, and the game continues as follows:</p>
<p>Golovin: kill figure with sword Game: The hooded figure is hit with a quick slash! The figure has a light wound which hasn't affected its seemingly great strength. the agent's score improves and the fight continues. . .</p>
<p>Golovin: kill figure with sword Game: You charge, but the hooded figure jumps nimbly aside. The hooded figure stabs nonchalantly with its sword and misses.</p>
<p>E. The Comparison with BYU-Agent</p>
<p>Finally, we validate our approach by comparing it with the BYU-Agent. We were using the same set of 50 Z-machine games 7 as in [15].</p>
<p>The results of the comparison are presented in Table II. The BYU agent was trained for 1000 epochs (each epoch containing 1000 game steps), and its score was noted after each epoch. Because the learning curves vary depending on the game, including degeneration of the results (see [15, Figure 5]), as the main measure we took the maximum score achieved over all epochs.</p>
<p>As for the Golovin, we restricted his playing time to 1000 steps (i.e. an equivalent of one epoch) and use our commonsense restarting mechanism.</p>
<p>The BYU-Agent results are obtained using the verb and action space reduction algorithm, except the games marked with an asterisk, where the verb space reduction experienced errors, so we present scores obtained by the action space reduction variant instead.</p>
<p>Eventually, there are 24 games, out of 50, where some of the agents received any positive reward. Golovin scored better in 12 games, including 7 games where BYU-Agent received no reward. BYU-Agent scored better in 11 games, including 6 games where Golovin scored no points. One game is a nonzero tie.</p>
<p>Thus, despite significantly shorter learning time (i.e. available number of steps), our agent is able to outperform BYU-Agent on a larger number of games than he is outperformed on. On the other hand, BYU-Agent gains in the games where the Q-learning is effective and gradually increases score through the epochs, e.g. curses, detective or Parc.</p>
<p>Last observation concerns the number of games where only one of the agents scored 0, which is surprisingly large. This may suggest that the two compared approaches are effective on a different types of games, and may, in some sense, complement each other. </p>
<p>V. CONCLUSIONS AND FUTURE WORK</p>
<p>We have presented an agent able to play any interactive fiction game created for human players, on the level comparable to the winner of the last year Text-Based Adventure AI Competition. Due to the number of domain-based mechanisms, our agent can successfully handle the game in a limited number of available steps. The results of the presented experiments show that the mechanisms we embed (battle mode, mapping) and a choice of learning sources, indeed improves the agent's performance.</p>
<p>Although the results are promising, we are still at the beginning of the path towards creating the agent that can really understand the natural language descriptions in order to efficiently play the text-based adventure games.</p>
<p>There are multiple future work directions we would like to point out. First, and one of the most important, is to embed a learning mechanisms: the in-game learning, that uses restart functionality to improve player efficiency in one particular game; and preliminary learning, that is able to gain useful knowledge from playing entire set of games. Also, we plan to take a closer look at the decompiled game codes, as we believe that analyzing them may provide very useful knowledge.</p>
<p>We would like to improve the battle mode behavior, especially mitigate the agent and make it more sensitive to the particular situation. We hope that the mapping mechanism can be further extended to allow the casual approach, where the agent travels to distant locations for some specific reason (e.g. item usage), instead of a simple reactive system that we have now.</p>
<p>Lastly, we would like to continue the domain-based approach, and so focus our efforts on discovering the subgames (like we did with fighting and exploring) that we are able to properly detect, and handle significantly better than the general case.</p>
<p>if A = B then return True end if 2: if label(A) =label(B) then return False end if 3: mergelist ← {} 4: for all m ∈ M for all (A , B ) ∈ mergelist do 11:</p>
<p>Fig. 1 .
1Comparison of agent version with and without map and battle mode. Best variant scaled to 100%.</p>
<p>Fig. 2 .
2Comparison of agent using different sources of commands. Best variant scaled to 100%.</p>
<p>TABLE I
IBEST 10 (OUT OF 25) COMMANDS PROPOSED BY OUR AGENT FOR THE SITUATION DESCRIBED IN THE D E T E C T I V E EXAMPLE (SECTION IV-D)action 
score 
get gun 
0.1736 
drop gun 
0.1129 
take gun 
0.0887 
open closet 
0.0557 
examine gun 
0.0309 
fire gun 
0.0252 
load gun 
0.0237 
examine closet 
0.0128 
buy gun 
0.0042 
sharp gun 
0.0006 </p>
<p>TABLE II AVERAGE
IISCORES FOR 10 RUNS OF EACH GAME. FOR BYU-AGENT WE TOOK THE MAXIMUM ACHIEVED SCORE DURING THE 1000 EPOCHS TRAINING. GOLOVIN PLAYS FOR ONE EPOCH. IN THE GAMES THAT ARE NOT LISTED BOTH AGENTS GAIN NO REWARD. THE ASTERISK MARKS GAMES THAT USES OTHER VERSION OF BYU-AGENTgame 
Golovin BYU-Agent 
max score 
balances 
9.0 
0 
51 
break-in 
0 
0.3 
150 
bunny 
2.7 
2.0 
60 
candy 
10.0 
10.0 
41 
cavetrip 
15.0 
10.5 
500 
curses 
0.4 
1.9 
550 
deephome 
1.0 
0 
300 
detective 
71.0 
213.0 
360 
gold 
0.3 
0 
100 
library 
5.0 
0 
30 
mansion 
0.1 
2.2 
68 
Murdac 
10.0 
0 
250 
night 
0.8 
0 
10 
omniquest 
7,5 
5.0 
50 
parallel 
0 
5.0 
150 
Parc 
1.6 
5.0 
50 
reverb 
0 
1.8 
50 
spirit 
3.2 
2.0 
250 
tryst205 
0.2 
2.0 
350 
zenon 
0 
2.8 
20 
zork1 
13.5 
<em>8.8 
350 
zork2 
-0.1 
</em>3.3 
400 
zork3 
0.7 
*0 
7 
ztuu 
0 
0.5 
100 
better in: 
12 
11 
games </p>
<p>http://frotz.sourceforge.net.3 The agent is open source and available at https://github.com/danielricks/ BYU-Agent-2016.
This tutorials were downloaded from the following sites: http://www. ifarchive.org/, https://solutionarchive.com/, http://www.gameboomers.com/, http://www.plover.net/~davidw/sol/
Spearmint is a package which performs Bayesian optimization of hyperparameters. It allows to treat the optimized function as a black-box, and tries to choose the parameters for the next run considering the knowledge gathered during previous runs. See[35].
The difference between T+W and G+T+W is not very big. In the previous version of this experiment the winner was G+T+W.
The game set is available at https://github.com/danielricks/textplayer/tree/ master/games.
ACKNOWLEDGMENTSThe authors would like to thank Szymon Malik for his valuable contribution in the early stage of developing Golovin.We would also like to thank Nancy Fulda for helpful answers to our questions and providing up-to date results of the BYU-Agent.
Deep Blue. M Campbell, A J Hoane, F Hsu, Artificial intelligence. 1341M. Campbell, A. J. Hoane, and F. Hsu, "Deep Blue," Artificial intelli- gence, vol. 134, no. 1, pp. 57-83, 2002.</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 529D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanc- tot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, "Mastering the game of Go with deep neural networks and tree search," Nature, vol. 529, pp. 484-503, 2016.</p>
<p>M Genesereth, M Thielscher, General Game Playing. Morgan &amp; ClaypoolM. Genesereth and M. Thielscher, General Game Playing. Morgan &amp; Claypool, 2014.</p>
<p>General Game Playing: Overview of the AAAI Competition. M Genesereth, N Love, B Pell, AI Magazine. 26M. Genesereth, N. Love, and B. Pell, "General Game Playing: Overview of the AAAI Competition," AI Magazine, vol. 26, pp. 62-72, 2005.</p>
<p>General Game Playing: Game Description Language Specification. N Love, T Hinrichs, D Haley, E Schkufza, M Genesereth, Stanford Logic Group, Tech. Rep.N. Love, T. Hinrichs, D. Haley, E. Schkufza, and M. Genesereth, "General Game Playing: Game Description Language Specification," Stanford Logic Group, Tech. Rep., 2006.</p>
<p>A General Game Description Language for Incomplete Information Games. M Thielscher, AAAI Conference on Artificial Intelligence. M. Thielscher, "A General Game Description Language for Incomplete Information Games," in AAAI Conference on Artificial Intelligence, 2010, pp. 994-999.</p>
<p>Towards a Real-time Game Description Language. J Kowalski, A Kisielewicz, International Conference on Agents and Artificial Intelligence. 2J. Kowalski and A. Kisielewicz, "Towards a Real-time Game Descrip- tion Language," in International Conference on Agents and Artificial Intelligence, vol. 2, 2016, pp. 494-499.</p>
<p>The 2014 General Video Game Playing Competition. D Perez, S Samothrakis, J Togelius, T Schaul, S Lucas, A Couëtoux, J Lee, C Lim, T Thompson, IEEE Transactions on Computational Intelligence and AI in Games. 83D. Perez, S. Samothrakis, J. Togelius, T. Schaul, S. Lucas, A. Couëtoux, J. Lee, C. Lim, and T. Thompson, "The 2014 General Video Game Playing Competition," IEEE Transactions on Computational Intelligence and AI in Games, vol. 8, no. 3, pp. 229-243, 2015.</p>
<p>General Video Game AI: Competition, Challenges and Opportunities. D Perez, S Samothrakis, J Togelius, T Schaul, S M Lucas, AAAI Conference on Artificial Intelligence. D. Perez, S. Samothrakis, J. Togelius, T. Schaul, and S. M. Lucas, "General Video Game AI: Competition, Challenges and Opportunities," in AAAI Conference on Artificial Intelligence, 2016, pp. 4335-4337.</p>
<p>General Video Game Level Generation. A Khalifa, D Perez, S Lucas, J Togelius, Genetic and Evolutionary Computation Conference. A. Khalifa, D. Perez, S. Lucas, and J. Togelius, "General Video Game Level Generation," in Genetic and Evolutionary Computation Conference, 2016, pp. 253-259.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., "Human-level control through deep reinforcement learning," Nature, vol. 518, no. 7540, pp. 529-533, 2015.</p>
<p>Emergent Tangled Graph Representations for Atari Game Playing Agents. S Kelly, M I Heywood, EuroGP 2017: Genetic Programming, ser. 10196S. Kelly and M. I. Heywood, "Emergent Tangled Graph Representations for Atari Game Playing Agents," in EuroGP 2017: Genetic Program- ming, ser. LNCS, 2017, vol. 10196, pp. 64-79.</p>
<p>being-in-the-world. M A Depristo, R Zubek, Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment. the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive EntertainmentM. A. DePristo and R. Zubek, "being-in-the-world," in Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment, 2001, pp. 31-34.</p>
<p>Adventure games: A challenge for cognitive robotics. E Amir, P Doyle, Proc. Int. Cognitive Robotics Workshop. Int. Cognitive Robotics WorkshopE. Amir and P. Doyle, "Adventure games: A challenge for cognitive robotics," in Proc. Int. Cognitive Robotics Workshop, 2002, pp. 148- 155.</p>
<p>What can you do with a rock? Affordance extraction via word embeddings. N Fulda, D Ricks, B Murdoch, D Wingate, International Joint Conference on Artificial Intelligence. to appearN. Fulda, D. Ricks, B. Murdoch, and D. Wingate, "What can you do with a rock? Affordance extraction via word embeddings," in International Joint Conference on Artificial Intelligence, 2017, (to appear).</p>
<p>Put my galakmid coin into the dispenser and kick it: Computational linguistics and theorem proving in a computer game. A Koller, R Debusmann, M Gabsdil, K Striegnitz, Journal of Logic, Language and Information. 132A. Koller, R. Debusmann, M. Gabsdil, and K. Striegnitz, "Put my galakmid coin into the dispenser and kick it: Computational linguistics and theorem proving in a computer game," Journal of Logic, Language and Information, vol. 13, no. 2, pp. 187-206, 2004.</p>
<p>Knowledge-gathering agents in adventure games. B Hlubocky, E Amir, AAAI-04 workshop on Challenges in Game AI. B. Hlubocky and E. Amir, "Knowledge-gathering agents in adventure games," in AAAI-04 workshop on Challenges in Game AI, 2004.</p>
<p>Goal Achievement in Partially Known, Partially Observable Domains. A Chang, E Amir, Proceedings of the Sixteenth International Conference on International Conference on Automated Planning and Scheduling. the Sixteenth International Conference on International Conference on Automated Planning and SchedulingA. Chang and E. Amir, "Goal Achievement in Partially Known, Partially Observable Domains," in Proceedings of the Sixteenth International Conference on International Conference on Automated Planning and Scheduling, 2006, pp. 203-211.</p>
<p>Language Understanding for Text-based Games using Deep Reinforcement Learning. K Narasimhan, T Kulkarni, R Barzilay, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingK. Narasimhan, T. Kulkarni, and R. Barzilay, "Language Understanding for Text-based Games using Deep Reinforcement Learning," in Proceed- ings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1-11.</p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, Neural Comput. 98S. Hochreiter and J. Schmidhuber, "Long Short-Term Memory," Neural Comput., vol. 9, no. 8, pp. 1735-1780, 1997.</p>
<p>Computing machinery and intelligence. A M Turing, 59MindA. M. Turing, "Computing machinery and intelligence," Mind, vol. 59, no. 236, pp. 433-460, 1950. [Online]. Available: http: //www.jstor.org/stable/2251299</p>
<p>Natural language processing (almost) from scratch. R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P P Kuksa, abs/1103.0398CoRR. R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa, "Natural language processing (almost) from scratch," CoRR, vol. abs/1103.0398, 2011. [Online]. Available: http://arxiv.org/abs/1103.0398</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, abs/1301.3781CoRR. T. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient estimation of word representations in vector space," CoRR, vol. abs/1301.3781, 2013. [Online]. Available: http://arxiv.org/abs/1301.3781</p>
<p>An empirical study of smoothing techniques for language modeling. S F Chen, J Goodman, Proceedings of the 34th. the 34thS. F. Chen and J. Goodman, "An empirical study of smoothing techniques for language modeling," in Proceedings of the 34th</p>
<p>10.3115/981863.981904Annual Meeting on Association for Computational Linguistics, ser. ACL '96. Stroudsburg, PA, USAAssociation for Computational LinguisticsAnnual Meeting on Association for Computational Linguistics, ser. ACL '96. Stroudsburg, PA, USA: Association for Computational Linguistics, 1996, pp. 310-318. [Online]. Available: http://dx.doi.org/ 10.3115/981863.981904</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, C Jauvin, JOURNAL OF MACHINE LEARNING RE-SEARCH. 3Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, "A neural prob- abilistic language model," JOURNAL OF MACHINE LEARNING RE- SEARCH, vol. 3, pp. 1137-1155, 2003.</p>
<p>Feed-forward networks with attention can solve some long-term memory problems. C Raffel, D P W Ellis, abs/1512.08756CoRR. C. Raffel and D. P. W. Ellis, "Feed-forward networks with attention can solve some long-term memory problems," CoRR, vol. abs/1512.08756, 2015. [Online]. Available: http://arxiv.org/abs/1512.08756</p>
<p>Long short-term memory-networks for machine reading. J Cheng, L Dong, M Lapata, abs/1601.06733CoRR. J. Cheng, L. Dong, and M. Lapata, "Long short-term memory-networks for machine reading," CoRR, vol. abs/1601.06733, 2016. [Online].</p>
<p>Report on a general problem solving program. A Newell, J C Shaw, H A Simon, Proceedings of the International Conference on Information Processing. the International Conference on Information ProcessingA. Newell, J. C. Shaw, and H. A. Simon, "Report on a general problem solving program," in Proceedings of the International Conference on Information Processing, 1959, pp. 256-264.</p>
<p>Q-learning. C J Watkins, P Dayan, Machine learning. 83-4C. J. Watkins and P. Dayan, "Q-learning," Machine learning, vol. 8, no. 3-4, pp. 279-292, 1992.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781cs.CLT. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient estimation of word representations in vector space," 2013, arXiv:1301.3781 [cs.CL].</p>
<p>TensorFlow: Large-scale machine learning on heterogeneous systems. M A , 2015, software available from tensorflow.org. M. A. et al., "TensorFlow: Large-scale machine learning on heterogeneous systems," 2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/</p>
<p>Application of artificial neural networks with attention mechanism for discovering distant dependencies in time series. S Malik, University of WrocławBachelor ThesisS. Malik, "Application of artificial neural networks with attention mech- anism for discovering distant dependencies in time series," Bachelor Thesis, University of Wrocław, 2016.</p>
<p>Learning to Win by Reading Manuals in a Monte-Carlo Framework. S Branavan, D Silver, R Barzilay, Journal of Artificial Intelligence Research. 43S. Branavan, D. Silver, and R. Barzilay, "Learning to Win by Reading Manuals in a Monte-Carlo Framework," Journal of Artificial Intelligence Research, vol. 43, pp. 661-704, 2012.</p>
<p>S Bird, E Klein, E Loper, Natural Language Processing with Python. Reilly Media, Inc1st ed. O'S. Bird, E. Klein, and E. Loper, Natural Language Processing with Python, 1st ed. O'Reilly Media, Inc., 2009.</p>
<p>Practical bayesian optimization of machine learning algorithms. J Snoek, H Larochelle, R P Adams, Advances in Neural Information Processing Systems. F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. WeinbergerCurran Associates, Inc25J. Snoek, H. Larochelle, and R. P. Adams, "Practical bayesian optimiza- tion of machine learning algorithms," in Advances in Neural Information Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2012, pp. 2951-2959.</p>            </div>
        </div>

    </div>
</body>
</html>
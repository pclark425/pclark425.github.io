<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2570 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2570</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2570</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9" target="_blank">ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is shown that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs.</p>
                <p><strong>Paper Abstract:</strong> Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2570.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2570.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolSandbox</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TOOLSANDBOX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stateful, conversational, interactive Python-native benchmark and execution environment for evaluating LLM tool-use agents via three roles (User, Agent, Execution Environment), a Message Bus, an Execution Context (world state), and milestone/minefield based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TOOLSANDBOX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TOOLSANDBOX is an environment/framework that simulates multi-role interactions between a User (LLM user simulator), an Agent (LLM acting as assistant), and an Execution Environment that runs tools (Python functions / wrapped RapidAPI calls). It provides a shared Execution Context (world state and tool trace history) and a Message Bus for turn-based message passing and visibility-controlled subviews. Agents submit JSON-formatted tool calls (name + arguments) which are converted into executable Python snippets by the Execution Environment; results and exceptions are recorded back into the Execution Context as traces and are used both by agents and by the milestone/minefield evaluation system. The framework supports stateful tools, implicit tool dependencies, an LLM user simulator with Knowledge Boundary and Demonstration prompt components, interactive on-policy rollouts, and a flexible milestone/minefield matching evaluator that computes similarity scores between trajectories and expected milestone DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>3 (User simulator, Agent, Execution Environment)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>User simulator (LLM; simulates human user goals, follows Knowledge Boundary and Demonstration prompts, terminates via end_conversation tool), Agent (LLM; issues natural language replies or structured tool calls as JSON), Execution Environment (executor/orchestrator; runs Python tool code, captures outputs/exceptions, enforces race conditions and returns results).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>interaction/execution/evaluation of task-oriented tool use (user-agent dialog simulation, tool invocation/execution, intermediate & final evaluation); not designed to perform automated literature review or original scientific discovery itself.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized turn-based orchestration with a simple orchestrator: the most recently addressed role becomes the next sender; a Message Bus mediates communication and a global Execution Context provides shared state; coordination is effectively sequential pipeline (agent acts, execution environment executes, user simulator responds) though parallel tool call requests from agents are supported and resolved (Execution Environment enforces race conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Message-passing via Message Bus with per-message visibility control; messages include sender, recipient, content; tool calls are represented as structured JSON (function name + arguments) which are converted to Python snippets for execution; tool responses and exceptions are forwarded back as messages and committed as tool traces in Execution Context.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Execution Environment provides direct feedback to Agent via tool outputs and exceptions (e.g., ConnectionError) enabling iterative trial-and-error; user simulator provides natural-language reactions and can terminate via end_conversation; evaluation feedback is provided post-run via Milestones/Minefields similarity scoring; tool traces and world-state snapshots serve as persistent feedback for subsequent agent decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand, turn-based: agents communicate after each user or environment response (the role addressed speaks next); tool calls and returns happen synchronously per turn (though agents may request parallel tool calls, which the environment evaluates and may surface race-condition failures).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General task-oriented dialog / tool-use evaluation (contact/messaging/reminder/system settings/time/map/weather/stock/conversion/holiday domains); not targeted at a specific domain of scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Milestone similarity scores (0--100 scale reported as percent), average similarity per scenario category and model (e.g., GPT-4o: 75.0 average similarity), user simulator error rates (Table 2: e.g., combined Knowledge Boundary + Demonstration hallucination 6.97%, instruction-following 0.77%), other reported metrics: avg turns (TOOLSANDBOX avg turns 13.9), avg tool calls 3.80; per-model category breakdowns in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared models (agents) include proprietary (GPT-4o, Claude-3 family, Gemini) and open-source models (Hermes, Mistral, Gorilla variants, Command-R). The benchmark also compares TOOLSANDBOX statistics versus other tool-use benchmarks (BFCL, ToolEval, API-Bank, ToolTalk, Ï„-bench). Results show proprietary models outperform open-source ones (e.g., GPT-4o 75.0 vs Hermes ~31.4 average similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Message Bus + Execution Context enable: (1) stateful interactions and implicit state-dependency reasoning, (2) reproducible tool traces for evaluation, (3) flexible trajectories handling and intermediate milestone assessment; quantitatively, these design choices enable fine-grained scoring and reveal gaps in agent capabilities (e.g., identify state-dependency and canonicalization failures).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Race conditions from parallel tool calls (agents issuing parallel calls for dependent tools lead to failures), user-simulator hallucination and instruction-following errors (mitigated but not eliminated with Knowledge Boundary and Demonstration), scalability and annotation cost for milestone/minefield authoring, and tools backed by external web services affecting reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>User-simulator prompt ablation (Table 2): User Goal only vs +Knowledge Boundary vs +Demonstration; combined Knowledge Boundary + Demonstration reduced hallucination and instruction-following errors (hallucination 6.97%, IF 0.77%). Tool-augmentation ablations (Tool name/description/argument scrambling and addition of distraction tools) assessed sensitivity of agents to tool schema (Table 5 shows per-model drops under augmentations).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Empirical findings: (1) user simulator prompts that include Knowledge Boundary and Demonstration minimize simulator errors; (2) minimal uniform agent prompt used for fair comparison (no per-model prompt engineering); (3) discourage parallel tool calls for dependent tools (Execution Environment enforces race-condition penalties). No single universally optimal multi-agent configuration is prescribed beyond these observations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2570.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gorilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gorilla</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model/system that connects a large language model to a large number of APIs to enable tool-assisted behavior; referenced as an example of a tool-using system with limitations in consuming tool responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gorilla: Large language model connected with massive apis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gorilla (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a previous tool-use approach that connects LLMs to many APIs via a self-instruct paradigm; in this paper it is noted that some models like Gorilla are incapable of consuming tool responses in the TOOLSANDBOX setup and thus fail at multi-step scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>tool invocation/execution (general tool-use), not specifically scientific research phases in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper beyond implication of API calls; in TOOLSANDBOX Gorilla could not consume tool responses</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general tool-use / API-enabled tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In TOOLSANDBOX experiments Gorilla variants performed poorly and could not solve multi-step scenarios because they could not consume tool responses (no numeric score for Gorilla specifically provided in main tables beyond inclusion in model list with low scores).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned in contrast to other models; highlighted as theoretically capable of single tool-call scenarios but failing multi-tool sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not discussed beyond enabling many API connections.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Inability to consume tool responses, leading to failure on multi-tool tasks and interactive trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>No ablation results specific to Gorilla presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2570.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system to enable LLMs to use many real-world APIs via automated instruction data generation and a neural API retriever, cited as related work on scaling tool-use capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolllm: Facilitating large language models to master 16000+ real-world apis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ToolLLM (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a method that automates generation of instructional data and uses a neural API retriever to let LLMs call many real-world APIs (16k+). The paper references it in context of prior work on tool-use agents but does not integrate or evaluate ToolLLM within TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>API retrieval/invocation and general tool-use; not described here as covering research-specific phases</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general tool-use / API-driven tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as prior work; not empirically compared within TOOLSANDBOX experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Scalability to many APIs (described in reference), but not expanded on here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2570.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method showing language models can learn to use tools through self-supervised learning, cited as foundational work on autonomous tool-use by LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Toolformer (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as early influential work demonstrating that language models can autonomously learn to call tools via a self-supervised approach; mentioned in the related work section as part of the background on tool-use agents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>tool-use learning and API invocation (not research-specific phases in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general tool-use</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as prior art; not evaluated in TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2570.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeACT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that integrates executable code actions into training to improve LLM decision-making and task-solving, cited to support executable-action-based agent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Executable code actions elicit better llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeACT (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as work that integrates executable code actions into agent training, improving decision-making; mentioned among other tool-use agent approaches in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>execution/action integration (not research phases in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general tool-use and action execution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not evaluated within TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not detailed here beyond citation context.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2570.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoEx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GoEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed runtime/orchestration design for autonomous LLM applications; cited as a possible inspiration for enforcing confirmation/authentication orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Goex: Perspectives and designs towards a runtime for autonomous llm applications</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GoEx (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in Limitations as an orchestration-level inspiration for enforcing mandatory confirmation/authentication in tool-use systems; GoEx is described in the paper as a design perspective toward runtime orchestration for autonomous LLM applications.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>runtime orchestration and execution management (not scientific-research phases in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>orchestration/runtime-level control (suggested inspiration; details not provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous LLM application orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not evaluated within TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Proposed as inspiration for enforcing confirmations; detailed benefits not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2570.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist agent benchmark suite that includes tool-use evaluation for LLM-based agents, cited as related work on agent evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentbench: Evaluating llms as agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentBench (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited among broader agent benchmarks that evaluate generalist LLM agents including tool-use capability; mentioned for context and comparison but not used or implemented in TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>evaluation of agent capabilities (reference only)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>generalist agent tasks and tool-use</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as complementary work; not directly compared in experiments beyond general benchmark comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2570.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentBoard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentBoard</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analytical evaluation board for multi-turn LLM agents that includes tool-use evaluation, cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentboard: An analytical evaluation board of multi-turn llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentBoard (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced in related work as an evaluation suite that examines multi-turn LLM agents, including tool-use; not used directly by this paper but noted as related benchmarking effort.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>multi-turn evaluation (reference only)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-turn agent evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not used as baseline here beyond being listed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2570.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebArena</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebArena</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A realistic web environment for building autonomous agents; cited as related work for web-browsing and web-interaction agent evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Webarena: A realistic web environment for building autonomous agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WebArena (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a prior environment that evaluates agents' ability to browse and leverage the web; included in related work as complementary to TOOLSANDBOX's focus on tool-use and stateful interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>web interaction and browsing tasks for agents (reference only)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>web browsing / web-based agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared experimentally within TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2570.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MINT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MINT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method using GPT-4 to simulate natural-language user feedback for multi-turn LLM evaluation; cited as related work on user simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MINT (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited among approaches that use LLMs as user simulators to enable on-policy multi-turn evaluation; TOOLSANDBOX uses a GPT-4o powered user simulator with additional prompt components (Knowledge Boundary, Demonstration).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>user simulation for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>evaluation of multi-turn interactions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually to TOOLSANDBOX's own user-simulator design; specific metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2570.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAUS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based user simulator for task-oriented dialogue (LLM finetuned on task-oriented dialog trajectories), cited as related work in user simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reliable LLM-based user simulator for task-oriented dialogue systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DAUS (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in the related work as an approach to building reliable LLM-based user simulators trained/finetuned on task-oriented dialogue; TOOLSANDBOX uses an LLM (GPT-4o) user simulator with prompt-engineered enhancements.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>user simulation for dialog evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>task-oriented dialogue simulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared empirically within TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2570.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2570.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMIE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A built-in patient model used in medical agent evaluation (conversational diagnostic AI) that engages with a symptom collection agent; cited as related work on domain-specific simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards conversational diagnostic ai</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AMIE (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of a domain-specific built-in simulator (patient model) used for evaluating medical conversational agents; included in related work discussing user/patient simulators for interactive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>simulated patient-agent interactions for diagnostic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>medical conversational diagnosis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared within TOOLSANDBOX.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gorilla: Large language model connected with massive apis <em>(Rating: 2)</em></li>
                <li>Toolllm: Facilitating large language models to master 16000+ real-world apis <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Executable code actions elicit better llm agents <em>(Rating: 2)</em></li>
                <li>Goex: Perspectives and designs towards a runtime for autonomous llm applications <em>(Rating: 2)</em></li>
                <li>Agentbench: Evaluating llms as agents <em>(Rating: 2)</em></li>
                <li>Agentboard: An analytical evaluation board of multi-turn llm agents <em>(Rating: 2)</em></li>
                <li>Webarena: A realistic web environment for building autonomous agents <em>(Rating: 2)</em></li>
                <li>MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback <em>(Rating: 1)</em></li>
                <li>Reliable LLM-based user simulator for task-oriented dialogue systems <em>(Rating: 1)</em></li>
                <li>Towards conversational diagnostic ai <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2570",
    "paper_id": "paper-7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "ToolSandbox",
            "name_full": "TOOLSANDBOX",
            "brief_description": "A stateful, conversational, interactive Python-native benchmark and execution environment for evaluating LLM tool-use agents via three roles (User, Agent, Execution Environment), a Message Bus, an Execution Context (world state), and milestone/minefield based evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TOOLSANDBOX",
            "system_description": "TOOLSANDBOX is an environment/framework that simulates multi-role interactions between a User (LLM user simulator), an Agent (LLM acting as assistant), and an Execution Environment that runs tools (Python functions / wrapped RapidAPI calls). It provides a shared Execution Context (world state and tool trace history) and a Message Bus for turn-based message passing and visibility-controlled subviews. Agents submit JSON-formatted tool calls (name + arguments) which are converted into executable Python snippets by the Execution Environment; results and exceptions are recorded back into the Execution Context as traces and are used both by agents and by the milestone/minefield evaluation system. The framework supports stateful tools, implicit tool dependencies, an LLM user simulator with Knowledge Boundary and Demonstration prompt components, interactive on-policy rollouts, and a flexible milestone/minefield matching evaluator that computes similarity scores between trajectories and expected milestone DAGs.",
            "number_of_agents": "3 (User simulator, Agent, Execution Environment)",
            "agent_specializations": "User simulator (LLM; simulates human user goals, follows Knowledge Boundary and Demonstration prompts, terminates via end_conversation tool), Agent (LLM; issues natural language replies or structured tool calls as JSON), Execution Environment (executor/orchestrator; runs Python tool code, captures outputs/exceptions, enforces race conditions and returns results).",
            "research_phases_covered": "interaction/execution/evaluation of task-oriented tool use (user-agent dialog simulation, tool invocation/execution, intermediate & final evaluation); not designed to perform automated literature review or original scientific discovery itself.",
            "coordination_mechanism": "Centralized turn-based orchestration with a simple orchestrator: the most recently addressed role becomes the next sender; a Message Bus mediates communication and a global Execution Context provides shared state; coordination is effectively sequential pipeline (agent acts, execution environment executes, user simulator responds) though parallel tool call requests from agents are supported and resolved (Execution Environment enforces race conditions).",
            "communication_protocol": "Message-passing via Message Bus with per-message visibility control; messages include sender, recipient, content; tool calls are represented as structured JSON (function name + arguments) which are converted to Python snippets for execution; tool responses and exceptions are forwarded back as messages and committed as tool traces in Execution Context.",
            "feedback_mechanism": "Execution Environment provides direct feedback to Agent via tool outputs and exceptions (e.g., ConnectionError) enabling iterative trial-and-error; user simulator provides natural-language reactions and can terminate via end_conversation; evaluation feedback is provided post-run via Milestones/Minefields similarity scoring; tool traces and world-state snapshots serve as persistent feedback for subsequent agent decisions.",
            "communication_frequency": "On-demand, turn-based: agents communicate after each user or environment response (the role addressed speaks next); tool calls and returns happen synchronously per turn (though agents may request parallel tool calls, which the environment evaluates and may surface race-condition failures).",
            "task_domain": "General task-oriented dialog / tool-use evaluation (contact/messaging/reminder/system settings/time/map/weather/stock/conversion/holiday domains); not targeted at a specific domain of scientific research.",
            "performance_metrics": "Milestone similarity scores (0--100 scale reported as percent), average similarity per scenario category and model (e.g., GPT-4o: 75.0 average similarity), user simulator error rates (Table 2: e.g., combined Knowledge Boundary + Demonstration hallucination 6.97%, instruction-following 0.77%), other reported metrics: avg turns (TOOLSANDBOX avg turns 13.9), avg tool calls 3.80; per-model category breakdowns in Table 5.",
            "baseline_comparison": "Compared models (agents) include proprietary (GPT-4o, Claude-3 family, Gemini) and open-source models (Hermes, Mistral, Gorilla variants, Command-R). The benchmark also compares TOOLSANDBOX statistics versus other tool-use benchmarks (BFCL, ToolEval, API-Bank, ToolTalk, Ï„-bench). Results show proprietary models outperform open-source ones (e.g., GPT-4o 75.0 vs Hermes ~31.4 average similarity).",
            "coordination_benefits": "Message Bus + Execution Context enable: (1) stateful interactions and implicit state-dependency reasoning, (2) reproducible tool traces for evaluation, (3) flexible trajectories handling and intermediate milestone assessment; quantitatively, these design choices enable fine-grained scoring and reveal gaps in agent capabilities (e.g., identify state-dependency and canonicalization failures).",
            "coordination_challenges": "Race conditions from parallel tool calls (agents issuing parallel calls for dependent tools lead to failures), user-simulator hallucination and instruction-following errors (mitigated but not eliminated with Knowledge Boundary and Demonstration), scalability and annotation cost for milestone/minefield authoring, and tools backed by external web services affecting reproducibility.",
            "ablation_studies": "User-simulator prompt ablation (Table 2): User Goal only vs +Knowledge Boundary vs +Demonstration; combined Knowledge Boundary + Demonstration reduced hallucination and instruction-following errors (hallucination 6.97%, IF 0.77%). Tool-augmentation ablations (Tool name/description/argument scrambling and addition of distraction tools) assessed sensitivity of agents to tool schema (Table 5 shows per-model drops under augmentations).",
            "optimal_configurations": "Empirical findings: (1) user simulator prompts that include Knowledge Boundary and Demonstration minimize simulator errors; (2) minimal uniform agent prompt used for fair comparison (no per-model prompt engineering); (3) discourage parallel tool calls for dependent tools (Execution Environment enforces race-condition penalties). No single universally optimal multi-agent configuration is prescribed beyond these observations.",
            "uuid": "e2570.0",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Gorilla",
            "name_full": "Gorilla",
            "brief_description": "A model/system that connects a large language model to a large number of APIs to enable tool-assisted behavior; referenced as an example of a tool-using system with limitations in consuming tool responses.",
            "citation_title": "Gorilla: Large language model connected with massive apis",
            "mention_or_use": "mention",
            "system_name": "Gorilla (as referenced)",
            "system_description": "Referenced as a previous tool-use approach that connects LLMs to many APIs via a self-instruct paradigm; in this paper it is noted that some models like Gorilla are incapable of consuming tool responses in the TOOLSANDBOX setup and thus fail at multi-step scenarios.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "tool invocation/execution (general tool-use), not specifically scientific research phases in this paper",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper beyond implication of API calls; in TOOLSANDBOX Gorilla could not consume tool responses",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "general tool-use / API-enabled tasks",
            "performance_metrics": "In TOOLSANDBOX experiments Gorilla variants performed poorly and could not solve multi-step scenarios because they could not consume tool responses (no numeric score for Gorilla specifically provided in main tables beyond inclusion in model list with low scores).",
            "baseline_comparison": "Mentioned in contrast to other models; highlighted as theoretically capable of single tool-call scenarios but failing multi-tool sequences.",
            "coordination_benefits": "Not discussed beyond enabling many API connections.",
            "coordination_challenges": "Inability to consume tool responses, leading to failure on multi-tool tasks and interactive trajectories.",
            "ablation_studies": "No ablation results specific to Gorilla presented in this paper.",
            "optimal_configurations": "Not provided in paper.",
            "uuid": "e2570.1",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ToolLLM",
            "name_full": "ToolLLM",
            "brief_description": "A system to enable LLMs to use many real-world APIs via automated instruction data generation and a neural API retriever, cited as related work on scaling tool-use capabilities.",
            "citation_title": "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "mention_or_use": "mention",
            "system_name": "ToolLLM (as referenced)",
            "system_description": "Cited as a method that automates generation of instructional data and uses a neural API retriever to let LLMs call many real-world APIs (16k+). The paper references it in context of prior work on tool-use agents but does not integrate or evaluate ToolLLM within TOOLSANDBOX.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "API retrieval/invocation and general tool-use; not described here as covering research-specific phases",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "general tool-use / API-driven tasks",
            "performance_metrics": "Not reported within this paper.",
            "baseline_comparison": "Mentioned as prior work; not empirically compared within TOOLSANDBOX experiments.",
            "coordination_benefits": "Scalability to many APIs (described in reference), but not expanded on here.",
            "coordination_challenges": "Not discussed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not provided in this paper.",
            "uuid": "e2570.2",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer",
            "brief_description": "A method showing language models can learn to use tools through self-supervised learning, cited as foundational work on autonomous tool-use by LMs.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "system_name": "Toolformer (as referenced)",
            "system_description": "Referenced as early influential work demonstrating that language models can autonomously learn to call tools via a self-supervised approach; mentioned in the related work section as part of the background on tool-use agents.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "tool-use learning and API invocation (not research-specific phases in this paper)",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "general tool-use",
            "performance_metrics": "Not reported within this paper.",
            "baseline_comparison": "Cited as prior art; not evaluated in TOOLSANDBOX.",
            "coordination_benefits": "Not discussed here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "Not discussed here.",
            "optimal_configurations": "Not provided in paper.",
            "uuid": "e2570.3",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "CodeACT",
            "name_full": "CodeACT",
            "brief_description": "An approach that integrates executable code actions into training to improve LLM decision-making and task-solving, cited to support executable-action-based agent improvements.",
            "citation_title": "Executable code actions elicit better llm agents",
            "mention_or_use": "mention",
            "system_name": "CodeACT (as referenced)",
            "system_description": "Referenced as work that integrates executable code actions into agent training, improving decision-making; mentioned among other tool-use agent approaches in related work.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "execution/action integration (not research phases in this paper)",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "general tool-use and action execution",
            "performance_metrics": "Not reported within this paper.",
            "baseline_comparison": "Not evaluated within TOOLSANDBOX.",
            "coordination_benefits": "Not detailed here beyond citation context.",
            "coordination_challenges": "Not detailed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.4",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GoEx",
            "name_full": "GoEx",
            "brief_description": "A proposed runtime/orchestration design for autonomous LLM applications; cited as a possible inspiration for enforcing confirmation/authentication orchestration.",
            "citation_title": "Goex: Perspectives and designs towards a runtime for autonomous llm applications",
            "mention_or_use": "mention",
            "system_name": "GoEx (as referenced)",
            "system_description": "Mentioned in Limitations as an orchestration-level inspiration for enforcing mandatory confirmation/authentication in tool-use systems; GoEx is described in the paper as a design perspective toward runtime orchestration for autonomous LLM applications.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "runtime orchestration and execution management (not scientific-research phases in this paper)",
            "coordination_mechanism": "orchestration/runtime-level control (suggested inspiration; details not provided in this paper)",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "autonomous LLM application orchestration",
            "performance_metrics": "Not reported within this paper.",
            "baseline_comparison": "Not evaluated within TOOLSANDBOX.",
            "coordination_benefits": "Proposed as inspiration for enforcing confirmations; detailed benefits not provided here.",
            "coordination_challenges": "Not detailed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.5",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AgentBench",
            "name_full": "AgentBench",
            "brief_description": "A generalist agent benchmark suite that includes tool-use evaluation for LLM-based agents, cited as related work on agent evaluation.",
            "citation_title": "Agentbench: Evaluating llms as agents",
            "mention_or_use": "mention",
            "system_name": "AgentBench (as referenced)",
            "system_description": "Cited among broader agent benchmarks that evaluate generalist LLM agents including tool-use capability; mentioned for context and comparison but not used or implemented in TOOLSANDBOX.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "evaluation of agent capabilities (reference only)",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "generalist agent tasks and tool-use",
            "performance_metrics": "Not reported within this paper.",
            "baseline_comparison": "Mentioned as complementary work; not directly compared in experiments beyond general benchmark comparison table.",
            "coordination_benefits": "Not discussed here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.6",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AgentBoard",
            "name_full": "AgentBoard",
            "brief_description": "An analytical evaluation board for multi-turn LLM agents that includes tool-use evaluation, cited in related work.",
            "citation_title": "Agentboard: An analytical evaluation board of multi-turn llm agents",
            "mention_or_use": "mention",
            "system_name": "AgentBoard (as referenced)",
            "system_description": "Referenced in related work as an evaluation suite that examines multi-turn LLM agents, including tool-use; not used directly by this paper but noted as related benchmarking effort.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "multi-turn evaluation (reference only)",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "multi-turn agent evaluation",
            "performance_metrics": "Not reported within this paper.",
            "baseline_comparison": "Not used as baseline here beyond being listed in related work.",
            "coordination_benefits": "Not discussed here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.7",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "WebArena",
            "name_full": "WebArena",
            "brief_description": "A realistic web environment for building autonomous agents; cited as related work for web-browsing and web-interaction agent evaluations.",
            "citation_title": "Webarena: A realistic web environment for building autonomous agents",
            "mention_or_use": "mention",
            "system_name": "WebArena (as referenced)",
            "system_description": "Mentioned as a prior environment that evaluates agents' ability to browse and leverage the web; included in related work as complementary to TOOLSANDBOX's focus on tool-use and stateful interactions.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "web interaction and browsing tasks for agents (reference only)",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "web browsing / web-based agent tasks",
            "performance_metrics": "Not reported within this paper.",
            "baseline_comparison": "Not compared experimentally within TOOLSANDBOX.",
            "coordination_benefits": "Not discussed here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.8",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "MINT",
            "name_full": "MINT",
            "brief_description": "A method using GPT-4 to simulate natural-language user feedback for multi-turn LLM evaluation; cited as related work on user simulation.",
            "citation_title": "MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback",
            "mention_or_use": "mention",
            "system_name": "MINT (as referenced)",
            "system_description": "Cited among approaches that use LLMs as user simulators to enable on-policy multi-turn evaluation; TOOLSANDBOX uses a GPT-4o powered user simulator with additional prompt components (Knowledge Boundary, Demonstration).",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "user simulation for evaluation",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "evaluation of multi-turn interactions",
            "performance_metrics": "Not provided here.",
            "baseline_comparison": "Compared conceptually to TOOLSANDBOX's own user-simulator design; specific metrics not provided.",
            "coordination_benefits": "Not detailed here.",
            "coordination_challenges": "Not detailed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.9",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DAUS",
            "name_full": "DAUS",
            "brief_description": "An LLM-based user simulator for task-oriented dialogue (LLM finetuned on task-oriented dialog trajectories), cited as related work in user simulation.",
            "citation_title": "Reliable LLM-based user simulator for task-oriented dialogue systems",
            "mention_or_use": "mention",
            "system_name": "DAUS (as referenced)",
            "system_description": "Mentioned in the related work as an approach to building reliable LLM-based user simulators trained/finetuned on task-oriented dialogue; TOOLSANDBOX uses an LLM (GPT-4o) user simulator with prompt-engineered enhancements.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "user simulation for dialog evaluation",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "task-oriented dialogue simulation",
            "performance_metrics": "Not provided here.",
            "baseline_comparison": "Not compared empirically within TOOLSANDBOX.",
            "coordination_benefits": "Not discussed here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.10",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AMIE",
            "name_full": "AMIE",
            "brief_description": "A built-in patient model used in medical agent evaluation (conversational diagnostic AI) that engages with a symptom collection agent; cited as related work on domain-specific simulators.",
            "citation_title": "Towards conversational diagnostic ai",
            "mention_or_use": "mention",
            "system_name": "AMIE (as referenced)",
            "system_description": "Cited as an example of a domain-specific built-in simulator (patient model) used for evaluating medical conversational agents; included in related work discussing user/patient simulators for interactive evaluation.",
            "number_of_agents": "not specified in paper",
            "agent_specializations": "not specified in paper",
            "research_phases_covered": "simulated patient-agent interactions for diagnostic evaluation",
            "coordination_mechanism": "not specified in paper",
            "communication_protocol": "not specified in paper",
            "feedback_mechanism": "not specified in paper",
            "communication_frequency": "not specified in paper",
            "task_domain": "medical conversational diagnosis evaluation",
            "performance_metrics": "Not provided here.",
            "baseline_comparison": "Not compared within TOOLSANDBOX.",
            "coordination_benefits": "Not discussed here.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "Not provided here.",
            "optimal_configurations": "Not provided here.",
            "uuid": "e2570.11",
            "source_info": {
                "paper_title": "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gorilla: Large language model connected with massive apis",
            "rating": 2
        },
        {
            "paper_title": "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "Executable code actions elicit better llm agents",
            "rating": 2
        },
        {
            "paper_title": "Goex: Perspectives and designs towards a runtime for autonomous llm applications",
            "rating": 2
        },
        {
            "paper_title": "Agentbench: Evaluating llms as agents",
            "rating": 2
        },
        {
            "paper_title": "Agentboard: An analytical evaluation board of multi-turn llm agents",
            "rating": 2
        },
        {
            "paper_title": "Webarena: A realistic web environment for building autonomous agents",
            "rating": 2
        },
        {
            "paper_title": "MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback",
            "rating": 1
        },
        {
            "paper_title": "Reliable LLM-based user simulator for task-oriented dialogue systems",
            "rating": 1
        },
        {
            "paper_title": "Towards conversational diagnostic ai",
            "rating": 1
        }
    ],
    "cost": 0.0194965,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities</h1>
<p>Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, Ruoming Pang<br>Apple<br>{jiarui_lu, tholleis, yizhe_zhang, baumayer<br>f_nan, haoping_bai, shuang_ma2, sma7, mengyu_li2<br>gyin, ziruiw, r_pang}@apple.com</p>
<h4>Abstract</h4>
<p>Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, TOOLSANDBOX ${ }^{1}$ includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in TOOLSANDBOX are challenging even the most capable SOTA LLMs, providing brandnew insights into tool-use LLM capabilities.</p>
<h2>1 Introduction</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">State Dependency</th>
<th style="text-align: center;">Conversational</th>
<th style="text-align: center;">Interactive</th>
<th style="text-align: center;">Human Authored Ground Truth</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TOOLSANDBOX</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">BFCL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">ToolEval</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">API-Bank</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">ToolTalk</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">$\tau$-bench</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison of TOOLSANDBOX and other tool-use benchmarks.</p>
<p>Recent advancements in Large Language Models (LLMs) brought forth new opportunities treating LLMs as autonomous agents, capable of observing real-world environments and deciding upcoming actions. Among which, tool-use agents (Schick et al., 2023; Qin et al., 2023a; Patil et al., 2023; Qin et al., 2024) follow human instructions and utilize</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>real-world APIs to complete complex tasks. Contrary to prior approaches like dialog state tracking (Henderson et al., 2014; Budzianowski et al., 2018; Rastogi et al., 2020), which require the model to explicitly generate dialog states and actions under a predefined ontology, and derive a tool call from those structured outputs, tool-use studies allow the model to directly generate tool calls based on its observations, while keeping dialog and world state tracking implicit.</p>
<p>Despite the paradigm shift towards a more simplified problem formulation, the stateful, conversational and interactive nature of task oriented dialog remains, and poses a significant challenge for systematic and accurate evaluation of tool-using LLMs. Existing benchmarks like the Berkeley Function Calling Leaderboard (BFCL) (Yan et al., 2024), ToolEval (Qin et al., 2024), API-Bank (Li et al., 2023), ToolTalk (Farn and Shin, 2023) and $\tau$ bench (Yao et al., 2024) attempted to tackle some of these challenges, but there is yet to be an all encompassing solution.</p>
<p>Stateful Task oriented dialog often involves tools that are strongly coupled with a World State, e.g. a database. This can be a tool that can alter the world state, like turning on internet connection. More interestingly, there can be a tool that implicitly depends on a world state, for example, one cannot search for a nearby restaurant when internet connection is off. Sometimes, actions that deal with both of these scenarios need to be taken to complete a task, even if the user is agnostic to the underlying world state and only gives general instructions. The agent needs to use its own knowledge about the world and environment feedback to come up with a plan to modify the world state and complete the task. An example can be found in Figure 1.</p>
<p>BFCL (Yan et al., 2024) and ToolEval (Qin et al., 2024) both rely on stateless tools interacting with web services (through RESTful APIs). As such,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example evaluation trajectory from TOOLSANDBOX. Some message contents and milestones were truncated and streamlined for visual clarity. The <em>Message Bus</em> represents a full dialog history between the <em>User</em>, the <em>Agent</em> and the <em>Execution Environment</em>. The <em>World State</em> represents mutable database snapshots at a given turn. The <em>Milestones</em> represent predefined key events that need to happen in this trajectory. The light blue boxes represent the human authored scenario definition, including the user goal, initial user utterance, initial world state and milestones. The light yellow boxes represent on policy rollout collected during an interactive evaluation run. In this example, the <em>User</em> intended to send a message, while cellular service is turned off. The <em>Agent</em> should first understand the <em>User</em>'s intent, and prompt for necessary arguments from the <em>User</em>. After collecting all arguments with the help of the <em>search_contacts</em> tool, the <em>Agent</em> attempted to send the message, figured out it needs to enable cellular service upon failure, and retried. To evaluate this trajectory, we find the best match for all <em>Milestones</em> against <em>Message Bus</em> and <em>World State</em> in each turn while maintaining topological order.</p>
<p>These evaluation benchmarks are designed to assess how agents make trials with a static environment. API-Bank (Li et al., 2023), ToolTalk (Farn and Shin, 2023) and <em>Ï„</em>-bench (Yao et al., 2024) does include a set of tools to modify world states, but does not study the impact of state dependencies.</p>
<h3>Conversational</h3>
<p>Conversational evaluation is crucial yet challenging when assessing a dialog policy, due to the interdependency between a user and said policy, as well as the ambiguous nature of natural language. To facilitate automated conversational evaluation, a common practice is to implement a simulated user (Zhang et al., 2024; Sekulic et al., 2024). However, BFCL and ToolEval only evaluate self-contained, unambiguous single-turn user queries, which is hardly realistic. API-Bank and ToolTalk evaluates on unrolled predefined off-policy dialog trajectories, and thus cannot assess the agent's performance based on its own policy.</p>
<h3>Interactive</h3>
<p>Real world scenarios are full of surprises. The agent could issue an erroneous tool call. Tool execution could raise an unexpected exception. And the user could issue a follow-up correcting a previous statement. An interactive evaluation framework assessing the immediate return of key interactions with user or environment would be necessary to capture the intricate interaction between different roles. Such an interactive evaluation should provide full spectrum and fine-grained evaluation of any multi-turn session.</p>
<p>this regard, BFCL, API-Bank and ToolTalk rely on a predefined trajectory, and by extension relying on static turn wise evaluation metrics. $\tau$-bench requires agent action to match a single predetermined sequence, leaving no room for error correction in the model, or multiple possible answer sequences. Even though ToolEval allows multiple rounds of interaction between the Agent and tools, it relies solely on an LLM evaluator to judge the final pass rate and win rate of trajectories, which raises questions to its reliability and interpretability.</p>
<p>Driven by these motivations, we propose TOOLSANDBOX, a stateful, conversational and interactive tool-use benchmark. To the best of our knowledge, TOOLSANDBOX is the first LLM tool-use benchmark which</p>
<ul>
<li>Includes implicit state dependencies between stateful tools, allowing the agent to track and alter the world state based on its world/commonsense knowledge, which is implicit from the user query;</li>
<li>Includes an LLM simulated user, allowing for realistic, on-policy conversational evaluation to measure the agent's ability on implicit dialog state tracking;</li>
<li>Allows for fully interactive, dynamic trajectory collection with a representative set of highly composable tools, and a human authored, milestone / minefield based system for intermediate and final execution evaluation.
A comparison between TOOLSANDBOX and other benchmarks can be found in Table 1.</li>
</ul>
<h2>2 TOOLSANDBOX Design</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Interaction between the User, Agent and the Execution Environment. Boxes represent multiple rounds of interaction between involved roles.</p>
<p>At its core, TOOLSANDBOX is a Python native LLM testing environment, with Execution Context as world state abstraction and Python functions as Tools, where User, Agent and Execution Environment communicate with each other through a</p>
<p>Message Bus to complete a task, which is evaluated against predefined Milestones and Minefields. As shown in Figure 2, a typical test case starts with the User speaking to the Agent. From then on, the role being addressed gets to speak next, until the end state is reached. Upon receiving a User request, an Agent can decide to respond to the User asking for more information, or inform the Execution Environment to execute a Tool, providing intended tool name and arguments. The Execution Environment executes the Tool in an code.InteractiveConsole, (Foundation, 2024), which depending on the Tool modifies the world state stored in the Execution Context, and responds to the Agent. Once the User decides the task has been completed, it informs the Execution Environment to execute the end_conversation tool, which puts the system in the end state, ready to be evaluated based on the dialog's similarity to Milestones and Minefields. A thorough walk-through of TOOLSANDBOX implementation details can be found in Appendix A. The remainder of this section focuses on the key components that enables stateful, conversational and interactive evaluation.</p>
<h3>2.1 Stateful</h3>
<p>To construct challenging reasoning scenarios, TOOLSANDBOX includes a set of carefully designed stateful tools, defined as tools that inspects, depends on or manipulates world states. These world states include:</p>
<p>Cellular service Tools that require cellular service (e.g., send_message) depend on it to be true.</p>
<p>Wifi RapidAPI tools (e.g., search_stock) depend on it to be true.</p>
<p>Location Service All tools that utilize current location (e.g., get_current_location) depend on it to be true.</p>
<p>Low battery mode All tools that turn on cellular service, wifi, and location service status depend on it to be false, creating nested state dependency.</p>
<p>Stateful tools cover 44\% of TOOLSANDBOX toolbox. Naturally these tools forms an implicit dependency between each other, testing the agent's ability to maintain an internal image of world states and tool call stack. As an example, in Figure 1, when send_message tool is called while cellular service is off, a ConnectionError is raised, and the Agent should utilize set_cellular_service</p>
<p>to resolve the error, understand that cellular service should be turned on now, and retry send_message.</p>
<h3>2.2 Conversational</h3>
<p>On-policy conversational roll-out is supported by an LLM (GPT-4o) powered user simulator and carefully calibrated prompting design. The simulator represents a human interacting with an Agent, hoping to complete a task through possibly multiple rounds of conversation. When the User simulator decides the task has been completed, or could not be completed, it can terminate the conversation using the end_conversation tool, which is the single tool available to it. As related studies in user simulation (Zhang et al., 2024; Sekulic et al., 2024) suggest, one should include the user's overall goal in the simulator's system prompt. However, we found this is often insufficient for the complex interactions in TOOLSANDBOX, and can lead to two categories of failures. In some cases, it is infeasible for an LLM simulated user to judge task completion, or provide follow-up information with only access to the user goal, and not the expected result, which could lead to hallucination. Also, with only a single system prompt, the simulated user could be derailed by the tool-use agent, failing to follow instructions. Examples of these failures can be found in Appendix A.4.</p>
<p>In light of this, we propose two additional components in user simulator prompts: Knowledge Boundary, which inform the user simulator what it should and should not know, providing partial access to expected result, combating hallucination. And Demonstration, which provides few shot example dialogs to the user simulator. Prompt examples can be found in Appendix A.4. Note that demonstration is only visible to the user simulator and not the agent. We performed an ablation study for these components in Table 2. With both approaches combined, the LLM simulated user achieves the lowest error rate in all categories. User simulator error rate is also found to be consistent across agents, shown in Table 3, which should not affect the agent accuracy comparison in Table 5.</p>
<h3>2.3 Interactive</h3>
<p>With an stateful, conversational and interactive environment, evaluation trajectories are highly dynamic. Multiple trajectories can lead to the same outcome. A given task may be completed using different tools, the same tools in a different order, or through trial and error, and the evaluation strat-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Hallucination $\downarrow$</th>
<th style="text-align: left;">IF $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">User Goal</td>
<td style="text-align: left;">12.4</td>
<td style="text-align: left;">6.20</td>
</tr>
<tr>
<td style="text-align: left;">+ Knowledge Boundary</td>
<td style="text-align: left;">7.75</td>
<td style="text-align: left;">3.88</td>
</tr>
<tr>
<td style="text-align: left;">+ Demonstration</td>
<td style="text-align: left;">$\mathbf{6 . 9 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 7}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Percentage of user simulation failures in each failure category for each user simulator prompting setup. IF stands for instruction following error. Statistics derived from 1032 manually annotated trajectories using GPT-4o user simulator and GPT-4o agent.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">GPT-4o</th>
<th style="text-align: left;">Claude-3-Opus</th>
<th style="text-align: left;">Gemini-1.5-Pro</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Hallucination</td>
<td style="text-align: left;">$6.90 \pm 1.45$</td>
<td style="text-align: left;">$6.40 \pm 0.97$</td>
<td style="text-align: left;">$7.15 \pm 0.71$</td>
</tr>
<tr>
<td style="text-align: left;">IF</td>
<td style="text-align: left;">$1.11 \pm 0.84$</td>
<td style="text-align: left;">$1.38 \pm 0.69$</td>
<td style="text-align: left;">$0.92 \pm 0.49$</td>
</tr>
<tr>
<td style="text-align: left;">Total Error</td>
<td style="text-align: left;">$8.02 \pm 1.36$</td>
<td style="text-align: left;">$7.78 \pm 0.52$</td>
<td style="text-align: left;">$8.07 \pm 1.03$</td>
</tr>
</tbody>
</table>
<p>Table 3: Percentage of user simulation failures in each failure category for each agent model. Mean and std collected from 4 repeated trials each containing 1032 trajectories. Both Knowledge Boundary and Demonstration are applied.
egy has to be flexible enough to accommodate for that. To combat this, we developed an evaluation strategy based on Milestones and Minefields, which defines key events that must or must not happen in a trajectory, allowing us to evaluate any trajectory with rich intermediate and final execution signals, providing deeper understanding of the model performance. An example can be found in Figure 10.</p>
<p>In specific, Milestones are the critical steps needed to achieve a goal. An example is shown in Figure 1, where cellular service is turned off and the user asks the agent to send a text message. The milestones, in this example, would be defined as:</p>
<ol>
<li>The cellular status in the settings database must be changed to True.</li>
<li>The Agent must issue a tool call using the search_contacts tool and the correct arguments, before or after milestone 1.</li>
<li>The Agent must issue a tool call using the send_message tool and the correct arguments, after milestone 1 and 2.</li>
<li>The messaging database must contain a message with a phone number matching the expected one exactly and the content loosely matching the expected text, after milestone 3.</li>
</ol>
<p>Each milestone also defines a similarity measure which calculates a 0 to 1 similarity between each turn and the milestone. Types of available similarity measures are introduced in Appendix A.7. Milestones form a directed acyclic graph (DAG)</p>
<p>based on temporal dependency. To evaluate a trajectory against a milestone DAG, we find the highest averaged similarity score $_{M+}$ among all possible mappings between turns and milestones, given that the resulting chronological milestone sequence is a topological sort of the DAG. Task efficiency is not considered by Milestones, and is instead tracked by a complementary turn count metric shown in Appendix D.3. We introduce the milestone matching process with more details in Appendix A.7.</p>
<p>Milestone evaluation combines the best of both worlds. As shown in Figure 1, it allows for explainable evaluation metrics like tool call AST matching and execution result exact match found in BFCL, while retaining the flexibility to evaluate any possible trajectory, similar to ToolEval.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example GPT-4 trajectory for Insufficient Information category Minefield Evaluation. This task is impossible to complete due to the current timestamp not being available. Because of this, the model should never call the tool <em>timestamp_diff</em>, since any argument provided is bound to be incorrect. GPT-4 hallucinated the current timestamp and called <em>timestamp_diff</em>, matching the minefield, resulting in a similarity score of 0.</p>
<p>On the other side of <em>Milestones</em>, there are <em>Minefields</em>, which define events that must NOT occur, as shown in Figure 3. This is mainly used in scenarios where we test that an agent understands that it cannot complete a task with the given tools instead of hallucinating. <em>Minefields</em> are otherwise identical to <em>Milestones</em>, except when the final trajectory similarity score is calculated. Assuming using Equation 2 we found the similarity score score $<em M-="M-">{M-}$ for minefield DAG $G</em>)$, the final similarity score of the trajectory would be}(V_{M-},E_{M-</p>
<p>$$
\text{score} = \text{score}<em M-="M-">{M+} \times \mathbb{I}(\text{score}</em>
$$} = 0), \tag{1</p>
<p>ensuring if minefields are violated (non-zero minefield similarity), the similarity score for the whole trajectory is 0.</p>
<table>
<thead>
<tr>
<th></th>
<th>Avg Turns</th>
<th>Avg Tool calls</th>
<th>Test cases</th>
<th>Tools</th>
</tr>
</thead>
<tbody>
<tr>
<td>TOOLSANDBOX</td>
<td>13.9</td>
<td>3.80</td>
<td>1032</td>
<td>34</td>
</tr>
<tr>
<td>BFCL</td>
<td>2.00</td>
<td>0.78</td>
<td>2000</td>
<td>1193</td>
</tr>
<tr>
<td>ToolEval</td>
<td>7.53</td>
<td>1.46</td>
<td>1625</td>
<td>3917</td>
</tr>
<tr>
<td>API-Bank</td>
<td>3.88</td>
<td>2.04</td>
<td>261</td>
<td>73</td>
</tr>
<tr>
<td>ToolTalk</td>
<td>7.42</td>
<td>3.68</td>
<td>78</td>
<td>28</td>
</tr>
<tr>
<td>$\tau$-bench</td>
<td>29.33</td>
<td>4.48</td>
<td>165</td>
<td>24</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistics between TOOLSANDBOX and other tool-use benchmarks.</p>
<h2>3 Test Scenarios</h2>
<p>A test scenario is defined by the initial world state, the initial messages, the available tools and the evaluation milestone and minefields, as illustrated by the light blue boxes in Figure 1. TOOLSANDBOX contains 1032 test scenarios meticulously crafted by 2 internal domain experts to capture challenging tool-use scenarios, with human authored and carefully calibrated Milestones and Minefields to support evaluation. One annotator is tasked to create test scenarios, while the other acts as an agent to validate milestones and minefields. We designed a rigorous annotation process to ensure coverage across realistic, complex use case scenarios, detailed in Appendix B.2. Statistics comparison between TOOLSANDBOX and other benchmarks can be found in Table 4.</p>
<p>We designed 34 tools in TOOLSANDBOX, covering 11 domains including Contact, Messaging, Reminder, System settings, Time utilities, Math utilities, Map, Weather, Stock, Conversion, and Holiday, backed by python native implementation when possible, carefully selected RapidAPI endpoints when necessary. Additional details about tool domain coverage and design principles can be found in Appendix B.3. Tools in TOOLSANDBOX are designed to be representative, diverse and composable in conversational dialogs, while making tool count manageable for milestone annotation. As a result, TOOLSANDBOX test scenarios contain on average much higher number of tool calls and turns per dialog compared to other benchmarks.</p>
<p>To closely inspect the intricate challenges in LLM tool-use applications, test scenarios are organized into the following categories:</p>
<p><strong>Single / Multiple Tool Call</strong> These categories apply to scenarios where one / multiple tool calls are needed to fulfill the user task. Examples are shown in Appendix C.2. Note that this definition is different from the Berkeley Function-Calling leaderboard (Yan et al., 2024), which resembles</p>
<p>distraction tools in TOOLSANDBOX described in Tool Augmentation.</p>
<p>Single / Multiple User Turn In the single user turn category, the first user message provides all necessary information to complete the task, whereas multiple user turn scenarios start with an ambiguous request or missing information, requiring further clarification from the user. An example is shown in Appendix C.2.</p>
<p>State Dependency The state dependency category describes scenarios where successful tool execution depends on the world state, e.g. settings like cellular service. The world state can be modified by the agent through the use of another tool. Thus, an implicit dependency is formed between the tools, which can only be discovered through trial and error, as shown in Figure 1. There can even be nested state dependencies. As shown in Figure 19, sending a message would require cellular service to be turned on, but turning on cellular service requires low battery mode to be turned off. This requires the agent to implicitly keep track of a call stack, and backtrack when necessary to fulfill the task efficiently.</p>
<p>Canonicalization Canonicalization refers to the process of transforming surface form representation commonly seen in a natural language query, to its corresponding canonical form, similar to INFORM dialog act in Schema Guided Dialog (Rastogi et al., 2020). This is particularly crucial when an API is less intelligent, and requires canonical form as argument. In some cases, canonicalization can be performed by the model itself, for example transforming $I B$ to $1 _000 _000 _000$, or $\$$ to corresponding ISO 4217 currency code USD. However, there are also cases where canonicalization requires the help of tools, for example transforming this Friday to 5/24/2024, which requires knowledge about the current date, or transforming Golden Gate Bridge to the latitude longitude pair (37.8199, -122.4786), which requires a lookup in an external knowledge base. This scenario category captures both cases, probing the Agent's ability to perform canonicalization with or without the aid of tools.</p>
<p>Insufficient Information The insufficient information category is used for scenarios where the agent is not able to perform the task on purpose, by withholding a tool that would be needed for the task. This category exercises if the agent is able
to identify that it cannot complete the task, as opposed to hallucinating tools or tool arguments, as shown in Figure 3. In these scenarios, minefields are defined to evaluate if tools that would imply hallucination are called or not. Comparing to relevance detection in BFCL where provided tools are often irrelevant to the task at hand, this is a much more challenging scenario, which requires the agent to reason over highly relevant tools to figure out the missing pieces. Comparing to solvability in ToolEval, which assumes full credit for any task deemed unsolvable, this is much more fine-grained, testing if the agent would hallucinate when the task is unsolvable.</p>
<p>Tool Augmentation Orthogonal to the above categories, to support ablation studies on the effect of tool schema representation on agent accuracy, we have implemented multiple tool augmentations, including adding distraction tools, making tool or argument names less informative, and removing argument descriptions or type hints. For more details on the augmentations please refer to Appendix A.2.1.</p>
<p>Categories including Multiple Tool Call, Multiple User Turn, State Dependency and Insufficient Information are difficult challenges which requires complex reasoning capability from the agent, $85 \%$ of TOOLSANDBOX scenarios are associated with at least one of these challenging categories. Further scenario statistics including category wise breakdown and milestone coverage can be found in Appendix B.4.</p>
<h2>4 Evaluation Results</h2>
<p>When evaluating the models, all models use the same minimalist prompt shown in Figure 8 for comparison fairness. We do not include additional prompt engineering for the models, as we consider prompt engineering gains orthogonal to the innate model capability surfaced by simpler prompting. Table 5 shows the average similarity for each of the scenario categories described in Section 3. Additional prompting experiments can be found in Appendix 9.</p>
<p>Open Source Models There is a significant performance gap between proprietary and open source models, with the best performing open source model Hermes (interstellarninja et al.) lagging more than 20 points behind the second to last proprietary model Claude-3-Haiku (Anthropic, 2024).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg Score $\uparrow$</th>
<th style="text-align: center;">Scenario Categories</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tool Augmentations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">STC</td>
<td style="text-align: center;">MTC</td>
<td style="text-align: center;">SUT</td>
<td style="text-align: center;">MUT</td>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">II</td>
<td style="text-align: center;">0 DT</td>
<td style="text-align: center;">3 DT</td>
<td style="text-align: center;">10 DT</td>
<td style="text-align: center;">AT</td>
<td style="text-align: center;">TNS</td>
<td style="text-align: center;">TDS</td>
<td style="text-align: center;">ADS</td>
<td style="text-align: center;">ATS</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o-2024-05-13</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">71.9</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3-Opus-20240229</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">71.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo-0125</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.9</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0125-Peerime</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">63.5</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3-Sonnet-20240229</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-1.5-Pro-001</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">54.4</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3-Hakio-20240307</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-1.0-Pro</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">34.9</td>
</tr>
<tr>
<td style="text-align: center;">Hermes-2-Pro-Mistral-7B</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">32.2</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Inertact-&lt;0.3</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;">C4AI-Command-R-&lt;01</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">28.3</td>
</tr>
<tr>
<td style="text-align: center;">Gorilla-Openfunctions-&lt;2</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">28.6</td>
</tr>
<tr>
<td style="text-align: center;">C4AI-Command R+</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">24.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparing the average similarity score broken down by scenario category and tool augmentations. Columns from left to right represent average similarity score across all categories, then Single Tool Call, Multiple Tool Call, Single User Turn, Multiple User Turn, State Dependency, Canonicalization, Insufficient Information, 0 Distraction Tools, 3 Distraction Tools, 10 Distraction Tools, All Tools, Tool Name Scrambled, Tool Description Scrambled, Argument Description Scrambled and Argument Type Scrambled.</p>
<p>This is partly due to the fact that models like Gorilla (Patil et al., 2023) and Command-R (Cohere and for AI, 2024) are incapable of consuming tool responses, as shown in Appendix D.2. They can theoretically solve Single Tool Call test scenarios, but would fail in any scenario that requires multiple tool calls. As for Hermes and Mistral (Jiang et al., 2023), both models struggle at identifying when a tool call should be issued. Mistral for example would often mistake a tool-use scenario for a code generation task, as shown in Figure 11. These models' subpar performance unexpectedly caused them to achieve higher rating in the Insufficient Information category, which rewards the model for not generating hallucinated tool calls or arguments when provided tools are insufficient to complete the task. This should be considered a side effect instead of a positive outcome.</p>
<p>Proprietary Models Of the proprietary models, GPT-4o (OpenAI, 2024) achieves the highest similarity score, with Claude-3-Opus closely behind. Both models have their own strengths. While GPT4o achieves the higher score, Claude-3-Opus maintains a lower average turn count as shown in Appendix D.3, achieving the user goal with higher efficiency. Interestingly, comparing the largest and smallest models in the GPT, Claude and Gemini families (Reid et al., 2024), Multiple Tool Call and Multiple User Turn categories deteriorate much faster than Single Tool Call and Single User Turn, showing that reasoning about complex tool call sequences and ambiguous user requests requires much more model capacity.</p>
<p>State Dependency The State Dependency category shows an interesting trend where, larger
models like GPT-4 (Achiam et al., 2023) and Claude-3-Opus perform significantly worse than mid to smaller sized models like GPT-3.5-Turbo and Claude-3-Sonnet. This is due to erroneous parallel tool calls in face of state dependency. As mentioned in Appendix A.6, the Execution Environment always surfaces race conditions when present. Larger models like GPT-4 and Claude-3-Opus are prune to issuing parallel tool calls even for dependent tools, leading to a performance deficiency. An example is shown in Figure 17. Nested state dependency is also tricky to solve efficiently. As shown in Figure 18, models often forget about open issues and would not optimally backtrack, leading to repeated errors and as a result a much higher than optimal turn count.</p>
<p>Canonicalization Canonicalization remains a challenging category for all models, especially in tool assisted canonicalization. Larger models would tend to memorize world knowledge that is unlikely to change, like latitude longitude for famous geographical location, while smaller models are more keen on using tools.</p>
<p>However, time related arguments in specific show to be really challenging to canonicalize and reason about. Models would frequently hallucinate timestamps (Figure 15), and incorrectly canonicalize relative date and time (Figure 14).</p>
<p>In addition, models could take premature decisions in face of ambiguity, also leading to canonicalization errors. In Figure 16, multiple location entities were returned in the tool response, while the model simply chose the first one, without returning to the user for disambiguation.</p>
<p>Insufficient Information Insufficient Information performance overall negatively correlates with other categories. The stronger the model performance on complex tasks, the worse the insufficient information performance, showing its value at evaluating model reasoning capabilities. Even with simple tasks and very little tools, top performing models like GPT-3.5-Turbo and GPT-4 could hallucinate tool name, or hallucinate arguments, as shown in Figure 3 and 20. The test scenario's difficulty positively correlates with the number of steps involved in the tasks, as the models would get lost in solving immediate errors, and forget about the main objective.</p>
<p>Tool Augmentations Robustness against tool augmentations seems to vary model by model. While adding distraction tools, Claude-3-Sonnet drops almost 10 points between 0 distraction tools, and making all TOOLSANDBOX tools available. GPT-4o is particularly susceptible to Tool Description Scrambling, GPT-4 pays extra attention to argument descriptions, and Gemini-1.5 doesn't do well with Argument Type Scrambling.</p>
<h2>5 Related Work</h2>
<p>Tool-use Benchmarks Various tool-use benchmarks have been developed to evaluate LLM-based agent performance in different tool-use domains. The Berkeley Function Calling Leaderboard (Yan et al., 2024), ToolBench (Qin et al., 2023b), StableToolBench (Guo et al., 2024), NexusRaven V2 Function Calling Benchmark (team, 2023), and API-BLEND (Basu et al., 2024) assess the ability of LLM agents to plan and perform function calls. WebArena (Zhou et al., 2023), MiniWoB++ (Humphreys et al., 2022), Webshop (Yao et al., preprint), Mind2Web (Deng et al., 2023) and VisualWebArena (Koh et al., 2024) focus on the agent's ability to call search functionality to browse and leverage the web to solve the task. Apart from benchmarks specifically designed for tool-use, generalist agent benchmark suites like AgentBench (Liu et al., 2023) and AgentBoard (Ma et al., 2024) include evaluating the tool-use capability of agents as a central task to examine the general problemsolving ability of LLM-based agents.</p>
<p>Tool-use agent Various tool-use model have been developed to solve the complicated tool-use scenarios in real-world. Toolformer (Schick et al., 2023) first demonstrated that language models could au-
tonomously learn to use various tools, through a self-supervised learning approach. Gorilla (Patil et al., 2023) employs a self-instruct paradigm to generate {instruction, API $}$ pairs and is trained both with and without a retriever. ToolLLM (Qin et al., 2024) enables LLMs to use over 16,000 realworld APIs by automating the generation of diverse instructional data and leveraging a neural API retriever, showing better generalization across unseen APIs. CodeACT (Wang et al., 2024a) integrates executable code actions into training to enhance the decision-making and task-solving capabilities of LLMs, leading to more effective agents.</p>
<p>Dialogue State Tracking Dialogue State Tracking (DST) requires the agent to maintain and update dialogue states and actions. The MultiWOZ dataset (Budzianowski et al., 2018) offers a diverse set of dialogues requiring complex state tracking across multiple domains. Building on this, Rastogi et al. (2020) proposed a schema-guided approach to DST, addressing scalability issues in multi-domain settings and enhancing the model's adaptability and reducing the need for extensive domain-specific annotations. However, these datasets focus on explicit state tracking on off-policy trajectories. Our benchmark complements them by introducing world states, typically implicit and requiring to be inferred from world knowledge, and an interactive environment that offers a more diverse and extensive online evaluation.</p>
<p>User Simulator in SandBox When assessing the agent's core competencies in state tracking, memorization, and long-term planning, dialogues between users and the system can span over several turns, and off-policy evaluation may not always suffice. On the other hand, human-in-the-loop online evaluation is costly and time-consuming. Some studies have investigated incorporating a built-in user simulator to facilitate the evaluation process. DAUS (Sekulic et al., 2024) utilizes LLM finetuned on task oriented dialog trajectories. MINT (Wang et al., 2024b) utilizes GPT-4 to simulate natural language user feedback for multi-turn LLM evaluation. For medical agents, AMIE (Tu et al., 2024) integrates a built-in patient model to engage with a symptom collection agent. Zhang et al. (2024) develop a virtual environment for the agent model to predict unknown entities by interacting with a user simulator that responds with only yes or no. Our approach resonates with these approaches.</p>
<h1>6 Conclusion</h1>
<p>TOOLSANDBOX presents a stateful, conversational and interactive evaluation benchmark for LLM tooluse capabilities. With stateful and state dependent tools, LLM simulated user and flexible evaluation with milestones and minefields, it showcased a significant performance gap between open source and proprietary models, and unveiled challenging scenarios even for SOTA models, including State Dependency, Canonicalization and Insufficient Information, bringing new insights to understanding tool-use capabilities. We hope TOOLSANDBOX could be a valuable addition to LLM evaluation suites, pushing the boundary of tool-use research.</p>
<h2>7 Limitations</h2>
<p>While TOOLSANDBOX is powerful, being the first work of its kind, it still has many areas to be improved upon. In this section, we introduce some of these limitations, to motivate future research in this area.</p>
<p>Even though Milestone and Minefields are powerful interactive metrics that offer insights into intermediate and final outcomes, authoring them, especially mandatory intermediate milestones, requires deep knowledge around the tool capacities in TOOLSANDBOX and many iterations, hindering its scalability. A simplified, or fully automatic method for identifying Milestones and Minefields could be the key to further scale up the data volume in TOOLSANDBOX.</p>
<p>Despite our best effort at controlling user simulator behavior, it is still subject to non-negligible hallucination and instruction following errors. In this work we toyed with the idea of tool assisted user simulator. Only one tool end_conversation was offered to the user simulator, and we saw a noticeable improvement in instruction following in the dialog termination aspect. By expanding its tool set, a tool assisted user simulator could be a promising direction to further reduce hallucination and improve instruction following.</p>
<p>Mandatory confirmation and authentication is an interesting problem currently not addressed in TOOLSANDBOX. In dialog state tracking, confirmation is modeled by its corresponding dialog action before a transactional service is called. However, in most tool-use LLM designs, we are at the liberty of the model to decide when a confirmation is necessary. An orchestration level solution to enforce confirmation, similar to GoEx (Patil et al., 2024) could be a potential inspiration, and an interesting problem for models to reason over.</p>
<p>A challenging category of tools, namely tools that spawn a daemon process, e.g. setting a timer, is not addressed in TOOLSANDBOX currently. These tools complete and return in the main process after the daemon is spawned, and at some point in the future, the daemon would interrupt the main process, e.g. when the time is up. This kind of interruption poses a novel problem for both execution orchestration, and the model itself.</p>
<p>While most of TOOLSANDBOX tools are selfcontained implementations, some tools that depend on an external knowledge base, like searching for weather, are still backed by external web services,
affecting its reproducibility. A cached solution similar to StableToolBench (Guo et al., 2024) could maintain tool implementation simplicity, while providing stable, reproducible results.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card.</p>
<p>Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis A. Lastras. 2024. Api-blend: A comprehensive corpora for training and benchmarking api llms. ArXiv, abs/2402.15491.</p>
<p>PaweÅ‚ Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, IÃ±igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica GaÅ¡iÄ‡. 2018. MultiWOZ - a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016-5026, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Cohere and Cohere for AI. 2024. Command r. https://huggingface.co/CohereForAI/ c4ai-command-r-v01.</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards a generalist agent for the web. Preprint, arXiv:2306.06070.</p>
<p>Nicholas Farn and Richard Shin. 2023. Tooltalk: Evaluating tool-usage in a conversational setting. Preprint, arXiv:2311.10775.</p>
<p>Python Software Foundation. 2024. Python 3.9.19 documentation.</p>
<p>Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. Preprint, arXiv:2403.07714.</p>
<p>Matthew Henderson, Blaise Thomson, and Jason D. Williams. 2014. The second dialog state tracking challenge. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263-272, Philadelphia, PA, U.S.A. Association for Computational Linguistics.</p>
<p>Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. 2022. A data-driven approach for learning to control computers. In International Conference on Machine Learning, pages 9466-9482. PMLR.
interstellarninja, teknium, theemozilla, karan4d, and huemin_art. Hermes-2-pro-mistral-7b. https://huggingface.co/NousResearch/ Hermes-2-Pro-Mistral-7B.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. ArXiv preprint, abs/2310.06825.</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. 2024. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649.</p>
<p>Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-bank: A comprehensive benchmark for tool-augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3102-3116, Singapore. Association for Computational Linguistics.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688.</p>
<p>Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An analytical evaluation board of multi-turn llm agents. Preprint, arXiv:2401.13178.</p>
<p>OpenAI. 2024. Gpt-4o. https://openai.com/ index/hello-gpt-4o/.</p>
<p>Shishir G. Patil, Tianjun Zhang, Vivian Fang, Noppapon C., Roy Huang, Aaron Hao, Martin Casado, Joseph E. Gonzalez, Raluca Ada Popa, and Ion Stoica. 2024. Goex: Perspectives and designs towards a runtime for autonomous llm applications. Preprint, arXiv:2404.06921.</p>
<p>Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334.</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023a. Tool learning with foundation models. Preprint, arXiv:2304.08354.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. 2024. Toolllm: Facilitating large language models to master 16000+ real-world apis. In The Twelfth International Conference on Learning Representations.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023b. Toolllm: Facilitating large language models to master 16000+ real-world apis. Preprint, arXiv:2307.16789.</p>
<p>Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8689-8696.</p>
<p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. In Advances in Neural Information Processing Systems, volume 36, pages 68539-68551. Curran Associates, Inc.</p>
<p>Ivan Sekulic, Silvia Terragni, Victor GuimarÃ£es, Nghia Khau, Bruna Guedes, Modestas Filipavicius, Andre Ferreira Manso, and Roland Mathis. 2024. Reliable LLM-based user simulator for task-oriented dialogue systems. In Proceedings of the 1st Workshop on Simulating Conversational Intelligence in Chat (SCI-CHAT 2024), pages 19-35, St. Julians, Malta. Association for Computational Linguistics.</p>
<p>Nexusflow.ai team. 2023. Nexusraven-v2: Surpassing gpt-4 for zero-shot function calling.</p>
<p>Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. 2024. Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024a. Executable code actions elicit better llm agents. In ICML.</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2024b. MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations.</p>
<p>Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley function calling leaderboard.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. preprint. Webshop: Towards scalable real-world web interaction with grounded language agents. In ArXiv.</p>
<p>Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. $\tau$-bench: A benchmark for tool-agent-user interaction in real-world domains. Preprint, arXiv:2406.12045.</p>
<p>Yizhe Zhang, Jiarui Lu, and Navdeep Jaitly. 2024. Probing the multi-turn planning capabilities of llms via 20 question games.</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854.</p>
<h1>Appendix</h1>
<h2>A Implementation Details</h2>
<p>This appendix section introduces implementation details about the TOOLSANDBOX design.</p>
<h2>A. 1 Execution Context</h2>
<p>Execution Context represents the complete state of the TOOLSANDBOX. More specifically, it contains tool databases for stateful tools, referred to as World State in Figure 1, and the dialog history between different roles, referred to as Message Bus. It maintains a snapshot of all tool databases and dialog history at any given turn, allowing for easy introspection and evaluation. The Execution Context exists as a global variable for all roles and tools to easily access, while prohibiting direct manipulation from the LLM agent. This allows us to implement stateful tools that can manipulate or access database stored in the Execution Context, without defining it as function argument.</p>
<h2>A. 2 Tools</h2>
<p>Tools in TOOLSANDBOX are a set of highly composable, explicitly or implicitly dependent Python functions, creating complex reasoning challenges. Besides python native tools, a handful of carefully selected RapidAPI tools were also included with a thin layer of python wrapper. Tools manipulate world state through the Execution Context when necessary, and raise informative exceptions when execution conditions were not met. As an example, in Figure 1, when send_message tool is called while cellular service is off, a ConnectionError is raised. This allows the Agent to reason over possible exceptions, and deduce the tool needed to resolve the exception.</p>
<p>Tools are implemented as type-hinted, doc-string equipped Python functions, as shown in Listing 1. When a tool is passed to the Agent as an available tool, type hints and doc-string are converted into JSON API schema, as shown in Figure 9.</p>
<p>Listing 1: Example tool declaration</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">send_message</span><span class="p">(</span><span class="n">phone_number</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">,</span><span class="w"> </span><span class="n">content</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">str</span><span class="o">:</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;Send a message to a phone number.</span>
<span class="s">    Args:</span>
<span class="s">        phone_number: Phone number to send a message to.</span>
<span class="s">        content: The content of the message to send.</span>
<span class="s">    Returns:</span>
<span class="s">        Unique identifier of the sent message.</span>
<span class="s">    Raises:</span>
<span class="s">        ConnectionError: If cellular service is not on</span>
</code></pre></div>

<p>When a tool is executed, the name, input and output of the tool is committed into the current Execution Context as a tool trace, which allows for automatic evaluation.</p>
<h2>A.2.1 Tool Augmentations</h2>
<p>To enable ablation studies of how the tool schema affects agent accuracy we have implemented multiple augmentations.</p>
<p>Distraction Tools In addition to necessary tools that must be present to complete a task, $0,3,10$ or all the rest of the tools in the sandbox are made available to the Agent in addition, to evaluate the model's ability to pick the right tool. Distraction tools are chosen from a sorted list, where tools with domain overlap and textual similarities with necessary tools are prioritized. In addition, all of the scrambling augmentations below are applied in conjunction to adding 3 distraction tools, to ensure the augmentation is challenging yet feasible.</p>
<p>Tool name scrambling The name of the tool is modified to a less informative form, e.g. messages_0 instead of send_message. The agent LLM should be able to infer the purpose of the tool based on the description that is also part of the tool definition.</p>
<p>Tool description scrambling This removes the one-liner summary of the documentation, see Listing 2. The agent LLM still has access to the tool name, argument names as well as argument and return value documentations.</p>
<p>Listing 2: Tool description scrambling example</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">send_message</span><span class="p">(</span><span class="n">phone_number</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">,</span><span class="w"> </span><span class="n">content</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">str</span><span class="o">:</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    Args:</span>
<span class="s">        phone_number: Phone number to send a message to.</span>
<span class="s">        content: The content of the message to send.</span>
<span class="s">    Returns:</span>
<span class="s">        Unique identifier of the sent message.</span>
<span class="s">    Raises:</span>
<span class="s">        ConnectionError: If cellular service is not on</span>
<span class="s">    &quot;&quot;&quot;</span>
</code></pre></div>

<p>Argument description scrambling The section of the documentation explaining the arguments is being removed, see Listing 3. The agent LLM can infer the arguments from the tool definition as well as discover the correct usage through trial and error.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Argument type scrambling The type hints in the tool declaration are removed. The agent LLM still has access to the argument documentation. Note that when an agent generate argument does not conform to the annotated data type, an exception will be raised, presenting the expected data type, ensuring the problem is always solvable in face of this augmentation.</p>
<h2>A. 3 Roles and Message Bus</h2>
<p>In ToolSandbox there are three roles: User, Agent (Assistant) and Execution Environment. The Execution Environment, as a dedicated role, is responsible for executing tool-use requests from the Agent and returning the results. Interaction between the roles is enabled through a message passing system. Each message contains a sender role, recipient role, content as well as to which roles the message is visible to. A simple orchestrator determines message passing order by allowing the most recent recipient to be the next sender. Instead of representing the conversation as a single message thread, we use a collection of messages, i.e. Message Bus, stored within the Execution Context. The Message Bus contains a linear history of message transactions between all roles. As is shown in Figure 4, each role writes to the same Message Bus. However, when reading from the Message Bus, each role can only access a sub-view of the Message Bus based on which roles are allowed to "see" the individual messages. We will introduce each role in the following paragraphs.</p>
<h2>A. 4 User Role</h2>
<p>As is shown in Figure 7, user simulator prompts consists of 3 components:</p>
<ul>
<li>A User Goal section describing the general instructions and the goal of the simulated user. The idea of role reversal was challenging for the simulator, so we opted to refer to the agent as another user (User B), which improves instruction following.
<img alt="img-4.jpeg" src="img-4.jpeg" /></li>
</ul>
<p>Figure 4: Example Message Bus and corresponding subview for each role. By default each role can only view messages sent from or to said role. Visibility can also be explicitly controlled if needed.</p>
<ul>
<li>A Knowledge Boundary section describing what the user simulator should or should not know.</li>
<li>A Demonstration section including few shot dialog examples to further improve instruction following capabilities. Note that demonstration is only visible to the user role. It does not affect the agent and execution environment roles.</li>
</ul>
<p>An example of these components can be found in Figure 7. Without Knowledge Boundary and Demonstration, hallucination and instruction fol-</p>
<p>lowing errors can happen much more frequently, as shown in Figure 5 and 6.</p>
<p>User Simulator Instruction Following Error
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Example Prompts for user simulator instruction following error. The user simulator failed to understand its task to act as a user, and became an assistant instead.</p>
<p>User Simulator Hallucination
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Example Prompts for user simulator hallucination. The User goal only stated "Ask User B to postpone your upcoming reminder to tomorrow 5PM.", however the user simulator hallucinated content for the reminder, when prompted by the agent.</p>
<h2>A. 5 Agent Role</h2>
<p>Initially, the Agent role will receive a message from the User in natural language. The Agent could decide to prompt the User again for additional information, or decide to issue a tool call towards the Execution Environment. When issuing a tool call, the Agent selects the name of the tool from a list of available tools and provides necessary arguments, commonly expressed as JSON objects. These JSON objects are converted to executable Python code, see Figure 9, and sent to the Execution Environment for execution. Prompts used for all agents is shown in Figure 8. The prompt is meant to be consistent for all agents, and does not leak specific information about the testing environment.</p>
<p>Figure 9 shows how tool calls requested by the Agent are converted to Python code, which can then be executed by the Execution Environment.</p>
<p>User Goal
You are no longer an assistant. From now on role play as a user (User A) talking to another user (User B). Make sure you follow these instructions:
(c) DO NOT act as if you (User A) are an assistant. ALWAYS treat yourself as a user (User A). Do not ask questions to the other user (User B).</p>
<ol>
<li>Answer any question User B asks you (User A) accurately. Use only the information provided.Do not make up false information.</li>
<li>Use natural, short, casual language.</li>
<li>If User B says it could not complete the task, either provide more information you (User A) posses, or ask it to use the tools it figure it out itself if you (User A) do not posses information that could help. 4. Allow User B to turn off low battery mode, turn on cellular service, with or location service in order to complete the task.</li>
<li>When User B completed the task, even if you (User A) don't have enough information to validate the correctness, break out of role playing and use the provided tool named 'end_conversation' to stop the conversation.</li>
<li>When User B cannot complete the request after 5 tries, break out of role playing and use the provided tool named 'end_conversation' to stop the conversation.</li>
</ol>
<p>Answer User B's questions given the following task you (User A) want User B to complete. Ask User B to postpone your upcoming reminder to tomorrow 5PM.</p>
<p>Knowledge Boundary
The reminder should say "buy a nice rich navy bathing dress". Do not leak this information. You do not have any more information.</p>
<p>Demonstration
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Example Prompts for user simulator. Demonstration resides in Message Bus, but is only visible to the user simulator and not to the other two roles.</p>
<p>Agent Prompt
Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.</p>
<p>Figure 8: Prompt for Agent role.</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;call_hJSxpA34paYiXARbBVSRA4kq&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;function&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;arguments&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;holiday_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Christmas Day&quot;</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">            </span><span class="s">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;search_holiday&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;function&quot;</span>
<span class="p">}</span>
</code></pre></div>

<p>Convert JSON tool call to Python code
# (tool_id) is a unique ID for this tool call. (tool_id)_args = ("holiday_name": "Christmas Day") (tool_id)_result = search_holiday(==(tool_id)_args) print(repr((tool_id)_result))</p>
<p>Figure 9: Conversion from JSON tool call format to Python code</p>
<h2>A. 6 Execution Environment Role</h2>
<p>The execution environment role is responsible for executing tool calls requested by the Agent and User roles in the form of Python snippets, mimicking the behavior of interactive consoles like IPython and Jupyter. Exceptions raised while executing the code are captured through stderr, enabling the Agent to refine its tool calls through trial and error.</p>
<p>Some LLMs support parallel tool calling, intended to increase efficiency when multiple, independent tools need to be called. For example, if the user asks for the weather in two cities, the queries to the weather API can happen in parallel. However, if an LLM uses parallel tool calls for dependent tools, it should be penalized accordingly. For example, in Figure 1 where the agent has to enable cellular service before sending a text message, parallel tool calls should not be used. Execution Environment handles race conditions in parallel tool calls by following Murphy's Law, ensuring race condition always happens if detected.</p>
<h2>A. 7 Evaluation</h2>
<p>To evaluate a trajectory against a milestone DAG, We attempt to find the best match between milestone nodes and trajectory snapshots among all possible mappings. Formally, suppose we have a milestone DAG $G_{M+}\left(V_{M+}, E_{M+}\right),\left|V_{M+}\right|=m$, a sequence of database snapshots at each conversation turn $S_{n}=\left(s^{1}, s^{2}, \ldots, s^{n}\right)$ and a similarity measure $\operatorname{sim}: V_{M+} \times S \rightarrow[0,1]$ measuring how close a milestone is to a snapshot, we aim to find the best mapping function $f^{+}: S \rightarrow V_{M+}$ which achieves the highest averaged similarity score, under the constraint that mapped milestones forms a possible topological sort of $G_{M+}$ :</p>
<p>$$
\begin{aligned}
&amp; \operatorname{avgsim}<em i="1">{+}=\frac{1}{m} \sum</em>\right)\right) \
&amp; f^{+}=\underset{f^{+}\left(S_{n}\right) \in \operatorname{top}\left(G_{M+}\right)}{\arg \max } \operatorname{avgsim}}^{m} \operatorname{sim}\left(v_{M+}^{i}, f\left(v_{M+}^{i<em M_="M+">{+} \
&amp; \operatorname{score}</em>
\end{aligned}
$$}=\max \operatorname{avgsim}_{+</p>
<p>The max value is the similarity score between the trajectory and the milestone DAG.</p>
<p>The similarity measure $\operatorname{sim}\left(v^{i}, s^{j}\right)$ calculates similarity between a database snapshot and a target database defined in the milestone. The milestone defines the column wise similarity function used
for each column. These function have a $[0,1]$ output space, and could be exact matching for cellular service, ROUGE-L F measure for message content, AST matching for tool trace similar to the AST metric found in BFCL (Yan et al., 2024), and many more. This allows for great flexibility when defining milestones. For a snapshot database and milestone target both containing $k$ rows, we calculate a pairwise similarity for those rows $d_{a, b}$ by calculating the geometric mean of column similarities. Then we solve for the best assignment problem between snapshot and milestone rows, by maximizing the geometric mean of row similarities, which will be the similarity measure $\operatorname{sim}\left(v^{i}, s^{j}\right)$. We use the geometric mean throughout to ensure that, if any column similarity must not be violated, it could emit a 0 similarity, which would nullify the overall similarity measure.</p>
<p>In addition to similarities conditioning on the current milestone and snapshot, in some cases we also allow an additional "reference milestone" to be provided, enabling similarity to be conditioned on two milestones. This can unlock powerful constraints including guradrail_similarity, which checks if any changes are made to a certain database between two milestone events, and tool_trace_dependant_similarity, which allows one to extract tool trace output from a reference milestone, and ingest into the current milestone, allowing one to track the information flow of tools. An example can be found in Figure 10.</p>
<p>Milestone evaluation is a powerful tool that unlocks a deeper understanding into model performance, and hints at possible areas of improvement. An example is shown in Figure 10. In the end, the task was not completed before the maximum allowed number of turns. However, intermediate milestones showcased that the model was capable of solving the state dependency challenges and requesting the current location. In order to successfully resolve this test case, we should improve the model's turn efficiency on state dependency.</p>
<h2>B Test Scenarios</h2>
<h2>B. 1 Tool-use Benchmark Comparisons</h2>
<p>The tool-use benchmark statistics shown in Table 4 are calculated as follows:</p>
<ul>
<li>Average turn considers any message between the user, the agent or the tools as 1 turn.</li>
<li>For ToolSandbox, statistics are calculated</li>
</ul>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Example GPT-4o Trajectory with partially matched milestones. Some messages are elided for visual clarity. In this example, GPT-4o spent most of its time resolving state dependency issues, and could not finish the task in the maximum allowed number of turns. Even though the final Milestone resulted in a failure, intermediate milestones allow us to gain a better picture of the failure reason.
from trajectories collected on GPT-4o agent.</p>
<ul>
<li>BFCL only evaluates tool call generation from a single user prompt, which we consider as 2 turns.</li>
<li>ToolEval was calculated from ToolLlama DFS Retriever trajectories.</li>
<li>API-BANK was calculated from level 1 and 2 test set.</li>
</ul>
<h2>B. 2 Annotation Process</h2>
<p>Test scenarios are created by 2 internal domain experts from our institute who are familiar with the tool capacities in TOOLSANDBOX, and the field of task-oriented dialog. 1 annotator creates test scenarios including the starting world state, user task, initial message, and milestone/minefield. To ensure dataset diversity while making the annotation task feasible, the annotator followed the process below:</p>
<ul>
<li>The annotation process starts by creating seed scenarios. These are simple, single user turn, single tool call, self-contained requests that are supposed to cover most tools, as well as most of their arguments. For example, a seed scenario for add_reminder tool that requires timestamp, latitude, longitude would likely contain a starting user utterance saying Create a reminder to buy chocolate milk at timestamp 111222333 at latitude 37 longitude -122. Creating milestone for said scenario is trivial.</li>
<li>Next, starting from the seed scenario, the annotator branches off to create derived scenarios that are more involved. This could be a Multiple Tool Call scenario, which requires the agent to invoke other tools before this one, e.g. Create a reminder to buy chocolate milk tomorrow 5PM at Whole Foods on McKinley</li>
</ul>
<p>Ave, which requires reasoning for the relative datetime, as well as searching for location latitude longitude. Note that this will also be considered a Canonicalization scenario.</p>
<ul>
<li>It could be a Multiple User Turn scenario, which requires the agent to request more slots from the user. e.g. Create a reminder.</li>
<li>It could be a State Dependency scenario, which requires the model to solve state dependencies, e.g. Create a reminder to buy chocolate milk tomorrow 5PM at Whole Foods on McKinley Ave, but wifi is set to off. Preventing access to location search capability unless the dependency is resolved.</li>
<li>It could be an Insufficient Information scenario, which requires the model to figure out this task cannot be solved, e.g Create a reminder to buy chocolate milk tomorrow 5PM at Whole Foods on McKinley Ave, but the model does not have access to current timestamp.</li>
<li>These branches can be combined as well, creating numerous combinatorial and complex scenarios. Building branches off of seed also makes annotating milestones an incremental process that's easier to accomplish.</li>
<li>Finally, to cover the diverse and ambiguous nature of realistic user dialog, the annotator paraphrased the scenarios above into alternative phrasing, e.g. Create a reminder to buy chocolate milk tomorrow $\rightarrow$ I need to buy chocolate milk tomorrow. For these alternative phrasings, milestone definitions can be reused, reducing annotation workload.</li>
</ul>
<p>After the test scenarios were collected, the other annotator acts as an agent to validate the feasibility of the task and Milestones / Minefields. This annotator has the same message sub-view as a model agent and is asked to try to complete the task. After the test scenarios are validated through this process, at least 4 rounds of tests are issued to multiple model-based agents, to further confirm test and milestone/minefield validity against correct and incorrect trajectories.</p>
<h2>B. 3 Tool Design</h2>
<p>Tool design choices in TOOLSANDBOX are driven by two major principles:</p>
<ul>
<li>Tools should be representative and diverse, to cover key task oriented dialog use cases as well as TOOLSANDBOX test scenario categories.</li>
<li>Tool capacities should be well-defined, and tool counts should be manageable, so that milestone / minefield annotation is feasible for annotators.</li>
</ul>
<p>Driven by these guiding principles, we designed 34 tools in TOOLSANDBOX covering 11 domains including Contact, Messaging, Reminder, System settings, Time utilities, Math utilities, Map, Weather, Stock, Conversion, and Holiday, backed by python native implementation when possible, carefully selected RapidAPI endpoints when necessary. In more details:</p>
<ul>
<li>Each domain designed at least one "omnisearch" tool. All possible search fields should be present as arguments for this tool, and all relevant information for this domain should be returned in the response. As an example, if a user would like to ask for the lowest temperature or humidity for a location, the agent should invoke the search_weather tool for both requests, and the agent is expected to retrieve corresponding key values based on user intent. This ensures "search" requests within a domain have 1 single entry point.</li>
<li>Stateful domains should implement at least one state manipulation tool. This could be adding a new database entry, e.g. send_message, modifying an existing entry, e.g. set_wifi_status, or both, e.g. add_reminder and modify_reminder.</li>
<li>Utilities should be created to ensure the agent could transform necessary surface / canonical form slot types, e.g. timestamp_to_datetime_info turns Unix timestamp into year, month, day, and weekday; and calculate expected slot values, e.g. calculate_lat_lon_distance calculates the distances between two latitude-longitude pairs. While agents are allowed to infer these with its own inherent abilities, utility tools should be created to ensure the agents are not forced to.</li>
</ul>
<h2>B. 4 TOOLSANDBOX Statistics</h2>
<p>The number of test scenarios per scenario category in TOOLSANDBOX can be found in Table 6</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Scenario Count</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SINGLE_TOOL_CALL</td>
<td style="text-align: center;">152</td>
</tr>
<tr>
<td style="text-align: left;">MULTIPLE_TOOL_CALL</td>
<td style="text-align: center;">656</td>
</tr>
<tr>
<td style="text-align: left;">SINGLE_USER_TURN</td>
<td style="text-align: center;">584</td>
</tr>
<tr>
<td style="text-align: left;">MULTIPLE_USER_TURN</td>
<td style="text-align: center;">224</td>
</tr>
<tr>
<td style="text-align: left;">STATE_DEPENDENCY</td>
<td style="text-align: center;">192</td>
</tr>
<tr>
<td style="text-align: left;">CANONICALIZATION</td>
<td style="text-align: center;">472</td>
</tr>
<tr>
<td style="text-align: left;">INSUFFICIENT_INFORMATION</td>
<td style="text-align: center;">224</td>
</tr>
</tbody>
</table>
<p>Table 6: Number of test scenarios per category. Note that one test scenario can be assigned with multiple scenario categories.</p>
<p>The number of milestones defined per world state database and column name can be found in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Database</th>
<th style="text-align: left;">Column</th>
<th style="text-align: center;">Associated <br> Milestone Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CONTACT</td>
<td style="text-align: left;">is_self</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: left;">CONTACT</td>
<td style="text-align: left;">name</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: left;">CONTACT</td>
<td style="text-align: left;">person_id</td>
<td style="text-align: center;">136</td>
</tr>
<tr>
<td style="text-align: left;">CONTACT</td>
<td style="text-align: left;">phone_number</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: left;">CONTACT</td>
<td style="text-align: left;">relationship</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: left;">MESSAGING</td>
<td style="text-align: left;">content</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">MESSAGING</td>
<td style="text-align: left;">recipient_phone_number</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">REMINDER</td>
<td style="text-align: left;">content</td>
<td style="text-align: center;">136</td>
</tr>
<tr>
<td style="text-align: left;">REMINDER</td>
<td style="text-align: left;">latitude</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">REMINDER</td>
<td style="text-align: left;">longitude</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">REMINDER</td>
<td style="text-align: left;">reminder_id</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">REMINDER</td>
<td style="text-align: left;">reminder_timestamp</td>
<td style="text-align: center;">152</td>
</tr>
<tr>
<td style="text-align: left;">SETTING</td>
<td style="text-align: left;">cellular</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: left;">SETTING</td>
<td style="text-align: left;">location_service</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: left;">SETTING</td>
<td style="text-align: left;">low_battery_mode</td>
<td style="text-align: center;">120</td>
</tr>
<tr>
<td style="text-align: left;">SETTING</td>
<td style="text-align: left;">wifi</td>
<td style="text-align: center;">136</td>
</tr>
</tbody>
</table>
<p>Table 7: Number of milestones defined per world state database and column name. Note that one test scenario can define multiple milestones, with multiple database constraints each.</p>
<p>The number of milestones defined per tool can be found in Table 8.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tool</th>
<th style="text-align: center;">Associated <br> Milestone Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">calculate_lat_lon_distance</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">convert_currency</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">datetime_info_to_timestamp</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">get_cellular_service_status</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">get_current_location</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: left;">get_current_timestamp</td>
<td style="text-align: center;">296</td>
</tr>
<tr>
<td style="text-align: left;">get_wifi_status</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">search_contacts</td>
<td style="text-align: center;">168</td>
</tr>
<tr>
<td style="text-align: left;">search_holiday</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: left;">search_lat_lon</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: left;">search_location_around_lat_lon</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: left;">search_messages</td>
<td style="text-align: center;">104</td>
</tr>
<tr>
<td style="text-align: left;">search_reminder</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: left;">search_stock</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: left;">search_weather_around_lat_lon</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: left;">timestamp_diff</td>
<td style="text-align: center;">96</td>
</tr>
<tr>
<td style="text-align: left;">unit_conversion</td>
<td style="text-align: center;">56</td>
</tr>
</tbody>
</table>
<p>Table 8: Number of milestones defined per tool. Note that one test scenario can define multiple milestones, with multiple tool trace constraints each. Most state modification tool calls are tracked by corresponding database milestones instead of tool trace.</p>
<h2>C Example Trajectories</h2>
<h2>C. 1 Tool Call Detection</h2>
<p>Message Bus
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Example trajectory where Mistral mistook the tool-use task for a code generation task.</p>
<h2>C. 2 Single/Multiple Tool Call/User Turn</h2>
<p>Message Bus
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Example trajectory with Single Tool Call Single User Turn</p>
<p>Message Bus
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Example trajectory with Multiple Tool Call Multiple User Turn</p>
<h2>C. 3 Canonicalization</h2>
<p>Message Bus
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Example trajectories where GPT-4 incorrectly inferred relative time. Instead of deducing next Friday 5PM by inspecting current day and weekday, GPT-4 randomly shifted current time stamp by 6 days and 16 hours.</p>
<p>Message Bus
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Example trajectories where GPT-4o hallucinated time, instead of deducing relative time based on current timestamp.</p>
<p>Message Bus
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>User Goal:
Create a reminder to buy chocolate milk tomorrow 5PM at Whole Foods on McKinley Ave.</p>
<p>Figure 16: Disambiguation failure from GPT-4o. The User intended to set a reminder at Whole Foods McKinley Ave in multiple turns. However, upon receiving multiple possible entities, GPT-4o chose to set the reminder with the first location unannounced, without disambiguating with the User, leading to undesired results.</p>
<h2>C. 4 State Dependency</h2>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 17: GPT-4 incorrect parallel tool call trajectory in State Dependency. When the first search_holiday call resulted in a ConnectionError, the model should realize the dependency between Wi-Fi status and search_holiday, and issue a sequential tool call to solve them. Instead, GPT-4 issued parallel tool calls, causing a race condition.</p>
<p>Message Bus
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 18: GPT-4 inefficient nested State Dependency trajectory. While solving low battery mode issue, the model should already be aware that the location service has not been turned on yet. Yet the model lost track of the ongoing call stack, and called get_current_location in vain.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ TOOLSANDBOX evaluation framework is released at https://github.com/apple/ToolSandbox&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
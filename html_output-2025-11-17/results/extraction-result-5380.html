<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5380 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5380</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5380</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-5b65975d2c561d91e4d9806356f3c11d465192e2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5b65975d2c561d91e4d9806356f3c11d465192e2" target="_blank">Unifying Structured Data as Graph for Data-to-Text Pre-Training</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer, and devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph.</p>
                <p><strong>Paper Abstract:</strong> Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5380.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5380.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniD2T unified-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniD2T unified graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified representation that casts diverse structured inputs (tables, key-value records, knowledge graphs) into a single unlabeled, connected graph and linearizes it for graph-to-text pre-training and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Unified graph (graph-to-text linearization with prefixes)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>All input items are treated as graph nodes and connected by unlabeled edges according to the source structure. Knowledge-graph relations are converted into nodes (Levi graph) and edges; table cells become nodes with forward/reverse edges between cells in the same row and same column; key-value pairs become nodes with key↔value links and additional key↔key and value↔value links. Two input prefixes are prepended: a data-independent prefix ('describe the following data') and a data-specific prefix tailored per dataset. The graph is then linearized into a token sequence for the encoder, with an explicit connectivity matrix carried separately to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (Levi graph), Table (cell graph), Key-Value data</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves explicit connectivity and local structure across heterogeneous data; uses unlabeled, connected graphs (bidirectional edges by default); retains relation information by turning relations into nodes for KGs; adds dataset-specific contextual prefixes to provide global context; designed to be expressive across multiple D2T formats while remaining implementation-compatible with Transformer encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Data-to-text generation across six benchmarks: WebNLG, DART (graph-to-text), ToTTo and CoSQL (table-to-text), WikiBio and WikiTableT (key-value/table-to-text); few-shot E2ENLG; human factuality eval; diversity and ablation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Multiple metrics reported per task. Representative UniD2T results: ToTTo BLEU 49.9, PARENT 59.8; CoSQL dev BLEU 32.68, ROUGE-L 61.47; DART test BLEU 54.96, METEOR 0.42, TER 0.42; WebNLG test BLEU 60.41, METEOR 44.35, chrF++ 73.4, TER 34.1, BLEURT 0.65; WikiBio BLEU 50.4, PARENT 79.8; WikiTableT BLEU 33.7, PARENT 50.7. Few-shot (E2ENLG): UniD2T achieves e.g. 45.6 BLEU (0.1% training) and 64.8 BLEU (5% training).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>UniD2T (graph-structured pretraining + graph-structured finetuning) consistently outperforms linearized-sequence baselines and prior D2T pretraining approaches: e.g., on DART UniD2T BLEU 54.96 vs CONTROL PREFIXES 51.95 (+3.0 BLEU); on WebNLG UniD2T BLEU 60.41 vs TRIPLE 57.64; on CoSQL UniD2T BLEU 32.68 vs FALCON 25.65 (+7.03 BLEU). Ablations show graph-structured pretraining and graph-aware encoder outperform models using linearized pretraining/finetuning (Table 10 total score improvement from 255.6 for T5-Large+F_Linear to 282.3 for UniD2T).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pre-training corpus limited to aggregated existing datasets (two pretraining sources + six downstream datasets). Graph construction is heuristic/simple and may not be optimal for all data types. Using directed-only edges degrades performance (UniD2T_directed underperforms undirected version: e.g., WikiBio BLEU 48.8 vs 50.4). Potential room for richer objective functions and more advanced graph-construction strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5380.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5380.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi-graph KG conversion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi graph conversion for knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversion that turns labeled KG triples into an unlabeled bipartite graph by treating relation types as nodes and substituting each triple by (entity—relation—entity) node links, with reverse edges added.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph conversion</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given a KG with triples (vs, r, vt), the relation r is introduced as a node; the triple becomes two unlabeled undirected edges (vs, r) and (r, vt), and reverse edges (r, vs) and (vt, r) are added so the final graph is unlabeled, connected, and bipartite between original entities and relation-nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph → unlabeled bipartite (Levi) graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encodes relation types as explicit nodes so the model can process relations without changing the base Transformer architecture; converts labeled edges to unlabeled edges and thus normalizes KG structure to the same unlabeled-graph format as other data types; uses bidirectional edges to improve generality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as input representation for graph-to-text pretraining and downstream graph-to-text benchmarks (WebNLG, DART).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated in isolation; included as part of UniD2T. Overall UniD2T DART BLEU 54.96 (see unified representation performance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared indirectly versus linearized treatments of RDF triples (e.g., sequence baselines/control-prefix methods); using Levi graph as part of unified graph representation plus structure-aware Transformer outperformed prior graph/text baselines on DART/WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Transforms relations to nodes which increases node count and graph complexity; relies on bidirectional edges—experiments show directed-only graphs reduced performance; may not be optimal for all KG sizes or relation-heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5380.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5380.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Table→graph heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Table-to-graph heuristic conversion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic that converts tables into graphs by making each cell a node and adding (forward + reverse) edges between cells in the same row and between cells in the same column, optionally restricting to highlighted cells for datasets like ToTTo.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Table cell graph (row/column connectivity)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each table cell v_{i,j} becomes a node; for any two cells in the same row, forward and reverse edges are added; similarly for cells in the same column. Only highlighted cells are used in ToTTo. The construction is heuristic and motivated by empirical data analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Tabular data → cell-node graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves local row/column relations and invariance of table content; produces densely connected graphs (row/column cliques) which capture both local and column/row semantics; simple and dataset-agnostic but potentially over-connected for some tables.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used for table-to-text tasks ToTTo and CoSQL and for pretraining on aggregated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of UniD2T, improves table-to-text results: ToTTo BLEU 49.9 and PARENT 59.8 vs comparable baselines; CoSQL BLEU 32.68 and ROUGE-L 61.47 (development).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms baselines that either ignore structure or use different table encoders (e.g., TABT5, LATTICE, CoNT) in combined pretraining+graph-aware encoding setting. Ablations show graph-structured encoding beats linearized table sequence encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Heuristic connectivity choices may be suboptimal; authors note room for exploration of alternative node connectivity settings and potential over-connection; construction can produce many edges (dense graphs) which affects modeling complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5380.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5380.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KV→graph conversion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key-value pair graph conversion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Converts key-value structured data into graphs by treating both keys and values as nodes and connecting key↔value pairs plus additional key↔key and value↔value connections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Key-value node graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each key and each value is a node. Mandatory edges connect each key to its value; additional edges connect keys among themselves and values among themselves, with forward and reverse versions for each added connection.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Key-value records (infobox-like) → unlabeled graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Maintains explicit key/value structure and local relations among keys and values; allows shared structural treatment with tables/KGs; can be denser than a pure key→value bipartite graph due to added inter-key/value links.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used for key-value-to-text datasets (WikiBio, WikiTableT) and included in pretraining corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Part of UniD2T results: WikiBio BLEU 50.4, PARENT 79.8; WikiTableT BLEU 33.7, PARENT 50.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Combined with structure-aware Transformer, this conversion outperforms prior key-value-oriented pretraining methods such as KGPT and T5-Large baselines on WikiBio/WikiTableT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice to connect keys among themselves and values among themselves is heuristic; may introduce redundant edges and increase graph density; effect of alternative connectivity strategies not exhaustively explored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5380.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5380.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized sequence (flat string) representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline representation that flattens structured data into a string sequence (linearization) and feeds it to a standard Transformer without explicit structural encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / flat sequence serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Structured inputs (triples, table cells, key-value pairs) are serialized into a flat token sequence (often with simple separators or prefixes) and processed by a standard text-to-text Transformer (e.g., T5) without graph-aware position/attention matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Any structured type flattened to sequence: KGs, tables, key-value</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple to implement and compatible with standard pretrained text-to-text models; loses explicit structural connectivity and relative positional information; may be adequate for simple graphs but degrades on complex structures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as baseline in ablations and comparisons (e.g., T5-Large+F_Linear; P_Linear pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples from paper: T5-Large+F_Linear total score 255.6 across six datasets vs UniD2T total 282.3; T5-Large+F_Linear ToTTo BLEU 48.1 vs UniD2T 50.2; on WebNLG and other tasks linearization-based models underperform when graphs are large/complex.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>UniD2T (graph-aware) outperforms linearized models consistently. Table 10 shows T5-Large+F_Graph (graph-aware finetuning) > T5-Large+F_Linear, and additional graph-structured pretraining yields further gains. Performance gap widens with graph complexity (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ignores explicit connectivity; inferior for large/complex graphs; structural signals must be relearned implicitly, requiring more data or task-specific adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5380.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5380.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Position matrix</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-aware position matrix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new Transformer position matrix replacing scalar T5 position embeddings that encodes relative positional distances only between directly connected nodes, leaving non-connected pairs marked as infinite (±inf).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structure-aware relative position matrix</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each directly connected node pair an auxiliary matrix encodes relative distances between their internal tokens; these auxiliary matrices are copied into the final position matrix P_emb^new. Non-connected node pairs are assigned ±inf (treated as distant). Special prefixes are connected to all nodes to provide global context.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applies to unified graph inputs (KG Levi graphs, table graphs, key-value graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicitly encodes relative positions only for relevant (connected) pairs, reducing noise from irrelevant token distances and enabling modelling of graph-local relative ordering; more precise than learned scalar positional embeddings for graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ablation and full evaluation on WebNLG, DART, ToTTo, CoSQL, WikiBio, WikiTableT; used during pretraining and finetuning in generative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation on WebNLG: full UniD2T BLEU 60.4; removing position matrix yields BLEU 58.3 (drop ≈ 2.1 BLEU). Combined removal with attention causes larger drops (see attention ablation). Overall inclusion contributes to top reported results above baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Explicit graph-aware position encoding outperforms standard T5 scalar position embeddings and learned positional schemes in prior table/graph pretraining work (T5-Large baselines) when combined with attention matrix modifications; ablation shows position and attention matrices are complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires computing and storing per-edge auxiliary matrices (scales with graph connectivity); distances for non-connected node pairs are set to ±inf which is a hard cutoff and may discard useful long-range structure; design choices (distance metric, prefix connections) are heuristic and could be tuned further.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5380.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5380.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention matrix</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-aware attention mask / attention matrix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention matrix that replaces the standard Transformer attention mask with a binary relation-aware mask where attention is allowed only between directly connected tokens (and global prefixes attend to all tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relation-aware attention matrix (A_mask^new)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a binary attention matrix where entries are 1 if two elements have a direct graph relationship and 0 otherwise. Prefix tokens receive global attention (attend-to-all). This matrix substitutes the default fully-visible attention mask of T5 encoder, constraining self-attention to respect explicit graph connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Unified graph inputs (KGs, tables, key-value graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encodes explicit connectivity constraints into self-attention, reducing spurious interactions across disconnected nodes; enforces locality consistent with graph edges while allowing prefixes to propagate global information.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used in pretraining and finetuning for all targeted D2T evaluation datasets; ablations isolate its contribution (WebNLG, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation on WebNLG: removing attention matrix reduces BLEU from 60.4 to 58.6 (drop ≈ 1.8 BLEU). Removing both attention and position reduces BLEU to 56.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>When combined with the position matrix, achieves better performance than the original Transformer attention (fully-visible) and than models that only adopt linearized inputs with standard attention; ablations indicate both position and attention contribute complementary gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Binary mask is a hard constraint—long-range interactions that are not explicitly connected cannot be considered except via prefixes; mask construction relies on precomputed connectivity and may be brittle if the chosen graph edges are incorrect or incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5380.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5380.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining objectives</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Struct denoising and graph-to-text generation objectives</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two multi-task pretraining objectives: struct denoising (mask-and-reconstruct nodes in the graph) and graph-to-text generation (autoregressive negative log-likelihood to generate text from graph).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Struct denoising + Graph-to-Text generative objective</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Struct denoising: randomly mask ~15% of nodes (keeping edges) and train model to reconstruct masked node tokens (sentinel-delimited). Graph-to-text: train autoregressively to generate target text conditioned on linearized graph and explicit connectivity, using standard negative log-likelihood loss.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Unified graph inputs used in pretraining (tables, key-value, KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Struct denoising encourages the model to learn local node relationships and graph context; graph-to-text objective directly optimizes the target generative task. Multi-task setup blends reconstruction and generation learning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used during UniD2T pretraining and evaluated on the same downstream D2T tasks (WebNLG, DART, ToTTo, CoSQL, WikiBio, WikiTableT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No isolated metric solely for pretraining objectives, but combined pretraining yields substantial downstream boosts: e.g., UniD2T (pretrained with these objectives) obtains DART BLEU 54.96 and WebNLG BLEU 60.41, improving over T5 baselines and prior pretraining schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Different from pure sequence denoising (T5 style) or KG-specific pretraining (KGPT), struct denoising operates on graph nodes preserving connectivity; combining struct denoising with graph-to-text gives better downstream performance than pretraining only with linearized data (Table 10 comparisons between P_Graph and P_Linear).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Masking strategy is simple (node masking only) and may not capture more complex graph corruption/noise patterns; scaling to larger or more diverse pretraining corpora could require more varied or task-specific objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Structured Data as Graph for Data-to-Text Pre-Training', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tapas: Weakly supervised table parsing via pre-training <em>(Rating: 2)</em></li>
                <li>Kgpt: Knowledge-grounded pre-training for data-to-text generation <em>(Rating: 2)</em></li>
                <li>Text-to-text pre-training for data-to-text tasks <em>(Rating: 2)</em></li>
                <li>Structural adapters in pretrained language models for amr-to-text generation <em>(Rating: 2)</em></li>
                <li>Robust (controlled) table-to-text generation with structure-aware equivariance learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5380",
    "paper_id": "paper-5b65975d2c561d91e4d9806356f3c11d465192e2",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "UniD2T unified-graph",
            "name_full": "UniD2T unified graph representation",
            "brief_description": "A unified representation that casts diverse structured inputs (tables, key-value records, knowledge graphs) into a single unlabeled, connected graph and linearizes it for graph-to-text pre-training and generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Unified graph (graph-to-text linearization with prefixes)",
            "representation_description": "All input items are treated as graph nodes and connected by unlabeled edges according to the source structure. Knowledge-graph relations are converted into nodes (Levi graph) and edges; table cells become nodes with forward/reverse edges between cells in the same row and same column; key-value pairs become nodes with key↔value links and additional key↔key and value↔value links. Two input prefixes are prepended: a data-independent prefix ('describe the following data') and a data-specific prefix tailored per dataset. The graph is then linearized into a token sequence for the encoder, with an explicit connectivity matrix carried separately to the model.",
            "graph_type": "Knowledge graph (Levi graph), Table (cell graph), Key-Value data",
            "representation_properties": "Preserves explicit connectivity and local structure across heterogeneous data; uses unlabeled, connected graphs (bidirectional edges by default); retains relation information by turning relations into nodes for KGs; adds dataset-specific contextual prefixes to provide global context; designed to be expressive across multiple D2T formats while remaining implementation-compatible with Transformer encoders.",
            "evaluation_task": "Data-to-text generation across six benchmarks: WebNLG, DART (graph-to-text), ToTTo and CoSQL (table-to-text), WikiBio and WikiTableT (key-value/table-to-text); few-shot E2ENLG; human factuality eval; diversity and ablation analyses.",
            "performance_metrics": "Multiple metrics reported per task. Representative UniD2T results: ToTTo BLEU 49.9, PARENT 59.8; CoSQL dev BLEU 32.68, ROUGE-L 61.47; DART test BLEU 54.96, METEOR 0.42, TER 0.42; WebNLG test BLEU 60.41, METEOR 44.35, chrF++ 73.4, TER 34.1, BLEURT 0.65; WikiBio BLEU 50.4, PARENT 79.8; WikiTableT BLEU 33.7, PARENT 50.7. Few-shot (E2ENLG): UniD2T achieves e.g. 45.6 BLEU (0.1% training) and 64.8 BLEU (5% training).",
            "comparison_to_other_representations": "UniD2T (graph-structured pretraining + graph-structured finetuning) consistently outperforms linearized-sequence baselines and prior D2T pretraining approaches: e.g., on DART UniD2T BLEU 54.96 vs CONTROL PREFIXES 51.95 (+3.0 BLEU); on WebNLG UniD2T BLEU 60.41 vs TRIPLE 57.64; on CoSQL UniD2T BLEU 32.68 vs FALCON 25.65 (+7.03 BLEU). Ablations show graph-structured pretraining and graph-aware encoder outperform models using linearized pretraining/finetuning (Table 10 total score improvement from 255.6 for T5-Large+F_Linear to 282.3 for UniD2T).",
            "limitations_or_challenges": "Pre-training corpus limited to aggregated existing datasets (two pretraining sources + six downstream datasets). Graph construction is heuristic/simple and may not be optimal for all data types. Using directed-only edges degrades performance (UniD2T_directed underperforms undirected version: e.g., WikiBio BLEU 48.8 vs 50.4). Potential room for richer objective functions and more advanced graph-construction strategies.",
            "uuid": "e5380.0",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Levi-graph KG conversion",
            "name_full": "Levi graph conversion for knowledge graphs",
            "brief_description": "A conversion that turns labeled KG triples into an unlabeled bipartite graph by treating relation types as nodes and substituting each triple by (entity—relation—entity) node links, with reverse edges added.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Levi graph conversion",
            "representation_description": "Given a KG with triples (vs, r, vt), the relation r is introduced as a node; the triple becomes two unlabeled undirected edges (vs, r) and (r, vt), and reverse edges (r, vs) and (vt, r) are added so the final graph is unlabeled, connected, and bipartite between original entities and relation-nodes.",
            "graph_type": "Knowledge graph → unlabeled bipartite (Levi) graph",
            "representation_properties": "Encodes relation types as explicit nodes so the model can process relations without changing the base Transformer architecture; converts labeled edges to unlabeled edges and thus normalizes KG structure to the same unlabeled-graph format as other data types; uses bidirectional edges to improve generality.",
            "evaluation_task": "Used as input representation for graph-to-text pretraining and downstream graph-to-text benchmarks (WebNLG, DART).",
            "performance_metrics": "Not evaluated in isolation; included as part of UniD2T. Overall UniD2T DART BLEU 54.96 (see unified representation performance).",
            "comparison_to_other_representations": "Compared indirectly versus linearized treatments of RDF triples (e.g., sequence baselines/control-prefix methods); using Levi graph as part of unified graph representation plus structure-aware Transformer outperformed prior graph/text baselines on DART/WebNLG.",
            "limitations_or_challenges": "Transforms relations to nodes which increases node count and graph complexity; relies on bidirectional edges—experiments show directed-only graphs reduced performance; may not be optimal for all KG sizes or relation-heterogeneity.",
            "uuid": "e5380.1",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Table→graph heuristic",
            "name_full": "Table-to-graph heuristic conversion",
            "brief_description": "A heuristic that converts tables into graphs by making each cell a node and adding (forward + reverse) edges between cells in the same row and between cells in the same column, optionally restricting to highlighted cells for datasets like ToTTo.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Table cell graph (row/column connectivity)",
            "representation_description": "Each table cell v_{i,j} becomes a node; for any two cells in the same row, forward and reverse edges are added; similarly for cells in the same column. Only highlighted cells are used in ToTTo. The construction is heuristic and motivated by empirical data analysis.",
            "graph_type": "Tabular data → cell-node graph",
            "representation_properties": "Preserves local row/column relations and invariance of table content; produces densely connected graphs (row/column cliques) which capture both local and column/row semantics; simple and dataset-agnostic but potentially over-connected for some tables.",
            "evaluation_task": "Used for table-to-text tasks ToTTo and CoSQL and for pretraining on aggregated datasets.",
            "performance_metrics": "As part of UniD2T, improves table-to-text results: ToTTo BLEU 49.9 and PARENT 59.8 vs comparable baselines; CoSQL BLEU 32.68 and ROUGE-L 61.47 (development).",
            "comparison_to_other_representations": "Outperforms baselines that either ignore structure or use different table encoders (e.g., TABT5, LATTICE, CoNT) in combined pretraining+graph-aware encoding setting. Ablations show graph-structured encoding beats linearized table sequence encoding.",
            "limitations_or_challenges": "Heuristic connectivity choices may be suboptimal; authors note room for exploration of alternative node connectivity settings and potential over-connection; construction can produce many edges (dense graphs) which affects modeling complexity.",
            "uuid": "e5380.2",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "KV→graph conversion",
            "name_full": "Key-value pair graph conversion",
            "brief_description": "Converts key-value structured data into graphs by treating both keys and values as nodes and connecting key↔value pairs plus additional key↔key and value↔value connections.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Key-value node graph",
            "representation_description": "Each key and each value is a node. Mandatory edges connect each key to its value; additional edges connect keys among themselves and values among themselves, with forward and reverse versions for each added connection.",
            "graph_type": "Key-value records (infobox-like) → unlabeled graph",
            "representation_properties": "Maintains explicit key/value structure and local relations among keys and values; allows shared structural treatment with tables/KGs; can be denser than a pure key→value bipartite graph due to added inter-key/value links.",
            "evaluation_task": "Used for key-value-to-text datasets (WikiBio, WikiTableT) and included in pretraining corpus.",
            "performance_metrics": "Part of UniD2T results: WikiBio BLEU 50.4, PARENT 79.8; WikiTableT BLEU 33.7, PARENT 50.7.",
            "comparison_to_other_representations": "Combined with structure-aware Transformer, this conversion outperforms prior key-value-oriented pretraining methods such as KGPT and T5-Large baselines on WikiBio/WikiTableT.",
            "limitations_or_challenges": "Choice to connect keys among themselves and values among themselves is heuristic; may introduce redundant edges and increase graph density; effect of alternative connectivity strategies not exhaustively explored.",
            "uuid": "e5380.3",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Linearization baseline",
            "name_full": "Linearized sequence (flat string) representation",
            "brief_description": "A baseline representation that flattens structured data into a string sequence (linearization) and feeds it to a standard Transformer without explicit structural encodings.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Linearization / flat sequence serialization",
            "representation_description": "Structured inputs (triples, table cells, key-value pairs) are serialized into a flat token sequence (often with simple separators or prefixes) and processed by a standard text-to-text Transformer (e.g., T5) without graph-aware position/attention matrices.",
            "graph_type": "Any structured type flattened to sequence: KGs, tables, key-value",
            "representation_properties": "Simple to implement and compatible with standard pretrained text-to-text models; loses explicit structural connectivity and relative positional information; may be adequate for simple graphs but degrades on complex structures.",
            "evaluation_task": "Used as baseline in ablations and comparisons (e.g., T5-Large+F_Linear; P_Linear pretraining).",
            "performance_metrics": "Examples from paper: T5-Large+F_Linear total score 255.6 across six datasets vs UniD2T total 282.3; T5-Large+F_Linear ToTTo BLEU 48.1 vs UniD2T 50.2; on WebNLG and other tasks linearization-based models underperform when graphs are large/complex.",
            "comparison_to_other_representations": "UniD2T (graph-aware) outperforms linearized models consistently. Table 10 shows T5-Large+F_Graph (graph-aware finetuning) &gt; T5-Large+F_Linear, and additional graph-structured pretraining yields further gains. Performance gap widens with graph complexity (Figure 7).",
            "limitations_or_challenges": "Ignores explicit connectivity; inferior for large/complex graphs; structural signals must be relearned implicitly, requiring more data or task-specific adaptations.",
            "uuid": "e5380.4",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Position matrix",
            "name_full": "Structure-aware position matrix",
            "brief_description": "A new Transformer position matrix replacing scalar T5 position embeddings that encodes relative positional distances only between directly connected nodes, leaving non-connected pairs marked as infinite (±inf).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structure-aware relative position matrix",
            "representation_description": "For each directly connected node pair an auxiliary matrix encodes relative distances between their internal tokens; these auxiliary matrices are copied into the final position matrix P_emb^new. Non-connected node pairs are assigned ±inf (treated as distant). Special prefixes are connected to all nodes to provide global context.",
            "graph_type": "Applies to unified graph inputs (KG Levi graphs, table graphs, key-value graphs)",
            "representation_properties": "Explicitly encodes relative positions only for relevant (connected) pairs, reducing noise from irrelevant token distances and enabling modelling of graph-local relative ordering; more precise than learned scalar positional embeddings for graphs.",
            "evaluation_task": "Ablation and full evaluation on WebNLG, DART, ToTTo, CoSQL, WikiBio, WikiTableT; used during pretraining and finetuning in generative tasks.",
            "performance_metrics": "Ablation on WebNLG: full UniD2T BLEU 60.4; removing position matrix yields BLEU 58.3 (drop ≈ 2.1 BLEU). Combined removal with attention causes larger drops (see attention ablation). Overall inclusion contributes to top reported results above baselines.",
            "comparison_to_other_representations": "Explicit graph-aware position encoding outperforms standard T5 scalar position embeddings and learned positional schemes in prior table/graph pretraining work (T5-Large baselines) when combined with attention matrix modifications; ablation shows position and attention matrices are complementary.",
            "limitations_or_challenges": "Requires computing and storing per-edge auxiliary matrices (scales with graph connectivity); distances for non-connected node pairs are set to ±inf which is a hard cutoff and may discard useful long-range structure; design choices (distance metric, prefix connections) are heuristic and could be tuned further.",
            "uuid": "e5380.5",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Attention matrix",
            "name_full": "Relation-aware attention mask / attention matrix",
            "brief_description": "An attention matrix that replaces the standard Transformer attention mask with a binary relation-aware mask where attention is allowed only between directly connected tokens (and global prefixes attend to all tokens).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Relation-aware attention matrix (A_mask^new)",
            "representation_description": "Construct a binary attention matrix where entries are 1 if two elements have a direct graph relationship and 0 otherwise. Prefix tokens receive global attention (attend-to-all). This matrix substitutes the default fully-visible attention mask of T5 encoder, constraining self-attention to respect explicit graph connectivity.",
            "graph_type": "Unified graph inputs (KGs, tables, key-value graphs)",
            "representation_properties": "Encodes explicit connectivity constraints into self-attention, reducing spurious interactions across disconnected nodes; enforces locality consistent with graph edges while allowing prefixes to propagate global information.",
            "evaluation_task": "Used in pretraining and finetuning for all targeted D2T evaluation datasets; ablations isolate its contribution (WebNLG, etc.).",
            "performance_metrics": "Ablation on WebNLG: removing attention matrix reduces BLEU from 60.4 to 58.6 (drop ≈ 1.8 BLEU). Removing both attention and position reduces BLEU to 56.7.",
            "comparison_to_other_representations": "When combined with the position matrix, achieves better performance than the original Transformer attention (fully-visible) and than models that only adopt linearized inputs with standard attention; ablations indicate both position and attention contribute complementary gains.",
            "limitations_or_challenges": "Binary mask is a hard constraint—long-range interactions that are not explicitly connected cannot be considered except via prefixes; mask construction relies on precomputed connectivity and may be brittle if the chosen graph edges are incorrect or incomplete.",
            "uuid": "e5380.6",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Pretraining objectives",
            "name_full": "Struct denoising and graph-to-text generation objectives",
            "brief_description": "Two multi-task pretraining objectives: struct denoising (mask-and-reconstruct nodes in the graph) and graph-to-text generation (autoregressive negative log-likelihood to generate text from graph).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Struct denoising + Graph-to-Text generative objective",
            "representation_description": "Struct denoising: randomly mask ~15% of nodes (keeping edges) and train model to reconstruct masked node tokens (sentinel-delimited). Graph-to-text: train autoregressively to generate target text conditioned on linearized graph and explicit connectivity, using standard negative log-likelihood loss.",
            "graph_type": "Unified graph inputs used in pretraining (tables, key-value, KGs)",
            "representation_properties": "Struct denoising encourages the model to learn local node relationships and graph context; graph-to-text objective directly optimizes the target generative task. Multi-task setup blends reconstruction and generation learning signals.",
            "evaluation_task": "Used during UniD2T pretraining and evaluated on the same downstream D2T tasks (WebNLG, DART, ToTTo, CoSQL, WikiBio, WikiTableT).",
            "performance_metrics": "No isolated metric solely for pretraining objectives, but combined pretraining yields substantial downstream boosts: e.g., UniD2T (pretrained with these objectives) obtains DART BLEU 54.96 and WebNLG BLEU 60.41, improving over T5 baselines and prior pretraining schemes.",
            "comparison_to_other_representations": "Different from pure sequence denoising (T5 style) or KG-specific pretraining (KGPT), struct denoising operates on graph nodes preserving connectivity; combining struct denoising with graph-to-text gives better downstream performance than pretraining only with linearized data (Table 10 comparisons between P_Graph and P_Linear).",
            "limitations_or_challenges": "Masking strategy is simple (node masking only) and may not capture more complex graph corruption/noise patterns; scaling to larger or more diverse pretraining corpora could require more varied or task-specific objectives.",
            "uuid": "e5380.7",
            "source_info": {
                "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tapas: Weakly supervised table parsing via pre-training",
            "rating": 2
        },
        {
            "paper_title": "Kgpt: Knowledge-grounded pre-training for data-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Text-to-text pre-training for data-to-text tasks",
            "rating": 2
        },
        {
            "paper_title": "Structural adapters in pretrained language models for amr-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Robust (controlled) table-to-text generation with structure-aware equivariance learning",
            "rating": 1
        }
    ],
    "cost": 0.018848499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unifying Structured Data as Graph for Data-to-Text Pre-Training</h1>
<p>Shujie $\mathbf{L i}^{1 *}$ Liang $\mathbf{L i}^{3}$ Ruiying Geng ${ }^{2}$ Min Yang ${ }^{1 \dagger}$ Binhua $\mathbf{L i}^{2}$ Guanghu Yuan ${ }^{1}$ Wanwei $\mathrm{He}^{1}$ Shao Yuan ${ }^{2}$ Can $\mathrm{Ma}^{3}$ Fei Huang ${ }^{2}$ Yongbin $\mathbf{L i}^{2 \dagger}$<br>${ }^{1}$ Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China<br>${ }^{2}$ DAMO Academy, Alibaba Group, China<br>${ }^{3}$ Institute of Information Engineering, Chinese Academy of Sciences, China<br>{sj.li1, min.yang}@siat.ac.cn, shuide.lyb@alibaba-inc.com</p>
<h4>Abstract</h4>
<p>Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structureenhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch /DAPD-ConvAI/tree/main/unid2t.</p>
<h2>1 Introduction</h2>
<p>Data-to-text (D2T) generation, which aims to generate a target natural language text conditioned on source structured data, has attracted noticeable attention due to its many applications such as journalism (Rebuffel et al., 2020), medical diagnosis (Nishino et al., 2020), financial and weather re-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ports (Liang et al., 2009), and sports broadcasting (Chen and Mooney, 2008). The input structured data can include tables of records, simulations of physical systems, spreadsheets, knowledge graphs, and so on. Transforming structured data into textual data can facilitate a wide range of users to understand and use the structured data, which is needed in many real-life scenarios.</p>
<p>Recently, large-scale pre-trained models have proved to be powerful in D2T generation and yield impressive performance (Kale and Rastogi, 2020; Xing and Wan, 2021; Liu et al., 2022), which benefit from the rich knowledge contained in large-scale pre-training corpora. Xing and Wan (2021) proposed a structure-aware table-to-text pre-training model, which devised three selfsupervised training objectives tailored for modeling tables and their contexts. Ke et al. (2021) adopted a structure-aware semantic aggregation module to model the structure of an input graph at each Transformer layer, and explicitly learned graph-text alignments instead of directly finetuning text-to-text pre-trained models on graph-to-text corpora.</p>
<p>Although significant progress has been made in this field, there are still several technical challenges with existing D2T pre-training methods. Most prior studies made a cumbersome design tailored for a specific data structure such as tables (Liu et al., 2022) or knowledge graphs (Li et al., 2022), which could not effectively deal with diverse structured data in a unified framework. Kale and Rastogi (2020) were the first to study the "pretrain and fine-tune" strategy on several benchmarks spanning task-oriented dialogue, table-to-text, and graph-to-text. However, they oversimplified the input structured data into a flat string and adopted an original Transformer without capturing the structural information of source structured data.</p>
<p>In this paper, we unify the structured data into the graph format for data-to-text pre-training (denoted as UniD2T). We convert diverse types of structured data into a unified graph format, keeping the structural information of the structured data. We treat the items in the structured data as a set of nodes and connect the nodes according to the connectivity of the structured data. In this way, we can cast various D2T tasks as the graph-to-text generation task.</p>
<p>To effectively encode the graph structure, we propose a structure-enhanced pre-training model, which can be applied to various downstream D2T generation tasks. Our proposed D2T pre-training model is built upon the T5 model (Raffel et al., 2020). Since the T5 model is a text-to-text transfer Transformer framework and cannot effectively encode the graph structure, we propose a structureenhanced Transformer to encode the structural information. Concretely, we propose an explicit position matrix for the Transformer, encoding the relative positional information of connected nodes in the input graph. In addition, we build a new attention matrix to replace the attention mask in self-attention of the original Transformer, which encodes graph structures and takes the available explicit connectivity structure into account.</p>
<p>Our main contributions are three-fold. (1) We unify diverse types of structured data into a graph format and cast all D2T tasks as the graph-to-text generation task taking a graph as input and producing a text as output. (2) We propose a structureaware pre-training method for D2T generation based on the T5 model, which incorporates relative positional information and graph structures into the original Transformer via two new position and attention matrices, respectively. (3) We conduct extensive experiments on six D2T benchmarks and achieve substantially better performance than strong baselines. We believe that the release of our unified D2T pre-training model will advance the research in this area.</p>
<h2>2 Related Works</h2>
<h3>2.1 Data-to-Text Generation</h3>
<p>Data-to-text (D2T) generation aims to produce output texts from structured data and has attracted noticeable attention from the natural language processing (NLP) community (Reiter and Dale, 1997). Recently, neural D2T models (Song et al., 2018; Zhu et al., 2019) have been the mainstream
for this task and made impressive progress. The end-to-end neural models generate text directly from structured data by using an encoder-decoder architecture (Sutskever et al., 2014). These studies usually focus on improving the encoder structures based on attention mechanisms (Koncel-Kedziorski et al., 2019; Mehta et al., 2022) or graph neural networks (GNNs) (Philipp and Schütze, 2021; Ribeiro et al., 2021a,b). For example, Wang et al. (2020) proposed a graph-to-sequence model using a pairwise interaction function to obtain semantic relations between concepts. Puduppully et al. (2022) suggested a neural architecture that incorporated a planning module to manage high-level information in a logical and meaningful manner. Liu et al. (2018) proposed a structure-aware sequence-to-sequence architecture, which incorporated the filed information as additional input to the table encoder. Song et al. (2018) introduced graph recurrent networks (GRNs) to encode the AMR nodes directly. Subsequently, Shi et al. (2020) proposed GNNs as the structural encoder, which updated the representations of nodes based on their immediate neighbors. To integrate both local and non-local features and learn a better structural representation of a graph, Guo et al. (2019) introduced dense connection and allowed deep GCNs. Different from the local information aggregation scheme, Cai and Lam (2020) proposed a graph transformer that used explicit relation encoding and allowed direct communication between two distant nodes.</p>
<h3>2.2 Data-to-Text Pre-training Models</h3>
<p>Recently, we have witnessed the remarkable success of pre-training methods in a wide range of NLP tasks (Kenton and Toutanova, 2019; Radford et al., 2018; Lan et al., 2019; Bi et al., 2020). Most pre-training models are initially designed to text-to-text generation, lacking the ability to encode structural information. Recently, there exist some pre-training models designed for D2T tasks (Chen et al., 2020b; Agarwal et al., 2021; Ke et al., 2021; Bai et al., 2022). For example, KGPT (Chen et al., 2020b) proposed a distantly supervised learning method to exploit large-scale unlabeled web text for data-to-text pre-training. However, these pretraining models consider only one specific data structure and cannot be applied to diverse downstream D2T tasks. Although Tang et al. (2022) proposed a multi-task supervised pre-training model</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Statistics</th>
<th style="text-align: right;">PreData</th>
<th style="text-align: right;">DownData</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Datasets</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;"># Instances</td>
<td style="text-align: right;">$4,951,267$</td>
<td style="text-align: right;">$2,240,927$</td>
</tr>
<tr>
<td style="text-align: left;">Avg. input tokens</td>
<td style="text-align: right;">84.1</td>
<td style="text-align: right;">63.7</td>
</tr>
<tr>
<td style="text-align: left;">Avg. target tokens</td>
<td style="text-align: right;">90.8</td>
<td style="text-align: right;">100.9</td>
</tr>
<tr>
<td style="text-align: left;">Avg. Nodes</td>
<td style="text-align: right;">17.8</td>
<td style="text-align: right;">19.4</td>
</tr>
<tr>
<td style="text-align: left;">Avg. Edges</td>
<td style="text-align: right;">112.3</td>
<td style="text-align: right;">103.1</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of our pre-training data.
(MVP) for a series of D2T generation tasks, they utilized the original Transformer to encode the linearized input data without considering the graph structures. UniLM (Dong et al., 2019) was a pre-trained universal language model, which incorporated modified self-attention masks to facilitate bidirectional encoding or unidirectional decoding. While UniLM offers the flexibility of bidirectional encoding, its encoding attention mask is designed primarily for processing unstructured text, thereby restricting its ability to capture the structural characteristics of input graphs.</p>
<p>Different from previous work, we propose a unified pre-training model that casts all D2T tasks as the graph-to-text generation task. In addition, we incorporate graph structures into the original Transformer via two new position and attention matrices to effectively model the structured input data.</p>
<h2>3 Pre-training Data Construction</h2>
<p>Previous data-to-text pre-training datasets are usually tailored to specific structured data. In this paper, we collect eight D2T datasets from previous works and aggregate these datasets into a large corpus for pre-training our model. The statistics of pre-training data are provided in Table 1.</p>
<h3>3.1 Existing Pre-training Datasets (PreData)</h3>
<p>We first collect the table-text dataset TaPas (Herzig et al., 2020) and the graph-text dataset KGTEXT (Chen et al., 2020b), which were originally designed for table-to-text and graph-to-text pre-training, respectively. TAPAs contains 6.2 M tables from Wikipedia, and KGTEXT consists of 1.8 M hyperlinked sentences from Wikipedia with the corresponding knowledge subgraphs from
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>TARGET SEONTENCE: At the very end of his life, Arthur III became duke of Brittany, succeeding Peter II.</p>
<p>Key-Value Data
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: Unifying data in three formats into one graph structure.</p>
<p>WikiData. We further devise a rule-based datacleaning strategy to guarantee data quality. Finally, we obtain 4.9 M data-text pairs (called PreData).</p>
<h3>3.2 Existing Downstream Datasets (DownData)</h3>
<p>We also collect the training sets from six data-totext datasets, including WebNLG (Gardent et al., 2017), DART (Nan et al., 2020), ToTTo (Parikh et al., 2020), WikiBio (Lebret et al., 2016), WikiTableT (Chen et al., 2021), and CoSQL (Yu et al., 2019). These datasets were designed for downstream data-to-text generation tasks. Concretely, WebNLG and DART are graph-to-text datasets; WikiBio and WikiTableT contain key-value pairs; ToTTo and CoSQL are table-based datasets. In total, there are about 2.2 M instances (DownData). Notably, the test sets utilized for downstream tasks are expressly omitted from the pre-training data, ensuring the integrity of our experimental results by eliminating any potential data leakage.</p>
<h3>3.3 Unifying Structured Data</h3>
<p>As illustrated in Figure 1, we unify different structured data (knowledge graph, table, key-value pairs) into an unlabeled and connected graph $\mathcal{G}=$ $(\mathcal{V}, \mathcal{E})$ that consists of a set of nodes $v \in \mathcal{V}$ and unlabeled edges $\left(v_{i}, v_{j}\right) \in \mathcal{V}$. Next, we elucidate the process of transforming the three distinct types</p>
<p>of data (i.e., knowledge graphs, tables, and keyvalue pairs) into a unified graph $\mathcal{G}$.
(1) On the left side of Figure 1's Graph Data section, a knowledge graph can be formally expressed as $\mathcal{G}<em 0="0">{0}=\left(\mathcal{V}</em>}, \mathcal{E<em 0="0">{0}, \mathcal{R}</em>}\right)$, where nodes are denoted by $v \in \mathcal{V<em s="s">{0}$, and labeled edges are represented as $\left(v</em>}, r, v_{t}\right) \in \mathcal{E<em 0="0">{0}$, with $r \in \mathcal{R}</em>}$ signifying the relation type. To more effectively model the relationships between nodes within the knowledge graph $\mathcal{G<em 0="0">{0}$ without modifying the underlying model architecture, we transform it into its equivalent Levi graph, as shown on the right side of the Graph Data section in Figure 1, following similar methodologies as in prior studies (Ribeiro et al., 2021b; Li et al., 2022). A Levi graph is formally characterized as an unlabeled, connected bipartite graph, denoted as $\mathcal{G}=(\mathcal{V}, \mathcal{E})$. Specifically, each relation in $\mathcal{R}</em>}$ is treated as a new graph node within $\mathcal{G}$ and amalgamated with all nodes in $\mathcal{V<em s="s">{0}$ to form the comprehensive node set $\mathcal{V}$. Subsequently, each edge $\left(v</em>}, r, v_{t}\right) \in \mathcal{E<em s="s">{0}$ labeled with a relation type is converted into two unlabeled, undirected edges $\left(v</em>$.
(2) In the Table Data section of Figure 1, situated on the left side, Tabular data is conventionally structured with numerous cells organized based on their respective roles and interrelations. A table can be formally represented as $\mathcal{T}=$ $v_{i, j} \mid i \in[1, N], j \in[1, M]$, where $v_{i, j}$ denotes a table cell, and $N$ and $M$ represent the number of rows and columns in the table, respectively. Inspired by recent studies (Wang et al., 2022; Li et al., 2023a), we use a heuristic rule to transform the tabular data into a unified graph $\mathcal{G}$ by introducing unlabeled edges between cells based on their roles and relationships. This structural transformation serves to maintain the invariance of the table content and proficiently articulate the relationships among cells in the table. More precisely, all cells within $\mathcal{T}$ are considered as graph nodes in $\mathcal{G}$, denoted as $\mathcal{V}=\mathcal{T}$. Furthermore, we establish the set of unlabeled edges $\mathcal{E}$ in accordance with two guiding principles. First, for any two cells $v_{i, j}$ and
$v_{i, z}$ situated within the same row, we introduce a forward edge $\left(v_{i, j}, v_{i, z}\right)$ along with a corresponding reverse edge $\left(v_{i, z}, v_{i, j}\right)$ into $\mathcal{E}$. Second, for any two cells $v_{i, j}$ and $v_{i, z}$ located in the same column, we append a forward edge $\left(v_{i, j}, v_{z, j}\right)$ and its corresponding reverse edge $\left(v_{z, j}, v_{i, j}\right)$ to $\mathcal{E}$. For instance, contemplating the right Table Data section in Figure 1, the cell "Arthur III" is linked not only to cells " 1457 " and " 1458 " in the same row but also to cells "Name" and "Peter II" in the same column. This intentional configuration is based on empirical observations and insights gained from data analysis. However, we acknowledge that there exists room for further exploration and experimentation concerning diverse node connectivity settings in future research. Given that the ToTTo dataset exclusively generates text for highlighted data, only the highlighted cells are considered as nodes.
(3) For Key-Value data in Figure 1, both key and value are regarded as nodes within $\mathcal{V}$. In addition to the requisite connection edges linking each (key, value) pair (e.g., the connection between the key name and the value walter extra), we extend our connectivity framework to include connections among keys themselves (e.g., the connection between nationality and birth_date) and value themselves (e.g., the connection between walter extra and gernman), drawing inspiration from the graph construction methodology commonly employed in table data analysis. In line with tabular data, we introduce both forward and reverse edges for any connected nodes within $\mathcal{V}$.}, r\right),\left(r, v_{t}\right) \in \mathcal{E}$. In addition, for each unlabeled edge, corresponding reverse edges $\left(r, v_{s}\right),\left(v_{t}, r\right)$ are introduced. For instance, given a labeled edge (Dance of the Seven Veils, GENRE, incidental music), this conversion results in four unlabeled edges (Dance of the Seven Veils, GENRE), (GENRE, Dance of the Seven Veils), (GENRE, incidental music), and (incidental music, Dance of the Seven Veils), constituting the final Levi graph $\mathcal{G</p>
<p>To ensure clarity and context in the generated text, we introduce two specific prefixes before the actual input data: (1) A data-independent prefix that universally states "describe the following data." (2) A data-specific prefix, tailored according to the nature and structure of the data at hand. We provide the data-specific prefixes for the three data structures in Table 2. For example, the triple "Jens_Hartel I club iBerliner_AK_07" from the DART dataset will add the common prefix and its special prefix to form an input "[Prefix] describe the following data: [Prefix] The category of the DBpedia entities is: SportsTeam. [Node] Jens_Hartel [Node] club [Node] Berliner_AK_07''. We simplify the data-independent and data-specific prefixes to " $[$ Prefix-I]" and "[Prefix-S]", respectively. The final input sequence with connectivity information is shown in Figure 2.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Dataset</th>
<th>Prefix-S</th>
</tr>
</thead>
<tbody>
<tr>
<td>Table</td>
<td>ToTTo</td>
<td>The table page title is: A, <br> The table section title is: B</td>
</tr>
<tr>
<td></td>
<td>CoSQL</td>
<td>select A from B where C</td>
</tr>
<tr>
<td>Graph</td>
<td>DART</td>
<td>The source is: A</td>
</tr>
<tr>
<td></td>
<td>WebNLG</td>
<td>The category of the entities is: A, <br> The number of RDF triples is: B</td>
</tr>
<tr>
<td>Key-Value</td>
<td>WikiBio</td>
<td>The article title is: A</td>
</tr>
<tr>
<td></td>
<td>WikiTableT</td>
<td>the document title is: A, <br> the section title is: B</td>
</tr>
</tbody>
</table>
<p>Table 2: The data-specific prefixes that are tailored for different types of data. Here, A, B, and C can be replaced by the content of specific samples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prefix-S</th>
<th style="text-align: center;">Prefix-S</th>
<th style="text-align: center;">[Node] Java Hartel</th>
<th style="text-align: center;">[Node] club</th>
<th style="text-align: center;">[Node]Borfmer AK GT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: Simplified version of model input and connections between nodes.</p>
<h2>4 Methodology</h2>
<h3>4.1 Problem Definition</h3>
<p>We convert different structured data into a graph format and cast all data-to-text tasks as the graph-to-text (G2T) generation task. Formally, the G2T model takes a graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ as input and produces a text $Y=\left{y_{1}, \ldots, y_{n}\right}$ as output, where $\mathcal{V}$ represents the entity set, $\mathcal{E}$ represents the relations between entities, and $n$ is the length of the output text. Following previous studies (Ribeiro et al., 2020), we convert the graph $\mathcal{G}$ into an input sequence $\mathcal{G}<em 1="1">{\text {linear }}=\left{x</em>\right}$ consisting of $m$ tokens.}, \ldots, x_{m</p>
<h3>4.2 Model Architecture</h3>
<p>Our model is built upon the pre-trained T5 model given the impressive performance of T5 on text generation tasks. It is noteworthy that our pretraining strategy is model-agnostic and potentially applicable to any Transformer-based backbone networks. The encoder of Transformer is composed of a stack of blocks, each of which contains a self-attention layer followed by a feed-forward network. The decoder has a similar structure to the encoder except that it adopts a standard attention mechanism following a self-attention layer.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Transformer blocks on the T5-encoder side. The relative position and attention matrices in the self-attention calculation will be replaced by two novel position and attention matrices.</p>
<p>Preliminary In the case of the T5-encoder, a "fully-visible" attention mask is used, which permits the self-attention mechanism to consider all input entries when generating each output entry. In addition, T5 adopts a simplified form of position embeddings, where each embedding is a scalar. Formally, as illustrated in Figure 3, the attention calculation of encoder can be expressed as:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{Q}=\mathbf{X} \mathbf{W}^{Q}, \mathbf{K}=\mathbf{X} \mathbf{W}^{K}, \mathbf{V}=\mathbf{X} \mathbf{W}^{V} \
&amp; \alpha=\frac{1}{\sqrt{d}}\left(\mathbf{Q K}^{T}+\mathbf{P}<em _mask="{mask" _text="\text">{\text {emb }}+\mathbf{A}</em>\right) \
&amp; \mathbf{Z}=\frac{\exp (\alpha)}{\sum \exp (\alpha)} \times \mathbf{V}
\end{aligned}
$$}</p>
<p>where $\mathbf{X}$ is the input sequence. $\mathbf{W}^{Q} \in \mathbb{R}^{d \times d_{Q}}$, $\mathbf{W}^{K} \in \mathbb{R}^{d \times d_{K}}$ and $\mathbf{W}^{V} \in \mathbb{R}^{d \times d_{V}}$ are learnable project parameters. $\alpha$ is the attention weight between the query vector $\mathbf{Q}$ and the key vector $\mathbf{K}$. $d$ is the dimensionality of the hidden representations. $\mathbf{Z}$ is the output of the attention module. $\mathbf{P}<em _mask="{mask" _text="\text">{\text {emb }}$ is position embedding and $\mathbf{A}</em>$ is attention mask.}</p>
<p>The original attention mechanism is designed to process unstructured natural language texts proves inadequate in effectively capturing the inherent structures within graphs. To better process our structured graph data, we replace the position embeddings $\mathbf{P}<em _mask="{mask" _text="\text">{\text {emb }}$ and attention mask $\mathbf{A}</em>$ in Equation (2) with two new position and attention matrices respectively, ensuring their awareness of the underlying graph structures. Next, we will elaborate on the processes of constructing the position and attention matrices.}</p>
<h1>4.3 Structure-enhanced Transformer</h1>
<p>T5 is based on an encoder-decoder Transformer, which does not necessarily capture graph structures. To address this issue, we propose a structureenhanced Transformer, which is built upon the new position and attention matrices on the T5 encoder side. As illustrated in Figure 3, we use new position embedding and attention mask matrices (denoted as $\mathbf{P}<em _mask="{mask" _text="\text">{\text {emb }}^{\text {new }}$ and $\mathbf{A}</em>}}^{\text {new }}$ ) to replace the $\mathbf{P<em _mask="{mask" _text="\text">{\text {emb }}$ and $\mathbf{A}</em>$. In addition, we propose a new attention matrix to replace the attention mask in the selfattention, which takes the available explicit connectivity structure of the input graph into account.}}$ in the Equation (2), respectively. Specifically, we devise a position matrix for the Transformer to encode the relative positional information of connected nodes in the original input graph $\mathcal{G</p>
<h3>4.3.1 Position Matrix Construction</h3>
<p>Integrating relational information about the graph structure into the Transformer architecture is essential for graph-to-text generation. Nevertheless, most previous Transformer-based methods (Xing and Wan, 2021; Han and Shareghi, 2022) learned position embeddings automatically, instead of explicitly encoding the structural relationships. For the input graph, we should only consider the relative position between connected nodes but ignore the relative position between irrelevant nodes. To this end, we replace the positional embeddings of the original Transformer with a position matrix that only establishes the relative position between each relevant node pair (connected items). In this way, we can explicitly capture the relative positions of all relevant nodes precisely.</p>
<p>Specifically, we first establish an auxiliary position matrix for each pair of connected nodes, similar to the green and yellow boxes in Figure 4. No matter how physically distant the two relevant nodes may be, the corresponding auxiliary position matrix solely takes into account the relative distance between these two nodes' internal tokens, disregarding the nodes situated between the two target nodes. For example, consider the input nodes '‘[Node] club" and '‘[Node] Jens Hartel", since "club" is 3 units to the right of "Jens", the value of cell [Jens, club] is 3. Notably, we only compute the relative distance between each connected note pair, while the distances of nodes lacking direct connections will be set to " $\pm i n f$ ", signifying an infinite distance between them. For
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: We construct a new position matrix $\mathbf{P}<em _emb="{emb" _text="\text">{\text {emb }}^{\text {new }}$ to replace the original position matrix $\mathbf{P}</em>$ used in Equation (2). We first set an auxiliary matrix for each edge between two nodes, and then copy the content of the auxiliary matrix into the final position matrix. The distances of nodes lacking direct connections will be set to " $\pm i n f$ ". The lighter the color, the farther the distance is.
instance, the value assigned to the cell [Jens, Berliner] is " $+i n f$ "' due to the absence of a direct connection between '‘[Node] Jens Hartel" and '‘[Node] Berliner AK 07'".}</p>
<p>After obtaining the auxiliary position matrix for each pair of connected items, we can construct the position matrix for the entire input sequence by copying the cell values from the corresponding auxiliary position matrices. It is noteworthy that we seek to endow the prefixes (denoted as '‘[Prefix-I]" and '‘[Prefix-S]") embedded within the input with the capacity to encapsulate comprehensive global information. Therefore, we postulate that these prefixes establish direct connections with other nodes within the input. Finally, we replace the positional embeddings $\mathbf{P}_{\text {emb }}$ of original Transformer with the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: We construct a new attention matrix $\mathbf{A}<em _mask="{mask" _text="\text">{\text {mask }}^{\text {new }}$ to replace the attention mask $\mathbf{A}</em>$ used in Equation (2). The attention matrix used to replace the attention mask of self-attention in Transformer. The values of the cells with colors are set to 1, while the values of the cells without colors are set to 0 . The blue color represents global attention, the gray color represents the self-connection of nodes, and the green and yellow colors represent the two connected edges.
learned position matrix $\mathbf{P}_{\text {emb }}^{\text {new }}$, so as to effectively capture the explicit relative distance between each pair of connected items.}</p>
<h3>4.3.2 Attention Matrix Construction</h3>
<p>The self-attention in the original Transformer processes the input sequence by transforming the input sequence through the substitution of each element with a weighted average. Without refining the conventional attention mechanism, the present input data would be perceived as a fully interconnected graph, potentially hindering the optimal extraction of inherent structural information. Given the above reasons, we construct a relation-aware attention matrix to replace the original attention mask in self-attention. Concretely, if two elements have a direct relationship, we set the value of the corresponding cell to 1 ; otherwise, the value is set to 0 . For example, as illustrated in Figure 5, since the items 'Jens Hartel" and 'club" have direct connection, the values of cells (Jens, club) and (Hartel, club) are set to 1; while since 'Jens Hartel" and 'Berliner Ak 07'' have no direct connection, the values of the corre-
sponding cells such as (Jens, Berliner) and (Jens, $A K)$ are set to 0 . Here, we hope that the prefixes (i.e., '‘[Prefix-I]" and '‘[Prefix-S]") within the input can carry global information, thus we make the prefixes attend to all other elements. After obtaining the attention matrix (denoted as $\mathbf{A}<em _mask="{mask" _text="\text">{\text {mask }}^{\text {new }}$ ), we replace the attention matrix $\mathbf{A}</em>$ so as to effectively capture the graph structures as shown in Figure 3.}}$ of self-attention in Equation (2) with our new attention matrix $\mathbf{A}_{\text {mask }}^{\text {new }</p>
<h3>4.4 Pre-training Objectives</h3>
<p>Similar to Andrejczuk et al. (2022), we first use the publicly available T5 checkpoints provided by Herzig et al. (2020) as the initialization. Then, we pre-train our model on our pre-training data. We employ two objectives to pre-train our model in a multi-task learning paradigm, including struct denoising and text generation objectives. In Table 3, we provide two specific training instances (input and output pairs) for the struct denoising and graph-to-text generation objectives.</p>
<p>Struct Denoising Objective We design a struct denoising strategy for table-like data, following the method used in T5, by training the model to predict a target sequence containing the missing or corrupted tokens in the input graph. We apply a noise function to construct a noisy input graph. In particular, the noise function is implemented by masking $15 \%$ of nodes while maintaining related edges in the graph. The goal of struct denoising objective is to reconstruct the target output that contains all the dropped-out nodes, delimited by the sentinel token. This pre-training objective helps the UniD2T model capture relationships between neighboring nodes in the input graph.</p>
<p>Graph-to-Text Generation Objective Given the linearized graph $\mathcal{G}<em _mathrm_TG="\mathrm{TG">{\text {linear }}$ and its explicit connectivity structure $\mathcal{E}$, the graph-to-text generation task is carried out to produce the appropriate text to describe the given graph in an auto-regressive manner. We adopt the standard negative loglikelihood loss $\mathcal{L}</em>$ for the graph-to-text generation task:}</p>
<p>$$
\mathcal{L}<em i="1">{\mathrm{TG}}=-\frac{1}{N} \sum</em>\right)
$$}^{n} \log p\left(y_{i} \mid y_{1}, \ldots, y_{i-1} ; \mathcal{G}_{\text {linear }}, \mathcal{E</p>
<p>where $n$ is the length of the target sequence $Y$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Inputs</th>
<th style="text-align: center;">Targets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Struct Denoising</td>
<td style="text-align: center;">The category of the DBpedia entities is: $<e x t r a \cdot i d_{0}>$, <br> 'Bakewell pudding', 'dish variation', ' $<e x t r a \cdot i d_{1}>$ ', <br> 'main ingredients', 'Ground almond, jam, butter, eggs'</td>
<td style="text-align: center;">$<e x t r a \cdot i d_{0}>$ Food <br> $<e x t r a \cdot i d_{1}>$ Bakewell tart</td>
</tr>
<tr>
<td style="text-align: center;">Graph-to-Text Generation</td>
<td style="text-align: center;">Describe the following data: The category of the DBpedia entities is: Food. 'Bakewell pudding', 'dish variation', 'Bakewell tart', 'main ingredients', 'Ground almond, jam, butter, eggs'</td>
<td style="text-align: center;">Bakewell tart is a variation of Bakewell pudding and some of the main ingredients are ground almonds, jam, butter and eggs.</td>
</tr>
</tbody>
</table>
<p>Table 3: The examples of input-output pairs for struct denoising and graph-to-text generation objectives.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">Train</th>
<th style="text-align: right;">Valid</th>
<th style="text-align: right;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ToTTo</td>
<td style="text-align: right;">120,761</td>
<td style="text-align: right;">7,700</td>
<td style="text-align: right;">7,700</td>
</tr>
<tr>
<td style="text-align: left;">CoSQL</td>
<td style="text-align: right;">7,845</td>
<td style="text-align: right;">1,074</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: right;">13,211</td>
<td style="text-align: right;">1,667</td>
<td style="text-align: right;">1,779</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: right;">62,659</td>
<td style="text-align: right;">6,980</td>
<td style="text-align: right;">12,552</td>
</tr>
<tr>
<td style="text-align: left;">WikiBio</td>
<td style="text-align: right;">582,657</td>
<td style="text-align: right;">72,831</td>
<td style="text-align: right;">72,831</td>
</tr>
<tr>
<td style="text-align: left;">WikiTableT</td>
<td style="text-align: right;">$1,453,794$</td>
<td style="text-align: right;">4,533</td>
<td style="text-align: right;">4,351</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistics of downstream datasets.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Tasks and Datasets</h3>
<p>To verify the generality and effectiveness of UniD2T, we conduct experiments on three types of data-to-text datasets. In particular, WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020) are used for evaluating graph-to-text generation; WikiBio (Lebret et al., 2016) and WikiTableT (Chen et al., 2021) are utilized for evaluating key-value-to-text generation; ToTTo (Parikh et al., 2020) and CoSQL (Yu et al., 2019) are used for evaluating table-to-text generation. Table 4 provides the statistics of these six datasets.</p>
<h3>5.2 Implementation Details</h3>
<p>In the pre-training stage, our model is initialized with T5-Large. We pre-train our UniD2T model on NVIDIA A100 GPUs. The maximum sequence lengths of the input and target sequences are set to 1024 and 512 , respectively. We set the batch size to 8 . Gradient clipping is applied to the model with a maximum gradient value of 1 . To alleviate the overfitting issue, the maximum number of training steps is 500 k . Moreover, a patient step number is set to 25 k , i.e., if the evaluation metrics does not increase for the patient step number, the training process will carry out an early stop. We set the maximum learning rate to $1 \mathrm{e}-5$.</p>
<h2>6 Experimental Results</h2>
<h3>6.1 Table-to-Text Generation</h3>
<p>We conduct experiments on two table-to-text datasets, including ToTTo and CoSQL. The SQL queries within CoSQL and the table header information from ToTTo are strategically positioned within the data-specific prefixes, denoted as '‘[Prefix-S]'', as illustrated in Table 2.</p>
<p>ToTTo ToTTo is an open-domain table-to-text task dataset that uses crowd annotators to highlight the table cells and revise the corresponding natural language descriptions. We compare our UniD2T with several strong baselines, including BERT2BERT (Rothe et al., 2020), LATTICE (Wang et al., 2022), CoNT (An et al., 2022), PlanGen (Su et al., 2021), and TABT5 (Andrejczuk et al., 2022). TABT5 is a pre-trained model tailored for table-to-text generation. We adopt BLEU (Papineni et al., 2002) and PARENT (Dhingra et al., 2019) as the evaluation metrics. The experimental results on ToTTo are summarized in Table 5. Our model achieves substantially better performance than the compared methods on ToTTo in terms of overall, overlap, and nonoverlap settings. First, our model shows an improvement over T5 and TABT5, especially in terms of PARENT. Second, our model also achieves better results than the strong downstream methods.</p>
<p>CoSQL CoSQL serves as a prevalent benchmark for evaluating table-to-text models (Fang et al., 2022b; Li et al., 2023b). Each instance within CoSQL comprises an SQL query, the resultant table, and the corresponding response, where the SQL query gives explicit signals for models on what to generate. The generated description could provide a concise and easy-to-understand summary of the result table and help users verify whether the queried result is consistent with the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overlap</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Non-Overlap</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">:--</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT(gpt-3.5-turbo)</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">47.7</td>
</tr>
<tr>
<td style="text-align: left;">BERT-to-BERT(Rothe et al., 2020)</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">46.8</td>
</tr>
<tr>
<td style="text-align: left;">LATTICE (Wang et al., 2022)</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">53.9</td>
</tr>
<tr>
<td style="text-align: left;">CoNT (An et al., 2022)</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: left;">PlanGen (Su et al., 2021)</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">54.2</td>
</tr>
<tr>
<td style="text-align: left;">T5-3B</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">54.2</td>
</tr>
<tr>
<td style="text-align: left;">TABT5 (Andrejczuk et al., 2022)</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: center;">$\mathbf{4 9 . 9}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on the ToTTo test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">ROUGE-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GraphWriter</td>
<td style="text-align: left;">16.86</td>
<td style="text-align: left;">47.44</td>
</tr>
<tr>
<td style="text-align: left;">FALCON</td>
<td style="text-align: left;">25.65</td>
<td style="text-align: left;">57.89</td>
</tr>
<tr>
<td style="text-align: left;">BART-Base</td>
<td style="text-align: left;">24.60</td>
<td style="text-align: left;">57.39</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: left;">25.25</td>
<td style="text-align: left;">57.54</td>
</tr>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: left;">$\mathbf{3 2 . 6 8}$</td>
<td style="text-align: left;">$\mathbf{6 1 . 4 7}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on CoSQL development set.
original question. We compare our model with GraphWriter (Koncel-Kedziorski et al., 2019), BART-Base, T5-Large, and FALCON (Fang et al., 2022a) which is a faithful contrastive generation framework based on T5. We adopt BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) as evaluation metrics. Since CoSQL does not release the test set, we follow FALCON and report the experimental results on the development set in Table 6. Our UniD2T model achieves significantly better performance than baselines. The BLEU and ROUGE scores increase by 7.03 and 3.58 , respectively, over the best-performing baseline FALCON.</p>
<h3>6.2 Graph-to-Text Generation</h3>
<p>We conduct experiments on two graph-to-text datasets, including DART and WebNLG.</p>
<p>DART DART is a large dataset for open-domain text generation that treats the input as a set of RDF entity-relation triples. We compare our UniD2T model with several pre-training models including Transformer, BART, T5, and the state-of-the-art method CONTROL PREFIXES (Clive et al., 2021). BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2005) are adopted as evaluation metrics.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">TER</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">End-to-End Transformer $\dagger$</td>
<td style="text-align: center;">27.24</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">LSTM with Attention $\dagger$</td>
<td style="text-align: center;">29.66</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">CONTROL PREFIXES</td>
<td style="text-align: center;">51.95</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT(gpt-3.5-turbo)</td>
<td style="text-align: center;">40.51</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: left;">BART-Base $\dagger$</td>
<td style="text-align: center;">47.11</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: left;">BART-Large $\dagger$</td>
<td style="text-align: center;">48.56</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: left;">T5-Small $\dagger$</td>
<td style="text-align: center;">47.69</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base $\dagger$</td>
<td style="text-align: center;">49.21</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $\dagger$</td>
<td style="text-align: center;">50.66</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: center;">$\mathbf{5 4 . 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Evaluation results on DART test set. Results with $\dagger$ are token from DART (Nan et al., 2020).</p>
<p>As shown in Table 7, our model surpasses the best-performing model CONTROL PREFIXES by a $3.0 \%$ BLEU.</p>
<p>WebNLG WebNLG (Zhou and Lampouras, 2020) consists of a set of triples collected from DBpedia and the corresponding manually annotated text. BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), chrF++ (Popović, 2015), TER (Snover et al., 2005), and BLEURT (Sellam et al., 2020) are adopted as evaluation metrics. We compare our method with both pre-trained language models and strong downstream baselines. The overall experimental results on WebNLG are shown in Table 8. Our model achieves the highest performance among all baseline models, including the graph pre-training model TRIPLE (Han and Shareghi, 2022).</p>
<h3>6.3 Key-Value-to-Text Generation</h3>
<p>We conduct experiments on two key-value-based datasets, including WikiBio and WikiTableT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">chrF++</th>
<th style="text-align: center;">TER</th>
<th style="text-align: center;">BLEURT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CP</td>
<td style="text-align: center;">54.97</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">0.62</td>
</tr>
<tr>
<td style="text-align: left;">CP + DART</td>
<td style="text-align: center;">55.41</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">51.74</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: left;">TRIPLE</td>
<td style="text-align: center;">57.64</td>
<td style="text-align: center;">42.24</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: center;">$\mathbf{6 0 . 4 1}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 3 5}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 4 . 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 5}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Evaluation results on WebNLG test set. CP stands for CONTROL PREFIXES (Clive et al., 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">WikiBio</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WikiTableT</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr>
<td style="text-align: left;">SANA</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">CoNT</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">KGPT</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">47.6</td>
</tr>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: center;">$\mathbf{5 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 8}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Results on WikiBio and WikiTableT test sets.</p>
<p>WikiBio WikiBio is designed to generate descriptions from a Wikipedia infobox and aims to generate the first sentence of a biography. We compare UniD2T with previous state-of-the-art model (i.e, CoNT [An et al., 2022]), pre-trained models (T5-Large, KGPT) and Non-autoregressive model SANA (Wang et al., 2021) on WikiBio. BLEU and PARENT are adopted as evaluation metrics. The results are reported in Table 9. UniD2T outperforms the best baseline CoNT by $3.7 \%$ on BLEU.</p>
<p>WikiTableT WikiTableT is collected from Wikipedia sections with their corresponding tabular data, which contains millions of instances. We compare UniD2T with Transformer, T5-Large and KGPT (Chen et al., 2020b). Experiment results on Table 9 show that UniD2T exceeds the best competitor KGPT by $1.9 \%$ on BLEU and $2.2 \%$ on PARENT.</p>
<h3>6.4 Further Analysis</h3>
<h3>6.4.1 Ablation Study</h3>
<p>We conduct experiments to investigate the impact of pre-training with graph structure and linear structure. The ablation results are summarized in Table 10, which is divided into two parts: The first part shows the results of directly fine-tuning
the pre-trained language model (i.e., T5-Large) on the downstream datasets, referred to as DownData, while the second part presents the results of incorporating additional pre-training data, denoted as PreData, on top of T5-Large. Through careful analysis, we observe that UniD2T (T5Large $+P_{\text {Graph }}+F_{\text {Graph }}$ ) consistently outperforms T5-Large $+F_{\text {Graph }}$ across all six data-to-text datasets, resulting in a notable improvement in the total score of +20.6 . In addition, we observe that T5-Large $+F_{\text {Graph }}$ outperforms T5-Large $+F_{\text {Linear }}$ in terms of the total score by +6.1 . This result clearly indicates that our method significantly improves the performance of the data-to-text models which linearize the structured data as input during fine-tuning the models on downstream datasets. Finally, we delve into the effects of the pre-training datasets. By comparing the results of $P_{\text {Graph }}^{<em>}+F_{\text {Graph }}$ and $P_{\text {Graph }}+F_{\text {Graph }}, P_{\text {Linear }}^{</em>}+$ $F_{\text {Linear }}$ and $P_{\text {Linear }}+F_{\text {Linear }}$, we observe that the downstream datasets contribute to improving the model's performance and accelerating the pretraining process. It is noteworthy that the pretraining involving both PreData and DownData achieves the best performance across all the experimental datasets.</p>
<p>We also delve into the effects of two Transformer modifications (position and attention matrix construction). The results are illustrated in Table 11. From the results, we observe a significant performance drop when either the structureaware position or attention matrices are removed, demonstrating the benefits of two Transformer modifications. It is no surprise that combining all the factors achieves the best performance. These findings collectively demonstrate the effectiveness of our proposed method, which explicitly models its graph structure through the use of structure-aware position and attention matrices.</p>
<h3>6.4.2 Few-Shot Results</h3>
<p>We conduct few-shot experiments on the E2ENLG (Dušek et al., 2020) dataset sourced from the restaurant domain other than Wikipedia. This serves as an additional validation of the model's generalization capabilities. The E2ENLG dataset, assembled through the CrowdFlower platform, encompasses details about restaurants and comprises over 50,000 combinations of dialogue-actbased meaning representations (MRs) with an average of 8.1 references. We fine-tune UniD2T using varying proportions of the training instances</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">ToTTo</th>
<th style="text-align: center;">CoSQL</th>
<th style="text-align: center;">DART</th>
<th style="text-align: center;">WebNLG</th>
<th style="text-align: center;">WikiBio</th>
<th style="text-align: center;">WikiTableT</th>
<th style="text-align: center;">Total Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Only Fine-tuning</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Previous SOTA</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $+F_{\text {Linear }}$</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">255.6</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $+F_{\text {Graph }}$</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">261.7</td>
</tr>
<tr>
<td style="text-align: left;">With Additional Pre-training</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $+P_{\text {Graph }}+F_{\text {Graph }}$ (UniD2T)</td>
<td style="text-align: center;">$\mathbf{5 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 4 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{2 8 2 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $+P_{\text {Graph }}^{+}+F_{\text {Graph }}$</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">268.0</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $+P_{\text {Linear }}+F_{\text {Linear }}$</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">265.1</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $+P_{\text {Linear }}^{+}+F_{\text {Linear }}$</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">257.9</td>
</tr>
</tbody>
</table>
<p>Table 10: Ablation test results on six benchmark datasets. $P_{\text {Linear }}$ and $P_{\text {Graph }}$ represent the models pre-training with linear structure and graph structure, respectively. $F_{\text {Linear }}$ and $F_{\text {Graph }}$ represent the models fine-tuning with graph structure and linear structure, respectively. $P^{*}$ stands for pre-training only with PreData; $P$ indicates pre-training with both PreData and DownData.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">chrF++</th>
<th style="text-align: center;">TER</th>
<th style="text-align: center;">BLEURT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: center;">$\mathbf{6 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 4 . 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 5}$</td>
</tr>
<tr>
<td style="text-align: left;">- attention</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: left;">- position</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: left;">- all</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">0.63</td>
</tr>
</tbody>
</table>
<p>Table 11: Ablation test results on WebNLG test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">$\mathbf{0 . 1 \%}$</th>
<th style="text-align: right;">$\mathbf{0 . 5 \%}$</th>
<th style="text-align: right;">$\mathbf{1 \%}$</th>
<th style="text-align: right;">$\mathbf{5 \%}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TGen</td>
<td style="text-align: right;">3.6</td>
<td style="text-align: right;">27.9</td>
<td style="text-align: right;">35.2</td>
<td style="text-align: right;">57.3</td>
</tr>
<tr>
<td style="text-align: left;">Template-GPT-2</td>
<td style="text-align: right;">22.5</td>
<td style="text-align: right;">47.8</td>
<td style="text-align: right;">53.3</td>
<td style="text-align: right;">59.9</td>
</tr>
<tr>
<td style="text-align: left;">KGPT-Graph</td>
<td style="text-align: right;">39.8</td>
<td style="text-align: right;">53.3</td>
<td style="text-align: right;">55.1</td>
<td style="text-align: right;">61.5</td>
</tr>
<tr>
<td style="text-align: left;">KGPT-Seq</td>
<td style="text-align: right;">40.2</td>
<td style="text-align: right;">53.0</td>
<td style="text-align: right;">54.1</td>
<td style="text-align: right;">61.1</td>
</tr>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: right;">$\mathbf{4 5 . 6}$</td>
<td style="text-align: right;">$\mathbf{5 7 . 3}$</td>
<td style="text-align: right;">$\mathbf{5 7 . 6}$</td>
<td style="text-align: right;">$\mathbf{6 4 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 12: Few-shot results on the E2ENLG test set.
(i.e., $0.1 \%, 0.5 \%, 1 \%, 5 \%$, and $10 \%$ ) from E2ENLG (Dušek et al., 2020). We compare UniD2T with several few-shot learning methods including TGen (Dušek and Jurčíček, 2016), Template-GPT-2 (Chen et al., 2020a), and KGPT (Chen et al., 2020b). The experimental results are summarized in Table 12. We can see that UniD2T significantly outperforms all baselines in various few-shot settings.</p>
<h3>6.4.3 Human Evaluation</h3>
<p>We also conduct a human evaluation to analyze the generated sentences following Chen et al. (2020b). It is worth noting that each evaluator is unaware of which model generates the text being evaluated so as to avoid evaluation bias. Specifically, we choose 100 test samples from WebNLG
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Human evaluation of the factual consistency of different models on WebNLG samples.
and observe the factual consistency between the gold sentences and generated sentences. We invite four NLP workers to assign each text a label from {Hallucination, Missing Fact, Accurate}, similar to (Chen et al., 2020b). As shown in Figure 6, our UniD2T is less prone to hallucinating non-existing facts and can generate more accurate sentences.</p>
<h3>6.4.4 Impact on Graph Sizes</h3>
<p>To illustrate the effectiveness of the graph structure, we further investigate the performance of $P_{\text {Linear }}+F_{\text {Linear }}$ and $P_{\text {Graph }}^{+}+F_{\text {Graph }}$ by concerning different graph sizes on the WebNLG validation set. Experimental results in terms of BLEU are shown in Figure 7. When the graph structure is simple, the impact of the graph structure is limited. However, as the graph structure becomes complex, the model with graph structure ( $P_{\text {Graph }}^{+}+$ $F_{\text {Graph }}$ ) performs much better than the model with linear structure ( $P_{\text {Linear }}+F_{\text {Linear }}$ ). Thus, the structure-enhanced model UniD2T demonstrates</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparing $P_{\text {Linear }}+F_{\text {Linear }}$ and $P_{\text {Graph }}+$ $F_{\text {Graph }}$ BLEU score changes in increasing the number of triples on WebNLG's seen and unseen.
greater stability and better performance on largescale inputs when compared to linear sequence models.</p>
<h3>6.4.5 Impact on Model Sizes</h3>
<p>To investigate the influence of different model scales on the experimental results, we conducted experiments using $F_{\text {Graph }}$ on T5-Small, T5-Base, T5-Large, and T5-3B on the DART and ToTTo dev sets without pre-training. It is important to note that for our experiments, we conduct evaluations on the dev sets rather than the test sets. This decision is made due to the constraints imposed by the ToTTo dataset, where obtaining test results requires submitting predictions to the leaderboard and awaiting the evaluation process, which can be time-consuming. Therefore, to expedite our research and streamline the experimentation process, we relied on the readily available development sets for conducting our evaluations. The results are presented in the Table 13. Notably, the transition from T5-Large to T5-3B resulted in a substantial increase in the number of parameters by approximately 3.9 times. However, the corresponding improvement in efficacy was found to be less than $1 \%$. This analysis sheds light on the limited impact of scaling up the model size beyond a certain threshold, given the marginal gains in performance despite the significant increase in parameter count.</p>
<h3>6.5 The Zero-shot Performance of ChatGPT</h3>
<p>We conducted zero-shot experiments using ChatGPT on the ToTTo and DART datasets to establish baselines for performance evaluation. The</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ToTTo</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DART</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">TER</td>
</tr>
<tr>
<td style="text-align: left;">T5-Small $+F_{\text {Graph }}$</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base $+F_{\text {Graph }}$</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large $+F_{\text {Graph }}$</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: left;">T5-3B $+F_{\text {Graph }}$</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.43</td>
</tr>
</tbody>
</table>
<p>Table 13: The performance of T5 with different model scales on the dev sets of DART and ToTTo datasets, without performing any pre-training.</p>
<h2>ToTTo</h2>
<p>Prompt: Put the highlighted-table together to form a sentence:</p>
<p>Structured Input: <page_title> List of Malayalam films of 1976 </page_title><table> <cell> Surveykkallu <col_header> Film </col_header> </cell> <cell> Thoppil Bhasi <col_header> Director </col_header> </cell></table></p>
<h2>DART</h2>
<p>Prompt: Put the triples together to form a sentence:
Structured Input: Mars Hill College: joined: 1973 I Mars Hill College: location: Mars Hill, North Carolina</p>
<p>Table 14: Input examples for ChatGPT on ToTTo and DART. Here, Prompt represents task description, and Structured Input represents data input with specific formats.
results of these experiments are presented in Table 5 and Table 7 as baselines. The prompt structure of ChatGPT comprises two parts, and detailed information regarding these prompts can be found in Table 14.</p>
<p>From the results, we observe that ChatGPT demonstrates consistent performance across various measures. For instance, in the non-overlap subset of the ToTTo dataset, when compared to BERT-to-BERT, the BLEU score shows a decrease of $17.6 \%$, while the PARENT score exhibits a slight increase of $0.9 \%$. This divergence in BLEU performance indicates that ChatGPT generates responses with different word choices, leading to reduced word overlap with the reference. However, the improvement in the PARENT score suggests enhanced structural and content-related aspects in the generated responses. These findings underscore the importance of employing multiple evaluation metrics to comprehensively assess the</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Examples of generated sentences. The main entity is highlighted in green, and the words that are not faithful to the input are in red. Important information common to both models is indicated in blue.
performance of sophisticated language generation systems in future work.</p>
<h3>6.5.1 Impact on Edge Directionality</h3>
<p>We take an examination into the significance of edge directionality and present the experimental results of incorporating the edge direction in Table 16. For UniD2T ${ }_{\text {directed }}$, we consider the input directed graph using only its original directed edges (uni-directional) and remove the reverse edges added by UniD2T. Please refer to Section 3.3 for more details about the reverse edges. From Table 16, we can observe that the incorporation of edge direction has a deleterious effect on the performance of pre-trained models. There are several possible factors that may underlie these observed outcomes. (1) First, the pre-training models aim to learn the general representations of structured data. However, due to the vast scale of multi-source data, it is often un-
feasible to assign a direction to each data pair. For example, the tabular format constitutes a fundamental type of structured data; however, the absence of explicit edge directionality is a typical characteristic between individual data pairs within this format. Therefore, we default to using bidirectional edges to signify mutual relation ships between two entities. (2) Second, we anticipate learning the coarse relationships between two entities through undirected graphs during the pretraining phase offer greater flexibility to accommodate various types of relationships in different fields. For instance, the directional link "Jay Chou $\rightarrow$ Common Jasmine Orange" conveys that Jay Chou released the album Common Jasmine Orange, while the reverse link "Common Jasmine Orange $\rightarrow$ Jay Chou" signifies that Common Jasmine Orange is one of Jay Chou's albums. In most cases, it is unnecessary to provide elaborate descriptions of specific relationships, as the data primarily requires indicating connections.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Distinct-1</th>
<th style="text-align: center;">Distinct-2</th>
<th style="text-align: center;">Distinct-3</th>
<th style="text-align: center;">Distinct-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$\mathbf{7 . 5 6}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 9 3}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 3 3}$</td>
<td style="text-align: center;">$\mathbf{3 5 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">6.94</td>
<td style="text-align: center;">13.94</td>
<td style="text-align: center;">19.00</td>
<td style="text-align: center;">23.00</td>
</tr>
<tr>
<td style="text-align: left;">UniD2T</td>
<td style="text-align: center;">6.58</td>
<td style="text-align: center;">14.72</td>
<td style="text-align: center;">21.22</td>
<td style="text-align: center;">26.38</td>
</tr>
</tbody>
</table>
<p>Table 15: The results of diversity evaluation on DART test set.</p>
<h3>6.6 Case Study</h3>
<p>As illustrated in Figure 8, we further verify the effectiveness of UniD2T qualitatively by demonstrating some generated sentences by UniD2T and T5-Large. Both UniD2T and T5-Large are capable of generating main entities. However, there are notable differences in the quality and coherence of the generated sentences. Specifically, the sentences generated by T5-Large tend to exhibit shortcomings in terms of including key information and logical reasoning. For instance, in the first case, T5-Large fails to infer that the "Baltimore World Trade Center" is the tallest building. This illustrates the limitation of T5-Large in capturing and incorporating specific facts with logical reasoning. In contrast, UniD2T can produce sentences that are more accurate, complete, and encompass the main entities and logical information with greater precision. This highlights the advantages of UniD2T in generating more contextually appropriate and logically grounded sentences.</p>
<h3>6.7 The Diversity of Generated Sentences</h3>
<p>We conduct an evaluation of the diversity exhibited in the target sentences generated by UniD2T and compare it with strong baselines (i.e., T5-Large and ChatGPT). To quantify the diversity of the generated sentences, we utilized the Distinct-N metric (Li et al., 2016), which calculates the number of distinct N-grams divided by the total number of generated tokens. The experimental results are presented in Table 15, providing insights into the diversity performance of the models. By analyzing the results, it is evident that UniD2T achieves notably higher Distinct-2/3/4 scores compared to T5-Large. This suggests that UniD2T generates sentences with a greater variety of unique unigrams and bigrams than T5-Large, indicating a higher level of linguistic diversity in the output. However, ChatGPT achieves better diversity scores than UniD2T. It tends to generate more diverse words which are not included</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Edge</th>
<th style="text-align: center;">WikiBio</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WikiTableT</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
</tr>
<tr>
<td style="text-align: center;">UniD2T</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">50.7</td>
</tr>
<tr>
<td style="text-align: center;">UniD2T $_{\text {directed }}$</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">48.3</td>
</tr>
</tbody>
</table>
<p>Table 16: The results of our models with undirected graphs (i.e., UniD2T) and directed graphs (denoted as UniD2T $_{\text {directed }}$ ), respectively.
in our vocabulary, although these words may be non-existing content.</p>
<h3>6.8 Limitations</h3>
<p>Based on our empirical observation, we reveal several limitations of this work, which can be divided into three primary categories. (1) Our pre-training data is limited, which only contains two existing pre-training datasets and six downstream datasets. In the future, we would like to collect more D2T datasets so as to construct a large-scale diverse pre-training corpus. (2) In this work, we unify different structured data into the graph format by using a simple and direct method. We will attempt to exploit more advanced strategies to construct graphs from different structured data. (3) This study focuses on modeling the graph structures and incorporating the structural information into Transformer. However, the pre-training objectives can be further improved so as to further improve the representation learning.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we proposed a unified data-to-text pre-training method, which could be applied to various downstream data-to-text generation tasks. Concretely, we first converted different types of structured data into graph format. Then, we devised a structure-enhanced Transformer to capture graph structures by introducing two new position and attention matrices to replace the position embedding and attention mask in the self-attention of the Transformer. Extensive experiments on six data-to-text benchmark datasets demonstrated that UniD2T achieved substantially better performance than strong baselines by enabling better information sharing and representation learning of data structures across diverse data-to-text datasets.</p>
<h2>Acknowledgments</h2>
<p>Min Yang was supported by National Key Research and Development Program of China (2022YFF0902100), National Natural Science Foundation of China (62376262), Shenzhen Science and Technology Innovation Program (KQTD20190929172835662), and Shenzhen Basic Research Foundation (JCYJ20210324115614039 and JCYJ20200109113441941). This work was supported by Alibaba Group through Alibaba Innovative Research Program.</p>
<h2>References</h2>
<p>Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2021. Knowledge graph based synthetic corpus generation for knowledgeenhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3554-3565. https://doi.org /10.18653/v1/2021.naacl-main. 278</p>
<p>Chenxin An, Jiangtao Feng, Kai Lv, Lingpeng Kong, Xipeng Qiu, and Xuanjing Huang. 2022. Cont: Contrastive neural text generation. arXiv preprint arXiv:2205.14690.</p>
<p>Ewa Andrejczuk, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, and Yasemin Altun. 2022. Table-to-text generation and pre-training with tabt5. arXiv preprint arXiv:2210.09162. https://doi.org/10 .18653/v1/2022.findings-emnlp. 503</p>
<p>Xuefeng Bai, Yulong Chen, and Yue Zhang. 2022. Graph pre-training for amr parsing and generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6001-6015.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72.</p>
<p>Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang,
and Luo Si. 2020. Palm: Pre-training an autoencoding\&amp;autoregressive language model for context-conditioned generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8681-8691. https://doi.org/10 .18653/v1/2020.emnlp-main. 700</p>
<p>Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning. In AAAI. https://doi.org/10.1609/aaai.v34i05 .6243</p>
<p>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In Proceedings of the 25th International Conference on Machine Learning, pages 128-135. https://doi.org/10 . 1145/1390156.1390173</p>
<p>Mingda Chen, Sam Wiseman, and Kevin Gimpel. 2021. Wikitablet: A large-scale data-to-text dataset for generating Wikipedia article sections. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 193-209. https://doi.org/10.18653 /v1/2021.findings-acl. 17</p>
<p>Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020a. Logical natural language generation from opendomain tables. arXiv preprint arXiv:2004.10404. https://doi.org/10.18653/v1/2020 .acl-main. 708</p>
<p>Wenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020b. Kgpt: Knowledgegrounded pre-training for data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8635-8648. https://doi.org/10.18653/v1/2020 .emnlp-main. 697</p>
<p>Jordan Clive, Kris Cao, and Marek Rei. 2021. Control prefixes for text generation. arXiv preprint arXiv:2110.08329.</p>
<p>Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William W. Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In ACL. https://doi.org/10.18653/v1 /P19-1483</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13042-13054.</p>
<p>Ondřej Dušek and Filip Jurčíček. 2016. Sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings. arXiv preprint arXiv:1606.05491. https://doi.org/10 .18653/v1/P16-2008</p>
<p>Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG challenge. Computer Speech \&amp; Language, 59:123-156. https://doi.org/10 .1016/j.csl.2019.06.009</p>
<p>Shineng Fang, Jiangjie Chen, Xinyao Shen, Yunwen Chen, and Yanghua Xiao. 2022a. A faithful contrastive framework for response generation in tableqa systems. In International Conference on Database Systems for Advanced Applications, pages 197-212. Springer. https://doi.org/10.1007/978-3-031 -00129-1_13</p>
<p>Shineng Fang, Jiangjie Chen, Xinyao Shen, Yunwen Chen, and Yanghua Xiao. 2022b. Falcon: A faithful contrastive framework for response generation in tableqa systems. In International Conference on Database Systems for Advanced Applications, pages 197-212. Springer. https://doi.org/10.1007/978 -3-031-00129-1_13</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for nlg microplanning. In 55th Annual Meeting of the Association for Computational Linguistics (ACL). https://doi.org/10.18653/v1/P17 -1017</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for</p>
<p>Computational Linguistics, 7. https://doi .org/10.1162/tacl_a_00269</p>
<p>Jiuzhou Han and Ehsan Shareghi. 2022. Selfsupervised graph masking pre-training for graph-to-text generation. In Empirical Methods in Natural Language Processing 2022, pages 4845-4853. Association for Computational Linguistics (ACL). https://doi.org /10.18653/v1/2022.emnlp-main. 321</p>
<p>Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Mueller, Francesco Piccinno, and Julian Eisenschlos. 2020. Tapas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4320-4333. https://doi.org/10 .18653/v1/2020.acl-main. 398</p>
<p>Mihir Kale and Abhinav Rastogi. 2020. Text-to-text pre-training for data-to-text tasks. In Proceedings of the 13th International Conference on Natural Language Generation, pages 97-102, Dublin, Ireland. Association for Computational Linguistics. https://doi.org /10.18653/v1/2020.inlg-1.14</p>
<p>Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, and Minlie Huang. 2021. Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2526-2538. https://doi.org/10 .18653/v1/2021.findings-acl. 223</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171-4186.</p>
<p>Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and</p>
<p>Radu Soricut. 2019. Albert: A lite bert for selfsupervised learning of language representations. In International Conference on Learning Representations.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213. https://doi .org/10.18653/v1/D16-1128</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 110-119. The Association for Computational Linguistics.</p>
<p>Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. 2023a. CATS: A pragmatic Chinese answer-to-sequence dataset with large scale and high quality. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2983-3000, Toronto, Canada. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2023.acl-long. 168</p>
<p>Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. 2023b. Cats: A pragmatic chinese answer-to-sequence dataset with large scale and high quality. arXiv preprint arXiv:2306.11477.</p>
<p>Liang Li, Ruiying Geng, Bowen Li, Can Ma, Yinliang Yue, Binhua Li, and Yongbin Li. 2022. Graph-to-text generation with dynamic structure pruning. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6115-6127.</p>
<p>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP, pages 91-99. https://doi.org /10.3115/1687878.1687893</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81.</p>
<p>Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, and Dongmei Zhang. 2022. Plog: Table-to-logic pretraining for logical table-to-text generation. arXiv preprint arXiv:2205.12697. https:// doi.org/10.18653/v1/2022.emnlp-main . 373</p>
<p>Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang Sui. 2018. Table-to-text generation by structure-aware seq2seq learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 4881-4888. AAAI Press. https://doi .org/10.1609/aaai.v32i1.11925</p>
<p>Sanket Vaibhav Mehta, Jinfeng Rao, Yi Tay, Mihir Kale, Ankur Parikh, and Emma Strubell. 2022. Improving compositional generalization with self-training for data-to-text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4205-4219.</p>
<p>Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani, 2020. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871.</p>
<p>Toru Nishino, Ryota Ozaki, Yohei Momoki, Tomoki Taniguchi, Ryuji Kano, Norihisa Nakano, Yuki Tagawa, Motoki Taniguchi, Tomoko Ohkuma, and Keigo Nakamura. 2020. Reinforcement learning with imbalanced dataset for data-to-text medical report generation. In</p>
<p>Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2223-2236. https://doi.org/10.18653/v1/2020 .findings-emnlp. 202</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318. https://doi.org /10.3115/1073083.1073135</p>
<p>Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. arXiv preprint arXiv:2004.14373. https:// doi.org/10.18653/v1/2020.emnlp-main. 89</p>
<p>Martin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, Iryna Gurevych, and Hinrich Schütze. 2021. Modeling graph structure via relative position for text generation from knowledge graphs. NAACL-HLT 2021, page 10. https:// doi.org/10.18653/v1/2021.textgraphs-1.2</p>
<p>Maja Popović. 2015. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395. https:// doi.org/10.18653/v1/W15-3049</p>
<p>Ratish Puduppully, Yao Fu, and Mirella Lapata. 2022. Data-to-text generation with variational sequential planning. Transactions of the Association for Computational Linguistics, 10:697-715. https://doi.org/10.1162 /tacl_a_00484</p>
<p>Alec Radford and Karthik Narasimhan. 2018. Improving Language Understanding by Generative Pre-Training. https://api .semanticscholar.org/CorpusID:49313245</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Clément Rebuffel, Laure Soulier, Geoffrey Scoutheeten, and Patrick Gallinari. 2020. A hierarchical model for data-to-text generation. In European Conference on Information Re-
trieval, pages 65-80. Springer. https://doi .org/10.1007/978-3-030-45439-5_5</p>
<p>Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. Natural Language Engineering, 3(1):57-87. https://doi.org/10.1017 /S1351324997001502</p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv:2007 .08426. https://doi.org/10.18653/v1 /2021.nlp4convai-1.20</p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2021a. Investigating pretrained language models for graph-to-text generation. In Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, pages 211-227. https://doi.org/10.18653/v1/2021 .nlp4convai-1.20</p>
<p>Leonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021b. Structural adapters in pretrained language models for amr-to-text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4269-4282. https://doi .org/10.18653/v1/2021.emnlp-main. 351</p>
<p>Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8:264-280. https://doi.org /10.1162/tacl_a_00313</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. Bleurt: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696. https://doi.org/10 .18653/v1/2020.acl-main. 704</p>
<p>Yunzhou Shi, Zhiling Luo, Pengcheng Zhu, Feng Ji, Wei Zhou, Haiqing Chen, and Yujiu Yang. 2020. G2t: Generating fluent descriptions for knowledge graph. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1861-1864. https://doi .org/10.1145/3397271.3401289</p>
<p>Mathew Snover, Bonnie Dorr, Richard Schwartz, John Makhoul, Linnea Micciulla, and Ralph Weischedel. 2005. A study of translation error rate with targeted human annotation, Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for amr-to-text generation. In ACL. https://doi.org/10.18653/v1/P18 -1150</p>
<p>Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, and Nigel Collier. 2021. Plan-thengenerate: Controlled data-to-text generation via planning. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 895-909.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104-3112.</p>
<p>Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Mvp: Multi-task supervised pre-training for natural language generation. arXiv preprint arXiv:2206.12131. https://doi.org/10.18653/v1/2023 .findings-acl. 558</p>
<p>Fei Wang, Zhewei Xu, Pedro Szekely, and Muhao Chen. 2022. Robust (controlled) table-to-text generation with structure-aware equivariance learning. arXiv preprint arXiv:2205.03972. https://doi.org/10.18653/v1/2022 .naacl-main. 371</p>
<p>Peng Wang, Junyang Lin, An Yang, Chang Zhou, Yichang Zhang, Jingren Zhou, and Hongxia Yang. 2021. Sketch and refine: Towards faithful and informative table-to-text generation. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 4831-4843. Association for Computa-
tional Linguistics. https://doi.org/10 .18653/v1/2021.findings-acl. 427</p>
<p>Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. Amr-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics, 8:19-33. https:// doi.org/10.1162/tacl_a_00297</p>
<p>Xinyu Xing and Xiaojun Wan. 2021. Structureaware pre-training for table-to-text generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2273-2278. https://doi.org/10 .18653/v1/2021.findings-acl. 200</p>
<p>Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. 2019. CoSQL: A conversational text-to-SQL challenge towards cross-domain natural language interfaces to databases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1962-1979, Hong Kong, China. Association for Computational Linguistics. https://doi.org/10 .18653/v1/D19-1204</p>
<p>Giulio Zhou and Gerasimos Lampouras. 2020. WebNLG challenge 2020: Language agnostic delexicalisation for multilingual RDF-to-text generation. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 186-191, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better amr-to-text generation. In EMNLP-IJCNLP. https://doi.org/10.18653/v1/D19 -1548</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work done during an internship at Alibaba DAMO Academy.
${ }^{\dagger}$ Corresponding authors.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-687 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-687</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-687</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-1d57d10fb1e51c0130795cafbb511a7a9c196a3a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1d57d10fb1e51c0130795cafbb511a7a9c196a3a" target="_blank">Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis</a></p>
                <p><strong>Paper Venue:</strong> 2023 IEEE 34th International Symposium on Software Reliability Engineering Workshops (ISSREW)</p>
                <p><strong>Paper TL;DR:</strong> A discriminative feature set is developed that yields high accuracy in distinguishing ChatGPT-generated code from human-authored code in binary classification tasks and a dataset cleansing strategy is introduced that extracts pristine, high-quality code datasets from open-source repositories, thereby achieving exceptional accuracy in code authorship attribution tasks.</p>
                <p><strong>Paper Abstract:</strong> The ubiquitous adoption of Large Language Generation Models (LLMs) in programming has highlighted the importance of distinguishing between human-written code and code generated by intelligent models. This paper specifically aims to distinguish ChatGPT-generated code from human-generated code. Our investigation reveals differences in programming style, technical level and readability between these two sources. Consequently, we develop a discriminative feature set for differentiation and evaluate its effectiveness through ablation experiments. In addition, we develop a dataset cleaning technique using temporal and spatial segmentation to mitigate dataset scarcity and ensure high quality, uncontaminated datasets. To further enrich the data resources, we apply "code transformation", "feature transformation" and "feature adaptation" techniques, generating a rich dataset of 100,000 lines of ChatGPT-generated code. The main contributions of our research include: proposing a discriminative feature set that yields high accuracy in distinguishing ChatGPT-generated code from human-authored code in binary classification tasks; devising methods for generating rich ChatGPT-generated code; and introducing a dataset cleansing strategy that extracts pristine, high-quality code datasets from open-source repositories, thereby achieving exceptional accuracy in code authorship attribution tasks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e687.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e687.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Authorship Contamination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset Authorship Contamination and Noise</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch between claimed single-author code in repository metadata / descriptions and the actual presence of externally copied, forked, or collaboratively authored code which can distort analysis of individual coding style.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human-code dataset harvesting and cleansing pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline used to collect human-authored code from GitHub and to clean it via initial filtering, temporal segmentation, and spatial segmentation to obtain author-specific, time-bounded code samples.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>repository metadata / README / author attribution</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>open-source repository code (multi-file projects)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification of authorship / external code inclusion</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Repository-level natural language metadata (author name, README, repo description) and naive harvesting imply single-author ownership, but many repositories contain forks, organizational accounts, third-party libraries, or code copied from other sources; this creates a mismatch between natural language claims of authorship and the actual provenance of code files used for authorship attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data collection / preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>metadata analysis (filtering by account type), removal of forks and duplicates, contributor-count analysis to detect multi-contributor repositories, and removal of repos containing common third‑party libraries (spatial segmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>categorical filtering and segmentation: repos were classified by creation year (2008–2022) and the authors focused on 2021–2022; removed organization accounts, forked repositories and duplicates; compiled a list of common third-party libraries for C++/Java to filter out dependency-heavy repos. Final purified dataset reported as ≈20GB of code.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Unclean datasets can obscure individual coding styles and bias authorship attribution models; after applying cleansing, the authors report improved performance when datasets are used for attribution tasks (they claim subsequent application yielded >=95% accuracy on attribution tasks), indicating substantial impact on model performance and validity.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified precisely in the paper (no percent of repos removed reported); authors describe the issue as pervasive enough to require extensive cleansing and temporal/spatial segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>collaborative development practices, forks and mirrors, inclusion of third-party libraries, and superficial reliance on repository metadata and README claims for author attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Initial cleanup to remove org accounts, forks, and duplicates; temporal segmentation by repository creation year to capture stable style windows; spatial segmentation to remove repos with multiple contributors or containing third-party libraries (using contributors metadata and library name heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported as effective: authors state they 'amassed and purified approximately 20GB of code datasets' and that using these datasets in attribution tasks yielded no less than 95% accuracy; the paper does not provide ablation numbers showing accuracy before/after cleansing.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>code authorship attribution / software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e687.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e687.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training-data Replication</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-data Contamination and Replication in LLM-generated Code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instances where code produced by ChatGPT appears to reproduce or closely mirror code present in its (opaque) training data, causing a misalignment between claims of generated novelty and actual provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT code-generation study (GPT-3.5 and GPT-4) using three generation strategies</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generation experiments where ChatGPT produced code using 'code translation', 'functional translation', and 'functional customization' to build a ChatGPT code dataset for comparison with human code.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>programming problem specification / functional description / input code snippets (prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>ChatGPT-generated code (responses to prompts, single-file code outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>training-data replication / incomplete novelty / provenance mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language problem descriptions or prompts that correspond to problems present in the model's training data can lead ChatGPT to output code that replicates or closely resembles existing human-authored solutions, producing a divergence between the assumed 'freshly generated' code and code that is effectively memorized or restructured from training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model generation / provenance of generated artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>comparative analysis between ChatGPT outputs and human-authored solutions for the same tasks; semantic similarity inspection and exact-match checks across 100 programming tasks; runtime testing of executability and correctness on testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counts and rates: exact replication observed in 2 out of 100 tasks; executability/correctness measured for pre-cutoff tasks (100% executability and 100% correctness reported) and post-cutoff tasks (90.9% executability; initial 41% correctness rising to 63.63% after iterative refinement). Also measured performance comparisons (e.g., temporal efficiency improvements in a given percent of cases).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can inflate apparent performance (high correctness/executability on tasks present in training data), create intellectual property concerns, reduce perceived originality of generated code, and bias classifiers trained on such data; empirically, ChatGPT achieved perfect results on pre-cutoff tasks suggesting strong memorization effects that would misrepresent true generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Exact verbatim replication rare in the tested sample (2%), but semantic similarity to human solutions was common across the 100-task sample (majority of cases).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Opaque and large-scale training corpora that likely include online solutions, combined with model capacity for memorization and rephrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Experiment strategies intended to reduce overlap with training data: code translation (generate equivalent code in a different language), functional translation (generate from an abstracted description), and functional customization (generate from competition/textbook problem descriptions); select tasks outside model knowledge where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Intended to increase novelty, but the paper provides no quantitative comparison proving these strategies fully prevent replication; authors report code translation is 'relatively novel' but may still mimic logic/wording of originals.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>LLM code generation / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e687.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e687.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Dependence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt and Generation-Setting Dependence Affecting Feature-based Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variability in natural-language prompts or generation instructions (including omission of prompt details) changes properties of generated code and can invalidate feature-based detection or analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT generation experiment (unconstrained prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ChatGPT code samples were collected without specialized prompts; the study's feature extraction and classification are based on these unconstrained outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>generation prompt / instruction</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>ChatGPT-generated code (default conversational outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/omitted generation-specification leading to feature-distribution shift</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper notes that all ChatGPT-generated code samples were produced without specific prompts or instructions; using particular prompts (e.g., 'no comments', or 'follow production coding style') could systematically alter lexical, structural, and semantic features used for discrimination, causing a mismatch between the studied natural-language conditions and other plausible generation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data generation / feature extraction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>recognized as a methodological limitation (authors' self-report); potential detection would require controlled prompt-variation experiments and ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not measured in this study; authors explicitly state they did not vary prompts and therefore cannot quantify the effect.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Threatens the external validity of the discriminative feature set — classifiers trained on unconstrained outputs may perform differently if code is generated under targeted prompts; magnitude unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Omission of prompt metadata and generation parameters when collecting ChatGPT outputs; high sensitivity of LLM outputs to instruction phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors recommend future work to analyze prompt effects and collect more diverse ChatGPT samples generated with varied, documented prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>code authorship attribution / LLM experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e687.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e687.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inadequacy of Traditional Code-Authorship Features for AI-vs-Human Discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional feature sets designed to distinguish among human authors may not capture the stylistic and semantic differences between human-authored and LLM-generated code, necessitating new feature design and ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Discriminative feature extraction and classification pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Feature engineering pipeline producing lexical, structural/layout, and semantic features (22+ structural features plus lexical segmentation and runtime-based semantic tests) used as input to classifiers (Random Forest, SMO, Simple Logistic, J48) for binary origin classification.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>prior literature descriptions of features / conventional code-authorship feature sets</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>feature-extraction code and classification scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / mismatched feature assumptions (traditional features insufficient)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The literature's standard feature sets (vocabulary/layout/syntax focused on distinguishing human programmers) do not capture characteristic differences introduced by LLM-generated code (e.g., consistent formatting, lack of commented-out developmental artifacts, different choice of language features). This is a misalignment between expectations from prior natural-language descriptions of 'useful features' and their effectiveness when code origin includes AI.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>feature engineering / model input</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Ablation studies comparing lexical-only, layout-only, and combined feature sets across classifiers; exploratory semantic analyses including runnability, correctness, and time/space performance tests.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Standard classification metrics (Accuracy, Precision, Recall, F1) reported in Tables 1 and 2; examples: Java combined features achieved up to 0.978 accuracy (J48), lexical-only and layout-only had lower accuracies; C++ combined features achieved up to 0.930 accuracy (Random Forest).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using traditional features alone would reduce classification accuracy; the tailored discriminative set produced substantially higher accuracy (>90%), demonstrating a concrete impact on detection performance.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across languages studied (C++ and Java) in this work; general prevalence in literature not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Prior feature sets were designed for human-authorship variability and do not target systematic artifacts introduced by LLM generation; missing semantic/run-time features in many prior approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Heuristic code segmentation and construction of a new discriminative feature set consisting of lexical categories, 22 structural/layout features, and semantic runnability/correctness/performance tests plus ablation to validate contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective within this study: combined feature sets yield highest classifier metrics (examples provided in Tables 1 and 2, and overall claimed task accuracy >90%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning for code / code authorship attribution</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e687.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e687.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inter-file Coupling Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch in Inter-file Dependencies and Production Readiness between Human and LLM-generated Code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences in project-level organization and development artifacts (inter-file dependencies, commented-out development remnants, logging vs console output) create misalignment between natural-language descriptions of production software and single-file, template-like LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Comparative code characteristic analysis</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Analysis of tokens, structural/layout features, and project organization comparing multi-file human repositories to ChatGPT single-file outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>project README / release notes / implied production intent</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>multi-file repository code (human) vs single-file generated code (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different code-organization assumptions / incomplete specification of system-level context</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions or specifications that imply multi-file, coupled systems or production-ready code do not align with ChatGPT outputs, which commonly are single-file, minimally coupled, lack commented-out development artifacts, and use console outputs instead of production logging; this causes a mismatch between expected system-level code structure and generated implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>project organization / implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Structural and token-frequency analysis (comment ratios, inter-file dependency observations, library usage comparisons) and manual qualitative inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Observational comparative statistics (e.g., comment frequency, library usage differences) presented in figures; no direct numeric prevalence of multi-file absence provided, but authors list this as a consistent characteristic.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Affects feature distributions used by classifiers and could mislead tools or analyses that assume multi-file dependencies or production-ready structure; may limit the use of generated code for realistic system-level evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as typical of ChatGPT-generated code in authors' dataset (qualitative statement; not given as a precise proportion).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>LLM output constraints and prompt framing that elicit self-contained single-file solutions; human projects evolve with multiple files and development artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Not evaluated in this study; implied approaches include generating multi-file code via more detailed prompts or augmenting generated code with synthetic dependency scaffolding for parity during analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated / no quantitative data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation <em>(Rating: 2)</em></li>
                <li>How Secure is Code Generated by ChatGPT? <em>(Rating: 2)</em></li>
                <li>De-anonymizing programmers via code stylometry <em>(Rating: 2)</em></li>
                <li>Large-Scale and Language-Oblivious Code Authorship Identification <em>(Rating: 2)</em></li>
                <li>Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-687",
    "paper_id": "paper-1d57d10fb1e51c0130795cafbb511a7a9c196a3a",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Authorship Contamination",
            "name_full": "Dataset Authorship Contamination and Noise",
            "brief_description": "Mismatch between claimed single-author code in repository metadata / descriptions and the actual presence of externally copied, forked, or collaboratively authored code which can distort analysis of individual coding style.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Human-code dataset harvesting and cleansing pipeline",
            "system_description": "Pipeline used to collect human-authored code from GitHub and to clean it via initial filtering, temporal segmentation, and spatial segmentation to obtain author-specific, time-bounded code samples.",
            "nl_description_type": "repository metadata / README / author attribution",
            "code_implementation_type": "open-source repository code (multi-file projects)",
            "gap_type": "incomplete specification of authorship / external code inclusion",
            "gap_description": "Repository-level natural language metadata (author name, README, repo description) and naive harvesting imply single-author ownership, but many repositories contain forks, organizational accounts, third-party libraries, or code copied from other sources; this creates a mismatch between natural language claims of authorship and the actual provenance of code files used for authorship attribution.",
            "gap_location": "data collection / preprocessing",
            "detection_method": "metadata analysis (filtering by account type), removal of forks and duplicates, contributor-count analysis to detect multi-contributor repositories, and removal of repos containing common third‑party libraries (spatial segmentation).",
            "measurement_method": "categorical filtering and segmentation: repos were classified by creation year (2008–2022) and the authors focused on 2021–2022; removed organization accounts, forked repositories and duplicates; compiled a list of common third-party libraries for C++/Java to filter out dependency-heavy repos. Final purified dataset reported as ≈20GB of code.",
            "impact_on_results": "Unclean datasets can obscure individual coding styles and bias authorship attribution models; after applying cleansing, the authors report improved performance when datasets are used for attribution tasks (they claim subsequent application yielded &gt;=95% accuracy on attribution tasks), indicating substantial impact on model performance and validity.",
            "frequency_or_prevalence": "Not quantified precisely in the paper (no percent of repos removed reported); authors describe the issue as pervasive enough to require extensive cleansing and temporal/spatial segmentation.",
            "root_cause": "collaborative development practices, forks and mirrors, inclusion of third-party libraries, and superficial reliance on repository metadata and README claims for author attribution.",
            "mitigation_approach": "Initial cleanup to remove org accounts, forks, and duplicates; temporal segmentation by repository creation year to capture stable style windows; spatial segmentation to remove repos with multiple contributors or containing third-party libraries (using contributors metadata and library name heuristics).",
            "mitigation_effectiveness": "Reported as effective: authors state they 'amassed and purified approximately 20GB of code datasets' and that using these datasets in attribution tasks yielded no less than 95% accuracy; the paper does not provide ablation numbers showing accuracy before/after cleansing.",
            "domain_or_field": "code authorship attribution / software engineering",
            "reproducibility_impact": true,
            "uuid": "e687.0",
            "source_info": {
                "paper_title": "Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Training-data Replication",
            "name_full": "Training-data Contamination and Replication in LLM-generated Code",
            "brief_description": "Instances where code produced by ChatGPT appears to reproduce or closely mirror code present in its (opaque) training data, causing a misalignment between claims of generated novelty and actual provenance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ChatGPT code-generation study (GPT-3.5 and GPT-4) using three generation strategies",
            "system_description": "Generation experiments where ChatGPT produced code using 'code translation', 'functional translation', and 'functional customization' to build a ChatGPT code dataset for comparison with human code.",
            "nl_description_type": "programming problem specification / functional description / input code snippets (prompts)",
            "code_implementation_type": "ChatGPT-generated code (responses to prompts, single-file code outputs)",
            "gap_type": "training-data replication / incomplete novelty / provenance mismatch",
            "gap_description": "Natural-language problem descriptions or prompts that correspond to problems present in the model's training data can lead ChatGPT to output code that replicates or closely resembles existing human-authored solutions, producing a divergence between the assumed 'freshly generated' code and code that is effectively memorized or restructured from training examples.",
            "gap_location": "model generation / provenance of generated artifacts",
            "detection_method": "comparative analysis between ChatGPT outputs and human-authored solutions for the same tasks; semantic similarity inspection and exact-match checks across 100 programming tasks; runtime testing of executability and correctness on testcases.",
            "measurement_method": "Counts and rates: exact replication observed in 2 out of 100 tasks; executability/correctness measured for pre-cutoff tasks (100% executability and 100% correctness reported) and post-cutoff tasks (90.9% executability; initial 41% correctness rising to 63.63% after iterative refinement). Also measured performance comparisons (e.g., temporal efficiency improvements in a given percent of cases).",
            "impact_on_results": "Can inflate apparent performance (high correctness/executability on tasks present in training data), create intellectual property concerns, reduce perceived originality of generated code, and bias classifiers trained on such data; empirically, ChatGPT achieved perfect results on pre-cutoff tasks suggesting strong memorization effects that would misrepresent true generalization.",
            "frequency_or_prevalence": "Exact verbatim replication rare in the tested sample (2%), but semantic similarity to human solutions was common across the 100-task sample (majority of cases).",
            "root_cause": "Opaque and large-scale training corpora that likely include online solutions, combined with model capacity for memorization and rephrasing.",
            "mitigation_approach": "Experiment strategies intended to reduce overlap with training data: code translation (generate equivalent code in a different language), functional translation (generate from an abstracted description), and functional customization (generate from competition/textbook problem descriptions); select tasks outside model knowledge where possible.",
            "mitigation_effectiveness": "Intended to increase novelty, but the paper provides no quantitative comparison proving these strategies fully prevent replication; authors report code translation is 'relatively novel' but may still mimic logic/wording of originals.",
            "domain_or_field": "LLM code generation / machine learning",
            "reproducibility_impact": true,
            "uuid": "e687.1",
            "source_info": {
                "paper_title": "Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Prompt Dependence",
            "name_full": "Prompt and Generation-Setting Dependence Affecting Feature-based Detection",
            "brief_description": "Variability in natural-language prompts or generation instructions (including omission of prompt details) changes properties of generated code and can invalidate feature-based detection or analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ChatGPT generation experiment (unconstrained prompting)",
            "system_description": "ChatGPT code samples were collected without specialized prompts; the study's feature extraction and classification are based on these unconstrained outputs.",
            "nl_description_type": "generation prompt / instruction",
            "code_implementation_type": "ChatGPT-generated code (default conversational outputs)",
            "gap_type": "ambiguous/omitted generation-specification leading to feature-distribution shift",
            "gap_description": "The paper notes that all ChatGPT-generated code samples were produced without specific prompts or instructions; using particular prompts (e.g., 'no comments', or 'follow production coding style') could systematically alter lexical, structural, and semantic features used for discrimination, causing a mismatch between the studied natural-language conditions and other plausible generation settings.",
            "gap_location": "data generation / feature extraction",
            "detection_method": "recognized as a methodological limitation (authors' self-report); potential detection would require controlled prompt-variation experiments and ablation.",
            "measurement_method": "Not measured in this study; authors explicitly state they did not vary prompts and therefore cannot quantify the effect.",
            "impact_on_results": "Threatens the external validity of the discriminative feature set — classifiers trained on unconstrained outputs may perform differently if code is generated under targeted prompts; magnitude unknown.",
            "frequency_or_prevalence": "Not evaluated in this paper.",
            "root_cause": "Omission of prompt metadata and generation parameters when collecting ChatGPT outputs; high sensitivity of LLM outputs to instruction phrasing.",
            "mitigation_approach": "Authors recommend future work to analyze prompt effects and collect more diverse ChatGPT samples generated with varied, documented prompts.",
            "mitigation_effectiveness": "Not evaluated in this paper.",
            "domain_or_field": "code authorship attribution / LLM experiments",
            "reproducibility_impact": true,
            "uuid": "e687.2",
            "source_info": {
                "paper_title": "Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Feature Mismatch",
            "name_full": "Inadequacy of Traditional Code-Authorship Features for AI-vs-Human Discrimination",
            "brief_description": "Traditional feature sets designed to distinguish among human authors may not capture the stylistic and semantic differences between human-authored and LLM-generated code, necessitating new feature design and ablation studies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Discriminative feature extraction and classification pipeline",
            "system_description": "Feature engineering pipeline producing lexical, structural/layout, and semantic features (22+ structural features plus lexical segmentation and runtime-based semantic tests) used as input to classifiers (Random Forest, SMO, Simple Logistic, J48) for binary origin classification.",
            "nl_description_type": "prior literature descriptions of features / conventional code-authorship feature sets",
            "code_implementation_type": "feature-extraction code and classification scripts",
            "gap_type": "incomplete specification / mismatched feature assumptions (traditional features insufficient)",
            "gap_description": "The literature's standard feature sets (vocabulary/layout/syntax focused on distinguishing human programmers) do not capture characteristic differences introduced by LLM-generated code (e.g., consistent formatting, lack of commented-out developmental artifacts, different choice of language features). This is a misalignment between expectations from prior natural-language descriptions of 'useful features' and their effectiveness when code origin includes AI.",
            "gap_location": "feature engineering / model input",
            "detection_method": "Ablation studies comparing lexical-only, layout-only, and combined feature sets across classifiers; exploratory semantic analyses including runnability, correctness, and time/space performance tests.",
            "measurement_method": "Standard classification metrics (Accuracy, Precision, Recall, F1) reported in Tables 1 and 2; examples: Java combined features achieved up to 0.978 accuracy (J48), lexical-only and layout-only had lower accuracies; C++ combined features achieved up to 0.930 accuracy (Random Forest).",
            "impact_on_results": "Using traditional features alone would reduce classification accuracy; the tailored discriminative set produced substantially higher accuracy (&gt;90%), demonstrating a concrete impact on detection performance.",
            "frequency_or_prevalence": "Observed across languages studied (C++ and Java) in this work; general prevalence in literature not quantified.",
            "root_cause": "Prior feature sets were designed for human-authorship variability and do not target systematic artifacts introduced by LLM generation; missing semantic/run-time features in many prior approaches.",
            "mitigation_approach": "Heuristic code segmentation and construction of a new discriminative feature set consisting of lexical categories, 22 structural/layout features, and semantic runnability/correctness/performance tests plus ablation to validate contributions.",
            "mitigation_effectiveness": "Effective within this study: combined feature sets yield highest classifier metrics (examples provided in Tables 1 and 2, and overall claimed task accuracy &gt;90%).",
            "domain_or_field": "machine learning for code / code authorship attribution",
            "reproducibility_impact": true,
            "uuid": "e687.3",
            "source_info": {
                "paper_title": "Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Inter-file Coupling Mismatch",
            "name_full": "Mismatch in Inter-file Dependencies and Production Readiness between Human and LLM-generated Code",
            "brief_description": "Differences in project-level organization and development artifacts (inter-file dependencies, commented-out development remnants, logging vs console output) create misalignment between natural-language descriptions of production software and single-file, template-like LLM outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Comparative code characteristic analysis",
            "system_description": "Analysis of tokens, structural/layout features, and project organization comparing multi-file human repositories to ChatGPT single-file outputs.",
            "nl_description_type": "project README / release notes / implied production intent",
            "code_implementation_type": "multi-file repository code (human) vs single-file generated code (ChatGPT)",
            "gap_type": "different code-organization assumptions / incomplete specification of system-level context",
            "gap_description": "Natural-language descriptions or specifications that imply multi-file, coupled systems or production-ready code do not align with ChatGPT outputs, which commonly are single-file, minimally coupled, lack commented-out development artifacts, and use console outputs instead of production logging; this causes a mismatch between expected system-level code structure and generated implementations.",
            "gap_location": "project organization / implementation details",
            "detection_method": "Structural and token-frequency analysis (comment ratios, inter-file dependency observations, library usage comparisons) and manual qualitative inspection.",
            "measurement_method": "Observational comparative statistics (e.g., comment frequency, library usage differences) presented in figures; no direct numeric prevalence of multi-file absence provided, but authors list this as a consistent characteristic.",
            "impact_on_results": "Affects feature distributions used by classifiers and could mislead tools or analyses that assume multi-file dependencies or production-ready structure; may limit the use of generated code for realistic system-level evaluations.",
            "frequency_or_prevalence": "Reported as typical of ChatGPT-generated code in authors' dataset (qualitative statement; not given as a precise proportion).",
            "root_cause": "LLM output constraints and prompt framing that elicit self-contained single-file solutions; human projects evolve with multiple files and development artifacts.",
            "mitigation_approach": "Not evaluated in this study; implied approaches include generating multi-file code via more detailed prompts or augmenting generated code with synthetic dependency scaffolding for parity during analysis.",
            "mitigation_effectiveness": "Not evaluated / no quantitative data provided.",
            "domain_or_field": "software engineering / code generation",
            "reproducibility_impact": null,
            "uuid": "e687.4",
            "source_info": {
                "paper_title": "Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
            "rating": 2
        },
        {
            "paper_title": "How Secure is Code Generated by ChatGPT?",
            "rating": 2
        },
        {
            "paper_title": "De-anonymizing programmers via code stylometry",
            "rating": 2
        },
        {
            "paper_title": "Large-Scale and Language-Oblivious Code Authorship Identification",
            "rating": 2
        },
        {
            "paper_title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT",
            "rating": 1
        }
    ],
    "cost": 0.01657025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis</h1>
<p>Ke Li ${ }^{\star}$<br>Sheng Hong*<br>luck_ke@hust.edu.cn<br>hongsheng@hust.edu.cn<br>School of Cyber Science and Engineering, Huazhong<br>University of Science and Technology, Wuhan, Hubei, China<br>Wuhan, Hubei, China<br>Yunhe Zhang<br>yhzhang@hust.edu.cn<br>School of Cyber Science and Engineering, Huazhong<br>University of Science and Technology, Wuhan, Hubei, China<br>Wuhan, Hubei, China</p>
<h2>ABSTRACT</h2>
<p>The ubiquitous adoption of Large Language Generation Models (LLMs) in programming has underscored the importance of differentiating between human-written code and code generated by intelligent models. This paper specifically aims to distinguish code generated by ChatGPT from that authored by humans. Our investigation reveals disparities in programming style, technical level, and readability between these two sources. Consequently, we develop a discriminative feature set for differentiation and evaluate its efficacy through ablation experiments. Additionally, we devise a dataset cleansing technique, which employs temporal and spatial segmentation, to mitigate the dearth of datasets and to secure high-caliber, uncontaminated datasets. To further enrich data resources, we employ "code transformation," "feature transformation," and "feature customization" techniques, generating an extensive dataset comprising 10,000 lines of ChatGPT-generated code. The salient contributions of our research include: proposing a discriminative feature set yielding high accuracy in differentiating ChatGPT-generated code from human-authored code in binary classification tasks; devising methods for generating extensive ChatGPT-generated codes; and introducing a dataset cleansing strategy that extracts immaculate, high-grade code datasets from open-source repositories, thus achieving exceptional accuracy in code authorship attribution tasks.</p>
<h2>KEYWORDS</h2>
<p>ChatGPT, Code Differentiation, Dataset Cleansing, Machine Learning</p>
<h2>1 INTRODUCTION</h2>
<p>Since its introduction in November 2022, OpenAI's ChatGPT has become the cynosure in numerous fields, generating palpable enthusiasm. By capitalizing on human feedback reinforcement learning (RLHF) for fine-tuning and judiciously curated datasets, ChatGPT</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Cai Fu</h2>
<p>fucai@hust.edu.cn
School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China
Wuhan, Hubei, China</p>
<h2>Ming Liu <br> liuming@hust.edu.cn</h2>
<p>School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China
Wuhan, Hubei, China
exhibits exemplary capabilities across a plethora of challenging natural language processing (NLP) tasks. These include code synthesis via natural language[4], text summarization[6], and the creation of stylistic narratives based on designated elements[15], in addition to its adeptness in conventional NLP tasks such as translation and text categorization. Moreover, ChatGPT demonstrates the prudence of refraining from responding to inquiries that exceed its knowledge base, contravene ethical norms, or broach sensitive political topics.</p>
<p>ChatGPT's prowess, particularly in facilitating programming through natural language, has engendered significant intrigue. However, reservations concerning the safety and legality of employing intelligent programming assistants like ChatGPT have been voiced by the scholarly community. The opaque nature of ChatGPT's training data spawns uncertainties regarding the provenance of its generated code snippets and the possibility of their harboring security vulnerabilities or unsafe code fragments[8, 10], which could entangle developers in copyright disputes and code security quandaries.</p>
<p>For example, Stack Overflow, a renowned platform for programming inquiries, has imposed a temporary restraining on ChatGPTgenerated content due to its low accuracy which did not match the high quality demanded by users. Concurrently, academic institutions face dilemmas in assessment settings where students might exploit ChatGPT to accomplish programming assignments, thus creating hurdles in evaluating their true acumen and knowledge.</p>
<p>Addressing these issues hinges on the adept discernment of human-written code versus code generated by intelligent models like ChatGPT. This entails two facets: first, the extraction of discriminative features between human-authored and ChatGPT-synthesized code. Conventional feature sets for code author attribution, which are primarily geared toward distinguishing among human authors, may fall short in capturing subtle distinctions, such as code structuring traits, when discerning between ChatGPT and human-authored code. This necessitates ablation studies to pinpoint a discriminative feature set capable of enhancing classification accuracy.</p>
<p>Through ablation studies, we identified variances in programming style, technical level, and readability between ChatGPT-generated and human-authored code. Building upon existing research in code authorship attribution, we formulate a discriminative feature set that enables distinction between these code sources, thereby surmounting feature analysis challenges.</p>
<p>The second facet concerns the compilation of an associated highquality dataset. Our examination ascertained that prevailing research on code authorship attribution relies on datasets of heterogeneous quality and sources, bereft of standardized benchmarks. Moreover, datasets specifically comprising ChatGPT-generated code are scant owing to its recent advent.</p>
<p>To redress this dataset deficiency, we conceive a dataset cleansing technique grounded in temporal and spatial segmentation. This technique facilitates the procurement of untainted, high-quality datasets from open-source repositories while confirming data authenticity and eliminating extraneous factors related to authorship. Additionally, we employ a semi-automated approach to generate an expansive dataset, with ChatGPT synthesizing millions of lines of code via three strategies: "code transformation," "function transformation," and "custom functionality." This enriched dataset, which amalgamates human-authored and ChatGPT-generated code, furnishes robust data support for our research.</p>
<p>In summation, our research offers the following seminal contributions:
(1) We devise a discriminative feature set predicated on heuristic code segmentation, which efficaciously differentiates ChatGPTgenerated code from human-authored code, achieving a classification accuracy of over $90 \%$ in code origin identification tasks through synergistic use with machine learning algorithms.
(2) We introduce three strategies to amass a copious volume of ChatGPT-generated code: "code transformation," "function transformation," and "function customization."
(3) We put forth a dataset cleansing technique centered on temporal and spatial segmentation, facilitating the extraction of immaculate, high-quality code datasets from open-source repositories. Employing this technique, we have amassed and purified approximately 20GB of code datasets. Subsequent application of these datasets to code author attribution tasks yielded an impressive accuracy rate of no less than $95 \%$.</p>
<p>Our contributions establish a robust theoretical framework for distinguishing code generated by advanced models like ChatGPT from human-authored code. These findings bear considerable practical implications, including promoting academic integrity, protecting intellectual property, and bolstering software security, while fostering the sustainable development of AI in programming. This research is poised to catalyze advancements in related domains and pave the way for a harmonious coexistence between human developers and intelligent programming assistants.</p>
<p>The remainder of this paper is organized as follows: Section 2 presents research work related to AI-generated content detection, code authorship attribution and ChatGPT-generated code. Section 3 provides an overview of the creation of datasets comprising humangenerated code and ChatGPT-synthesized code. Section 4 elucidates the methodologies employed in developing the discriminative
feature set. Section 5 details the experiments conducted and the ensuing analysis of the results. Section 6 concludes the paper and provides an outlook for further research. Finally, Section 7 outlines the limitations of our research and suggests potential directions for future research.</p>
<h2>2 RELATED WORK</h2>
<h3>2.1 AIGC Detector</h3>
<p>Research is already emerging in the field of detecting and identifying content generated by artificial intelligence (AI) models. For instance, in the domain of natural language processing, there have been notable studies focused on distinguishing AI-generated content from human-created content.</p>
<p>Mitrović et al. [11] employed a machine learning approach to differentiate ChatGPT by extracting features from text messages. Their study achieved an accuracy of $79 \%$ by focusing on shorter text responses generated by ChatGPT in comparison to manually generated text.</p>
<p>In another study, Guo et al. [7] collected 40K questions and corresponding answers from both human experts and ChatGPT to construct the Human ChatGPT Comparison Corpus (HC3) dataset. This dataset encompassed various domains, including open domain, finance, medical, legal, and psychological fields. Through the HC3 dataset, the researchers investigated differences between the features of ChatGPT-generated texts and those of human experts. They conducted feature analysis from multiple perspectives, including Vocabulary Features, Part-of-Speech and Dependency Analysis, and Sentiment Analysis. Additionally, they employed classical machine learning and deep learning techniques, performing experiments with three representative methods to further validate their findings.</p>
<p>Wenxiong Liao et al.[9] conducted a comprehensive analysis of medical texts to distinguish between content written by human experts and text generated by ChatGPT. Their study involved constructing a specialized dataset for medical texts and analyzing linguistic features, such as vocabulary, among others. To detect medical texts generated by ChatGPT, they employed a machine learning approach, specifically utilizing a BERT-based model. The results were promising, with the model achieving an impressive F1 score exceeding $95 \%$. Furthermore, the researchers observed that medical texts authored by humans tend to be more concrete, diverse, and contain a wealth of useful information. Conversely, medical texts generated by ChatGPT prioritize fluency and logic, often expressing general terminologies rather than providing context-specific, problem-related information.</p>
<h3>2.2 Code authorship attribution</h3>
<p>To date, there has been limited academic research addressing the specific problem of distinguishing code generated by AI models from code written by humans. However, this research can be aligned with the traditional code authorship attribution problem, which focuses on extracting code-level features to differentiate or trace the authorship of code.</p>
<p>In the field of code authorship attribution, classical machine learning approaches have been explored. Caliskan-Islam et al. [2] utilized a dataset comprising the code of 1600 authors from Google Code Jam. They extracted approximately 120,000 features based</p>
<p>on vocabulary, layout, and syntax from the source code. By employing a random forest model consisting of 300 decision trees, they achieved up to $98 \%$ accuracy on a test set comprising code from 250 programmers who participated in Google Code Jam 2014, effectively pushing the boundaries of machine learning models.</p>
<p>Regarding deep learning, Abuhamad et al. [1] employed a dataset consisting of 1600 authors from Google Code Jam (spanning from 2008 to 2016) and 1987 repositories from GitHub, with $142 \mathrm{C}++$ and 745 C programmers. They initially utilized the text analysis tool TF-IDF (Term Frequency-Inverse Document Frequency) for preprocessing the source code, which served as input for a deep learning network. They designed a deep learning framework based on recurrent neural networks to extract code features. The author attribution task was then accomplished using a random forest classifier. This approach achieved an accuracy of $96 \%$ in the Google Code Jam experiment with 1600 authors and $94.38 \%$ on the real dataset of 745 C programmers.</p>
<h3>2.3 Code generated by ChatGPT</h3>
<p>The current research on code generated by ChatGPT encompasses several areas, including code security, code correctness, code quality improvement, code error resolution, code meaning interpretation, and the potential of ChatGPT as a programming assistant.</p>
<p>Raphaël Khoury et al. [8] explored the capabilities of ChatGPT in generating programs and assessed the security of the resulting source code. They also investigated the effectiveness of prompting ChatGPT to enhance code security and delved into the ethical considerations associated with leveraging AI for code generation. The findings indicate that ChatGPT demonstrates some awareness of potential vulnerabilities. However, it frequently generates source code that lacks robustness against certain attacks.</p>
<p>Jiawei Liu et al. [10] introduced EvalPlus, a code synthesis benchmarking framework designed to thoroughly evaluate the functional correctness of code synthesized by Language Models (LLMs). In their work, they extended the widely used HUMANEVAL benchmark and created HUMANEVAL+, which includes an additional $81+$ generated tests. Through extensive evaluation across 14 popular LLMs, including GPT-4 and ChatGPT, they demonstrated that HUMANEVAL+ effectively detects a significant number of previously undetected erroneous code synthesized by LLMs. On average, it reduces the pass@k metric by $15.1 \%$.</p>
<p>The research conducted by Burak Yetiştiren et al.[16] aims to conduct a comparative analysis of prominent code generation tools, including GitHub Copilot, Amazon CodeWhisperer, and ChatGPT, in terms of various code quality metrics such as Code Validity, Code Correctness, Code Security, Code Reliability, and Code Maintainability. To achieve this, they utilize the benchmark HumanEval Dataset to evaluate the generated code based on the proposed code quality metrics.The analysis reveals that the latest versions of ChatGPT, GitHub Copilot, and Amazon CodeWhisperer achieve correct code generation rates of $65.2 \%, 46.3 \%$, and $31.1 \%$ respectively. Moreover, the newer versions of GitHub Copilot and Amazon CodeWhisperer demonstrate improvement rates of $18 \%$ and $7 \%$ respectively in terms of generating correct code. Additionally, the average technical debt, considering code smells, is found to be 8.9 minutes for ChatGPT,
9.1 minutes for GitHub Copilot, and 5.6 minutes for Amazon CodeWhisperer.</p>
<p>Jules White et al. [14]explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, refactoring, and system design.</p>
<p>Dominik Sobania et al. [12] evaluate the bug fixing performance of ChatGPT on the widely used QuixBugs benchmark set. They compare its performance with several other approaches reported in the literature. The findings reveal that ChatGPT's bug fixing capability is on par with common deep learning approaches like CoCoNut and Codex, and significantly outperforms standard program repair methods. Moreover, by providing hints to ChatGPT, they were able to further enhance its success rate, with ChatGPT successfully fixing 31 out of 40 bugs, surpassing the state-of-the-art performance.</p>
<p>Eason Chen et al.[3] presents GPTutor, a ChatGPT-powered rogramming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers indicated that GPTutor is user-friendly and can explain given codes satisfactorily.</p>
<p>Haoye Tian et al. [13] conduct an empirical analysis to evaluate the capabilities of ChatGPT as a fully automated programming assistant, with a focus on code generation, program repair, and code summarization. The study specifically examines ChatGPT's performance in solving common programming problems and compares it to state-of-the-art approaches using two benchmark datasets. The findings demonstrate that ChatGPT effectively addresses typical programming challenges. However, the researchers also identify limitations in its attention span. They observe that when faced with comprehensive problem descriptions, ChatGPT's focus can be constrained, thus hindering its ability to leverage its vast knowledge for effective problem-solving.</p>
<p>Yihong Dong et al.[5] present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLMs act as distinct "experts", each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. They conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves $29.9 \%-47.1 \%$ Pass@1 compared to direct code generation, achieving state-of-the-art performance and even surpassing GPT-4.</p>
<h2>3 HUMAN AND CHATGPT CODE DATASET</h2>
<p>The dataset as a whole can be divided into two parts: those generated by ChatGPT and those crawled from Github.</p>
<h3>3.1 ChatGPT Code Dataset</h3>
<p>Our endeavor to comprehensively discern the attributes of code generated by ChatGPT necessitated the creation of corresponding datasets. We employed two versions of ChatGPT: GPT-3.5, which avails batch generation through an API, and GPT-4, which presently only facilitates generation through human interaction due to the absence of an open API.</p>
<p>It is imperative to recognize that ChatGPT's training data could encompass code from open-source repositories on Github. To investigate the viability of employing ChatGPT for code creation, and to discern its idiosyncratic features, we applied three distinct strategies: "code translation," "function translation," and "custom functions" for each version of ChatGPT. These strategies are explicated as follows:</p>
<ul>
<li>Code Translation: This strategy entails supplying ChatGPT with a code snippet written in programming language $X$, and tasking it to generate equivalent functionality code in a different programming language Y. The generated code must span at least 100 lines to circumvent the production of "boilerplate code" replete with annotations, which could impair data quality. As the generated code is translated into a different programming language, it is less likely to have been part of ChatGPT's training data, hence, it is relatively novel. But this also runs the risk of mimicking the wording and logic of the original code.</li>
<li>Functional Translation: In contrast to Code Translation, Functional Translation requires ChatGPT to first analyze and abstract a functional description from a given code snippet. Subsequently, ChatGPT is instructed to generate code in the same programming language as the original snippet based on the derived functional description. This method prevents direct imitation of the original code, but it is slightly less innovative compared to Code Translation since the output is in the same language as the original code, which could be part of ChatGPT's training data.</li>
<li>Functional customisation: This strategy diverges from the prior two by prompting ChatGPT to generate code based on pre-existing functional descriptions sourced from programming competitions, textbooks, or other typical programming tasks. As this approach neither supplies ChatGPT with sample code nor restricts it to the original programming language, it engenders greater originality compared to Function Translation. Nonetheless, since solutions to these programming tasks might be accessible online and included in ChatGPT's training data, this approach is not as novel as Code Translation.</li>
</ul>
<h3>3.2 Human Code Dataset</h3>
<p>Human-authored code datasets are integral to research in the domain of code authorship attribution. Present studies predominantly utilize datasets procured from four principal sources:
(1) Educational Programming Tasks: These datasets encompass code penned by students for class assignments. However, their restrictive scope and educational focus impede their generalizability.
(2) Competitive Programming Archives (e.g., Google Code Jam): Although these datasets are of high caliber, they might not accurately depict programming methodologies employed in real-world software development, owing to the specialized nature of competition problems and environments.
(3) Open Source Repositories (e.g., Github): Being the most reflective of real-world programming practices, these datasets are invaluable. However, ascertaining the sole authorship of the code is challenging due to collaborative projects and the opacity of development processes.
(4) Textbook Supplements: Code written by authors to supplement programming textbooks is also used. However, this data is limited by the scope of the textbooks and lacks generalization.</p>
<p>Among these, datasets derived from open-source repositories are ostensibly the most authentic and efficacious for capturing coding styles. Github, being the world's largest open-source community, hosts numerous repositories. Although individuals frequently upload their code, the prevalence of shared or borrowed code is significant. Additionally, coding styles evolve over time, with discernible differences between code written at the nascent stages of learning and that written post-acquiring professional expertise. Consequently, indiscriminate code harvesting from repositories could compromise dataset quality and obfuscate the analysis of distinct coding styles.</p>
<p>To alleviate this issue, this paper introduces a methodology premised on temporal and spatial segmentation to collate code from individual repositories, thereby capturing the essence of an author's coding style during a specific timeframe. The ensuing section delineates the steps undertaken to clean the dataset.
(1) Initial Cleanup: In this step, organization accounts, forked repositories, and duplicates are eliminated. This culling is predicated on existing literature and conventions. The filtering of accounts is restricted to individual users, thereby excluding organization accounts. Forked repositories are omitted, as they typically do not contain original code. Moreover, duplicate repositories and files are removed to economize on storage and computation.
(2) Temporal Segmentation: Repositories are categorically segmented by their creation year. Studies[1, 2] suggest that an author's coding style remains relatively stable for approximately two years. Consequently, the dataset is divided based on the creation year, ensuring each segment contains code within a specific timeframe. In our experiments, repositories are classified from 2008 to 2022, rendering 15 categories. Subsequently, we focus on the code produced in the recent year (2021-2022).
(3) Spatial Segmentation: This step entails the removal of thirdparty libraries and collaborative repositories. Repositories containing third-party libraries are excluded based on naming conventions. Furthermore, repositories with multiple contributors are identified and removed by analyzing the "contributors" metadata. A comprehensive list of common third-party libraries for C++ and Java was compiled based on frequency analysis and practical development experience, serving as a filter criterion.</p>
<p>In theory, post these cleaning steps, the residual source code should predominantly be authored by the individuals themselves. Nevertheless, some exceptions may include code obtained from external sources. Such instances are considered data noise and are disregarded.</p>
<p>The aforementioned steps ensure that the final dataset chiefly comprises code that reflects the individual authors' distinct coding styles.</p>
<h2>4 DISCRIMINATIVE FEATURES SET</h2>
<p>In this study, we draw upon methodologies from traditional code authorship attribution; however, we tailor the feature extraction process to suit the distinct nature of our task. Traditional code authorship aims to discriminate between code written by different individuals, while our objective is to classify code written by humans as one category and code generated by ChatGPT as another. Consequently, this necessitates a modified approach to feature selection. We adapt the feature selection from traditional code authorship by conducting a heuristic code feature analysis. This enables us to construct a discriminative feature set that effectively discerns between human-authored code and ChatGPT-generated code. This feature set comprises three main categories: lexical features, structural layout features, and semantic features. Notably, this refined feature set diverges from those traditionally employed in code authorship studies and is specifically tailored for our task. In the ensuing subsections, we expound upon the design methods for each category within the feature set.</p>
<h3>4.1 Lexical features</h3>
<p>Instead of analyzing the lexical features of the entire code, we segregate the vocabulary within the code into four distinct categories:
(1) Comments and Strings: This category includes single and multi-line comments, as well as strings enclosed in double quotes. These text blocks are indicative of the author's textual style.
(2) Identifiers: Comprising class names, method names, variable names, and interface names, identifiers reveal the author's naming conventions and library usage patterns.
(3) Keywords: These reserved words are intrinsic to the programming language, governing syntax structures, control flow, data types, and variable declarations. Analyzing the usage of keywords sheds light on the author's programming practices within the language.
(4) Imported Libraries: This category encompasses the standard and third-party libraries incorporated in the code through "include" (C++) or "import" (Java) statements. This reflects the author's familiarity with various libraries.
Prior to lexical analysis, we tokenize the code, taking into account conventions such as camel case or underscores in identifiers. We separate words in comments, strings, and identifiers using spaces and punctuation marks. Subsequently, we split these tokens according to naming conventions and normalize them to lowercase. For keywords, we compare the tokens against a set of language-specific keywords. For imported libraries, we retain the complete names as they represent entities and are indicative of the author's style.</p>
<p>We tally the count of each vocabulary type and compute the term frequency (TF) of each word within these categories.</p>
<h3>4.2 Structural Layout Features</h3>
<p>In our preliminary analysis of ChatGPT code datasets, we observed that ChatGPT adheres to certain conventional formatting standards regarding layout features. While this is also typical of human-authored code, directly employing layout features from traditional code authorship as distinguishing factors would not be efficacious.</p>
<p>However, upon rigorous comparative analysis, we discerned subtle yet distinguishing layout and structural features that are characteristic of ChatGPT-generated code. We identified 22 such features, encompassing aspects such as comment ratio, blank line ratio, presence of line breaks preceding braces, average nesting depth, indentation length, and the average number of parameters in functions. These features are reflective of coding conventions and styles, and exhibit discernable disparities between human-authored code and ChatGPT-generated code. For a comprehensive listing of these features, refer to Table 3.</p>
<h3>4.3 Semantic features</h3>
<p>In this study, we undertook an exhaustive, hands-on comparative analysis supplemented with extensive literature review to devise a set of semantic features intrinsic to code, an aspect that had remained untouched in prior research concerning code authorship attribution. It is our conviction that humans and AI embody distinct proficiencies and constraints when it comes to programming. For instance, human programmers excel in logical reasoning and possess the invaluable ability to collaborate and brainstorm, yet they are shackled by the boundaries of their knowledge and are susceptible to emotional biases such as procrastination or frustration. Contrarily, AI boasts an encyclopedic reservoir of knowledge, and remains impervious to emotional fluctuations; however, it falls short in logical reasoning prowess and lacks the initiative to scrutinize code autonomously. Interestingly, these disparities are mirrored in the semantics of the code written by humans and AI, and these semantics play a pivotal role in various facets like the code's execution efficiency, accuracy in real-world deployment, performance metrics, and so on. After meticulously evaluating various semantic features, we have zeroed in on the following three core aspects:
(1) Runnability Analysis: This entails compiling and executing the code to identify compilation or runtime errors.
(2) Correctness Analysis: If the code is runnable, we input test cases for algorithmic problems to verify whether the code produces accurate outputs.
(3) Time-Space Performance Analysis: For code that correctly solves the problem, we analyze the execution time and memory usage under large-scale test cases.</p>
<h2>5 DIFFERENCES BETWEEN CODE AUTHORED BY HUMANS AND CHATGPT</h2>
<p>In Chapter 3, we presented the three categories of features that we identified as discriminative for distinguishing code produced by ChatGPT from human-authored code. In this chapter, we discuss</p>
<p>experiments designed around these feature sets: binary classification, explanatory word frequency analysis, and exploratory analysis based on semantic features.</p>
<h3>5.1 Experiment Design</h3>
<p>5.1.1 Binary Classification Experiment. This experiment seeks to ascertain the feasibility of utilizing lexical and layout structural features to distinguish ChatGPT-generated code from human-written code, focusing on C++ and Java. We specifically employ lexical and layout structural features as they are readily quantifiable for machine learning models. For performance evaluation, we employ Accuracy, Precision, Recall, and F1 Score metrics, and also conduct ablation studies to investigate the contributions of each feature set.
5.1.2 Explanatory Word Frequency Analysis Experiment. This experiment visually represents and statistically analyzes the discrepancies in word usage within C++ and Java code authored by ChatGPT and humans. Specifically, we study the frequency of comments, strings, identifiers, keywords, and imported packages/headers. We contrast the frequencies and consider ChatGPT's documentation and relevant studies for additional context, offering analysis for particular variations.
5.1.3 Exploratory Semantic Analysis. This experiment investigates the semantic distinctions between code produced by ChatGPT and humans when solving identical programming problems. Due to the intricate nature of semantic feature extraction, and constraints in resources and time, this experiment mainly serves to provide insights and stimulate future research. Specifically, we present ChatGPT with 100 algorithm problems from LeetCode and evaluate various aspects, such as difficulty level, pass rate, executability, correctness, and time-space performance.</p>
<h3>5.2 Result Summarization</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of of Java code ablation experiment.</p>
<h3>5.2.1 Effectiveness of Code Detection.</h3>
<p>In the binary classification experiment, we analyzed the individual and combined effects of lexical and layout structural features in differentiating between human and ChatGPT-generated code. The
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of C++ code ablation experiment.
results, as depicted in Table 1 and Table 2, were visually presented in Figures 2 and 1, respectively. These visual representations demonstrate that the combination of both feature sets yields the highest accuracy in classification. Notably, the complementary nature of these features in code classification tasks is underscored, implying the integral role each feature plays in distinguishing between the two code variants.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison of comment and string frequencies in Java code.</p>
<h3>5.2.2 Characteristic Differences.</h3>
<p>In this subsection, we delve into the distinctions between ChatGPTgenerated code and human-authored code by analyzing comments and strings, identifiers, keywords, and library usages. Figures 3 to 10 depict these aspects, focusing on tokens with discrepancies exceeding $50 \%$ and ranking them in descending order based on the difference. Our investigation aims to illuminate the inherent characteristics distinguishing ChatGPT-generated code from humanwritten code. The following observations can be drawn from the disparity in frequencies of these elements:
(1) ChatGPT generally employs more elaborate and semantically rich terminology, whereas humans often favor concise and simpler words.</p>
<p>Table 1: Comparison of Java code ablation experiment.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">lexical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">layout</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">all</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Algorithm</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-Measure</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-Measure</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-Measure</td>
</tr>
<tr>
<td style="text-align: center;">Random Forest</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.960</td>
</tr>
<tr>
<td style="text-align: center;">SMO</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.958</td>
</tr>
<tr>
<td style="text-align: center;">Simple Logistic</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.969</td>
</tr>
<tr>
<td style="text-align: center;">J48</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.978</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of C++ code ablation experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">lexical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">layout</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">all</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Algorithm</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-Measure</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-Measure</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-Measure</td>
</tr>
<tr>
<td style="text-align: center;">Random Forest</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;">SMO</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: center;">Simple Logistic</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.906</td>
</tr>
<tr>
<td style="text-align: center;">J48</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.890</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of comment and string frequencies in $\mathrm{C}++$ code.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of keyword frequencies in Java code.
(2) Human code frequently exhibits remnants of development iterations, such as commented-out code snippets, which are absent in ChatGPT-generated code. Additionally, ChatGPT
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of keyword frequencies in $\mathrm{C}++$ code.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of identifier frequencies in Java code.
does not generate extraneous code, while human code sometimes includes unreferenced variables or methods.
(3) The code generated by ChatGPT adheres more closely to coding standards compared to human-authored code. For</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Comparison of identifier frequencies in C++ code.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Comparison of library usage in Java code.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Comparison of library usage in C++ code.
instance, in C++, ChatGPT prefers the standard "endl" line terminator over the "in" newline character.
(4) ChatGPT-generated code typically does not exhibit inter-file dependencies, unlike human-written code, which often has a high degree of coupling between multiple files.
(5) ChatGPT emphasizes readability, employing meaningful words for identifiers, whereas humans may use abbreviations or letters with ambiguous meanings.
(6) ChatGPT is more likely to utilize newer language features. For example, in Java, it favors the "foreach" loop iteration over the traditional "for" loop, and in C++, it makes frequent use of the "auto" keyword for type inference.
(7) ChatGPT-generated code resembles sample or template code sometimes, with comments often suggesting areas requiring custom implementation. In contrast, human code is typically intended for release, and employs logging for debugging instead of console output statements which are common in ChatGPT's code.</p>
<h3>5.2.3 Examining the Originality and Semantic Similarities of ChatGPTGenerated Code.</h3>
<p>In our exploratory experiments, we sought to investigate if ChatGPT tends to replicate code authored by humans and to analyze the semantic differences between the code produced by humans and ChatGPT. To this end, we assembled a dataset composed of 100 programming tasks along with their respective human-authored solutions and codes. We then employed ChatGPT to generate code solutions for these tasks and performed a comparative analysis of the outcomes.</p>
<p>In the case of programming tasks predating ChatGPT's knowledge base cutoff date, the code generated by ChatGPT exhibited an exceptional executability rate of $100 \%$ and a correctness rate of $100 \%$. In contrast, human solutions achieved an average correctness rate of $50 \%$ under comparable circumstances. Additionally, ChatGPT's solutions demonstrated superior temporal efficiency in approximately $72.75 \%$ of the cases and spatial efficiency in roughly $49.16 \%$ of cases compared to human-authored code.</p>
<p>For programming tasks postdating ChatGPT's knowledge base cutoff date, ChatGPT achieved an executability rate of $90.9 \%$. The initial correctness rate of ChatGPT's solutions stood at $41 \%$, falling short of the human average of around $50 \%$. However, through subsequent refinements and iterations informed by follow-up questions, ChatGPT's solutions saw their correctness rate ascend to $63.63 \%$. Regarding temporal efficiency, ChatGPT's solutions outperformed human-authored code in approximately $58.91 \%$ of cases. Similarly, in spatial efficiency, ChatGPT's solutions outstripped human-authored code in approximately $42 \%$ of cases.</p>
<p>Furthermore, we conducted an in-depth comparison of ChatGPTgenerated code against human-written code for the same programming tasks. Among the 100 code solutions, only two instances were observed in which ChatGPT exactly replicated human-authored code. In the majority of cases, ChatGPT manifested semantic similarity to human-authored code, echoing the underlying logic and ideas but employing distinct syntactic constructs and variations.</p>
<p>It is crucial to delve into the implications of ChatGPT's semantic similarity with human-authored code. The manifestation of semantic similarity suggests that ChatGPT is capable of discerning and emulating the core logic underlying human coding practices. While this could streamline the code generation process and uphold code quality, it also raises questions regarding innovation and originality in coding. ChatGPT's inclination to reflect existing coding conventions and logic could, in some instances, stifle novel approaches and solutions. Additionally, ethical considerations such as intellectual property rights and plagiarism need to be taken into account,</p>
<p>especially in contexts where code is generated for commercial or proprietary applications. Ensuring that ChatGPT-generated code is sufficiently distinct from existing code, and that it acknowledges and credits foundational sources where necessary, is integral to the responsible and ethical deployment of this technology.</p>
<h2>6 CONCLUSION</h2>
<p>This paper elucidates the critical distinctions between ChatGPTgenerated code and human-authored code, along with their ramifications. The salient conclusions drawn from this investigation are enumerated below:
(1) The discriminative feature set proposed in this study demonstrates exceptional efficacy in discerning ChatGPT-generated code from human-authored code, with an accuracy rate surpassing $90 \%$. This underscores the viability of employing the identified feature set for distinguishing between ChatGPTgenerated and human-authored code.
(2) Examination of token frequencies unveils pronounced disparities between ChatGPT-generated and human-authored code in the utilization of specific tokens. These discrepancies are indicative of disparate coding practices and preferences. Moreover, ChatGPT-generated code exhibits semantic variations compared to human-authored code when addressing identical programming tasks. These semantic differences represent varying levels of programming expertise and knowledge reservoirs.
(3) ChatGPT has demonstrated a propensity to reproduce existing code in scenarios where tasks are congruent with its pre-existing knowledge base. However, it is worth noting that this is not a simple replication; ChatGPT often recontextualizes and restructures the code, showcasing its ability to assimilate and adapt human programming expertise.
(4) While ChatGPT is proficient in generating functionally correct and efficient code, the semantic similarity with humanauthored code raises questions regarding innovation and the ethical aspects of code generation. Ensuring originality and proper acknowledgment in ChatGPT-generated code is vital, especially in contexts involving proprietary or commercial applications.
The revelations from this study deepen our comprehension of the code generation capabilities of ChatGPT and bring into focus the parallels and distinctions between ChatGPT-generated and human-authored code. These insights are invaluable for diverse applications including code identification, analysis, and the integration of AI-assisted code generation into development workflows. Future inquiries can leverage these insights to bolster the precision and efficiency of code identification mechanisms and to investigate strategies for optimally combining the proficiencies of ChatGPT with human ingenuity. Moreover, additional studies could explore the ethical considerations and frameworks needed to guide the responsible use of AI in code generation.</p>
<h2>7 LIMITATIONS</h2>
<p>Notwithstanding the invaluable revelations procured from this investigation, it is imperative to recognize certain constraints that warrant consideration:
(1) The ChatGPT code dataset used in this study is relatively small in size. Due to constraints in time and resources, the collected data is still insufficient and unbalanced across different sources, programming languages, styles, and tasks. To enhance the accuracy and reliability of ChatGPT code analysis and differentiation, a larger and more diverse dataset encompassing a wider range of coding styles and sources is needed.
(2) The characterization of the ChatGPT code dataset is not exhaustive. It should be noted that all ChatGPT-generated code samples collected for this study were generated without any specific prompts or instructions. Therefore, the analysis and conclusions presented in this paper are based on the general programming style and state of ChatGPT. It is important to recognize that using specific prompts or instructions during code generation, such as excluding comments or employing a particular programming approach, may potentially interfere with our feature detection process and challenge the validity of the conclusions drawn in this study.
(3) The ChatGPT code dataset might not fully represent the programming style of large-scale code generation models. Currently, ChatGPT is a generic large-scale language generation model that addresses various domains, including code generation. However, it is not exclusively designed as a dedicated large-scale code generation model. Therefore, the programming characteristics observed in the HCCD dataset may not entirely reflect the programming style and behavior of specialized large-scale code generation models.
These constraints delineate avenues for future inquiries and underscore the necessity for broader and more heterogeneous datasets, exhaustive examination of code generation directives, and exploration of the programming attributes of specialized large-scale code generation models. Redressing these constraints will catalyze a more encompassing understanding of code generation models and foster the evolution of more precise and dependable methodologies for differentiating between code engendered by artificial intelligence and that which is authored by humans. Additionally, ethical considerations surrounding code generation and intellectual property should also be integral components of future studies.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We express our heartfelt gratitude to Professor Fu Cai for generously providing the necessary datasets and computational resources that facilitated this study, whose invaluable guidance, support, and insights have been indispensable throughout the course of this research. We also acknowledge the tireless efforts and contributions of our fellow researchers and colleagues at Information Security Laboratory, School of Cyber Science and Engineering, Huazhong University of Science and Technology, especially Professor Wen Ming, Dr. Jiang Shuai, Li Wenke, M.A.</p>
<h2>REFERENCES</h2>
<p>[1] Mohammed Abuhamad, Tamer AbuHmed, Aziz Mohaisen, and DaeHun Nyang. 2018. Large-Scale and Language-Oblivious Code Authorship Identification. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (Toronto, Canada) (CCS '18). Association for Computing Machinery, New York, NY, USA, 101-114. https://doi.org/10.1145/3243734.3243738</p>
<p>[2] Aylin Caliskan-Islam, Richard Harang, Andrew Liu, Arvind Narayanan, Clare Voss, Fabian Yamaguchi, and Rachel Greenstadt. 2015. De-anonymizing programmers via code stylometry. In Proceedings of the 24th USENIX Security Symposium (Proceedings of the 24th USENIX Security Symposium). USENIX Association, 255270. 24th USENIX Security Symposium ; Conference date: 12-08-2015 Through 14-08-2015.
[3] Eason Chen, Ray Huang, Han-Shin Chen, Yuen-Hsien Tseng, and Liang-Yi Li. 2023. GPTutor: a ChatGPT-powered programming tool for code explanation. arXiv:2305.01863 [cs.HC]
[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qinning Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs.LG]
[5] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023. Self-collaboration Code Generation via ChatGPT. arXiv:2304.07590 [cs.SE]
[n] Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang Huang, Nan Duan, and Weizhu Chen. 2022. GENIUS: Sketch-based Language Model Pretraining via Extreme and Selective Masking for Text Generation and Augmentation. arXiv:2211.10330 [cs.CL]
[7] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. arXiv:2301.07597 [cs.CL]
[8] Raphaël Khoury, Anderson R. Avila, Jacob Brunelle, and Baba Mamadou Camara. 2023. How Secure is Code Generated by ChatGPT? arXiv:2304.09655 [cs.CR]
[9] Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Dajiang Zhu, Hongmin Cai, Tianming Liu, and Xiang Li. 2023. Differentiate ChatGPT-generated and Human-written Medical Texts. arXiv:2304.11567 [cs.CL]
[10] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. arXiv:2305.01210 [cs.SE]
[11] Sandra Mitrovic, Davide Andreoletti, and Osteun Ayoub. 2023. ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text. arXiv:2301.13852 [cs.CL]
[12] Dominik Subania, Martin Briesch, Carol Hanna, and Justyna Petke. 2023. An Analysis of the Automatic Bug Fixing Performance of ChatGPT.
[13] Haoye Tian, Wenji Lu, Tez On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, and Tegawendé F. Bissyandé. 2023. Is ChatGPT the Ultimate Programming Assistant - How far is it? arXiv:2304.11938 [cs.SE]
[14] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design. arXiv:2303.07839 [cs.SE]
[15] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-And-Write: Towards Better Automatic Storytelling. arXiv:1811.05701 [cs.CL]
[16] Burak Yetiztiren, Işık Özsoy, Mizay Ayerdem, and Eray Tüzün. 2023. Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. arXiv:2304.10778 [cs.SE]</p>
<h2>APPENDIX</h2>
<p>This appendix contains a table that summarizes the features used in the analysis. Each feature is accompanied by a brief description to provide clarity on its role within the study. The table can be found on the following page.</p>
<p>Table 3: Summary of Features Used in Analysis</p>
<table>
<thead>
<tr>
<th>Feature Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Control Structure Density</td>
<td>Logarithm of the ratio of the count of seven control structure-related keywords</td>
</tr>
<tr>
<td></td>
<td>(do, else if, if, else, switch, for, while) to file length</td>
</tr>
<tr>
<td>Ternary Operator Density</td>
<td>Logarithm of the ratio of the count of ternary operators to file length</td>
</tr>
<tr>
<td>Token Density</td>
<td>Logarithm of the ratio of the count of tokens to file length</td>
</tr>
<tr>
<td>Comment Density</td>
<td>Logarithm of the ratio of the count of comments to file length</td>
</tr>
<tr>
<td>Literal Density</td>
<td>Logarithm of the ratio of the count of literals to file length</td>
</tr>
<tr>
<td>Keyword Density</td>
<td>Logarithm of the ratio of the count of keywords to file length</td>
</tr>
<tr>
<td>Function Density</td>
<td>Logarithm of the ratio of the count of functions to file length</td>
</tr>
<tr>
<td>Maximum Nesting Depth</td>
<td>Maximum nesting depth of control and loop structures</td>
</tr>
<tr>
<td>Average Branching Factor</td>
<td>Average number of subtrees per code block</td>
</tr>
<tr>
<td>Average Parameters per Function</td>
<td>Average number of parameters in functions</td>
</tr>
<tr>
<td>Standard Deviation of Parameter Count</td>
<td>Standard deviation of parameter count in functions</td>
</tr>
<tr>
<td>Average Line Length</td>
<td>Average length of lines in the code file</td>
</tr>
<tr>
<td>Line Length Standard Deviation</td>
<td>Standard deviation of line lengths in the code file</td>
</tr>
<tr>
<td>Macro Density</td>
<td>Logarithm of the ratio of the count of preprocessor macros to file length</td>
</tr>
<tr>
<td>Tab Character Density</td>
<td>Logarithm of the ratio of the count of tab characters to file length</td>
</tr>
<tr>
<td>Space Character Density</td>
<td>Logarithm of the ratio of the count of space characters to file length</td>
</tr>
<tr>
<td>Empty Line Density</td>
<td>Logarithm of the ratio of the count of empty lines to file length</td>
</tr>
<tr>
<td>Whitespace Ratio</td>
<td>Ratio of whitespace characters (spaces, tabs, new lines) to non-whitespace</td>
</tr>
<tr>
<td></td>
<td>characters</td>
</tr>
<tr>
<td>New Line Preceding Open Brace</td>
<td>Presence of a new line character before opening braces in code blocks</td>
</tr>
<tr>
<td>Leading Indentation Type</td>
<td>Indentation at the beginning of each line using tabs or spaces</td>
</tr>
<tr>
<td>Maximum AST Node Depth</td>
<td>Maximum depth of nodes in the Abstract Syntax Tree (AST)</td>
</tr>
<tr>
<td>AST Node Bigram Frequencies</td>
<td>Relative frequencies of AST node bigrams</td>
</tr>
<tr>
<td>Average AST Node Type Depth</td>
<td>Average depth of nodes of each type in the AST</td>
</tr>
<tr>
<td>Keyword Frequencies</td>
<td>Relative frequencies of keywords in the code</td>
</tr>
<tr>
<td>Average Code Depth in AST Leaves</td>
<td>Average depth of code in AST leaf nodes</td>
</tr>
<tr>
<td>Line Length Frequencies</td>
<td>Frequencies of different line lengths in the code file</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Both authors contributed equally to this research.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1197 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1197</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1197</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-273463850</p>
                <p><strong>Paper Title:</strong> Adaptive and transparent decision-making in autonomous robots through graph-structured world models</p>
                <p><strong>Paper Abstract:</strong> ABSTRACT The growing ubiquity of autonomous robots across different fields necessitates that agents adapt to diverse tasks and ensure transparency and intelligibility in their decision-making processes. This study presents a novel framework that combines a graph-structured world model with large language models (LLMs) to address these requirements. First, a latent space is created to capture the reachability between distinct states. Next, a graph is constructed within this latent space by clustering an offline dataset that effectively captures the complex dynamics of the environment. Subsequently, LLMs are employed to redefine the reward function and relabel the dataset, thereby establishing a well-defined Markov decision process based on the previously learned graph. This relabeling process ensures that the agent's decision space aligns with user intentions. Consequently, a predictive world model is obtained, offering insights into potential future states and facilitating graph-based planning. Moreover, by inputting the planned path of the agent in the graph-structured world model into LLMs, natural language explanations can be generated to provide transparency in the decision-making process. Experiments on the D4RL benchmark validated the effectiveness of our approach in long-horizon planning, its adaptability to different user tasks, and its inherent explainability. GRAPHICAL ABSTRACT</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1197.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1197.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GWM-Latent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-structured world model in latent space (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-structured predictive world model built by (1) learning a latent space where L2 distance reflects reachability (HCDA), (2) clustering latent states into graph vertices, (3) defining a graph-MDP and planning with value iteration/Dijkstra, and (4) converting graph actions to agent actions via self-imitation behavior cloning (ICBC); LLMs are used to relabel data (user intent) and to generate natural-language explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph-structured world model (latent-space clustering)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learn a latent embedding (State Encoder, Latent Displacement Predictor, Action Decoder) trained with Hindsight Contrastive Distance Alignment (HCDA: contrastive + hindsight + LDP + action reconstruction losses). Project offline dataset into latent space, cluster states within radius γ into vertices, create directed edges for observed transitions, define an MDP on vertices with rewards relabeled by LLM-extracted user tasks; use value iteration + Dijkstra for planning and ICBC (self-imitation + goal-conditioned behavior cloning) to map planned graph transitions to low-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (graph abstraction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (Kitchen), navigation (Maze2D, AntMaze), dexterous manipulation (Adroit) — D4RL benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Training losses used as fidelity proxies: contrastive loss (L_contrast) between predicted and true next-state in latent space, hindsight loss (L_hind) enforcing reachability bounds, latent-displacement constraint (L_LDP), and action reconstruction MSE (L_AD and L_cbc) for action imitator</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability at the abstract (graph) level: vertices are clusters of mutually reachable states (visualized via UMAP), planned graph paths and identified subgoal vertices (self-loop reward increases) provide human-understandable subgoals; LLMs translate planned subgoals/paths into natural-language explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of latent-space / clustered vertices (UMAP for plotting), value-iteration vertex value heatmaps, identification of subgoals via self-loop reward changes, and natural-language explanation generation with LLMs (GPT-4) fed the planned path and subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified numerically in paper; qualitative remarks: avoids training a separate high-level neural policy by using classical value iteration and Dijkstra on the compact graph (so lower retraining cost for new tasks); training costs include encoder/LDP/AD and ICBC networks on offline dataset (no GPU/time numbers provided).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitatively more adaptable than hierarchical methods requiring two separately trained networks because high-level planning uses classical algorithms (value iteration) on a compact graph; authors claim rapid task adaptation without neural network retraining. No quantitative timing or FLOP comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported strong performance on D4RL tasks (see Table 2). Example normalized scores from paper: Maze2D-total 475.8, AntMaze-total 498.1, Kitchen-partial 96.5, Kitchen-complete 75.4, Pen-human 87.1; overall competitive or superior to multiple SOTA baselines in many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Graph abstraction prioritizes task-relevant reachability relations over full high-fidelity next-state prediction; this yields strong long-horizon planning performance and adaptability (via relabeling with LLMs). However, success depends on offline dataset coverage — if trajectories that achieve tasks are absent, the world model cannot plan them (dataset-imposed ceiling on achievable tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs noted: (1) Abstraction/granularity (clustering radius γ) — larger γ reduces graph size (efficiency) but can hurt performance (fidelity/task success); (2) Interpretability (graph nodes/subgoals) is gained at the cost of discarding some fine-grained state detail, which can reduce low-level action accuracy; (3) Reliance on offline data improves safety and reduces exploration cost but limits generalization to out-of-distribution transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: learn latent metric where L2 reflects reachability (HCDA); use contrastive and hindsight losses; cluster states with radius γ (empirically γ≈0.5); treat graph actions as transitions between vertices and define deterministic transition probabilities from observed edges; relabel rewards using LLM-extracted task lists; plan via value iteration and Dijkstra; map graph actions to agent actions with ICBC (CBC + self-imitation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically to model-free and model-based offline RL baselines (CQL, IQL, ATAC, COMBO, Diffusion-QL) and graph-based baselines (VMG). Authors claim competitive or superior performance on long-horizon and sparse-reward tasks, and additional advantages in interpretability and rapid task relabeling without network retraining; limitations relative to generative/diffusion policies include dataset-dependency and reduced ability to generalize to out-of-distribution dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends: include contrastive and hindsight components in HCDA and use self-imitation (ICBC) — ablation shows these components are critical; set clustering radius γ empirically (authors used 0.5 as default to yield inter-vertex center distance >1); use LLMs for rapid relabeling of rewards to align with user intent. No single numeric 'optimal' configuration beyond these guidelines is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive and transparent decision-making in autonomous robots through graph-structured world models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1197.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1197.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-structured WMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-structured world models (general class)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of world models that abstract environment states into discrete landmarks/vertices and transitions into edges, enabling graph-based planning (value iteration, shortest-path) and subgoal extraction for long-horizon reasoning and improved interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph-structured world models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Construct graph by clustering states (or using learned landmarks) to produce vertices and edges that capture state transitions; planning is performed on the graph (e.g., shortest-path, value iteration) and vertices can serve as subgoals for low-level controllers or action imitators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>abstraction / symbolic-latent hybrid (discrete graph over latent embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-horizon planning, navigation, sparse-reward tasks, hierarchical control (cited across Maze2D, AntMaze, Kitchen, and general RL tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically evaluated via downstream task performance (success rate, normalized D4RL scores) and sometimes next-state prediction or landmark reachability metrics; specific metrics vary by implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generally more interpretable than black-box latent predictors because nodes and edges map to human-understandable subgoals/landmarks and paths can be visualized; however interpretability depends on clustering and visualization methods.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Subgoal extraction, visualization of graph/paths, mapping vertices to states/coordinates (UMAP, plotting), natural-language annotation (in this paper via LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generally lower at high level (planning on compact graph) but requires upfront cost to learn encoder/latent representation and to build graph; specifics depend on clustering granularity and encoder complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Often more sample-efficient for long-horizon tasks compared to end-to-end training of hierarchical policies, since planning can reuse offline data; exact efficiency gains are implementation-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used successfully in prior work for long-horizon and sparse-reward tasks (cited improvements in planning and subgoal discovery); this paper reports strong D4RL numbers when using a graph-structured abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Graph abstractions focus on task-relevant connectivity (reachability) rather than pixel-perfect predictions, which improves planning utility for long-horizon tasks; however, coarse abstraction can hurt low-level action fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs include abstraction granularity vs. planning fidelity, dataset coverage vs. ability to find feasible paths, and computational cost of graph construction vs. online planning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choices include clustering method and radius, latent-space metric, how to compute edge weights and rewards, whether transitions are stochastic/deterministic, and how to map graph actions to low-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to end-to-end neural hierarchical policies or pure model-free methods, graph-structured models provide better interpretability and planning but can be limited by the dataset and lose fine-grained control fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests balancing clustering radius (γ) to avoid excessively coarse graphs and including contrastive + hindsight objectives when learning the latent metric; no universal optimal configuration provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive and transparent decision-making in autonomous robots through graph-structured world models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1197.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1197.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WorldModels (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early influential latent world-model paradigm where an agent learns a generative latent model of its environment to support imagination and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative latent dynamics model (VAE + RNN-style dynamics) that compresses observations into a latent and learns predictive dynamics for planning and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (generative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Simulated control tasks, video-game-like environments (original work used VizDoom / car racing, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not emphasized in this paper; general world-model latent spaces can be difficult to interpret directly unless additional structure is imposed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in current paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as motivation that latent world models enable perception and imagination training to improve agent performance; used as conceptual background for latent-space modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned as a foundational latent world-model approach contrasted with graph-structured simplifications to mitigate long-horizon distortion.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive and transparent decision-making in autonomous robots through graph-structured world models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1197.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1197.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mastering diverse domains through world models (DreamerV3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modern latent world-model based agent that trains policies via imagination from a learned latent dynamics model; demonstrated across many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent dynamics model used to perform imagination-based policy learning and perception/imagination training; integrates representation learning and policy/value learning in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (generative/imaginative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Diverse control domains including games (cited in paper: Minecraft example) and continuous control benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not detailed in this paper; Dreamer-family models are typically black-box neural latent models with limited innate interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as an example of how learned world models can enable rich perception and imagination training to address complex decision-making tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive and transparent decision-making in autonomous robots through graph-structured world models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1197.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1197.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Structured Surrogate Model (GSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Thompson-sampling based graph-structured surrogate model for environment dynamics that reportedly outperformed prior methods in predictive dynamics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-based meta reinforcement learning using graph structured surrogate models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Structured Surrogate Model (GSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph-based surrogate model using Thompson-sampling for dynamics prediction and planning (details summarized from citation; paper references GSSM as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-structured surrogate (probabilistic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL / predictive dynamics (referenced generically in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Paper notes GSSM as demonstrating strong predictive performance; interpretability specifics not discussed in current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited as outperforming state-of-the-art methods in dynamics prediction (qualitative statement from paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned among graph-structured model approaches; contrasted conceptually with the present paper's objectives (transparency and adaptability).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive and transparent decision-making in autonomous robots through graph-structured world models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1197.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1197.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VMG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Memory Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-structured world model built from offline datasets that stores value information at graph nodes and uses graph-based planning for long-horizon, sparse-reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value memory graph: a graph-structured world model for offline reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Memory Graph (VMG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructs graph from offline data with vertex-level value memories and uses the graph to generate subgoals and guide planning; designed for goal-conditioned, long-horizon offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-structured world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Goal-conditioned tasks, sparse-reward and long-horizon offline RL (e.g., navigation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides moderate adaptability by reusing VMG with new reward functions, but paper notes VMG's low-level policy can suffer from inaccuracies due to abstraction granularity loss and lacks transparency in decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Graph-based subgoal extraction and planning; interpretability is limited by abstraction granularity according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper compares empirically and states VMG is practical and moderately adaptable; no quantitative compute comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported as a competitive baseline in D4RL comparisons (authors include VMG in Table 2 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for long-horizon and sparse tasks, but abstraction granularity may degrade low-level policy accuracy and reduce transparency compared to the authors' LLM-augmented graph approach.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Abstraction granularity vs. low-level accuracy and transparency; VMG reuse of graph for new rewards offers flexibility but may lose fine control fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with the paper's method: both are graph-based but the present paper emphasizes LLM relabeling for task alignment and explicit explainability; VMG has reuse advantages but suffers from granularity-related low-level inaccuracies per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive and transparent decision-making in autonomous robots through graph-structured world models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Value memory graph: a graph-structured world model for offline reinforcement learning <em>(Rating: 2)</em></li>
                <li>Model-based meta reinforcement learning using graph structured surrogate models <em>(Rating: 1)</em></li>
                <li>Replay memory as an empirical MDP: combining conservative estimation with experience replay <em>(Rating: 1)</em></li>
                <li>Combo: conservative offline model-based policy optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1197",
    "paper_id": "paper-273463850",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "GWM-Latent",
            "name_full": "Graph-structured world model in latent space (this paper)",
            "brief_description": "A graph-structured predictive world model built by (1) learning a latent space where L2 distance reflects reachability (HCDA), (2) clustering latent states into graph vertices, (3) defining a graph-MDP and planning with value iteration/Dijkstra, and (4) converting graph actions to agent actions via self-imitation behavior cloning (ICBC); LLMs are used to relabel data (user intent) and to generate natural-language explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Graph-structured world model (latent-space clustering)",
            "model_description": "Learn a latent embedding (State Encoder, Latent Displacement Predictor, Action Decoder) trained with Hindsight Contrastive Distance Alignment (HCDA: contrastive + hindsight + LDP + action reconstruction losses). Project offline dataset into latent space, cluster states within radius γ into vertices, create directed edges for observed transitions, define an MDP on vertices with rewards relabeled by LLM-extracted user tasks; use value iteration + Dijkstra for planning and ICBC (self-imitation + goal-conditioned behavior cloning) to map planned graph transitions to low-level actions.",
            "model_type": "latent world model (graph abstraction)",
            "task_domain": "robotic manipulation (Kitchen), navigation (Maze2D, AntMaze), dexterous manipulation (Adroit) — D4RL benchmark",
            "fidelity_metric": "Training losses used as fidelity proxies: contrastive loss (L_contrast) between predicted and true next-state in latent space, hindsight loss (L_hind) enforcing reachability bounds, latent-displacement constraint (L_LDP), and action reconstruction MSE (L_AD and L_cbc) for action imitator",
            "fidelity_performance": null,
            "interpretability_assessment": "High interpretability at the abstract (graph) level: vertices are clusters of mutually reachable states (visualized via UMAP), planned graph paths and identified subgoal vertices (self-loop reward increases) provide human-understandable subgoals; LLMs translate planned subgoals/paths into natural-language explanations.",
            "interpretability_method": "Visualization of latent-space / clustered vertices (UMAP for plotting), value-iteration vertex value heatmaps, identification of subgoals via self-loop reward changes, and natural-language explanation generation with LLMs (GPT-4) fed the planned path and subgoals.",
            "computational_cost": "Not quantified numerically in paper; qualitative remarks: avoids training a separate high-level neural policy by using classical value iteration and Dijkstra on the compact graph (so lower retraining cost for new tasks); training costs include encoder/LDP/AD and ICBC networks on offline dataset (no GPU/time numbers provided).",
            "efficiency_comparison": "Qualitatively more adaptable than hierarchical methods requiring two separately trained networks because high-level planning uses classical algorithms (value iteration) on a compact graph; authors claim rapid task adaptation without neural network retraining. No quantitative timing or FLOP comparisons provided.",
            "task_performance": "Reported strong performance on D4RL tasks (see Table 2). Example normalized scores from paper: Maze2D-total 475.8, AntMaze-total 498.1, Kitchen-partial 96.5, Kitchen-complete 75.4, Pen-human 87.1; overall competitive or superior to multiple SOTA baselines in many domains.",
            "task_utility_analysis": "Graph abstraction prioritizes task-relevant reachability relations over full high-fidelity next-state prediction; this yields strong long-horizon planning performance and adaptability (via relabeling with LLMs). However, success depends on offline dataset coverage — if trajectories that achieve tasks are absent, the world model cannot plan them (dataset-imposed ceiling on achievable tasks).",
            "tradeoffs_observed": "Trade-offs noted: (1) Abstraction/granularity (clustering radius γ) — larger γ reduces graph size (efficiency) but can hurt performance (fidelity/task success); (2) Interpretability (graph nodes/subgoals) is gained at the cost of discarding some fine-grained state detail, which can reduce low-level action accuracy; (3) Reliance on offline data improves safety and reduces exploration cost but limits generalization to out-of-distribution transitions.",
            "design_choices": "Key choices: learn latent metric where L2 reflects reachability (HCDA); use contrastive and hindsight losses; cluster states with radius γ (empirically γ≈0.5); treat graph actions as transitions between vertices and define deterministic transition probabilities from observed edges; relabel rewards using LLM-extracted task lists; plan via value iteration and Dijkstra; map graph actions to agent actions with ICBC (CBC + self-imitation).",
            "comparison_to_alternatives": "Compared empirically to model-free and model-based offline RL baselines (CQL, IQL, ATAC, COMBO, Diffusion-QL) and graph-based baselines (VMG). Authors claim competitive or superior performance on long-horizon and sparse-reward tasks, and additional advantages in interpretability and rapid task relabeling without network retraining; limitations relative to generative/diffusion policies include dataset-dependency and reduced ability to generalize to out-of-distribution dynamics.",
            "optimal_configuration": "Paper recommends: include contrastive and hindsight components in HCDA and use self-imitation (ICBC) — ablation shows these components are critical; set clustering radius γ empirically (authors used 0.5 as default to yield inter-vertex center distance &gt;1); use LLMs for rapid relabeling of rewards to align with user intent. No single numeric 'optimal' configuration beyond these guidelines is provided.",
            "uuid": "e1197.0",
            "source_info": {
                "paper_title": "Adaptive and transparent decision-making in autonomous robots through graph-structured world models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Graph-structured WMs (general)",
            "name_full": "Graph-structured world models (general class)",
            "brief_description": "A class of world models that abstract environment states into discrete landmarks/vertices and transitions into edges, enabling graph-based planning (value iteration, shortest-path) and subgoal extraction for long-horizon reasoning and improved interpretability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Graph-structured world models",
            "model_description": "Construct graph by clustering states (or using learned landmarks) to produce vertices and edges that capture state transitions; planning is performed on the graph (e.g., shortest-path, value iteration) and vertices can serve as subgoals for low-level controllers or action imitators.",
            "model_type": "abstraction / symbolic-latent hybrid (discrete graph over latent embeddings)",
            "task_domain": "Long-horizon planning, navigation, sparse-reward tasks, hierarchical control (cited across Maze2D, AntMaze, Kitchen, and general RL tasks)",
            "fidelity_metric": "Typically evaluated via downstream task performance (success rate, normalized D4RL scores) and sometimes next-state prediction or landmark reachability metrics; specific metrics vary by implementation.",
            "fidelity_performance": null,
            "interpretability_assessment": "Generally more interpretable than black-box latent predictors because nodes and edges map to human-understandable subgoals/landmarks and paths can be visualized; however interpretability depends on clustering and visualization methods.",
            "interpretability_method": "Subgoal extraction, visualization of graph/paths, mapping vertices to states/coordinates (UMAP, plotting), natural-language annotation (in this paper via LLMs).",
            "computational_cost": "Generally lower at high level (planning on compact graph) but requires upfront cost to learn encoder/latent representation and to build graph; specifics depend on clustering granularity and encoder complexity.",
            "efficiency_comparison": "Often more sample-efficient for long-horizon tasks compared to end-to-end training of hierarchical policies, since planning can reuse offline data; exact efficiency gains are implementation-dependent.",
            "task_performance": "Used successfully in prior work for long-horizon and sparse-reward tasks (cited improvements in planning and subgoal discovery); this paper reports strong D4RL numbers when using a graph-structured abstraction.",
            "task_utility_analysis": "Graph abstractions focus on task-relevant connectivity (reachability) rather than pixel-perfect predictions, which improves planning utility for long-horizon tasks; however, coarse abstraction can hurt low-level action fidelity.",
            "tradeoffs_observed": "Trade-offs include abstraction granularity vs. planning fidelity, dataset coverage vs. ability to find feasible paths, and computational cost of graph construction vs. online planning efficiency.",
            "design_choices": "Choices include clustering method and radius, latent-space metric, how to compute edge weights and rewards, whether transitions are stochastic/deterministic, and how to map graph actions to low-level controllers.",
            "comparison_to_alternatives": "Compared to end-to-end neural hierarchical policies or pure model-free methods, graph-structured models provide better interpretability and planning but can be limited by the dataset and lose fine-grained control fidelity.",
            "optimal_configuration": "Paper suggests balancing clustering radius (γ) to avoid excessively coarse graphs and including contrastive + hindsight objectives when learning the latent metric; no universal optimal configuration provided.",
            "uuid": "e1197.1",
            "source_info": {
                "paper_title": "Adaptive and transparent decision-making in autonomous robots through graph-structured world models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "WorldModels (Ha & Schmidhuber)",
            "name_full": "World models",
            "brief_description": "Early influential latent world-model paradigm where an agent learns a generative latent model of its environment to support imagination and planning.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World models (Ha & Schmidhuber)",
            "model_description": "Generative latent dynamics model (VAE + RNN-style dynamics) that compresses observations into a latent and learns predictive dynamics for planning and policy learning.",
            "model_type": "latent world model (generative)",
            "task_domain": "Simulated control tasks, video-game-like environments (original work used VizDoom / car racing, etc.)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Not emphasized in this paper; general world-model latent spaces can be difficult to interpret directly unless additional structure is imposed.",
            "interpretability_method": "Not discussed in current paper's mention.",
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Cited as motivation that latent world models enable perception and imagination training to improve agent performance; used as conceptual background for latent-space modeling.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": "Mentioned as a foundational latent world-model approach contrasted with graph-structured simplifications to mitigate long-horizon distortion.",
            "optimal_configuration": null,
            "uuid": "e1197.2",
            "source_info": {
                "paper_title": "Adaptive and transparent decision-making in autonomous robots through graph-structured world models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "DreamerV3",
            "name_full": "Mastering diverse domains through world models (DreamerV3)",
            "brief_description": "A modern latent world-model based agent that trains policies via imagination from a learned latent dynamics model; demonstrated across many domains.",
            "citation_title": "Mastering diverse domains through world models",
            "mention_or_use": "mention",
            "model_name": "DreamerV3",
            "model_description": "Latent dynamics model used to perform imagination-based policy learning and perception/imagination training; integrates representation learning and policy/value learning in latent space.",
            "model_type": "latent world model (generative/imaginative)",
            "task_domain": "Diverse control domains including games (cited in paper: Minecraft example) and continuous control benchmarks",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Not detailed in this paper; Dreamer-family models are typically black-box neural latent models with limited innate interpretability.",
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Cited as an example of how learned world models can enable rich perception and imagination training to address complex decision-making tasks.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1197.3",
            "source_info": {
                "paper_title": "Adaptive and transparent decision-making in autonomous robots through graph-structured world models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GSSM",
            "name_full": "Graph Structured Surrogate Model (GSSM)",
            "brief_description": "A Thompson-sampling based graph-structured surrogate model for environment dynamics that reportedly outperformed prior methods in predictive dynamics tasks.",
            "citation_title": "Model-based meta reinforcement learning using graph structured surrogate models",
            "mention_or_use": "mention",
            "model_name": "Graph Structured Surrogate Model (GSSM)",
            "model_description": "Graph-based surrogate model using Thompson-sampling for dynamics prediction and planning (details summarized from citation; paper references GSSM as prior work).",
            "model_type": "graph-structured surrogate (probabilistic)",
            "task_domain": "Model-based RL / predictive dynamics (referenced generically in related work)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Paper notes GSSM as demonstrating strong predictive performance; interpretability specifics not discussed in current paper.",
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": "Cited as outperforming state-of-the-art methods in dynamics prediction (qualitative statement from paper).",
            "task_performance": null,
            "task_utility_analysis": null,
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": "Mentioned among graph-structured model approaches; contrasted conceptually with the present paper's objectives (transparency and adaptability).",
            "optimal_configuration": null,
            "uuid": "e1197.4",
            "source_info": {
                "paper_title": "Adaptive and transparent decision-making in autonomous robots through graph-structured world models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "VMG",
            "name_full": "Value Memory Graph",
            "brief_description": "A graph-structured world model built from offline datasets that stores value information at graph nodes and uses graph-based planning for long-horizon, sparse-reward tasks.",
            "citation_title": "Value memory graph: a graph-structured world model for offline reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "Value Memory Graph (VMG)",
            "model_description": "Constructs graph from offline data with vertex-level value memories and uses the graph to generate subgoals and guide planning; designed for goal-conditioned, long-horizon offline RL.",
            "model_type": "graph-structured world model",
            "task_domain": "Goal-conditioned tasks, sparse-reward and long-horizon offline RL (e.g., navigation benchmarks)",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Provides moderate adaptability by reusing VMG with new reward functions, but paper notes VMG's low-level policy can suffer from inaccuracies due to abstraction granularity loss and lacks transparency in decision-making.",
            "interpretability_method": "Graph-based subgoal extraction and planning; interpretability is limited by abstraction granularity according to the paper.",
            "computational_cost": null,
            "efficiency_comparison": "Paper compares empirically and states VMG is practical and moderately adaptable; no quantitative compute comparisons provided here.",
            "task_performance": "Reported as a competitive baseline in D4RL comparisons (authors include VMG in Table 2 comparisons).",
            "task_utility_analysis": "Useful for long-horizon and sparse tasks, but abstraction granularity may degrade low-level policy accuracy and reduce transparency compared to the authors' LLM-augmented graph approach.",
            "tradeoffs_observed": "Abstraction granularity vs. low-level accuracy and transparency; VMG reuse of graph for new rewards offers flexibility but may lose fine control fidelity.",
            "design_choices": null,
            "comparison_to_alternatives": "Compared with the paper's method: both are graph-based but the present paper emphasizes LLM relabeling for task alignment and explicit explainability; VMG has reuse advantages but suffers from granularity-related low-level inaccuracies per the paper.",
            "optimal_configuration": null,
            "uuid": "e1197.5",
            "source_info": {
                "paper_title": "Adaptive and transparent decision-making in autonomous robots through graph-structured world models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Value memory graph: a graph-structured world model for offline reinforcement learning",
            "rating": 2,
            "sanitized_title": "value_memory_graph_a_graphstructured_world_model_for_offline_reinforcement_learning"
        },
        {
            "paper_title": "Model-based meta reinforcement learning using graph structured surrogate models",
            "rating": 1,
            "sanitized_title": "modelbased_meta_reinforcement_learning_using_graph_structured_surrogate_models"
        },
        {
            "paper_title": "Replay memory as an empirical MDP: combining conservative estimation with experience replay",
            "rating": 1,
            "sanitized_title": "replay_memory_as_an_empirical_mdp_combining_conservative_estimation_with_experience_replay"
        },
        {
            "paper_title": "Combo: conservative offline model-based policy optimization",
            "rating": 1,
            "sanitized_title": "combo_conservative_offline_modelbased_policy_optimization"
        }
    ],
    "cost": 0.01556,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Adaptive and transparent decision-making in autonomous robots through graph-structured world models</p>
<p>Site Hu 
Graduate School of Engineering Science
Osaka University
OsakaJapan</p>
<p>Takato Horii 
Graduate School of Engineering Science
Osaka University
OsakaJapan</p>
<p>Takayuki Nagai 
Graduate School of Engineering Science
Osaka University
OsakaJapan</p>
<p>Artificial Intelligence Exploration Research Center
The University of Electro-Communications
TokyoJapan</p>
<p>Adaptive and transparent decision-making in autonomous robots through graph-structured world models
6C2C02C6BD5F9E5C9E996818AF473F7E10.1080/01691864.2024.2415995Received 15 March 2024 Revised 11 July 2024 and 1 October 2024 Accepted 4 October 2024Autonomous agentsexplainabilityinterpretabilityoffline reinforcement learningworld models
The growing ubiquity of autonomous robots across different fields necessitates that agents adapt to diverse tasks and ensure transparency and intelligibility in their decision-making processes.This study presents a novel framework that combines a graph-structured world model with large language models (LLMs) to address these requirements.First, a latent space is created to capture the reachability between distinct states.Next, a graph is constructed within this latent space by clustering an offline dataset that effectively captures the complex dynamics of the environment.Subsequently, LLMs are employed to redefine the reward function and relabel the dataset, thereby establishing a well-defined Markov decision process based on the previously learned graph.This relabeling process ensures that the agent's decision space aligns with user intentions.Consequently, a predictive world model is obtained, offering insights into potential future states and facilitating graph-based planning.Moreover, by inputting the planned path of the agent in the graph-structured world model into LLMs, natural language explanations can be generated to provide transparency in the decision-making process.Experiments on the D4RL benchmark validated the effectiveness of our approach in long-horizon planning, its adaptability to different user tasks, and its inherent explainability.</p>
<p>Introduction</p>
<p>The introduction of autonomous robots to a wide array of applications has significantly altered task performance in modern society.Autonomous systems are increasingly being integrated into various fields, from healthcare to logistics [1,2].However, these systems are expected not only to perform tasks autonomously, but also to operate with a degree of explainability.This attribute is crucial for bridging the gap between human understanding and Artificial Intelligence (AI) decision-making processes [3][4][5].This explainability requirement arises from the escalating demand for transparency and accountability in AI systems.This is a pivotal factor in cultivating trust within human-robot interactions, as highlighted in [6].</p>
<p>World models [7] present a compelling solution to the challenge of enhancing decision-making transparency in autonomous systems.World models empower these systems to predict and interpret complex environments, thereby enabling more rational and transparent decisionmaking.For instance, DreamerV3 [8] leverages a world model derived from experience for rich perception and imagination training, facilitating autonomous agents in CONTACT Takato Horii takato@sys.es.osaka-u.ac.jp successfully addressing complex decision-making tasks, such as those encountered in Minecraft.Additionally, several studies [9][10][11] have focused on the development of offline reinforcement learning algorithms.These algorithms aim to minimize the extensive costs associated with online exploration by distilling an abstract world model from preexisting datasets.These datasets contained episodes rich in dynamic information pertinent to the original environment.Moreover, these algorithms often incorporate specific constraints into the model through modifications to the reward or value functions, which address the issue of a distributional shift.However, these world models tend to deviate from reality as the prediction horizon [12].Several previous studies have explored the development of simplified graph-structured world models of the original complex environments to mitigate this distortion and enhance performance [12][13][14].Graph-structured world models offer a robust framework that is capable of capturing diverse environmental dynamics with considerable accuracy.However, these approaches are often confined to specific tasks, and there have been limited investigation into the application of graph-structured world models to augment decision-making transparency in the realm of autonomous robotics.</p>
<p>In contrast to these existing methodologies, this study introduces a novel approach by integrating a graphstructured world model with the interpretive capabilities of large language models (LLMs).The distinctiveness of our work lies in the conceptualization of the decisionmaking process as a well-defined Markov Decision Process (MDP) defined on a graph that is an abstract of environmental dynamics.By utilizing LLMs, we ensure that the agent's decision-making process is not only aligned with the user's intentions but also rendered comprehensible to the user.This approach not only achieves transparency but also adapts to various tasks without retraining the neural network, offering a distinctive and effective solution to prevailing challenges.</p>
<p>Our proposed framework begins by learning a latent space that imitates the reachability between different states.After that, we construct a graph in this latent space by clustering the dataset.By adopting LLMs, we identified user intentions and relabeled the dataset to align the agents' actions with them.After relabeling, the decisionmaking process of the agent is abstracted into the MDP defined in this graph.The agent then employs this graphstructured world model for planning and explanation generation.</p>
<p>The contributions of this paper are threefold: introduction of an innovative frame that constructs a graph in latent space, delineation the reachability among distinct states, and application of self-imitation learning for effectively translating graph actions into agent actions.Next, we leverage LLMs to discern user intentions and relabel the dataset, thereby aligning the agent's decision-making with user preferences and enhancing task adaptability.Finally, our empirical evaluation in the Franka-kitchen domain of D4RL [15] substantiates both the adaptability and explainability of our approach.</p>
<p>Related works</p>
<p>Here, we discuss related works and the differences of previous methods with our proposed method.Relevant prior work includes studies on offline reinforcement learning (RL), hierarchical reinforcement learning (HRL), graphstructured models in RL, and explainable autonomous robots.Table 1 provides a comparative overview of the methods discussed in the following subsections and the proposed framework.</p>
<p>Offline reinforcement learning</p>
<p>Several innovative algorithms have been developed for offline reinforcement learning.In [11], a model-based offline RL algorithm called COMBO was developed, which introduced regularization into the value function.This algorithm demonstrated a consistent and superior performance relative to previous offline model-free and model-based methods, even in the context of complex tasks.In contrast, pessimistic algorithms such as CQL [16] and ATAC [17] focus on learning a conservative Qfunction.This approach was effective in mitigating the overestimation issues inherent in Q-learning.However, the performance of these algorithms can be compromised by their intrinsic excessive pessimism.</p>
<p>Another critical aspect of offline RL is the avoidance of out-of-distribution (OOD) actions and states.In this regard, IQL [18] is a method that allows the learned policy to surpass the best behavior in a dataset through generalization, without necessitating the evaluation of actions outside the dataset.In addition, recent advancements in generative modeling, particularly those based on diffusion models [19], have demonstrated competitive performance.These models have been successfully applied to both model-free [20] and model-based [21] paradigms.
× × × HRL MAPLE [22] × × Option discovery [23] × × × × × h-DQN [24] × × × HiGoC [25] × × HDMI [26] × × Graph-structured models GSSM [27] × × RM-MDP [14] × × × × × SoRB [28]/UVFA landmarks [29]/SFL [30]/L3P [12] × × × VMG [13] × Explainable autonomous robots (XAR) Sakai framework [32] × × × ×
XAR based on graph-structured world model [5] ×</p>
<p>Hierarchical reinforcement learning</p>
<p>HRL addresses long temporal problems by dividing tasks into high-level policy commands (abstract actions or skills) and low-level policies that convert these into concrete actions.A hierarchical policy with predefined primitives was trained in [22], but this limits scalability.The PCCA+ spectral clustering algorithm was used to partition the state-space and provide connectivity information in [23].This is effective even without an exact model, but is limited to small discrete state spaces.Similarly, h-DQN has been introduced by combining hierarchical value functions with intrinsically motivated deep RL [24], although its scalability to complex tasks is limited.A framework using a low-level goal-conditioned RL policy and a high-level goal planner has been proposed [25], but it faces challenges in adaptability across various tasks due to the complexity inherent in the training process.A hierarchical diffusion probabilistic model for long-horizon offline decision-making problems has been reported in [26].This method clusters similar states to identify subgoals, which are used to train the upper-level goal diffuser.However, the graph constructed in state space has unspecified cluster sizes, potentially causing metric space distortions and reducing performance.Moreover, these methods typically use two different neural networks to train the high-level and low-level policies individually.In contrast, our method considers the high-level policy as a graph action representing the transition between two nodes, and utilizes a trained neural network to convert the graph action into a lowlevel action.The high-level policy of our method is derived through graph-based planning with value iteration instead of training a neural network, enabling rapid adaptability across various tasks.</p>
<p>Graph-structured models in reinforcement learning</p>
<p>Graph-structured models have gained prominence as a formidable instrument in the field of Reinforcement Learning.Wang and van Hoof [27] introduced a Thompson-sampling based Graph Structured Surrogate Model (GSSM) which demonstrated superior performance over existing state-of-the-art methods in the prediction of environmental dynamics.A conservative estimate of Q has been obtained in [14] by building the replay buffer as a graph.However, this approach is limited in its applicability to continuous environments.Further, studies such as [12,[28][29][30] have developed graph-structured world models that encapsulate state transitions.Subsequently, graph-based planning was utilized for the generation of subgoals, serving as part of high-level policies.These approaches have endowed agents with the capability to reason over extended periods.However, their application in high-dimensional tasks has been constrained by low exploration efficiency.</p>
<p>In goal-conditioned tasks, value memory graph (VMG) has been proposed [13] using offline datasets, achieving competitive results, especially in sparse reward and long-horizon environments.While VMG shows practicality and moderate adaptability by reusing the VMG with new reward functions, its low-level policy suffers from inaccuracies due to abstraction granularity loss and lacks transparency in decision-making.</p>
<p>Explainable autonomous robots</p>
<p>The explainability of autonomous robots has been recognized as a pivotal element in enhancing user trust and the overall interaction experience.In a discussion on fair and accountable AI and robotics, [6] underscored the significance of explainability.In addition, [3] probed the typical forms of AI decision-making, delineating a set of principles and corresponding requirements that AI systems should respect for optimizing user satisfaction.However, their contributions, although theoretically robust, offered limited guidance for practical implementation strategies.</p>
<p>Similarly, [31] expounded the role of argumentation in fostering explainability within decision-making frameworks, advocating the amalgamation of Machine Learning and Argumentation Theory to yield more interpretable models.Despite the theoretical appeal of this integration, its practical application has encountered complexities.Further discussion [4] embarked on an exploration of explainability within the realm of autonomous robots and how it fits into the AI landscape.However, they left several questions unresolved regarding practical implementation.</p>
<p>Sakai et al. [32], Hu and Nagai [5] introduced a methodology that utilized MDPs to extract important scenarios, serving as an explanation for elucidating the agent's transition from the current state to the goal.Although this approach has demonstrated efficacy in specific domains, its applicability is limited to relatively simple environments.Moreover, explanations rendered as images at times pose challenges with respect to user comprehensibility.</p>
<p>Comparison of methods</p>
<p>The proposed framework integrates the advantages of existing methods by providing a graph-structured world model combined with large language models (LLMs) to enhance transparency, adaptability, and explainability across various tasks without requiring retraining.</p>
<p>Proposed method</p>
<p>To ensure adaptability of autonomous agents in complex environments, the ability to perform various tasks without frequent neural network retraining is essential.Furthermore, offering clear and comprehensible explanations for the agents' decisions is crucial for fostering user trust and transparency.To address these challenges, we propose a framework that integrates a graph-structured world model with LLMs.This integration ensures that the agent's actions align with user-defined goals, and facilitates the generation of natural language explanations, thereby enhancing transparency in the decision-making process.</p>
<p>As shown in Figure 1, our framework begins by learning a latent space that captures the state reachability.A graph is then constructed in this space by clustering an offline dataset that effectively represents the complex dynamics of the environment.Leveraging LLMs, we redefined the reward function and relabeled the dataset, establishing a well-defined MDP based on the graph akin to a predictive world model as described in [7].This graph-structured world model aids in the prediction of future states and supports graph-based planning algorithms.Within this model, the agent's planned trajectory identifies relevant subgoals that LLMs can articulate in natural language, clarifies the agent's decision-making processes, and enhancing action transparency.</p>
<p>Learn the latent space</p>
<p>In this study, we constructed a graph-structured world model within a latent space, using the L2 distance as a metric to measure state reachability, a concept defined in line with the framework in [5] as the number of time steps required for an agent to move from one state to another.</p>
<p>We introduce the Hindsight Contrastive Distance Alignment (HCDA) method, which combines contrastive learning with hindsight sampling to learn the latent space, as illustrated in Figure 2.This approach includes several components: a State Encoder (SE), f SE : s → z, maps states to the latent space; a Latent Displacement Predictor (LDP), f LDP : z, a → z, predicts the effect of an action in the latent space; and an Action Decoder (AD), f AD : z, z → a, converts latent space displacement back into agent actions.</p>
<p>For a specific state s l i in a transition sequence τ l within a batch, with the LDP, we can predict the latent displacement z i = f LDP (z l i , a l i ).Consistent with our definition of reachability, we aim to maintain the output of the LDP within a threshold of 1, ensuring that adjacent states remain close in the latent space:
L LDP = max(( z i ) 2 − 1, 0)(1)
The subsequent predicted state is given by zl
i+1 = z l i + f LDP (z l i , a l i ).
Following the approach of [13], the true subsequent state z l i+1 serves as the positive sample, whereas all other states z m j from different sequences in the batch are negative samples.Our goal is to minimize the distance between the predicted and true subsequent states (positive samples) and maximize the distance from irrelevant states (negative samples).This leads to the following contrastive loss formulation:
L contrast = L pos + L neg = (z l i+1 − z l i+1 ) 2 + max(1 − (z m j − zl i+1 ) 2 , 0)(2)
Then, random states s l i+h observed post s l i within the same episode are sampled.Here, h is sampled from {1, 10} to avoid the influence of the behavior policy that collects the offline dataset.Under hindsight, these states are expected to be within a maximum of h steps from z l i , leading to the formulation of the hindsight loss:
L hind = max((z l i+h − zl i+1 ) 2 − h + 1, 0)
In Addition, the deployment of AD, as delineated by Zhu et al. [13], has been empirically shown to yield transitions within the latent space that more accurately reflect agent actions:
L AD = (ã i − a i ) 2(3)
Here, ãi refers to the output of the AD, which represents the reconstructed agent actions from latent space displacement.The a i is the true value of the action.The cumulative training loss function within the latent space was formulated by amalgamating various previously delineated loss components:
L latent = L LDP + L contrast + L hind + L AD(4)
This integrative approach guarantees a comprehensive optimization strategy in the latent space.</p>
<p>Construct the graph</p>
<p>Upon completion of the latent space learning process, our framework is capable of projecting the entirety of the states from episodes within the offline training dataset into this newly defined latent space.States that exhibit proximity in terms of reachability are then aggregated to form graph vertices.</p>
<p>The procedure begins with an empty vertex set and a predetermined clustering threshold γ .In each iteration, a state s i is randomly chosen from the training dataset.</p>
<p>A hyper-sphere with center f SE (s i ) and radius γ is then formed.States within this hyper-sphere are merged into a single vertex and included in the vertex set to reflect the ease of transition between these states.These states are removed from the training dataset to prevent vertex set redundancy in future iterations.</p>
<p>This process is repeated until all states within the training dataset are allocated to specific vertices.For the state transitions (s i , a i , s i+1 ) in the training dataset, if s i and s i+1 correspond to vertices v j and v k , a directed edge from v j to v k is created.This results in a graph G = (V, ε), where vertices represent distinct states and directed edges denote transitions driven by actions.The algorithm steps are detailed in Appendix 1.</p>
<p>Construct the graph-based world model</p>
<p>In the preceding section, we developed a graph representing environmental dynamics and then relabeled the rewards to align the agent's actions with user intentions.This process conceptualizes the agent's decision-making process as an MDP, underscoring the crucial role of LLMs.</p>
<p>The user-specified tasks, denoted by T, are pivotal for directing the actions of the agent.To extract these tasks accurately, we employed the advanced LLM GPT-4 [33] developed by OpenAI.GPT-4's exceptional capabilities in language understanding and generation enable the effective conversion of user input U into a coherent task list T. The complete prompts are presented in Appendix 2.
T = LLMsTaskExtraction(U)(5)
In the Kitchen domain, T represents the tasks the user wants to complete (one or more of the seven tasks).For the AntMaze or Maze2D domains, T represents the target coordinates that the user wishes the agent to reach.For the Adroit domain, the use of LLMs for this purpose has not been pursued because of the complexities in linguistically specifying user-specific task objectives.Instead, the inherent goals of the environment were used for relabeling.The transformation of user instructions from natural language into specific agent goals serves two essential purposes.This ensures that the agent's actions are in line with the user's intentions.Additionally, it bridges the gap between human language and machine-executable commands, thereby fostering a more integrated and userfriendly interface between users and agents.</p>
<p>The reward structure for the training data was recalibrated to reflect the user-defined task list, with reward r i for each transition (s i , a i , s i+1 ) varying across the domains.In the Kitchen domain, the reward is defined as follows:
r i = NumTasksCompleted(s i+1 , T) (6)
whereas in the AntMaze, Maze2D, and Adroit domains, the criteria are as follows:
r i = 1 if d(s i+1 , T) ≤ δ 0 otherwise.(7)
This formulation provides a clear and concise method for determining rewards in various environments.Here, NumTasksCompleted(s, T) counts the tasks from task list T that are fulfilled by state s.This framework allows for the representation of transitions within the dataset as a graph-structured MDP, as expounded in [13,34], parameterized by (V, A, P, R, γ ), where V represents the vertices serving as MDP states and γ ∈ (0, 1) denotes the discount factor.Additionally, we define the available action list for a state v j as the transitions from v j to all other vertices v k in the graph.These transitions are considered as graph actions and represented as a graph = v j → v k .These graph actions can be used to reconstruct the agent's actions using the action imitator, which will be discussed later.The transition probability P(v k | v j , a graph ) is defined as:
P(v k | v j , a graph = v j → v k ) = 1 if e v j →v k exists 0 otherwise. (8)
The reward function is then defined as:
R v k →v k = max{r i | (s i , a i , s i+1 , r i ) ∈ D, s i ∈ v k , s i+1 ∈ v k } (9) R v j →v k = avg{r i | (s i , a i , s i+1 , r i ) ∈ D, s i ∈ v j , s i+1 ∈ v k } + R v k →v k (10)
Where R v k →v k represents the maximal reward for selfloop transitions at vertex v k .In the Kitchen domain, this reward reflected the anticipated number of tasks completed at that vertex.In other domains, this indicates whether the target goal is encompassed within the vertex.This reward is combined with the transition reward between distinct vertices to create a balanced reward structure, which encourages movement towards vertices that enable the completion of more tasks.Fundamentally, this MDP constitutes our world model, encapsulating both the dynamics of the environment and the user-specified objectives, thereby ensuring that an agent's behavior is congruent with user preferences.</p>
<p>Plan and explain based on the world model</p>
<p>Planing based on the world model</p>
<p>Abstracting the environment into a succinct MDP allows for a straightforward implementation of classical value iterations, as detailed in [35].This approach circumvents the extensive training of deep neural networks for value function approximation, thereby facilitating swift state value estimation.States aligned with the maximal task completion or the target goal, as defined in 9, are pinpointed, with the highest-value state becoming the target.</p>
<p>Subsequently, the graph vertex nearest the current state of the environment was defined as the initial state.The calculation of edge weights in the graph involved summing the deviation of the maximum value function from the value at the target node of the edge and the length of the edge.Using Dijkstra's algorithm [36], an optimal trajectory path = (v 0 , v 1 , . . ., v n ) is established, guiding the agent to achieve maximal task completion within minimal steps.</p>
<p>The constructed graph and the agent's path in the latent space is depicted in Figure 3.The color shades represent the vertex values, with lower values indicated by darker purple and higher values by yellow.More detailed visualizations can be found in Appendix 3.</p>
<p>executing based on the planned graph path</p>
<p>The planned state transition sequence can be obtained from the planned path.We propose self-Imitating goalconditioned behavior cloning (ICBC) to train an action imitator f ICBC : s, g → a, converting planned state transitions into agent actions.By leveraging the optimal substructure principle in goal-conditioned reinforcement learning [37], any segment of an optimal path to a goal is also optimal to its endpoint.This suggests that goal-conditioned policies can be effectively replaced by policies conditioned on intermediate 'subgoals' along  Since any segment of an optimal path from the start state s to the final goal g is also optimal with respect to its subgoal sg i , a goal-conditioned policy can be distilled into a subgoalconditioned policy through self-imitation.Concurrently, an action imitator is developed, employing the self-imitation loss L imit in conjunction with the loss function inherent to goal-Conditioned Behavior Cloning (CBC).</p>
<p>the agent-goal trajectory.Consequently, we employ selfimitation to learn goal-conditioned policies, as shown in Figure 4.</p>
<p>With hindsight relabeling, we can derive the goal-Conditioned Behavior Cloning (CBC) loss for the action imitator f ICBC as follows:
L cbc = (f ICBC (s i , s i+h ) − a i ) 2(11)
Then, our methodology promotes action alignment between goal-and subgoal-conditioned policies by treating path nodes as subgoals.The self-imitation loss is defined as:
L imit = 1 h − 1 i=1,...,h−1 (a h − SG(a i )) 2 (12)
where SG represents the stop-gradient operation.This loss component, combined with the CBC loss function to encourage the goal-conditioned policy to imitate the more reasonable actions generated by a closer subgoal-conditioned policy, forms the ICBC loss: (13) with λ set to 10 − 3, following [37].Thus, the action imitator effectively translates state transitions into the corresponding agent actions.
L ICBC = L cbc + λL imit</p>
<p>Explain based on the world model</p>
<p>As the agent traverses the planned path within the graphstructured world model, its decision-making becomes transparent to the user.In the Kitchen domain, a key vertex v i is identified where
R v i →v i &gt; R v i−1 →v i−1 .
This is determined based on the self-loop reward associated with each vertex along the planned path, indicating new task completion at v i , which is viewed as a subgoal.For the AntMaze and Maze2D domains, the subgoals correspond to the agent's coordinates at each vertex on the path.This approach allows the inference of a detailed plan for the agent.To further augment the interpretability of this plan, explanations can be generated using LLMs.These explanations bridge the gap between the operational logic of the agent and the understanding of the user, thereby enhancing the transparency and explainability of the actions and strategies of the agent.</p>
<p>Experiments</p>
<p>In this section, we compare our novel world model generation method with several state-of-the-art offline reinforcement learning methods using the popular D4RL benchmark [15].We then assessed the adaptability of our method to diverse user intents and its interpretability.This is illustrated through two comprehensive examples demonstrating the proficiency of the model in adapting to and fulfilling varied user-defined goals and its ability to leverage the adjusted world model to explain the agent's decision-making process.</p>
<p>Performance evaluation on D4RL environments</p>
<p>Datasets.We considered four different task damains in the D4RL benchmark: Maze2D, AntMaze, Kitchen, and Adroit.The Maze2D and AntMaze domains are navigation tasks requiring a 2D agent or an 8-DoF 'Ant' quadruped robot to reach a fixed goal location.The Maze2D domain was designed to evaluate the ability to find the shortest path to the evaluation goal.In contrast, the AntMaze domain used a sparse 0-1 reward, which was activated upon reaching the goal.Both domains present considerable challenges owing to the prevalence of suboptimal trajectories in their datasets.</p>
<p>The Kitchen domain involved the agent completing four distinct sub-tasks within a single episode, each requiring interaction with an object to achieve a specific configuration.This domain requires the stitching of various suboptimal trajectories to find a complex path through the state space toward an overarching goal.It included three human demonstration datasets, as proposed in [38].The 'complete' dataset features the robot executing all tasks sequentially, whereas the 'partial' and 'mixed' datasets contain undirected data, with the robot performing subtasks unrelated to the goal configuration.The 'partial' dataset guarantees a subset of trajectories that successfully complete the task, in contrast to the 'mixed' dataset, which lacks any comprehensive taskcompleting trajectories.</p>
<p>In the Adroit domain, the agent is required to manipulate a robotic hand to perform intricate tasks such as rotating a pen or opening a door, with the provision of dense rewards.The Adroit datasets primarily originate from human demonstrations, leading to a state-action space narrowly represented in the offline data.</p>
<p>In Table 2, we compare the performance of our method with those of other offline RL methods for D4RL tasks.</p>
<p>Baselines.We compare our method with Behavior Cloning (BC) and six other SOTA offline reinforcement learning methods: CQL [16], IQL [18], ATAC [17], COMBO [11], VMG [13], and Diffusion-QL [20].</p>
<p>Performance.The results are summarized in Table 2.The comparison is based on the original task settings in the dataset.Consequently, the relabeled dataset generated by the proposed method remains consistent with the original dataset.This ensures the fairness of the comparison and aligns the performance evaluation of our method with that of the baseline methods.Our findings highlight that the proposed method of leveraging a graphstructured world model for planning effectively assists agents in making informed decisions for long-horizon tasks.The most pronounced performance enhancement was observed in the kitchen-partial environment, which was attributed to its larger dataset.This extensive dataset facilitated the construction of a more precise graph.Moreover, the subset of this dataset ensures task resolution, allowing our method to identify graph vertices that complete all tasks efficiently, and facilitated optimal path planning.</p>
<p>In the contexts of Maze2D and AntMaze, our method also achieves competitive performance with a similar graph-based method, VMG, and surpasses other SOTA algorithms, including recent advancements in diffusionbased SOTA algorithms.This underscores the robustness of our method and its ability to harness intrinsic graph structures for enhanced decision-making.</p>
<p>In contrast to navigation tasks, adroit tasks are more challenging, primarily because of the narrow expert data distributions and intricacies of the high-dimensional state space.Despite these challenges, our method still achieves competitive or leading results compared with Performance across various tasks is normalized to a scale ranging from 0 to 100, where a score of 100 is indicative of the performance level of an 'expert' policy.</p>
<p>other methodologies.Overall, our approach consistently outperformed existing SOTA algorithms, underscoring its performance in a variety of task environments.</p>
<p>Adaptability and interpretability across tasks</p>
<p>By leveraging LLMs to identify the user's intended tasks, our method enables rapid relabeling of the original dataset to reflect user preferences and reevaluating its value function within mere seconds.LLMs generate a task list tailored to user specifications, outlining the specific goals expected of the agent.For example, in the Kitchen domain (see Figure 5), the original task list for the kitchen-partial environment included actions such opening the microwave, placing the kettle on the top burner, turning on the light switch, and opening the slide cabinet.However, when the user's intent changes to include tasks such as activating the bottom burner and light, accessing the slide cabinet, and opening the microwave, the agent dynamically updates its world model to align with the revised task list extracted from the LLM.This demonstrates the adaptability of the agent to evolving user preferences.This revised world model underpins the agent's graph-based planning, enabling the identification of critical subgoals and their subsequent communication with the user via LLMs for transparent user interaction.User feedback can then be utilized for further dataset relabeling, ensuring the alignment of the agent's plan with the user's intentions.</p>
<p>A further illustration of this is shown in Figure 6. 1 After relabeling in response to the user's intent, the agent determines it cannot fulfill the user's request due to the absence of a feasible path to a vertex that accomplishes all The normalized score is defined as the ratio of the number of tasks finished by the agent to the number of tasks anticipated for completion by the user.The task list * in the last line is the original task list in kitchen-partial environment.</p>
<p>tasks.Here, the graph-structured world model not only provides information about an agent's affordance but also enhances its explanatory interactions with the user.The agent communicates its constraints via LLMs, such as the inability to open both the slide and hinge cabinets simultaneously, thus guiding the user toward a more viable goal selections.Following the user's confirmation or modification of the goal (opening the microwave, turning on the light switch, and opening the hinge cabinet), the agent updates its world model and executes a revised task.In this scenario, the agent turns on the top burner.Based on the inherent world model, the agent surmised that it was necessary to turn on the top burner before turning on the light switch and accessing the hinge cabinet.This behavior can be attributed to the biases inherent in the collected dataset.More results can be found in Table 3, which highlights the adaptability of the model to user-specified tasks without necessitating neural network retraining.</p>
<p>Ablation study</p>
<p>We conduct a series of ablation studies to evaluate the effectiveness of various components in our method.Specifically, we examine the impact of four factors on the performance of our approach: the radius used in graph construction, importance of contrastive loss, importance of hindsight loss, and importance of self-imitation loss.</p>
<p>Radius in graph construction</p>
<p>The radius γ influences the size of the graph by determining the number of states to be included in one vertex.</p>
<p>As the L2 distance in the latent space represents the state reachability, we empirically set the radius to 0.5, meaning that the distance between the centers of two vertices should be greater than 1.A larger γ will reduce the number of vertices in the graph at the cost of potentially decreasing the performance.We vary the radius used in the graph construction process to understand its effect on model performance.Figure 7 shows the results, indicating that the method is not highly sensitive to γ when it is not significantly large.However, a hyperparameter sweep of the radius can be performed to achieve the best performance for each task.</p>
<p>Contrastive loss</p>
<p>The contrastive loss L contrast is a crucial component of HCDA for learning a meaningful metric space.It encourages states that are mutually easily reachable to be close to each other in the latent space, while states that are difficult to reach remain far apart.We verify its importance by removing L contrast from HCDA.As shown in Table 4, the variant without contrastive loss fails completely (0 scores) in the 'antmaze' tasks, and performance in 'maze2d' and 'kitchen' tasks decreases significantly.The performance in 'pen' tasks does not drop significantly, probably due to the small dataset and narrow data distribution that prevent several dissimilar states from being close together in the latent space.These results underscore the importance of contrastive loss in learning a robust metric space.</p>
<p>Hindsight loss</p>
<p>The hindsight loss, L hind , is an auxiliary loss term that ensures states that can be reached within a few steps from each other are not too far apart in the latent space.We verify the corresponding contributions by removing L hind from our approach.As shown in Table 4, adding hindsight loss significantly improved the performance in 'maze2d' tasks and also enhanced the performance in most other environments, while the performance remained comparable in 'antmaze-umaze', 'antmazeumaze-diverse', and 'pen-human' tasks.This suggests that the latent space learned solely by contrastive loss can easily push less similar states far apart, and the hindsight loss term mitigates this effect.</p>
<p>Self-Imitation loss</p>
<p>The self-imitation loss, L imit , encourages the goalconditioned policy to learn reasonable actions produced by closer subgoal-conditioned policies.We verify it's effectiveness by removing L contrast from HCDA.As shown in Table 4, removing the self-imitation loss results in decreased performance in most tasks.This indicates that self-imitation loss improves performance by enhancing the correctness of planning.</p>
<p>Discussion</p>
<p>In this study, we develop a graph-structured world model by clustering states from an offline dataset into vertices and abstracting the state transitions as graph edges.In addition, LLMs are used to decipher user intentions, thereby aligning the agent's decision-making process with user intentions through dataset relabeling.However, a critical limitation of this method is its reliance on a dataset for representing environment dynamics.Consequently, the model's capacity to learn from out-ofdistribution transitions and to generalize to tasks absent from the dataset is limited.This issue was particularly evident in the kitchen-mix dataset, where the absence of trajectories completing all tasks imposed an inherent ceiling on the normalized score achievable by any plan, with the maximum score being 75/100, indicating the completion of three out of four tasks.This limitation becomes more pronounced in datascarce environments such hammer and door tasks in the Adroit domain, where the effectiveness of the model is significantly hampered.Moreover, although our approach effectively leverages LLMs to discern user intentions and generate explanations in natural language based on planned paths within a graph-structured world model, it does not fully exploit the potential of LLMs for high-level planning.A promising direction for future research involves using LLMs to generate high-level plans that encourage agents to explore out-of-distribution states and dynamically update graphstructured world models.Such a strategy can overcome the current generalization and adaptability limitations, thereby broadening the utility of the model across diverse tasks and environments.</p>
<p>Conclusion</p>
<p>This paper presents a novel framework that combines graph-structured world models with the interpretive power of LLMs.This approach not only enhances decision-making transparency but also ensures adaptability to diverse tasks without the need to retrain the neural network.Our methodology begins with the construction of a latent space that mirrors state reachability, followed by the utilization of LLMs to discern user intentions and relabel the dataset accordingly.This process aligns the agent's actions with user preferences, thereby enhancing the adaptability of the system.The agent's decision-making process is then abstracted into the MDP defined in this graph, which is used for both planning and providing explanations of its actions.Notably, applying self-imitation learning capitalizes on the optimal substructure property, enhancing accuracy in translating planned state transitions into agent actions.Our empirical evaluation of several D4RL domains demonstrated the adaptability and explainability of our approach, affirming its effectiveness.</p>
<p>Although our presented work demonstrated promising results in D4RL environments, the scalability of our framework, its efficacy in larger and more complex environments, and its performance with smaller datasets warrant additional exploration.One promising direction involves the exploration of various clustering techniques during the construction of the graph within the latent space.Additionally, capitalizing on the inferential capabilities of LLMs to extrapolate out-of-distribution transitions from existing datasets presents a strategic approach for overcoming the challenges posed by out-ofdistribution scenarios, which remains another promising trajectory for future research endeavors.We hope that our findings inspire further research and contribute to making autonomous robots more understandable and reliable entities in society.</p>
<p>include reinforcement learning and explainable autonomous robots (XAR).</p>
<p>Takato Horii received the M.E.and Ph.D. degrees in engineering from Osaka University, Osaka, Japan, in 2013 and 2018, respectively.He was a Project Researcher with the University of Electro-Communications from 2017 to 2019.He was then an Assistant Professor with Osaka University from 2019 to 2020 and has been an Associate Professor with Osaka University in 2020.His current research interests include computational modeling of human cognitive functions such as emotion, tactile sensation, and creativity, and machine learning algorithms.Your primary function involves interpreting user directives and converting them into executable actions, presented in the form of a Python list.This list should commence with the statement 'Understood, here is the tasks list: '.It is imperative that the tasks in the outputted list precisely mirror those in the predefined task array.In instances where a user requests an action that falls outside your programmed capabilities, such as closing a device, the response should be formatted in a Python list, preceded by 'I'm sorry, [that task] is not supported', with [that task] replaced by a list of all unfeasible tasks.Additionally, you are tasked with discerning user consent for task execution.Upon confirmation of user consent, the response 'Starting execution.'should be issued.Conversely, in scenarios where consent is not granted, or the user's intent remains ambiguous, a request for further clarification is warranted.</p>
<p>Takayuki</p>
<p>B.1.1. Prompts for kitchen domain</p>
<p>Query: Turn on the bottom burner and move the kettle.Answer: Understood, here is the tasks list: ['turn on the bottom burner', 'move the kettle to the top burner position'].</p>
<p>Query: Close the bottom burner and move the kettle.Answer: I'm sorry, close the bottom burner is not supported.Here is the tasks list:[ 'move the kettle to the top burner position'].</p>
<p>Query: Yes, please go ahead with that.Answer: Starting execution.Query: No, I changed my mind.Answer: Understood.Let me know if you change your mind.Query: Maybe later.Answer: Sure.Let me know when you're ready.</p>
<p>B.1.2. Prompts for maze domain</p>
<p>Full prompt:</p>
<p>You are programmed as an automated navigational assistant operating in a maze environment.Your primary function is to interpret user commands related to movement and translate these into specific two-dimensional coordinate outputs.For example, if your current position is (1,2) and the user instructs you to move up by 3cm, your task is to calculate and output the new position, such as (4,2).Each response should begin with 'Understood, target position: '.Ensure that the output coordinates accurately reflect the movement instructed by the user.In cases where a user's command exceeds the maze's limits, respond with 'I'm sorry, target position: [calculated position] is invalid.',with [calculated position] replaced by the calculated position.Additionally, you are tasked with discerning user consent for executing the movement.Upon confirmation of user consent, the response 'Starting execution.'should be issued.Conversely, in scenarios where consent is not granted, or the user's intent remains ambiguous, a request for further clarification is warranted.</p>
<p>Info: Your current position is (0,0), and the dimensions of the maze are specified as 25cm by 25cm.Query: Move right by 10cm and move up by 5cm Answer: Understood, target position: [10,5].</p>
<p>B.2. Explanation</p>
<p>Upon extracting the target objective from the user input, the agent refreshes its world model and proceeds to formulate a feasible plan.If a feasible path is identified, the subgoals extracted via the graph-structured world model are then furnished to the LLMs to explain the agent's entire plan or the subsequent step to the user.Conversely, in the absence of a feasible path, alternative plans are presented to the LLMs.Corresponding explanations were then generated, elucidating the reasons for the infeasibility of the initial plan and guiding the user towards selecting an achievable goal.This guidance is based on the difference between the user's initial task list and the alternative tasks proposed by the world model.</p>
<p>Full prompt:</p>
<p>You are programmed as an automated robot programmed for operation within a specialized environment.Your primary role involves the interpretation and execution of user-defined objectives.This entails a systematic analysis of the action plan derived from user input.If the user asks you to complete an unachievable task list, it is essential for you to convey the inability to fulfill these tasks comprehensively.Moreover, in situations where the user seeks understanding of the reasons behind this limitation, your responsibility extends to guiding them towards establishing a viable goal.This guidance should be informed by a comparative analysis of the user's initial task list against the alternative task lists that can be feasibly executed.Your objective is to ensure that the user is well-informed and assisted in redirecting their focus towards achievable tasks.</p>
<p>Figure 1 .
1
Figure 1.An overview of the proposed method.</p>
<p>Figure 2 .
2
Figure 2. Illustration of (a) sampling a batch of transition sequences for contrastive learning and (b) training the SE, LDP, and AD to learn the latent space where the L2 distance serves as a reflection of reachability between distinct states.</p>
<p>Figure 3 .
3
Figure 3.An example of the constructed graph and the agent's path within the 'antmaze-medium-diverse' environment.The starting position is denoted by a blue circle, and the target position is marked with a red circle.A green line represents the agent's path, which follows the optimal path as determined by Dijkstra's algorithm.The color shades represent the vertices values, with lower values indicated by darker purple and higher values by yellow.The vertices are strategically aligned with corresponding maze locations to elucidate the spatial relationships.Notably, vertices proximal to the target position exhibit increased values, as calculated by value iteration.</p>
<p>Figure 4 .
4
Figure 4. Illustration of self-Imitating goal-conditioned Behavior Cloning (ICBC).Since any segment of an optimal path from the start state s to the final goal g is also optimal with respect to its subgoal sg i , a goal-conditioned policy can be distilled into a subgoalconditioned policy through self-imitation.Concurrently, an action imitator is developed, employing the self-imitation loss L imit in conjunction with the loss function inherent to goal-Conditioned Behavior Cloning (CBC).</p>
<p>Figure 5 .
5
Figure 5.The agent executes tasks based on the user's intent and provides explanations.</p>
<p>Figure 6 .
6
Figure 6.The agent knows what it can do with the help of relabelling.</p>
<p>Figure 7 .
7
Figure 7. Ablation study for radius in graph construction.</p>
<p>Figure 7 .
7
Figure 7. (Continued).</p>
<p>You are programmed as an automated robot operating within an kitchen environment.Your capabilities encompass a specific array of tasks, namely: ['open the bottom burner', 'open the top burner', 'turn on the light switch', 'open the slide cabinet', 'open the hinge cabinet', 'open the microwave', 'move the kettle to the top burner position'].</p>
<p>Query: Move left by 10cm and move up by 5cm Answer: I'm sorry, target position: [-10, 5] is invalid.'].Query: Yes, please go to that point.Answer: Starting execution.Query: No, I changed my mind.Answer: Understood.Let me know if you change your mind.Query: Maybe later.Answer: Sure.Let me know when you're ready.</p>
<p>Info: Your action plan comprises a systematic sequence as follows: ['open the microwave', 'move the kettle to the top burner position', 'turn on the light switch', 'open the slide cabinet'].</p>
<p>Figure A2 .
A2
Figure A2.Visualizations of the constructed graph for 'antmaze-large-play' dataset.</p>
<p>Figure A3 .
A3
Figure A3.Visualizations of the constructed graphs for the 'kitchen-partial' dataset illustrate the efficacy of our method.When the user's intended tasks change, our approach enables rapid relabeling of the original dataset to reflect user preferences and reevaluation of the value function within seconds, as demonstrated in figure (b).</p>
<p>Figure A4 .
A4
Figure A4.Visualizations of the constructed graph for 'pen-human' dataset.</p>
<p>Table 1 .
1
Comparison of the proposed method and previous works.
ContinuousComplexLong-horizonFlexibility toFrameworkMethodenvironmenttaskstasksdifferent tasksExplainabilityOffline RLCOMBO[11]/CQL[16]/ATAC[17]/IQL [18]/Diffusion [20,21]</p>
<p>Table 2 .
2
Experimental results on domains Maze2D, Kitchen, AntMaze, and Adroit from D4RL benchmark.
DatasetBCCQLIQLATACCOMBOVMGDiffusion-QLOur methodmaze2d-umaze3.85.7---134.8-124.9maze2d-medium30.35.0---134.6-159.9maze2d-large5.012.5---198.5-191.0maze2d-total39.123.2---467.9-475.8antmaze-umaze54.674.087.576.072.093.793.491.0antmaze-umaze-diverse45.684.062.287.00.094.066.290.5antmaze-medium-play0.061.271.20.00.082.776.684.0antmaze-medium-diverse0.053.770.01.01.084.378.692.0antmaze-large-play0.015.839.60.00.067.346.471.3antmaze-large-diverse0.014.947.50.00.074.356.669.3antmaze-total100.2303.6378.0164.073.0496.3417.8498.1kitchen-complete65.043.862.555.00.873.084.075.4kitchen-partial38.049.846.335.024.568.860.596.5kitchen-mixed51.551.051.032.525.050.662.671.7kitchen-total154.5144.6159.8122.550.3192.4207.1243.6pen-human63.937.571.5--70.772.887.1pen-cloned3739.237.3--58.257.375.1hammer-human1.24.41.4--4.1-6.2hammer-cloned0.62.12.1--2.2-2 . 0door-human2.09.94.3--1.5-4.7door-cloned0.00.41.6--2.2-0 . 2adroit-total104.793.5118.2--138.9-175.3</p>
<p>Table 3 .
3
Normalized scores for user-selected task lists.
NormalizedUser task listscore['bottom burner', 'slide cabinet', 'microwave', 'kettle']1.0['bottom burner', 'hinge cabinet', 'microwave', 'kettle']1.0['top burner', 'light switch', 'hinge cabinet', 'microwave']0.81['top burner', 'light switch', 'microwave', 'kettle']0.9[light switch', 'slide cabinet', 'hinge cabinet', 'microwave']0.7['light switch', 'hinge cabinet', 'microwave', 'kettle']0.89['slide cabinet', 'hinge cabinet', 'microwave', 'kettle']0.91['bottom burner', 'light switch', 'slide cabinet', 'microwave'] ['microwave', 'kettle', 'light switch', 'slide cabinet']  *0.92 1.0</p>
<p>Table 4 .
4
Ablation study of the loss items.
Ourw/o contrastivew/o hindersightw/o self-imitatingDatasetmethodlosslosslossmaze2d-umaze124.90.081.079.6maze2d-medium159.936.872.787.6maze2d-large191.012598119maze2d-total475.8161.8251.7286.2antmaze-umaze91.00.090.893.7antmaze-umaze-diverse90.50.095.292.1antmaze-medium-play84.00.069.773.2antmaze-medium-diverse92.00.072.176.3antmaze-large-play71.30.062.965.4antmaze-large-diverse69.30.061.364.7antmaze-total498.10.0452.0465.4kitchen-complete75.413.866.370.8kitchen-partial96.525.063.891.6kitchen-mixed71.71.2538.869.5kitchen-total243.640.05168.9231.9pen-human87.174.192.183.7pen-cloned75.168.649.164.2pen-total162.2142.7141.2147.9</p>
<p>Nagai received the B.E., M.E., and Ph.D. degrees from the Department of Electrical Engineering, Keio University, in 1993, 1995, and 1997, respectively.Since 1998, he has been with The University of Electro-Communications. From 2002 to 2003, he was a Visiting Scholar at the Department of Electrical Computer Engineering, University of California at San Diego, San Diego, CA, USA.Since 2018, he has been a Professor with the Graduate School of Engineering Science, Osaka University.He currently works as a Specially-Appointed Professor at UEC AIX, a Visiting Researcher at the Tamagawa University Brain Science Institute, and a Visiting Researcher at the AIST AIRC.His research interests include intelligent robotics, cognitive developmental robotics, and robot learning.He aims at realizing flexible and general intelligence like human by combining AI and robot technologies.He has received the IROS Best Paper Award Finalist, the Advanced Robotics Best Paper Award, and the JSAI Best Paper Award.</p>
<p>FundingThis work was supported by the New Energy and Industrial Technology Development Organization (NEDO) and JST Moonshot Research and Development Program [grant number JPMJMS2011].Disclosure statementNo potential conflict of interest was reported by the author(s).Notes on contributorsSiteHuB.1. LLMsTaskExtractionLLMsTaskExtraction(U) takes user instruction U and generates a coherent subtask list T that is used to relabel the dataset.In the Kitchen domain, T represents the tasks the user wants to complete (one or more of the seven tasks).For the AntMaze or Maze2D domains, T represents the target coordinates that the user wishes the agent to reach.Appendix C. Visualization of Constructed graphsWe provide additional visualizations of the constructed graphs for the 'maze2d-large' (FigureA1), 'antmaze-large-play' (FigureA2), 'kitchen-partial' (FigureA3), and 'pen-human' datasets (FigureA4).For 'maze2d-large' and 'antmaze-large-play', the vertices are strategically aligned with the corresponding maze locations to elucidate the spatial relationships.In the 'kitchen-partial' and 'penhuman' datasets, UMAP[39]is employed to reduce the dimensionality of the latent vertices to two-dimensional coordinates for better visualization.The color shades represent vertex values, with lower values indicated by darker purple and higher values by yellow.Notably, vertices near the target position exhibit increased values, as determined by value iteration.The agent's path in the graph-structured world model is depicted as a green path, with the initial state marked by a blue circle and the goal by a red circle.
A demonstration video can be. </p>
<p>Combating COVID-19 -the role of robotics in managing public health and infectious diseases. Sci Robot. G Z Yang, B Nelson, R Murphy, 10.1126/scirobotics.abb5589202055589</p>
<p>A review of further directions for artificial intelligence, machine learning, and deep learning in smart logistics. M Woschank, E Rauch, H Zsifkovits, 10.3390/su12093760Sustainability. 12937602020</p>
<p>Explanations in artificial intelligence decision making: a user acceptance perspective. N G Vinson, H Molyneaux, J D Martin, Handbook of research on humancomputer interfaces and new modes of interactivity. IGI Global2019</p>
<p>Explainable autonomous robots: a survey and perspective. T Sakai, T Nagai, 10.1080/01691864.2022.2029720Adv Robot. 365-62022</p>
<p>Explainable autonomous robots in continuous state space based on graph-structured world model. S Hu, T Nagai, Adv Robot. 37162023</p>
<p>Transparent, explainable, and accountable AI for robotics. Sci Robot. S Wachter, B Mittelstadt, L Floridi, 10.1126/scirobotics.aan6080201726080</p>
<p>. D Ha, J Schmidhuber, arXiv:1803101222018World models. arXiv preprint</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, arXiv:230 104104. 2023arXiv preprint</p>
<p>Mopo: model-based offline policy optimization. T Yu, G Thomas, L Yu, Adv Neural Inf Process Syst. 332020</p>
<p>Settling the sample complexity of model-based offline reinforcement learning. G Li, L Shi, Y Chen, arXiv:220405275. 2022arXiv preprint</p>
<p>Combo: conservative offline model-based policy optimization. T Yu, A Kumar, R Rafailov, Adv Neural Inf Process Syst. 342021</p>
<p>World model as a graph: learning latent landmarks for planning. L Zhang, G Yang, B C Stadie, International Conference on Machine Learning. 2021</p>
<p>Value memory graph: a graph-structured world model for offline reinforcement learning. D Zhu, L E Li, M Elhoseiny, arXiv:220604384. 2022arXiv preprint</p>
<p>Replay memory as an empirical MDP: combining conservative estimation with experience replay. H Zhang, C Xiao, H Wang, The Eleventh International Conference on Learning Representations. 2022</p>
<p>D4RL: datasets for deep data-driven reinforcement learning. J Fu, A Kumar, O Nachum, arXiv:200407219. 2020arXiv preprint</p>
<p>Conservative qlearning for offline reinforcement learning. A Kumar, A Zhou, G Tucker, Adv Neural Inf Process Syst. 332020</p>
<p>Adversarially trained actor critic for offline reinforcement learning. C A Cheng, T Xie, N Jiang, International Conference on Machine Learning. 2022</p>
<p>Offline reinforcement learning with implicit q-learning. I Kostrikov, A Nair, S Levine, arXiv:21100 6169. 2021arXiv preprint</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Adv Neural Inf Process Syst. 332020</p>
<p>Diffusion policies as an expressive policy class for offline reinforcement learning. Z Wang, J J Hunt, M Zhou, arXiv:220806193. 2022arXiv preprint</p>
<p>Planning with diffusion for flexible behavior synthesis. M Janner, Y Du, J B Tenenbaum, arXiv:220509991. 2022arXiv preprint</p>
<p>Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks. S Nasiriany, H Liu, Y Zhu, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Option discovery in hierarchical reinforcement learning using spatio-temporal clustering. A Srinivas, R Krishnamurthy, P Kumar, arXiv:1605 053592016arXiv preprint</p>
<p>Hierarchical deep reinforcement learning: integrating temporal abstraction and intrinsic motivation. T D Kulkarni, K Narasimhan, A Saeedi, Adv Neural Inf Process Syst. 292016</p>
<p>Hierarchical planning through goal-conditioned offline reinforcement learning. J Li, C Tang, M Tomizuka, 10.1109/LRA.2022.3190100IEEE Robot Autom Lett. 742022</p>
<p>Hierarchical diffusion for offline decision making. W Li, X Wang, Jin B , International Conference on Machine Learning. 2023</p>
<p>Model-based meta reinforcement learning using graph structured surrogate models. Q Wang, H Van Hoof, arXiv:210208291. 2021arXiv preprint</p>
<p>Search on the replay buffer: bridging planning and reinforcement learning. B Eysenbach, R R Salakhutdinov, S Levine, Adv Neural Inf Process Syst. 322019</p>
<p>Mapping state space using landmarks for universal goal reaching. Z Huang, F Liu, H Su, Adv Neural Inf Process Syst. 322019</p>
<p>Successor feature landmarks for long-horizon goal-conditioned reinforcement learning. C Hoang, S Sohn, J Choi, Adv Neural Inf Process Syst. 342021</p>
<p>Argumentation and explainable artificial intelligence: a survey. A Vassiliades, N Bassiliades, T Patkos, 10.1017/S0269888921000011Knowl Eng Rev. 36e52021</p>
<p>A framework of explanation generation toward reliable autonomous robots. T Sakai, K Miyazawa, T Horii, 10.1080/01691864.2021.1946423Adv Robot. 35172021</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>Distal explanations for explainable reinforcement learning agents. P Madumal, T Miller, L Sonenberg, arXiv:200110284. 2020arXiv preprint</p>
<p>Markov decision processes: discrete stochastic dynamic programming. M L Puterman, 2014John Wiley &amp; Sons</p>
<p>A note on two problems in connexion with graphs. E W Dijkstra, 10.1007/BF01386390Numer Math. 111959</p>
<p>Imitating graph-based planning with goal. J Kim, Y Seo, S Ahn, conditioned policies International Conference on Learning Representations. 2023</p>
<p>Relay policy learning: solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, arXiv:191011956. 2019arXiv preprint</p>
<p>UMAP: Uniform manifold approximation and projection for dimension reduction. L Mcinnes, J Healy, J Melville, arXiv:1802034262018arXiv preprintQuery: Give me all your action plan. Answer: Sure. Here is my plan: 1. First, I will open the microwave</p>
<p>. Next, I will move the kettle to the top burner position</p>
<p>Then, I will turn on the light switch. </p>
<p>Finally, I will open the slide cabinet. Info: Your action plan comprises a systematic sequence as follows. open the microwave', 'move the kettle to the top burner position', 'turn on the light switch', 'open the slide cabinet'</p>
<p>Query: What's the first step in your action plan? Answer: I plan to open the microwave first. Info: I want to finish the following tasks. open the bottom burner', 'open the slide cabinet', 'open the hinge cabinet', 'open the microwave', 'turn on the light switch'</p>
<p>Plan 2: ['open the microwave', 'open the hinge cabinet', 'open the slide cabinet', 'open the bottom burner'] Query: Can you finish these tasks? Answer: I'm sorry, but I can't finished all the tasks. Query: Why can't you finish all the tasks? Answer: Generating a plan that finishing all the tasks is not supported. You can choose to turn on the light switch or open the hinge cabinet. However, it seems this might not be feasible. Here are alternative task lists based on feasibility: Plan 1: ['open the microwave', 'turn on the light switch', 'open the slide cabinet', 'open the bottom burner'. but not both. Info: I want to finish the following tasks: ['open the bottom burner', 'open the slide cabinet', 'open the hinge cabinet', 'open the microwave', 'turn on the light switch'</p>
<p>Plan 1: ['open the microwave', 'turn on the light switch', 'open the slide cabinet', 'open the bottom burner'] Plan 2: ['open the microwave', 'turn on the light switch', 'open the hinge cabinet', 'open the bottom burner'] Query: Can you finish these tasks? Answer: I'm sorry, but I can't finished all the tasks. Query: Why can't you finish all the tasks? Answer: Generating a plan that finishing all the tasks is not supported. However, Here are alternative task lists based on feasibility. it seems this might not be feasible. You can choose to open the hinge cabinet or open the hinge cabinet, but not both</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-286 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-286</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-286</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-955191363c3676f71766af3d14d1e6bbc0f040d6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/955191363c3676f71766af3d14d1e6bbc0f040d6" target="_blank">The Parallelism Tradeoff: Limitations of Log-Precision Transformers</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is proved that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits, providing insight on the power of transformers using known results in complexity theory.</p>
                <p><strong>Paper Abstract:</strong> Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L≠P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture’s high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e286.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e286.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>⊕ (approx. p-precision float add)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterated p-precision floating-point addition (operator ⊕)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approximate addition operator for p-bit floats defined by mapping floats to scaled integers, summing exactly as integers, and truncating back to p-bit floats to preserve O(log n) precision; used to model how transformers perform addition of many small floating values while bounding precision growth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>log-precision transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>constant-depth transformer (encoder-style layers; abstracted attention heads)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>iterated floating-point addition / summation of up to n p-precision floats</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>p-bit floats with p = O(log n) (mantissa and exponent each ≈ p/2 bits); summation of up to k ≤ n terms</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>definition of ⊕ as g_p mapping floats to integers, integer summation, then f_p truncation back to p-bit float; implemented within transformer arithmetic subroutines and simulated by threshold circuits</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>The paper proves that iterated p-precision float addition (for p ≤ c log n) is computable in uniform TC^0 (constant-depth, poly-size threshold circuits), so transformers with O(log n) precision can perform such sums in constant depth; no empirical accuracy metrics are reported (the result is theoretical).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Addition is performed approximately by (1) mapping each float ⟨m,e⟩ to an integer z = m·2^{e - bias} (g_p), (2) computing the exact integer sum with threshold-circuit constructions, and (3) truncating/rescaling back to a p-bit float (f_p), which discards low-order bits; this bounded-precision integer-reduction lets addition be implemented in highly parallel threshold computations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Asymptotically: with p = O(log n) the sum is in uniform TC^0; increasing n requires only logarithmic growth in precision to preserve the theoretical simulation, but exact precise sums may lose small terms due to truncation (loss scales with 2^{-p/2}).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Loss of low-magnitude terms (small addends can be discarded when exponents differ widely); overflow handled by saturating to max representable float; precision loss bounded multiplicatively by ≈1 ± 2^{-p/2+2}.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared (theoretically) to exact high-precision floating add (which can require more than p bits) and to integer iterated addition constructions in circuit complexity (Hesse et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Transformers with O(log n) precision can implement parallel approximate summation of up to n floats by reducing floats to integers and summing in TC^0, but this requires truncation that discards small terms and bounds achievable exactness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Parallelism Tradeoff: Limitations of Log-Precision Transformers', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e286.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e286.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention counting / thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using attention heads to compute counts/fractions and thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Construction showing attention heads (notably saturated attention) can compute fractional counts (e.g., c_i / m_i) and normalized sums which are then thresholded to implement arithmetic-like comparisons (e.g., checking if count ≥ threshold) within transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>log-precision transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>transformer with saturated/hard attention and thresholded linear pooling functions</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>counting (popcount over subsets), computing normalized fractions c_i/m_i, integer comparisons / threshold tests (c_i ≥ k_i), pointer arithmetic via fractional positional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Counts over up to n positions; fractions represented as rational multiples normalized by n (i/n), values expressed with O(log n)-bit precision</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>saturated (hard-max) attention to select uniform distributions over subsets; fractional positional embeddings (i/n) to encode indices and pointers; pooling functions are thresholded linear functions used to implement comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Constructive proof: heads can compute c_i/m_i and k_i/m_i in constant numbers of layers (two layers per circuit depth), enabling exact evaluation of TC^0 threshold gates under the log-precision model; this is a theoretical construction, not empirical performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Attention can act as a parallel counting/aggregation mechanism: (1) saturated attention selects the relevant subset of positions and averages their (0/1) values, yielding counts scaled by arity, (2) fractional positional embeddings allow attention to address and retrieve specific indices, and (3) thresholded linear pooling compares normalized counts to normalized thresholds to decide gate outputs — effectively realizing discrete arithmetic comparisons via attention + linear thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Scales in parallel: operations remain constant-depth with respect to n (placed in uniform TC^0) so long as precision grows as O(log n); as n grows large without precision growth operations that require exact global counts may fail due to precision limits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Depends on sufficient (log n) precision; if precision is too small, coarse fractional encoding (i/n) and normalized counts can underflow/round and produce incorrect threshold decisions; also relies on attention patterns (saturated attention) that may not be induced by learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Contrasted with RNN/LSTM inability to evaluate Boolean formulae; compared with other attention regimes (hard attention, saturated, soft), saturated attention provides the control needed for exact counting in the construction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Attention heads, combined with fractional positional embeddings and thresholded pooling, can implement counting and threshold comparisons in constant-depth parallel computations, enabling transformers to simulate threshold-gate arithmetic logic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Parallelism Tradeoff: Limitations of Log-Precision Transformers', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e286.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e286.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformers → uniform TC^0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Log-precision transformers simulated by logspace-uniform TC^0 threshold circuits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Main theoretical result: any transformer whose intermediate values have O(log n) bits of precision and whose subnets are computable in O(log n) space can be simulated by logspace-uniform, constant-depth threshold circuits (uniform TC^0), bounding the class of exact computations such transformers can perform.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>log-precision transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>constant-depth transformer (general attention allowed, O(log n) precision everywhere)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general arithmetic expressible within O(log n)-bit operations: iterated addition, integer arithmetic over O(log n)-bit operands, threshold comparisons, normalization/division via approximate arithmetic circuits</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>All intermediate numbers have p = O(log n) bits; per-layer computations operate on O(log n)-bit values; summations over n terms of p-bit values allowed (with truncation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Proof technique: construct small-depth threshold circuits that simulate each transformer layer using lemmas that map O(log n)-bit functions to depth-3 circuits and known TC^0 constructions for iterated addition; uniformity enforced via log-space generation of circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Theoretical containment: log-precision transformers are in logspace-uniform TC^0; corollary: they cannot (asymptotically, exactly) solve problems outside TC^0 (e.g., many P-complete problems such as linear equation solving, universal CFG recognition, SAT under standard complexity separations) when n is large enough.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>The key mechanism is that limited precision forces computations to be local to O(log n)-bit blocks, enabling highly parallel threshold computations (sums, comparisons) but preventing deep sequential algorithms that require large-space accumulation; attention provides parallel aggregation while bounded precision prevents lossless global pooling, so the overall computation maps naturally onto constant-depth threshold gates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Asymptotic limitation: holding precision at O(log n) yields containment in TC^0 regardless of model parameter count; increasing model size/parameters does not escape the class if precision remains O(log n). Exact computation of problems outside TC^0 would require precision or sequential depth to grow beyond these bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Cannot exactly compute problems that are provably harder than TC^0 (assuming standard complexity separations) as input length grows, e.g., solving arbitrary linear systems, general context-free membership, SAT, computing permanent; limited precision implies inability to losslessly pool arbitrarily large inputs into a single representation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to infinite-precision transformer results showing Turing-completeness; compared to prior results on hard/saturated attention placing transformers in non-uniform AC^0/TC^0; contrast between non-uniform and uniform circuit classes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Bounded (log n) precision fundamentally restricts transformer arithmetic/algorithmic power: all such transformers correspond to uniform TC^0 computations — highly parallel constant-depth threshold operations like approximate addition and thresholding — and thus cannot perform arbitrary poly-time arithmetic algorithms asymptotically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Parallelism Tradeoff: Limitations of Log-Precision Transformers', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e286.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e286.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction/Circuit eval arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformers evaluating TC^0 circuits via instruction-like input (Circuit Value Problem construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Constructive lower-bound showing a transformer with fractional positional embeddings and saturated attention can evaluate any given TC^0 threshold circuit serialized as input, effectively executing arithmetic/threshold logic when supplied the circuit as an instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>constructed transformer (hand-designed weights)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>transformer with fractional positional embeddings, saturated attention, thresholded linear pooling; depth 2d to evaluate depth-d TC^0 circuits</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>evaluation of threshold-circuit arithmetic: gate evaluation (threshold tests), counting of 1s in gate arguments, normalization of counts (c_i/m_i) and comparison to threshold k_i/m_i</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Circuits of depth d (arbitrary constant), arities m_i per gate; arithmetic limited to threshold comparisons and counts encoded as rational fractions normalized by n</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Serialize a threshold circuit into tokens and feed it as input ('instruction'); use fractional positional embeddings i/n to represent pointers; saturated attention retrieves arguments and counts; thresholded pooling produces gate outputs; this is a hand-constructed weight setting, not learned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Theoretical guarantee: a depth-2d transformer can compute the Circuit Value Problem for depth-d TC^0 circuits exactly under the paper's model assumptions; no empirical accuracies provided.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>When provided an explicit circuit as input, attention + positional encodings act as an addressable memory and parallel aggregator, enabling the transformer to implement the same threshold arithmetic as the circuit: arguments attend to their gate, retrieve values, aggregate via attention to count 1s, and apply threshold tests, so the transformer 'runs' the circuit via attention-based routing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>To evaluate deeper circuits (larger d) requires transformer depth scaling linearly (2d), but each circuit depth layer is simulated by constant transformer layers; the construction depends on O(log n) precision to represent fractional positions/pointers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires the circuit description as input (advice/instruction); without that, the transformer cannot necessarily discover algorithmic evaluation for arbitrary circuits — the result is a lower bound under instruction/advice provisioning, not a claim about learned generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to RNN/LSTM inability to evaluate Boolean formulae; compared to non-instruction setting where transformer alone (without advice) is upper-bounded by TC^0.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Given an explicit TC^0 circuit as input (instruction), a transformer can exactly evaluate its arithmetic/threshold logic using attention and fractional positional embeddings, showing transformers can execute parallel threshold arithmetic when guided by suitable instruction tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Parallelism Tradeoff: Limitations of Log-Precision Transformers', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Saturated transformers are constant-depth threshold circuits <em>(Rating: 2)</em></li>
                <li>Formal language recognition by hard attention transformers: Perspectives from circuit complexity <em>(Rating: 2)</em></li>
                <li>Division in logspace-uniform NC1 <em>(Rating: 2)</em></li>
                <li>Division is in uniform TC^0 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-286",
    "paper_id": "paper-955191363c3676f71766af3d14d1e6bbc0f040d6",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "⊕ (approx. p-precision float add)",
            "name_full": "Iterated p-precision floating-point addition (operator ⊕)",
            "brief_description": "An approximate addition operator for p-bit floats defined by mapping floats to scaled integers, summing exactly as integers, and truncating back to p-bit floats to preserve O(log n) precision; used to model how transformers perform addition of many small floating values while bounding precision growth.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "log-precision transformer",
            "model_size": null,
            "model_architecture": "constant-depth transformer (encoder-style layers; abstracted attention heads)",
            "arithmetic_operation_type": "iterated floating-point addition / summation of up to n p-precision floats",
            "number_range_or_complexity": "p-bit floats with p = O(log n) (mantissa and exponent each ≈ p/2 bits); summation of up to k ≤ n terms",
            "method_or_intervention": "definition of ⊕ as g_p mapping floats to integers, integer summation, then f_p truncation back to p-bit float; implemented within transformer arithmetic subroutines and simulated by threshold circuits",
            "performance_result": "The paper proves that iterated p-precision float addition (for p ≤ c log n) is computable in uniform TC^0 (constant-depth, poly-size threshold circuits), so transformers with O(log n) precision can perform such sums in constant depth; no empirical accuracy metrics are reported (the result is theoretical).",
            "mechanistic_insight": "Addition is performed approximately by (1) mapping each float ⟨m,e⟩ to an integer z = m·2^{e - bias} (g_p), (2) computing the exact integer sum with threshold-circuit constructions, and (3) truncating/rescaling back to a p-bit float (f_p), which discards low-order bits; this bounded-precision integer-reduction lets addition be implemented in highly parallel threshold computations.",
            "performance_scaling": "Asymptotically: with p = O(log n) the sum is in uniform TC^0; increasing n requires only logarithmic growth in precision to preserve the theoretical simulation, but exact precise sums may lose small terms due to truncation (loss scales with 2^{-p/2}).",
            "failure_modes": "Loss of low-magnitude terms (small addends can be discarded when exponents differ widely); overflow handled by saturating to max representable float; precision loss bounded multiplicatively by ≈1 ± 2^{-p/2+2}.",
            "comparison_baseline": "Compared (theoretically) to exact high-precision floating add (which can require more than p bits) and to integer iterated addition constructions in circuit complexity (Hesse et al.).",
            "key_finding": "Transformers with O(log n) precision can implement parallel approximate summation of up to n floats by reducing floats to integers and summing in TC^0, but this requires truncation that discards small terms and bounds achievable exactness.",
            "uuid": "e286.0",
            "source_info": {
                "paper_title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Attention counting / thresholding",
            "name_full": "Using attention heads to compute counts/fractions and thresholds",
            "brief_description": "Construction showing attention heads (notably saturated attention) can compute fractional counts (e.g., c_i / m_i) and normalized sums which are then thresholded to implement arithmetic-like comparisons (e.g., checking if count ≥ threshold) within transformers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "log-precision transformer",
            "model_size": null,
            "model_architecture": "transformer with saturated/hard attention and thresholded linear pooling functions",
            "arithmetic_operation_type": "counting (popcount over subsets), computing normalized fractions c_i/m_i, integer comparisons / threshold tests (c_i ≥ k_i), pointer arithmetic via fractional positional embeddings",
            "number_range_or_complexity": "Counts over up to n positions; fractions represented as rational multiples normalized by n (i/n), values expressed with O(log n)-bit precision",
            "method_or_intervention": "saturated (hard-max) attention to select uniform distributions over subsets; fractional positional embeddings (i/n) to encode indices and pointers; pooling functions are thresholded linear functions used to implement comparisons",
            "performance_result": "Constructive proof: heads can compute c_i/m_i and k_i/m_i in constant numbers of layers (two layers per circuit depth), enabling exact evaluation of TC^0 threshold gates under the log-precision model; this is a theoretical construction, not empirical performance numbers.",
            "mechanistic_insight": "Attention can act as a parallel counting/aggregation mechanism: (1) saturated attention selects the relevant subset of positions and averages their (0/1) values, yielding counts scaled by arity, (2) fractional positional embeddings allow attention to address and retrieve specific indices, and (3) thresholded linear pooling compares normalized counts to normalized thresholds to decide gate outputs — effectively realizing discrete arithmetic comparisons via attention + linear thresholding.",
            "performance_scaling": "Scales in parallel: operations remain constant-depth with respect to n (placed in uniform TC^0) so long as precision grows as O(log n); as n grows large without precision growth operations that require exact global counts may fail due to precision limits.",
            "failure_modes": "Depends on sufficient (log n) precision; if precision is too small, coarse fractional encoding (i/n) and normalized counts can underflow/round and produce incorrect threshold decisions; also relies on attention patterns (saturated attention) that may not be induced by learning.",
            "comparison_baseline": "Contrasted with RNN/LSTM inability to evaluate Boolean formulae; compared with other attention regimes (hard attention, saturated, soft), saturated attention provides the control needed for exact counting in the construction.",
            "key_finding": "Attention heads, combined with fractional positional embeddings and thresholded pooling, can implement counting and threshold comparisons in constant-depth parallel computations, enabling transformers to simulate threshold-gate arithmetic logic.",
            "uuid": "e286.1",
            "source_info": {
                "paper_title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Transformers → uniform TC^0",
            "name_full": "Log-precision transformers simulated by logspace-uniform TC^0 threshold circuits",
            "brief_description": "Main theoretical result: any transformer whose intermediate values have O(log n) bits of precision and whose subnets are computable in O(log n) space can be simulated by logspace-uniform, constant-depth threshold circuits (uniform TC^0), bounding the class of exact computations such transformers can perform.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "log-precision transformer",
            "model_size": null,
            "model_architecture": "constant-depth transformer (general attention allowed, O(log n) precision everywhere)",
            "arithmetic_operation_type": "general arithmetic expressible within O(log n)-bit operations: iterated addition, integer arithmetic over O(log n)-bit operands, threshold comparisons, normalization/division via approximate arithmetic circuits",
            "number_range_or_complexity": "All intermediate numbers have p = O(log n) bits; per-layer computations operate on O(log n)-bit values; summations over n terms of p-bit values allowed (with truncation)",
            "method_or_intervention": "Proof technique: construct small-depth threshold circuits that simulate each transformer layer using lemmas that map O(log n)-bit functions to depth-3 circuits and known TC^0 constructions for iterated addition; uniformity enforced via log-space generation of circuits.",
            "performance_result": "Theoretical containment: log-precision transformers are in logspace-uniform TC^0; corollary: they cannot (asymptotically, exactly) solve problems outside TC^0 (e.g., many P-complete problems such as linear equation solving, universal CFG recognition, SAT under standard complexity separations) when n is large enough.",
            "mechanistic_insight": "The key mechanism is that limited precision forces computations to be local to O(log n)-bit blocks, enabling highly parallel threshold computations (sums, comparisons) but preventing deep sequential algorithms that require large-space accumulation; attention provides parallel aggregation while bounded precision prevents lossless global pooling, so the overall computation maps naturally onto constant-depth threshold gates.",
            "performance_scaling": "Asymptotic limitation: holding precision at O(log n) yields containment in TC^0 regardless of model parameter count; increasing model size/parameters does not escape the class if precision remains O(log n). Exact computation of problems outside TC^0 would require precision or sequential depth to grow beyond these bounds.",
            "failure_modes": "Cannot exactly compute problems that are provably harder than TC^0 (assuming standard complexity separations) as input length grows, e.g., solving arbitrary linear systems, general context-free membership, SAT, computing permanent; limited precision implies inability to losslessly pool arbitrarily large inputs into a single representation.",
            "comparison_baseline": "Compared to infinite-precision transformer results showing Turing-completeness; compared to prior results on hard/saturated attention placing transformers in non-uniform AC^0/TC^0; contrast between non-uniform and uniform circuit classes.",
            "key_finding": "Bounded (log n) precision fundamentally restricts transformer arithmetic/algorithmic power: all such transformers correspond to uniform TC^0 computations — highly parallel constant-depth threshold operations like approximate addition and thresholding — and thus cannot perform arbitrary poly-time arithmetic algorithms asymptotically.",
            "uuid": "e286.2",
            "source_info": {
                "paper_title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Instruction/Circuit eval arithmetic",
            "name_full": "Transformers evaluating TC^0 circuits via instruction-like input (Circuit Value Problem construction)",
            "brief_description": "Constructive lower-bound showing a transformer with fractional positional embeddings and saturated attention can evaluate any given TC^0 threshold circuit serialized as input, effectively executing arithmetic/threshold logic when supplied the circuit as an instruction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "constructed transformer (hand-designed weights)",
            "model_size": null,
            "model_architecture": "transformer with fractional positional embeddings, saturated attention, thresholded linear pooling; depth 2d to evaluate depth-d TC^0 circuits",
            "arithmetic_operation_type": "evaluation of threshold-circuit arithmetic: gate evaluation (threshold tests), counting of 1s in gate arguments, normalization of counts (c_i/m_i) and comparison to threshold k_i/m_i",
            "number_range_or_complexity": "Circuits of depth d (arbitrary constant), arities m_i per gate; arithmetic limited to threshold comparisons and counts encoded as rational fractions normalized by n",
            "method_or_intervention": "Serialize a threshold circuit into tokens and feed it as input ('instruction'); use fractional positional embeddings i/n to represent pointers; saturated attention retrieves arguments and counts; thresholded pooling produces gate outputs; this is a hand-constructed weight setting, not learned.",
            "performance_result": "Theoretical guarantee: a depth-2d transformer can compute the Circuit Value Problem for depth-d TC^0 circuits exactly under the paper's model assumptions; no empirical accuracies provided.",
            "mechanistic_insight": "When provided an explicit circuit as input, attention + positional encodings act as an addressable memory and parallel aggregator, enabling the transformer to implement the same threshold arithmetic as the circuit: arguments attend to their gate, retrieve values, aggregate via attention to count 1s, and apply threshold tests, so the transformer 'runs' the circuit via attention-based routing.",
            "performance_scaling": "To evaluate deeper circuits (larger d) requires transformer depth scaling linearly (2d), but each circuit depth layer is simulated by constant transformer layers; the construction depends on O(log n) precision to represent fractional positions/pointers.",
            "failure_modes": "Requires the circuit description as input (advice/instruction); without that, the transformer cannot necessarily discover algorithmic evaluation for arbitrary circuits — the result is a lower bound under instruction/advice provisioning, not a claim about learned generalization.",
            "comparison_baseline": "Compared to RNN/LSTM inability to evaluate Boolean formulae; compared to non-instruction setting where transformer alone (without advice) is upper-bounded by TC^0.",
            "key_finding": "Given an explicit TC^0 circuit as input (instruction), a transformer can exactly evaluate its arithmetic/threshold logic using attention and fractional positional embeddings, showing transformers can execute parallel threshold arithmetic when guided by suitable instruction tokens.",
            "uuid": "e286.3",
            "source_info": {
                "paper_title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Saturated transformers are constant-depth threshold circuits",
            "rating": 2
        },
        {
            "paper_title": "Formal language recognition by hard attention transformers: Perspectives from circuit complexity",
            "rating": 2
        },
        {
            "paper_title": "Division in logspace-uniform NC1",
            "rating": 2
        },
        {
            "paper_title": "Division is in uniform TC^0",
            "rating": 2
        }
    ],
    "cost": 0.01309325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Parallelism Tradeoff: Limitations of Log-Precision Transformers</h1>
<p>William Merrill<br>Center for Data Science New York<br>University, New York, NY, USA<br>willm@nyu.edu</p>
<h4>Abstract</h4>
<p>Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if $\mathrm{L} \neq \mathrm{P}$ (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture's high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.</p>
<h2>1 Introduction</h2>
<p>This work aims to characterize the computational model implicit in transformer neural networks (Vaswani et al., 2017), which form the basis of recent breakthroughs in large language models such as BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), and GPT-3 (Brown et al., 2020). What computational primitives can the transformer's components implement, and what problems can the full system solve in aggregate? These questions are important for interpreting transformers in a principled way, understanding potential limitations of their reasoning capabilities, and building trust in deployed transformerbased systems.</p>
<p>Early theoretical work on transformers established their Turing completeness, albeit with assumptions like infinite precision and arbitrarily</p>
<h2>Ashish Sabharwal <br> Allen Institute for AI <br> Seattle, WA, USA <br> ashishs@allenai.org</h2>
<p>powerful feedforward subnets (Pérez et al., 2019; Dehghani et al., 2019). On the other hand, a strand of more recent work uses techniques from circuit complexity theory to derive strong limitations on the types of problems transformers can solve given restrictions on the form of attention allowed in the transformer. Specifically, Hahn (2020) and Hao et al. (2022) showed that transformers restricted to hard attention are very limited: they can only solve problems in a weak complexity class (non-uniform $\mathrm{AC}^{0}$ ) that doesn't even contain basic problems like majority of $n$ bits. Merrill et al. (2022) extended this to a more general class of 'saturated attention' transformers with a floating point datatype, and showed a larger class of problems (non-uniform $\mathrm{TC}^{0}$ ) as an upper bound. This motivates analyzing a setting that strikes a middle ground: Can we characterize transformers whose precision and feedforward nets' computational power are realistically bounded, but where attention is also realistically expressive?</p>
<p>An important practical limitation of these prior results is the "non-uniform" nature of the considered circuit classes, which makes these classes non-realizable and the findings difficult to interpret. This is because non-uniform $\mathrm{AC}^{0}$ and $\mathrm{TC}^{0}$, while highly limited in computation, also contain some problems that are not even decidable, i.e., for which there doesn't exist any exact algorithm. Thus, non-uniform classes cannot be directly compared with standard algorithmic complexity classes such as $\mathrm{P}, \mathrm{NP}$, etc. This motivates our second key question: Can we derive uniform upper bounds on transformers?</p>
<p>We show that one can achieve both of these goals by making the modest assumption that all values in the transformer have $\mathrm{O}(\log n)$ precision (where $n$ is the number of input tokens), and, similarly, that transformer's subnetworks are computable in $\mathrm{O}(\log n)$ space. Log precision is enough to represent the positional encodings at the input layer of the transformer, and to encode pointers to all other positions in the sequence at</p>
<p>later transformer layers. Assuming log precision across all layers captures the idea that the hidden representations contain a constant number of hidden states whose precision ( 16 or 32 bits) is small relative to the length of the input (2048 in GPT-3). On long sequences, the precision will not be enough to losslessly encode the full input sequence into a single vector. Instead, the processing of the sequence must somehow be distributed in each layer and performed in parallel.</p>
<p>Upper Bound on Transformers. Our main contribution is proving that log-precision transformers can be simulated by uniform constantdepth threshold circuits. Thus, such transformers can only solve problems in uniform $\mathrm{TC}^{0}$. This characterization is strikingly weak compared to the Turing-completeness of infinite-precision transformers. Since we believe log precision is more realistic for practical transformers than infinite precision, these results point to the conclusion that transformers are not Turing-complete in practice.</p>
<p>In contrast to past results, our upper bound on transformers is a uniform circuit class, enabling direct comparison of log-precision transformers to many natural complexity classes. These connections reveal specific problems that define the upper limits of log-precision transformers' capabilities, as discussed further in $\S 2$.</p>
<p>Intuitively, our upper bound says that log-precision transformers are computationally shallow, and that this shallowness can be understood to emerge from their parallelizability. Transformers' inherent parallelism is useful for training them efficiently at massive scale, but may limit the complexity of the computations they can express. We introduce the term parallelism tradeoff to capture this idea, which represents a potential fundamental weakness of the current paradigm of scaling language models. Formally characterizing reasoning capabilities relevant to language models and understanding whether they likely fall outside upper bounds implied by the tradeoff would clarify the practical implications of this limitation of scaling.</p>
<p>It could also be that the limitations of parallelism are not a curse but a blessing, if they constrain the hypothesis space in a way useful for learning. We have no evidence that this is true, but mention it as an alternate interpretation of the results that could be clarified in future work.</p>
<p>Instruction Following and Advice Transformers. We also consider an instruction following setting (Brown et al., 2020) where the transformer is provided the description of a task along with an input on which to execute the instruction. We construct a practically parameterizable transformer that can execute instructions perfectly if they are provided in the form of $\mathrm{TC}^{0}$ circuits. This complements recent work that studies transformers' ability to follow other forms of instructions such as regular expressions (Finlayson et al., 2022).</p>
<p>Based on the fundamental property that transformers can correctly evaluate any given $\mathrm{TC}^{0}$ circuit on a given input, we introduce the notion of advice transformers akin to advice-taking Turing machines. We show that transformers can recognize any (non-uniform) $\mathrm{TC}^{0}$ language if provided appropriate poly-size advice.</p>
<p>In summary, our findings provide new insights on both the abilities and the limitations of transformers, and bring out bounded precision, threshold computations, and parallelism as key notions for understanding the implicit computational model of transformers in practice.</p>
<p>Roadmap. Before diving into technical details, we discuss in $\S 2$ the implications of our results on both fundamental as well as practical abilities of transformers. $\S 3$ provides a brief primer on circuits as a model of computation. It then discusses a way of serializing a circuit into a string; we later show how to generate such serializations using a resource-bounded algorithm, which is the key to proving containment of transformers in uniform circuit classes. $\S 4$ defines our formal model of bounded-precision transformers. $\S 5$ derives our first formal bound on log-precision transformers. This bound involves non-uniform circuit families, similar in spirit to prior results in this area. $\S 6$ proves our more technical main result: the first uniform circuit complexity upper bound for transformers (specifically, uniform $\mathrm{TC}^{0}$ ). Finally, $\S 7$ provides a lower bound on transformers, introduces the notion of an Advice Transformer, and connects these to the machine learning problems of Instruction Learning and Following.</p>
<h2>2 Implications of Our Findings</h2>
<p>Before diving into technical details, we discuss the general implications of our findings on the abilities and limitations of transformers. We will focus here on our main result (Theorem 2), which</p>
<p>shows that log-precision transformers are in the complexity class logspace-uniform $\mathrm{TC}^{0}$.</p>
<p>The Parallelism Tradeoff. One interpretation of complexity classes such as $\mathrm{NC}^{0}, \mathrm{AC}^{0}$, and $\mathrm{TC}^{0}$ is sets of poly-time solvable problems that are parallelizable to a very high degree-they can be solved in parallel in constant time with enough parallel processors. This gives some intuitive explanation of our result: log-precision transformers end up in $\mathrm{TC}^{0}$ because they were designed to be highly parallelizable. Since parallelism is an important property of today's dominant paradigm of training models at massive scale, this points to the conclusion that any massively scaled up model-transformer or otherwise-will likely obey restrictions similar to the ones derived here for log-precision transformers. There is thus an important tradeoff between the massive parallelizability of today's networks and their representation power.</p>
<p>What Transformers Can/Cannot Compute. Our result places log-precision transformers in the complexity class logspace-uniform $\mathrm{TC}^{0}$. This has immediate implications on the kinds of problems such transformers can and cannot accurately solve.</p>
<p>Consider any problem $X$ that is complete for a complexity class $C$ that contains logspace-uniform $\mathrm{TC}^{0}$. By definition of completeness, every problem log-precision transformers can solve perfectly is efficiently reducible to $X$ and is thus no harder than $X$. This implies that-despite their massive size-the computation performed by such transformers is, for instance, no harder than solving basic L-complete problems like graph connectivity: the problem of checking whether there is a path between two nodes in an undirected graph (Lewis and Papadimitriou, 1982; Reingold, 2008).</p>
<p>By the same token, if C is strictly larger than logspace-uniform $\mathrm{TC}^{0}$, then such transformers cannot perfectly solve $X$. Thus, log-precision transformers cannot perfectly solve the following reasoning problems:</p>
<ul>
<li>Linear equalities: find $x$ s.t. $A x=b^{1}$</li>
<li>Universal context-free recognition ${ }^{1,2}$</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>- Propositional satisfiability (SAT) ${ }^{3}$
- Horn-clause satisfiability (HORN-SAT) ${ }^{1}$
- AI planning (Bylander, 1991)
- Permanent computation ${ }^{4}$</p>
<p>This highlights the limits of practical transformers with limited-precision arithmetic, indicating that they are far from being universal or all-powerful as suggested by some prior studies.</p>
<p>One important caveat about these negative results is that they are asymptotic in nature-they apply for 'large enough' input size $n$. It's possible for log-precision transformers to solve such problems easily when $n$ is small. Further, these negative results are about exact solutions, but they often also extend beyond this when formal hardness-of-approximation results are known.</p>
<p>Limitations of Our Formal Model. Prior formal characterizations of transformers either make unrealistically strong assumptions (Pérez et al., 2019; Dehghani et al., 2019) or place unrealistic restrictions (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022). In contrast, we make only one assumption-namely, all intermediate values in the transformer are limited to $\mathrm{O}(\log n)$ bits, where $n$ is the number of input tokens. We next discuss some implications of this assumption and what our findings mean for practical transformers.</p>
<p>As mentioned above, our bounds are asymptotic in nature and thus apply when $n$ is sufficiently large. In practice, transformers use fixed precision at each computation node, which is more restrictive than precision growing with the input sequence length $n$, as $\mathrm{O}(\log n)$ bits. However, this constant could be large and thus, for relatively small $n$, our results do not rule out practical transformers solving difficult problems. Our results, however, do show that as $n$ grows sufficiently large, log-precision transformers are fundamentally limited to problems within $\mathrm{TC}^{0}$ and cannot accurately solve various commonly studied problems mentioned earlier under "What Transformers Cannot Compute". Extending our analysis to small $n$ will help close the gap to practice.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Our formal model is based on a binary classification view of transformers. However, our results apply directly to multi-class classification as well and can be extended to generation problems by viewing, for instance, next word prediction in NLP as a multi-class classification problem. However, if the transformer decoder is allowed to condition on its previous output in a generation problem, then this would violate our formal setup.</p>
<h3>2.1 Potential Applications</h3>
<p>Extracting Circuits from Transformers. Elhage et al. (2021) propose extracting circuits ${ }^{5}$ that capture the computational structure of transformers. Our results suggest threshold circuit families are a good formalism for expressing mechanisms extracted from transformers. Constructively converting transformers to threshold circuits is beyond the scope of the current paper, although we hope to explore this in more detail in future work.</p>
<p>Testing Separation Candidates in Complexity Theory. Theorem 2 also motivates a paradigm for quickly testing complexity theory conjectures. If a problem is believed to separate $\mathrm{TC}^{0}$ and $\mathrm{NC}^{1}$, a transformer can be trained on problem instances. If the transformer generalizes perfectly to harder instances than it was trained on, this gives an empirical hint that the problem is in $\mathrm{TC}^{0}$, providing evidence against the conjecture.</p>
<h2>3 Circuit Computation</h2>
<p>Let ${0,1}^{<em>}$ be the set of finite binary strings. For $x \in{0,1}^{</em>}$, let $|x|$ be its length. We refer to a function from ${0,1}^{<em>}$ to ${0,1}^{</em>}$ as a Boolean function. Boolean functions can implement arithmetic operations if we define a semantics for binary strings as numbers. We will treat the intermediate values in a transformer as binary strings, and the internal operations as Boolean functions.</p>
<p>Circuits are a model of computation for computing Boolean functions of fixed-length binary strings. ${ }^{6}$ Formally, a circuit is a directed acyclic computation graph. The leaf nodes represent binary variables and their negations. The internal nodes represent functions in some set $\mathcal{G}$, and the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>directed edges represent the flow of function outputs into inputs of other functions. One or more nodes in the circuit are marked such that their value is the output of the circuit.</p>
<p>Definition 1. For a set of functions $\mathcal{G}$, a $\mathcal{G}$-circuit is a directed acyclic computation graph where the internal nodes have labels from $\mathcal{G}$.</p>
<p>Complexity Measures. The size of a circuit is the total number of gates in it, including negation. The depth of a circuit is the length of the longest path from any input node to any output node.</p>
<p>Circuit Families. A circuit family generalizes a circuit to take variable-length binary strings as input. Formally, a circuit family is a sequence of circuits $C_{n}:{0,1}^{n} \rightarrow{0,1}$ for $n \in \mathbb{N}$. A circuit family implicitly recognizes a formal language defined as follows:</p>
<p>Definition 2. A circuit family $C_{n}$ recognizes $L \subseteq$ ${0,1}^{<em>}$ if, for all $x \in{0,1}^{</em>}, C_{|x|}(x)=1$ if and only if $x \in L$.</p>
<p>We now define classes of languages by constraining the complexity of the circuit families needed to recognize them:</p>
<p>Definition 3. Let non-uniform $A C^{0}$ be the set of $L \subseteq{0,1}^{*}$ such that $L$ is recognizable by a polysize, constant-depth ${\neg, \wedge, \vee}$-circuit family.</p>
<p>For $k \in \mathbb{N}$, a threshold gate $\theta_{\leq k}$ takes $m$ input bits and returns whether $\sum_{i=1}^{m} x_{i} \leq k$. We define $\theta_{\geq k}$ analogously. For example, $\theta_{\leq 3}(110011)=0$.</p>
<p>Definition 4. Let $\mathrm{TC}^{0}$ be the set of $L \subseteq{0,1}^{*}$ such that $L$ is recognizable by a poly-size, constant-depth $\left{\theta_{\leq k}, \theta_{\geq k}\right}_{k \in \mathbb{N}}$-circuit.</p>
<p>The gates $\neg, \wedge$, and $\vee$ are all just special cases of thresholds, so we can imagine $\mathrm{TC}^{0}$ circuits to have access to these as well. Thus, $\mathrm{TC}^{0}$ circuits can implement $\mathrm{AC}^{0}$ circuits.</p>
<p>Circuit Serialization. We identify a circuit with its serialization in a formal language that identifies each node's label and adjacency list. We will adopt a specific grammar for concreteness, but our construction can be adapted to other string representations of circuits.</p>
<p>We define a circuit serialization as a traversal of a circuit ordered by some topological sort. In this serialization, leaf nodes (variables) are represented by the string X. An internal node (gate) is represented in Polish notation by the function it</p>
<p>computes (AND, OR, or NOT) followed by a list of pointers to its arguments. Each argument $\&amp; 1^{j}$ of gate $i$ encodes (in a unary) a zero-indexed pointer to the $j$-th gate in the circuit, where $j&lt;i$. The final node is interpreted as the circuit output.</p>
<p>To serialize ${\wedge, \vee}$-circuits, we use the following grammar, where the $i$ parameter is passed through Gate $[i]$ nonterminals to track the index of the gate in left-to-right order:</p>
<p>$$
\begin{aligned}
\text { Circuit } &amp; \rightarrow \text { Gate[1] Gate[2] } \cdots \text { Gate }[g] \
\text { Gate }[i] &amp; \rightarrow \mathrm{X} \mid \text { NOT } \operatorname{Arg}[i] \mid \text { Op } \operatorname{Arg}[i]^{*} \
\operatorname{Arg}[i] &amp; \rightarrow \&amp; 1^{j} \quad \text { s.t. } j&lt;i \
\text { Op } &amp; \rightarrow \text { AND } \mid \text { OR }
\end{aligned}
$$</p>
<p>In the $\operatorname{Arg}[i]$ rule, we enforce that $j&lt;i$ so that arguments must be pointers to already defined gates. As an example of this serialization language, the circuit for $x_{1} \vee \neg x_{2} \vee x_{3}$ is represented as ${ }^{7}$</p>
<p>$$
\text { X X X NOT \&amp;1 OR \&amp; \&amp;111 \&amp;11 }
$$</p>
<p>By convention (cf. $\S 3$ ), negations in $\mathrm{AC}^{0}$ circuits are usually taken to occur at the beginning of the circuit, rather than after $\wedge$ or $\vee$ nodes. ${ }^{8}$ Our serialization grammar does not enforce this property, but of course any circuit with this property can be serialized by our grammar.</p>
<p>It is a bit more complicated to serialize threshold circuits. Formally, a threshold circuit serialization is generated by the following grammar:</p>
<p>$$
\begin{aligned}
\text { Circuit } &amp; \rightarrow \text { Gate[1] Gate[2] } \cdots \text { Gate }[g] \
\text { Gate }[i] &amp; \rightarrow \mathrm{X} \mid \text { Dir } 1^{k} 0^{m-k} \operatorname{Arg}[i]^{m} \
\operatorname{Arg}[i] &amp; \rightarrow \&amp; 1^{j} \quad \text { s.t. } j&lt;i \
\text { Dir } &amp; \rightarrow&lt;=\mid&gt;=
\end{aligned}
$$</p>
<p>In the rewrite rule for Gate $[i], m \in \mathbb{N}$ is the arity of the gate, and $k \leq m$ is its threshold. The span $1^{k}$ after Dir can be interpreted semantically as a unary encoding of the parameter $k$ for a threshold gate, padded by 0 's to the number of total arguments of gate $i$. For simplicity, we imagine $\neg$ gates are represented as unary $\theta_{\leq 0}$ gates. Thus, the circuit $\theta_{\geq 1}\left(x_{1}, \neg x_{2}\right)$ would be represented as</p>
<p>$$
X X&lt;=00 \&amp; 1&gt;=10 \&amp; \&amp; 11
$$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Uniformity. The circuit families we have defined above are non-uniform, meaning that we do not enforce that the circuits processing different input sizes must be related in any way. In degenerate cases, non-uniform circuit families can solve undecidable problems ${ }^{9}$ because they have infinite description length, making them a physically unrealizable model of computation. Complexity theorists have thus introduced uniform circuit families. Uniform circuit families are a realizable model of computation with relations to classes in computational complexity and formal language theory.</p>
<p>Intuitively, in a uniform circuit family, the circuits for different input sizes must be "somewhat similar" to each other. We formalize this (cf. Arora and Barak, 2009) by saying that there exists a resource-constrained Turing machine that maps the input $1^{n}$ to a serialization of circuit $C_{n}$.
Definition 5. A language $L$ is $(S(n), I(n))$-space uniformly computable by a circuit model $M$ iff there exists a Turing machine that, for all $n \geq$ 0 , uses $S(n)$ space to map $1^{n}$ to an $M$-circuit recognizing $L$ on inputs of size $I(n)$.</p>
<p>This notion of uniformity is more general than the standard notion in that the input size $I(n)$ is a function of the problem complexity $n$. The reason for this is that we will apply uniformity to subcomputations with different input sizes $I(n)$ within a larger computation of input size $n$. The standard notion of uniformity corresponds to $I(n)=n$.</p>
<p>Furthermore, we will refer to a circuit family as uniform if it is uniformly computable with $S(n)=\mathrm{O}(\log n)$ (cf. Arora and Barak, 2009). We can define uniform versions of $\mathrm{AC}^{0}$ and $\mathrm{TC}^{0}$ by adopting the previous definitions exactly, but also enforcing uniformity. For the rest of the paper we will clarify whether we mean the uniform or non-uniform variant of $\mathrm{TC}^{0}$ when unclear from context, since both classes will come up.</p>
<h2>4 Bounded-Precision Transformers</h2>
<p>A transformer (Vaswani et al., 2017) is a neural network architecture made up of a constant number of transformer layers. A transformer layer is a module that computes self-attention</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>over a sequence followed by an elementwise transformation of the output vectors.</p>
<h3>4.1 Precision and Space</h3>
<p>We will assume that each transformer is resource bounded in terms of the precision of each value it computes and, for some of our results, the space it uses for the computation of key operations such as embedding, attention, and activation. Specifically, we will assume precision $p$, i.e., the values at all layers, as well as the outputs of all key intermediate operations in it (attention, activation, arithmetic operators, etc.), are represented using $p$ bits. This is a realistic assumption as, in practice, today's transformers are typically limited to the 64-bit precision of the underlying hardware. Formally, we define $p$-precision as follows:</p>
<p>Definition 6. A $k$-ary function $f: x_{1}, \ldots, x_{k} \mapsto$ $y$ is $p$-precision if $x_{1}, \ldots, x_{k}, y \in{0,1}^{*}$ have size at most $p$ bits, and $f$ can be computed by a $p$-space-bounded Turing machine.</p>
<p>This says the size of the function input and output are bounded below $p$. Similarly, the intermediate space used by the computation must also be bounded below $p$. Thus, higher precision computations cannot somehow be hidden inside $f$.</p>
<p>Definition 6 naturally applies to functions with bounded arity $k$. We will also need to define $p$ precision for the summation operator in the transformer, which adds $n$ different floats of size $p .{ }^{10}$ Adding $n$ floats can blow up the precision needed to represent their sum. For example, imagine adding the floating points $1 \cdot 2^{0}+1 \cdot 2^{c}$. We obtain $\left(2^{c}+1\right) \cdot 2^{0}$, whose mantissa takes $c+1$ bits to represent. In practice, computers do not preserve full precision in such situations: instead, small terms like $1 \cdot 2^{0}$ are discarded. Thus, we define the transformer's addition operation $\oplus$ to be similarly approximate (and thus preserve precision); see $\S \mathrm{A}$.</p>
<h3>4.2 Transformer Definition</h3>
<h3>4.3 Attention Heads</h3>
<p>The core building block of a transformer is an attention head. We define this at a high level of abstraction as follows:</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Definition 7. A $p$-precision attention head is specified by a binary $p$-precision similarity function $s:{0,1}^{p} \times{0,1}^{p} \rightarrow{0,1}^{p}$.</p>
<p>Let $\mathbf{h}<em n="n">{1}, \ldots, \mathbf{h}</em>$ ).
Definition 8. For all $\ell \geq 0$, a $p$-precision attention head $H_{h}^{\ell+1}$ computes a vector $\mathbf{a}_{i h}^{\ell+1} \in{0,1}^{p}$ via} \in{0,1}^{p}$ be the input sequence to a $p$-precision attention head, and let $\oplus$ be approximate floating-point addition ( $\S \mathrm{A</p>
<p>$$
\mathbf{a}<em j="1">{i h}^{\ell+1}=\bigoplus</em>}^{n} \frac{s\left(\mathbf{h<em j="j">{i}^{\ell}, \mathbf{h}</em>
$$}^{\ell}\right)}{Z_{i}} \cdot \mathbf{h}_{j}^{\ell</p>
<p>where $Z_{i}=\bigoplus_{j=1}^{n} s\left(\mathbf{h}<em j="j">{i}^{\ell}, \mathbf{h}</em>\right)$.
Standard transformer attention heads (Vaswani et al., 2017) are a special case of this definition where $s$ is scaled dot-product similarity between keys and queries. Standard transformers also have a linear or affine value function applied to each $\mathbf{h}_{j}^{\ell}$ in the sum over $j$. By its affineness, the value function can, without loss of generality, be removed from the attention head and considered to be part of the transformer layer (i.e., applied to the output of the attention head).}^{\ell</p>
<h3>4.4 Transformer Layers</h3>
<p>A $p$-precision transformer layer is then a tuple of heads and a function $f$ used to combine them.
Definition 9 ( $p$-precision transformer layer). A $p$-precision transformer layer is a tuple $L^{\ell+1}=$ $\left\langle H_{1}, \cdots, H_{k}, f\right\rangle$, where each $H_{h}$ is an attention head and $f:\left({0,1}^{p}\right)^{k} \times{0,1}^{p} \rightarrow{0,1}^{p}$ is a $p$-precision activation function.</p>
<p>A $p$-precision transformer layer can be understood to define a sequence of vectors $\mathbf{h}<em n="n">{1}^{\ell+1}, \ldots, \mathbf{h}</em>}^{\ell+1}$ in terms of an input sequence of vectors $\mathbf{h<em n="n">{1}^{\ell}, \ldots, \mathbf{h}</em>}^{\ell}$ (coming from the previous layer in the transformer) by first computing $k$ attention heads in parallel and then combining their output using $f$. The first $k$ inputs to $f$ will correspond to the attention head outputs, and the additional input is the original input from the previous layer. Recall that $\mathbf{a<em h="h" i="i">{i h}^{\ell+1}$ is the output of head $H</em>$ at position $i$. The function computed by a transformer layer can be described formally as follows.
Definition 10 (Transformer layer computation). For $\ell \geq 0$, a $p$-precision transformer layer $L^{\ell+1}$ recurrently computes the output sequence $\mathbf{h}}^{\ell+1}$ on input $\mathbf{h}^{\ell<em n="n">{1}^{\ell+1}, \ldots, \mathbf{h}</em>$ as a function of the inputs}^{\ell+1</p>
<p>$\mathbf{h}<em _mathrm_n="\mathrm{n">{1}^{\ell}, \ldots, \mathbf{h}</em>$, where, for $1 \leq i \leq n$, the $i$ th component is computed according to}}^{\ell</p>
<p>$$
\mathbf{h}<em 1="1" i="i">{i}^{\ell+1}=f\left(\mathbf{a}</em>}^{\ell+1}, \ldots, \mathbf{a<em i="i">{i k}^{\ell+1}, \mathbf{h}</em>\right)
$$}^{\ell</p>
<p>$f$ can be understood to encapsulate layernorm, residual connections, and the feedforward sublayer of a standard transformer (Vaswani et al., 2017). $\mathbf{h}_{i}^{\ell}$ is given to $f$ to allow residual connections. As mentioned in $\S 4.3, f$ can also encapsulate the value function for each head.</p>
<h3>4.5 Transformer Encoder</h3>
<p>Finally, we define a transformer of depth $d$ as a cascade of $d$ transformer layers:</p>
<p>Definition 11 ( $p$-precision transformer). A $p$-precision transformer over alphabet $\Sigma$ is a pair consisting of a $p$-precision position embedding function ${ }^{11} \phi: \Sigma \times \mathbb{N} \rightarrow{0,1}^{p}$ and a $d$-tuple of $p$-precision transformer layers $\left\langle L^{1}, \ldots, L^{d}\right\rangle$.</p>
<p>For a position embedding function $\phi$ and $w \in$ $\Sigma^{n}$, let $\phi(w)$ be the position-wise broadcasted embedding of $w$ : for $1 \leq i \leq n, \phi_{i}(w) \triangleq \phi\left(w_{i}, i\right)$.</p>
<p>Definition 12 (Transformer computation). A transformer $\left(\phi,\left\langle L^{1}, \cdots L^{d}\right\rangle\right)$ computes the following function of a string $w \in \Sigma^{*}$ :</p>
<p>$$
T(w)=\left(L^{d} \circ L^{d-1} \circ \cdots \circ L^{1}\right)(\phi(w))
$$</p>
<p>We will use $n$ to denote the length of $w$, and take the transformer's depth $d$ to be fixed w.r.t. $n$.</p>
<p>The input to the transformer can thus be represented with $N=n \log |\Sigma|$ bits using a binary encoding for the vocabulary. The circuits we construct in subsequent sections to simulate transformers will also have input size $N$. We will assume transformers have log-precision relative to the size of the input, specifically, $\mathrm{O}(\log N)$-precision. Since $|\Sigma|$ is fixed (typically 30000 in practice), we will think in terms of $\mathrm{O}(\log n)$-precision. Thus, by Definition 6, all of the intermediate functions of such transformers are computable in $\mathrm{O}(\log n)$ space and output (at most) these many bits. Note that this is enough precision to represent positional encodings and for each position to point to a constant number of other values, but not enough precision for non-lossy pooling of the entire input into a single value.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Relationship to Practical Transformers. Our log-precision transformers do not enforce that $s$ (Definition 7) and $f$ (Definition 9) follow the transformer structure. However, a feedforward net whose primitive operations (e.g., scalar multiplication) are defined over $\mathrm{O}(\log n)$-size numbers can be computed in $\mathrm{O}(\log n)$ space. Thus, bounded-precision practical transformers are a special case of our log-precision transformers. This makes our setup appropriate for proving upper bounds on transformers, which is our main contribution.</p>
<h2>5 Log-Precision Transformers as Non-Uniform Threshold Circuits</h2>
<p>We first show that log-precision transformers can be simulated by non-uniform threshold circuits, before presenting the more technical uniform version of the results in $\S 6$. The initial non-uniform result extends the findings of Merrill et al. (2022), who showed that saturated attention transformers ${ }^{12}$ can be simulated in $\mathrm{TC}^{0}$. Here, we remove the simplifying saturated attention assumption and other restrictions on the underlying datatype. Instead, we show that our log-precision assumption is enough to prove that a transformer can be simulated in $\mathrm{TC}^{0}$ with any attention function.</p>
<p>Hao et al. observed that any Boolean function of $\mathrm{O}(\log n)$ bits can be computed by a poly $(n)$ size circuit. We extend this to $m$-bit outputs, which is both more convenient and more efficient than constructing $m$ separate Boolean circuits:</p>
<p>Lemma 1 (Extended from Hao et al., 2022). Let $f:{0,1}^{*} \rightarrow{0,1}^{m}$ be a function. For all $c \in \mathbb{R}^{+}$and $n \in \mathbb{N}$, there exists an AND/OR circuit of size at most $n^{c}+c \log n+m$ and depth 3 that computes $f$ on inputs of size $c \log n$.</p>
<p>Proof. Like Hao et al. (2022), we construct a circuit using a DNF representation of $f$ on inputs of size $c \log n$, except we use a combined DNF representation for all output bits of $f$. The DNF formula has at most $2^{c \log n}=n^{c}$ terms. The circuit has a NOT gate for each input bit, an AND gate for each DNF term, and, for each of the $m$ output bits, an OR gate combining the outputs of those AND gates (i.e., DNF terms) for which that bit is 1 .</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>We now use Lemma 1 to prove the following non-uniform result. We note that the proof goes through even if the notion of $p$-precision (Definition 6) is relaxed to not require computability in space $p$. This requirement will, however, become important for our subsequent result in $\S 6$.</p>
<p>Theorem 1 (Non-uniform). Any $c \log n$-precision depth-d transformer operating on inputs in $\Sigma^{n}$ can be simulated by a threshold circuit family of depth $3+\left(9+2 d_{\odot}\right) d$.</p>
<p>Proof. Let $w \in \Sigma^{n}$ be the input of a $c \log n$-precision transformer. We show by induction that we can construct a composition of constant-depth, poly-size threshold circuits to compute each layer of this transformer. Thus, any constant-depth transformer will be computable by a constant-depth threshold circuit.</p>
<p>In the base case of layer 0 and token $i$, we construct gates representing the constant $i$ encoded in binary. We can then compute $\mathbf{h}<em i="i">{i}^{0}=\phi\left(w</em>, i\right)$ using Lemma 1, yielding a poly-size depth-3 circuit.</p>
<p>In the inductive case of computing layer $\mathbf{h}<em i="i">{i}^{\ell+1}$ for $1 \leq \ell+1 \leq d$, we note that each vector output of layer $\mathbf{h}</em>$ has size (at most) $c \log n$ bits because of the log-precision assumption.}^{\ell</p>
<p>We first fix a head $\mathbf{a}<em i="i">{i k}^{\ell+1}$ (Definition 8) to simulate. Applying Lemma 1, we can compute $s\left(\mathbf{h}</em>}^{\ell}, \mathbf{h<em _odot="\odot">{j}^{\ell}\right)$ with a poly-size depth-3 circuit, in parallel for all $j$. Since $n$ floats with $c \log n$ precision can be approximately added in $\mathrm{TC}^{0}$ ( $\S \mathrm{A}$ ), we can construct a $\mathrm{TC}^{0}$ circuit of depth $d</em>}$ to compute $Z_{j}$. Since $s\left(\mathbf{h<em j="j">{i}^{\ell}, \mathbf{h}</em>}^{\ell}\right), Z_{i}$, and $\mathbf{h<em i="i">{i}^{\ell}$ all have $c \log n$ bits, we can compute $\frac{s\left(\mathbf{h}</em>}^{\ell}, \mathbf{h<em i="i">{j}^{\ell}\right)}{Z</em>}} \mathbf{h<em i="i" k="k">{j}^{\ell}$ with a poly-size depth-3 circuit; ${ }^{13}$ we do this in parallel for all $j$. Next, we again use the fact that approximate addition of $n$ floats is in $\mathrm{TC}^{0}$ to compute $\mathbf{a}</em>$ circuit.}^{\ell+1}$ as the approximate sum over $j$ with a depth- $d_{\odot</p>
<p>We now simulate a layer $\mathbf{h}<em i="i">{i}^{\ell+1}$ (Definition 10) in terms of its constituent heads. Since all arguments of $g$ have size $c \log n$, we apply Lemma 1 to compute $g$ with a poly-size depth-3 circuit, yielding $\mathbf{h}</em>$.}^{\ell+1}$. We repeat this in parallel for all $i$. This completes the inductive step new to compute all values in the $\ell+1$-st layer with a circuit depth of $9+2 d_{\odot</p>
<p>Aggregating the circuit over all $d$ layers, the overall circuit depth is $3+\left(9+2 d_{\odot}\right) d$.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Corollary 1.1 (Non-uniform). Any log-precision transformer can be simulated by a non-uniform $\mathrm{TC}^{0}$ circuit family. ${ }^{14}$</p>
<h2>6 Log-Precision Transformers as Uniform Threshold Circuits</h2>
<p>We will now extend the argument from the last section to show that $\mathrm{O}(\log n)$-precision transformers can be simulated by uniform constant-depth threshold circuits by capitalizing on the assumption that $\phi, s$, and $f$ are log-precision, and thus can be computed in $\mathrm{O}(\log n)$ space. The overall proof idea is similar, but due to the uniformity condition, the proof becomes substantially more technical. We must not just show the existence of a threshold circuit family computing a transformer, but also show that this circuit family can be generated by a log-space Turing machine.</p>
<p>We first extend Lemma 1 to respect uniformity:
Lemma 2. Let $f:{0,1}^{*} \rightarrow{0,1}^{m}$ be a linear-space computable function. There exists a Turing machine that, for all $n \in \mathbb{N}$ and $c \in \mathbb{R}^{+}$, uses at most $c \log n+\log m$ space to map input $1^{n}$ to a circuit of size at most $n^{c}+c \log n+m$ and depth 3 that computes $f$ on inputs of size at most $c \log n$.</p>
<p>Proof. We give the proof in the form of an algorithm to construct a circuit as a function of $n$ and then justify its correctness and space complexity.</p>
<p>Algorithm. We first print $2 c \log n$ nodes representing unnegated and negated input nodes. ${ }^{15}$</p>
<p>Now, we need to show how to construct nodes corresponding to $n^{c}$ DNF terms. To this end, we loop over all possible inputs $x \in{0,1}^{c \log n}$ by maintaining the $c \log n$ bit binary representation of $x$ (initialized with $0^{c \log n}$ ) and incrementing it by 1 at each step of the loop. We create a new $\wedge$ node $i$ with $c \log n$ arguments, defined as follows. For $j \in[c \log n]$, we create an argument pointer to (unnegated) node $j$ if $x_{j}=1$ and to (negated) node $c \log n+j$ otherwise.</p>
<p><sup id="fnref4:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Now, we construct nodes computing each of the $m$ output nodes. We loop over $k \in[m]$, constructing a single node for each $k$. We loop over all $x \in{0,1}^{c \log n}$ analogously above to construct a list of arguments. By our linear-space computability assumption and because $x$ has $c \log n$ bits, we can compute $f(x)$ as a subroutine in $\mathrm{O}(\log n)$-space to obtain $f_{k}(x)$. If $f_{k}(x)=1$, we print node $2 c \log n+j$ as an argument of node $k$.</p>
<p>Correctness. We show that this Turing machine maps input $n$ to a serialized circuit computing $f$ on inputs of size $n$. The first layer simply produces unnegated and negated input values. The second layer then produce all possible DNF terms. Finally, node $k$ of the third layer computes the disjunction over all terms $x$ such that $f_{k}(x)=1$. Thus, node $k$ of the third layer computes $f_{k}$.</p>
<p>Log Space. To complete the proof, we justify that $M$ uses $\mathrm{O}(\log n+\log m)$ space. Looping over $x \in{0,1}^{c \log n}$ is accomplished by treating $x$ as a binary number initialized to 0 and incrementing it at each step. Thus, the loop pointer for building the DNF terms takes $c \log n$ space to store. For building the $m$ output nodes, we maintain a similar loop pointer as well as an index $k \leq m$, taking $c \log n+\log m$ space. Thus, the overall algorithm uses $c \log n+\log m$ space.</p>
<p>Thus, $M$ uses $c \log n+\log m$ space to map $1^{n}$ to a circuit of size at most $n^{c}+c \log n+m$ and depth 3 that computes $f$ on size $c \log n$ inputs.</p>
<p>We can leverage this lemma to derive the uniform analog of Theorem 1, as follows.</p>
<p>Theorem 2 (Uniform, main result). Any $c \log n$-precision depth-d transformer operating on inputs in $\Sigma^{n}$ can be simulated by a logspace-uniform threshold circuit family of depth $3+\left(9+2 d_{\odot}\right) d$.</p>
<p>Proof. We will provide a proof by induction over transformer layers $\ell$ that there is a Turing machine $M$ operating in $\mathrm{O}(\log n)$ space that, on input $1^{n}$, outputs a circuit that simulates the transformer's computation on inputs of size $n$. This circuit is identical to the one in the proof of Theorem 1, and thus has the same circuit depth.</p>
<p>In the base case, we use log space to track a counter maintaining the current token $i$ (between 1 and $n$ ) throughout the circuit construction. We construct gates encoding the constant $i$ in binary.</p>
<p>We can then apply Lemma 2 to construct a Turing machine that maps $1^{n}$ to a constant-depth threshold circuit computing $\mathbf{h}<em i="i">{i}^{0}=\phi\left(w</em>, i\right)$.</p>
<p>In the inductive case, we assume we can output in $\mathrm{O}(\log n)$ space a circuit computing every value $\mathbf{h}_{i}^{\ell}$ in the previous layer $\ell$. We will show that we can, in $\mathrm{O}(\log n)$ space, now output a circuit computing every value in layer $\ell+1$.</p>
<p>As in Theorem 1, we first fix a head $\mathbf{a}_{i h}^{\ell+1}$ to simulate. Recall (Definition 8) that</p>
<p>$$
\mathbf{a}<em j="1">{i h}^{\ell+1}=\bigoplus</em>}^{n} \frac{s\left(\mathbf{h<em j="j">{i}^{\ell}, \mathbf{h}</em>
$$}^{\ell}\right)}{Z_{i}} \cdot \mathbf{h}_{j}^{\ell</p>
<p>By Lemma 2, we can generate a depth-3 circuit of size at most $z=n^{c^{\prime}}+c^{\prime} \log n+1$, where $c^{\prime}=2 c$ (since the input to $f$ is of size $2 c \log n$ ) that computes $s\left(\mathbf{h}<em j="j">{i}^{\ell}, \mathbf{h}</em>$ of the previous layer.}^{\ell}\right)$ for specific $i, j$. We do this sequentially for $1 \leq j \leq n$ and $1 \leq h \leq k$, padding each circuit with unused nodes so that each one has size exactly $z$, and the $z$-th node corresponds to the output. Thus, the indices of the output nodes for each of the columns will be $w_{\ell}+z(j k+h)$ for $1 \leq j \leq n$, where $w_{\ell}$ is the index of the last output node $\mathbf{h}_{n}^{\ell</p>
<p>At this point, we use the fact that for $p=c \log n$, the $p$-precision approximate sum of $n p$-precision numbers can be computed by a uniform threshold circuit (§A). We can thus use a Turing machine as a sub-routine to generate, on input $1^{n}$, a $k$ threshold circuits, where each has size $z^{\prime}$ that computes an $\oplus$ gate over $n$ items of precision $p$ each. We set the inputs of circuit $h$ to be nodes $w_{\ell}+z(j k+$ $h)$ for $1 \leq j \leq n$. By construction, this yields the normalizing constants $Z_{i}=\bigoplus_{j=1}^{n} s\left(\mathbf{h}<em j="j">{\ell}^{\ell}, \mathbf{h}</em>$ for head $h$.}^{\ell}\right)$, whose value is located at the node at index $w_{\ell}+$ $z n k+z^{\prime</p>
<p>Using $p$-precision arithmetic operator circuits, we can now also generate a circuit to compute $\frac{s\left(\mathbf{h}<em j="j">{i}^{\ell}, \mathbf{h}</em>}^{\ell}\right)}{Z_{i}} \mathbf{h<em _ell="\ell">{j}^{\ell}$ for each $1 \leq j \leq n$ and $1 \leq h \leq k$, by using index $w</em>}+z(j k+h)$ as before for the value of $s\left(\mathbf{h<em j="j">{i}^{\ell}, \mathbf{h}</em>$ by applying $f$ via Lemma 2.}^{\ell}\right)$ and index $w_{\ell}+z n k+z^{\prime} h$ for the normalizing constant $Z_{i}$ of head $h$. Here too we use circuits of identical size $z^{\prime \prime}$, making $w_{\ell}+k\left(z n+z^{\prime}+z^{\prime \prime} i\right)$ the index of the output nodes of these $n$ circuits. Next, we again employ a $\oplus$ circuit of size $z^{\prime}$, similar to the computation of $Z_{i}$, to compute the sum of these $n$ values. Finally, we compute $h_{i}^{\ell+1</p>
<p>Note that this requires keeping only $\ell, i$, and $n$ in memory, each of which takes $\mathrm{O}(\log n)$ bits.</p>
<p>We repeat this process for all $1 \leq i \leq n$ to compute the entire $\ell+1$ layer, which finishes the inductive step: if we can output a circuit computing layer $\ell$ in $\mathrm{O}(\log n)$ space, then we can do the same for layer $\ell+1$.</p>
<p>Because the depth derived in Theorem 2 is constant with respect to $n$, it follows that:</p>
<p>Corollary 2.1 (Uniform, main result). Any log-precision transformer can be simulated by a uniform $\mathrm{TC}^{0}$ circuit family.</p>
<h2>7 Lower Bounds for Instruction Following and Advice Transformers</h2>
<p>So far, we have shown that uniform $\mathrm{TC}^{0}$ is an upper bound for log-precision transformers. Is this upper bound tight, i.e., also a lower bound? While we do not answer this question here, we address a related question as a first step: we construct a transformer that can evaluate $\mathrm{TC}^{0}$ circuits on binary inputs, showing that transformers can compute any $\mathrm{TC}^{0}$ function when their input is augmented with the right "instructions".</p>
<p>More formally, we consider the Circuit Value Problem (CVP) (Ladner, 1975), also referred to as the Circuit Evaluation Problem, where the input is a Boolean circuit $C$ and a string $x \in{0,1}^{n}$, and the task is to return the value of $C(x) \in{0,1}$. This problem is known to be complete for the class P under $\mathrm{AC}^{0}$ reductions (Ladner, 1975). We will assume $C$ is serialized as described in $\S 3$ and prove that log-precision transformers can evaluate any $\mathrm{TC}^{0}$ circuit. Note that this is an extension of the typical CVP since the circuit has threshold gates, not just standard AND/OR gates.</p>
<p>It is known that LSTMs cannot evaluate Boolean formulae (Merrill, 2020), a special case of the CVP. In contrast, we show that transformers can.</p>
<p>To demonstrate the practicality of our lower bound construction, we will not just prove the existence of transformers that can evaluate $\mathrm{TC}^{0}$ circuits but also specify concrete choices for the positional embedding scheme and the class of attention functions that are sufficient to do so.</p>
<p>Fractional Positional Embeddings. For a vector $\mathbf{x}$ and scalar $y$, let $\langle\mathbf{x}, y\rangle$ be the vector appending $y$ onto $\mathbf{x} .{ }^{16}$ For $\sigma \in \Sigma$, let $v(\sigma)$ be the one-hot encoding of $\sigma$ into $\mathbb{R}^{|\Sigma|}$. For $w \in \Sigma^{*}$,</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>we define the fractional positional embedding at token $i$ as</p>
<p>$$
\phi\left(w_{i}, i\right)=\left\langle v\left(w_{i}\right), i / n\right\rangle
$$</p>
<p>Saturated Attention. We imagine $f\left(\mathbf{h}<em j="j">{i}^{\ell}, \mathbf{h}</em>}^{\ell}\right)$ is computed via saturated attention (cf. Merrill et al., 2022), which provides a simple model of the types of attention we can expect to be learned in transformers (Merrill et al., 2021). First, queries are computed as $\mathbf{q<em i="i">{i}=\mathbf{Q} \mathbf{h}</em>}^{\ell}$, and then keys $\mathbf{k<em j="j">{j}=\mathbf{K} \mathbf{h}</em>}^{\ell}$. Define the dot-product attention score $\sigma_{i j}=\mathbf{q<em j="j">{i}^{\top} \mathbf{k}</em>$. We can then define saturated attention as</p>
<p>$$
s\left(\mathbf{h}<em j="j">{i}^{\ell}, \mathbf{h}</em>=\max }^{\ell}\right)= \begin{cases}1 &amp; \text { if } \sigma_{i j<em i="i" k="k">{k} \sigma</em>
$$} \ 0 &amp; \text { otherwise }\end{cases</p>
<p>After normalization, saturated attention creates a distribution that is uniform over a subset of positions. Thus, it is capable of parameterizing hard attention, uniform attention over the full sequence, and various attention patterns in between.</p>
<p>Simple Pooling Functions. For simplicity, we assume pooling functions $f$ are thresholded linear functions of their inputs. Thus, they could be implemented by a feedforward neural net. Without loss of generality, we let attention heads have a value function, which can be folded into the pooling function from the last layer (see $\S 4$ ).</p>
<p>Now, we are ready to present the main result. Our construction below is specific to the circuit serialization scheme discussed in $\S 3$, but can be extended to other serializations as well.
Lemma 3. For all d, there exists a transformer with fractional positional embeddings, saturated attention, thresholded linear pooling functions, and depth $2 d$ that, for any threshold circuit $C$ of depth $d$, maps input $\langle C, x\rangle$ to the value $C(x)$.
Proof. We will construct a pair of two transformer layers that evaluate all the nodes at depth $\ell$ in the threshold circuit, for any $\ell$. It follows that a transformer of depth $2 d$ can compute the value $C(x)$.</p>
<p>We refer to a token of type $X$ as an input node. Similarly, we call a token of type Dir a gate node. Finally we call a token of type \&amp; an argument.</p>
<p>Base Case: Input Nodes. We construct one attention layer that attends uniformly over all positions whose value returns 1 if $w_{i}=\mathrm{X}$ and 0 otherwise. Thus, this head computes $#(\mathrm{X}) / n$,</p>
<p>where $#(\mathrm{X})$ is the number of occurrences of X in $w$. We then define a second layer that, at input node $i$, retrieves the token at position $j$ defined by</p>
<p>$$
j=\frac{1-#(\mathrm{X})+i}{n}
$$</p>
<p>$j$ is the index of the $i$ th input value. Thus, after this layer, each input node $i$ stores its value $x_{i}$.</p>
<p>In the base case, we also construct an attention head that, at the $i$ th gate node, counts the fraction of nodes (out of $n$ ) that are gate nodes to the left of the current node. Thus, this head computes $i / n$. Also at gate node $i$, we construct an attention head that counts the fraction of nodes to its right before the next \&amp; node that have value 1 . This head thus has value $k_{i} / m_{i}$ where $k_{i}$ is the threshold value of the $i$-th gate and $m_{i}$ is its arity. We apply the same construction at each argument \&amp; to count the 1's that follow until the next non-1 symbol.</p>
<p>Finally, using the first attention layer, we have each $J$ node attend to the first argument symbol \&amp; to its left and retrieve its index $j / n$. Then, in the second attention layer, each argument attends uniformly over all nodes with values $j / n$. The net effect is for each argument node to store $j / n$, i.e., the pointer it is encoding in unary as $\&amp; 1^{j}$.</p>
<p>Inductive Case: Gate Nodes. By our inductive assumption over prior layers, all tokens corresponding to circuit nodes at depth $\leq \ell$ contain their appropriate value. We now construct 2 transformer layers to evaluate gate nodes at depth $\ell+1$.</p>
<p>In the first attention layer, each argument $j$ attends to the the closest gate node $i$ to its left, which is the gate it belongs to. Recall from the base case that argument $j$ already stores $j / n$. Each argument $\&amp; 0^{j}$ attends with position key $j / n$ to gate node $j$ and retrieves its value in the previous layer.</p>
<p>The second attention layer applies at gate nodes, not arguments. At gate $i$ of arity $m_{i}$, we set the attention $s(i, j)$ to indicate whether argument $j$ belongs to gate node $i$, which holds for exactly $m$ arguments. We set the attention value to be the binary value of the referent of argument $j$. Thus, the attention head computes $c_{i} / m_{i}$, where $c_{i}$ is the number of arguments of node $i$ that are 1 . We repeat this for all gate nodes.</p>
<p>At this point, for the $i$-th gate node, we have computed both $c_{i} / m_{i}$ and $k_{i} / m_{i}$. Thresholding $\left(c_{i}-k_{i}\right) / m_{i}$ at 0 allows us to decide, based on
whether Dir is $&lt;=$ or $&gt;=$, whether the current gate node should output a 0 or a 1 . Repeating this for all gates at layer $\ell+1$ completes the inductive step: We can evaluate all gate nodes in this layer.</p>
<p>Theorem 3. Depth-2d transformers can solve CVP for depth-d $\mathrm{TC}^{0}$ circuits.</p>
<h3>7.1 Instruction Following</h3>
<p>CVP is closely related to instruction learning (Brown et al., 2020) and instruction following tasks (Finlayson et al., 2022). The latter task setup provides a transformer two inputs: a regular expression $r$ as an "instruction", and $z \in{0,1}^{*}$. The goal of the task is to return whether $z$ belongs to the regular language represented by $r$. Viewed from this lens, the circuit evaluation setup asks: Can transformers follow instructions provided in the form of a circuit? As discussed below, our result says the answer is yes for all constant depth threshold circuits. This, to the best of our knowledge, provides the first non-trivial lower bound for transformers in the instruction learning setting.</p>
<p>Formally, an instruction $I$ is any description ${ }^{17}$ of a function $f_{I}$ of ${0,1}^{<em>}$. We say a transformer correctly follows an instruction $I$ if, for all $x \in{0,1}^{</em>}$, it correctly computes $f_{I}(x)$ on input $\langle I, x\rangle$. A non-uniform instruction description is a family of length-specific descriptions $\left{I_{n}\right}<em n="n">{n=1}^{\infty}$. We say a transformer correctly follows a non-uniform instruction family $\left{I</em>$ circuit, it follows that:
Corollary 3.1. There exists a depth- $2 d$ transformer that can correctly follow any depth-d $\mathrm{TC}^{0}$ instruction description.}\right}$ if, for all $n$ and all $x \in{0,1}^{n}$, it correctly computes $f_{I}(x)$ on input $\left\langle I_{n}, x\right\rangle$. The non-uniform description $\left{I_{n}\right}$ may take any form. When it forms a $\mathrm{TC}^{0}$ circuit family, we refer to it as a $\mathrm{TC}^{0}$ instruction description. Since Theorem 3 constructs a transformer that can evaluate any $\mathrm{TC}^{0</p>
<p>Thus, transformers with simple position embeddings, attention, and pooling functions can simulate any instruction provided in the form of a $\mathrm{TC}^{0}$ circuit. We note that while it is unknown whether the class of regular languages, considered by Finlayson et al. (2022), is contained in $\mathrm{TC}^{0}$, the other side is known: There are problems computable by $\mathrm{TC}^{0}$ circuits that are not computable</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>by a regular language. These include problems involving counting and arithmetic, which are beyond regular languages. Our results thus expand the known kinds of instructions transformers are able to follow, at least with hand-constructed weights.</p>
<h3>7.2 Advice Transformers</h3>
<p>We can also view circuit evaluation abilities of transformers (Lemma 3) from the lens of advice-taking Turing machines which, in addition to their usual input, are also provided an input length dependent (but input independent) advice string. For instance, P/poly is the class of problems decidable in polynomial time when the Turing machine is given an advice string of size polynomial in the input length (cf. Arora and Barak, 2009).</p>
<p>In the same vein, let $T /$ poly be the class of log-precision, constant-depth transformers with polynomial advice strings. In other words, on an input of size $n$, we allow the transformer to receive an additional $\operatorname{poly}(n)$ bits of input that cannot depend on the standard input. Now let $\left{C_{n}\right}<em n="n">{n=1}^{\infty}$ be a circuit family demonstrating that a problem is in non-uniform $\mathrm{TC}^{0}$. Then, by passing the description of $C</em>$ :}$ as advice for input length $n$, it immediately follows from Lemma 3 that advice transformers can simulate non-uniform $\mathrm{TC}^{0</p>
<p>Corollary 3.2. Non-uniform $\mathrm{TC}^{0} \subseteq \mathrm{~T} /$ poly.
Since non-uniform $\mathrm{TC}^{0}$ even contains some undecidable languages (Arora and Barak, 2009, Claim 6.8), T/poly is clearly a very powerful class and a strict superset of $T$, the class of decision problems recognized by transformers (which are all decidable). Thus, a problem in T/poly cannot always be solved by a transformer on its own. However, if given a description of how to do so ("advice") in the form of a $\mathrm{TC}^{0}$ circuit, our result shows that a transformer could solve that problem.</p>
<h2>8 Conclusion</h2>
<p>Answering two open questions from Merrill et al. (2022), we prove log-precision transformers with any (including soft) attention can be simulated by uniform constant-depth threshold circuits. This establishes thresholded addition as a fundamental operation for understanding the computational model of transformers: Any log-precision transformer can be re-expressed as a polynomial number of threshold gates stacked to a constant
depth. This result also establishes potential limits on the computational power of log-precision transformers; e.g., if $\mathrm{L} \subset \mathrm{P}$, transformers cannot compute all poly-time functions. They are certainly very far from being universal. The intuition at the heart of this result is that forcing a model to be highly parallelizable likely sacrifices its expressiveness. Since parallelism seems essential to pretraining any massive model at scale, any large language model-transformer or otherwise-may suffer from a similar tradeoff.</p>
<h2>Acknowledgments</h2>
<p>The authors are grateful for the valuable feedback from the anonymous reviewers and the TACL action editor. They also thank Paul Beame and colleagues at AI2 including Kyle Richardson, Michal Guerquin, Peter Clark, Tushar Khot, and especially Matthew Finlayson, whose empirical findings about instruction learning inspired $\S 7$. Feedback from Sam Bowman, Arya McCarthy, Roma Patel, and Lena Strobl, and discussions with the FLaNN, ML for Code (MILA), and Foundations of Language Processing (Umeå) research groups helped improve earlier drafts. The authors also appreciate Rahul Santhanam's feedback. This work was funded in part by NSF award 1922658. William Merrill was supported by an NSF graduate research fellowship and by AI2.</p>
<h2>References</h2>
<p>Eric Allender. 1999. The permanent requires large uniform threshold circuits. Chicago Journal of Theoretical Computer Science.</p>
<p>Sanjeev Arora and Boaz Barak. 2009. Computational Complexity: A Modern Approach. Cambridge University Press. https://doi .org/10.1017/CBO9780511804090</p>
<p>Arin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh. 2009. Handbook of Satisfiability: Volume 185 Frontiers in Artificial Intelligence and Applications. IOS Press.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya</p>
<p>Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Tom Bylander. 1991. Complexity results for planning. In Proceedings of the International Joint Conference on Artificial Intelligence. https://doi.org/10.1016 /B978-0-08-049944-4.50008-2</p>
<p>Andrew Chiu, George I. Davida, and Bruce E. Litow. 2001. Division in logspace-uniform nc1. RAIRO Theoretical Informatics and Applications, 35:259-275. https://doi.org/10 .1051/ita:2001119</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal transformers. In International Conference on Learning Representations.</p>
<p>Tim Dettmers, Mike Lewis, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread.</p>
<p>Matthew Finlayson, Kyle Richardson, Ashish Sabharwal, and Peter Clark. 2022. What makes instruction learning hard? An investigation and
a new challenge in a synthetic environment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Raymond Greenlaw, James M. Hoover, and Walter L. Ruzzo. 1991. A compendium of problems complete for P. Technical Report TR91-11, University of Alberta.</p>
<p>Michael Hahn. 2020. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171. https://doi.org /10.1162/tacl_a_00306</p>
<p>Yiding Hao, Dana Angluin, and Robert Frank. 2022. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10:800-810. https://doi.org/10.1162/tacl_a_00490</p>
<p>William Hesse. 2001. Division is in uniform $T C^{0}$. In International Colloquium on Automata, Languages, and Programming, pages 104-114. https://doi.org/10 .1007/3-540-48224-5_9</p>
<p>Neil Immerman. 2012. Descriptive Complexity. Springer Science \&amp; Business Media.</p>
<p>Neil D. Jones and William T. Laaser. 1976. Complete problems for deterministic polynomial time. Theoretical Computer Science, 3(1):105-117. https://doi.org /10.1016/0304-3975(76)90068-2</p>
<p>Richard E. Ladner. 1975. The circuit value problem is log space complete for P. ACM SIGACT News, 7(1):18-20. https://doi.org/10 .1145/990518.990519</p>
<p>Harry R. Lewis and Christos H. Papadimitriou. 1982. Symmetric space-bounded computation. Theoretical Computer Science, 19:161-187. https://doi.org/10.1016/03045 -3975(82) 90058-</p>
<p>William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. 2021. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. https://doi.org /10.18653/v1/2021.emnlp-main. 133</p>
<p>William Cooper Merrill. 2020. On the linguistic capacity of real-time counter automata. ArXiv, abs/2004.06866.</p>
<p>William Cooper Merrill, Ashish Sabharwal, and Noah A. Smith. 2022. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843-856. https:// doi.org/10.1162/tacl_a_00493</p>
<p>Jorge Pérez, Javier Marinković, and Pablo Barceló. 2019. On the Turing completeness of modern neural network architectures. In International Conference on Learning Representations.</p>
<p>Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140).</p>
<p>Omer Reingold. 2008. Undirected connectivity in log-space. Journal of the ACM, 55:17:1-17:24. https://doi.org/10.1145/1391289 . 1391291</p>
<p>Leslie G. Valiant. 1979. The complexity of computing the permanent. Theoretical Computer Science, 8:189-201. https://doi.org /10.1016/0304-3975(79)90044-6</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<h2>A Iterated $p$-Precision Float Addition</h2>
<p>We interpret a $p$-bit string $x$ as a $p$-precision float by taking the first $p / 2$ bits ${ }^{18}$ of $x$ as a signed integer $m$ encoding the mantissa and the remaining $p / 2$ bits of $x$ as another signed integer $e$ encoding the exponent. A float with mantissa $m$ and exponent $e$, denoted $\langle m, e\rangle$, encodes $m \cdot 2^{e}$.</p>
<p>Computing the sum of $n n$-bit integers (known as iterated addition or simply summation) is well-known to be in uniform $\mathrm{TC}^{0}$ (Hesse, 2001; Chiu et al., 2001). We leverage this fact to</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>show that the same holds for the sum of $n$ $\mathrm{O}(\log n)$-precision floats. A subtlety of adding $p$-precision floats is that their sum can require more than $p$ bits to represent precisely as a float. For instance, while each of $2^{r}$ and 1 is representable with a (signed) mantissa of only 2 bits, their exact sum, $2^{r}+1$, requires a mantissa of $r+1$ bits. Hence, $p$-precision transformers must sacrifice some precision when performing summation.</p>
<p>We define float addition by mapping the floats to integers, adding the integers exactly, and then mapping the sum back to a float (with possible loss of precision). Let $I_{q}^{\max }=2^{q}-1$ be the greatest $q$-bit signed integer, and $I_{q}^{\min }=-I_{q}^{\max }$. Let $F_{p}^{\max }$ be the greatest value representable by a $p$-precision float. Since the exponent of a float $\phi$ can be negative and represent a fraction, we rescale $\phi$ by $2^{-I_{p / 2}^{\min }}$ when mapping it to an integer $g_{p}(\phi)$ :</p>
<p>Definition 13. The integer mapping of a $p$-bit float $\phi=\langle m, e\rangle$ is defined as $g_{p}(\phi)=m \cdot 2^{e-I_{p / 2}^{\min }}$.</p>
<p>Definition 14. The $p$-truncated float mapping of an integer $z$ is defined as $f_{p}(z)=\langle m, e\rangle$ where ${ }^{19}$</p>
<p>$$
\begin{aligned}
m &amp; =\operatorname{rshift}(z, \max {0, \operatorname{sizeof}(z)-p / 2}) \
e &amp; =\operatorname{sizeof}(z)-\operatorname{sizeof}(m)+I_{p / 2}^{\min }
\end{aligned}
$$</p>
<p>when $e \leq I_{p / 2}^{\max }$; otherwise (i.e., when $z&gt;F_{p}^{\max }$ ), we set $m=e=I_{p / 2}^{\max }$ to properly handle overflow.</p>
<p>Definition 15 (Iterated $p$-precision float addition). We define the sum of $k p$-precision floats as</p>
<p>$$
\bigoplus_{i=1}^{k} \phi_{i}=f_{p}\left(\sum_{i=1}^{k} g_{p}\left(\phi_{i}\right)\right)
$$</p>
<p>We first verify that Definition 14 closely approximates exact addition.</p>
<p>Lemma 4. Let $\phi=\langle e, m\rangle$ be a float such that $|\phi| \leq F_{p}^{\max }$ and $e \geq I_{p / 2}^{\min }$. Then $\phi$ and $f_{p}\left(g_{p}(\phi)\right)$ differ by a factor of at most $1 \pm 2^{-p / 2+2}$.</p>
<p>Proof. Let $z=g_{p}(\phi)$, which is well-defined because of the precondition $e \geq I_{p / 2}^{\min }$ of the lemma. Let $\phi^{\prime}=\left\langle m^{\prime}, e^{\prime}\right\rangle=f_{p}(z)$.</p>
<p><sup id="fnref5:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>First consider the easy case where $\operatorname{sizeof}(z) \leq$ $p / 2$. Then $m^{\prime}=z$ and $e^{\prime}=I_{p / 2}^{\min }$ from Definition 14. Since $z=m \cdot 2^{e-I_{p / 2}^{\min }}$ by Definition 13, it follows that $\phi$ and $\phi^{\prime}$ have exactly the same value.</p>
<p>Now assume $\operatorname{sizeof}(z)&gt;p / 2$. It follows from the precondition $|\phi| \leq F_{p}^{\max }$ of the lemma that there is no overflow when applying Definition 14 to compute $\left\langle m^{\prime}, e^{\prime}\right\rangle$. Thus $m^{\prime}$ consists of the $p / 2$ highest-order bits (including the sign bit) of $z$ and $e^{\prime}=\ell+I_{p / 2}^{\min }$, where $\ell=\operatorname{sizeof}(z)-p / 2$ is the number of bits truncated from $z$ to obtain $m^{\prime}$. Let $\delta$ denote the (non-negative) integer formed by the $\ell$ lowest-order bits of $z$ that are truncated. Then $\delta \leq 2^{\ell}-1=2^{\operatorname{sizeof}(z)-p / 2}-1&lt;z \cdot 2^{-p / 2+2}$.</p>
<p>Recall that the value of $\phi$ is $g_{p}(\phi) \cdot 2^{-I_{p / 2}^{\min }}=$ $z \cdot 2^{-I_{p / 2}^{\min }}$. By the above argument, we also have that the value of $\phi^{\prime}$ is within $(z \pm \delta) \cdot 2^{-I_{p / 2}^{\min }}$, which is within $z \cdot\left(1 \pm 2^{-p / 2+2}\right) \cdot 2^{-I_{p / 2}^{\min }}$. Thus, $\phi$ and $\phi^{\prime}$ are within a factor of $1 \pm 2^{-p / 2+2}$ of each other.</p>
<p>Finally, we show that, with $\log$ precision, computing $\oplus$ (Definition 14) is in uniform TC ${ }^{0}$.</p>
<p>Lemma 5. Let $p \leq c \log n$ and $\phi=\bigoplus_{i=1}^{k} \phi_{i}$, where $k \leq n$ and each $\phi_{i}$ is p-precision. Then $\phi$ is computable by a constant-depth uniform threshold circuit of size poly $(n)$.</p>
<p>Proof. Let $N=c \log n+2 n^{c}$. We first use Lemma 1 to map each $\phi_{i}=\left\langle m_{i}, e_{i}\right\rangle$ to the integer $z_{i}=m_{i} \cdot 2^{e_{i}-I_{p / 2}^{\min }}$, which has size $\operatorname{sizeof}\left(m_{i}\right)+$ $\left(e_{i}-I^{\min }\right) \leq p / 2+2 \cdot 2^{p / 2} \leq c \log n+2 n^{c}=N$. For $1 \leq i \leq k$, we pad $z_{i}$ to $N$ bits, and for $k&lt;i \leq N$, we create an $N$-bit integer $z_{i}=0$. We can then compute $z=\sum_{i=1}^{k} z_{i}$ with a constant-depth uniform threshold circuit of size poly $(N)$ using the classical construction to sum $N N$-bit integers (cf. Immerman, 2012, exercise 5.29). The size of this circuit is also polynomial in $n$ by the definition of $N$. Finally, we compute $f^{\dagger}(z)$ using a constant-depth AND/OR circuit.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{18}$ We assume w.l.o.g. that $p$ is even.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{19}$ For $x \neq 0, \operatorname{sizeof}(x)=\lfloor\log |x|\rfloor+2 ; \operatorname{sizeof}(0)=2$. For $y \geq 0, \operatorname{rshift}(x, y)$ right-shifts $x$ by $y$ bits.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
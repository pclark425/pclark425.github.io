<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2602 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2602</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2602</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-04d64be16fb402f28348faffef484bd419c8bd8f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04d64be16fb402f28348faffef484bd419c8bd8f" target="_blank">Self-Rewarding Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work shows that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself is shown during Iterative DPO training.</p>
                <p><strong>Paper Abstract:</strong> We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2602.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2602.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Rewarding LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Rewarding Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-alignment framework in which a single LLM both generates instruction-following outputs and judges those outputs (LLM-as-a-Judge) to create preference data that is used to fine-tune the same model via Iterative DPO, producing successive improved models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Rewarding Language Models (SR-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A single LLM is trained to perform two roles: (1) generate instruction-following responses (generation/assistant role) and (2) evaluate candidate responses via LLM-as-a-Judge prompting to produce scores and rankings (reward modelling role). The procedure iterates: starting from a seed SFT model (IFT + optional EFT), the model (Mt) generates new prompts (prompt generation done by a fixed model in main experiments), samples N candidate responses per prompt, evaluates those responses multiple times (sampling the judge outputs and averaging) to produce scalar scores (0-5), forms preference pairs (highest vs lowest scored responses, discarding ties), and trains the next model (Mt+1) with DPO on a training set augmented by these self-generated preference pairs (AIFT). Key capabilities: autonomous data generation (prompts, responses), self-evaluation into preference pairs, offline preference-based fine-tuning (DPO), and iteration to improve both generation and self-evaluation abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated self-alignment / automated data-generation and reward-modeling system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Natural language model alignment and instruction-following (NLP / language model fine-tuning / reward modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve instruction-following behavior of a large language model and simultaneously improve its reward-modeling ability without relying on large external human preference datasets or a frozen external reward model. The system must create new instruction prompts (via few-shot prompt generation), produce multiple candidate responses, evaluate them consistently, and generate preference pairs usable for preference-tuning (DPO).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: involves large discrete output spaces (text), multi-attribute quality evaluation (relevance, coverage, usefulness, clarity, expertise), stochastic generation and evaluation processes, and multi-iteration dynamics where the evaluation model changes across iterations; search/control space includes prompt space, response space (high-dimensional), and evaluation scoring; multi-objective trade-offs (helpfulness vs harmlessness). Quantities reported: N=4 candidate responses per prompt; judge evaluations averaged over 3 sampled decodings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Seed data: small human-authored Instruction Fine-Tuning (IFT) set from Open Assistant (3200 high-quality English first-turn examples) and an Evaluation Fine-Tuning (EFT) set derived from Open Assistant (1,630 train, 541 eval after processing). Self-generated AIFT data sizes: AIFT(M1) = 3,964 preference pairs; AIFT(M2) = 6,942 pairs. Data quality: seed human data is high-quality; self-generated data quality improves across iterations as measured by correlation with human rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses Llama 2 70B as base model. Training used supervised fine-tuning and DPO with batch size 16, specific learning rates (SFT lr 5.5e-6 decaying to 1.1e-6; DPO lr 1e-6 decaying to 1e-7, beta=0.1). Generation decoding parameters: T=0.7, p=0.9, judge evaluation repeated 3 times per candidate and averaged. Exact wall-clock compute hours / GPU counts not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Semi-structured and partly well-defined: discrete sequence-generation problem (stochastic), clear evaluation metrics exist (preference pairs, pairwise agreement with human rankings, Spearman/Kendall correlations, head-to-head win rates, leaderboard win rates, MT-Bench scores). Open-ended in that generated prompts/responses can be diverse; evaluation (LLM-as-a-Judge) is learned and changes over iterations (non-stationary). Requires domain knowledge mainly in instruction-following and evaluation criteria rather than domain-specific scientific expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Multiple metrics: instruction-following head-to-head win rates (GPT-4 evaluator and human annotators), AlpacaEval 2.0 leaderboard win rate vs GPT-4 Turbo (GPT-4 judgments), MT-Bench numeric score (out of 10), and reward-modeling metrics (pairwise accuracy vs human rankings, exact match of full ranking, Spearman and Kendall correlations, percent of model-scored 5s that are highest-ranked by humans).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Quantified improvements across iterations: AlpacaEval win rate vs GPT-4 Turbo: M1 9.94%, M2 15.38%, M3 20.44%. Head-to-head (GPT-4) comparisons: M2 vs M1 — M2 wins 55.5% (M1 wins 11.7%); M2 vs SFT baseline — M2 wins 49.2% (baseline wins 14.5%). M3 vs M2 — M3 wins 47.7% (M2 wins 12.5%); M3 vs SFT baseline — M3 wins 62.5% (baseline wins 9.8%). MT-Bench overall scores: SFT baseline 6.85, M1 6.78, M2 7.01, M3 7.25 (out of 10). Reward modeling (alignment with human rankings): pairwise accuracy — SFT 65.1%, M1 78.7%, M2 80.4%, M3 81.7%; Spearman corr: 0.253 -> 0.279 -> 0.331 -> 0.349 across SFT, M1, M2, M3 respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed weaker improvements or limited gains on mathematics and logical reasoning tasks and on some benchmark NLP tasks (e.g., ARC-challenge, GSM8K showed mixed trends). Potential biases observed: generations grew longer across iterations (avg lengths increased substantially) which may correlate to perceived quality; risks include reward-hacking (not fully explored), judge-model overfitting to its own artifacts, and saturation of gains (only three iterations studied). Safety evaluation and lengthy/hallucinated outputs remain unassessed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of a small high-quality seed dataset (IFT and EFT) that teaches the model to act as a judge; incorporation of explicit EFT examples to train the LLM-as-a-Judge (substantially increased valid AIFT pairs versus starting from IFT alone); using preference pairs (winning vs losing responses) rather than only positive examples; iterative DPO fine-tuning that leverages self-produced preference data; careful LLM-as-a-Judge prompt design (additive 5-point criteria) which produced far better judge alignment with human ranking than alternative prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Iterative self-rewarding training consistently improved both generation quality and reward-model alignment across three iterations. Starting from IFT+EFT (M1) then adding AIFT(M1) to train M2 produced a large jump in instruction-following and judge metrics; further adding AIFT(M2) to train M3 produced further gains. Models trained without EFT had far fewer valid AIFT samples and lagged substantially behind corresponding models trained with EFT (EFT is important). Gains were larger in humanities, writing, roleplay and extraction categories; smaller in math/code/reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human baseline is not provided as a single numeric oracle in the paper for the same tasks; comparisons are made against a supervised fine-tuning (SFT) baseline and against large systems: the Iteration 3 SR-LM achieves 20.44% win rate vs GPT-4 Turbo on AlpacaEval (GPT-4 judgments), and the paper notes that many leaderboard models use proprietary data or distilled targets—no direct human researcher performance numbers are given for the same evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2602.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2602.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge prompting / evaluation mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that formulates response evaluation as an instruction-following task, producing chain-of-thought justification and a scalar score; used here to make the LLM act as its own reward model and produce preference labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use of an LLM prompted with explicit evaluation criteria to score candidate responses (here a 5-point additive criteria: relevance, coverage, usefulness, clarity, expertise). The judge produces justifications and final scores; evaluations are generated multiple times (sampling) and averaged to reduce variance. The judge outputs are converted into preference pairs for DPO training.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated evaluation / synthetic reward-model generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Automated evaluation for language model outputs and synthetic reward-label generation for alignment/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide consistent, human-aligned scalar scores or rankings for model-generated responses so they can be used as training signals (preference pairs) in preference-based fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: must parse and apply multi-criteria evaluation consistently across diverse prompts and responses, handle stochastic decoding variance, and produce scores that correlate with human rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained initially with EFT seed examples (1,630 train, 541 eval) derived from Open Assistant; when EFT is absent judge outputs collapsed to common scores making few valid pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Judge outputs are produced via LLM sampling; the paper uses 3 sampled decodings per evaluation and averages scores. No absolute compute/time figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Stochastic evaluation; clear scalar output expected; evaluation criteria are explicit so metric is well-defined, but alignment to human preferences depends on prompt formulation and training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Agreement with human rankings: pairwise accuracy, exact match of ordering, Spearman and Kendall correlations, percent of model 5/5 rated responses being top-ranked by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Using the additive scoring prompt and EFT training, pairwise accuracy for the judge on held-out human-evaluated data increased from 65.1% (SFT baseline) to 78.7% (M1), then to 80.4% (M2) and 81.7% (M3). Alternative (multiple-choice) prompts performed poorly: 26.6% pairwise accuracy for the multiple-choice prompt on the same SFT model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without explicit EFT training data, judge outputs often collapse to a common score (e.g., many scores of 4), producing few usable preference pairs. Prompt format matters greatly; poorly structured judge prompts lead to low agreement with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit multi-criteria additive scoring prompt and seed EFT examples; averaging multiple sampled judge decodings; training the model to perform evaluation as an instruction-following task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Additive scoring prompt vastly outperformed the multiple-choice prompt variant (pairwise accuracy 65.1% vs 26.6% on SFT baseline), demonstrating prompt design is critical. EFT augmentation improved judge performance relative to SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human rankings used as ground truth for judge evaluation; judge pairwise accuracy up to 81.7% vs human rankings for the best SR-LM iteration, but no single 'human evaluator' quantitative baseline (e.g., inter-annotator agreement) reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2602.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2602.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative DPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Direct Preference Optimization (Iterative DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative preference-based fine-tuning scheme where each model generates preference data that is used to train the next model using DPO, enabling multiple rounds of self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative DPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DPO (Direct Preference Optimization) is used to train models on preference data (winning vs losing responses) directly. Iterative DPO repeats a loop where a model's generations are judged (by a judge model or LLM-as-a-Judge) to produce new preference pairs which train the next model via DPO. In this work, the judge is the same model (self-rewarding) or seeded via EFT.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated preference-based fine-tuning / iterative training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Language model alignment and instruction-following improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve an LLM's behavior by repeatedly augmenting its training set with preference pairs generated (and judged) by the prior iteration's model and applying DPO to learn from those preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Iterative, non-stationary optimization with changing data distribution; requires stability across iterations to avoid collapse or reinforcement of artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on number of valid preference pairs generated each iteration (paper reports 3,964 and 6,942 pairs for two iterations when using EFT; far fewer when starting from IFT only).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses DPO training cycles; specific hyperparameters provided (batch size 16, DPO lr 1e-6→1e-7, beta 0.1). Exact compute not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Discrete, stochastic generation with clear pairwise preference labels used for supervised preference-based fine-tuning; evaluation metrics are well-defined (win rates, judge alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Improvement in instruction-following win rates, leaderboard placement, and judge/human agreement metrics across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In this work, Iterative DPO with self-rewarding judge produced substantial gains across iterations (see SR-LM numbers). The paper also notes previous work (PCO / Iterative DPO in Xu et al. 2023) showed improvements over single-step DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Potential for saturation of gains, generating skewed data distributions (e.g., length bias), and dependence on judge quality; insufficient valid preference pairs if judge is not well-trained.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality of initial seed data (IFT+EFT), judge quality, sufficient diversity and validity of generated preference pairs, and stable DPO optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Iterative DPO with a self-improving judge (this paper) outperformed the iteration-1 and SFT baselines; prior work with an external fixed reward model also showed Iterative DPO improvements over single-step DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Compared against SFT baseline and various strong LLMs; iterative DPO models eventually outperform SFT baseline and approach/beat many strong models on AlpacaEval as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2602.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2602.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Instruct (self-instruction creation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method for generating instruction prompts and responses from a model via few-shot prompting to augment instruction-following training data without human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-instruct: Aligning language models with self-generated instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Instruct prompt-based data generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses few-shot prompting to generate new instruction prompts (and optionally responses); in this paper a fixed Llama 2-Chat 70B model with 8-shot prompting was used to produce new prompts, with additional filtering (ROUGE-L similarity checks, keyword and length filters).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated data-generation / prompt generation system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Generating diverse instruction prompts for instruction-following fine-tuning in NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Create new instruction prompts to expand training data coverage without manual labeling, enabling subsequent candidate response generation and preference labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Relatively low complexity to produce prompts, but quality and diversity control are important to avoid low-value or redundant prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Prompt generation is synthetic; quantity depends on sampling and filtering; in paper prompt generation was fixed to a pretrained prompt-generator model rather than the self-rewarding model in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Generation cost for prompts only (decoding parameters reported T=0.6, p=0.9) — modest relative to response generation for large-scale fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended creative generation task with post-filtering for quality; evaluation is by heuristics and downstream effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Usefulness of generated prompts as judged by the quality/diversity of resulting candidate responses and contribution to improved downstream instruction-following after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified as a standalone system in this paper; used as a component to provide prompts for SR-LM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Poor prompt quality or low diversity can reduce effectiveness; reliance on fixed prompt-generator model in main experiments to control quality.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Few-shot examples drawn from high-quality IFT data and filtering (ROUGE-L, keywords, length) improve prompt usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper used Self-Instruct style prompt generation as a component; does not provide direct head-to-head comparisons of different prompt-generation methods here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not applicable; prompts are synthetic and evaluated by downstream model improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2602.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2602.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLAIF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from AI Feedback (RLAIF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that use an LLM (rather than humans) to provide feedback to train reward models or directly fine-tune agents, e.g., via creating labeled preferences or training a reward model used for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RLAIF / LLM-feedback pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An off-the-shelf LLM is used to produce feedback (rankings, critiques) on candidate model outputs; that feedback can be used to train a separate fixed reward model which is then used in RL (PPO) or to directly create training data for fine-tuning. Alternatives include using the LLM-as-a-Judge as a fixed curation model or as part of iterative offline pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated feedback-based reward construction / automated curation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Language model alignment and training via synthetic feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Replace or augment human preference data with synthetic judgments from LLMs to scale preference data for reward training or direct training.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Depends on quality of off-the-shelf LLM judge and mismatch between judge and human preferences; introduces non-stationarity if judge differs from downstream evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Synthetic feedback is cheap to generate at scale but quality varies; paper cites Lee et al. (2023) who compared RLAIF and RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Generation of judge outputs at scale can be expensive if using large external LLMs within RL loops; Lee et al. reported computational expense when using the judge inside PPO (vs offline).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined preference labeling objective, but judge-model bias and computational cost of using powerful external LLMs can be limiting.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Improvement vs RLHF baselines, final model quality and alignment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper cites Lee et al. (2023) stating RLAIF and RLHF methods compared performed roughly equally in the experiments reported there. No numeric success rates given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Computational expense (especially within online RL), judge-model bias, potential misalignment with human preferences, and cost of obtaining large-scale high-quality synthetic judgments if judge is expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality and calibration of the off-the-shelf judge model; using judge outputs offline for dataset construction is cheaper than online use during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>RLAIF methods can roughly match RLHF in some reported comparisons (Lee et al. 2023) but differ in computational trade-offs; this paper prefers an offline iterative DPO approach for cost and stability reasons.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Compared qualitatively against RLHF (human-labeled preference pipelines) — reported to be roughly comparable in Lee et al. 2023 according to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2602.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2602.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforced Self-Training (ReST)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses a fixed, external reward to curate high-quality examples iteratively and add them to the training set to improve language model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforced self-training (rest) for language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reinforced Self-Training (ReST)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses an external reward signal (fixed) to select or curate positive examples generated by a model; those curated examples are added to the training set iteratively to improve model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated curation / self-training</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Language modeling and instruction-following improvement via curated self-generated data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Curate model-generated outputs with an external reward to form training targets that iterate into improved model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Less complex conceptually but depends on a reliable external reward; selection/curration thresholds matter.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on plentiful model-generated outputs and an external scoring function or reward model to filter them.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Costs for generation and scoring; exact numbers not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended generation plus classification/selection by reward model; straightforward to implement but quality depends on reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Downstream improvement on target tasks after iterative fine-tuning with curated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper references ReST as having been used to iteratively add high-quality examples and improve performance; no numeric rates in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If the external reward is miscalibrated, curated data can be poor and degrade performance; limited to the capabilities of the reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Robust external reward and good diversity in generated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper notes ReST uses a fixed external reward in contrast to the self-improving judge used in SR-LM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Compared implicitly to approaches that use human-curated examples; no direct numeric comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2602.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2602.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPIN (Iterative DPO-like framework avoiding reward models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent method that avoids reward models entirely in an Iterative DPO-like framework by using human labels as the winning responses and previous-iteration generations as losing responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SPIN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs preference pairs where the human-provided response is treated as the winning response and the last iteration's model-generated responses are treated as losing responses; uses these to train via an Iterative DPO-like process without training a reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Iterative human-anchored preference training / automated fine-tuning approach</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Instruction-following model alignment where human references exist for inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Leverage human labeled responses and previous model outputs to create preference training data iteratively and fine-tune models via DPO-like methods without a separate reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Requires human-annotated targets for each input; limited by availability of human references (bottleneck once model reaches human-level performance).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires human annotated response for each input prompt; paper notes this is a limitation relative to fully synthetic self-rewarding pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Similar to iterative DPO training but does not require building or using a separate reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined when human targets exist; not applicable to domains lacking human-supervised responses.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Improvement over iterations relative to baselines; avoids dependence on reward model training.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper; paper cites SPIN as having advantage of avoiding reward models but notes limitations (human target requirement and bottlenecking).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Bottleneck: requires a human response per input, so gains stop when model reaches human performance; costly data requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of high-quality human reference responses for inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Contrasted with SR-LM which can generate both responses and rewards synthetically, SPIN trades off by requiring human targets but avoiding reward model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>SPIN explicitly uses human responses as the winning examples, so performance is anchored to human targets; no numeric comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2602.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2602.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Backtranslation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Backtranslation (Self-alignment with instruction backtranslation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation and curation approach that augments the instruction dataset by backtranslating from web documents and uses the LLM (or LLM-as-a-Judge) to curate examples, considered a specialized form of self-rewarding model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-alignment with instruction backtranslation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Instruction Backtranslation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates synthetic instruction–response pairs by transforming web documents into prompts (backtranslation) and uses the LLM itself to curate or score those pairs for inclusion in training; the curation step can be seen as self-rewarding.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated data-augmentation and curation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Creating and curating instruction-following datasets to improve LLM alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Expand training data coverage by deriving prompts/responses from available web content and automatically selecting high-quality examples using the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Depends on document coverage, quality of backtranslation heuristics, and reliability of LLM curation; could be large-scale.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses abundant web documents as raw sources; curation reduces usable samples to high-quality subset.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Costs associated with backtranslation generation and LLM scoring/curation at scale; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended generation plus selection; clear thresholding selection step but subjective quality criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Downstream improvement in instruction-following performance after augmenting with curated backtranslated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper cites Instruction Backtranslation as an instance of self-rewarding-type curation but does not provide numerical success rates here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Quality depends on source documents and curation effectiveness; domain mismatch risk.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large diverse web corpora and effective curation (LLM-as-a-Judge) improve usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as a complementary approach to SR-LM; not directly compared numerically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Rewarding Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Some things are more cringe than others: Preference optimization with the pairwise cringe loss <em>(Rating: 2)</em></li>
                <li>RLAIF: Scaling reinforcement learning from human feedback with ai feedback <em>(Rating: 2)</em></li>
                <li>Self-instruct: Aligning language models with self-generated instructions. <em>(Rating: 2)</em></li>
                <li>Reinforced self-training (rest) for language modeling. <em>(Rating: 2)</em></li>
                <li>Self-alignment with instruction backtranslation. <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model. <em>(Rating: 2)</em></li>
                <li>Constitutional AI: Harmlessness from AI feedback. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2602",
    "paper_id": "paper-04d64be16fb402f28348faffef484bd419c8bd8f",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Self-Rewarding LLM",
            "name_full": "Self-Rewarding Language Models",
            "brief_description": "An iterative self-alignment framework in which a single LLM both generates instruction-following outputs and judges those outputs (LLM-as-a-Judge) to create preference data that is used to fine-tune the same model via Iterative DPO, producing successive improved models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Self-Rewarding Language Models (SR-LM)",
            "system_description": "A single LLM is trained to perform two roles: (1) generate instruction-following responses (generation/assistant role) and (2) evaluate candidate responses via LLM-as-a-Judge prompting to produce scores and rankings (reward modelling role). The procedure iterates: starting from a seed SFT model (IFT + optional EFT), the model (Mt) generates new prompts (prompt generation done by a fixed model in main experiments), samples N candidate responses per prompt, evaluates those responses multiple times (sampling the judge outputs and averaging) to produce scalar scores (0-5), forms preference pairs (highest vs lowest scored responses, discarding ties), and trains the next model (Mt+1) with DPO on a training set augmented by these self-generated preference pairs (AIFT). Key capabilities: autonomous data generation (prompts, responses), self-evaluation into preference pairs, offline preference-based fine-tuning (DPO), and iteration to improve both generation and self-evaluation abilities.",
            "system_type": "Automated self-alignment / automated data-generation and reward-modeling system",
            "problem_domain": "Natural language model alignment and instruction-following (NLP / language model fine-tuning / reward modeling).",
            "problem_description": "Improve instruction-following behavior of a large language model and simultaneously improve its reward-modeling ability without relying on large external human preference datasets or a frozen external reward model. The system must create new instruction prompts (via few-shot prompt generation), produce multiple candidate responses, evaluate them consistently, and generate preference pairs usable for preference-tuning (DPO).",
            "problem_complexity": "Moderate-to-high: involves large discrete output spaces (text), multi-attribute quality evaluation (relevance, coverage, usefulness, clarity, expertise), stochastic generation and evaluation processes, and multi-iteration dynamics where the evaluation model changes across iterations; search/control space includes prompt space, response space (high-dimensional), and evaluation scoring; multi-objective trade-offs (helpfulness vs harmlessness). Quantities reported: N=4 candidate responses per prompt; judge evaluations averaged over 3 sampled decodings.",
            "data_availability": "Seed data: small human-authored Instruction Fine-Tuning (IFT) set from Open Assistant (3200 high-quality English first-turn examples) and an Evaluation Fine-Tuning (EFT) set derived from Open Assistant (1,630 train, 541 eval after processing). Self-generated AIFT data sizes: AIFT(M1) = 3,964 preference pairs; AIFT(M2) = 6,942 pairs. Data quality: seed human data is high-quality; self-generated data quality improves across iterations as measured by correlation with human rankings.",
            "computational_requirements": "Uses Llama 2 70B as base model. Training used supervised fine-tuning and DPO with batch size 16, specific learning rates (SFT lr 5.5e-6 decaying to 1.1e-6; DPO lr 1e-6 decaying to 1e-7, beta=0.1). Generation decoding parameters: T=0.7, p=0.9, judge evaluation repeated 3 times per candidate and averaged. Exact wall-clock compute hours / GPU counts not reported in the paper.",
            "problem_structure": "Semi-structured and partly well-defined: discrete sequence-generation problem (stochastic), clear evaluation metrics exist (preference pairs, pairwise agreement with human rankings, Spearman/Kendall correlations, head-to-head win rates, leaderboard win rates, MT-Bench scores). Open-ended in that generated prompts/responses can be diverse; evaluation (LLM-as-a-Judge) is learned and changes over iterations (non-stationary). Requires domain knowledge mainly in instruction-following and evaluation criteria rather than domain-specific scientific expertise.",
            "success_metric": "Multiple metrics: instruction-following head-to-head win rates (GPT-4 evaluator and human annotators), AlpacaEval 2.0 leaderboard win rate vs GPT-4 Turbo (GPT-4 judgments), MT-Bench numeric score (out of 10), and reward-modeling metrics (pairwise accuracy vs human rankings, exact match of full ranking, Spearman and Kendall correlations, percent of model-scored 5s that are highest-ranked by humans).",
            "success_rate": "Quantified improvements across iterations: AlpacaEval win rate vs GPT-4 Turbo: M1 9.94%, M2 15.38%, M3 20.44%. Head-to-head (GPT-4) comparisons: M2 vs M1 — M2 wins 55.5% (M1 wins 11.7%); M2 vs SFT baseline — M2 wins 49.2% (baseline wins 14.5%). M3 vs M2 — M3 wins 47.7% (M2 wins 12.5%); M3 vs SFT baseline — M3 wins 62.5% (baseline wins 9.8%). MT-Bench overall scores: SFT baseline 6.85, M1 6.78, M2 7.01, M3 7.25 (out of 10). Reward modeling (alignment with human rankings): pairwise accuracy — SFT 65.1%, M1 78.7%, M2 80.4%, M3 81.7%; Spearman corr: 0.253 -&gt; 0.279 -&gt; 0.331 -&gt; 0.349 across SFT, M1, M2, M3 respectively.",
            "failure_modes": "Observed weaker improvements or limited gains on mathematics and logical reasoning tasks and on some benchmark NLP tasks (e.g., ARC-challenge, GSM8K showed mixed trends). Potential biases observed: generations grew longer across iterations (avg lengths increased substantially) which may correlate to perceived quality; risks include reward-hacking (not fully explored), judge-model overfitting to its own artifacts, and saturation of gains (only three iterations studied). Safety evaluation and lengthy/hallucinated outputs remain unassessed.",
            "success_factors": "Availability of a small high-quality seed dataset (IFT and EFT) that teaches the model to act as a judge; incorporation of explicit EFT examples to train the LLM-as-a-Judge (substantially increased valid AIFT pairs versus starting from IFT alone); using preference pairs (winning vs losing responses) rather than only positive examples; iterative DPO fine-tuning that leverages self-produced preference data; careful LLM-as-a-Judge prompt design (additive 5-point criteria) which produced far better judge alignment with human ranking than alternative prompts.",
            "comparative_results": "Iterative self-rewarding training consistently improved both generation quality and reward-model alignment across three iterations. Starting from IFT+EFT (M1) then adding AIFT(M1) to train M2 produced a large jump in instruction-following and judge metrics; further adding AIFT(M2) to train M3 produced further gains. Models trained without EFT had far fewer valid AIFT samples and lagged substantially behind corresponding models trained with EFT (EFT is important). Gains were larger in humanities, writing, roleplay and extraction categories; smaller in math/code/reasoning.",
            "human_baseline": "Human baseline is not provided as a single numeric oracle in the paper for the same tasks; comparisons are made against a supervised fine-tuning (SFT) baseline and against large systems: the Iteration 3 SR-LM achieves 20.44% win rate vs GPT-4 Turbo on AlpacaEval (GPT-4 judgments), and the paper notes that many leaderboard models use proprietary data or distilled targets—no direct human researcher performance numbers are given for the same evaluation sets.",
            "uuid": "e2602.0",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge prompting / evaluation mechanism",
            "brief_description": "A prompting technique that formulates response evaluation as an instruction-following task, producing chain-of-thought justification and a scalar score; used here to make the LLM act as its own reward model and produce preference labels.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM-as-a-Judge",
            "system_description": "Use of an LLM prompted with explicit evaluation criteria to score candidate responses (here a 5-point additive criteria: relevance, coverage, usefulness, clarity, expertise). The judge produces justifications and final scores; evaluations are generated multiple times (sampling) and averaged to reduce variance. The judge outputs are converted into preference pairs for DPO training.",
            "system_type": "Automated evaluation / synthetic reward-model generation",
            "problem_domain": "Automated evaluation for language model outputs and synthetic reward-label generation for alignment/fine-tuning.",
            "problem_description": "Provide consistent, human-aligned scalar scores or rankings for model-generated responses so they can be used as training signals (preference pairs) in preference-based fine-tuning.",
            "problem_complexity": "Moderate: must parse and apply multi-criteria evaluation consistently across diverse prompts and responses, handle stochastic decoding variance, and produce scores that correlate with human rankings.",
            "data_availability": "Trained initially with EFT seed examples (1,630 train, 541 eval) derived from Open Assistant; when EFT is absent judge outputs collapsed to common scores making few valid pairs.",
            "computational_requirements": "Judge outputs are produced via LLM sampling; the paper uses 3 sampled decodings per evaluation and averages scores. No absolute compute/time figures provided.",
            "problem_structure": "Stochastic evaluation; clear scalar output expected; evaluation criteria are explicit so metric is well-defined, but alignment to human preferences depends on prompt formulation and training examples.",
            "success_metric": "Agreement with human rankings: pairwise accuracy, exact match of ordering, Spearman and Kendall correlations, percent of model 5/5 rated responses being top-ranked by humans.",
            "success_rate": "Using the additive scoring prompt and EFT training, pairwise accuracy for the judge on held-out human-evaluated data increased from 65.1% (SFT baseline) to 78.7% (M1), then to 80.4% (M2) and 81.7% (M3). Alternative (multiple-choice) prompts performed poorly: 26.6% pairwise accuracy for the multiple-choice prompt on the same SFT model.",
            "failure_modes": "Without explicit EFT training data, judge outputs often collapse to a common score (e.g., many scores of 4), producing few usable preference pairs. Prompt format matters greatly; poorly structured judge prompts lead to low agreement with human judgments.",
            "success_factors": "Explicit multi-criteria additive scoring prompt and seed EFT examples; averaging multiple sampled judge decodings; training the model to perform evaluation as an instruction-following task.",
            "comparative_results": "Additive scoring prompt vastly outperformed the multiple-choice prompt variant (pairwise accuracy 65.1% vs 26.6% on SFT baseline), demonstrating prompt design is critical. EFT augmentation improved judge performance relative to SFT.",
            "human_baseline": "Human rankings used as ground truth for judge evaluation; judge pairwise accuracy up to 81.7% vs human rankings for the best SR-LM iteration, but no single 'human evaluator' quantitative baseline (e.g., inter-annotator agreement) reported in the paper.",
            "uuid": "e2602.1",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Iterative DPO",
            "name_full": "Iterative Direct Preference Optimization (Iterative DPO)",
            "brief_description": "An iterative preference-based fine-tuning scheme where each model generates preference data that is used to train the next model using DPO, enabling multiple rounds of self-improvement.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Iterative DPO",
            "system_description": "DPO (Direct Preference Optimization) is used to train models on preference data (winning vs losing responses) directly. Iterative DPO repeats a loop where a model's generations are judged (by a judge model or LLM-as-a-Judge) to produce new preference pairs which train the next model via DPO. In this work, the judge is the same model (self-rewarding) or seeded via EFT.",
            "system_type": "Automated preference-based fine-tuning / iterative training pipeline",
            "problem_domain": "Language model alignment and instruction-following improvement.",
            "problem_description": "Improve an LLM's behavior by repeatedly augmenting its training set with preference pairs generated (and judged) by the prior iteration's model and applying DPO to learn from those preferences.",
            "problem_complexity": "Iterative, non-stationary optimization with changing data distribution; requires stability across iterations to avoid collapse or reinforcement of artifacts.",
            "data_availability": "Depends on number of valid preference pairs generated each iteration (paper reports 3,964 and 6,942 pairs for two iterations when using EFT; far fewer when starting from IFT only).",
            "computational_requirements": "Uses DPO training cycles; specific hyperparameters provided (batch size 16, DPO lr 1e-6→1e-7, beta 0.1). Exact compute not reported.",
            "problem_structure": "Discrete, stochastic generation with clear pairwise preference labels used for supervised preference-based fine-tuning; evaluation metrics are well-defined (win rates, judge alignment).",
            "success_metric": "Improvement in instruction-following win rates, leaderboard placement, and judge/human agreement metrics across iterations.",
            "success_rate": "In this work, Iterative DPO with self-rewarding judge produced substantial gains across iterations (see SR-LM numbers). The paper also notes previous work (PCO / Iterative DPO in Xu et al. 2023) showed improvements over single-step DPO.",
            "failure_modes": "Potential for saturation of gains, generating skewed data distributions (e.g., length bias), and dependence on judge quality; insufficient valid preference pairs if judge is not well-trained.",
            "success_factors": "Quality of initial seed data (IFT+EFT), judge quality, sufficient diversity and validity of generated preference pairs, and stable DPO optimization.",
            "comparative_results": "Iterative DPO with a self-improving judge (this paper) outperformed the iteration-1 and SFT baselines; prior work with an external fixed reward model also showed Iterative DPO improvements over single-step DPO.",
            "human_baseline": "Compared against SFT baseline and various strong LLMs; iterative DPO models eventually outperform SFT baseline and approach/beat many strong models on AlpacaEval as reported.",
            "uuid": "e2602.2",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Instruct",
            "name_full": "Self-Instruct (self-instruction creation)",
            "brief_description": "A method for generating instruction prompts and responses from a model via few-shot prompting to augment instruction-following training data without human annotation.",
            "citation_title": "Self-instruct: Aligning language models with self-generated instructions.",
            "mention_or_use": "mention",
            "system_name": "Self-Instruct prompt-based data generation",
            "system_description": "Uses few-shot prompting to generate new instruction prompts (and optionally responses); in this paper a fixed Llama 2-Chat 70B model with 8-shot prompting was used to produce new prompts, with additional filtering (ROUGE-L similarity checks, keyword and length filters).",
            "system_type": "Automated data-generation / prompt generation system",
            "problem_domain": "Generating diverse instruction prompts for instruction-following fine-tuning in NLP.",
            "problem_description": "Create new instruction prompts to expand training data coverage without manual labeling, enabling subsequent candidate response generation and preference labeling.",
            "problem_complexity": "Relatively low complexity to produce prompts, but quality and diversity control are important to avoid low-value or redundant prompts.",
            "data_availability": "Prompt generation is synthetic; quantity depends on sampling and filtering; in paper prompt generation was fixed to a pretrained prompt-generator model rather than the self-rewarding model in main experiments.",
            "computational_requirements": "Generation cost for prompts only (decoding parameters reported T=0.6, p=0.9) — modest relative to response generation for large-scale fine-tuning.",
            "problem_structure": "Open-ended creative generation task with post-filtering for quality; evaluation is by heuristics and downstream effectiveness.",
            "success_metric": "Usefulness of generated prompts as judged by the quality/diversity of resulting candidate responses and contribution to improved downstream instruction-following after fine-tuning.",
            "success_rate": "Not quantified as a standalone system in this paper; used as a component to provide prompts for SR-LM pipeline.",
            "failure_modes": "Poor prompt quality or low diversity can reduce effectiveness; reliance on fixed prompt-generator model in main experiments to control quality.",
            "success_factors": "Few-shot examples drawn from high-quality IFT data and filtering (ROUGE-L, keywords, length) improve prompt usefulness.",
            "comparative_results": "Paper used Self-Instruct style prompt generation as a component; does not provide direct head-to-head comparisons of different prompt-generation methods here.",
            "human_baseline": "Not applicable; prompts are synthetic and evaluated by downstream model improvements.",
            "uuid": "e2602.3",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "RLAIF",
            "name_full": "Reinforcement Learning from AI Feedback (RLAIF)",
            "brief_description": "A family of methods that use an LLM (rather than humans) to provide feedback to train reward models or directly fine-tune agents, e.g., via creating labeled preferences or training a reward model used for RL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "RLAIF / LLM-feedback pipelines",
            "system_description": "An off-the-shelf LLM is used to produce feedback (rankings, critiques) on candidate model outputs; that feedback can be used to train a separate fixed reward model which is then used in RL (PPO) or to directly create training data for fine-tuning. Alternatives include using the LLM-as-a-Judge as a fixed curation model or as part of iterative offline pipelines.",
            "system_type": "Automated feedback-based reward construction / automated curation",
            "problem_domain": "Language model alignment and training via synthetic feedback.",
            "problem_description": "Replace or augment human preference data with synthetic judgments from LLMs to scale preference data for reward training or direct training.",
            "problem_complexity": "Depends on quality of off-the-shelf LLM judge and mismatch between judge and human preferences; introduces non-stationarity if judge differs from downstream evaluator.",
            "data_availability": "Synthetic feedback is cheap to generate at scale but quality varies; paper cites Lee et al. (2023) who compared RLAIF and RLHF.",
            "computational_requirements": "Generation of judge outputs at scale can be expensive if using large external LLMs within RL loops; Lee et al. reported computational expense when using the judge inside PPO (vs offline).",
            "problem_structure": "Well-defined preference labeling objective, but judge-model bias and computational cost of using powerful external LLMs can be limiting.",
            "success_metric": "Improvement vs RLHF baselines, final model quality and alignment metrics.",
            "success_rate": "Paper cites Lee et al. (2023) stating RLAIF and RLHF methods compared performed roughly equally in the experiments reported there. No numeric success rates given in this paper.",
            "failure_modes": "Computational expense (especially within online RL), judge-model bias, potential misalignment with human preferences, and cost of obtaining large-scale high-quality synthetic judgments if judge is expensive.",
            "success_factors": "Quality and calibration of the off-the-shelf judge model; using judge outputs offline for dataset construction is cheaper than online use during RL.",
            "comparative_results": "RLAIF methods can roughly match RLHF in some reported comparisons (Lee et al. 2023) but differ in computational trade-offs; this paper prefers an offline iterative DPO approach for cost and stability reasons.",
            "human_baseline": "Compared qualitatively against RLHF (human-labeled preference pipelines) — reported to be roughly comparable in Lee et al. 2023 according to this paper.",
            "uuid": "e2602.4",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ReST",
            "name_full": "Reinforced Self-Training (ReST)",
            "brief_description": "A method that uses a fixed, external reward to curate high-quality examples iteratively and add them to the training set to improve language model performance.",
            "citation_title": "Reinforced self-training (rest) for language modeling.",
            "mention_or_use": "mention",
            "system_name": "Reinforced Self-Training (ReST)",
            "system_description": "Uses an external reward signal (fixed) to select or curate positive examples generated by a model; those curated examples are added to the training set iteratively to improve model performance.",
            "system_type": "Automated curation / self-training",
            "problem_domain": "Language modeling and instruction-following improvement via curated self-generated data.",
            "problem_description": "Curate model-generated outputs with an external reward to form training targets that iterate into improved model behavior.",
            "problem_complexity": "Less complex conceptually but depends on a reliable external reward; selection/curration thresholds matter.",
            "data_availability": "Relies on plentiful model-generated outputs and an external scoring function or reward model to filter them.",
            "computational_requirements": "Costs for generation and scoring; exact numbers not provided in this paper.",
            "problem_structure": "Open-ended generation plus classification/selection by reward model; straightforward to implement but quality depends on reward model.",
            "success_metric": "Downstream improvement on target tasks after iterative fine-tuning with curated examples.",
            "success_rate": "Paper references ReST as having been used to iteratively add high-quality examples and improve performance; no numeric rates in this paper.",
            "failure_modes": "If the external reward is miscalibrated, curated data can be poor and degrade performance; limited to the capabilities of the reward model.",
            "success_factors": "Robust external reward and good diversity in generated examples.",
            "comparative_results": "Paper notes ReST uses a fixed external reward in contrast to the self-improving judge used in SR-LM.",
            "human_baseline": "Compared implicitly to approaches that use human-curated examples; no direct numeric comparisons provided here.",
            "uuid": "e2602.5",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SPIN",
            "name_full": "SPIN (Iterative DPO-like framework avoiding reward models)",
            "brief_description": "A recent method that avoids reward models entirely in an Iterative DPO-like framework by using human labels as the winning responses and previous-iteration generations as losing responses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SPIN",
            "system_description": "Constructs preference pairs where the human-provided response is treated as the winning response and the last iteration's model-generated responses are treated as losing responses; uses these to train via an Iterative DPO-like process without training a reward model.",
            "system_type": "Iterative human-anchored preference training / automated fine-tuning approach",
            "problem_domain": "Instruction-following model alignment where human references exist for inputs.",
            "problem_description": "Leverage human labeled responses and previous model outputs to create preference training data iteratively and fine-tune models via DPO-like methods without a separate reward model.",
            "problem_complexity": "Requires human-annotated targets for each input; limited by availability of human references (bottleneck once model reaches human-level performance).",
            "data_availability": "Requires human annotated response for each input prompt; paper notes this is a limitation relative to fully synthetic self-rewarding pipelines.",
            "computational_requirements": "Similar to iterative DPO training but does not require building or using a separate reward model.",
            "problem_structure": "Well-defined when human targets exist; not applicable to domains lacking human-supervised responses.",
            "success_metric": "Improvement over iterations relative to baselines; avoids dependence on reward model training.",
            "success_rate": "Not quantified in this paper; paper cites SPIN as having advantage of avoiding reward models but notes limitations (human target requirement and bottlenecking).",
            "failure_modes": "Bottleneck: requires a human response per input, so gains stop when model reaches human performance; costly data requirements.",
            "success_factors": "Availability of high-quality human reference responses for inputs.",
            "comparative_results": "Contrasted with SR-LM which can generate both responses and rewards synthetically, SPIN trades off by requiring human targets but avoiding reward model complexity.",
            "human_baseline": "SPIN explicitly uses human responses as the winning examples, so performance is anchored to human targets; no numeric comparisons provided here.",
            "uuid": "e2602.6",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Instruction Backtranslation",
            "name_full": "Instruction Backtranslation (Self-alignment with instruction backtranslation)",
            "brief_description": "A data-augmentation and curation approach that augments the instruction dataset by backtranslating from web documents and uses the LLM (or LLM-as-a-Judge) to curate examples, considered a specialized form of self-rewarding model.",
            "citation_title": "Self-alignment with instruction backtranslation.",
            "mention_or_use": "mention",
            "system_name": "Instruction Backtranslation",
            "system_description": "Generates synthetic instruction–response pairs by transforming web documents into prompts (backtranslation) and uses the LLM itself to curate or score those pairs for inclusion in training; the curation step can be seen as self-rewarding.",
            "system_type": "Automated data-augmentation and curation",
            "problem_domain": "Creating and curating instruction-following datasets to improve LLM alignment.",
            "problem_description": "Expand training data coverage by deriving prompts/responses from available web content and automatically selecting high-quality examples using the LLM.",
            "problem_complexity": "Depends on document coverage, quality of backtranslation heuristics, and reliability of LLM curation; could be large-scale.",
            "data_availability": "Uses abundant web documents as raw sources; curation reduces usable samples to high-quality subset.",
            "computational_requirements": "Costs associated with backtranslation generation and LLM scoring/curation at scale; not quantified in this paper.",
            "problem_structure": "Open-ended generation plus selection; clear thresholding selection step but subjective quality criteria.",
            "success_metric": "Downstream improvement in instruction-following performance after augmenting with curated backtranslated examples.",
            "success_rate": "Paper cites Instruction Backtranslation as an instance of self-rewarding-type curation but does not provide numerical success rates here.",
            "failure_modes": "Quality depends on source documents and curation effectiveness; domain mismatch risk.",
            "success_factors": "Large diverse web corpora and effective curation (LLM-as-a-Judge) improve usefulness.",
            "comparative_results": "Mentioned as a complementary approach to SR-LM; not directly compared numerically in this paper.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2602.7",
            "source_info": {
                "paper_title": "Self-Rewarding Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Some things are more cringe than others: Preference optimization with the pairwise cringe loss",
            "rating": 2
        },
        {
            "paper_title": "RLAIF: Scaling reinforcement learning from human feedback with ai feedback",
            "rating": 2
        },
        {
            "paper_title": "Self-instruct: Aligning language models with self-generated instructions.",
            "rating": 2
        },
        {
            "paper_title": "Reinforced self-training (rest) for language modeling.",
            "rating": 2
        },
        {
            "paper_title": "Self-alignment with instruction backtranslation.",
            "rating": 2
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "rating": 2
        },
        {
            "paper_title": "Constitutional AI: Harmlessness from AI feedback.",
            "rating": 1
        }
    ],
    "cost": 0.02158775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Rewarding Language Models</h1>
<p>Weizhe Yuan ${ }^{1,2}$ Richard Yuanzhe Pang ${ }^{1,2}$ Kyunghyun Cho ${ }^{2}$<br>Xian $\mathrm{Li}^{1}$ Sainbayar Sukhbaatar ${ }^{1}$ Jing Xu ${ }^{1}$ Jason Weston ${ }^{1,2}$<br>${ }^{1}$ Meta ${ }^{2}$ NYU</p>
<h4>Abstract</h4>
<p>We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.</p>
<h2>1 Introduction</h2>
<p>Aligning Large Language Models (LLMs) using human preference data can vastly improve the instruction following performance of pretrained models [Ouyang et al., 2022, Bai et al., 2022a]. The standard approach of Reinforcement Learning from Human Feedback (RLHF) learns a reward model from these human preferences. The reward model is then frozen and used to train the LLM using RL, e.g., via PPO [Schulman et al., 2017]. A recent alternative is to avoid training the reward model at all, and directly use human preferences to train the LLM, as in Direct Preference Optimization [DPO; Rafailov et al., 2023]. In both cases, the approach is bottlenecked by the size and quality of the human preference data, and in the case of RLHF the quality of the frozen reward model trained from them as well.</p>
<p>In this work, we instead propose to train a self-improving reward model that, rather than being frozen, is continually updating during LLM alignment, in order to avoid this bottleneck. The key to such an approach is to develop an agent that possesses all the abilities desired during training, rather than separating them out into distinct models such as a reward model and a language model. In the same way that pretraining and multitasking training of instruction following tasks allow task transfer by training on many tasks at once [Collobert and Weston, 2008, Radford et al., 2019, Ouyang et al., 2022], incorporating the reward model into that same system allows task transfer between the reward modeling task and the instruction following tasks.
We thus introduce Self-Rewarding Language Models, that both (i) act as instruction following models generating responses for given prompts; and (ii) can generate and evaluate new instruction following examples to add to their own training set. We train these models using an Iterative DPO framework similar to that recently introduced in Xu et al. [2023].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Self-Rewarding Language Models. Our self-alignment method consists of two steps: (i) Self-Instruction creation: newly created prompts are used to generate candidate responses from model M<sup>t</sup>, which also predicts its own rewards via LLM-as-a-Judge prompting. (ii) Instruction following training: preference pairs are selected from the generated data, which are used for training via DPO, resulting in model M<sup>t+1</sup>. This whole procedure can then be iterated resulting in both improved instruction following and reward modeling ability.</p>
<p>Starting from a seed model, in each iteration there is a process of Self-Instruction creation whereby candidate responses are generated by the model for newly created prompts, and are then assigned rewards by that same model. The latter is implemented via LLM-as-a-Judge prompting, which can also be seen as an instruction following task. A preference dataset is built from the generated data, and the next iteration of the model is trained via DPO, see Figure 1.</p>
<p>In our experiments, we start with a Llama 2.70B [Touvron et al., 2023] seed model fine-tuned on Open Assistant [Köpf et al., 2023], and then perform the above training scheme. We find that not only does the instruction following performance improve from Self-Rewarding LLM alignment compared to the baseline seed model, but importantly the reward modeling ability, which is no longer fixed, improves as well. This means that the model during iterative training is able, at a given iteration, to provide a higher quality preference dataset to itself than in the previous iteration. While this effect likely saturates in real-world settings, it provides the intriguing possibility of obtaining reward models (and hence LLMs) that are superior to ones that could have been trained from the original human-authored seed data alone.</p>
<h2>2 Self-Rewarding Language Models</h2>
<p>Our approach first assumes access to a base pretrained language model, and a small amount of human-annotated seed data. We then build a model that aims to possess two skills simultaneously:</p>
<ol>
<li>Instruction following: given a prompt that describes a user request, the ability to generate a high quality, helpful (and harmless) response.</li>
<li>Self-Instruction creation: the ability to generate and evaluate new instruction-following examples to add to its own training set.</li>
</ol>
<p>These skills are used so that the model can perform self-alignment, i.e., they are the components used to iteratively train itself using AI Feedback (AIF).</p>
<p>Self-instruction creation consists of generating candidate responses and then the model itself judging their quality, i.e., it acts as its own reward model, replacing the need for an external one. This is implemented via the LLM-as-a-Judge mechanism [Zheng et al., 2023b], i.e., by formulating the evaluation of responses as an instruction following task. This self-created AIF preference data is used as a training set.</p>
<p>Our overall self-alignment procedure is an iterative one, which proceeds by building a series of such models, with the aim that each improves over the last. Importantly, because the model can both improve its generation ability, and act as its own reward model through the same generation mechanism, this means the reward model itself can improve through these iterations, deviating from standard practices where the reward model is fixed [Ouyang et al.,</p>
<p>2022]. We believe this can increase the ceiling of the potential for self-improvement of these learning models going forward, removing a constraining bottleneck.</p>
<p>We describe these steps in more detail below. An overview of the approach is illustrated in Figure 1.</p>
<h1>2.1 Initialization</h1>
<p>Seed instruction following data We are given a seed set of human-authored (instruction prompt, response) general instruction following examples that we use for training in a supervised fine-tuning (SFT) manner, starting from a pretrained base language model. Subsequently this will be referred to as Instruction Fine-Tuning (IFT) data.</p>
<p>Seed LLM-as-a-Judge instruction following data We also assume we are provided a seed set of (evaluation instruction prompt, evaluation result response) examples which can also be used for training. While this is not strictly necessary, as the model using IFT data will already be capable of training an LLM-as-a-Judge, we show that such training data can give improved performance (see Appendix A. 3 for supporting results). In this data, the input prompt asks the model to evaluate the quality of a given response to a particular instruction. The provided evaluation result response consists of chain-of-thought reasoning (a justification), followed by a final score (in our experiments out of 5). The exact prompt format we chose is given in Figure 2, which instructs the LLM to evaluate the response using five additive criteria (relevance, coverage, usefulness, clarity and expertise), covering various aspects of quality. Subsequently this will be referred to as Evaluation Fine-Tuning (EFT) data.</p>
<p>We use both these seed sets together during training.</p>
<h3>2.2 Self-Instruction Creation</h3>
<p>Using the model we have trained, we can make it self-modify its own training set. Specifically, we generate additional training data for the next iteration of training.</p>
<p>This consists of the following steps:</p>
<ol>
<li>Generate a new prompt: We generate a new prompt $x_{i}$ using few-shot prompting, sampling prompts from the original seed IFT data, following the approach of Wang et al. [2023] and Honovich et al. [2023]. ${ }^{1}$</li>
<li>Generate candidate responses: We then generate $N$ diverse candidate responses $\left{y_{i}^{1}, \ldots, y_{i}^{N}\right}$ for the given prompt $x_{i}$ from our model using sampling.</li>
<li>Evaluate candidate responses: Finally, we use the LLM-as-a-Judge ability of our same model to evaluate its own candidate responses with scores $r_{i}^{n} \in[0,5]$ (exact prompt given in Figure 2).</li>
</ol>
<h3>2.3 Instruction Following Training</h3>
<p>As previously described, training is initially performed with the seed IFT and EFT data (Section 2.1). This is then augmented with additional data via AI (Self-)Feedback.</p>
<p>AI Feedback Training After performing the self-instruction creation procedure, we can augment the seed data with additional examples for training, which we refer to as AI Feedback Training (AIFT) data.</p>
<p>To do this, we construct preference pairs, which are training data of the form (instruction prompt $x_{i}$, winning response $y_{i}^{w}$, losing response $y_{i}^{l}$ ). To form the winning and losing pair we take the highest and lowest scoring responses from the $N$ evaluated candidate responses (see</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Review the user's question and the corresponding response using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion:</p>
<ul>
<li>Add 1 point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content.</li>
<li>Add another point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer.</li>
<li>Award a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results.</li>
<li>Grant a fourth point if the response is clearly written from an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus.</li>
<li>Bestow a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer.</li>
</ul>
<p>User: <INSTRUCTION_HERE>
$&lt;$ response $&gt;&lt;$ RESPONSE_HERE $&gt;&lt;/$ response $&gt;$
After examining the user's instruction and the response:</p>
<ul>
<li>Briefly justify your total score, up to 100 words.</li>
<li>Conclude with the score using the format: "Score: $&lt;$ total points $&gt;$ "</li>
</ul>
<p>Remember to assess from the AI Assistant perspective, utilizing web search knowledge as necessary. To evaluate the response in alignment with this additive scoring model, we'll systematically attribute points based on the outlined criteria.</p>
<p>Figure 2: LLM-as-a-Judge prompt for our LLM to act as a reward model and provide self-rewards for its own model generations. The model is initially trained with seed training data of how to perform well at this task, and then improves at this task further through our self-rewarding training procedure.</p>
<p>Section 2.2), following Xu et al. [2023], discarding the pair if their scores are the same. These pairs can be used for training with a preference tuning algorithm. We use DPO [Rafailov et al., 2023].</p>
<h1>2.4 Overall Self-Alignment Algorithm</h1>
<p>Iterative Training Our overall procedure trains a series of models $M_{1}, \ldots, M_{T}$ where each successive model $t$ uses augmented training data created by the $t-1^{\text {th }}$ model. We thus define $\operatorname{AIFT}\left(M_{t}\right)$ to mean AI Feedback Training data created using model $M_{t}$.</p>
<p>Model Sequence We define the models, and the training data they use as follows:
$M_{0}$ : Base pretrained LLM with no fine-tuning.
$M_{1}$ : Initialized with $M_{0}$, then fine-tuned on the IFT+EFT seed data using SFT.
$M_{2}$ : Initialized with $M_{1}$, then trained with $\operatorname{AIFT}\left(M_{1}\right)$ data using DPO.
$M_{3}$ : Initialized with $M_{2}$, then trained with $\operatorname{AIFT}\left(M_{2}\right)$ data using DPO.
This iterative training resembles the procedure used in Pairwise Cringe Optimization and specifically is termed Iterative DPO, introduced in Xu et al. [2023]; however, an external fixed reward model was used in that work.</p>
<h1>3 Experiments</h1>
<h3>3.1 Experimental Setup</h3>
<p>Base Model In our experiments we use Llama 2 70B [Touvron et al., 2023] as our base pretrained model.</p>
<h3>3.1.1 Seed Training Data</h3>
<p>IFT Seed Data We use the human-authored examples provided in the Open Assistant dataset [Köpf et al., 2023] for instruction fine-tuning. Following Li et al. [2024] we use 3200 examples, by sampling only first conversational turns in the English language that are high-quality, based on their human annotated rank (choosing only the highest rank 0 ). In our experiments, we compare to a model fine-tuned from the base model using only this data via supervised fine-tuning, and refer to it as our SFT baseline.</p>
<p>EFT Seed Data The Open Assistant data also provides multiple ranked human responses per prompt from which we can construct evaluation fine-tuning data. We split this into train and evaluation sets, and use it to create LLM-as-a-Judge data. This is done by placing it in the input format given in Figure 2, which consists of the scoring criteria description, and the given instruction and response to be evaluated. ${ }^{2}$ For training targets, chain-of-thought justifications and final scores out of 5 are not directly provided, so we use the SFT baseline to generate such output evaluations for each input, and accept them into the training set if the ranking of their scores agrees with the human rankings in the dataset. We resample the training set by discarding some of the data that receives the most common score so that the scores are not too skewed, as we observe many samples receive a score of 4 . This results in 1,630 train and 541 evaluation examples (which do not overlap with the IFT data).</p>
<h3>3.1.2 Evaluation Metrics</h3>
<p>We evaluate the performance of our self-rewarding models in two axes: their ability to follow instructions, and their ability as a reward model (ability to evaluate responses).</p>
<p>Instruction Following We evaluate head-to-head performance between various models using GPT-4 [Achiam et al., 2023] as an evaluator over 256 test prompts (which we refer to as IFT test data) derived from various sources following Li et al. [2024] using the AlpacaEval evaluation prompt [Li et al., 2023]. We try the prompt in both orders comparing pairwise, and if the GPT-4 evaluations disagree we count the result as a tie. We also perform a similar evaluation with humans (authors). We additionally report results in the AlpacaEval 2.0 leaderboard format which is evaluated over 805 prompts, and compute the win rate against the baseline GPT-4 Turbo model based on GPT-4 judgments. Further, we report results on MT-Bench [Zheng et al., 2023b] a set of challenging multi-turn questions in various categories from math and coding to roleplay and writing, which uses GPT-4 to grade the model responses out of 10 . Finally we also test the models on a set of 9 NLP benchmarks: ARC-Easy [Clark et al., 2018], ARC-Challenge [Clark et al., 2018], HellaSwag [Zellers et al., 2019], SIQA [Sap et al., 2019], PIQA [Bisk et al., 2020], GSM8K [Cobbe et al., 2021], MMLU [Hendrycks et al., 2021], OBQA [Mihaylov et al., 2018] and NQ [Kwiatkowski et al., 2019].</p>
<p>Reward Modeling We evaluate the correlation with human rankings on the evaluation set we derived from the Open Assistant dataset, as described in Section 3.1.1. Each instruction has on average 2.85 responses with given rankings. We can thus measure the pairwise accuracy, which is how many times the order of the ranking between any given pair agrees between the model's evaluation and the human ranking. We also measure the exact match count, which is how often the total ordering is exactly the same for an instruction. We also report the Spearman correlation and Kendall's $\tau$. Finally, we report how often the responses that the model scores a perfect 5 out of 5 are rated as the highest ranked by humans.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.1.3 Training Details</h1>
<p>Instruction following training The training hyperparameters we use are as follows. For SFT we use learning rate $5.5 \mathrm{c}-6$ which decays (cosine) to $1.1 \mathrm{c}-6$ at the end of training, batch size 16 and dropout 0.1 . We only calculate the loss on target tokens instead of the full sequence. For DPO we use learning rate $1 \mathrm{c}-6$ which decays to $1 \mathrm{c}-7$, batch size 16 , dropout 0.1 , and a $\beta$ value of 0.1 . We perform early stopping by saving a checkpoint every 200 steps and evaluating generations using Claude 2 [Anthropic, 2023] on 253 validation examples derived from various sources following Li et al. [2024]. This is evaluated pairwise against the previous step's generations using the AlpacaEval evaluation prompt format [Li et al., 2023].</p>
<p>Self-Instruction creation To generate new prompts we use a fixed model, Llama 2-Chat 70B with 8 -shot prompting following Self-Instruct [Wang et al., 2023], where we sample six demonstrations from the IFT data and two from the model generated data, and use decoding parameters $\mathrm{T}=0.6, \mathrm{p}=0.9$. We use their prompt template for non-classification tasks and apply the same filtering techniques, including the ROUGE-L [Lin, 2004] similarity check, keyword filtering, and length filtering. Except for the prompt generation part, the other parts of the creation pipeline (generating the response, and evaluating it) use the Self-Rewarding model being trained. For candidate response generation we sample $N=4$ candidate responses with temperature $T=0.7, p=0.9$. When evaluating candidate responses, as there is variance to these scores, in our experiments we also use sampled decoding (with the same parameters) and generate these evaluations multiple (3) times and take the average. We added 3,964 such preference pairs to form the $\operatorname{AIFT}\left(M_{1}\right)$ dataset used to train $M_{2}$ via DPO, and 6,942 pairs to form $\operatorname{AIFT}\left(M_{2}\right)$ used to train $M_{3}$.</p>
<h3>3.2 Results</h3>
<h3>3.2.1 Instruction Following Ability</h3>
<p>Head to head performance results are provided in Figure 3.
EFT+IFT seed training performs similarly to IFT alone We find that adding the Evaluation Fine-Tuning (EFT) task to training does not impact instruction following performance compared to using Instruction Fine-Tuning (IFT) data alone with an almost equal head to head ( $30.5 \%$ wins vs. $30.9 \%$ wins). This is a positive result because it means the increased capability of a model to self-reward does not affect its other skills. We can thus use IFT+EFT training as Iteration $1\left(M_{1}\right)$ of our Self-Rewarding model, and then run further iterations.</p>
<p>Iteration $2\left(M_{2}\right)$ improves over Iteration $1\left(M_{1}\right)$ and SFT Baseline Iteration 2 of Self-Rewarding training $\left(M_{2}\right)$ provides superior instruction following to Iteration $1\left(M_{1}\right)$ with $55.5 \%$ wins for $M_{2}$ compared to only $11.7 \%$ for $M_{1}$ in a head to head evaluation. It provides similar gains over the SFT Baseline as well ( $49.2 \%$ wins vs. $14.5 \%$ wins). Clearly, there is a large jump in performance from $M_{1}$ to $M_{2}$ by using the preference data $\operatorname{AIFT}\left(M_{1}\right)$ provided by the reward model from Iteration 1.</p>
<p>Iteration $3\left(M_{3}\right)$ improves over Iteration $2\left(M_{2}\right)$ We see a further gain in Iteration 3 over Iteration 2, with $47.7 \%$ wins for $M_{3}$ compared to only $12.5 \%$ for $M_{2}$ in a head to head evaluation. Similarly, the win rate over the SFT Baseline for $M_{3}$ increases to $62.5 \%$ wins vs. $9.8 \%$, i.e., winning more often than the $M_{2}$ model did. Overall, we see large gains from $M_{2}$ to $M_{3}$ through training using the preference data $\operatorname{AIFT}\left(M_{2}\right)$ provided by the reward model from Iteration 2 .</p>
<p>Self-Rewarding models perform well on AlpacaEval 2 leaderboard We evaluate our models on the AlpacaEval 2.0 leaderboard format, with results given in Table 1. We observe the same findings as in the head-to-head evaluations, that training iterations yield improved win rates, in this case over GPT4-Turbo, from $9.94 \%$ in Iteration 1, to $15.38 \%$ in Iteration 2, to $20.44 \%$ in Iteration 3. Our Iteration 3 model outperforms many existing models in this metric, including Claude 2, Gemini Pro, and GPT4 0613. We show some</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Instruction following ability improves with Self-Training: We evaluate our models using head-to-head win rates on diverse prompts using GPT-4. The SFT Baseline is on par with Self-Rewarding Iteration 1 $\left(M_{1}\right)$. However, Iteration $2\left(M_{2}\right)$ outperforms both Iteration $1\left(M_{1}\right)$ and the SFT Baseline. Iteration $3\left(M_{3}\right)$ gives further gains over Iteration 2 $\left(M_{2}\right)$, outperforming $M_{1}, M_{2}$ and the SFT Baseline by a large margin.
selected models from the leaderboard in the table. We note that many of those competing models contain either proprietary alignment data (which is typically large, e.g., over 1 M annotations in <em>Touvron et al. (2023)</em>) or use targets that are distilled from stronger models. In contrast, our Self-Rewarding model starts from a small set of seed data from Open Assistant, and then generates targets and rewards from the model itself for further iterations of training.</p>
<p>Fine-grained analysis As described earlier, the overall performance of the model in AlpacaEval improves with each iteration of training. It would be interesting to break down the overall performance improvement to see exactly what type of tasks these improvements come from. Therefore, we cluster the instructions in AlpacaEval test set into different groups based on three perspectives: (1) instruction category (2) instruction complexity (3) expected response length. We achieve this by using GPT-4. The detailed statistical information of the breakdown and the prompting techniques we used for getting this breakdown can be found in Appendix A.6. Results for the instruction category are given in Figure 4, and the other two in Appendix Figure 11. From the results we can conclude that (i) Self-Rewarding models can substantially improve the win rate in most categories, but there are some tasks for which this approach does not improve, such as mathematics and logical reasoning, indicating that our current training approach mainly allows the models to better utilize their existing knowledge. (ii) Through Self-Rewarding model training, the model's win rate increases on almost all tasks of different complexity, and especially on slightly more difficult tasks (complexity of 5, 6,7 out of 10). (iii) The models also show a steady increase in the win rate on tasks with instructions with different expected response lengths.</p>
<p>Table 1: AlpacaEval 2.0 results (win rate over GPT-4 Turbo evaluated by GPT-4). Self-Rewarding iterations yield improving win rates. Iteration $3\left(M_{3}\right)$ outperforms many existing models that use proprietary training data or targets distilled from stronger models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Alignment Targets</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Win Rate</td>
<td style="text-align: center;">Distilled</td>
<td style="text-align: center;">Proprietary</td>
</tr>
<tr>
<td style="text-align: center;">Self-Rewarding 70B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Iteration 1 $\left(M_{1}\right)$</td>
<td style="text-align: center;">$9.94 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Iteration 2 $\left(M_{2}\right)$</td>
<td style="text-align: center;">$15.38 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Iteration 3 $\left(M_{3}\right)$</td>
<td style="text-align: center;">$20.44 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Selected models from the leaderboard</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0314</td>
<td style="text-align: center;">$22.07 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Mistral Medium</td>
<td style="text-align: center;">$21.86 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Claude 2</td>
<td style="text-align: center;">$17.19 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Gemini Pro</td>
<td style="text-align: center;">$16.85 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 0613</td>
<td style="text-align: center;">$15.76 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">GPT 3.5 Turbo 0613</td>
<td style="text-align: center;">$14.13 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2 Chat 70B</td>
<td style="text-align: center;">$13.87 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna 33B v1.3</td>
<td style="text-align: center;">$12.71 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Humpback LLaMa2 70B</td>
<td style="text-align: center;">$10.12 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Guanaco 65B</td>
<td style="text-align: center;">$6.86 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Davinci001</td>
<td style="text-align: center;">$2.76 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca 7B</td>
<td style="text-align: center;">$2.59 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: AlpacaEval win rate breakdown for instruction categories (full names given in Appendix). Self-Rewarding models give gains across several topics, but tend to e.g. give less gains on mathematics and reasoning tasks.</p>
<p>Data distribution analysis We perform a t-SNE [Van der Maaten and Hinton, 2008] visualization of the IFT, EFT and $\operatorname{AIFT}\left(M_{1}\right)$ data, shown in Appendix A.1. We find good overlap between the IFT and $\operatorname{AIFT}\left(M_{1}\right)$ examples, which is desired, while the EFT examples lie in a different part of the embedding space, which can help explain why they would not affect IFT performance. We observe that generations from $M_{1}$ on AlpacaEval have an average length of 1092 , for $M_{2}$ they are 1552 , and for $M_{3}$ they are 2552 , so the model is learning to generate longer responses, which we note may be a factor in relative performance.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Human evaluation results. Iterations of Self-Rewarding ( $M_{1}, M_{2}$ and $M_{3}$ ) provide progressively better head-to-head win rates compared to the SFT baseline, in agreement with the automatic evaluation results.</p>
<p>Table 2: MT-Bench Results (on a scale of 10). Self-Rewarding iterations yield improving scores across various categories. Math, code \&amp; reasoning performance and iteration gains are smaller than for other categories, likely due to the makeup of the Open Assistant seed data we use.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Overall <br> Score</th>
<th style="text-align: center;">Math, Code <br> \&amp; Reasoning</th>
<th style="text-align: center;">Humanities, Extraction, <br> STEM, Roleplay \&amp; Writing</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SFT Baseline</td>
<td style="text-align: center;">6.85</td>
<td style="text-align: center;">3.93</td>
<td style="text-align: center;">8.60</td>
</tr>
<tr>
<td style="text-align: left;">$M_{1}$</td>
<td style="text-align: center;">6.78</td>
<td style="text-align: center;">3.83</td>
<td style="text-align: center;">8.55</td>
</tr>
<tr>
<td style="text-align: left;">$M_{2}$</td>
<td style="text-align: center;">7.01</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">8.79</td>
</tr>
<tr>
<td style="text-align: left;">$M_{3}$</td>
<td style="text-align: center;">7.25</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">9.10</td>
</tr>
</tbody>
</table>
<p>Table 3: NLP Benchmarks. Self-Rewarding models mostly tend to maintain performance compared to the Llama 2 70B base model and the SFT Baseline, despite being fine-tuned on very different instruction-following prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ARC $(\uparrow)$ <br> challenge</th>
<th style="text-align: center;">HellaSwag <br> $(\uparrow)$</th>
<th style="text-align: center;">GSM8K <br> $(\uparrow)$</th>
<th style="text-align: center;">MMLU <br> $(\uparrow)$</th>
<th style="text-align: center;">NQ <br> $(\uparrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama 2</td>
<td style="text-align: center;">57.40</td>
<td style="text-align: center;">85.30</td>
<td style="text-align: center;">56.80</td>
<td style="text-align: center;">68.90</td>
<td style="text-align: center;">25.30</td>
</tr>
<tr>
<td style="text-align: left;">SFT Baseline</td>
<td style="text-align: center;">55.97</td>
<td style="text-align: center;">85.17</td>
<td style="text-align: center;">50.72</td>
<td style="text-align: center;">69.76</td>
<td style="text-align: center;">34.35</td>
</tr>
<tr>
<td style="text-align: left;">$M_{1}$</td>
<td style="text-align: center;">57.51</td>
<td style="text-align: center;">84.99</td>
<td style="text-align: center;">60.27</td>
<td style="text-align: center;">69.34</td>
<td style="text-align: center;">35.48</td>
</tr>
<tr>
<td style="text-align: left;">$M_{2}$</td>
<td style="text-align: center;">54.51</td>
<td style="text-align: center;">84.27</td>
<td style="text-align: center;">59.29</td>
<td style="text-align: center;">69.31</td>
<td style="text-align: center;">33.07</td>
</tr>
<tr>
<td style="text-align: left;">$M_{3}$</td>
<td style="text-align: center;">53.13</td>
<td style="text-align: center;">83.29</td>
<td style="text-align: center;">57.70</td>
<td style="text-align: center;">69.37</td>
<td style="text-align: center;">31.86</td>
</tr>
</tbody>
</table>
<p>Human evaluation To examine whether human judgments align with automatic evaluation results, we conduct human evaluations that compare SFT baseline generations with the generations from each iteration of Self-Rewarding training, i.e., models $M_{1}, M_{2}$, and $M_{3}$. Specifically, we randomly select 50 instructions from the IFT test set. Each instruction corresponds to three pairs of generations (i.e., baseline vs. $M_{1}$, baseline vs. $M_{2}$, baseline vs. $M_{3}$ ). For each pair of generations, we assign them to three different annotators (blind evaluation performed by the authors) to make a pairwise judgment, and take a majority vote to decide which generation is better. The human evaluation results are shown in Figure 5. We find that Self-Rewarding models from later iterations show a larger advantage over the SFT baseline model, which is consistent with GPT-4's judgments, and demonstrates the effectiveness of our iterative training procedure.</p>
<p>MT-Bench performance further validates these results We report performance on MT-Bench in Table 2 for the SFT baseline and iterations of the Self-Rewarding model. We</p>
<p>Table 4: Reward Modeling ability improves with Self-Training: We evaluate the LLM-as-a-Judge via various metrics which measure alignment with held-out human preference data. Self-Rewarding Iteration 2 (Model $M_{2}$ ), which is trained using the self-reward model derived from its previous iteration $M_{1}$ outperforms Iteration $1\left(M_{1}\right)$, while $M_{1}$ itself outperforms a standard SFT baseline model trained on only Instruction Fine-Tuning (IFT) data. Iteration 3 (Model $M_{3}$ ) gives further improvements over Iteration 2.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>SFT Baseline</th>
<th>Iter $1\left(M_{1}\right)$</th>
<th>Self-Rewarding Models</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Iter $2\left(M_{2}\right)$</td>
<td>Iter $3\left(M_{3}\right)$</td>
</tr>
<tr>
<td>Training data</td>
<td>IFT</td>
<td>IFT+EFT</td>
<td>IFT+EFT</td>
<td>IFT+EFT+AIFT $\left(M_{1}\right)$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$+\operatorname{AIFT}\left(M_{1}\right)$</td>
<td>$+\operatorname{AIFT}\left(M_{2}\right)$</td>
</tr>
<tr>
<td>Pairwise acc. $(\uparrow)$</td>
<td>$65.1 \%$</td>
<td>$78.7 \%$</td>
<td>$80.4 \%$</td>
<td>$81.7 \%$</td>
</tr>
<tr>
<td>5-best $\%(\uparrow)$</td>
<td>$39.6 \%$</td>
<td>$41.5 \%$</td>
<td>$44.3 \%$</td>
<td>$43.2 \%$</td>
</tr>
<tr>
<td>Exact Match $\%(\uparrow)$</td>
<td>$10.1 \%$</td>
<td>$13.1 \%$</td>
<td>$14.3 \%$</td>
<td>$14.3 \%$</td>
</tr>
<tr>
<td>Spearman corr. $(\uparrow)$</td>
<td>0.253</td>
<td>0.279</td>
<td>0.331</td>
<td>0.349</td>
</tr>
<tr>
<td>Kendall $\tau$ corr. $(\uparrow)$</td>
<td>0.233</td>
<td>0.253</td>
<td>0.315</td>
<td>0.324</td>
</tr>
</tbody>
</table>
<p>again see improvements across the iterations of training from $M_{1}$ to $M_{3}$, from 6.78 (out of 10) up to 7.25 , with larger relative gains in the humanities, STEM, roleplay, writing and extraction categories, and smaller gains in the math, code and reasoning categories. We expect that the latter is due to the seed prompts we use from Open Assistant tending to underemphasize the reasoning-based tasks. We note also that these improvements are in spite of our method using and constructing prompts that only involve a single turn, given the MT-Bench benchmark itself is a multi-turn evaluation.</p>
<p>Self-rewarding models did not lose ability on NLP Benchmarks As shown in Table 3, the performance of most NLP benchmark tasks evaluated are roughly similar to the baselines, with further detailed results on more datasets given in Appendix Table 9 that follow the same pattern. We hypothesize that given that our training data (seed data and synthetically generated data) are based on the Open Assistant prompts which may not be especially relevant to skills needed in the Table 3 tasks, it is expected that the task performance stays roughly similar, or may even drop. For example, in InstructGPT training [Ouyang et al., 2022] they found that "during RLHF fine-tuning, we observe performance regressions compared to GPT-3 on certain public NLP datasets" which they refer to as an "alignment tax." A clear future direction is to extend the self-rewarding paradigm to these types of tasks, by relying not only on seed prompts from Open Assistant, but also on seed prompts found in a larger variety of datasets.</p>
<h1>3.2.2 Reward Modeling Ability</h1>
<p>Reward modeling evaluation results are provided in Table 4.
EFT augmentation improves over SFT baseline Firstly, we find that adding Evaluation Fine-Tuning (EFT) data into training, which gives examples to the model of how to act as an LLM-as-a-Judge, naturally improves its performance compared to training with Instruction Fine-Tuning (IFT) data alone. IFT data covers a wide range of general instruction tasks, and so does endow the SFT Baseline with the ability to evaluate responses; however, EFT data gives more examples of this specific task. We find improvements across all five metrics measured when using IFT+EFT vs. IFT alone, e.g., the pairwise accuracy agreement with humans increases from $65.1 \%$ to $78.7 \%$.</p>
<p>Reward Modeling ability improves with Self-Training We find that performing a round of self-reward training improves the ability of the model at providing self-rewards for the next iteration, in addition to its improved instruction following ability. Model $M_{2}$ (Iteration 2) is trained using the reward model from $M_{1}$ (Iteration 1), but provides improved performance on all five metrics compared to $M_{1}$. For example, pairwise accuracy improves</p>
<p>from $78.7 \%$ to $80.4 \%$. Iteration $3\left(M_{3}\right)$ improves several of these metrics further compared to $M_{2}$, for example pairwise accuracy increases from $80.4 \%$ to $81.7 \%$. This performance gain is achieved despite there being no additional EFT data provided, and the examples created during the Self-Instruction creation loop do not tend to look like LLM-as-a-Judge training examples. We hypothesize that because the model is becoming better at general instruction following, it nevertheless also improves at the LLM-as-a-Judge task.</p>
<p>Importance of the LLM-as-a-Judge Prompt In these experiments we used the LLM-as-a-Judge prompt format shown in Figure 2. In preliminary experiments we also tried various other prompts to decide the most effective one to use. For example, we tried the prompt proposed in Li et al. [2024] which also proposes a 5-point scale, but describes the options as multiple choice in a range of quality buckets, see Appendix Figure 7. In contrast, our prompt describes the points as additive, covering various aspects of quality. We find a large difference between these two prompts when using the SFT Baseline, e.g. $65.1 \%$ pairwise accuracy for ours, and only $26.6 \%$ pairwise accuracy for theirs. See Appendix A. 2 for further details.</p>
<h1>4 Related Work</h1>
<p>Automatically improving or self-correcting large language models is becoming a major focus of research. A recent survey from Pan et al. [2023] attempts to summarize the topic. However, this is a rapidly moving area, and there are already promising new works not covered there.</p>
<p>Reinforcement Learning from Human Feedback (RLHF) Preference learning approaches such as in Ziegler et al. [2019], Stiennon et al. [2020], Ouyang et al. [2022], Bai et al. [2022a] train a fixed reward model from human preference data, and then use the reward model to train via reinforcement learning (RL), e.g. via Proximal Policy Optimization (PPO) [Schulman et al., 2017]. Thus, the reward signal in a certain sense already comes from a model even in these works, but distilled from human data. Nevertheless, this is commonly referred to as RL from Human Feedback (RLHF). Methods such as Direct Preference Optimization (DPO) [Rafailov et al., 2023] avoid training the reward model entirely, and instead directly train the LLM using human preferences. Several other such competing methods exist as well [Zhao et al., 2023, Zheng et al., 2023a, Yuan et al., 2023], including Pairwise Cringe Optimization (PCO) [Xu et al., 2023]. PCO uses an iterative training approach similar to the one in our work, except with a fixed reward model, and that work also showed that Iterative DPO improves over DPO using the same scheme. We note that other works have developed iterative preference training schemes as well, e.g. Adolphs et al. [2023], Gulcehre et al. [2023], Xiong et al. [2023].</p>
<p>Reinforcement Learning from AI Feedback (RLAIF) Constitutional AI [Bai et al., 2022b] uses an LLM to give feedback and refine responses, and uses this data to train a reward model. This fixed, separate reward model is then used to train the language model via RL, called "RL from AI Feedback" (RLAIF). Lee et al. [2023] compare RLAIF and RLHF procedures and find the methods they compare perform roughly equally. They use an "off-the-shelf" LLM to perform LLM-as-a-Judge prompting to build a training set to train a fixed reward model, which is then used for RL training. They also experiment with using the fixed but separate LLM-as-a-Judge model directly, which the authors report is computationally expensive due to using it within PPO training (rather than the offline step in the iterative approach we use in our work, which is relatively computationally cheap). Finally, SPIN [Chen et al., 2024b] recently showed they can avoid reward models entirely in an Iterative DPO-like framework by using human labels as the winning response in a pair, and the last iteration's generations as the losing response in the pair. The authors note this has the limitation that once the model generations reach human performance, they are bottlenecked. Further, each input prompt is required to have a human annotated response, in contrast to our work.</p>
<p>Improving LLMs via data augmentation (and curation) Several methods have improved LLMs by (self-)creating training data to augment fine-tuning. Self-Instruct [Wang</p>
<p>et al., 2023] is a method for self-instruction creation of prompts and responses, which can be used to improve a base LLM. We make use of a similar technique in our work, and then use our self-reward model to score them. Several approaches have also created training data by distilling from powerful LLMs, and shown a weaker LLM can then perform well. For example, Alpaca [Taori et al., 2023] fine-tuned a Llama 7B model with text-davinci-003 instructions created in the style of self-instruct. Alpagasus [Chen et al., 2024a] employed a strong LLM-as-a-Judge (ChatGPT) to curate the Alpaca dataset and filter to a smaller set, obtaining improved results. Instruction Backtranslation [Li et al., 2024] similarly augments and curates training data, but augmenting via backtranslating from web documents to predict prompts. The curation is done by the LLM(-as-a-Judge) itself, so can be seen as an instance of a self-rewarding model, but in a specialized setting. Reinforced Self-Training (ReST) [Gulcehre et al., 2023] uses a fixed, external reward to curate new high-quality examples to iteratively add to the training set, improving performance. In our experiments, we found that adding only positive examples in a related manner did not help, whereas preference pairs did help (see Appendix Section A. 4 for details).</p>
<p>LLM-as-a-Judge Using LLM-as-a-Judge prompting to evaluate language models has become a standard approach [Dubois et al., 2023, Li et al., 2023, Fernandes et al., 2023, Bai et al., 2023, Saha et al., 2023], and is being used to train reward models or curate data as well, as described above [Lee et al., 2023, Chen et al., 2024a, Li et al., 2024]. While some works such as Kim et al. [2023] create training data to train an LLM to perform well as a judge, to our knowledge it is not common to combine this training with general instruction following skills as in our work.</p>
<h1>5 Conclusion</h1>
<p>We have introduced Self-Rewarding Language Models, models capable of self-alignment via judging and training on their own generations. The method learns in an iterative manner, where in each iteration the model creates its own preference-based instruction training data. This is done by assigning rewards to its own generations via LLM-as-a-Judge prompting, and using Iterative DPO to train on the preferences. We showed that this training both improves the instruction following capability of the model, as well as its reward-modeling ability across the iterations. While there are many avenues left unexplored, we believe this is exciting because this means the model is better able to assign rewards in future iterations for improving instruction following - a kind of virtuous circle. While this improvement likely saturates in realistic scenarios, it still allows for the possibility of continual improvement beyond the human preferences that are typically used to build reward models and instruction following models today.</p>
<h2>6 Limitations</h2>
<p>While we have obtained promising experimental results, we currently consider them preliminary because there are many avenues yet to explore, among them the topics of further evaluation, including safety evaluation, and understanding the limits of iterative training.
We showed that the iterations of training improve both instruction following and reward modeling ability, but only ran three iterations in a single setting. A clear line of further research is to understand the "scaling laws" of this effect both for more iterations, and with different language models with more or less capabilities in different settings.
We observed an increase in length in model generations, and there is a known correlation between length and estimated quality, which is a topic that should be understood more deeply in general, and in our results in particular as well. It would also be good to understand if so-called "reward-hacking" can happen within our framework, and in what circumstances. As we are using both a language model as the training reward, and a language model for final evaluation (GPT-4) in some of our benchmarks, even if they are different models, this may require a deeper analysis than we have provided. While the human evaluation we conducted did provide validation of the automatic results, further study could bring more insights.</p>
<p>Another clear further avenue of study is to conduct safety evaluations - and to explore safety training within our framework. Reward models have been built exclusively for safety in existing systems [Touvron et al., 2023], and a promising avenue here would be to use the LLM-as-a-Judge procedure to evaluate for safety specifically in our self-rewarding training process. Given that we have shown that reward modeling ability improves over training iterations, this could mean that the safety of the model could potentially improve over time as well, with later iterations being able to catch and mitigate more challenging safety situations that earlier iterations cannot.</p>
<h1>References</h1>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. The CRINGE loss: Learning what language not to model. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8854-8874, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.493. URL https://aclanthology.org/2023.acl-long. 493.</p>
<p>Anthropic. Claude 2. https://www.anthropic.com/index/claude-2, 2023.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b.</p>
<p>Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. Benchmarking foundation models with language-model-as-an-examiner. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=IiRHQ7gvnq.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.</p>
<p>Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. AlpaGasus: Training a better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=FdVXgSJhvz.</p>
<p>Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024b.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160-167, 2008.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation.</p>
<p>In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 1066-1083, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.100. URL https://aclanthology.org/2023.wmt-1.100.</p>
<p>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023 .</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3QmQ.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409-14428, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.806. URL https://aclanthology.org/2023.acl-long.806.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023.</p>
<p>Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. OpenAssistant conversations-democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.</p>
<p>Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.</p>
<p>Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=1oijHJBRsT.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.</p>
<p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HPuSIXJaa9.</p>
<p>Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. arXiv preprint arXiv:2310.15123, 2023.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. CoRR, abs/1904.09728, 2019. URL http://arxiv.org/abs/1904.09728.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(11), 2008.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with selfgenerated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long. 754.</p>
<p>Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456, 2023.</p>
<p>Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023.</p>
<p>Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to align language models with human feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=EdIGMCHk4l.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791-4800. Association for Computational Linguistics, 2019. doi: 10.18653/V1/ P19-1472. URL https://doi.org/10.18653/v1/p19-1472.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLiC-HF: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.</p>
<p>Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1022-1040, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / 2023$.findings-acl.65. URL https://aclanthology.org/ 2023. findings-acl. 65.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023b. URL https://openreview.net/forum?id=uccHPGDlao.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>A Appendix</h1>
<h2>A. 1 Distributions of IFT, EFT and AIFT data</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Distributions of both instructions and responses for IFT, EFT and AIFT data.
We have plotted the distribution of instructions for IFT, EFT and $\operatorname{AIFT}\left(M_{1}\right)$ data, and the distribution of responses for IFT, EFT and $\operatorname{AIFT}\left(M_{1}\right)$ data in Figure 6. It is clear that the IFT data and EFT data come from very different distributions while the IFT and $\operatorname{AIFT}\left(M_{1}\right)$ data come from similar distributions.</p>
<h2>A. 2 EFT Prompts</h2>
<p>The EFT prompt which we use in our main experiments is shown in Figure 2.
Other EFT prompts we have tried At first, we took the EFT prompt from Li et al. [2024] as shown in Figure 7. However, we found that this prompt was not as effective as our additive score-counting prompt because the model needed to treat the task as a multiple-choice problem, and it was difficult for the model to break down this multiple-choice problem into sub-problems involving evaluating various aspects of the response. When using the model trained on 3,200 IFT data only, its performance on the EFT test set using our additive score-counting prompt and prompt from Li et al. [2024] is shown in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">EFT Prompt</th>
<th style="text-align: center;">Multiple Choice Prompt</th>
<th style="text-align: center;">Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pairwise accuracy ( $\uparrow$ )</td>
<td style="text-align: center;">$26.6 \%$</td>
<td style="text-align: center;">$65.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">5-BEST \% ( $\uparrow$ )</td>
<td style="text-align: center;">$23.5 \%$</td>
<td style="text-align: center;">$39.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Exact Match \% ( $\uparrow$ )</td>
<td style="text-align: center;">$1.1 \%$</td>
<td style="text-align: center;">$10.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Spearman CORr. ( $\uparrow$ )</td>
<td style="text-align: center;">-0.18</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">Kendall $\tau$ CORr. ( $\uparrow$ )</td>
<td style="text-align: center;">-0.16</td>
<td style="text-align: center;">0.23</td>
</tr>
</tbody>
</table>
<p>Table 5: We tried various LLM-as-Judge prompts using the model trained with 3,200 IFT data only and found that our additive score-counting prompt worked best which demonstrates significant improvements in EFT performance comparing to the prompt used by Li et al. [2024].</p>
<h2>A. 3 Self-rewarding Models Using IFT Data Only</h2>
<p>To demonstrate the importance of the EFT data, we also trained a series of models starting with the model trained only on the IFT data. The following is the model sequence.
$M_{0}$ : Base pretrained LLM with no fine-tuning.</p>
<p>Below is a question from an user and a candidate response. Please grade the response on a 5-point scale using the following criteria:</p>
<p>1: It means the answer is incomplete, vague, off-topic, controversial, or not exactly what the user asked for. For example, some content seems missing, numbered list does not start from the beginning, the opening sentence repeats user's question. Or the response is from another person's perspective with their personal experience (e.g. taken from blog posts), or looks like an answer from a forum. Or it contains promotional text, navigation text, or other irrelevant information.
2: It means the answer addresses most of the asks from the user. It does not directly address the user's question. For example, it only provides a high-level methodology instead of the exact solution to user's question.
3: It means the answer is helpful but not written by an AI Assistant. It addresses all the basic asks from the user. It is complete and self contained with the drawback that the response is not written from an AI assistant's perspective, but from other people's perspective. The content looks like an excerpt from a blog post, web page, or web search results. For example, it contains personal experience or opinion, mentions comments section, or share on social media, etc.
4: It means the answer is written from an AI assistant's perspective with a clear focus of addressing the instruction. It provide a complete, clear, and comprehensive response to user's question or instruction without missing or irrelevant information. It is well organized, self-contained, and written in a helpful tone. It has minor room for improvement, e.g. more concise and focused.
5: It means it is a perfect answer from an AI Assistant. It has a clear focus on being a helpful AI Assistant, where the response looks like intentionally written to address the user's question or instruction without any irrelevant sentences. The answer provides high quality content, demonstrating expert knowledge in the area, is very well written, logical, easy-to-follow, engaging and insightful.</p>
<p>User: <INSTRUCTION_HERE>
<response><RESPONSE_HERE></response>
Please first briefly describe your reasoning (in less than 100 words), and then write "Score: <rating>" in the last line. Answer in the style of an AI Assistant, with knowledge from web search if needed. To derive the final score based on the criteria, let's think step-by-step.</p>
<p>Figure 7: LLM-as-a-Judge prompt taken from Li et al. [2024].
$M_{1}^{\prime}:$ Initialized with $M_{0}$, then fine-tuned on the IFT seed data only using SFT.
$M_{2}^{\prime}:$ Initialized with $M_{1}^{\prime}$, then trained with $\operatorname{AIFT}\left(M_{1}^{\prime}\right)$ data using DPO.
$M_{3}^{\prime}:$ Initialized with $M_{2}^{\prime}$, then trained with $\operatorname{AIFT}\left(M_{2}^{\prime}\right)$ data using DPO.</p>
<p>Since we did not use EFT data to train the series of models, they were not always able to score the responses according to the format and even when they did, the scores given typically converged to 4 . Therefore, even when starting from the same number of generated new prompts, we could only collect a very small number of valid training samples for DPO. In total, we collected 541 pairs to form the $\operatorname{AIFT}\left(M_{1}^{\prime}\right)$ dataset used to train $M_{2}^{\prime}$ via DPO, and 429 pairs to form $\operatorname{AIFT}\left(M_{2}^{\prime}\right)$ used to train $M_{3}^{\prime}$. The win rates are shown in Figure 8. From the figure we can conclude that EFT data helps to get better performance in the same number of iterations and the gap in performance between the model trained with EFT data and the model trained without EFT data widens in the later iterations.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: EFT data helps the self-rewarding loop: We evaluated the series of models trained using self-reward loops starting from the model trained using only IFT data. We performed head-to-head win rates comparisons on the IFT test set. While $M_{2}^{\prime}$ can improve over the SFT baseline and $M_{3}^{\prime}$ can improve even more over the SFT baseline, they lag far behind the corresponding models $\left(M_{2}, M_{3}\right)$ that started from a base model trained using both IFT and EFT data, see Figure 3.</p>
<h1><LIST ALL ALPACAEVAL INSTRUCTIONS></h1>
<p>Given the above list of possible instructions, define a maximum of 20 categories that would cover the types of intructions, for example recipes, reasoning tasks, general knowledge etc. Try to cover as many of the instructions as possible with the maximum 20 categories, while keeping the categories high-level, simple and easy to understand.</p>
<p>Figure 9: Prompt used to obtain instruction categories on the AlpacaEval test set.</p>
<p>Instruction: <INSTRUCTION>
Given the above, categorize it into one of the following 20 categories:
<LIST ALL CATEGORIES>
Secondly, score the instruction in terms of complexity: how complex you think it is to answer from 1-10 (where 10 is a complex question whereby first reasoning or breaking down the question into multiple subquestions for example might help improve the answer).</p>
<p>Thirdly, indicate how long you think the response to the instruction should be, either (a) 1 sentence, (b) 1-3 sentences, (c) 1 paragraph, (d) 2 paragraphs, or (e) 3 or more paragraphs.</p>
<p>Provide your final response in the following format:
Category: <one of the 20 categories>
Complexity: <score out of 10>
Length: <length category>. Do not provide the actual response.</p>
<p>Figure 10: Prompt for categorizing instructions based on their topics, complexities and expected response lengths.</p>
<p>Table 6: Breakdown of AlpacaEval test set instructions by instruction category.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: center;">Number</th>
<th style="text-align: right;">Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Science / Technology / Engineering</td>
<td style="text-align: center;">134</td>
<td style="text-align: right;">$16.65 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Professional / Business / Marketing</td>
<td style="text-align: center;">77</td>
<td style="text-align: right;">$9.57 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Social Interaction / Relationships / Human Behavior</td>
<td style="text-align: center;">68</td>
<td style="text-align: right;">$8.45 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Miscellaneous / Other</td>
<td style="text-align: center;">61</td>
<td style="text-align: right;">$7.58 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mathematics / Logical Reasoning</td>
<td style="text-align: center;">52</td>
<td style="text-align: right;">$6.46 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Cooking / Recipes</td>
<td style="text-align: center;">48</td>
<td style="text-align: right;">$5.96 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Software Development / Coding / Algorithms</td>
<td style="text-align: center;">44</td>
<td style="text-align: right;">$5.47 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Travel / Geography / Exploration</td>
<td style="text-align: center;">41</td>
<td style="text-align: right;">$5.09 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Literature / Writing / Communication</td>
<td style="text-align: center;">39</td>
<td style="text-align: right;">$4.84 \%$</td>
</tr>
<tr>
<td style="text-align: left;">History / Social Studies</td>
<td style="text-align: center;">38</td>
<td style="text-align: right;">$4.72 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Entertainment / Media Analysis</td>
<td style="text-align: center;">34</td>
<td style="text-align: right;">$4.22 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Language Learning / Linguistics</td>
<td style="text-align: center;">32</td>
<td style="text-align: right;">$3.98 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Music / Audio / Arts</td>
<td style="text-align: center;">30</td>
<td style="text-align: right;">$3.73 \%$</td>
</tr>
<tr>
<td style="text-align: left;">DIY Projects / Hobbies</td>
<td style="text-align: center;">24</td>
<td style="text-align: right;">$2.98 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Technology / Gadgets / Consumer Products</td>
<td style="text-align: center;">20</td>
<td style="text-align: right;">$2.48 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Gaming / Game Development</td>
<td style="text-align: center;">18</td>
<td style="text-align: right;">$2.24 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Exercise / Health / Wellness</td>
<td style="text-align: center;">16</td>
<td style="text-align: right;">$1.99 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Philosophy / Ethics / Ideology</td>
<td style="text-align: center;">15</td>
<td style="text-align: right;">$1.86 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Sports / Athletics / Physical Activity</td>
<td style="text-align: center;">12</td>
<td style="text-align: right;">$1.49 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Strategy / Problem-Solving / Critical Thinking</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;">$0.24 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Breakdown of AlpacaEval test set instructions by instruction complexity. The instructions increase in complexity from 1 to 9 , where 10 is a complex question that requires first reasoning or breaking the problem into sub-problems before it can be solved.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Complexity</th>
<th style="text-align: right;">Number</th>
<th style="text-align: right;">Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: right;">238</td>
<td style="text-align: right;">$29.57 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: right;">206</td>
<td style="text-align: right;">$25.59 \%$</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: right;">122</td>
<td style="text-align: right;">$15.16 \%$</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">$9.81 \%$</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: right;">68</td>
<td style="text-align: right;">$8.45 \%$</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">$5.09 \%$</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: right;">34</td>
<td style="text-align: right;">$4.22 \%$</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">$1.74 \%$</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">$0.37 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 11: AlpacaEval win rate breakdown for instruction complexities (left) and expected response lengths (right). Self-Rewarding models give gains across most complexities and all response length ranges.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Note, the prompt, derived from Li et al. [2024], mentions "utilizing web search", but our model is not actually capable of this action.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
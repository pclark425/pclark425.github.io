<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-873 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-873</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-873</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-ca0caee3687905dc4175c3db4f07c6e54eff2a3c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ca0caee3687905dc4175c3db4f07c6e54eff2a3c" target="_blank">LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work presents a deep RL agent—LeDeepChef—that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions, and uses an actor-critic framework and prune the action-space to build an agent that achieves high scores across a whole family of games.</p>
                <p><strong>Paper Abstract:</strong> While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent—LeDeepChef—that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's First TextWorld Problems: A Language and Reinforcement Learning Challenge and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e873.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e873.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeDeepChef</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LeDeepChef Deep Reinforcement Learning Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic deep RL agent for families of text-based games (cooking theme) that reduces action-space via learned helper modules (Recipe Manager and Navigator), encodes a multi-part textual belief state with recurrency, and selects commands from a pruned/grouped action set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LeDeepChef</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An advantage actor-critic agent that scores candidate textual commands given a rich, encoded textual context. The architecture: (1) Context encoder: eight text-derived features (observation, missing items, unnecessary items, room description, previous commands, required utilities, discovered locations, location) are embedded (100-d GloVe init) then passed through separate bidirectional GRUs to obtain 32-d vectors per feature; these are concatenated (256-d) and fed into a recurrent GRU (GRU_s) whose hidden state carries information across timesteps producing final context encoding h*. (2) Command encoder: each admissible command is embedded and encoded by a GRU_c into a 32-d vector. (3) Scoring: for each candidate command, concatenation of h* and command encoding is passed through an MLP (256 ReLU) to produce scalar scores; softmax yields a policy distribution p_t. (4) Critic: h* passed through MLP to produce scalar state value. Trained with n-step TD actor-critic objective (policy loss using advantage, value MSE, entropy regularizer). Key components acting as external/tool-like modules: Recipe Manager (supervised GRU-based classifier that inspects recipe directions + inventory and outputs which directions remain, required utilities, and constructs high-level grouped commands like "take all required ingredients from here" and "drop unnecessary items") and Navigator (supervised GRU that predicts which cardinal directions lead to connected rooms and identifies closed doors at the word level, producing go <dir> and open <doorname> commands). The agent uses these modules to prune and augment the action set and to provide structured signals into the context encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Microsoft's First TextWorld Problems: A Language and Reinforcement Learning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A family of text-based games where interaction is purely textual: at each timestep the agent issues a natural language command and receives a textual observation (room descriptions, feedback to actions, inventory listings, recipe descriptions). Games are partially observable (only the current observation and prior textual feedback are available), vary in room layouts and object placements across instances, include blocked passages (closed doors), and require multi-step tasks (collect ingredients and apply cooking actions). Large, combinatorial action and state spaces and sparse rewards make the environment challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Recipe Manager (supervised neural module trained on recipe directions and inventories that outputs: per-direction binary necessity labels, list of missing ingredients, required utilities, and constructs high-level grouped commands and low-level recipe-specific commands); Navigator (supervised neural module trained on room descriptions that predicts which cardinal directions lead to connected rooms and identifies closed doors and their names, adding go <dir> and open <doorname> commands); Freebase-derived food-item augmentation (used to augment Recipe Manager training data); deterministic rules for a small set of always-available commands (look, inventory, prepare meal, eat meal, examine cookbook) — these are rule-based helpers rather than learned modules.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Recipe Manager: structured outputs (binary labels per recipe direction), lists (missing ingredients, unnecessary items, required utilities), and generated high-level textual commands (e.g., 'take all required ingredients from here') and low-level action commands mapped from recipe actions; Navigator: structured predictions (per-cardinal-direction binary connectivity flags) and identified door-name text spans (structured text), which are converted into textual commands (go <dir>, open <doorname>). Augmentation: lists of food items (structured categorical data) from Freebase used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A textual-context-based belief state: eight explicit text-derived features (observation, missing items, unnecessary items, room description, previous commands, required utilities, discovered locations, current location) are embedded and encoded with bidirectional GRUs; their fixed-size encodings are concatenated and then fed into a recurrent GRU (GRU_s) whose hidden state constitutes the temporally-updated belief representation h*. Additional symbolic lists (discovered locations, previous commands) are maintained explicitly and included as features each timestep. Inventory-derived signals (from the inventory command) are parsed to set 'unnecessary items' and feed into the recipe comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At every timestep the agent re-constructs the belief/context from (i) the latest observation text, (ii) outputs of the Recipe Manager (missing items, required utilities, per-direction necessity labels) and Navigator (detected connected cardinal directions and closed door name spans), (iii) parsed inventory results (to produce unnecessary items), (iv) last room description and list of discovered locations, and (v) previous commands. These textual/structured features are embedded and encoded into vectors which are concatenated and passed to GRU_s; GRU_s uses its previous hidden state as the recurrent memory, thereby updating the belief representation h* in a sequential manner. In other words, tool outputs are converted into context features and then integrated into the recurrent textual belief state through the encoder stack and GRU_s recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (advantage actor-critic) operating over a pruned and partially abstracted action set; hierarchical/feudal-style abstraction is applied by the Recipe Manager which groups multiple low-level actions into high-level macro commands (reducing action-space). No explicit model-based planner is used; navigation planning and long-term sequencing are learned by the recurrent policy (GRU_s) and the critic's value estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Symbolic detection of navigable directions via the learned Navigator module (supervised GRU predicts connectivity per cardinal direction and extracts closed-door names). The agent does not run an explicit graph search (e.g., A*); instead it records discovered locations and relies on the recurrent policy and value function to decide sequences of go/open commands (planning emerges from policy learning over the symbolic navigation actions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>With the Recipe Manager, Navigator, action pruning and high-level command grouping, LeDeepChef achieves mean points-per-game of 74.4% on the unseen validation set (mean steps 24.1) and 69.3% on the final test set (mean steps 43.9), averaged over 10 runs (reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating specialized helper modules (Recipe Manager and Navigator) as 'tools' that produce structured outputs and grouped high-level commands dramatically reduces the effective action space, improves sample efficiency, and enables generalization to unseen games; tool outputs are explicitly folded into the agent's textual belief/state representation (as features) and then integrated over time via a recurrent encoder (GRU_s), allowing the actor-critic policy to perform multi-step planning in a partially observable setting. The approach relies on learned policy planning (actor-critic with recurrency) rather than explicit path-finding; Navigator provides symbolic connectivity and door information but no explicit shortest-path computation. Grouping of low-level actions into high-level commands (hierarchical abstraction) is a key mechanism that accelerates learning and stabilizes optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e873.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e873.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recipe Manager (module)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recipe Manager (supervised neural module)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised GRU-based module trained on recipe directions and inventories to predict which recipe directions still need execution, list missing ingredients, and required utilities; it constructs high-level grouped commands and low-level recipe-specific commands to prune the action set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recipe Manager (module used by LeDeepChef)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encodes each recipe direction and the current inventory using GRUs (with pre-trained GloVe embeddings). For each direction it outputs a probability that the direction still needs to be performed (binary classification). Outputs are used to form structured lists (missing ingredients, required utilities) and to add two high-level grouped textual commands: 'take all required ingredients from here' (constructed by intersecting needed ingredients with items present in current room) and 'drop unnecessary items' (for inventory management), plus low-level recipe actions when ingredients and utilities are present.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (used inside LeDeepChef)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>As above: text-based cooking games where recipe steps and ingredient states are provided in text; inventory and environmental descriptions are noisy and partially observable.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Freebase food-item list (used to augment supervised training data); pre-trained GloVe embeddings (used for generalization across ingredient vocabulary).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured labels (per-direction binary necessity), lists of missing ingredients, required utilities, and generated textual high-level commands and specific low-level commands.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Provides structured outputs that are injected into the agent's textual context as explicit features (missing items, required utilities, per-direction flags); it is not itself a recurrent belief module but its outputs inform the agent's belief encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep the recipe model is run on current recipe + inventory; its outputs (which directions remain, missing ingredients, required utilities) replace or update the corresponding context features (missing items, required utilities) that are then re-encoded by the context encoder and passed into GRU_s, thereby updating the belief state.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Supervised prediction to inform hierarchical action generation; planning remains within the actor-critic policy, leveraging outputs from this module to create abstract actions that the policy can treat as single options.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A supervised recipe module that outputs structured, task-specific signals and grouped commands can make a high-dimensional language action space tractable and improve learning/generalization by focusing the policy on relevant high-level steps.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e873.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e873.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Navigator (module)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Navigator (supervised navigation detector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised GRU-based model that takes a room description and predicts which cardinal directions lead to connected rooms and identifies closed doors (word-level binary predictions), converting these into go and open commands to augment the agent's admissible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Navigator (module used by LeDeepChef)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encodes room descriptions with a GRU to obtain a fixed-size vector; feeds this into four binary MLPs to predict whether each cardinal direction (north, east, south, west) leads to a connected room. For closed doors, per-word hidden representations are fed into a shared binary MLP to predict which words form the name of a closed door (to handle multi-word door names). The outputs are converted into textual commands (go <dir>, open <doorname>) and added to the candidate command set.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (used inside LeDeepChef)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable room descriptions in text where connections and door states are described in natural language and must be extracted by the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Binary connectivity flags for cardinal directions; identified door-name text spans (structured text) that are then turned into commands.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Navigator outputs are injected into the context features (available movement directions and closed doors) and thus indirectly become part of the belief encoding used by GRU_s.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep the navigator runs on the current room description and its outputs (available directions, closed-door names) update the context features; these are encoded and folded into the recurrent context representation (GRU_s), enabling the policy to use updated navigation information in decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Supervised detection of admissible navigation actions; planning over sequences of navigation actions is learned by the recurrent actor-critic policy rather than by explicit search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Symbolic connectivity detection plus learned policy sequencing; no explicit graph search algorithm reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A lightweight supervised navigator that extracts symbolic movement affordances (directions and closed doors) from free-text room descriptions suffices to provide admissible navigation actions for a learned policy in partially observable text worlds, removing the need for heavy natural language parsing at action-selection time.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e873.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e873.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ammanabrolu & Riedl 2018 (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that prunes admissible actions using a graph representation of game state; cited as an alternative approach to action pruning in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>mention of graph representation for state</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>mention: graph-based pruning and planning used in related work</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>mention: graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as an approach that uses an explicit graph representation of the game's state to prune actions (alternative to LeDeepChef's learned modules).</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e873.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e873.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yin & May 2019b (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related recent work (cited) that uses map familiarization and curriculum learning to tackle family-of-games generalization in TextWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>mention: uses map familiarization (map-like internal representations) in related work</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>mention: curriculum learning and map familiarization for navigation and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>mention: map-based familiarization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited for using explicit map familiarization and curriculum learning to improve navigation/generalization in families of TextWorld games.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e873.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e873.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zahavy et al. 2018 (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learn what not to learn: Action elimination with deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that begins with an over-complete set of actions and learns to eliminate irrelevant ones using feedback from the environment (action-elimination network).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn what not to learn: Action elimination with deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>mention: action-elimination learned from environment feedback</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>mention: action elimination combined with RL</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as an alternative approach to reduce action-space by learning which actions not to consider using game feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games <em>(Rating: 2)</em></li>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-873",
    "paper_id": "paper-ca0caee3687905dc4175c3db4f07c6e54eff2a3c",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "LeDeepChef",
            "name_full": "LeDeepChef Deep Reinforcement Learning Agent",
            "brief_description": "An actor-critic deep RL agent for families of text-based games (cooking theme) that reduces action-space via learned helper modules (Recipe Manager and Navigator), encodes a multi-part textual belief state with recurrency, and selects commands from a pruned/grouped action set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LeDeepChef",
            "agent_description": "An advantage actor-critic agent that scores candidate textual commands given a rich, encoded textual context. The architecture: (1) Context encoder: eight text-derived features (observation, missing items, unnecessary items, room description, previous commands, required utilities, discovered locations, location) are embedded (100-d GloVe init) then passed through separate bidirectional GRUs to obtain 32-d vectors per feature; these are concatenated (256-d) and fed into a recurrent GRU (GRU_s) whose hidden state carries information across timesteps producing final context encoding h*. (2) Command encoder: each admissible command is embedded and encoded by a GRU_c into a 32-d vector. (3) Scoring: for each candidate command, concatenation of h* and command encoding is passed through an MLP (256 ReLU) to produce scalar scores; softmax yields a policy distribution p_t. (4) Critic: h* passed through MLP to produce scalar state value. Trained with n-step TD actor-critic objective (policy loss using advantage, value MSE, entropy regularizer). Key components acting as external/tool-like modules: Recipe Manager (supervised GRU-based classifier that inspects recipe directions + inventory and outputs which directions remain, required utilities, and constructs high-level grouped commands like \"take all required ingredients from here\" and \"drop unnecessary items\") and Navigator (supervised GRU that predicts which cardinal directions lead to connected rooms and identifies closed doors at the word level, producing go &lt;dir&gt; and open &lt;doorname&gt; commands). The agent uses these modules to prune and augment the action set and to provide structured signals into the context encoding.",
            "environment_name": "TextWorld (Microsoft's First TextWorld Problems: A Language and Reinforcement Learning Challenge)",
            "environment_description": "A family of text-based games where interaction is purely textual: at each timestep the agent issues a natural language command and receives a textual observation (room descriptions, feedback to actions, inventory listings, recipe descriptions). Games are partially observable (only the current observation and prior textual feedback are available), vary in room layouts and object placements across instances, include blocked passages (closed doors), and require multi-step tasks (collect ingredients and apply cooking actions). Large, combinatorial action and state spaces and sparse rewards make the environment challenging.",
            "is_partially_observable": true,
            "external_tools_used": "Recipe Manager (supervised neural module trained on recipe directions and inventories that outputs: per-direction binary necessity labels, list of missing ingredients, required utilities, and constructs high-level grouped commands and low-level recipe-specific commands); Navigator (supervised neural module trained on room descriptions that predicts which cardinal directions lead to connected rooms and identifies closed doors and their names, adding go &lt;dir&gt; and open &lt;doorname&gt; commands); Freebase-derived food-item augmentation (used to augment Recipe Manager training data); deterministic rules for a small set of always-available commands (look, inventory, prepare meal, eat meal, examine cookbook) — these are rule-based helpers rather than learned modules.",
            "tool_output_types": "Recipe Manager: structured outputs (binary labels per recipe direction), lists (missing ingredients, unnecessary items, required utilities), and generated high-level textual commands (e.g., 'take all required ingredients from here') and low-level action commands mapped from recipe actions; Navigator: structured predictions (per-cardinal-direction binary connectivity flags) and identified door-name text spans (structured text), which are converted into textual commands (go &lt;dir&gt;, open &lt;doorname&gt;). Augmentation: lists of food items (structured categorical data) from Freebase used during training.",
            "belief_state_mechanism": "A textual-context-based belief state: eight explicit text-derived features (observation, missing items, unnecessary items, room description, previous commands, required utilities, discovered locations, current location) are embedded and encoded with bidirectional GRUs; their fixed-size encodings are concatenated and then fed into a recurrent GRU (GRU_s) whose hidden state constitutes the temporally-updated belief representation h*. Additional symbolic lists (discovered locations, previous commands) are maintained explicitly and included as features each timestep. Inventory-derived signals (from the inventory command) are parsed to set 'unnecessary items' and feed into the recipe comparison.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At every timestep the agent re-constructs the belief/context from (i) the latest observation text, (ii) outputs of the Recipe Manager (missing items, required utilities, per-direction necessity labels) and Navigator (detected connected cardinal directions and closed door name spans), (iii) parsed inventory results (to produce unnecessary items), (iv) last room description and list of discovered locations, and (v) previous commands. These textual/structured features are embedded and encoded into vectors which are concatenated and passed to GRU_s; GRU_s uses its previous hidden state as the recurrent memory, thereby updating the belief representation h* in a sequential manner. In other words, tool outputs are converted into context features and then integrated into the recurrent textual belief state through the encoder stack and GRU_s recurrence.",
            "planning_approach": "Learned policy (advantage actor-critic) operating over a pruned and partially abstracted action set; hierarchical/feudal-style abstraction is applied by the Recipe Manager which groups multiple low-level actions into high-level macro commands (reducing action-space). No explicit model-based planner is used; navigation planning and long-term sequencing are learned by the recurrent policy (GRU_s) and the critic's value estimates.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Symbolic detection of navigable directions via the learned Navigator module (supervised GRU predicts connectivity per cardinal direction and extracts closed-door names). The agent does not run an explicit graph search (e.g., A*); instead it records discovered locations and relies on the recurrent policy and value function to decide sequences of go/open commands (planning emerges from policy learning over the symbolic navigation actions).",
            "performance_with_tools": "With the Recipe Manager, Navigator, action pruning and high-level command grouping, LeDeepChef achieves mean points-per-game of 74.4% on the unseen validation set (mean steps 24.1) and 69.3% on the final test set (mean steps 43.9), averaged over 10 runs (reported in Table 2).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Integrating specialized helper modules (Recipe Manager and Navigator) as 'tools' that produce structured outputs and grouped high-level commands dramatically reduces the effective action space, improves sample efficiency, and enables generalization to unseen games; tool outputs are explicitly folded into the agent's textual belief/state representation (as features) and then integrated over time via a recurrent encoder (GRU_s), allowing the actor-critic policy to perform multi-step planning in a partially observable setting. The approach relies on learned policy planning (actor-critic with recurrency) rather than explicit path-finding; Navigator provides symbolic connectivity and door information but no explicit shortest-path computation. Grouping of low-level actions into high-level commands (hierarchical abstraction) is a key mechanism that accelerates learning and stabilizes optimization.",
            "name_full_display": null,
            "uuid": "e873.0",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Recipe Manager (module)",
            "name_full": "Recipe Manager (supervised neural module)",
            "brief_description": "A supervised GRU-based module trained on recipe directions and inventories to predict which recipe directions still need execution, list missing ingredients, and required utilities; it constructs high-level grouped commands and low-level recipe-specific commands to prune the action set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Recipe Manager (module used by LeDeepChef)",
            "agent_description": "Encodes each recipe direction and the current inventory using GRUs (with pre-trained GloVe embeddings). For each direction it outputs a probability that the direction still needs to be performed (binary classification). Outputs are used to form structured lists (missing ingredients, required utilities) and to add two high-level grouped textual commands: 'take all required ingredients from here' (constructed by intersecting needed ingredients with items present in current room) and 'drop unnecessary items' (for inventory management), plus low-level recipe actions when ingredients and utilities are present.",
            "environment_name": "TextWorld (used inside LeDeepChef)",
            "environment_description": "As above: text-based cooking games where recipe steps and ingredient states are provided in text; inventory and environmental descriptions are noisy and partially observable.",
            "is_partially_observable": true,
            "external_tools_used": "Freebase food-item list (used to augment supervised training data); pre-trained GloVe embeddings (used for generalization across ingredient vocabulary).",
            "tool_output_types": "Structured labels (per-direction binary necessity), lists of missing ingredients, required utilities, and generated textual high-level commands and specific low-level commands.",
            "belief_state_mechanism": "Provides structured outputs that are injected into the agent's textual context as explicit features (missing items, required utilities, per-direction flags); it is not itself a recurrent belief module but its outputs inform the agent's belief encoding.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep the recipe model is run on current recipe + inventory; its outputs (which directions remain, missing ingredients, required utilities) replace or update the corresponding context features (missing items, required utilities) that are then re-encoded by the context encoder and passed into GRU_s, thereby updating the belief state.",
            "planning_approach": "Supervised prediction to inform hierarchical action generation; planning remains within the actor-critic policy, leveraging outputs from this module to create abstract actions that the policy can treat as single options.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "A supervised recipe module that outputs structured, task-specific signals and grouped commands can make a high-dimensional language action space tractable and improve learning/generalization by focusing the policy on relevant high-level steps.",
            "name_full_display": null,
            "uuid": "e873.1",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Navigator (module)",
            "name_full": "Navigator (supervised navigation detector)",
            "brief_description": "A supervised GRU-based model that takes a room description and predicts which cardinal directions lead to connected rooms and identifies closed doors (word-level binary predictions), converting these into go and open commands to augment the agent's admissible actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Navigator (module used by LeDeepChef)",
            "agent_description": "Encodes room descriptions with a GRU to obtain a fixed-size vector; feeds this into four binary MLPs to predict whether each cardinal direction (north, east, south, west) leads to a connected room. For closed doors, per-word hidden representations are fed into a shared binary MLP to predict which words form the name of a closed door (to handle multi-word door names). The outputs are converted into textual commands (go &lt;dir&gt;, open &lt;doorname&gt;) and added to the candidate command set.",
            "environment_name": "TextWorld (used inside LeDeepChef)",
            "environment_description": "Partially observable room descriptions in text where connections and door states are described in natural language and must be extracted by the agent.",
            "is_partially_observable": true,
            "external_tools_used": null,
            "tool_output_types": "Binary connectivity flags for cardinal directions; identified door-name text spans (structured text) that are then turned into commands.",
            "belief_state_mechanism": "Navigator outputs are injected into the context features (available movement directions and closed doors) and thus indirectly become part of the belief encoding used by GRU_s.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep the navigator runs on the current room description and its outputs (available directions, closed-door names) update the context features; these are encoded and folded into the recurrent context representation (GRU_s), enabling the policy to use updated navigation information in decision-making.",
            "planning_approach": "Supervised detection of admissible navigation actions; planning over sequences of navigation actions is learned by the recurrent actor-critic policy rather than by explicit search.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Symbolic connectivity detection plus learned policy sequencing; no explicit graph search algorithm reported.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "A lightweight supervised navigator that extracts symbolic movement affordances (directions and closed doors) from free-text room descriptions suffices to provide admissible navigation actions for a learned policy in partially observable text worlds, removing the need for heavy natural language parsing at action-selection time.",
            "name_full_display": null,
            "uuid": "e873.2",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Ammanabrolu & Riedl 2018 (related)",
            "name_full": "Playing text-adventure games with graph-based deep reinforcement learning",
            "brief_description": "Related work that prunes admissible actions using a graph representation of game state; cited as an alternative approach to action pruning in text-based games.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": "mention of graph representation for state",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "mention: graph-based pruning and planning used in related work",
            "uses_shortest_path_planning": null,
            "navigation_method": "mention: graph representation",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Referenced as an approach that uses an explicit graph representation of the game's state to prune actions (alternative to LeDeepChef's learned modules).",
            "name_full_display": null,
            "uuid": "e873.3",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Yin & May 2019b (related)",
            "name_full": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games",
            "brief_description": "Related recent work (cited) that uses map familiarization and curriculum learning to tackle family-of-games generalization in TextWorld.",
            "citation_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": "mention: uses map familiarization (map-like internal representations) in related work",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "mention: curriculum learning and map familiarization for navigation and generalization",
            "uses_shortest_path_planning": null,
            "navigation_method": "mention: map-based familiarization",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited for using explicit map familiarization and curriculum learning to improve navigation/generalization in families of TextWorld games.",
            "name_full_display": null,
            "uuid": "e873.4",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Zahavy et al. 2018 (related)",
            "name_full": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "brief_description": "Related work that begins with an over-complete set of actions and learns to eliminate irrelevant ones using feedback from the environment (action-elimination network).",
            "citation_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": "mention: action-elimination learned from environment feedback",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "mention: action elimination combined with RL",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Referenced as an alternative approach to reduce action-space by learning which actions not to consider using game feedback.",
            "name_full_display": null,
            "uuid": "e873.5",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games",
            "rating": 2
        },
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01516,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LeDeepChef Deep Reinforcement Learning Agent for Families of Text-Based Games</h1>
<p>Leonard Adolphs, Thomas Hofmann<br>Department of Computer Science<br>ETH Zurich<br>{firstname.lastname}@inf.ethz.ch</p>
<h4>Abstract</h4>
<p>While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of TextBased Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent-LeDeepChef-that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's First TextWorld Problems: A Language and Reinforcement Learning Challenge and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.</p>
<h2>Introduction</h2>
<p>"You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!", that's the starting instruction of every game in Microsoft's First TextWorld Problems: A Language and Reinforcement Learning Challenge; a competition that evaluates an agent on a family of unique and unseen text-based games (TBGs). While all the games share a similar theme-cooking in a modern house environmentthey differ in multiple aspects like the number of rooms, connection, and arrangement of rooms, the goal of the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>game (i.e., different recipes), as well as actions and tools needed to succeed. TBGs are computer games where the sole modality of interaction is text. In an iterative process, the player issues commands in natural language and, in return, is presented a (partial) textual description of the environment. The player works towards goals that may or may not be specified explicitly and receives rewards upon completion. To frame it more formally, both the observation and action-space are comprised of natural language and, thus, inherit its combinatorial and compositional properties (Côté et al. 2018). Training an agent to succeed in such games requires overcoming several common research challenges in reinforcement learning (RL), such as partial observability, large and sparse state-, and action-space and long term credit assignment. Moreover, the agent needs several human-like abilities including understanding the environment's feedback (e.g., realize that some command had no effect on the game's state), and common sense reasoning (e.g., extracting affordance verbs to an object in the game) (Fulda et al. 2017).</p>
<p>While TBGs reached their peak of popularity in the 1980s with games like Zork (Infocom 1980), they provide an interesting test-bed for AI agents today. Due to the dialoglike structure of the game and the goal to find a policy that maximizes the player's reward, they show great similarity to real-world tasks like question answering and open dialogue generation. Games like Zork are usually contained in a single environment that requires a variety of complex problem-solving abilities. The TextWorld framework (Côté et al. 2018) instead, generates a family of games with different worlds and properties but with straightforward and, most importantly, similar tasks. One can argue, that it is, therefore, more similar to human skill acquisition: once learned, a skill can easily be performed even in a slightly different environment or with new objects (Yin and May 2019b). Recent research has mainly focused on either learning a single TBG to high accuracy (Narasimhan, Kulkarni, and Barzilay 2015; He et al. 2015; Ammanabrolu and Riedl 2018) or generalization to a completely new family of games (Kostka et al. 2017) with only very poor performance. Microsoft's First TextWorld Problems: A Language and Reinforcement</p>
<p>Learning Challenge aims to cover a new research direction, that is in-between the two extremes of the single game and the general game setting. To succeed here, an agent needs to have generalization capabilities that allow it to transfer its learned cooking skills to never-before-seen recipes in unfamiliar house environments.
In this work, we present our final agent-LeDeepChef that achieved the high score on the (hidden) validation games and was ranked second in the overall competition. The code to train the agent, as well as an exemplary walkthrough of the game (with the agent ranking next moves), can be found on GitHub ${ }^{1}$. In order to design a successful agent, we make the following contributions:</p>
<ul>
<li>We design an architecture that uses different parts of the context to rank a set of commands, that is trained within an actor-critic framework. Through recurrency over timesteps, we construct a model that is aware of the past context and its previous decisions.</li>
<li>We improve generalization to unseen environments by abstracting away standard to "high-level" commands similar to feudal learning approaches (Dayan and Hinton 1993). We show that this reduces the action-space and therefore accelerates and stabilizes the optimization procedure.</li>
<li>We incorporate a task-specific module that predicts the missing steps to complete the task. We train it supervised on a dataset based on TextWorld recipes augmented with a list of the most common food items found in freebase to make it resilient to unseen recipes and ingredients.
The paper is organized as follows. Section "Related Work" gives an overview of the current state of research in the field of TBGs. In section "Gameplay", we explain the TextWorld challenge in more detail and provide an exemplary walkthrough. The architecture, as well as the RL training procedure of our final agent, is described in section "Agent". Section "Command Generation" presents our command generation approach. Finally, the section "Results" compares the performance of our agent to several reasonable baselines.</li>
</ul>
<h2>Related Work</h2>
<p>Since the pioneering work of (Mnih et al. 2013) that combines deep neural networks with reinforcement learning techniques to successfully play Atari games, there has been an increasing interest to modify these algorithms to a variety of problems. However, due to the combinatorial and compositional property of natural language, resulting in huge action- and state-spaces, no major improvements have been made in this area. Text-based games are regarded as a good testbed for research at the intersection of RL and NLP (Côté et al. 2018). Even though they heavily simplify the environment-compared to, e.g., a real-world open dialogue-they present a broad spectrum of challenges for learning algorithms.</p>
<p>Deep RL for TBGs To solve TBGs, (Narasimhan, Kulkarni, and Barzilay 2015) developed a deep RL model that</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>utilizes the representational power of the hidden state of Long Short-Term Memory (Hochreiter and Schmidhuber 1997) to learn a Q-function. An adaption of this approach by He et al. (2015), uses two separate models to encode the context and commands individually, and then uses a pairwise interaction function between them to compute the Q values. Since then, a variety of researchers (Ammanabrolu and Riedl 2018; Yin and May 2019b; Yin and May 2019a) used some form of DQN to solve TBGs; however, we find that an advantage-actor-critic approach (Mnih et al. 2016) helps to improve performance and speeds up convergence. Using Narasimhan, Kulkarni, and Barzilay (2015)'s LSTMDQN or He et al. (2015)'s adjusted DRRN on the family of games of the TextWorld challenge leads to extremely slow convergence due to the huge combinatorial action-space that arises from games with different objects and the combinatorial nature of natural language (Ammanabrolu and Riedl 2018).</p>
<p>Large action-space Text-based games can be divided by their type of input-interaction: (i) parser-based, where the agent issues commands in free form and (ii) choice-based, where the agent is presented a set of admissible commands at every turn. Assuming a fixed maximum length of the commands as well as a fixed-size vocabulary, a parser-based game is a special instance of a choice-based game with the set of all possible combinations of words in the vocabulary as the set of admissible commands. This illustrates the problem arising from combinatorial action spaces: they result in a huge set of possible options for the agent, which it cannot possibly explore in a reasonable amount of time. Hence, the major challenge is the generation of a small set of reasonable commands for a given context. Using a supervised learning approach with a pointer-softmax model Vinyals, Fortunato, and Jaitly (2015), Tao et al. (2018) as well as Yuan et al. (2018b) are able to generate admissible commands given a context for a specific TBG. A more general approach by (Fulda et al. 2017) learns to map nouns to affordant verbs by extracting replacement vectors from word embeddings using canonical examples. Zahavy et al. (2018), on the other hand, start from an over-complete set of actions and learn a binary action-elimination network by using the feedback provided by the game engine. Similarly, Ammanabrolu and Riedl (2018) also prune the available actions but using a fixed scoring function on top of a graph representation of the game's state. As far as we know, our model is the first in the area of TBGs to consider grouping commands together to 'high-level' actions as a way to reduce the action-space.</p>
<h2>Gameplay</h2>
<p>This section provides an overview of the structure of the games in the TextWorld challenge and explains the problems an agent needs to overcome to succeed. Figure 1 shows an example of a straightforward game which helps understand the basic structure. The agent starts at a random room around the house with the instruction to find the cookbook and prepare the meal therein. The initial description of the surround-</p>
<p>You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!
$\Rightarrow$ Kitchen $=$ You're now in the kitchen. I guess you better just go and list everything you see here. You hear a noise behind you and spin around, but you can't see anything other than a fridge. The fridge is empty! This is the worst thing that could possibly happen, ever! You see a closed oven. You make out a table. The table is massive. On the table you can make out a cookbook. You can make out a counter. The counter is vast. On the counter you can make out a sliced fried red hot pepper, a red apple, a yellow bell pepper and a knife. You smell an intriguing smell, and follow it to a stove. You shudder, but continue examining the stove. But the thing is empty, unfortunately. Hm. Oh well.
$&gt;$ examine recipe
You open the copy of "Cooking: A Modern Approach (3rd Ed.)" and start reading: Recipe #1
Gather all following ingredients and follow the directions to prepare this tasty meal.
Ingredients:
red hot pepper
Directions:
slice the red hot pepper
fry the red hot pepper
prepare meal
$&gt;$ inventory
You are carrying nothing.
$&gt;$ take red hot pepper
You take the red hot pepper from the counter.
Your score has just gone up by one point.
$&gt;$ inventory
You are carrying:
a sliced fried red hot pepper
$&gt;$ prepare meal
Adding the meal to your inventory.
Your score has just gone up by one point.
$&gt;$ eat meal
You eat the meal. Not bad.
Your score has just gone up by one point.
*** The End ${ }^{<em> * </em>}$</p>
<p>Figure 1:Simple game that shows the basic structure of the task. The player's commands are highlighted in yellow.
ing exemplifies one of the key challenges, namely filtering the vital information from the text: sentence like you hear a noise behind you and spin around, ... or This is the worst thing that could possibly happen, ever! provide no useful information for the game and make it harder to understand the context.
Once the agent finds the room with the cookbook (in the example in Figure 1, it is in the starting room already), the examine recipe reveals the recipe. It consists of two parts: the ingredients, and the directions. While the ingredients part lists the items that need to be collected, the directions give information about the status they need to be in to prepare the meal. In our example, the pepper needs to be sliced and fried. Here, the agent needs to be careful, because the initial description of the surrounding states that the pepper is already sliced and fried and additional frying, for example, would lead to burning the pepper and hence losing the game. The agent, therefore, needs to remember and recognize states of ingredients mentioned in the context. With the inventory command the agent can list all items it is currently
carrying. Once all ingredients, in their correct state, are in the inventory, the agent can prepare and then eat the meal.</p>
<h2>Agent</h2>
<p>We train an agent to select, at every step in the game, the most promising command (in terms of discounted future score) from a list of possible commands, given the observed context. Building a successful agent-not just for TBGs but for a wide range of sequential decision-making applications-is primarily determined by the presented set of choices at each time-step. Therefore, one of the most crucial questions is about how to generate the list of possible commands. The smaller this set is, the less time and effort the agent wastes in its exploratory phase on "useless" strategies. To effectively reduce the size of the action-space, we use an approach inspired by hierarchical reinforcement learning, that we explain in the next section about "Command Generation". In the current section, we outline the architecture and training procedure of the agent, acting on a given set of commands.</p>
<h2>Model</h2>
<p>Model context We build a textual context as an approximation for the (non-observable) game's state. It consists of the following text-based features:</p>
<ol>
<li>Observation: The response from the game engine at the current time-step. It can either be a description of what the agent sees in this room or a direct response to its last command.</li>
<li>Missing items: The list of items that are in the recipe but not yet in the inventory. This information is constructed using the neural recipe model described in Section "Command Generation".</li>
<li>Unnecessary items: The list of items that are in the inventory but are not needed to execute the recipe. We extract this information from the last response to the inventory command.</li>
<li>Description: The description of the current room. It is the output of the last look command.</li>
<li>Previous commands: The list of the ten previously executed commands.</li>
<li>Required utilities: The list of kitchen appliances needed for the recipe, e.g., knife or stove. This list is a result of the prediction by the recipe model described in Section "Command Generation".</li>
<li>Discovered locations: The list of previously visited locations.</li>
<li>Location: The name of the current location, extracted from the last observation (if it included a location).
The architecture of the model is shown in Figure 2. It consists of four building blocks: context encoding, commands encoding, computation of the value of the current state, and the command scoring.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2:Illustration of the model. From a textual description of the context together with $k$ different possible commands, it computes a categorical distribution over the commands as well as a scalar representing the value of the current game state.</p>
<p>Context Encoding The input to the context encoding are the eight text-based features described above. Each of them is a sequence of words that we embed using a trainable 100dimensional word embedding, initialized with pre-trained GloVe (Pennington, Socher, and Manning 2014). This results in eight matrices of shape ( $\operatorname{seqlen}<em f="f" i="i">{i} \times 100$ for $i=$ $1, \ldots, 8$ ) that are fed into eight separate bi-directional GRUs $\left(\mathrm{GRU}</em>\right)$ that has its recurrency over time, i.e., it takes as hidden state the context encoding from the previous time-step (Yuan et al. 2018a).}\right)$. Using the last hidden vector of each GRU, we construct a fixed size encoding of size 32 for every feature input sequence. By concatenating the individual vectors, we obtain a representation for the full context with a fixed size of 256. To obtain the final context encoding $h^{*}$, we pass this representation into another GRU $\left(\mathrm{GRU}_{s</p>
<p>Commands Encoding At every time-step, the model has a set with varying length $k_{t}$ of different possible commands to choose from. Each command is embedded using the same embedding matrix as the context, resulting in a set of $k$ matrices of size $\left(\mathrm{cmdlen}<em c="c">{i} \times 100\right)^{2}$. A single GRU $\left(\mathrm{GRU}</em>$ for $i=1, \ldots, k$.}\right)$ is used to encode the $k$ different commands individually to fixed-size representations $c_{i} \in \mathbb{R}^{32</p>
<p>Value Computation As described in more detail in the upcoming subsection, we use an advantage-actor-critic approach to train the agent. This approach requires a critic function that determines the value of the current state. In our model, we compute this scalar value by passing the encoded context $h^{*}$ through an MLP with a single hidden layer of size 256 and ReLU activations.</p>
<p>Scoring and Command Selection For each possible command, we compute a scalar score by feeding the concatena-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion of the encoded context $h^{*}$ and the encoded command $c_{i}$ for $i=1, \ldots, k$ into an MLP with a single hidden layer of size 256 and ReLU activations. We obtain a score vector $\mathbf{s}<em t="t">{t} \in \mathbb{R}^{k}$ that ranks the $k$ possible commands. On top of the score vector, we apply a softmax to turn it into a categorical distribution $\mathbf{p}</em>$, we sample the final command from the presented set of input commands.}$. Based on $\mathbf{p}_{t</p>
<h2>Training the agent with the Actor-Critic method</h2>
<p>We use an online actor-critic algorithm with a shared network design to optimize the agent. We compute the return $\mathcal{R}_{t}$ of a single time-step $t$ in the session of length $T$ by using the n -step temporal difference method (Sutton and Barto 2018, ch. 7)</p>
<p>$$
\mathcal{R}\left(s_{t}, a_{t}\right)=\gamma^{T-t} v\left(s_{T}\right)+\sum_{\tau=0}^{T-t} \gamma^{\tau} r\left(s_{t+\tau}, a_{t+\tau}\right)
$$</p>
<p>where $\gamma$ denotes the discount factor, and $v\left(s_{T}\right)$ denotes the value of the state, determined by the critic network, that depends on the state $s_{T}$. The game-environment determines the score $r$, based on the state $s$, and the chosen action $a$.</p>
<p>From $\mathcal{R}<em t="t">{t}$ we compute the advantage $\mathcal{A}</em>$ at time-step $t$ by subtracting the state value from the critic network, i.e.</p>
<p>$$
\mathcal{A}\left(s_{t}, a_{t}\right)=\mathcal{R}\left(s_{t}, a_{t}\right)-v_{t}\left(s_{t}\right)
$$</p>
<p>While the value function from the critic $v$ captures how good a certain state is, the advantage informs us how much extra reward we obtain from action $a$ compared to the expected reward in the current state $s$. For the sake of brevity, we will drop the indication of dependence of the state $s$ and action $a$ from now on.</p>
<p>Objective The full objective $\mathcal{L}$ consists of three individual terms: the policy loss, the value loss, and the entropy loss. The policy term optimizes the parameters of the actornetwork while keeping the critic's weights fixed. It encour-</p>
<p>ages (penalizes) the current policy if it led to a positive (negative) advantage. The policy loss is given by the following formula</p>
<p>$$
\mathcal{L}<em t="1">{p}=-\frac{1}{T} \sum</em>}^{T} \mathcal{A<em t="t">{t}^{*} \log \boldsymbol{p}</em>\right]
$$}\left[a_{t</p>
<p>where $\mathcal{A}<em t="t">{t}^{*}$ is the advantage $\mathcal{A}</em>}$ removed from the computational graph, and $\boldsymbol{p<em t="t">{t}\left[a</em>$ determined by the actor.
The value term uses a mean squared error between the return $\mathcal{R}$ and the value of the critic $v_{t}$ to encourage them to be close, i.e.}\right]$ is the probability of the chosen command $a_{t</p>
<p>$$
\mathcal{L}<em t="1">{v}=\frac{1}{2 T} \sum</em>}^{T}\left(\mathcal{R<em t="t">{t}-v</em>
$$}\right)^{2</p>
<p>Finally, the entropy loss penalizes the actor for putting a lot of probability mass on single commands and therefore encourages exploration:</p>
<p>$$
\mathcal{L}<em t="1">{e}=-\frac{1}{T} \sum</em>}^{T} \boldsymbol{p<em t="t">{t}^{T} \log \boldsymbol{p}</em>
$$</p>
<p>The final training objective is chosen as a linear combination of to three individual terms.</p>
<h2>Command Generation</h2>
<p>One of the primary challenges in TBGs is the construction of possible-or rather reasonable-commands in any given situation. Due to the combinatorial nature of the actions, the size of the search space is vast. Thus, brute force learning approaches are infeasible, and RL optimization is extremely difficult. We solve this problem by effectively generating only a small number of the most promising commands, as well as combining multiple actions to a single high-level command. We find that this step of reducing the action-space is the most important to guarantee successful and stable learning of the agent. To this end, we train a helper modelcalled Recipe Manager-that effectively extracts from the game's state which recipe actions still need to be performed. By comparing the state of the ingredients in the inventory with the given recipe and the description of the environment, it generates the next commands in the cooking process.</p>
<h2>Recipe Commands</h2>
<p>The task of this model is to determine, from the raw description of the inventory and the recipe, the following information for every listed ingredient:</p>
<ul>
<li>Does it still need to be collected?</li>
<li>Which cooking actions still need to be performed with it?</li>
</ul>
<p>Figure 4(b) in the Appendix shows an example of how the model extracts from the raw textual input the structured information needed. To achieve this, we train a model in a supervised manner with a self-constructed dataset. The dataset consists of recipes and inventories similar to those of the training games but augmented with multiple additional ingredients and adjectives to foster its generalization capabilities. Here, we query the freebase database to obtain a large selection of popular food items to make our classifier more resilient to ingredients not present in the training games.</p>
<p>Model The input to the recipe model is the individual recipe directions and the current inventory of the agent. We do a binary classification of each direction about whether or not it needs to be performed. The necessary information about the state of the ingredient is present in the inventory. Hence, we need to map and compare each direction to it. The names of the ingredients are of varying length and can have multiple adjectives describing it, e.g., a sliced red hot pepper or some water. Therefore, we treat each direction and the inventory as a variable-length sequence that we encode using a GRU, after embedding it with pre-trained GloVe (Pennington, Socher, and Manning 2014). Using pre-trained embeddings not just speeds up the convergence of the model but also helps to make it generalize across unseen ingredients, because all food-related items are close in the embedding space (Pennington, Socher, and Manning 2014). As can be seen in Figure 4(c) in the Appendix, each of the encoded recipe directions is concatenated with the encoded inventory to serve as the input to an MLP. The network outputs a single value for each of the inputs that represent the probability of the given direction still being necessary to perform.</p>
<p>Adding recipe actions to the possible commands The recipe manager adds two high-level commands to the action set. First, the take all required ingredients from here command, grouping all the necessary 'take' commands, that can be performed in the current room. We construct this list by the intersection of needed ingredients (determined by the recipe model) and ingredients present in the current context description. Second, the drop unnecessary items command that lists 'drop' commands for all the ingredients labeled as unnecessary from the learned recipe model. It is indeed crucial to learn to drop unwanted items because the inventory has a fixed capacity. In addition to the abstract high-level commands, it adds all action commands-specified by the recipe model-if the specific ingredient is in the inventory and the corresponding utility in the room. Figure 4(a) in the Appendix provides an example for how the mapping from high-level to low-level commands is constructed based on the room description, the inventory, and the output from the neural recipe model.</p>
<h2>Navigation Commands</h2>
<p>Another crucial challenge for an agent in a TBG is to efficiently navigate through the game-world; an especially hard task when presented with unseen room configurations at test time. This process can be divided into two tasks, namely (i) understanding from the context in which direction it is possible to move, and (ii) the planning required to move through the rooms efficiently. While the latter is learned by the model as part of its policy, the challenge of extracting the movement directions from the unstructured text remains. Moreover, in the TextWorld environment, every connected room can be blocked by a closed door that the agent has to explicitly open before going into this direction. Therefore, it is necessary not only to understand in which cardinal direction to move for the next room but also to identify</p>
<p>all closed doors in the way. For this task, we learned the Navigator model, that is supervised trained on augmented walkthroughs to identify (i) cardinal directions that lead to connected rooms, and (ii) find all closed doors in the current room. The model takes any room description as input and encodes the sequence with a GRU to obtain a fixed-size vector representation. This is fed into four individual MLPs that make a binary prediction on whether the corresponding cardinal direction leads to a connected room. To obtain the closed doors in the room, the hidden representation from each word of the description is fed into a shared binary MLP that predicts whether or not a particular word is part of the name of a closed door. This approach is necessary because there can be multiple different closed doors in a room, and the name of each door can consist of multiple words, e.g., sliding patio door.
The navigator adds for every detected cardinal direction (east, south, west, north) the respective go command to the list of possible commands. Additionally, it adds open $&lt;$ doorname $&gt;$ for every closed door in the room's description.</p>
<h2>Other Commands</h2>
<p>Besides the commands that handle the navigation and the cooking, there are a few additional actions that are necessary to succeed in the game. Since the number of these commands is minimal, they are either added at every time-step to the set of possible commands or based on very simple rules. We provide the list of additional commands and their rules in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">look,</td>
<td style="text-align: left;">Added at every step, except if they</td>
</tr>
<tr>
<td style="text-align: left;">inventory</td>
<td style="text-align: left;">were just performed in the previous <br> command.</td>
</tr>
<tr>
<td style="text-align: left;">prepare meal</td>
<td style="text-align: left;">Added once the recipe manager does <br> not output any recipe direction as miss- <br> ing anymore and the agent's location is <br> the kitchen.</td>
</tr>
<tr>
<td style="text-align: left;">eat meal <br> examine cook- <br> book</td>
<td style="text-align: left;">Added if meal is in agent's inventory. <br> Added if the cookbook is in the room's <br> description.</td>
</tr>
</tbody>
</table>
<p>Table 1:Rules for additional commands to be added to the list of possible commands.</p>
<h2>Results</h2>
<p>First and foremost, the model was evaluated quantitatively against more than 20 competitors in Microsoft's TextWorld challenge, where it scored 1st on the (hidden) validation set and 2 nd on the final test set of games. To show that our agent improves upon existing models for TBGs on never-before-seen games of the same family, we compare it against several baselines on the competition's training, validation, and test set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">valid</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">test</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\%</td>
<td style="text-align: center;">steps</td>
<td style="text-align: center;">\%</td>
<td style="text-align: center;">steps</td>
</tr>
<tr>
<td style="text-align: center;">Random WL</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">98.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .04$</td>
<td style="text-align: center;">$\pm .27$</td>
<td style="text-align: center;">$\pm .03$</td>
<td style="text-align: center;">$\pm .02$</td>
</tr>
<tr>
<td style="text-align: center;">LSTM-DQN</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">99.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .00$</td>
<td style="text-align: center;">$\pm .00$</td>
<td style="text-align: center;">$\pm .00$</td>
<td style="text-align: center;">$\pm .00$</td>
</tr>
<tr>
<td style="text-align: center;">Random AC</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .59$</td>
<td style="text-align: center;">$\pm 1.67$</td>
<td style="text-align: center;">$\pm .64$</td>
<td style="text-align: center;">$\pm .31$</td>
</tr>
<tr>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">50.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .12$</td>
<td style="text-align: center;">$\pm 1.65$</td>
<td style="text-align: center;">$\pm .25$</td>
<td style="text-align: center;">$\pm .05$</td>
</tr>
<tr>
<td style="text-align: center;">Random Pruned</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">95.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .66$</td>
<td style="text-align: center;">$\pm .81$</td>
<td style="text-align: center;">$\pm .14$</td>
<td style="text-align: center;">$\pm .36$</td>
</tr>
<tr>
<td style="text-align: center;">DRRN Pruned</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">92.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .31$</td>
<td style="text-align: center;">$\pm .41$</td>
<td style="text-align: center;">$\pm 2.01$</td>
<td style="text-align: center;">$\pm 1.80$</td>
</tr>
<tr>
<td style="text-align: center;">Yin and May 2019b</td>
<td style="text-align: center;">$58^{3}$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LeDeepChef</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">43.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .18$</td>
<td style="text-align: center;">$\pm .23$</td>
<td style="text-align: center;">$\pm .20$</td>
<td style="text-align: center;">$\pm .19$</td>
</tr>
</tbody>
</table>
<p>Table 2:Results on the unseen set of validation and test games from the TextWorld Challenge. We report the mean and standard deviation over ten runs with different random seeds of each best performing model on the training set.</p>
<p>As a metric, we always report the points per game relative to the total achievable points. A single game terminates upon successful completion of the task or when the agent fails, by either damaging an item or reaching the maximum number of a hundred steps.</p>
<p>Baseline Figure 3 (a) demonstrates that standard baselines for TBGs are not able to learn generalization capabilities to sufficiently solve a whole family of games. Both, LSTM-DQN (Narasimhan, Kulkarni, and Barzilay 2015) and DRRN (He et al. 2015), do not exceed the $20 \%$ mark of points per game relative to total achievable points ${ }^{4}$ during 3 epochs of training. The input to both of these models is the concatenated game's state, consisting of the room's description, the agent's inventory, the recipe, the feedback from the last command, and the set of previously issued commands. The main difference between DRRN and LSTM-DQN is that the former ranks the provided admissible commands, while the latter ranks (pre-selected) verbs and objects, from which a command is formed then. Due to the combinatorial nature of possible commands from the LSTM-DQN, the effective action-space is significantly larger than for DRRN. Thus, a random agent on this word-level task-Random $W L$-performs much worse than an agent that selects randomly from the admissible commands, Random AC. Both DRRN and LSTM-DQN significantly outperform their ran-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Training games scoring percentage of DRRN and LSTM-DQN over three epochs. The two baselines Random $A C$ and Random WL show the performance of a random agent on the admissible commands (like for DRRN) and the word-level (like LSTM-DQN), respectively.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Training games scoring percentage of LeDeepChef compared to a DRRN model and a random baseline on the same pruned commands, generated by the recipe module.</p>
<p>Figure 3:Comparison of our model to several baseline models on the TextWorld challenge games, as points per game relative to total achievable points throughout the training of 3 epochs with 10 different random seeds. Each shown point is an average over the past 80 games. The model details of the baselines can be found in Table 3 in the Appendix.
dom counterpart over the course of the training but are not able to learn to solve the games to a sufficient degree. The big scoring difference between the two random agents underlines the importance of effective action-space reduction.</p>
<p>Comparison on pruned commands In a second experiment, we use the same DRRN architecture as before, but with a pruned version of the admissible commands to exactly match the commands presented to our model; though, without the grouping to high-level actions. As we see in Figure 3 (b), the reduced set of possible commands massively improves both the random and the DRRN model to up to $50 \%{ }^{5}$. However, the DRRN model is still not capable of improving a lot upon the random model and-as before-does not show a steady upward trend throughout the training procedure. Our model, on the other hand, improves its percentage significantly over the training iterations to its final score of around $87 \%$. We believe that the advantage of our model over this specific baseline is mainly due to (i) the grouped high-level commands that let the agent learn a strategy more efficiently in an abstract space, (ii) the improvements in the neural architecture that acts on a more sophisticated version of the input features, and (iii) the superiority of the actorcritic over the DQN approach.</p>
<p>Comparison on Test Set Table 2 shows the quantitative results of different models on the (unseen) validation set, as well as the final test set of Microsoft's TextWorld challenge. As expected, our model generalizes best to the unseen games with a mean percentage of 74.4 and 69.3 for the respective sets of games. The standard baselines are not able</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to exceed the $15 \%$ mark, indicating that they are not suitable to be applied "out-of-the-box" on the specific task of solving families of TBGs. A recent model by Yin and May (2019b), designed explicitly for the TextWorld environment, uses a curriculum learning approach to train a DQN model and achieves $58 \%$ on their validation set (hold-out data from the challenge's training set).</p>
<h2>Conclusion</h2>
<p>In this work, we presented how to build a deep RL agent that not only performs well on a single TBG but generalizes to never-before-seen games of the same family. To achieve this result, we designed a model that effectively ranks a set of commands based on the context and context-derived features. By incorporating ideas from hierarchical RL, we significantly reduced the size of the action-space and were able to train the agent through an actor-critic approach. Additionally, we showed how to make the agent more resilient against never-before-seen recipes and ingredients by training with data augmented by a food-item database. The performance of our final agent on the unseen games of the FirstTextWorld challenge is substantially higher than any standard baseline. Moreover, it achieved the highest score, among more than 20 competitors, on the (unseen) validation set and beat all but one agent on the final test set.</p>
<h2>Acknowledgments</h2>
<p>We thank Florian Schmidt, Gary Becigneul, and Jonas Kohler for their comments and suggestions. This research was supported by the Swiss National Science Foundation (SNSF) grant number 407540_167176 under the project "Conversational Agent for Interactive Access to Information".</p>
<h2>References</h2>
<p>Ammanabrolu, P., and Riedl, M. O. 2018. Playing text-adventure games with graph-based deep reinforcement learning. CoRR abs/1812.01628.
Côté, M.; Kádár, Á.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M. J.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. Textworld: A learning environment for text-based games. CoRR abs/1806.11532.
Dayan, P., and Hinton, G. E. 1993. Feudal reinforcement learning. In Hanson, S. J.; Cowan, J. D.; and Giles, C. L., eds., Advances in Neural Information Processing Systems 5. Morgan-Kaufmann. 271-278.
Fulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What can you do with a rock? affordance extraction via word embeddings. CoRR abs/1703.03429.
He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Ostendorf, M. 2015. Deep reinforcement learning with an unbounded action space. CoRR abs/1511.04636.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural Comput. 9(8):1735-1780.
Infocom. 1980. Zork i.
Kostka, B.; Kwiecien, J.; Kowalski, J.; and Rychlikowski, P. 2017. Text-based adventures of the golovin AI agent. CoRR abs/1705.05637.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M. A. 2013. Playing atari with deep reinforcement learning. CoRR abs/1312.5602.
Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T. P.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous methods for deep reinforcement learning. CoRR abs/1602.01783.
Narasimhan, K.; Kulkarni, T. D.; and Barzilay, R. 2015. Language understanding for text-based games using deep reinforcement learning. CoRR abs/1506.08941.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In In EMNLP.
Sutton, R. S., and Barto, A. G. 2018. Reinforcement Learning: An Introduction. The MIT Press, second edition.
Tao, R. Y.; Côté, M.; Yuan, X.; and Asri, L. E. 2018. Towards solving text-based games by producing adaptive action spaces. CoRR abs/1812.00855.
Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer networks. In Cortes, C.; Lawrence, N. D.; Lee, D. D.; Sugiyama, M.; and Garnett, R., eds., Advances in Neural Information Processing Systems 28. Curran Associates, Inc. 2692-2700.
Yin, X., and May, J. 2019a. Comprehensible context-driven text game playing. CoRR abs/1905.02265.
Yin, X., and May, J. 2019b. Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games. arXiv e-prints arXiv:1908.04777.</p>
<p>Yuan, X.; Côté, M.; Sordoni, A.; Laroche, R.; des Combes, R. T.; Hausknecht, M. J.; and Trischler, A. 2018a. Counting to explore and generalize in text-based games. CoRR abs/1806.11525.
Yuan, X.; Wang, T.; Meng, R.; Thaker, K.; He, D.; and Trischler, A. 2018b. Generating diverse numbers of diverse keyphrases. CoRR abs/1810.05241.
Zahavy, T.; Haroush, M.; Merlis, N.; Mankowitz, D. J.; and Mannor, S. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 31. Curran Associates, Inc. 3562-3573.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} 50 \%$ is equivalent to the 5th place in the competition.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
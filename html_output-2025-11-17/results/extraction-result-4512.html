<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4512 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4512</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4512</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-0710f4d42f29bae38bb413097f92a8ac6e04227f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0710f4d42f29bae38bb413097f92a8ac6e04227f" target="_blank">OpenResearcher: Unleashing AI for Accelerated Scientific Research</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers, built based on Retrieval-Augmented Generation to integrate Large Language Models with up-to-date, domain-specific knowledge.</p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas. We introduce OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers. OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to integrate Large Language Models (LLMs) with up-to-date, domain-specific knowledge. Moreover, we develop various tools for OpenResearcher to understand researchers’ queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and self-refine these answers. OpenResearcher can flexibly use these tools to balance efficiency and effectiveness. As a result, OpenResearcher enables researchers to save time and increase their potential to discover new insights and drive scientific breakthroughs. Demo, video, and code are available at: https://github.com/GAIR-NLP/OpenResearcher.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scientific discovery in the age of artificial intelligence <em>(Rating: 2)</em></li>
                <li>Chime: Llmassisted hierarchical organization of scientific studies for literature review support <em>(Rating: 2)</em></li>
                <li>Kiwi: A dataset of knowledge-intensive writing instructions for answering research questions <em>(Rating: 1)</em></li>
                <li>CocoSciSum: A scientific summarization toolkit with compositional controllability <em>(Rating: 1)</em></li>
                <li>Qasa: advanced question answering on scientific articles <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4512",
    "paper_id": "paper-0710f4d42f29bae38bb413097f92a8ac6e04227f",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scientific discovery in the age of artificial intelligence",
            "rating": 2
        },
        {
            "paper_title": "Chime: Llmassisted hierarchical organization of scientific studies for literature review support",
            "rating": 2
        },
        {
            "paper_title": "Kiwi: A dataset of knowledge-intensive writing instructions for answering research questions",
            "rating": 1
        },
        {
            "paper_title": "CocoSciSum: A scientific summarization toolkit with compositional controllability",
            "rating": 1
        },
        {
            "paper_title": "Qasa: advanced question answering on scientific articles",
            "rating": 1
        }
    ],
    "cost": 0.00550525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>OpenResearcher: Unleashing AI for Accelerated Scientific Research</h1>
<p>Yuxiang Zheng ${ }^{1,8 <em>}$ Shichao Sun ${ }^{4,8 </em>}$ Lin Qiu ${ }^{1}$ Dongyu Ru ${ }^{1}$ Cheng Jiayang ${ }^{5}$ Xuefeng Li ${ }^{1,8}$ Jifan Lin ${ }^{1,8}$ Binjie Wang ${ }^{3,8}$ Yun Luo ${ }^{6}$ Renjie Pan ${ }^{1}$ Yang Xu ${ }^{1}$ Qingkai Min ${ }^{6}$ Zizhao Zhang ${ }^{7}$ Yiwen Wang ${ }^{1}$ Wenjie Li ${ }^{4}$ Pengfei Liu ${ }^{1,2,8 \dagger}$<br>${ }^{1}$ Shanghai Jiao Tong University ${ }^{2}$ Shanghai Artificial Intelligence Laboratory ${ }^{3}$ Fudan University<br>${ }^{4}$ The Hong Kong Polytechnic University ${ }^{5}$ Hong Kong University of Science and Technology<br>${ }^{6}$ Westlake University ${ }^{7}$ Tsinghua University ${ }^{8}$ Generative AI Research Lab (GAIR)<br>catchiz.1@sjtu.edu.cn, pengfei@sjtu.edu.cn</p>
<h4>Abstract</h4>
<p>The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas. We introduce OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers. OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to integrate Large Language Models (LLMs) with up-to-date, domain-specific knowledge. Moreover, we develop various tools for OpenResearcher to understand researchers' queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and selfrefine these answers. OpenResearcher can flexibly use these tools to balance efficiency and effectiveness. As a result, OpenResearcher enables researchers to save time and increase their potential to discover new insights and drive scientific breakthroughs. Demo, video, and code are available at: https://github.com/ GAIR-NLP/OpenResearcher.</p>
<h2>1 Introduction</h2>
<p>Global scientific publications are growing annually by about $4 \%-5 \%$ (Pinedo et al., 2024), leading researchers to invest significant time and effort in thoroughly reviewing countless academic papers to find the knowledge that propels their research. This involves daily engagement with a wide range of literature to stay updated with the latest developments in their field, which is essential for maintaining the relevance and innovation of their work.</p>
<p>Recognizing the challenges and inefficiencies inherent in this process, considerable academic efforts have focused on AI-assisted scientific research</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(Wang et al., 2023a; Zhai, 2023). They aim to answer the researcher questions from both junior and senior researchers. These questions can be broadly classified into three categories: (1) Scientific Question Answering (Pappas et al., 2020; Ruggeri et al., 2023; Lee et al., 2023; Pramanick et al., 2024), which seeks detailed information or clarification within a specific domain; (2) Scientific Text Summarization (Wang et al., 2022; Ding et al., 2023; Takeshita et al., 2024; Hsu et al., 2024; Zhang et al., 2024), aimed at condensing the latest findings and developments into comprehensive overviews; and (3) Scientific Paper Recommendation (Bai et al., 2019; Kreutz and Schenkel, 2022; Stergiopoulos et al., 2024; Pinedo et al., 2024), which involves suggesting relevant literature and studies based on the researcher's interests or current inquiries. However, academic applications typically focus on a single task, lacking a unified solution for all questions, allowing researchers to pose any inquiry freely.</p>
<p>Conversely, recent industry applications, like Perplexity AI, ${ }^{1}$ iAsk, ${ }^{2}$ You.com, ${ }^{3}$ phind, ${ }^{4}$ and SearchGPT, ${ }^{5}$ allow users to inquire about anything beyond specific tasks. They use RetrievalAugmented Generation (RAG) (Lewis et al., 2020) technique to offer an innovative integration of generative Large Language Model (LLM) with web search capability. The core idea behind them is to offer users not just any answer, but the most accurate and contextually relevant information available. However, the proprietary nature of industry applications has hindered their development and may impede academic research in this field.</p>
<p>Besides, both academic and industry applications serve as passive assistants, focusing solely on responding to user inquiries rather than engaging</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Main Workflow of OpenResearcher.
in active communication. To address these issues in academic and industry contexts, we developed OpenResearcher, an open-source project that harnesses AI to accelerate scientific research. Its main workflow is shown in Figure 1, and its main contributions are as follows:</p>
<ul>
<li>Unified Application OpenResearcher can address researchers' diverse questions, such as Scientific Text Summarization, Scientific Paper Recommendation, etc.</li>
<li>Open-Source OpenResearcher is an impressive open-source system to rival the performance of industry applications.</li>
<li>Active Assistant OpenResearcher can connect in the mind or imagination to pose heuristic questions, guiding users to clarify queries for capturing their intent.</li>
<li>Retrieval Augmented OpenResearcher can retrieve from the Internet and arXiv corpus to provide up-to-date, domain-specific, verified knowledge as supporting evidence.</li>
<li>Fexible Tool Usage OpenResearcher can flexibly utilize bespoke tools to build a workflow for a better answer. For example, OpenResearcher adaptively calls a refinement tool
to refine its initial outcomes. This approach helps avoid the computational cost associated with the unnecessary use of some tools.</li>
<li>Conversational Interaction OpenResearcher enables users to engage in deep discussions through conversational follow-up questions.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Academic Works</h3>
<p>Academic works for scientific research target a specific task, including Scientific Question Answering, Scientific Text Summarization, and Scientific Paper Recommendation.
Scientific Question Answering generates answers for questions within extensive scientific articles. In the early days, cloze-style paper question answering datasets, such as emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018) and BioMRC (Pappas et al., 2020), are automatically created with the pre-defined question formats (Kwiatkowski et al., 2019). On the other hand, PubMedQA (Jin et al., 2019), BioAsq (Krallinger et al., 2020) and QASPER (Dasigi et al., 2021) involve human annotators in question creation. However, the questions are based only on abstracts. Recently, QASA (Lee et al., 2023) offers advanced questions with annotators reading the entire paper. KIWI (Xu et al.,</p>
<p>2024) uses expert and LLM interactions to refine initial answers into improved long-form answers. SPIQA (Pramanick et al., 2024) expands text question answering to multimodal question answering.
Scientific Text Summarization aims to condense the long scientific articles into a concise summary. Early works primarily focus on a knowledge graph-centric view (Wang et al., 2022). Recently, Ding et al. (2023) present CocoSciSum, a novel toolkit for controlled summarization of scientific documents, tailored to the scientific community's needs. Takeshita et al. (2024) introduce ACLSum, an expert-curated dataset for multi-aspect summarization of scientific papers, thoroughly covering challenges, approaches, and outcomes. Hsu et al. (2024) release CHIME, a dataset that hierarchically organizes scientific studies to facilitate the generation of literature reviews. Zhang et al. (2024) introduces MASSW, a comprehensive dataset for summarizing multi-aspects of scientific workflows.
Scientific Paper Recommendation assists researchers in discovering relevant and suitable scientific information through recommendations. Early approaches (Tanner et al., 2019; Ma and Wang, 2019; Sakib et al., 2020; Manju et al., 2020) in Big Scholarly Data (Khan et al., 2017) have evolved into recently proposed hybrid recommender systems. Pinedo et al. (2024) develop ArZiGo, a webbased prototype system for searching, managing, and recommending scientific articles. Stergiopoulos et al. (2024) present a novel multi-stage recommendation system employing clustering, graph modeling, and deep learning, capable of operating on a large-scale scientific digital library with millions of users and papers.</p>
<p>However, these academic efforts focus on a single function without a unified solution for diverse inquiries and lack a user-friendly web application.</p>
<h3>2.2 Industry Research Applications</h3>
<p>Recent advancements in LLMs have prompted the industry to explore AI assistants for scientific research, like Perplexity AI, iAsk, You.com, phind, and SearchGPT, designed to handle all kinds of research inquiries in a dialogue. These applications combine chatbot-driven search engines with LLMs, which is academically termed Retrieval Augmented Generation (RAG). These applications also provide citations for the evidence behind their responses. However, the closed-source nature has limited their development and academic research in this area.</p>
<h2>3 OpenResearcher</h2>
<p>OpenResearcher is designed to leverage AI to speed up the research process by efficiently responding to researchers' inquiries. As shown in Figure 1, OpenResearcher employs RAG to combine LLMs' internal knowledge with the latest external information. We design a Data Routing strategy for quick and precise information retrieval that can meet time and domain requirements. Lastly, we have developed multiple tools, including query tools, retrieval tools, post-processing tools, generation tools, and refinement tools. OpenResearcher can flexibly use these tools to customize a workflow for each query.</p>
<h3>3.1 Query Tools</h3>
<p>A key challenge in retrieval is its dependence on the user's initial query, which, if imprecise or vague, leads to ineffective results. Junior researchers may struggle to articulate their questions, and scientific terms used across different disciplines add to this complexity. To address this, we have developed tools to help define straightforward questions.
Active Query OpenResearcher enhances a query by adding extra content and context. It asks users to specify their interest area or discipline. It can ensure that generated answers are highly relevant by covering nuances not initially mentioned.
Query Rewriting The users' queries are always suboptimal for retrieval, especially in real-world scenarios. Besides, the queries are commonly entailed in complex conversational interactions. Therefore, OpenResearcher rewrites the queries for better clarity and effectiveness.
Query Decomposition OpenResearcher decomposes the complex query into a series of subqueries, improving precision and efficiency for more satisfying responses. Then each sub-query is processed by information retrieval and LLM generation systems accordingly to get the sub-answer.</p>
<h3>3.2 Retrieval Tools</h3>
<p>OpenResearcher uses advanced retrieval tools to gather comprehensive and accurate information from the Internet and arXiv corpus.
Internet Retrieval OpenResearcher conducts Internet Retrieval through search engines API to collect relevant online information.
Hybrid Retrieval OpenResearcher supports Hybrid Retrieval that employs sparse vector and dense vector representations of both queries and docu-</p>
<p>ments. By leveraging these compact vector embeddings, Hybrid Retrieval can more effectively capture semantic similarities and improve the relevance of retrieved documents.
BM25 Retrieval OpenResearcher conducts BM25
Retrieval, an advanced algorithm used by search engines to rank documents based on their relevance to a query, factoring in term frequency and document length. BM25 stands out for its effectiveness in handling various search queries, making it a widely adopted method in information retrieval.</p>
<h3>3.3 Data Routing Strategy</h3>
<p>We develop an advanced Data Routing strategy aimed at optimizing the performance of our hybrid retrieval tool. This retrieval tool currently requires substantial processing times to calculate the similarity between a query and all arXiv paper chunks, which can be resource-intensive.</p>
<p>To address this issue, our strategy is to stratify the data based on both temporal and domainspecific information found in the metadata of the arXiv papers. It distributes data across multiple specialized databases, each aligned with a particular time frame and domain. Consequently, the retrieval tool only scans databases relevant to the query, which speeds up the search process and improves result accuracy by concentrating on the applicable data sets.</p>
<h3>3.4 Post-Processing Tools</h3>
<p>We develop Post-Processing Tools to rerank, fuse, and filter retrieved information, removing noise and redundancy to provide the most pertinent outcomes for the generation of LLMs.
Reranking: OpenResearcher can use a reranking tool to reorder document chunks, prioritizing the most relevant results to condense the retrieval pool.
Fusion: OpenResearcher can use a fusion tool to fuse the retrieved content from the same source into a single paragraph to enhance the context.
Filtering: OpenResearcher can use a filtering tool to filter out redundant and noisy content to preserve the most relevant information.</p>
<h3>3.5 Generation Tools</h3>
<p>OpenResearcher uses advanced LLMs to produce responses using retrieved information.
Generation OpenResearcher prompts LLMs to utilize retrieved information to generate appropriate responses for user queries.</p>
<p>Citation OpenResearcher can use a citation tool that employs the BM25 matching algorithm to link retrieved information with the response sentences, providing citations for each.</p>
<h3>3.6 Refinement Tools</h3>
<p>OpenResearcher utilizes LLMs to reflect and polish the initial responses, guaranteeing their accuracy and completeness.
Reflection OpenResearcher prompts LLMs to evaluate the accuracy and completeness of generated responses, meanwhile highlighting grammatical and semantic flaws.
Polishing OpenResearcher instructs LLMs to polish responses according to feedback received.</p>
<h2>4 Demonstration</h2>
<p>Our web application is built with Streamlit. ${ }^{6}$ Our databases encompass arXiv publications from Jan. 2023 to Jun. 2024, enriched with metadata. This is because most LLMs are trained on pre-2023 data, enabling them to retain this information. This fact also inspires OpenResearcher to answer simple questions without any retrieval, only using LLMs' internal knowledge. We utilize the state-of-the-art GTE-large model (Li et al., 2023) as dense vector and efficient-splade-VI-BT-large (Lassance and Clinchant, 2022) as sparse vector to vectorize our queries and paper chunks. These vectors serve for Hybrid Retrieval, and we use Qdrant ${ }^{7}$ for the vector storage. This Hybrid Retrieval tool extracts the 30 most similar chunks from each selected database. Elasticsearch ${ }^{8}$ supports our implementation of BM25 retriever, which extracts up to 80 chunks. The Bing ${ }^{9}$ API finds 10 relevant outcomes for the Internet Retrieval tool. Besides, we utilize bge-reranker-v2-m3 ${ }^{10}$ to implement our Reranking tool. This Reranking tool reduces the number of retrieved chunks to 10. Lastly, we use DeepSeek-V2-Chat (DeepSeek-AI et al., 2024) as our backbone LLM to implement all LLM-powered tools, while also supporting various online LLM APIs and locally deployed LLMs through Ollama. ${ }^{11}$</p>
<p>Figure 2, whose completed screenshot is shown in Figure 3 of Section A, demonstrates the strong</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Case between user and OpenResearcher.
capability of OpenResearcher. Firstly, OpenResearcher can flexibly construct a tailored workflow for different queries, including simple queries and complex queries. For simple questions like "What is PPO?", it directly employs LLMs to produce answers. For more complex queries like "Summarize the recent latest developments and variants of PPO?", it utilizes multiple tools and provides users with essential details, including active queries, rewritten query, decomposed sub-queries and their sub-answers, retrieved outcomes of each sub-query after post-processing, generated final answer, and citation. This example can showcase its flexibility in handling different queries. With this benefit, our OpenResearcher can speed up responses and reduce computational costs.</p>
<p>Secondly, this figure also shows OpenResearcher can pose questions to users for query clarification. Different from previous passive applications that only answer questions, OpenResearcher utilizes LLMs' internal knowledge to help users specify their question details. This tool is very crucial for junior students who often struggle to clearly express their questions and confusion.</p>
<p>Thirdly, Figure 2 demonstrates that OpenRe-
searcher supports conversational question answering, enabling users to engage in multi-turn dialogues. This feature allows for continuous and deeper discussions within OpenResearcher.</p>
<p>Lastly, this figure shows our OpenResearcher can enhance the quality and reliability of generated content by retrieving supporting evidence from the Internet and arXiv corpus. Additionally, we have developed a citation tool that links the generated text to the retrieved information, making it easy for researchers to verify the sources and delve deeper by reading the original papers.</p>
<h2>5 Experiment</h2>
<h3>5.1 Evaluation Data</h3>
<p>We have collected 109 research questions from more than 20 graduate students, comprising 38 questions on scientific paper recommendation, 38 on scientific text summarization, and 33 on others. These questions arise in their daily scientific research across areas including multimodal, agent, LLM alignment, tool learning, LLM safety, RAG, and others. Answers to these questions are commonly complex and lengthy, requiring graduate students to review many papers. Due to the considerable effort and cost of annotating ground truth answers, we opt to conduct a pairwise comparison instead of providing annotated ground truths.</p>
<h3>5.2 Evaluation Applications</h3>
<p>Our baseline includes recent industry applications, containing Perplexity AI, iAsk, You.com, and Phind, complemented by a Naive RAG that only utilizes our hybrid retrieval and LLM generation tools. Regarding our OpenResearcher, we remove the Active Query tool to directly obtain the answer. Our OpenResearcher flexibly uses these tools to generate answers without the need to follow the main workflow sequentially.</p>
<h3>5.3 Evaluation Metric</h3>
<p>In all evaluations, we compared the candidate outcomes from Naive RAG, OpenResearcher, iAsk, You.com, and Phind with those from Perplexity AI. If the candidate outcome outperforms Perplexity AI, it is notated as a "Win".</p>
<p>We evaluate the generations from the three quality dimensions: (1) Information Correctness assesses the factual accuracy of the answers provided by the candidates. It is critical to determine if the information in each output is correct, as inaccuracies</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Correctness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Richness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Win</td>
<td style="text-align: center;">Tie</td>
<td style="text-align: center;">Lose</td>
<td style="text-align: center;">Win</td>
<td style="text-align: center;">Tie</td>
<td style="text-align: center;">Lose</td>
<td style="text-align: center;">Win</td>
<td style="text-align: center;">Tie</td>
<td style="text-align: center;">Lose</td>
</tr>
<tr>
<td style="text-align: center;">Ask</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">You.com</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">Phind</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">Naive RAG</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">OpenResearcher</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<p>Table 1: Human Preference compared with Perplexity AI outcome. "Win" means that the current method beats Perplexity AI. More "Win" times means a superior application.
can severely undermine the utility of a QA system. (2) Information Richness involves evaluating the depth and scope of the information provided in the answers. Information richness captures whether an answer provides a thorough explanation or context beyond just addressing the question directly. (3) Information Relevance judges whether the information presented in the outputs is directly relevant to the question asked. Even if an answer is rich in information and correct, it may not be useful if it does not directly address the query.</p>
<h3>5.4 Human Preference</h3>
<p>We engaged 12 students with good research experience to conduct the human evaluation. Given the complexity of research questions, we randomly selected 30 questions for human evaluation, ensuring equal coverage of scientific question answering, scientific text summarization, and scientific paper recommendation. For quality control, each instance is annotated by two annotators whose agreement is measured. A third annotator can be involved to resolve disagreements between the two annotators.</p>
<p>The result is shown in Table 1 with an overall agreement of $90.67 \%$. Our OpenResearcher achieves superior information correctness, relevance, and richness compared to all other applications. OpenResearcher significantly outperforms Perplexity AI with more "Win" than "Lose". Specifically, compared to Naive RAG, OpenResearcher demonstrates better performance in all metrics. This suggests that our various tools significantly enhance the quality of the answers.</p>
<h3>5.5 LLM Preference</h3>
<p>Inspired by the widespread use of GPT-4 series for pairwise comparison (Zheng et al., 2023; Wang et al., 2023b; Sun et al., 2024) and their different preferences compared to humans (Li et al., 2024), we also utilize GPT-4o for LLM preference evaluation. We evaluate based on two criteria: infor-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Richness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Win</td>
<td style="text-align: center;">Tie</td>
<td style="text-align: center;">Lose</td>
<td style="text-align: center;">Win</td>
<td style="text-align: center;">Tie</td>
<td style="text-align: center;">Lose</td>
</tr>
<tr>
<td style="text-align: center;">iAsk</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">You.com</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">93</td>
</tr>
<tr>
<td style="text-align: center;">Phind</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">55</td>
</tr>
<tr>
<td style="text-align: center;">Naive RAG</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">OpenResearcher</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">35</td>
</tr>
</tbody>
</table>
<p>Table 2: GPT-4o Preference Results compared with Perplexity AI outcome.
mation richness and relevance, since GPT-4o struggles to verify information accuracy without external knowledge. Despite the availability of citation papers, their length and quantity exceed LLMs' capacity to confirm factuality.</p>
<p>The results are shown in Table 2. This supplemental LLM evaluation further demonstrates our system's powerful performance. These results show our OpenResearcher achieves the best information relevance and richness among all applications. Furthermore, OpenResearcher surpasses Naive RAG in both metrics, demonstrating its superior performance due to our design.</p>
<h2>6 Conclusion</h2>
<p>We introduce OpenResearcher, an active AI assistant to accelerate the research process, catering to a broad spectrum of inquiries from researchers. OpenResearcher employs Retrieval-Augmented Generation (RAG) to enhance LLMs with the latest, verified, and domain-specific knowledge. It interacts with users to clarify their queries. Moreover, we have developed various tools for OpenResearcher to understand researchers' queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and refine these answers. OpenResearcher can use these tools flexibly to build a pipeline that delivers accurate and comprehensive answers, outperforming those from industry applications, as</p>
<p>judged by human and GPT-40.</p>
<h2>Ethical Considerations</h2>
<p>OpenResearcher integrates LLMs and search engines, known as retrieval-augmented generation (RAG), to accelerate scientific research. Despite being instructed to ground the generated responses in retrieved knowledge from scientific publications, LLMs may still generate hallucinations. Consequently, users are advised to verify crucial information derived from our LLM-based features.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank the anonymous reviewers for their suggestions and feedback on the work. This work was partially funded by the National Natural Science Foundation of China (62476168), Qingyuan Research Project.</p>
<h2>References</h2>
<p>Xiaomei Bai, Mengyang Wang, Ivan Lee, Zhuo Yang, Xiangjie Kong, and Feng Xia. 2019. Scientific paper recommendation: A survey. Ieee Access, 7:93249339 .</p>
<p>Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599-4610, Online. Association for Computational Linguistics.</p>
<p>DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruoji Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang,</p>
<p>Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. Preprint, arXiv:2405.04434.</p>
<p>Yixi Ding, Yanxia Qin, Qian Liu, and Min-Yen Kan. 2023. CocoSciSum: A scientific summarization toolkit with compositional controllability. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 518-526, Singapore. Association for Computational Linguistics.</p>
<p>Chao-Chun Hsu, Erin Bransom, Jenna Sparks, Bailey Kuehl, Chenhao Tan, David Wadden, Lucy Lu Wang, and Aakanksha Naik. 2024. Chime: Llmassisted hierarchical organization of scientific studies for literature review support. arXiv preprint arXiv:2407.16148.</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Samiya Khan, Xiufeng Liu, Kashish A Shakil, and Mansaf Alam. 2017. A survey on scholarly data: From big data perspective. Information Processing \&amp; Management, 53(4):923-944.</p>
<p>Martin Krallinger, Anastasia Krithara, Anastasios Nentidis, Georgios Paliouras, and Marta Villegas. 2020. Bioasq at clef2020: Large-scale biomedical semantic indexing and question answering. In Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020, Proceedings, Part II 42, pages 550-556. Springer.</p>
<p>Christin Katharina Kreutz and Ralf Schenkel. 2022. Scientific paper recommendation systems: a literature review of recent publications. Preprint, arXiv:2201.00682.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,</p>
<p>Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Carlos Lassance and Stéphane Clinchant. 2022. An efficiency study for splade models. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22, page 2220-2226, New York, NY, USA. Association for Computing Machinery.</p>
<p>Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. 2023. Qasa: advanced question answering on scientific articles. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc.</p>
<p>Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, and Pengfei Liu. 2024. Dissecting human and llm preferences. arXiv preprint arXiv:2402.11296.</p>
<p>Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281.</p>
<p>Xiao Ma and Ranran Wang. 2019. Personalized scientific paper recommendation based on heterogeneous graph representation. IEEE Access, 7:79887-79894.</p>
<p>G Manju, P Abhinaya, MR Hemalatha, GG Manju, et al. 2020. Cold start problem alleviation in a research paper recommendation system using the random walk approach on a heterogeneous user-paper graph. International Journal of Intelligent Information Technologies (IJIIT), 16(2):24-48.</p>
<p>Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrQA: A large corpus for question answering on electronic medical records. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2357-2368, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Dimitris Pappas, Ion Androutsopoulos, and Haris Papageorgiou. 2018. BioRead: A new dataset for biomedical reading comprehension. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Dimitris Pappas, Petros Stavropoulos, Ion Androutsopoulos, and Ryan McDonald. 2020. BioMRC: A dataset for biomedical machine reading comprehension. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, pages 140149, Online. Association for Computational Linguistics.</p>
<p>Iratxe Pinedo, Mikel Larrañaga, and Ana Arruarte. 2024. Arzigo: A recommendation system for scientific articles. Information Systems, 122:102367.</p>
<p>Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. 2024. Spiga: A dataset for multimodal question answering on scientific papers. arXiv preprint arXiv:2407.09413.</p>
<p>Federico Ruggeri, Mohsen Mesgar, and Iryna Gurevych. 2023. A dataset of argumentative dialogues on scientific papers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7684-7699, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Nazmus Sakib, Rodina Binti Ahmad, and Khalid Haruna. 2020. A collaborative approach toward scientific paper recommendation using citation context. IEEE Access, 8:51246-51255.</p>
<p>Vaios Stergiopoulos, Michael Vassilakopoulos, Eleni Tousidou, and Antonio Corral. 2024. An academic recommender system on large citation data based on clustering, graph modeling and deep learning. Knowledge and Information Systems, pages 1-34.</p>
<p>Shichao Sun, Ruifeng Yuan, Ziqiang Cao, Wenjie Li, and Pengfei Liu. 2024. Prompt chaining or stepwise prompt? refinement in text summarization. Preprint, arXiv:2406.00507.</p>
<p>Sotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024. ACLSum: A new dataset for aspect-based summarization of scientific publications. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6660-6675, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>William Tanner, Esra Akbas, and Mir Hasan. 2019. Paper recommendation based on citation relation. In 2019 IEEE international conference on big data (big data), pages 3053-3059. IEEE.</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. 2023a. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60.</p>
<p>Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, and Ting Wang. 2022. Multi-document scientific summarization from a knowledge graph-centric view. In Proceedings of</p>
<p>the 29th International Conference on Computational Linguistics, pages 6222-6233, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023b. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087.</p>
<p>Fangyuan Xu, Kyle Lo, Luca Soldaini, Bailey Kuehl, Eunsol Choi, and David Wadden. 2024. Kiwi: A dataset of knowledge-intensive writing instructions for answering research questions. arXiv preprint arXiv:2403.03866.</p>
<p>Xiaoming Zhai. 2023. Chatgpt for next generation science learning. XRDS, 29(3):42-46.</p>
<p>Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, and Qiaozhu Mei. 2024. Massw: A new dataset and benchmark tasks for ai-assisted scientific workflows. Preprint, arXiv:2406.06357.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.</p>
<h1>A Completed Case</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<h1>OpenResearcher</h1>
<h2>1. What is OpenResearch?</h2>
<p>This is a comprehensive and open- and open-source open-resource application designed to provide a comprehensive and open-source framework for open-resource applications. The application is designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>2. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The application is designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>3. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>4. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>5. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>6. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>7. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>8. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>9. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>10. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>11. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>12. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<h2>13. What is OpenResearch?</h2>
<p>OpenResearch is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications. The OpenResearcher is a comprehensive and open-source open-source application designed to provide a comprehensive and open-source framework for open-resource applications, ensuring the full姓名, address, and呼叫名 are all essential for open-resource applications.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://streamlit.io/
${ }^{7}$ https://qdrant.tech/
${ }^{8}$ https://github.com/elastic/elasticsearch
${ }^{9}$ https://www.bing.com/
${ }^{10}$ https://huggingface.co/BAAI/
bge-reranker-large
${ }^{11}$ https://ollama.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ https://www.perplexity.ai/
${ }^{2}$ https://iask.ai/
${ }^{3}$ https://you.com/
${ }^{4}$ https://www.phind.com/
${ }^{5}$ https://chatgpt.com/search&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1403 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1403</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1403</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-0c3b69b5247ef18fd5bab1109d87a04184ea8f4b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0c3b69b5247ef18fd5bab1109d87a04184ea8f4b" target="_blank">A Recurrent Latent Variable Model for Sequential Data</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is argued that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1403.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1403.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Recurrent Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent extension of the variational autoencoder that injects timestep-wise latent random variables into the RNN hidden state and conditions the prior on the previous RNN hidden state to capture temporal dependencies in high-dimensional sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Variational Recurrent Neural Network (VRNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space sequence model that places a VAE at each timestep. At time t a latent z_t is drawn from a conditional Gaussian prior p(z_t | h_{t-1}) whose parameters are produced by a neural network of the previous RNN hidden state; the decoder models p(x_t | z_{≤ t}, x_{<t}) (parameterized as Gaussian or GMM) and the RNN state h_t is updated from features of x_t, z_t and h_{t-1}. Inference uses an encoder q(z_t | x_{≤ t}, z_{<t}) implemented as a neural network and trained using the reparameterization trick to maximize a timestep-wise variational lower bound.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>High-dimensional sequential data (raw audio waveform modelling / handwriting generation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Average log-likelihood on test set (negative log-likelihood), variational lower bound (per-timestep ELBO), importance-sampled marginal log-likelihood (importance sampling with 40 samples), and KL divergence between approximate posterior and conditional prior (per timestep).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>VRNN-Gauss approx. test log-likelihoods reported: Blizzard ≈ 9516, TIMIT ≈ 30235, Onomatopoeia ≈ 21332, Accent ≈ 4223, IAM-OnDB ≈ 1354 (values are average log-likelihoods from Table 1; variational lower-bound values also reported and are lower bounds).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: latent trajectories and per-timestep KL divergence reveal when the model switches modalities or responds to waveform transitions; however the mappings are neural-network parameterized and largely black-box beyond these visual diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of latent-space transitions (delta_t between successive posterior means), plotting timestep-wise KL divergence between approximate posterior and conditional prior, and inspection of generated samples (waveforms/handwriting) to qualitatively assess style coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Large RNNs used (single recurrent layer with 2000 LSTM units; Blizzard experiments used 4000 units; larger generation models stacked 3 layers × 3000 LSTM units). Generative/inference nets (φ_τ) use multi-layer DNNs (four hidden layers with ReLU in most experiments). Trained with Adam (learning rates 0.0003–0.001), minibatches of 64–128. No explicit GPU-hours or wall-clock times reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>No explicit runtime or FLOP comparison provided; experiments matched total numbers of DNN parameters across VRNN and baseline RNN-Gauss/RNN-GMM to make parameter-count comparisons fair. Authors do not claim lower computational cost vs baselines; VRNN adds encoder/decoder/prior networks which increase per-step computation but models were sized to match parameter budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On speech modelling and handwriting tasks VRNN variants outperform RNN baselines in log-likelihood and qualitative sample quality: VRNN-Gauss yields higher test log-likelihoods (see fidelity_performance) and generates less noisy audio and more style-consistent handwriting than RNN-GMM and RNN-Gauss baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Latent temporal variables improve modelling of high signal-to-noise, highly structured sequences: they allow a unimodal Gaussian output function to produce high-quality samples (i.e., high fidelity) by placing variability in latent space, thus avoiding the compromise between encoding variability and producing a clean signal that affects deterministic-RNN-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off noted: deterministic RNNs must compromise between representing a clean signal and encoding variability; VRNN shifts variability into latent space improving sample quality but at cost of added network components and per-timestep stochastic sampling/inference. Interpretability gains (via latent visualizations) are limited and remain coarse. No measured increase in wall-clock training time reported.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: conditional Gaussian prior p(z_t|h_{t-1}) (temporal conditioning) vs independent prior (VRNN-I); use of LSTM transition f for h_t; feature extractors φ^x, φ^z, separate encoder φ^{enc}, decoder φ^{dec}, and prior network φ^{prior}; option to use Gaussian or GMM output likelihood; reparameterization trick for low-variance gradient estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to RNN-Gauss and RNN-GMM baselines and VRNN-I (independent prior) and qualitatively to STORN: VRNN (with conditional prior) yields higher log-likelihoods and cleaner generated audio than RNN-GMM, and outperforms VRNN-I, demonstrating benefit of temporally-conditioned latent prior. No explicit comparison to model-free approaches; STORN (independent prior) is noted as related and generally weaker in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend temporally conditioning the latent prior (i.e., using p(z_t | h_{t-1}) rather than independent priors) as it improves representational power. They also report that including feature extractors for x and z and matching parameter budgets across models are important design decisions. No single universally optimal hyperparameter set is prescribed; experimental settings (LSTM sizes, number of DNN layers, output likelihood type) are task-dependent as in the presented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Recurrent Latent Variable Model for Sequential Data', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1403.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1403.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VRNN-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Recurrent Neural Network (Independent-prior variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of the VRNN in which the latent prior at each timestep is independent across time (no conditioning of p(z_t) on h_{t-1}); analogous to STORN-like formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VRNN-I (VRNN with independent prior)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same VAE-at-each-timestep architecture as VRNN but with the prior p(z_t) independent across timesteps (no φ^{prior}(h_{t-1})). The encoder and decoder remain conditioned on h_{t-1} for inference/generation in the experiments where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (independent-timestep prior variant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Speech modelling and handwriting generation (same datasets as VRNN)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Average log-likelihood on test set, variational lower bound (timestep-wise ELBO), importance-sampled marginal likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>VRNN-I-Gauss approx. reported test log-likelihoods: Blizzard ≈ 9188, TIMIT ≈ 29639, Onomatopoeia ≈ 19638, Accent ≈ 4180, IAM-OnDB ≈ 1353 (lower than VRNN with conditional prior in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Similar to VRNN: latent variables can be visualized but temporal coherence of latent trajectories is reduced because the prior is independent, making temporal interpretation less informative.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Same diagnostics as VRNN (delta_t and per-timestep KL), but temporal KL peaks related to transitions are weaker/fewer compared to conditional-prior VRNN.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to VRNN in parameter count for matched experiments but slightly simpler per-timestep computation because prior network φ^{prior} is absent or simplified.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Slightly cheaper per step than VRNN (no conditional prior network), but authors show VRNN-I underperforms VRNN, so savings may not justify fidelity loss.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Per Table 1, VRNN-I performs worse than VRNN (conditional prior) in average log-likelihood across speech datasets and similarly on handwriting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Independent prior reduces the model's ability to capture temporal dependencies in latent factors, which degrades modelling fidelity and sample consistency (e.g., less coherent style preservation in handwriting generation).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Saves some model complexity by removing conditional prior but loses fidelity and temporal coherence; demonstrates trade-off between model simplicity and temporal representational power.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choice examined to study impact of temporal conditioning: independent Gaussian priors per timestep vs conditional prior tied to h_{t-1}.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to conditional-prior VRNN and found inferior; also related to STORN (which places independent latent priors), which the authors identify as an instance of the VRNN-I family.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper's experiments indicate conditional prior is preferred for the tasks considered; VRNN-I serves as an ablation showing why temporal conditioning is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Recurrent Latent Variable Model for Sequential Data', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1403.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1403.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN-Gauss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network with Gaussian output</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline deterministic RNN (LSTM) model that parameterizes the per-timestep output distribution as a single Gaussian whose parameters are predicted from the RNN hidden state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN with unimodal Gaussian output (RNN-Gauss)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic RNN (LSTM) transition h_t = f(x_t, h_{t-1}) with output function g(h_{t-1}) = p(x_t | x_{<t}) parameterized as a single Gaussian (mean and diagonal covariance predicted by a DNN acting on h_{t-1}). No stochastic latent state is present in the transition.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>deterministic RNN observation model (no latent world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Raw audio waveform modelling and handwriting</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Exact average log-likelihood on test set (negative log-likelihood) using the Gaussian observation model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported test log-likelihoods (exact): Blizzard 3539, TIMIT -1900, Onomatopoeia -984, Accent -1293, IAM-OnDB 1016 (substantially lower than VRNN variants).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural network; no latent variables to visualize. Interpretability limited to inspecting outputs and hidden-state activations if desired (not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported for interpretability beyond qualitative inspection of generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable recurrence cost to VRNN (LSTM of same size) but lacks encoder/decoder/prior networks, so per-step computation is lower than VRNN variants with equivalent DNN parameter budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally efficient per step than VRNN (fewer networks), but at cost of modelling capacity; requires more complex output density (e.g., GMM) or loses sample quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Poor performance on speech modelling with unimodal Gaussian output; generated samples are nearly noise (authors exclude RNN-Gauss samples for that reason). On handwriting it attains lower log-likelihood and less stylistic diversity than VRNN.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Unimodal Gaussian observation model with deterministic transitions cannot capture complex multimodal conditional distributions present in high-SNR sequence data; leads to poor sample quality despite lower compute.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity and lower computational cost come at large fidelity cost for highly structured sequences; requires stronger output models (GMM) or latent variables to improve.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choice to use single Gaussian output to show limitations of deterministic RNNs for highly structured sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Underperforms RNN-GMM and VRNN variants on both log-likelihood and qualitative sample quality; highlighted as a failure case motivating latent-variable approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests deterministic RNN with unimodal Gaussian is not optimal for the studied tasks and that adding latent variables or richer output densities is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Recurrent Latent Variable Model for Sequential Data', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1403.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1403.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN-GMM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network with Gaussian Mixture Model output</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline deterministic RNN whose per-timestep output distribution is parameterized as a Gaussian mixture model (GMM) to model multimodal conditional distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN with GMM output (RNN-GMM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic RNN (LSTM) where the output function returns parameters of a GMM (mixture weights, component means, diagonal covariances) given the hidden state, enabling multimodal per-timestep observation distributions without latent transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>deterministic RNN with structured output density</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Raw audio waveform modelling and handwriting</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Exact average log-likelihood on test set (negative log-likelihood) using a GMM output model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported test log-likelihoods (exact): Blizzard 7413, TIMIT 26643, Onomatopoeia 18865, Accent 3453, IAM-OnDB 1358 (better than RNN-Gauss but generally worse than VRNN variants; see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural parametrization of mixture parameters; no explicit latent state to analyze for high-level dynamics, but mixture components can be inspected to some extent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>No specific interpretability analyses reported beyond inspection of generated samples; qualitative observation highlights high-frequency noise in generated waveforms.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher decoder/output cost than RNN-Gauss due to predicting many mixture parameters (authors used 20 mixture components). Parameter-matched comparisons were performed to align total model capacity with VRNN variants.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally expensive at output layer than unimodal Gaussian but still cheaper than VRNN in terms of auxiliary encoder/prior networks; however, authors show it produces noisy samples despite higher model complexity in the output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>RNN-GMM improves over RNN-Gauss in log-likelihood and sample plausibility but produces noisy high-frequency artifacts in generated audio compared to VRNN outputs; handwriting samples tend to change style during generation (less consistent).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>GMM output provides richer per-step likelihood modelling but cannot fully encode long-range or example-specific variability, leading to noisy samples and style inconsistency; latent temporal variables (VRNN) better preserve sample-level attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Using richer output density (GMM) increases parametric and computational burden at the output but still can produce lower sample quality compared to adding structured latent dynamics; trade-off between output complexity and latent modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Used 20 mixture components for GMM outputs; decoder networks sized to match parameters of other models so comparisons focus on representational differences rather than pure capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms RNN-Gauss but is outperformed by VRNN variants in both quantitative log-likelihood and qualitative sample quality; VRNN achieves cleaner audio despite simpler unimodal output by encoding variability in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper demonstrates that GMM outputs help but that combining temporal latent structure (conditional prior) with simpler output densities (Gaussian) can be superior; thus, optimal setting for these tasks favors temporal latent modelling over only richer output densities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Recurrent Latent Variable Model for Sequential Data', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1403.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1403.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Recurrent Network (STORN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior recurrent latent-variable model that generates a sequence of independent latent variables per timestep and integrates them into RNN dynamics; trained under a VAE-like variational inference objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning stochastic recurrent networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>STORN (sequence of independent latent variables)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RNN-based sequence model that samples latent z_t independently per timestep (no temporal conditioning of the prior) and conditions the RNN transition on these sampled latents; trained with variational inference similar to VAE.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (independent-timestep prior)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General sequential modelling (music, speech, other sequences) as referenced in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Variational lower bound (ELBO) / log-likelihood (as reported in the STORN paper; referenced here conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not analyzed in this paper; STORN is referenced as related work and as analogous to the VRNN-I family (independent priors).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not applicable in this paper (STORN only mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported in this paper for STORN (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper positions STORN as related but claims VRNN's conditional prior improves representational power over STORN's independent prior; no quantitative runtime comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper; referenced as related approach and used to motivate the temporal conditional prior in VRNN.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Authors argue (and empirically show via VRNN-I comparisons) that independent priors limit temporal representational power compared to conditional priors; hence STORN-like designs may be less effective for highly structured sequences that require temporal latent dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Independence simplifies the prior (less compute for prior) but sacrifices temporal modelling capacity and fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Independent per-timestep prior choice (as in STORN) is contrasted with VRNN's temporally conditioned prior; authors include VRNN-I (STORN-like) as an ablation to demonstrate impact.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually and by ablation (VRNN-I) in experiments: VRNN with conditional prior outperforms the independent-prior variant in log-likelihood and sample quality.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends temporally-conditioned priors for the tasks studied; STORN represents the baseline independent-prior class which is shown to be suboptimal here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Recurrent Latent Variable Model for Sequential Data', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1403.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1403.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-variable generative model for non-sequential data that couples a neural-network decoder p(x|z) with a variational encoder q(z|x) and trains both by maximizing a variational lower bound using the reparameterization trick.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-encoding variational bayes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Variational Autoencoder (VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Directed latent-variable model p(x,z)=p(x|z)p(z) where p(z) is typically Gaussian and p(x|z) is parameterized by a neural network; inference uses a parametric approximate posterior q(z|x) (Gaussian whose parameters are outputs of an encoder network) and the reparameterization trick to compute gradients of the ELBO.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent generative model (non-sequential)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General generative modelling; used here as conceptual building block for per-timestep VAE in VRNN</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO (variational lower bound) and importance-weighted marginal likelihood estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>VAE latent spaces can be inspected and manipulated; the paper leverages VAE concepts to enable latent-space modelling in sequences but does not evaluate VAE interpretability directly.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Reparameterization trick enables low-variance gradient estimates; latent-space visualizations are the common method (not specifically applied to standalone VAE in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Standard VAE training costs (encoder+decoder networks); this paper uses VAE principles at each timestep which multiplies inference/generative networks per timestep compared to a single static VAE.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared in this paper; VRNN adapts VAE to sequences and adds temporal conditioning which increases cost relative to a standard VAE but is necessary for sequential tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not directly evaluated in this paper; VAE is used as conceptual and technical foundation for VRNN.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The authors use the VAE insight that moving variability into a high-level latent representation with expressive decoders improves modelling of multimodal conditional distributions, motivating the VRNN design.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Applying VAE per timestep requires approximate inference and additional networks, increasing computational cost, but yields higher representational power for multimodal outputs when extended to sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Reparameterization trick for gradient estimation, Gaussian latent priors and Gaussian approximate posteriors parameterized by neural networks are adopted from standard VAE practice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>VAE-based latent modelling is contrasted with deterministic RNN-only approaches and explicit structured output densities (GMM); embedding VAE per timestep (with temporal conditioning) is shown to be beneficial relative to RNN-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper uses VAE architectural choices (Gaussian q and p with neural nets, reparameterization) as components of VRNN and demonstrates benefit when combined with conditional temporal priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Recurrent Latent Variable Model for Sequential Data', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning stochastic recurrent networks <em>(Rating: 2)</em></li>
                <li>Auto-encoding variational bayes <em>(Rating: 2)</em></li>
                <li>Variational recurrent auto-encoders <em>(Rating: 2)</em></li>
                <li>Stochastic backpropagation and approximate inference in deep generative models <em>(Rating: 2)</em></li>
                <li>Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription <em>(Rating: 1)</em></li>
                <li>Generating sequences with recurrent neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1403",
    "paper_id": "paper-0c3b69b5247ef18fd5bab1109d87a04184ea8f4b",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "VRNN",
            "name_full": "Variational Recurrent Neural Network",
            "brief_description": "A recurrent extension of the variational autoencoder that injects timestep-wise latent random variables into the RNN hidden state and conditions the prior on the previous RNN hidden state to capture temporal dependencies in high-dimensional sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Variational Recurrent Neural Network (VRNN)",
            "model_description": "Latent-space sequence model that places a VAE at each timestep. At time t a latent z_t is drawn from a conditional Gaussian prior p(z_t | h_{t-1}) whose parameters are produced by a neural network of the previous RNN hidden state; the decoder models p(x_t | z_{≤ t}, x_{&lt;t}) (parameterized as Gaussian or GMM) and the RNN state h_t is updated from features of x_t, z_t and h_{t-1}. Inference uses an encoder q(z_t | x_{≤ t}, z_{&lt;t}) implemented as a neural network and trained using the reparameterization trick to maximize a timestep-wise variational lower bound.",
            "model_type": "latent world model",
            "task_domain": "High-dimensional sequential data (raw audio waveform modelling / handwriting generation)",
            "fidelity_metric": "Average log-likelihood on test set (negative log-likelihood), variational lower bound (per-timestep ELBO), importance-sampled marginal log-likelihood (importance sampling with 40 samples), and KL divergence between approximate posterior and conditional prior (per timestep).",
            "fidelity_performance": "VRNN-Gauss approx. test log-likelihoods reported: Blizzard ≈ 9516, TIMIT ≈ 30235, Onomatopoeia ≈ 21332, Accent ≈ 4223, IAM-OnDB ≈ 1354 (values are average log-likelihoods from Table 1; variational lower-bound values also reported and are lower bounds).",
            "interpretability_assessment": "Partially interpretable: latent trajectories and per-timestep KL divergence reveal when the model switches modalities or responds to waveform transitions; however the mappings are neural-network parameterized and largely black-box beyond these visual diagnostics.",
            "interpretability_method": "Visualization of latent-space transitions (delta_t between successive posterior means), plotting timestep-wise KL divergence between approximate posterior and conditional prior, and inspection of generated samples (waveforms/handwriting) to qualitatively assess style coherence.",
            "computational_cost": "Large RNNs used (single recurrent layer with 2000 LSTM units; Blizzard experiments used 4000 units; larger generation models stacked 3 layers × 3000 LSTM units). Generative/inference nets (φ_τ) use multi-layer DNNs (four hidden layers with ReLU in most experiments). Trained with Adam (learning rates 0.0003–0.001), minibatches of 64–128. No explicit GPU-hours or wall-clock times reported.",
            "efficiency_comparison": "No explicit runtime or FLOP comparison provided; experiments matched total numbers of DNN parameters across VRNN and baseline RNN-Gauss/RNN-GMM to make parameter-count comparisons fair. Authors do not claim lower computational cost vs baselines; VRNN adds encoder/decoder/prior networks which increase per-step computation but models were sized to match parameter budgets.",
            "task_performance": "On speech modelling and handwriting tasks VRNN variants outperform RNN baselines in log-likelihood and qualitative sample quality: VRNN-Gauss yields higher test log-likelihoods (see fidelity_performance) and generates less noisy audio and more style-consistent handwriting than RNN-GMM and RNN-Gauss baselines.",
            "task_utility_analysis": "Latent temporal variables improve modelling of high signal-to-noise, highly structured sequences: they allow a unimodal Gaussian output function to produce high-quality samples (i.e., high fidelity) by placing variability in latent space, thus avoiding the compromise between encoding variability and producing a clean signal that affects deterministic-RNN-only approaches.",
            "tradeoffs_observed": "Trade-off noted: deterministic RNNs must compromise between representing a clean signal and encoding variability; VRNN shifts variability into latent space improving sample quality but at cost of added network components and per-timestep stochastic sampling/inference. Interpretability gains (via latent visualizations) are limited and remain coarse. No measured increase in wall-clock training time reported.",
            "design_choices": "Key choices: conditional Gaussian prior p(z_t|h_{t-1}) (temporal conditioning) vs independent prior (VRNN-I); use of LSTM transition f for h_t; feature extractors φ^x, φ^z, separate encoder φ^{enc}, decoder φ^{dec}, and prior network φ^{prior}; option to use Gaussian or GMM output likelihood; reparameterization trick for low-variance gradient estimates.",
            "comparison_to_alternatives": "Compared to RNN-Gauss and RNN-GMM baselines and VRNN-I (independent prior) and qualitatively to STORN: VRNN (with conditional prior) yields higher log-likelihoods and cleaner generated audio than RNN-GMM, and outperforms VRNN-I, demonstrating benefit of temporally-conditioned latent prior. No explicit comparison to model-free approaches; STORN (independent prior) is noted as related and generally weaker in experiments here.",
            "optimal_configuration": "Authors recommend temporally conditioning the latent prior (i.e., using p(z_t | h_{t-1}) rather than independent priors) as it improves representational power. They also report that including feature extractors for x and z and matching parameter budgets across models are important design decisions. No single universally optimal hyperparameter set is prescribed; experimental settings (LSTM sizes, number of DNN layers, output likelihood type) are task-dependent as in the presented experiments.",
            "uuid": "e1403.0",
            "source_info": {
                "paper_title": "A Recurrent Latent Variable Model for Sequential Data",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "VRNN-I",
            "name_full": "Variational Recurrent Neural Network (Independent-prior variant)",
            "brief_description": "Variant of the VRNN in which the latent prior at each timestep is independent across time (no conditioning of p(z_t) on h_{t-1}); analogous to STORN-like formulations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VRNN-I (VRNN with independent prior)",
            "model_description": "Same VAE-at-each-timestep architecture as VRNN but with the prior p(z_t) independent across timesteps (no φ^{prior}(h_{t-1})). The encoder and decoder remain conditioned on h_{t-1} for inference/generation in the experiments where applicable.",
            "model_type": "latent world model (independent-timestep prior variant)",
            "task_domain": "Speech modelling and handwriting generation (same datasets as VRNN)",
            "fidelity_metric": "Average log-likelihood on test set, variational lower bound (timestep-wise ELBO), importance-sampled marginal likelihood.",
            "fidelity_performance": "VRNN-I-Gauss approx. reported test log-likelihoods: Blizzard ≈ 9188, TIMIT ≈ 29639, Onomatopoeia ≈ 19638, Accent ≈ 4180, IAM-OnDB ≈ 1353 (lower than VRNN with conditional prior in experiments).",
            "interpretability_assessment": "Similar to VRNN: latent variables can be visualized but temporal coherence of latent trajectories is reduced because the prior is independent, making temporal interpretation less informative.",
            "interpretability_method": "Same diagnostics as VRNN (delta_t and per-timestep KL), but temporal KL peaks related to transitions are weaker/fewer compared to conditional-prior VRNN.",
            "computational_cost": "Comparable to VRNN in parameter count for matched experiments but slightly simpler per-timestep computation because prior network φ^{prior} is absent or simplified.",
            "efficiency_comparison": "Slightly cheaper per step than VRNN (no conditional prior network), but authors show VRNN-I underperforms VRNN, so savings may not justify fidelity loss.",
            "task_performance": "Per Table 1, VRNN-I performs worse than VRNN (conditional prior) in average log-likelihood across speech datasets and similarly on handwriting.",
            "task_utility_analysis": "Independent prior reduces the model's ability to capture temporal dependencies in latent factors, which degrades modelling fidelity and sample consistency (e.g., less coherent style preservation in handwriting generation).",
            "tradeoffs_observed": "Saves some model complexity by removing conditional prior but loses fidelity and temporal coherence; demonstrates trade-off between model simplicity and temporal representational power.",
            "design_choices": "Choice examined to study impact of temporal conditioning: independent Gaussian priors per timestep vs conditional prior tied to h_{t-1}.",
            "comparison_to_alternatives": "Compared directly to conditional-prior VRNN and found inferior; also related to STORN (which places independent latent priors), which the authors identify as an instance of the VRNN-I family.",
            "optimal_configuration": "Paper's experiments indicate conditional prior is preferred for the tasks considered; VRNN-I serves as an ablation showing why temporal conditioning is beneficial.",
            "uuid": "e1403.1",
            "source_info": {
                "paper_title": "A Recurrent Latent Variable Model for Sequential Data",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "RNN-Gauss",
            "name_full": "Recurrent Neural Network with Gaussian output",
            "brief_description": "Baseline deterministic RNN (LSTM) model that parameterizes the per-timestep output distribution as a single Gaussian whose parameters are predicted from the RNN hidden state.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RNN with unimodal Gaussian output (RNN-Gauss)",
            "model_description": "Deterministic RNN (LSTM) transition h_t = f(x_t, h_{t-1}) with output function g(h_{t-1}) = p(x_t | x_{&lt;t}) parameterized as a single Gaussian (mean and diagonal covariance predicted by a DNN acting on h_{t-1}). No stochastic latent state is present in the transition.",
            "model_type": "deterministic RNN observation model (no latent world model)",
            "task_domain": "Raw audio waveform modelling and handwriting",
            "fidelity_metric": "Exact average log-likelihood on test set (negative log-likelihood) using the Gaussian observation model.",
            "fidelity_performance": "Reported test log-likelihoods (exact): Blizzard 3539, TIMIT -1900, Onomatopoeia -984, Accent -1293, IAM-OnDB 1016 (substantially lower than VRNN variants).",
            "interpretability_assessment": "Black-box neural network; no latent variables to visualize. Interpretability limited to inspecting outputs and hidden-state activations if desired (not reported).",
            "interpretability_method": "None reported for interpretability beyond qualitative inspection of generated samples.",
            "computational_cost": "Comparable recurrence cost to VRNN (LSTM of same size) but lacks encoder/decoder/prior networks, so per-step computation is lower than VRNN variants with equivalent DNN parameter budgets.",
            "efficiency_comparison": "More computationally efficient per step than VRNN (fewer networks), but at cost of modelling capacity; requires more complex output density (e.g., GMM) or loses sample quality.",
            "task_performance": "Poor performance on speech modelling with unimodal Gaussian output; generated samples are nearly noise (authors exclude RNN-Gauss samples for that reason). On handwriting it attains lower log-likelihood and less stylistic diversity than VRNN.",
            "task_utility_analysis": "Unimodal Gaussian observation model with deterministic transitions cannot capture complex multimodal conditional distributions present in high-SNR sequence data; leads to poor sample quality despite lower compute.",
            "tradeoffs_observed": "Simplicity and lower computational cost come at large fidelity cost for highly structured sequences; requires stronger output models (GMM) or latent variables to improve.",
            "design_choices": "Choice to use single Gaussian output to show limitations of deterministic RNNs for highly structured sequences.",
            "comparison_to_alternatives": "Underperforms RNN-GMM and VRNN variants on both log-likelihood and qualitative sample quality; highlighted as a failure case motivating latent-variable approaches.",
            "optimal_configuration": "Paper suggests deterministic RNN with unimodal Gaussian is not optimal for the studied tasks and that adding latent variables or richer output densities is necessary.",
            "uuid": "e1403.2",
            "source_info": {
                "paper_title": "A Recurrent Latent Variable Model for Sequential Data",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "RNN-GMM",
            "name_full": "Recurrent Neural Network with Gaussian Mixture Model output",
            "brief_description": "Baseline deterministic RNN whose per-timestep output distribution is parameterized as a Gaussian mixture model (GMM) to model multimodal conditional distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RNN with GMM output (RNN-GMM)",
            "model_description": "Deterministic RNN (LSTM) where the output function returns parameters of a GMM (mixture weights, component means, diagonal covariances) given the hidden state, enabling multimodal per-timestep observation distributions without latent transitions.",
            "model_type": "deterministic RNN with structured output density",
            "task_domain": "Raw audio waveform modelling and handwriting",
            "fidelity_metric": "Exact average log-likelihood on test set (negative log-likelihood) using a GMM output model.",
            "fidelity_performance": "Reported test log-likelihoods (exact): Blizzard 7413, TIMIT 26643, Onomatopoeia 18865, Accent 3453, IAM-OnDB 1358 (better than RNN-Gauss but generally worse than VRNN variants; see Table 1).",
            "interpretability_assessment": "Black-box neural parametrization of mixture parameters; no explicit latent state to analyze for high-level dynamics, but mixture components can be inspected to some extent.",
            "interpretability_method": "No specific interpretability analyses reported beyond inspection of generated samples; qualitative observation highlights high-frequency noise in generated waveforms.",
            "computational_cost": "Higher decoder/output cost than RNN-Gauss due to predicting many mixture parameters (authors used 20 mixture components). Parameter-matched comparisons were performed to align total model capacity with VRNN variants.",
            "efficiency_comparison": "More computationally expensive at output layer than unimodal Gaussian but still cheaper than VRNN in terms of auxiliary encoder/prior networks; however, authors show it produces noisy samples despite higher model complexity in the output.",
            "task_performance": "RNN-GMM improves over RNN-Gauss in log-likelihood and sample plausibility but produces noisy high-frequency artifacts in generated audio compared to VRNN outputs; handwriting samples tend to change style during generation (less consistent).",
            "task_utility_analysis": "GMM output provides richer per-step likelihood modelling but cannot fully encode long-range or example-specific variability, leading to noisy samples and style inconsistency; latent temporal variables (VRNN) better preserve sample-level attributes.",
            "tradeoffs_observed": "Using richer output density (GMM) increases parametric and computational burden at the output but still can produce lower sample quality compared to adding structured latent dynamics; trade-off between output complexity and latent modelling.",
            "design_choices": "Used 20 mixture components for GMM outputs; decoder networks sized to match parameters of other models so comparisons focus on representational differences rather than pure capacity.",
            "comparison_to_alternatives": "Outperforms RNN-Gauss but is outperformed by VRNN variants in both quantitative log-likelihood and qualitative sample quality; VRNN achieves cleaner audio despite simpler unimodal output by encoding variability in latent space.",
            "optimal_configuration": "Paper demonstrates that GMM outputs help but that combining temporal latent structure (conditional prior) with simpler output densities (Gaussian) can be superior; thus, optimal setting for these tasks favors temporal latent modelling over only richer output densities.",
            "uuid": "e1403.3",
            "source_info": {
                "paper_title": "A Recurrent Latent Variable Model for Sequential Data",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "STORN",
            "name_full": "Stochastic Recurrent Network (STORN)",
            "brief_description": "A prior recurrent latent-variable model that generates a sequence of independent latent variables per timestep and integrates them into RNN dynamics; trained under a VAE-like variational inference objective.",
            "citation_title": "Learning stochastic recurrent networks",
            "mention_or_use": "mention",
            "model_name": "STORN (sequence of independent latent variables)",
            "model_description": "RNN-based sequence model that samples latent z_t independently per timestep (no temporal conditioning of the prior) and conditions the RNN transition on these sampled latents; trained with variational inference similar to VAE.",
            "model_type": "latent world model (independent-timestep prior)",
            "task_domain": "General sequential modelling (music, speech, other sequences) as referenced in the paper",
            "fidelity_metric": "Variational lower bound (ELBO) / log-likelihood (as reported in the STORN paper; referenced here conceptually).",
            "fidelity_performance": null,
            "interpretability_assessment": "Not analyzed in this paper; STORN is referenced as related work and as analogous to the VRNN-I family (independent priors).",
            "interpretability_method": "Not applicable in this paper (STORN only mentioned).",
            "computational_cost": "Not reported in this paper for STORN (only referenced).",
            "efficiency_comparison": "Paper positions STORN as related but claims VRNN's conditional prior improves representational power over STORN's independent prior; no quantitative runtime comparison provided.",
            "task_performance": "Not evaluated in this paper; referenced as related approach and used to motivate the temporal conditional prior in VRNN.",
            "task_utility_analysis": "Authors argue (and empirically show via VRNN-I comparisons) that independent priors limit temporal representational power compared to conditional priors; hence STORN-like designs may be less effective for highly structured sequences that require temporal latent dependencies.",
            "tradeoffs_observed": "Independence simplifies the prior (less compute for prior) but sacrifices temporal modelling capacity and fidelity.",
            "design_choices": "Independent per-timestep prior choice (as in STORN) is contrasted with VRNN's temporally conditioned prior; authors include VRNN-I (STORN-like) as an ablation to demonstrate impact.",
            "comparison_to_alternatives": "Compared conceptually and by ablation (VRNN-I) in experiments: VRNN with conditional prior outperforms the independent-prior variant in log-likelihood and sample quality.",
            "optimal_configuration": "Paper recommends temporally-conditioned priors for the tasks studied; STORN represents the baseline independent-prior class which is shown to be suboptimal here.",
            "uuid": "e1403.4",
            "source_info": {
                "paper_title": "A Recurrent Latent Variable Model for Sequential Data",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "VAE",
            "name_full": "Variational Autoencoder",
            "brief_description": "A latent-variable generative model for non-sequential data that couples a neural-network decoder p(x|z) with a variational encoder q(z|x) and trains both by maximizing a variational lower bound using the reparameterization trick.",
            "citation_title": "Auto-encoding variational bayes",
            "mention_or_use": "mention",
            "model_name": "Variational Autoencoder (VAE)",
            "model_description": "Directed latent-variable model p(x,z)=p(x|z)p(z) where p(z) is typically Gaussian and p(x|z) is parameterized by a neural network; inference uses a parametric approximate posterior q(z|x) (Gaussian whose parameters are outputs of an encoder network) and the reparameterization trick to compute gradients of the ELBO.",
            "model_type": "latent generative model (non-sequential)",
            "task_domain": "General generative modelling; used here as conceptual building block for per-timestep VAE in VRNN",
            "fidelity_metric": "ELBO (variational lower bound) and importance-weighted marginal likelihood estimates.",
            "fidelity_performance": null,
            "interpretability_assessment": "VAE latent spaces can be inspected and manipulated; the paper leverages VAE concepts to enable latent-space modelling in sequences but does not evaluate VAE interpretability directly.",
            "interpretability_method": "Reparameterization trick enables low-variance gradient estimates; latent-space visualizations are the common method (not specifically applied to standalone VAE in this paper).",
            "computational_cost": "Standard VAE training costs (encoder+decoder networks); this paper uses VAE principles at each timestep which multiplies inference/generative networks per timestep compared to a single static VAE.",
            "efficiency_comparison": "Not directly compared in this paper; VRNN adapts VAE to sequences and adds temporal conditioning which increases cost relative to a standard VAE but is necessary for sequential tasks.",
            "task_performance": "Not directly evaluated in this paper; VAE is used as conceptual and technical foundation for VRNN.",
            "task_utility_analysis": "The authors use the VAE insight that moving variability into a high-level latent representation with expressive decoders improves modelling of multimodal conditional distributions, motivating the VRNN design.",
            "tradeoffs_observed": "Applying VAE per timestep requires approximate inference and additional networks, increasing computational cost, but yields higher representational power for multimodal outputs when extended to sequences.",
            "design_choices": "Reparameterization trick for gradient estimation, Gaussian latent priors and Gaussian approximate posteriors parameterized by neural networks are adopted from standard VAE practice.",
            "comparison_to_alternatives": "VAE-based latent modelling is contrasted with deterministic RNN-only approaches and explicit structured output densities (GMM); embedding VAE per timestep (with temporal conditioning) is shown to be beneficial relative to RNN-only baselines.",
            "optimal_configuration": "Paper uses VAE architectural choices (Gaussian q and p with neural nets, reparameterization) as components of VRNN and demonstrates benefit when combined with conditional temporal priors.",
            "uuid": "e1403.5",
            "source_info": {
                "paper_title": "A Recurrent Latent Variable Model for Sequential Data",
                "publication_date_yy_mm": "2015-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning stochastic recurrent networks",
            "rating": 2
        },
        {
            "paper_title": "Auto-encoding variational bayes",
            "rating": 2
        },
        {
            "paper_title": "Variational recurrent auto-encoders",
            "rating": 2
        },
        {
            "paper_title": "Stochastic backpropagation and approximate inference in deep generative models",
            "rating": 2
        },
        {
            "paper_title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription",
            "rating": 1
        },
        {
            "paper_title": "Generating sequences with recurrent neural networks",
            "rating": 1
        }
    ],
    "cost": 0.01529925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Recurrent Latent Variable Model for Sequential Data</h1>
<p>Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio<em><br>Department of Computer Science and Operations Research<br>Université de Montréal<br></em>CIFAR Senior Fellow<br>{firstname.lastname}@umontreal.ca</p>
<h4>Abstract</h4>
<p>In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational $R N N(\mathrm{VRNN})^{1}$ can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.</p>
<h2>1 Introduction</h2>
<p>Learning generative models of sequences is a long-standing machine learning challenge and historically the domain of dynamic Bayesian networks (DBNs) such as hidden Markov models (HMMs) and Kalman filters. The dominance of DBN-based approaches has been recently overturned by a resurgence of interest in recurrent neural network (RNN) based approaches. An RNN is a special type of neural network that is able to handle both variable-length input and output. By training an RNN to predict the next output in a sequence, given all previous outputs, it can be used to model joint probability distribution over sequences.</p>
<p>Both RNNs and DBNs consist of two parts: (1) a transition function that determines the evolution of the internal hidden state, and (2) a mapping from the state to the output. There are, however, a few important differences between RNNs and DBNs.</p>
<p>DBNs have typically been limited either to relatively simple state transition structures (e.g., linear models in the case of the Kalman filter) or to relatively simple internal state structure (e.g., the HMM state space consists of a single set of mutually exclusive states). RNNs, on the other hand, typically possess both a richly distributed internal state representation and flexible non-linear transition functions. These differences give RNNs extra expressive power in comparison to DBNs. This expressive power and the ability to train via error backpropagation are the key reasons why RNNs have gained popularity as generative models for highly structured sequential data.</p>
<p>In this paper, we focus on another important difference between DBNs and RNNs. While the hidden state in DBNs is expressed in terms of random variables, the internal transition structure of the standard RNN is entirely deterministic. The only source of randomness or variability in the RNN is found in the conditional output probability model. We suggest that this can be an inappropriate way to model the kind of variability observed in highly structured data, such as natural speech, which is characterized by strong and complex dependencies among the output variables at different</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>timesteps. We argue, as have others [4, 2], that these complex dependencies cannot be modelled efficiently by the output probability models used in standard RNNs, which include either a simple unimodal distribution or a mixture of unimodal distributions.</p>
<p>We propose the use of high-level latent random variables to model the variability observed in the data. In the context of standard neural network models for non-sequential data, the variational autoencoder (VAE) [11, 17] offers an interesting combination of highly flexible non-linear mapping between the latent random state and the observed output and effective approximate inference. In this paper, we propose to extend the VAE into a recurrent framework for modelling high-dimensional sequences. The VAE can model complex multimodal distributions, which will help when the underlying true data distribution consists of multimodal conditional distributions. We call this model a variational RNN (VRNN).</p>
<p>A natural question to ask is: how do we encode observed variability via latent random variables? The answer to this question depends on the nature of the data itself. In this work, we are mainly interested in highly structured data that often arises in AI applications. By highly structured, we mean that the data is characterized by two properties. Firstly, there is a relatively high signal-tonoise ratio, meaning that the vast majority of the variability observed in the data is due to the signal itself and cannot reasonably be considered as noise. Secondly, there exists a complex relationship between the underlying factors of variation and the observed data. For example, in speech, the vocal qualities of the speaker have a strong but complicated influence on the audio waveform, affecting the waveform in a consistent manner across frames.</p>
<p>With these considerations in mind, we suggest that our model variability should induce temporal dependencies across timesteps. Thus, like DBN models such as HMMs and Kalman filters, we model the dependencies between the latent random variables across timesteps. While we are not the first to propose integrating random variables into the RNN hidden state [4, 2, 6, 8], we believe we are the first to integrate the dependencies between the latent random variables at neighboring timesteps.</p>
<p>We evaluate the proposed VRNN model against other RNN-based models - including a VRNN model without introducing temporal dependencies between the latent random variables - on two challenging sequential data types: natural speech and handwriting. We demonstrate that for the speech modelling tasks, the VRNN-based models significantly outperform the RNN-based models and the VRNN model that does not integrate temporal dependencies between latent random variables.</p>
<h1>2 Background</h1>
<h3>2.1 Sequence modelling with Recurrent Neural Networks</h3>
<p>An RNN can take as input a variable-length sequence $\mathbf{x}=\left(\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>}, \ldots, \mathbf{x<em t="t">{T}\right)$ by recursively processing each symbol while maintaining its internal hidden state $\mathbf{h}$. At each timestep $t$, the RNN reads the symbol $\mathbf{x}</em>$ by:} \in \mathbb{R}^{d}$ and updates its hidden state $\mathbf{h}_{t} \in \mathbb{R}^{p</p>
<p>$$
\mathbf{h}<em _theta="\theta">{t}=f</em>}\left(\mathbf{x<em t-1="t-1">{t}, \mathbf{h}</em>\right)
$$</p>
<p>where $f$ is a deterministic non-linear transition function, and $\theta$ is the parameter set of $f$. The transition function $f$ can be implemented with gated activation functions such as long short-term memory [LSTM, 9] or gated recurrent unit [GRU, 5]. RNNs model sequences by parameterizing a factorization of the joint sequence probability distribution as a product of conditional probabilities such that:</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>}, \ldots, \mathbf{x<em t="1">{T}\right) &amp; =\prod</em>}^{T} p\left(\mathbf{x<em _t="&lt;t">{t} \mid \mathbf{x}</em>\right) \
p\left(\mathbf{x}<em _t="&lt;t">{t} \mid \mathbf{x}</em>\right)
\end{aligned}
$$}\right) &amp; =g_{\tau}\left(\mathbf{h}_{t-1</p>
<p>where $g$ is a function that maps the RNN hidden state $\mathbf{h}_{t-1}$ to a probability distribution over possible outputs, and $\tau$ is the parameter set of $g$.</p>
<p>One of the main factors that determines the representational power of an RNN is the output function $g$ in Eq. (2). With a deterministic transition function $f$, the choice of $g$ effectively defines the family of joint probability distributions $p\left(\mathbf{x}<em T="T">{1}, \ldots, \mathbf{x}</em>\right)$ that can be expressed by the RNN.</p>
<p>We can express the output function $g$ in Eq. (2) as being composed of two parts. The first part $\varphi_{\tau}$ is a function that returns the parameter set $\phi_{t}$ given the hidden state $\mathbf{h}<em t="t">{t-1}$, i.e., $\phi</em>}=\varphi_{\tau}\left(\mathbf{h<em t="t">{t-1}\right)$, while the second part of $g$ returns the density of $\mathbf{x}</em>}$, i.e., $p_{\phi_{t}}\left(\mathbf{x<em _t="&lt;t">{t} \mid \mathbf{x}</em>\right)$.</p>
<p>When modelling high-dimensional and real-valued sequences, a reasonable choice of an observation model is a Gaussian mixture model (GMM) as used in [7]. For GMM, $\varphi_{\tau}$ returns a set of mixture coefficients $\alpha_{t}$, means $\boldsymbol{\mu}<em class="," t="t">{., t}$ and covariances $\Sigma</em>$ under the mixture distribution is:}$ of the corresponding mixture components. The probability of $\mathbf{x}_{t</p>
<p>$$
p_{\boldsymbol{\alpha}<em class="," t="t">{t}, \boldsymbol{\mu}</em>}, \Sigma_{., t}}\left(\mathbf{x<em _t="&lt;t">{t} \mid \mathbf{x}</em>}\right)=\sum_{j} \alpha_{j, t} \mathcal{N}\left(\mathbf{x<em j_="j," t="t">{t} ; \boldsymbol{\mu}</em>\right)
$$}, \Sigma_{j, t</p>
<p>With the notable exception of [7], there has been little work investigating the structured output density model for RNNs with real-valued sequences.</p>
<p>There is potentially a significant issue in the way the RNN models output variability. Given a deterministic transition function, the only source of variability is in the conditional output probability density. This can present problems when modelling sequences that are at once highly variable and highly structured (i.e., with a high signal-to-noise ratio). To effectively model these types of sequences, the RNN must be capable of mapping very small variations in $\mathbf{x}<em t="t">{t}$ (i.e., the only source of randomness) to potentially very large variations in the hidden state $\mathbf{h}</em>$. Limiting the capacity of the network, as must be done to guard against overfitting, will force a compromise between the generation of a clean signal and encoding sufficient input variability to capture the high-level variability both within a single observed sequence and across data examples.</p>
<p>The need for highly structured output functions in an RNN has been previously noted. Boulangerlewandowski et al. [4] extensively tested NADE and RBM-based output densities for modelling sequences of binary vector representations of music. Bayer and Osendorfer [2] introduced a sequence of independent latent variables corresponding to the states of the RNN. Their model, called STORN, first generates a sequence of samples $\mathbf{z}=\left(\mathbf{z}<em T="T">{1}, \ldots, \mathbf{z}</em>}\right)$ from the sequence of independent latent random variables. At each timestep, the transition function $f$ from Eq. (1) computes the next hidden state $\mathbf{h<em t-1="t-1">{t}$ based on the previous state $\mathbf{h}</em>}$, the previous output $\mathbf{x<em t="t">{t-1}$ and the sampled latent random variables $\mathbf{z}</em>$. They proposed to train this model based on the VAE principle (see Sec. 2.2). Similarly, Pachitariu and Sahani [16] earlier proposed both a sequence of independent latent random variables and a stochastic hidden state for the RNN.</p>
<p>These approaches are closely related to the approach proposed in this paper. However, there is a major difference in how the prior distribution over the latent random variable is modelled. Unlike the aforementioned approaches, our approach makes the prior distribution of the latent random variable at timestep $t$ dependent on all the preceding inputs via the RNN hidden state $\mathbf{h}_{t-1}$ (see Eq. (5)). The introduction of temporal structure into the prior distribution is expected to improve the representational power of the model, which we empirically observe in the experiments (See Table 1). However, it is important to note that any approach based on having stochastic latent state is orthogonal to having a structured output function, and that these two can be used together to form a single model.</p>
<h1>2.2 Variational Autoencoder</h1>
<p>For non-sequential data, VAEs [11, 17] have recently been shown to be an effective modelling paradigm to recover complex multimodal distributions over the data space. A VAE introduces a set of latent random variables $\mathbf{z}$, designed to capture the variations in the observed variables $\mathbf{x}$. As an example of a directed graphical model, the joint distribution is defined as:</p>
<p>$$
p(\mathbf{x}, \mathbf{z})=p(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z})
$$</p>
<p>The prior over the latent random variables, $p(\mathbf{z})$, is generally chosen to be a simple Gaussian distribution and the conditional $p(\mathbf{x} \mid \mathbf{z})$ is an arbitrary observation model whose parameters are computed by a parametric function of $\mathbf{z}$. Importantly, the VAE typically parameterizes $p(\mathbf{x} \mid \mathbf{z})$ with a highly flexible function approximator such as a neural network. While latent random variable models of the form given in Eq. (3) are not uncommon, endowing the conditional $p(\mathbf{x} \mid \mathbf{z})$ as a potentially highly non-linear mapping from $\mathbf{z}$ to $\mathbf{x}$ is a rather unique feature of the VAE.</p>
<p>However, introducing a highly non-linear mapping from $\mathbf{z}$ to $\mathbf{x}$ results in intractable inference of the posterior $p(\mathbf{z} \mid \mathbf{x})$. Instead, the VAE uses a variational approximation $q(\mathbf{z} \mid \mathbf{x})$ of the posterior that</p>
<p>enables the use of the lower bound:</p>
<p>$\log p(\mathbf{x}) \geq-\operatorname{KL}(q(\mathbf{z} \mid \mathbf{x}) | p(\mathbf{z}))+\mathbb{E}_{q(\mathbf{z} \mid \mathbf{x})}[\log p(\mathbf{x} \mid \mathbf{z})],$</p>
<p>where $\operatorname{KL}(Q | P)$ is Kullback-Leibler divergence between two distributions $Q$ and $P$.
In [11], the approximate posterior $q(\mathbf{z} \mid \mathbf{x})$ is a Gaussian $\mathcal{N}\left(\boldsymbol{\mu}, \operatorname{diag}\left(\boldsymbol{\sigma}^{2}\right)\right)$ whose mean $\boldsymbol{\mu}$ and variance $\boldsymbol{\sigma}^{2}$ are the output of a highly non-linear function of $\mathbf{x}$, once again typically a neural network.
The generative model $p(\mathbf{x} \mid \mathbf{z})$ and inference model $q(\mathbf{z} \mid \mathbf{x})$ are then trained jointly by maximizing the variational lower bound with respect to their parameters, where the integral with respect to $q(\mathbf{z} \mid \mathbf{x})$ is approximated stochastically. The gradient of this estimate can have a low variance estimate, by reparametrizing $\mathbf{z}=\boldsymbol{\mu}+\boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$ and rewriting:</p>
<p>$$
\mathbb{E}<em p_boldsymbol_epsilon="p(\boldsymbol{\epsilon">{q(\mathbf{z} \mid \mathbf{x})}[\log p(\mathbf{x} \mid \mathbf{z})]=\mathbb{E}</em>)]
$$})}[\log p(\mathbf{x} \mid \mathbf{z}=\boldsymbol{\mu}+\boldsymbol{\sigma} \odot \boldsymbol{\epsilon</p>
<p>where $\boldsymbol{\epsilon}$ is a vector of standard Gaussian variables. The inference model can then be trained through standard backpropagation technique for stochastic gradient descent.</p>
<h1>3 Variational Recurrent Neural Network</h1>
<p>In this section, we introduce a recurrent version of the VAE for the purpose of modelling sequences. Drawing inspiration from simpler dynamic Bayesian networks (DBNs) such as HMMs and Kalman filters, the proposed variational recurrent neural network (VRNN) explicitly models the dependencies between latent random variables across subsequent timesteps. However, unlike these simpler DBN models, the VRNN retains the flexibility to model highly non-linear dynamics.</p>
<p>Generation The VRNN contains a VAE at every timestep. However, these VAEs are conditioned on the state variable $\mathbf{h}_{t-1}$ of an RNN. This addition will help the VAE to take into account the temporal structure of the sequential data. Unlike a standard VAE, the prior on the latent random variable is no longer a standard Gaussian distribution, but follows the distribution:</p>
<p>$$
\mathbf{z}<em 0_="0," t="t">{t} \sim \mathcal{N}\left(\boldsymbol{\mu}</em>}, \operatorname{diag}\left(\boldsymbol{\sigma<em 0_="0," t="t">{0, t}^{2}\right)\right), \text { where }\left[\boldsymbol{\mu}</em>}, \boldsymbol{\sigma<em _tau="\tau">{0, t}\right]=\varphi</em>\right)
$$}^{\text {prior }}\left(\mathbf{h}_{t-1</p>
<p>where $\boldsymbol{\mu}<em 0_="0," t="t">{0, t}$ and $\boldsymbol{\sigma}</em>}$ denote the parameters of the conditional prior distribution. Moreover, the generating distribution will not only be conditioned on $\mathbf{z<em t-1="t-1">{t}$ but also on $\mathbf{h}</em>$ such that:</p>
<p>$$
\mathbf{x}<em t="t">{t} \mid \mathbf{z}</em>} \sim \mathcal{N}\left(\boldsymbol{\mu<em t="t" x_="x,">{x, t}, \operatorname{diag}\left(\boldsymbol{\sigma}</em>}^{2}\right)\right), \text { where }\left[\boldsymbol{\mu<em t="t" x_="x,">{x, t}, \boldsymbol{\sigma}</em>}\right]=\varphi_{\tau}^{\operatorname{dec}}\left(\varphi_{\tau}^{\mathbf{z}}\left(\mathbf{z<em t-1="t-1">{t}\right), \mathbf{h}</em>\right)
$$</p>
<p>where $\boldsymbol{\mu}<em t="t" x_="x,">{x, t}$ and $\boldsymbol{\sigma}</em>}$ denote the parameters of the generating distribution, $\varphi_{\tau}^{\text {prior }}$ and $\varphi_{\tau}^{\text {dec }}$ can be any highly flexible function such as neural networks. $\varphi_{\tau}^{\mathbf{z}}$ and $\varphi_{\tau}^{\mathbf{z}}$ can also be neural networks, which extract features from $\mathbf{x<em t="t">{t}$ and $\mathbf{z}</em>$, respectively. We found that these feature extractors are crucial for learning complex sequences. The RNN updates its hidden state using the recurrence equation:</p>
<p>$$
\mathbf{h}<em _theta="\theta">{t}=f</em>}\left(\varphi_{\tau}^{\mathbf{z}}\left(\mathbf{x<em _tau="\tau">{t}\right), \varphi</em>}^{\mathbf{z}}\left(\mathbf{z<em t-1="t-1">{t}\right), \mathbf{h}</em>\right)
$$</p>
<p>where $f$ was originally the transition function from Eq. (1). From Eq. (7), we find that $\mathbf{h}<em _leq="\leq" t="t">{t}$ is a function of $\mathbf{x}</em>}$ and $\mathbf{z<em t="t">{\leq t}$. Therefore, Eq. (5) and Eq. (6) define the distributions $p\left(\mathbf{z}</em>} \mid \mathbf{x<em _t="&lt;t">{&lt;t}, \mathbf{z}</em>}\right)$ and $p\left(\mathbf{x<em _leq="\leq" t="t">{t} \mid \mathbf{z}</em>\right)$, respectively. The parameterization of the generative model results in and - was motivated by - the factorization:}, \mathbf{x}_{&lt;t</p>
<p>$$
p\left(\mathbf{x}<em T="T" _leq="\leq">{\leq T}, \mathbf{z}</em>}\right)=\prod_{t=1}^{T} p\left(\mathbf{x<em _leq="\leq" t="t">{t} \mid \mathbf{z}</em>}, \mathbf{x<em t="t">{&lt;t}\right) p\left(\mathbf{z}</em>} \mid \mathbf{x<em _t="&lt;t">{&lt;t}, \mathbf{z}</em>\right)
$$</p>
<p>Inference In a similar fashion, the approximate posterior will not only be a function of $\mathbf{x}<em t-1="t-1">{t}$ but also of $\mathbf{h}</em>$ following the equation:</p>
<p>$$
\mathbf{z}<em t="t">{t} \mid \mathbf{x}</em>} \sim \mathcal{N}\left(\boldsymbol{\mu<em t="t" z_="z,">{z, t}, \operatorname{diag}\left(\boldsymbol{\sigma}</em>}^{2}\right)\right), \text { where }\left[\boldsymbol{\mu<em t="t" z_="z,">{z, t}, \boldsymbol{\sigma}</em>}\right]=\varphi_{\tau}^{\text {enc }}\left(\varphi_{\tau}^{\mathbf{z}}\left(\mathbf{x<em t-1="t-1">{t}\right), \mathbf{h}</em>\right)
$$</p>
<p>similarly $\boldsymbol{\mu}<em t="t" z_="z,">{z, t}$ and $\boldsymbol{\sigma}</em>}$ denote the parameters of the approximate posterior. We note that the encoding of the approximate posterior and the decoding for generation are tied through the RNN hidden state $\mathbf{h<em t-1="t-1">{t-1}$. We also observe that this conditioning on $\mathbf{h}</em>$ results in the factorization:</p>
<p>$$
q\left(\mathbf{z}<em T="T" _leq="\leq">{\leq T} \mid \mathbf{x}</em>}\right)=\prod_{t=1}^{T} q\left(\mathbf{z<em _leq="\leq" t="t">{t} \mid \mathbf{x}</em>\right)
$$}, \mathbf{z}_{&lt;t</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graphical illustrations of each operation of the VRNN: (a) computing the conditional prior using Eq. (5); (b) generating function using Eq. (6); (c) updating the RNN hidden state using Eq. (7); (d) inference of the approximate posterior using Eq. (9); (e) overall computational paths of the VRNN.</p>
<p>Learning The objective function becomes a timestep-wise variational lower bound using Eq. (8) and Eq. (10):</p>
<p>$$
\mathbb{E}<em T="T" _leq="\leq">{q\left(\mathbf{z}</em>} \mid \mathbf{x<em t="1">{\leq T}\right)}\left[\sum</em>}^{T}\left(-\mathrm{KL}\left(q\left(\mathbf{z<em _leq="\leq" t="t">{t} \mid \mathbf{x}</em>}, \mathbf{z<em t="t">{&lt;t}\right) | p\left(\mathbf{z}</em>} \mid \mathbf{x<em _t="&lt;t">{&lt;t}, \mathbf{z}</em>}\right)\right)+\log p\left(\mathbf{x<em _leq="\leq" t="t">{t} \mid \mathbf{z}</em>\right)\right)\right]
$$}, \mathbf{x}_{&lt;t</p>
<p>As in the standard VAE, we learn the generative and inference models jointly by maximizing the variational lower bound with respect to their parameters. The schematic view of the VRNN is shown in Fig. 1, operations (a)-(d) correspond to Eqs. (5)-(7), (9), respectively. The VRNN applies the operation (a) when computing the conditional prior (see Eq. (5)). If the variant of the VRNN (VRNN-I) does not apply the operation (a), then the prior becomes independent across timesteps. STORN [2] can be considered as an instance of the VRNN-I model family. In fact, STORN puts further restrictions on the dependency structure of the approximate inference model. We include this version of the model (VRNN-I) in our experimental evaluation in order to directly study the impact of including the temporal dependency structure in the prior (i.e., conditional prior) over the latent random variables.</p>
<h1>4 Experiment Settings</h1>
<p>We evaluate the proposed VRNN model on two tasks: (1) modelling natural speech directly from the raw audio waveforms; (2) modelling handwriting generation.</p>
<p>Speech modelling We train the models to directly model raw audio signals, represented as a sequence of 200-dimensional frames. Each frame corresponds to the real-valued amplitudes of 200 consecutive raw acoustic samples. Note that this is unlike the conventional approach for modelling speech, often used in speech synthesis where models are expressed over representations such as spectral features [see, e.g., 18, 3, 13].</p>
<p>We evaluate the models on the following four speech datasets:</p>
<ol>
<li>Blizzard: This text-to-speech dataset made available by the Blizzard Challenge 2013 contains 300 hours of English, spoken by a single female speaker [10].</li>
<li>TIMIT: This widely used dataset for benchmarking speech recognition systems contains 6,300 English sentences, read by 630 speakers.</li>
<li>Onomatopoeia ${ }^{2}$ : This is a set of 6,738 non-linguistic human-made sounds such as coughing, screaming, laughing and shouting, recorded from 51 voice actors.</li>
<li>Accent: This dataset contains English paragraphs read by 2,046 different native and nonnative English speakers [19].
<sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p>Table 1: Average log-likelihood on the test (or validation) set of each task.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Speech modelling</th>
<th></th>
<th></th>
<th></th>
<th>Handwriting</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Blizzard</td>
<td>TIMIT</td>
<td>Onomatopoeia</td>
<td>Accent</td>
<td>IAM-OnDB</td>
</tr>
<tr>
<td>RNN-Gauss</td>
<td>3539</td>
<td>-1900</td>
<td>-984</td>
<td>-1293</td>
<td>1016</td>
</tr>
<tr>
<td>RNN-GMM</td>
<td>7413</td>
<td>26643</td>
<td>18865</td>
<td>3453</td>
<td>1358</td>
</tr>
<tr>
<td>VRNN-I-Gauss</td>
<td>$\geq 8933$</td>
<td>$\geq 28340$</td>
<td>$\geq 19053$</td>
<td>$\geq 3843$</td>
<td>$\geq 1332$</td>
</tr>
<tr>
<td></td>
<td>$\approx 9188$</td>
<td>$\approx 29639$</td>
<td>$\approx 19638$</td>
<td>$\approx 4180$</td>
<td>$\approx 1353$</td>
</tr>
<tr>
<td>VRNN-Gauss</td>
<td>$\geq 9223$</td>
<td>$\geq 28805$</td>
<td>$\geq 20721$</td>
<td>$\geq 3952$</td>
<td>$\geq 1337$</td>
</tr>
<tr>
<td></td>
<td>$\approx 9516$</td>
<td>$\approx 30235$</td>
<td>$\approx 21332$</td>
<td>$\approx 4223$</td>
<td>$\approx 1354$</td>
</tr>
<tr>
<td>VRNN-GMM</td>
<td>$\geq 9107$</td>
<td>$\geq 28982$</td>
<td>$\geq 20849$</td>
<td>$\geq 4140$</td>
<td>$\geq 1384$</td>
</tr>
<tr>
<td></td>
<td>$\approx 9392$</td>
<td>$\approx 29604$</td>
<td>$\approx 21219$</td>
<td>$\approx 4319$</td>
<td>$\approx 1384$</td>
</tr>
</tbody>
</table>
<p>For the Blizzard and Accent datasets, we process the data so that each sample duration is $0.5s$ (the sampling frequency used is 16 kHz ). Except the TIMIT dataset, the rest of the datasets do not have predefined train/test splits. We shuffle and divide the data into train/validation/test splits using a ratio of $0.9 / 0.05 / 0.05$.</p>
<p>Handwriting generation We let each model learn a sequence of $(x, y)$ coordinates together with binary indicators of pen-up/pen-down, using the IAM-OnDB dataset, which consists of 13, 040 handwritten lines written by 500 writers [14]. We preprocess and split the dataset as done in [7].</p>
<p>Preprocessing and training The only preprocessing used in our experiments is normalizing each sequence using the global mean and standard deviation computed from the entire training set. We train each model with stochastic gradient descent on the negative log-likelihood using the Adam optimizer [12], with a learning rate of 0.001 for TIMIT and Accent and 0.0003 for the rest. We use a minibatch size of 128 for Blizzard and Accent and 64 for the rest. The final model was chosen with early-stopping based on the validation performance.</p>
<p>Models We compare the VRNN models with the standard RNN models using two different output functions: a simple Gaussian distribution (Gauss) and a Gaussian mixture model (GMM). For each dataset, we conduct an additional set of experiments for a VRNN model without the conditional prior (VRNN-I).</p>
<p>We fix each model to have a single recurrent hidden layer with 2000 LSTM units (in the case of Blizzard, 4000 and for IAM-OnDB, 1200). All of $\varphi_{\tau}$ shown in Eqs. (5)-(7), (9) have four hidden layers using rectified linear units [15] (for IAM-OnDB, we use a single hidden layer). The standard RNN models only have $\varphi_{\tau}^{\mathbf{x}}$ and $\varphi_{\tau}^{\text {dec }}$, while the VRNN models also have $\varphi_{\tau}^{\mathbf{z}}$, $\varphi_{\tau}^{\text {dec }}$ and $\varphi_{\tau}^{\text {pnet }}$. For the standard RNN models, $\varphi_{\tau}^{\mathbf{z}}$ is the feature extractor, and $\varphi_{\tau}^{\text {dec }}$ is the generating function. For the RNNGMM and VRNN models, we match the total number of parameters of the deep neural networks (DNNs), $\varphi_{\tau}^{\mathbf{x}, \mathbf{z}, \text { enc, dec, prior }}$, as close to the RNN-Gauss model having 600 hidden units for every layer that belongs to either $\varphi_{\tau}^{\mathbf{x}}$ or $\varphi_{\tau}^{\text {dec }}$ (we consider 800 hidden units in the case of Blizzard). Note that we use 20 mixture components for models using a GMM as the output function.</p>
<p>For qualitative analysis of speech generation, we train larger models to generate audio sequences. We stack three recurrent hidden layers, each layer contains 3000 LSTM units. Again for the RNNGMM and VRNN models, we match the total number of parameters of the DNNs to be equal to the RNN-Gauss model having 3200 hidden units for each layer that belongs to either $\varphi_{\tau}^{\mathbf{x}}$ or $\varphi_{\tau}^{\text {dec }}$.</p>
<h1>5 Results and Analysis</h1>
<p>We report the average log-likelihood of test examples assigned by each model in Table 1. For RNN-Gauss and RNN-GMM, we report the exact log-likelihood, while in the case of VRNNs, we report the variational lower bound (given with $\geq$ sign, see Eq. (4)) and approximated marginal log-likelihood (given with $\approx$ sign) based on importance sampling using 40 samples as in [17]. In general, higher numbers are better. Our results show that the VRNN models have higher loglikelihood, which support our claim that latent random variables are helpful when modelling com-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The top row represents the difference $\delta_{t}$ between $\boldsymbol{\mu}<em t-1="t-1" z_="z,">{z, t}$ and $\boldsymbol{\mu}</em>$. The middle row shows the dominant KL divergence values in temporal order. The bottom row shows the input waveforms.
plex sequences. The VRNN models perform well even with a unimodal output function (VRNNGauss), which is not the case for the standard RNN models.</p>
<p>Latent space analysis In Fig. 2, we show an analysis of the latent random variables. We let a VRNN model read some unseen examples and observe the transitions in the latent space. We compute $\delta_{t}=\sum_{j}\left(\boldsymbol{\mu}<em t-1="t-1" z_="z,">{z, t}^{j}-\boldsymbol{\mu}</em>$ that can affect the RNN dynamics to change modality.
}^{j}\right)^{2}$ at every timestep and plot the results on the top row of Fig. 2. The middle row shows the KL divergence computed between the approximate posterior and the conditional prior. When there is a transition in the waveform, the KL divergence tends to grow (white is high), and we can clearly observe a peak in $\delta_{t<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples from the training set and generated samples from RNN-GMM and VRNNGauss. Top three rows show the global waveforms while the bottom three rows show more zoomedin waveforms. Samples from (b) RNN-GMM contain high-frequency noise, and samples from (c) VRNN-Gauss have less noise. We exclude RNN-Gauss, because the samples are almost close to pure noise.</p>
<p>Speech generation We generate waveforms with $2.0 s$ duration from the models that were trained on Blizzard. From Fig. 3, we can clearly see that the waveforms from the VRNN-Gauss are much less noisy and have less spurious peaks than those from the RNN-GMM. We suggest that the large amount of noise apparent in the waveforms from the RNN-GMM model is a consequence of the compromise these models must make between representing a clean signal consistent with the training data and encoding sufficient input variability to capture the variations across data examples. The latent random variable models can avoid this compromise by adding variability in the latent space, which can always be mapped to a point close to a relatively clean sample.</p>
<p>Handwriting generation Visual inspection of the generated handwriting (as shown in Fig. 4) from the trained models reveals that the VRNN model is able to generate more diverse writing style while maintaining consistency within samples.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Handwriting samples: (a) training examples and unconditionally generated handwriting from (b) RNN-Gauss, (c) RNN-GMM and (d) VRNN-GMM. The VRNN-GMM retains the writing style from beginning to end while RNN-Gauss and RNN-GMM tend to change the writing style during the generation process. This is possibly because the sequential latent random variables can guide the model to generate each sample with a consistent writing style.</p>
<h1>6 Conclusion</h1>
<p>We propose a novel model that can address sequence modelling problems by incorporating latent random variables into a recurrent neural network (RNN). Our experiments focus on unconditional natural speech generation as well as handwriting generation. We show that the introduction of latent random variables can provide significant improvements in modelling highly structured sequences such as natural speech sequences. We empirically show that the inclusion of randomness into high-level latent space can enable the VRNN to model natural speech sequences with a simple Gaussian distribution as the output function. However, the standard RNN model using the same output function fails to generate reasonable samples. An RNN-based model using more powerful output function such as a GMM can generate much better samples, but they contain a large amount of high-frequency noise compared to the samples generated by the VRNN-based models.
We also show the importance of temporal conditioning of the latent random variables by reporting higher log-likelihood numbers on modelling natural speech sequences. In handwriting generation, the VRNN model is able to model the diversity across examples while maintaining consistent writing style over the course of generation.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank the developers of Theano [1]. Also, the authors thank Kyunghyun Cho, Kelvin Xu and Sungjin Ahn for insightful comments and discussion. We acknowledge the support of the following agencies for research funding and computing support: Ubisoft, Nuance Foundation, NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs and CIFAR.</p>
<h1>References</h1>
<p>[1] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[2] J. Bayer and C. Osendorfer. Learning stochastic recurrent networks. arXiv preprint arXiv:1411.7610, 2014.
[3] A. Bertrand, K. Demuynck, V. Stouten, and H. V. Hamme. Unsupervised learning of auditory filter banks using non-negative matrix factorisation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4713-4716. IEEE, 2008.
[4] N. Boulanger-lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 1159-1166, 2012.
[5] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17241734, 2014.
[6] O. Fabius, J. R. van Amersfoort, and D. P. Kingma. Variational recurrent auto-encoders. arXiv preprint arXiv:1412.6581, 2014.
[7] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[8] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image generation. In Proceedings of The 32nd International Conference on Machine Learning (ICML), 2015.
[9] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997.
[10] S. King and V. Karaiskos. The blizzard challenge 2013. In The Ninth annual Blizzard Challenge, 2013.
[11] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations (ICLR), 2014.
[12] D. P. Kingma and M. Welling. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
[13] H. Lee, P. Pham, Y. Largman, and A. Y. Ng. Unsupervised feature learning for audio classification using convolutional deep belief networks. In Advances in Neural Information Processing Systems (NIPS), pages $1096-1104,2009$.
[14] M. Liwicki and H. Bunke. Iam-ondb-an on-line english sentence database acquired from handwritten text on a whiteboard. In Proceedings of Eighth International Conference on Document Analysis and Recognition, pages 956-961. IEEE, 2005.
[15] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807-814, 2010.
[16] M. Pachitariu and M. Sahani. Learning visual motion in recurrent neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 1322-1330, 2012.
[17] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of The 31st International Conference on Machine Learning (ICML), pages 1278-1286, 2014.
[18] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and K. Oura. Speech synthesis based on hidden markov models. Proceedings of the IEEE, 101(5):1234-1252, 2013.
[19] S. Weinberger. The speech accent archieve. http://accent.gmu.edu/, 2015.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ This dataset has been provided by Ubisoft.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
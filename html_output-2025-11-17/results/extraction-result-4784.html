<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4784 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4784</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4784</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-26059f871eea2ef9aeeda228ebd40a69b61ab65c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/26059f871eea2ef9aeeda228ebd40a69b61ab65c" target="_blank">RecMind: Large Language Model Powered Agent For Recommendation</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> This work designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations, and proposes a Self-Inspiring algorithm to improve the planning ability.</p>
                <p><strong>Paper Abstract:</strong> While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4784.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4784.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecMind</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RecMind: Large Language Model Powered Agent For Recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered autonomous recommender agent that combines step-by-step planning (thought, action, observation), an external memory layer (personalized memory + world knowledge), and tool use (SQL, web search, summarization) to produce zero/few-shot personalized recommendations and explanations. Introduces a Self-Inspiring (SI) planning algorithm that retains and conditions on historical reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RecMind (with CoT, ToT, and Self-Inspiring planning variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RecMind is an autonomous recommendation agent built around a foundation LLM (gpt-3.5-turbo-16k in experiments) that generates multi-step plans composed of Thought, Action, and Observation. It integrates: (1) planning algorithms (Chain-of-Thoughts, Tree-of-Thoughts, and the proposed Self-Inspiring method), (2) an external Memory component (Personalized Memory and World Knowledge) accessible via tools, and (3) tools (SQL Database, Search/Web, Text Summarization) that the agent calls during planning to retrieve and summarize evidence used in recommendations and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory (database-backed personalized memory + web-accessible world knowledge) accessed via tools</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Personalized Memory: per-user interaction history (reviews, ratings) and item metadata stored in a MySQL database; World Knowledge: item metadata and real-time information retrieved via a Search tool (SerpApi/Google). Tools mediate memory access: an SQL tool converts natural-language queries to SQL and returns query results; a Search tool returns web results converted to text; a Summarization tool condenses long reviews. The LLM conditions planning steps on observations returned from these tools.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Recommendation tasks (rating prediction, sequential recommendation, direct recommendation, explanation generation, review summarization) and general multi-step reasoning benchmarks (Game of 24, Mini Crosswords)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple recommendation tasks: predict user ratings for items (rating prediction); rank/choose next items given interaction sequences (sequential and direct recommendation); generate textual explanations for user-item interactions (explanation generation); summarize user reviews (review summarization). Evaluations also include domain transfer (few-shot prompts from one domain tested on another). General reasoning tasks: Game of 24 and Mini Crosswords used to compare planning methods. Tasks require integrating personalized user history and external (world) knowledge for informed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Amazon Reviews (domains: Beauty, Sports & Outdoors, Toys & Games), Yelp, Game of 24, Mini Crosswords</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported RecMind results (these experiments used RecMind with memory/tools enabled):
- Rating prediction (Beauty domain, Table 1): RecMind-SI few-shot RMSE=1.0756, MAE=0.6892; RecMind-SI zero-shot RMSE=1.1894, MAE=0.7883. (Yelp: RecMind-SI few-shot RMSE=1.3674, MAE=0.9698; zero-shot RMSE=1.4530, MAE=1.0009.)
- Explanation generation (Beauty, Table 4): RecMind-SI few-shot BLEU2=1.3459, ROUGE-1=13.2560; zero-shot BLEU2=1.1589, ROUGE-1=11.6794. Human evaluation (100 examples, Table B.2): RecMind-SI had average top-1 ratio 0.347 (best among compared methods).
- Domain transfer (Table 5): Beauty→Toys: MAE=0.6779, HR@5=0.0902, BLEU2=1.5940; Beauty→Sports: MAE=0.6245, HR@5=0.1124, BLEU2=1.0537.
- General reasoning (Tables 6 & 7): Game of 24 accuracy: SI 80% vs ToT 74% vs CoT 4%; Mini Crosswords (game-level accuracy) SI 26% vs ToT 20% vs CoT 1%.
- Runtime (Table B.3): Average inference time (GPT-3.5 base): CoT 18.9s, ToT 53.2s, SI 29.7s.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The paper does not present an explicit ablation that turns memory on vs off. However, results compare RecMind (which uses external memory and tools) to LLM prompting baselines (ChatGPT zero/few-shot) and to P5 (a fine-tuned model). RecMind (with memory/tools and SI planning) outperforms bare ChatGPT prompting across tasks (examples given in rating prediction and recommendation metrics) and attains performance comparable to the fully trained P5 model in many tasks. Authors attribute improvements to access to personalized memory and world knowledge via tools plus the SI planning mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations related to memory usage and the approach: (1) retaining and exploring diverse reasoning paths (and their observations) increases prompt size and exacerbates LLM long-context limitations and position-bias; (2) increased context length may include noisy historical paths — authors propose future summarization of histories; (3) only a small number of external tools were used in current implementation (limiting memory modalities explored); (4) no explicit ablation isolating memory's individual contribution was reported, so attribution between planning algorithm gains and memory/tool gains is not fully disentangled.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Key conclusions about memory use: (1) Explicit external memory (personalized user history stored in a database and world knowledge via web search) accessed through tools enables an LLM agent to leverage up-to-date and in-domain signals that are not stored in model weights, improving recommendation accuracy and explanation quality. (2) Combining memory/tool access with a planning algorithm that retains and conditions on multiple historical reasoning states (Self-Inspiring) yields better task performance and more robust reasoning than standard CoT or ToT. (3) Memory access via structured tools (SQL) and search plus summarization makes long, noisy textual content tractable for downstream steps. (4) Practical trade-offs exist: richer memory and multi-path planning improve quality but increase prompt/context size and inference time; SI mitigates some runtime overhead compared to ToT by selectively exploring alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RecMind: Large Language Model Powered Agent For Recommendation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question answering with human feedback <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Gorilla: Large language model connected with massive apis <em>(Rating: 1)</em></li>
                <li>PALR: Personalization aware llms for recommendation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4784",
    "paper_id": "paper-26059f871eea2ef9aeeda228ebd40a69b61ab65c",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "RecMind",
            "name_full": "RecMind: Large Language Model Powered Agent For Recommendation",
            "brief_description": "An LLM-powered autonomous recommender agent that combines step-by-step planning (thought, action, observation), an external memory layer (personalized memory + world knowledge), and tool use (SQL, web search, summarization) to produce zero/few-shot personalized recommendations and explanations. Introduces a Self-Inspiring (SI) planning algorithm that retains and conditions on historical reasoning paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RecMind (with CoT, ToT, and Self-Inspiring planning variants)",
            "agent_description": "RecMind is an autonomous recommendation agent built around a foundation LLM (gpt-3.5-turbo-16k in experiments) that generates multi-step plans composed of Thought, Action, and Observation. It integrates: (1) planning algorithms (Chain-of-Thoughts, Tree-of-Thoughts, and the proposed Self-Inspiring method), (2) an external Memory component (Personalized Memory and World Knowledge) accessible via tools, and (3) tools (SQL Database, Search/Web, Text Summarization) that the agent calls during planning to retrieve and summarize evidence used in recommendations and explanations.",
            "memory_type": "external memory (database-backed personalized memory + web-accessible world knowledge) accessed via tools",
            "memory_description": "Personalized Memory: per-user interaction history (reviews, ratings) and item metadata stored in a MySQL database; World Knowledge: item metadata and real-time information retrieved via a Search tool (SerpApi/Google). Tools mediate memory access: an SQL tool converts natural-language queries to SQL and returns query results; a Search tool returns web results converted to text; a Summarization tool condenses long reviews. The LLM conditions planning steps on observations returned from these tools.",
            "task_name": "Recommendation tasks (rating prediction, sequential recommendation, direct recommendation, explanation generation, review summarization) and general multi-step reasoning benchmarks (Game of 24, Mini Crosswords)",
            "task_description": "Multiple recommendation tasks: predict user ratings for items (rating prediction); rank/choose next items given interaction sequences (sequential and direct recommendation); generate textual explanations for user-item interactions (explanation generation); summarize user reviews (review summarization). Evaluations also include domain transfer (few-shot prompts from one domain tested on another). General reasoning tasks: Game of 24 and Mini Crosswords used to compare planning methods. Tasks require integrating personalized user history and external (world) knowledge for informed outputs.",
            "benchmark_name": "Amazon Reviews (domains: Beauty, Sports & Outdoors, Toys & Games), Yelp, Game of 24, Mini Crosswords",
            "performance_with_memory": "Reported RecMind results (these experiments used RecMind with memory/tools enabled):\n- Rating prediction (Beauty domain, Table 1): RecMind-SI few-shot RMSE=1.0756, MAE=0.6892; RecMind-SI zero-shot RMSE=1.1894, MAE=0.7883. (Yelp: RecMind-SI few-shot RMSE=1.3674, MAE=0.9698; zero-shot RMSE=1.4530, MAE=1.0009.)\n- Explanation generation (Beauty, Table 4): RecMind-SI few-shot BLEU2=1.3459, ROUGE-1=13.2560; zero-shot BLEU2=1.1589, ROUGE-1=11.6794. Human evaluation (100 examples, Table B.2): RecMind-SI had average top-1 ratio 0.347 (best among compared methods).\n- Domain transfer (Table 5): Beauty→Toys: MAE=0.6779, HR@5=0.0902, BLEU2=1.5940; Beauty→Sports: MAE=0.6245, HR@5=0.1124, BLEU2=1.0537.\n- General reasoning (Tables 6 & 7): Game of 24 accuracy: SI 80% vs ToT 74% vs CoT 4%; Mini Crosswords (game-level accuracy) SI 26% vs ToT 20% vs CoT 1%.\n- Runtime (Table B.3): Average inference time (GPT-3.5 base): CoT 18.9s, ToT 53.2s, SI 29.7s.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The paper does not present an explicit ablation that turns memory on vs off. However, results compare RecMind (which uses external memory and tools) to LLM prompting baselines (ChatGPT zero/few-shot) and to P5 (a fine-tuned model). RecMind (with memory/tools and SI planning) outperforms bare ChatGPT prompting across tasks (examples given in rating prediction and recommendation metrics) and attains performance comparable to the fully trained P5 model in many tasks. Authors attribute improvements to access to personalized memory and world knowledge via tools plus the SI planning mechanism.",
            "limitations_or_challenges": "Reported limitations related to memory usage and the approach: (1) retaining and exploring diverse reasoning paths (and their observations) increases prompt size and exacerbates LLM long-context limitations and position-bias; (2) increased context length may include noisy historical paths — authors propose future summarization of histories; (3) only a small number of external tools were used in current implementation (limiting memory modalities explored); (4) no explicit ablation isolating memory's individual contribution was reported, so attribution between planning algorithm gains and memory/tool gains is not fully disentangled.",
            "key_insights": "Key conclusions about memory use: (1) Explicit external memory (personalized user history stored in a database and world knowledge via web search) accessed through tools enables an LLM agent to leverage up-to-date and in-domain signals that are not stored in model weights, improving recommendation accuracy and explanation quality. (2) Combining memory/tool access with a planning algorithm that retains and conditions on multiple historical reasoning states (Self-Inspiring) yields better task performance and more robust reasoning than standard CoT or ToT. (3) Memory access via structured tools (SQL) and search plus summarization makes long, noisy textual content tractable for downstream steps. (4) Practical trade-offs exist: richer memory and multi-path planning improve quality but increase prompt/context size and inference time; SI mitigates some runtime overhead compared to ToT by selectively exploring alternatives.",
            "uuid": "e4784.0",
            "source_info": {
                "paper_title": "RecMind: Large Language Model Powered Agent For Recommendation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question answering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "Gorilla: Large language model connected with massive apis",
            "rating": 1
        },
        {
            "paper_title": "PALR: Personalization aware llms for recommendation",
            "rating": 1
        }
    ],
    "cost": 0.01094875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RecMind: Large Language Model Powered Agent For Recommendation</h1>
<p>Yancheng Wang ${ }^{1}$; Ziyan Jiang ${ }^{2}$; Zheng Chen ${ }^{2}$; Fan Yang ${ }^{2}$; Yingxue Zhou ${ }^{2}$; Eunah Cho ${ }^{2}$, Xing Fan ${ }^{2}$, Xiaojiang Huang ${ }^{2}$, Yanbin Lu ${ }^{2}$, Yingzhen Yang ${ }^{1}$<br>${ }^{1}$ School of Computing and Augmented Intelligence, Arizona State University<br>${ }^{2}$ Amazon Alexa AI<br>{yancheng.wang, yingzhen.yang}@asu.edu<br>{ziyjiang, zgchen, ffanyang, zyingxue, eunahch, fanxing, xjhuang, luyanbin}@amazon.com</p>
<h4>Abstract</h4>
<p>While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and finetune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLMpowered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM "self-inspires" to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.</p>
<h2>1 Introduction</h2>
<p>The Recommender System (RS) plays a key role in search engines, e-commerce, and various other Internet platforms. An RS analyzes the historical interactions between users and items to recommend potential items (Koren et al., 2009b; Linden et al., 2003). The RS has been enhanced by Deep Neural Networks (DNNs) to more effectively learn the representations of users, items, and sequential behaviors (Hidasi et al., 2015; He et al., 2020; Sun et al., 2019). However, most existing DNN-based methods (e.g., CNN and LSTM) and pre-trained language models (e.g., BERT) cannot</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>sufficiently capture textual knowledge about users and items due to limitations in model scale and data size. Besides, most existing RS methods have been designed for specific tasks and are inadequate in generalizing to unseen recommendation tasks (Fan et al., 2023).</p>
<p>Recent advances in Large Language Models (LLMs), such as GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023a), LLaMa-2 (Touvron et al., 2023b), and PaLM-2 (Anil et al., 2023) have demonstrated remarkable results in a wide range of tasks, which have motivated the research of leveraging LLMs for recommendation to mitigate the aforementioned challenges (Liu et al., 2023; Fan et al., 2023; Lin et al., 2023). However, existing studies primarily rely on knowledge stored within the model's weights, neglecting the potential benefits of leveraging external tools to access real-time information and external knowledge (Yang et al., 2023; Bao et al., 2023). Furthermore, the reasoning ability of LLMs is not fully utilized for recommendation, resulting in suboptimal predictions due to the intricate nature of recommendation-related tasks (Liu et al., 2023).</p>
<p>To better utilize the strong reasoning and toolusing abilities of LLMs, we design a recommendation agent RecMind that leverages an LLMpowered API as its intellectual core and incorporates a few key components. The first key component is Planning which enables the agent to break complex recommendation tasks into manageable steps for efficient handling of complex situations. Each step of planning involves thought, action and observation (see Figure 1 for examples and Section 3 for details). The agent is also equipped with Memory consisting of Personalized Memory and World Knowledge, each accessible through specific tools. The Tools enhance the agent's functionality on top of the LLM, such as retrieving relevant knowledge, or assisting with the reasoning process.</p>
<p>To further enhance the planning ability of the</p>
<table>
<thead>
<tr>
<th>RecMind-ToT</th>
<th>RecMind-SI</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="img-0.jpeg" src="img-0.jpeg" /></td>
<td><img alt="img-1.jpeg" src="img-1.jpeg" /></td>
</tr>
<tr>
<td></td>
<td>Figure 1: Comparisons of rating prediction by RecMind-ToT (left) and RecMind-SI (right). After searching for the product category of the item in Step 2, RecMind-ToT first generates thought 3 (1) to retrieve the rating of a similar item. After being evaluated by the voting-based evaluator, RecMind-ToT prunes option 3 (1) and proposes another thought 3 (2) to retrieve the average rating of the item and then makes the prediction solely based on it. In contrast, although RecMind-SI proposed the same alternative options in step 3, it takes into account the thought, action, and observation from both options 3 (1) and 3 (2) to generate the thought for the next step.</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>agent, we propose a new planning algorithm Self-</td>
<td>baselines that do not involve any fine-tuning and achieves competitive performance with a fully pre-trained expert recommendation model such as P5 <em>Geng et al. (2022)</em>. In addition, SI outperforms CoT and ToT on general reasoning tasks, showing that the proposed the impact of SI is beyond recommendation tasks.</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>agent, we propose a new planning algorithm Self-</td>
<td>2 Related Work</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>Inspiring (SI). At each intermediate planning step, the agent “self-inspires” to consider all previously explored paths for the next planning. Unlike existing Chain-of-Thoughts (CoT) <em>Wei et al. (2022)</em> and Tree-of-Thoughts (ToT) <em>Yao et al. (2023)</em> which discards states (thoughts) in previously explored paths when generating a new state, SI retains all previous states from all history paths when generating new state. SI is inspired by the intuition that all historical states can provide useful information for better planning. Figure 1 provides an example of ToT and SI showing that SI achieves a more accurate rating than ToT due to better planning.</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>To the best of our knowledge, this is the first public research work on an LLM-powered autonomous agent for recommendation. The main contributions of our work are:</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 2: Here is an overview of our proposed RecMind architecture. It comprises four major components: "RecMind" is built based on ChatGPT API, "Tools" support various API call to retrieve knowledge from "Memory" component, "Planning" component is in charge of thoughts generation.</p>
<p>interactions and optional data, including profiles, are input into an LLM prompt with options: no fine-tuning <em>Wang and Lim (2023)</em>, full-model finetuning <em>Yang et al. (2023)</em>, or parameter-efficient fine-tuning <em>Bao et al. (2023)</em>. In sequential recommendation tasks, we use a pre-filtered set of item candidates in the prompts for focused ranking. <em>Liu et al. (2023)</em> use prompts to assess ChatGPT’s performance across five recommendation tasks showing LLM’s strong in-context learning abilities and generalization <em>Wei et al. (2021)</em>. Unlike existing studies, our work harnesses the LLM’s capabilities in reasoning, tool usage, and action.</p>
<h2>3 Architecture</h2>
<p>As shown in Figure 2, RecMind consists of key components: LLM-powered API such as ChatGPT to drive the overall reasoning, planning which breaks down a task to smaller sub-tasks for step-bystep planning, memory which provides the agent with the capability to retain and recall information over extended periods, and tools for obtaining relevant extra information from memory that is missing from the model weights and aiding the reasoning. We introduce the key components planning, memory and tools for RecMind in the subsequent parts.</p>
<p>Planning Planning helps LLM Agents decompose tasks into smaller, manageable subgoals for handling complex tasks. Consider the setting where the goal is to generate the final result $y$ given problem $x$ via an LLM Agent parameterized by $\theta$. The</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Comparison between Tree-of-Thoughts DFS and Self-Inspiring. Red arrows in the figure indicate the process for generating alternative thoughts at intermediate steps. Blue dashed arrows in the figure denote the backtracking process.</p>
<p>traditional input-output method gives the result by $y \sim p_{\theta}(y \mid x)$. With planning, RecMind generates the result $y \sim p_{\theta}(y \mid \operatorname{planing}(x))$, where $\operatorname{planing}(x)$ is a set of prompts that decomposes problem $x$ into a series sub-tasks that is composed of thought $h$, action $a$, and observation $o$. Figure 1 provides examples of planning including thoughts, actions, and observations. We first review existing popular reasoning methods such as CoT and ToT which we have explored for RecMind. Then we present the proposed SI algorithm. All these planning methods can be viewed as traversing through a latent reasoning tree, as shown in Figure 3.</p>
<ul>
<li>Chain-of-Thoughts (CoT) <em>Wei et al. (2022)</em> has been used in ReAct <em>Yao et al. (2022)</em> to synergize reasoning and action. This CoT planning method follows a single path in the reasoning tree. In our setting, at each time step $t$, the agent receives</li>
</ul>
<p>observation $o_{t}$ followed by thought $h_{t}$ and action $a_{t}$. Let $s_{t}=\left(h_{t}, a_{t}, o_{t}\right)$ denote the RecMind state at step $t$. The CoT planning method generates the next state $s_{t+1}=\left(h_{t+1}, a_{t+1}, o_{t+1}\right)$ by sampling $p_{\theta}\left(s_{t+1} \mid x, s_{1}, . ., s_{t}\right)$. Thus CoT only follows a single planning path $S=$ $\left{s_{1}, \ldots, s_{t}, \ldots, s_{T}\right}$ until reaching the final result $y \sim p_{\theta}\left(y \mid x, s_{1}, \ldots, s_{t}, \ldots, s_{T}\right)$ after $T$ steps.</p>
<ul>
<li>Tree-of-Thoughts (ToT) (Yao et al., 2023) extends CoT to explore multiple paths in the reasoning tree. At step $t$ and state $s_{t}$, ToTBFS explicitly generates multiple candidates $\left{s_{t+1}^{1}, \ldots, s_{t+1}^{k}\right}$ for next state by i.i.d. sampling $s_{t+1}^{i} \sim p_{\theta}\left(s_{t+1} \mid x, s_{1}, . ., s_{t}\right)$ for $i \in[k]$. Then it applies majority vote to select the state $s_{t+1}$ from $\left{s_{t+1}^{1}, \ldots, s_{t+1}^{k}\right}$. Eventually ToT-BFS generates a single path similar to CoT. In contrast, ToT-DFS explores one branch at a time, but might prune the current state, and backtracks to the previous state to start a new reasoning branch. Denote the first explored path as $z^{(1)}=$ $\left{s_{1}^{(1)}, \ldots, s_{t}^{(1)}, s_{t+1}^{(1)}\right}$. If the last state $s_{t+1}^{(1)}$ is pruned and it backtracks to the previous state $s_{t}^{(1)}$, and starts a new reasoning branch, then the path becomes $z^{(2)}=\left{s_{1}^{(1)}, \ldots, s_{t}^{(1)}, s_{t+1}^{(2)}, \ldots\right}$. After exploring $n$ branches, we denote the final path of ToT as $z^{(n)}=\left{s_{1}, \ldots, s_{j_{1}}^{(1)}, \ldots, s_{j_{2}}^{(2)}, \ldots, s_{T}^{(n)}\right}$ and the final result $y$ is obtained by $y \sim p_{\theta}\left(x, z^{(n)}\right)$.</li>
</ul>
<p>We find the discarded historical states from previously explored branches such as $s_{t+1}^{(1)}$ from branch $z^{(1)}$ usually contain helpful information for RecMind to generate a better state compared with only considering the final path of ToT. Thus, we propose Self-Inspiring (SI) as shown in Figure 3(b) and Algorithm 1, a new planning method for RecMind. SI inspires itself into exploring an alternative reasoning branch, while retaining all previous states. At $m$-th path and step $t$, SI generates the next step of planning by considering all previous paths, i.e., $s_{t+1}^{(m)} \sim p_{\theta}\left(s_{t+1} \mid z^{(1)}, \ldots, z^{(m)}\right)$. After exploring $n$ paths, the RecMind obtains the final result $y \sim P_{\theta}\left(x, z^{(1)}, \ldots, z^{(n)}\right)$. Figure 3 provides an example to illustrate the key difference between ToT and SI. In ToT (Figure 3(a)), The new state $N(2)$ at the second path is generated by only considering state $N-1$. The state $N(1)$ is discarded. However, in SI (Figure 3(b)), the new state $N(2)$ is generated based on both $N-1$ and $N(1)$.</p>
<p>The mechanism of SI makes it possible for the agent to analyze different perspectives of the obser-</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="k">Self</span><span class="o">-</span><span class="nx">Inspiring</span><span class="w"> </span><span class="nx">Planning</span>
<span class="nx">Require</span><span class="p">:</span><span class="w"> </span><span class="nx">Problem</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">planning</span><span class="w"> </span><span class="nx">path</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S</span><span class="p">=</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">z</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">z</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="o">-</span><span class="mi">1</span><span class="p">)},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">j_</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">j_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">LLM</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">limit</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="err">\</span><span class="p">).</span><span class="w"> </span><span class="nx">Let</span><span class="w"> </span><span class="nx">inspire</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="nx">cdot</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">API</span><span class="w"> </span><span class="nx">checking</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">planning</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">explore</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">alternative</span><span class="w"> </span><span class="nx">reasoning</span><span class="w"> </span><span class="nx">branch</span><span class="p">.</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">leq</span><span class="w"> </span><span class="nx">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)}=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)},</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)},</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">S</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">h_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="s">&quot;End of Planning&quot;</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">        </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">S</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">S</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">inspire</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">S</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nx">Sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">2</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="o">+</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">S</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">S</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">S</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">2</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">m</span><span class="o">+</span><span class="mi">1</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">},</span><span class="w"> </span><span class="nx">m</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">m</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">t</span><span class="o">+</span><span class="mi">2</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">S</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">S</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">y</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">S</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>vation of a previous step. For example, an agent for recommending a movie may summarize both the favorite movie director and the favorite movie genre of a user after retrieving the user's watching history. Next, it can make recommendations from a candidate list by considering both factors. In contrast, previous reasoning methods, such as CoT and ToT, generate the final output based on one single path. Even though ToT samples multiple options at intermediate steps, it only adopts the most confident option and proceeds to the next step. That might be enough for a simple reasoning task. However, recommendation tasks based on textual content require inclusive consideration of different perspectives of available content from both personalized memory and world knowledge.
Memory Information stored in memory, including Personalized Memory and World Knowledge, enables the model to access knowledge beyond what is inherently present in the LLM's parameters. Using the Amazon Reviews dataset as an illustrative example, Personalized Memory includes individualized user information, such as their reviews or ratings for a particular item. World Knowledge consists of two components: the first component is item metadata information, which also falls under the domain-specific knowledge category; the second component involves real-time information that can be accessed through Web search tool. In Figure 1, information of the product "Sewak Al-Falah" retrieved from world knowledge using a Web search tool, aids the reasoning path and ultimately influences the final prediction.</p>
<p>Tool Use By empowering LLMs to utilize tools, we can access vastly larger and dynamic knowledge bases, allowing us to tackle complex computational tasks. In RecMind system, we’ve incorporated three such tools:</p>
<ul>
<li>Database Tool: This tool translates natural language questions into SQL queries. Using this tool, the system can access domain-specific knowledge from memory that's essential for the final prediction. For instance, in the Amazon Reviews dataset, it encompasses personal information such as a user's reviews or ratings for an item, as well as item metadata like the item's description, brand, and price. When the database tool is called, the agent will prompt a question, such as "What is the average rating of product Sewak Al-Falah?", based on the database schema. Next, an LLM is called to transfer the question into an executable SQL query. The output of the SQL execution will then be passed to the agent.</li>
<li>Search Tool: This tool employs a search engine (e.g., Google) to access real-time information. For instance, in the Amazon Reviews dataset, this tool could assist us in obtaining the most recent information about each item. When the Search tool is called, the agent will prompt a question asking for external meta information, which is usually not available in the database, such as "What is the product category of Sewak Al-Falah?". Next, a search engine API will be called to search for the information and return it to the agent.</li>
<li>Text Summarization Tool: This tool helps summarize lengthy texts by invoking a text summarization model from the Hugging Face Hub. For example, within the Amazon Reviews dataset, this tool can produce a summarized description of an item by considering multiple reviews of that specific item from various users. It can generate summarization such as "Most customers think this product is durable and has a good price.", which can be easily used in different recommendation tasks related to the product.</li>
</ul>
<p>Details on the prompts for using the tools and executing self-inspiring are deferred to Section A of the supplementary.</p>
<h2>4 Experiments</h2>
<p>We evaluate the performance of the RecMind agent in various recommendation scenarios, i.e., rating
prediction, sequential recommendation, direct recommendation, explanation generation, review summarization. First, we provide an overview of the datasets and evaluation metrics in Section 4.1 and Section 4.2. Subsequently, we present the experimental settings and results of RecMind on each recommendation task in Section 4.3 and Section 4.4. Next, we study the domain transfer capability of RecMind in Section 4.5 and how RecMind performs with different foundation LLMs 4.6. In the end, we further explore how the performance of SI in general reasoning tasks in 4.7 and the running time of RecMind based on SI compared to ToT. The comparison on running time deferred to Section B. 3 of the supplementary shows that RecMind based on SI takes less inference time than the existing state-of-the-art diverse reasoning method ToT.</p>
<h3>4.1 Experimental Settings</h3>
<p>Following P5 (Geng et al., 2022), we conduct experiments on the Amazon Reviews (Ni et al., 2019) dataset. Since Amazon Reviews contains textual reviews and titles, it provides us the chance to explore how textual contents are utilized in performing recommendation tasks with LLM models. We evaluate RecMind and baselines on data in Sports \&amp; Outdoors, Beauty, as well as Toys \&amp; Games domains from Amazon Reviews. For a more comprehensive evaluation, we also evaluate the RecMind on Yelp (Geng et al., 2022) dataset. We show the results on the Beauty domain of Amazon Reviews and Telp in Section 4.3 and Section 4.4. The results on the Sports and Toys domains of Amazon Reviews are deferred to Section B. 4 of the supplementary.</p>
<p>To quantitatively evaluate the proposed RecMind across various recommendation tasks, we employ different metrics. For rating prediction, we report Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). In the case of sequential and direct recommendations, we use metrics such as top- $k$ Hit Ratio (HR@ $k$ ) and top- $k$ Normalized Discounted Cumulative Gain (NDCG@ $k$ ), specifically reporting results on HR@5,10 and NDCG@5,10. In addition, for the assessment of explanation generation, review summarization and conversational recommendation, we use $n$-gram Bilingual Evaluation Understudy (BLEU-n) and $n$-gram Recall-Oriented Understudy for Gisting Evaluation (ROUGE- $n$ ).</p>
<p>We use gpt-3.5-turbo-16k (Schulman et al.,</p>
<p>2022) as the core large language model in RecMind. To enable the access of RecMind to indomain knowledge, we store all the review data in a MySQL database, consisting of a table with the product meta information and a table with the interaction history of all the users.</p>
<h3>4.2 Compared Methods</h3>
<p>We compare the performance of RecMind with the following baselines, including both LLM finetuning methods, such as P5 (Geng et al., 2022), and ChatGPT prompting methods (Liu et al., 2023). In addition, we implement RecMind with three different planning methods, namely Chain-Of-Thoughts (CoT), Tree-of-Thoughts (ToT) (Yao et al., 2023), and the proposed Self-Inspiring(SI). In summary, the compared methods include:</p>
<ul>
<li>P5 (Geng et al., 2022) unifies different recommendation tasks into a shared generative large language model. A collection of personalized prompts has been created for various recommendation-related tasks. All raw data including user-item interactions, user descriptions, item metadata, and users' reviews are transformed into natural language sequences. Subsequently, the large language model is fine-tuned based on these sequences. In our evaluation, to avoid the influence of factors such as randomness in selecting recommendation candidates, we run the pre-trained P5 model loaded from the Hugging Face repo https://huggingface.co/ makitanikaze/P5 on the same test data we use to evaluate our method.</li>
<li>ChatGPT (Liu et al., 2023) is a powerful large language model developed by OpenAI. (Liu et al., 2023) constructs a benchmark to evaluate ChatGPT's performance in different recommendation tasks by designing specific prompts in both zero-</li>
</ul>
<p>Table 1: Performance comparison in rating prediction on Amazon Reviews (Beauty) and Yelp.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Beauty</th>
<th></th>
<th>Yelp</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>RMSE</td>
<td>MAE</td>
<td>RMSE</td>
<td>MAE</td>
</tr>
<tr>
<td>MF</td>
<td>1.1973</td>
<td>0.9461</td>
<td>1.2645</td>
<td>1.0426</td>
</tr>
<tr>
<td>MLP</td>
<td>1.3078</td>
<td>0.9597</td>
<td>1.2951</td>
<td>1.0340</td>
</tr>
<tr>
<td>AFM</td>
<td>1.1097</td>
<td>0.8815</td>
<td>1.2530</td>
<td>1.0019</td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>1.2982</td>
<td>0.8474</td>
<td>1.4685</td>
<td>1.0054</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>1.4173</td>
<td>1.1897</td>
<td>1.6725</td>
<td>1.2359</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>1.1589</td>
<td>0.7327</td>
<td>1.4725</td>
<td>1.0016</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>1.2250</td>
<td>0.8612</td>
<td>1.5302</td>
<td>1.1673</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>1.1326</td>
<td>0.7167</td>
<td>1.3925</td>
<td>0.9794</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>1.1972</td>
<td>0.8135</td>
<td>1.4956</td>
<td>1.0755</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>1.1197</td>
<td>0.7059</td>
<td>1.3875</td>
<td>0.9766</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>1.2006</td>
<td>0.8279</td>
<td>1.4937</td>
<td>1.1076</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>1.1205</td>
<td>0.7103</td>
<td>1.3826</td>
<td>0.9774</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>1.1894</td>
<td>0.7883</td>
<td>1.4530</td>
<td>1.0009</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>1.0756</td>
<td>0.6892</td>
<td>1.3674</td>
<td>0.9698</td>
</tr>
</tbody>
</table>
<p>shot and few-shot settings. In the zero-shot setting, the LLM is directly prompted for the final prediction, while in the few-shot setting, several in-context examples are provided. We name the ChatGPT baseline in these two settings as ChatGPT (zero-shot) and ChatGPT (few-shot).</p>
<ul>
<li>RecMind-CoT, where the planning is based on ReAct-CoT (Yao et al., 2022). ReAct is a novel prompt-based paradigm for general task solving. It extends Chain-Of-Thoughts (CoT) (Wei et al., 2022) to synergize reasoning and acting with external tools. In our experiments, we adopt the same tools we used for the ReAct baseline. We also explore both zero-shot and few-shot for this method and name them as RecMind-CoT (zeroshot) and RecMind-CoT (few-shot).</li>
<li>RecMind-ToT, where the planning is based on Tree-of-Thoughts (ToT) (Yao et al., 2023). ToT enables the exploration of coherent units of thought that serve as intermediate steps toward problem-solving. We implement RecMindToT with two strategies in searching among the choices in intermediate steps, which are breadthfirst search, named as RecMind-CoT (BFS, fewshot) and depth-first search, named as RecMindCoT (DFS, few-shot).
In addition to the above methods, we have considered different additional baselines for each task. The additional baselines are introduced in corresponding subsections. Details on the prompts for baseline methods are deferred to A of the supplementary.</li>
</ul>
<h3>4.3 Results on Precision-oriented Recommendation Tasks</h3>
<p>We first evaluate RecMind and baselines on three precision-oriented recommendation tasks, i.e., rating prediction, sequential recommendation, and direct recommendation.</p>
<p>Rating Prediction. Rating prediction is an essential task in recommendation systems that aims to predict the rating that a user would give to a particular item. In rating prediction, we further traditional recommendation baselines matrix factorization (MF) (Koren et al., 2009a), multi-layer perception (MLP) (Cheng et al., 2016), and attentional factorization machines (AFM) (Xiao et al., 2017) trained with mean square root loss baselines. The results of rating prediction on Amazon Reviews (beauty domain) and Yelp are shown in Table 1. The results show that RecMind with different</p>
<p>Table 2: Performance comparison in direct recommendation on Amazon Reviews (Beauty) and Yelp.</p>
<p>| Methods | Beauty | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>Table 4: Performance comparison on explanation generation on Amazon Reviews (Beauty) and Yelp.</p>
<p>| Methods | Beauty | | | | Yelp | | | |
| | BLEU2 | ROGUE1 | ROGUE2 | ROGUEL | BLEU2 | ROGUE1 | ROGUE2 | ROGUEL |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| P5 (pre-trained expert,few-shot) | 0.9783 | 17.0412 | 1.8962 | 12.1709 | 1.2784 | 18.1924 | 2.9517 | 13.2315 |
| ChatGPT (zero-shot) | 0.0359 | 9.7892 | 0.7994 | 5.1215 | 0.0419 | 8.9776 | 0.8549 | 6.1715 |
| ChatGPT (few-shot) | 1.1766 | 11.8905 | 2.5894 | 5.8920 | 1.1766 | 12.0901 | 3.2170 | 6.7823 |
| RecMind-CoT (zero-shot) | 0.8985 | 11.0597 | 1.9675 | 7.7471 | 1.1052 | 12.5719 | 2.1941 | 7.7471 |
| RecMind-CoT (few-shot) | 1.3096 | 12.7987 | 2.7015 | 8.0164 | 1.2759 | 13.9690 | 3.0173 | 9.1081 |
| RecMind-ToT (BFS, zero-shot) | 1.0279 | 11.1584 | 2.1024 | 7.7026 | 1.1135 | 11.7230 | 2.2355 | 7.7910 |
| RecMind-ToT (BFS, few-shot) | 1.3054 | 12.8249 | 2.7050 | 8.0596 | 1.2960 | 14.1728 | 3.4539 | 9.6125 |
| RecMind-ToT (DFS, zero-shot) | 1.0319 | 11.3564 | 2.1416 | 7.7166 | 1.1795 | 11.8433 | 2.2416 | 7.8252 |
| RecMind-ToT (DFS, few-shot) | 1.3159 | 12.8975 | 2.7125 | 8.1150 | 1.2896 | 14.2201 | 3.6710 | 9.6719 |
| RecMind-SI (zero-shot) | 1.1589 | 11.6794 | 2.2460 | 7.8974 | 1.1589 | 11.6794 | 2.2460 | 7.8974 |
| RecMind-SI (few-shot) | 1.3459 | 13.2560 | 2.7479 | 8.9614 | 1.3094 | 14.4220 | 3.8974 | 9.7125 |</p>
<p>items in chronological order: ['Item List']. Please recommend the next item that the user might interact with. Choose the top 10 products to recommend in order of priority, from highest to lowest.". We include traditional recommendation baselines $\mathrm{S}^{3}$-Rec (Zhou et al., 2020) and SASRec (Kang et al., 2018). The results in Table 3 show that RecMind with Self-Inspiring achieves comparable performance as fully-trained models P5 and $\mathrm{S}^{3}$-Rec. Without diverse planning methods such as tree-ofthoughts and our proposed self-inspiring, LLMs prefer items whose names are semantically similar to those of proceeding items. In contrast, with the help of explicit reasoning methods and access to domain knowledge, RecMind gradually explores helpful information, such as connections of items in the database with other users' interaction history.</p>
<h3>4.4 Results on Explainability-oriented Recommendation Tasks</h3>
<p>With the development of NLP techniques on recommendation tasks, recent works (Geng et al., 2022) have started to explore how NLP models can improve the explainability of recommendation systems, such as generating text explanations for a given interaction between a user and an item. In this section, we evaluate the performance of RecMind in two explainability-oriented recommendation tasks, which are explanation generation and review summarization. The results on explanation generation are shown in Table 4. The results on review summarization are deferred to Section B.1 of the supplementary.</p>
<p>Explanation Generation. In explanation generation, we assess the performance of RecMind in crafting textual explanations that justify a user's interaction with a specific item. Figure 2 shows an example of explanation generation in the beauty domain of Amazon Reviews. The text review given
by the user on the given item is taken as the ground truth. The results on explanation generation in Table 4 indicates that RecMind, when leveraging self-inspiring techniques, can achieve performance comparable to the fully trained P5 model. This is aided by the in-domain knowledge retrieved from personalized memory, such as reviews from other users on the same item. To better evaluate the quality and rationality of the explanation generated by RecMind and compare the results with baseline models, we perform human evaluation on the generated evaluation. The evaluation details and results are deferred to Section B. 2 of the supplementary.</p>
<h3>4.5 Transfer to Items in Unseen Domains</h3>
<p>Table 5: Performance on domain transfer. Comparisons are performed on MAE for rating prediction, HR@5 for direct recommendation, and BLEU2 for explanation generation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">MAE $\downarrow$</th>
<th style="text-align: center;">HR@5 $\uparrow$</th>
<th style="text-align: center;">BLEU2 $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">P5</td>
<td style="text-align: center;">Beauty $\rightarrow$ Toys</td>
<td style="text-align: center;">0.7932</td>
<td style="text-align: center;">0.0852</td>
<td style="text-align: center;">1.4326</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beauty $\rightarrow$ Sports</td>
<td style="text-align: center;">0.7013</td>
<td style="text-align: center;">0.1007</td>
<td style="text-align: center;">0.8924</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Beauty $\rightarrow$ Toys</td>
<td style="text-align: center;">0.7354</td>
<td style="text-align: center;">0.0649</td>
<td style="text-align: center;">1.4416</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beauty $\rightarrow$ Sports</td>
<td style="text-align: center;">0.6895</td>
<td style="text-align: center;">0.7210</td>
<td style="text-align: center;">0.8795</td>
</tr>
<tr>
<td style="text-align: center;">RecMind-ToT</td>
<td style="text-align: center;">Beauty $\rightarrow$ Toys</td>
<td style="text-align: center;">0.6845</td>
<td style="text-align: center;">0.0841</td>
<td style="text-align: center;">1.3994</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beauty $\rightarrow$ Sports</td>
<td style="text-align: center;">0.6457</td>
<td style="text-align: center;">0.0924</td>
<td style="text-align: center;">1.0002</td>
</tr>
<tr>
<td style="text-align: center;">RecMind-SI</td>
<td style="text-align: center;">Beauty $\rightarrow$ Toys</td>
<td style="text-align: center;">0.6779</td>
<td style="text-align: center;">0.0902</td>
<td style="text-align: center;">1.5940</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beauty $\rightarrow$ Sports</td>
<td style="text-align: center;">0.6245</td>
<td style="text-align: center;">0.1124</td>
<td style="text-align: center;">1.0537</td>
</tr>
</tbody>
</table>
<p>The advantage of using a large language model as a unified recommendation model is that it can judge the likelihood of any event by expressing the event in natural language. In our experiments in Section 4.3, we found that RecMind with indomain few-shot examples achieves much better performance. In this section, we aim to test if the in-domain few-shot examples can generalize to unseen domains, so no parameters need to be trained in such domain transfer experiments. Specifically, we include few-shot examples in the Beauty domain and test the performance of RecMind on rating prediction, direct recommendation, and explanation generation with test data in the Toys and</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Performance comparison of RecMind-SI with different types of foundation LLMs.</p>
<p>Sports domain. We include ChatGPT prompting baseline and P5 for comparisons. In the few-shot ChatGPT baseline, the user-specific examples included in the prompts are from the Beauty domain. In the P5, the model trained on the Beauty domain is used for evaluation. We evaluate the domain transfer capabilities of all approaches on rating prediction, direct recommendation, and explanation generation. We report the MAE for rating prediction, HR@5 for direct recommendation, and the BLEU2 for explanation in Table 5. It can be observed that RecMind shows better domain transfer performance compared with the baselines P5 and ChatGPT. In contrast, fine-tuned language model P5 tends to overfit to the domain of the training data.</p>
<h3>4.6 Ablation Study on Foundation LLMs</h3>
<p>In this section, we study how RecMind performs with different types of foundation LLMs as the controller. We test RecMind-SI using different types of LLMs, including Llama2 70b (Touvron et al., 2023a), GPT-3.5, text-davinci-003, and GPT-4, for sequential recommendation on three different domains in Amazon Reviews. In each domain, we randomly sample 500 test data for evaluation. We run the evaluation on each model five times and calculate the mean and standard deviation of different runs. The results are shown in Figure 4. The results show that the performance of RecMind-SI is not sensitive to the selection of Foundation LLMs. Although RecMind-SI with GPT-4 demonstrates enhanced reasoning in addressing complex problems, RecMind-SI with GPT-3.5 can also deliver commendable performance when leveraging the superior capabilities of the RecMind framework. RecMind-SI with Llama2 70b, also achieves pretty good performance. However, due to its limited input context length, the performance with Llama2 has a larger variance.</p>
<h3>4.7 Experiments in general reasoning scenarios</h3>
<p>To show that our proposed self-inspiring (SI) method not only outperforms CoT and ToT on recommendation tasks but also on general reasoning scenarios. We evaluate SI on two additional reasoning tasks from [2], which are Game of 24 and Mini Crosswords. We follow the same experimental set-
tings as in ToT [2]. In both tasks, ToT explores the 5 best candidate thoughts at each intermediate step. For a fair comparison, we also set the maximum number of alternative thoughts at each step as 5. We set the maximum number of intermediate steps for the Mini crosswords task to 100 following ToT. GPT-4 backend is used for CoT, ToT, and our SI. The results are shown in Table 6 and Table 7. It can be observed that SI outperforms CoT and ToT on both tasks.</p>
<p>Table 6: Experiment Results for Game of 24.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>CoT</th>
<th>ToT</th>
<th>SI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>4 %</td>
<td>74 %</td>
<td>80 %</td>
</tr>
</tbody>
</table>
<p>Table 7: Experiment Results for Mini Crosswords.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>CoT</th>
<th>ToT</th>
<th>SI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Letter-level Accuracy</td>
<td>40.6 %</td>
<td>78 %</td>
<td>81 %</td>
</tr>
<tr>
<td>Word-level Accuracy</td>
<td>15.6 %</td>
<td>60 %</td>
<td>65 %</td>
</tr>
<tr>
<td>Game-level Accuracy</td>
<td>1 %</td>
<td>20 %</td>
<td>26 %</td>
</tr>
</tbody>
</table>
<h2>5 Conclusions</h2>
<p>In this work, we propose a novel LLM-powered autonomous agent, RecMind, for various recommendation tasks. The RecMind consists of three major components, i.e., planning, which breaks down a task into smaller sub-tasks; memory, which provides the agent with the capability to retain and recall information over extended periods; and external tools for obtaining relevant extra information from memory that is missing from model weights. We further propose a novel planning technique self-inspiring, which can integrate the merits of exploring multiple reasoning paths for better planning. We evaluate RecMind across various recommendation tasks, including both precision-oriented tasks and explanability-oriented tasks. The evaluation results show that RecMind with self-inspiring outperforms existing LLM-based recommendation methods in different recommendation tasks and achieves comparable performance to a recent model P5, which is fully trained for the recommendation task. Future works can explore utilizing more external tools in our recommendation agent.</p>
<h2>Limitations</h2>
<p>One major limitation of our work is that more exploration of diverse reasoning paths greatly increases the prompt size, leading to well-known limitations of LLMs in long contexts and position bias. A future direction could be implementing a summarization step for historical paths, which might not only condense the long context but also potentially remove some of the noise in historical paths. In addition, only a small number of external tools are adopted in our current implementation.</p>
<h2>Ethical Concerns and Broader Impacts</h2>
<p>All experiments in our papers are performed on two widely used recommendation datasets, which are Amazon Reviews (Ni et al., 2019) and Yelp (Ni et al., 2019). To protect users' privacy, both datasets adopt anonymous user IDs to represent user identity. We follow the terms of use for both datasets and only use the datasets for academic purposes. The LLM-based recommendation system proposed in this work has the potential to influence consumer behavior and preferences. In addition, we have tested the method on top of different LLM models, including online and offline models, to avoid potential biases in pre-trained LLMs such as ChatGpt (Schulman et al., 2022).</p>
<h2>References</h2>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Harrison Chase. 2023. langchain. GitHub repository.
Chong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu, and Shaoping Ma. 2020. Efficient neural matrix factorization without sampling for recommendation. ACM Transactions on Information Systems (TOIS), 38(2):1-28.</p>
<p>Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide \&amp; deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pages 7-10.</p>
<p>Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046.</p>
<p>Shijie Geng et al. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt \&amp; predict paradigm (p5). In 16th ACM Conference on Recommender Systems.</p>
<p>Significant Gravitas. 2023. Auto-gpt. GitHub repository.</p>
<p>Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 639-648.</p>
<p>Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939.</p>
<p>Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845.</p>
<p>Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do llms understand user preferences? evaluating llms on user rating prediction. arXiv preprint arXiv:2305.06474.</p>
<p>Wang-Cheng Kang et al. 2018. Self-attentive sequential recommendation. In ICDM.</p>
<p>Yehuda Koren, Robert Bell, and Chris Volinsky. 2009a. Matrix factorization techniques for recommender systems. Computer, 42(8):30-37.</p>
<p>Yehuda Koren, Robert M. Bell, and Chris Volinsky. 2009b. Matrix factorization techniques for recommender systems. Computer, 42.</p>
<p>Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2023. How can recommender systems benefit from large language models: A survey. ArXiv, abs/2306.05817.</p>
<p>Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon.com recommendations: Item-to-item collaborative filtering. IEEE Distributed Syst. Online, 4.</p>
<p>Junling Liu et al. 2023. Is chatgpt a good recommender? a preliminary study. ArXiv, abs/2304.10149.</p>
<p>Yohei Nakajima. 2023. babyagi. GitHub repository.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLPIJCNLP), pages 188-197.</p>
<p>R OpenAI. 2023. Gpt-4 technical report. arXiv, pages 2303-08774.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761.</p>
<p>John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. 2022. Chatgpt: Optimizing language models for dialogue. OpenAI blog.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Fei Sun et al. 2019. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti</p>
<p>Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. ArXiv, abs/2304.03153.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617.</p>
<p>Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. 2023. Palr: Personalization aware llms for recommendation. arXiv e-prints, pages arXiv-2305.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In CIKM.</p>
<h2>A Additional Implementation Details</h2>
<p>Tool Descriptions in Agent Prompt To enable the LLM-based Agent to utilize external tools, the LLM Agent will be prompted an instruction with descriptions on different tools. The prompt is formulated as:
Perform a recommendation task with interleaving Thought, Action, and Observation steps. Thought can reason about the current situation, and Action can be the following types:</p>
<ul>
<li>SQL Tool: "SQL {question}, which aims to search for the answer to a question from the database. You can only put forward questions based on the available information in the database. Available information and schema of the database is provided in {database_info}."</li>
<li>Text Summarization Tool: "Summarize {content}, which condenses extensive text into a shorter version while retaining the core information and meaning by using a pre-trained text summarization model."</li>
<li>Search Tool: "Search {question}, which formulates a search query for Google search engine based on the question. This tool can be used to search for information that is unavailable in the database."</li>
<li>Finish: "Finish {answer}, which returns the answer and finishes the task."</li>
</ul>
<p>Search Tool Prompt In the search tool, we use SerpApi.com as our Google search API. Since the output of the search API is in a structured JSON format, we use the same LLM model of the agent to convert the output to a text response and then return it to the LLM agent. The prompt we use is "Your mission is to convert the Google search result {search_result} from search engine to meaningful sentences, which can be a response to question {question}."
SQL Tool Prompt In the SQL tool, we use the same LLM model of the agent to convert the question to an SQL query. The prompt we use in this text-to-SQL process is "Your mission is to convert SQL query from given {question}. The information about the tables in the database is {database_info}. Only output the SQL query." Next, the obtained SQL query will be executed. Similar to the search tool, the output will then be converted to a text response to the question and returned to the LLM agent. The prompt we use to convert the output is "Your mission is to convert SQL query execution results to meaningful sentences, which should be the answer to the question {question}. The query generated for this question is ${$ sql_query $}$. Here is the database result: {sql_result}"
Self-Inspiring Prompt In the implementation of self-inspiring, the same LLM model of the agent is used to decide whether another thought is necessary given the task and previously explored steps. The prompt for this request is "You are given multi-step problem-solving steps towards finishing the task ${$ task $}$. The previous steps are ${$ previous_steps $}$. You already have the thought, action, and observation in the current step {current_step}. Your mission is to decide if there is an alternative thought in the current step that can help finish this task following the previous steps. If there is, directly output the thought. If not, please respond {empty_response $}$."</p>
<p>For ChatGPT (zero-shot) and ChatGPT (fewshot), we use the exact same prompt templates from (Liu et al., 2023). We will attach the prompt templates for all baseline methods in the appendix of the revised version of our paper. We follow (Yao et al., 2022) to design the prompt for CoT. The prompt is "Solve a recommendation task with interleaving Thought, Action, and Observation steps." We follow (Yao et al., 2023) to design the prompts for ToT. In addition to the general instruction, "Solve a recommendation task with interleaving Thought, Action, and Observation steps", we also designed prompts for thought-sampling and decision-making. The thought sampling prompt is "Given the previous {previous_steps}, list five possible thoughts for the next step towards finishing the task ${$ task $}$." The decision-making prompt is "Given an instruction and several choices, decide which choice is most promising. Your instruction is {task_sepcific_instruction}. Your available options are {option_list}. Analyze each choice, then conclude in the last line, 'The best choice is ${\mathrm{s}}$ ', where s is the integer id of the choice."</p>
<h2>B Additional Experiment Results</h2>
<h2>B. 1 Results on Review Summarization</h2>
<p>In the review summarization task, we evaluate the performance of RecMind in summarizing review comments to shorter review titles. We filter out test data with automatically generated review titles</p>
<p>such as 'Five Stars'. Figure 2 shows an example of review summarization in the beauty domain of Amazon Reviews. The results of the review summarization on Amazon Reviews are shown in Table 8. The result shows that RecMind agent performs better that recent LLM such as ChatGPT. However, RecMind does not outperform P5 regarding the review summarization. This performans comes from the advantage of P5 which fully trained model towards optimizaing the review summarization task. In contrast, GPT-based models, such as RecMind, usually prioritize generating summaries after deeply understanding the reviews.</p>
<p>Table 8: Performance comparison on review summarization on Amazon Reviews (Beauty).</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Beauty</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BLEU2</td>
<td>ROGUE1</td>
<td>ROGUE2</td>
<td>ROGUEL</td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>2.0357</td>
<td>8.3079</td>
<td>1.5892</td>
<td>7.4820</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.6532</td>
<td>3.8579</td>
<td>0.3059</td>
<td>3.3552</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>0.9137</td>
<td>4.0179</td>
<td>0.4179</td>
<td>3.6790</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>1.3596</td>
<td>5.0279</td>
<td>0.7156</td>
<td>4.7689</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>1.3786</td>
<td>5.5397</td>
<td>0.8456</td>
<td>4.8024</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>1.3592</td>
<td>5.1103</td>
<td>0.7596</td>
<td>4.8069</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>1.3737</td>
<td>5.4187</td>
<td>0.8254</td>
<td>4.8157</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>1.3614</td>
<td>5.1435</td>
<td>0.7749</td>
<td>4.7985</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>1.3798</td>
<td>5.5794</td>
<td>0.8351</td>
<td>4.8976</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>1.3688</td>
<td>5.4579</td>
<td>0.8974</td>
<td>4.9746</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>1.4014</td>
<td>6.0354</td>
<td>1.0128</td>
<td>5.5716</td>
</tr>
</tbody>
</table>
<h2>B. 2 Human Evaluation</h2>
<p>In this section, we leverage human evaluation to assess the quality and rationality of the explanation generated by RecMind. Three human evaluators (Eva_1, Eva_2, Eva_3) are asked to rank the explanations generated by P5, few-shot ChatGPT, few-shot RecMind with tree-of-thoughts, few-shot RecMind with self-inspiring and the ground truth on 100 test data. We show the top-1 ratios on results generated by different methods in Table 9 for each evaluator. The top-1 ratio indicates the proportion of test data where the given method ranks first compared to other methods based on each annotator's selection. We also calculate the average top-1 ratios of all three evaluators on results generated by each method. Although annotators may have individual subjectivity, evaluations by different evaluators consistently show that the few-shot RecMind based on self-inspiring, i.e., RecMind-SI yields the most satisfactory results.</p>
<p>Table 9: Human evaluation results on explanation generation.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Evaluator</th>
<th></th>
<th></th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Eva_1</td>
<td>Eva_2</td>
<td>Eva_3</td>
<td></td>
</tr>
<tr>
<td>Ground Truth</td>
<td>0.12</td>
<td>0.13</td>
<td>0.22</td>
<td>0.157</td>
</tr>
<tr>
<td>P5</td>
<td>0.02</td>
<td>0.02</td>
<td>0.03</td>
<td>0.037</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>0.15</td>
<td>0.23</td>
<td>0.18</td>
<td>0.187</td>
</tr>
<tr>
<td>RecMind-ToT</td>
<td>0.29</td>
<td>0.28</td>
<td>0.25</td>
<td>$\underline{0.275}$</td>
</tr>
<tr>
<td>RecMind-SI</td>
<td>0.42</td>
<td>0.30</td>
<td>0.32</td>
<td>$\mathbf{0 . 3 4 7}$</td>
</tr>
</tbody>
</table>
<h2>B. 3 Running Time Analysis</h2>
<p>In this section, we provide a running time comparison between our proposed reasoning method SI and previous reasoning methods for the recommendation agent. We run RecMind with CoT, ToT, and SI on 100 randomly sampled test data from the Beauty domain of Amazon Reviews and calculate the average running time. We use GPT-3.5 as the base model. The results in Table 10 show that our proposed self-inspiring can not only improve the performance of the LLM-powered agent but also take less inference time than the existing state-of-the-art diverse reasoning method ToT. Such merit mainly stems from the fact that SI only explores alternative options at an intermediate step when it recognizes that the explored options at that step are not good enough. In contrast, ToT directly samples multiple options for exploration, which can lead to a waste of time.</p>
<p>Table 10: Average Running Time of RecMind with Different Reasoning Methods.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>CoT</th>
<th>ToT</th>
<th>SI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Average Running Time (s)</td>
<td>18.9 s</td>
<td>53.2 s</td>
<td>29.7 s</td>
</tr>
</tbody>
</table>
<h2>B. 4 Results on Sports and Toys Domains in Amazon Reviews</h2>
<p>In this section, we provide additional experiment results of RecMind and all compared methods on the Sports domain and Toys domain in Amazon Reviews. The results in rating prediction on the Sports and Toys domains of Amazon Reviews are shown in Table 11. The results in the direct recommendation and sequential recommendation on the Sports domain of Amazon Reviews are shown in Table 12. The results in the direct recommendation and sequential recommendation on the Toys domain of Amazon Reviews are shown in Table 13. The results in text summarization and explanation</p>
<p>Table 11: Performance comparison in rating prediction on Sports and Toys domains of Amazon Reviews.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Sports</th>
<th></th>
<th>Toys</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>RMSE</td>
<td>MAE</td>
<td>RMSE</td>
<td>MAE</td>
</tr>
<tr>
<td>MF</td>
<td>1.0274</td>
<td>0.7975</td>
<td>1.0193</td>
<td>0.8024</td>
</tr>
<tr>
<td>MLP</td>
<td>1.1277</td>
<td>0.7626</td>
<td>1.1215</td>
<td>0.8097</td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>1.0534</td>
<td>0.6784</td>
<td>1.0625</td>
<td>0.7134</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>1.2723</td>
<td>1.0637</td>
<td>1.3213</td>
<td>1.0117</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>1.0929</td>
<td>0.6957</td>
<td>1.0519</td>
<td>0.7047</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>1.1490</td>
<td>0.8042</td>
<td>1.1680</td>
<td>0.8232</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>1.0325</td>
<td>0.6446</td>
<td>1.0403</td>
<td>0.6905</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>1.1322</td>
<td>0.8014</td>
<td>1.1559</td>
<td>0.8164</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>1.0307</td>
<td>0.6289</td>
<td>1.0279</td>
<td>0.6823</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>1.1366</td>
<td>0.8021</td>
<td>1.1537</td>
<td>0.8155</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>1.0545</td>
<td>0.6433</td>
<td>1.0196</td>
<td>0.6801</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>1.1230</td>
<td>0.7913</td>
<td>1.1412</td>
<td>0.8103</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>1.0124</td>
<td>0.6122</td>
<td>1.0086</td>
<td>0.6712</td>
</tr>
</tbody>
</table>
<p>generation on the Sports domain of Amazon Reviews are shown in Table 14. The results in text summarization and explanation generation on the Toys domain of Amazon Reviews are shown in Table 15. As indicated in the experimental results, RecMind also performs well in different recommendation tasks on data from other domains of Amazon Reviews.</p>
<p>Table 12: Performance comparison in direct recommendation and sequential recommendation on Sports domain of Amazon Reviews.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Sports</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>HR@5</td>
<td>NDCG@5</td>
<td>HR@10</td>
<td>NDCG@10</td>
</tr>
<tr>
<td>Direct Recommendation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BPR-MLP</td>
<td>0.1520</td>
<td>0.0927</td>
<td>0.2671</td>
<td>0.1296</td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>0.1765</td>
<td>0.1196</td>
<td>0.2235</td>
<td>0.1325</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.0376</td>
<td>0.0317</td>
<td>0.0902</td>
<td>0.0459</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>0.0388</td>
<td>0.0267</td>
<td>0.1003</td>
<td>0.0502</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>0.0607</td>
<td>0.0435</td>
<td>0.1259</td>
<td>0.0757</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>0.0782</td>
<td>0.0527</td>
<td>0.1475</td>
<td>0.1034</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>0.0741</td>
<td>0.0512</td>
<td>0.1320</td>
<td>0.1054</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>0.0874</td>
<td>0.0542</td>
<td>0.1475</td>
<td>0.1218</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>0.0759</td>
<td>0.0519</td>
<td>0.1320</td>
<td>0.1079</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>0.0815</td>
<td>0.0557</td>
<td>0.1412</td>
<td>0.1272</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>0.0835</td>
<td>0.0684</td>
<td>0.1379</td>
<td>0.1103</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>0.1115</td>
<td>0.0814</td>
<td>0.1769</td>
<td>0.1303</td>
</tr>
<tr>
<td>Sequential Recommendation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>S3-Rec</td>
<td>0.0251</td>
<td>0.0161</td>
<td>0.0385</td>
<td>0.0204</td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>0.0357</td>
<td>0.0289</td>
<td>0.0416</td>
<td>0.0324</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.0039</td>
<td>0.0008</td>
<td>0.0051</td>
<td>0.0008</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>0.0130</td>
<td>0.0075</td>
<td>0.0207</td>
<td>0.0070</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>0.0135</td>
<td>0.0090</td>
<td>0.0248</td>
<td>0.0105</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>0.0300</td>
<td>0.0138</td>
<td>0.0437</td>
<td>0.0247</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>0.0205</td>
<td>0.0134</td>
<td>0.0319</td>
<td>0.0243</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>0.0338</td>
<td>0.0186</td>
<td>0.0473</td>
<td>0.0272</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>0.0218</td>
<td>0.0130</td>
<td>0.0336</td>
<td>0.0238</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>0.0316</td>
<td>0.0162</td>
<td>0.0448</td>
<td>0.0260</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>0.0290</td>
<td>0.0151</td>
<td>0.0420</td>
<td>0.0255</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>0.0366</td>
<td>0.0240</td>
<td>0.0525</td>
<td>0.0320</td>
</tr>
</tbody>
</table>
<p>Table 13: Performance comparison in direct recommendation and sequential recommendation on Toys domain of Amazon Reviews.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Toys</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>HR@5</td>
<td>NDCG@5</td>
<td>HR@10</td>
<td>NDCG@10</td>
</tr>
<tr>
<td>Direct Recommendation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BPR-MLP</td>
<td>0.1142</td>
<td>0.0688</td>
<td>0.2077</td>
<td>0.0988</td>
</tr>
<tr>
<td>P5 (pre-trained,few-shot)</td>
<td>0.1278</td>
<td>0.0743</td>
<td>0.1859</td>
<td>0.1089</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.0114</td>
<td>0.0075</td>
<td>0.0638</td>
<td>0.0191</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>0.0130</td>
<td>0.0059</td>
<td>0.0805</td>
<td>0.0270</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>0.0399</td>
<td>0.0233</td>
<td>0.1031</td>
<td>0.0542</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>0.0580</td>
<td>0.0295</td>
<td>0.1247</td>
<td>0.0719</td>
</tr>
<tr>
<td>RecMind-ToT (BFS,zero-shot)</td>
<td>0.0496</td>
<td>0.0297</td>
<td>0.1079</td>
<td>0.0697</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>0.0636</td>
<td>0.0300</td>
<td>0.1257</td>
<td>0.0813</td>
</tr>
<tr>
<td>RecMind-ToT (DFS,zero-shot)</td>
<td>0.0510</td>
<td>0.0301</td>
<td>0.1094</td>
<td>0.0712</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>0.0603</td>
<td>0.0315</td>
<td>0.1204</td>
<td>0.0817</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>0.0577</td>
<td>0.0432</td>
<td>0.1161</td>
<td>0.0828</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>0.0813</td>
<td>0.0532</td>
<td>0.1461</td>
<td>0.0998</td>
</tr>
<tr>
<td>Sequential Recommendation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>S3-Rec</td>
<td>0.0443</td>
<td>0.0294</td>
<td>0.0700</td>
<td>0.0376</td>
</tr>
<tr>
<td>P5 (pre-trained,few-shot)</td>
<td>0.0612</td>
<td>0.0524</td>
<td>0.0702</td>
<td>0.0569</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.0192</td>
<td>0.0158</td>
<td>0.0212</td>
<td>0.0165</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>0.0282</td>
<td>0.0231</td>
<td>0.0367</td>
<td>0.0230</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>0.0285</td>
<td>0.0246</td>
<td>0.0408</td>
<td>0.0265</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>0.0452</td>
<td>0.0294</td>
<td>0.0597</td>
<td>0.0407</td>
</tr>
<tr>
<td>RecMind-ToT (BFS,zero-shot)</td>
<td>0.0399</td>
<td>0.0287</td>
<td>0.0495</td>
<td>0.0359</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>0.0490</td>
<td>0.0342</td>
<td>0.0633</td>
<td>0.0432</td>
</tr>
<tr>
<td>RecMind-ToT (DFS,zero-shot)</td>
<td>0.0412</td>
<td>0.0295</td>
<td>0.0507</td>
<td>0.0376</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>0.0468</td>
<td>0.0318</td>
<td>0.0608</td>
<td>0.0420</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>0.0442</td>
<td>0.0307</td>
<td>0.0580</td>
<td>0.0415</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>0.0518</td>
<td>0.0396</td>
<td>0.0685</td>
<td>0.0480</td>
</tr>
</tbody>
</table>
<p>Table 14: Performance comparison on review summarization and explanation generation on Sports domain of Amazon Reviews.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Sports</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BLEU2</td>
<td>ROGUE1</td>
<td>ROGUE2</td>
<td>ROGUEL</td>
</tr>
<tr>
<td>Review Summarization</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>2.5874</td>
<td>11.8971</td>
<td>3.0257</td>
<td>10.5472</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.9024</td>
<td>5.7402</td>
<td>1.2493</td>
<td>3.6791</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>1.2579</td>
<td>6.3190</td>
<td>1.4257</td>
<td>3.8912</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>1.5840</td>
<td>6.5310</td>
<td>1.4390</td>
<td>5.0140</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>1.6014</td>
<td>6.7125</td>
<td>1.5479</td>
<td>5.2175</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>1.5940</td>
<td>6.5872</td>
<td>1.4780</td>
<td>5.1566</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>1.7125</td>
<td>6.7986</td>
<td>1.5724</td>
<td>5.3794</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>1.5874</td>
<td>6.5531</td>
<td>1.4726</td>
<td>5.1530</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>1.6542</td>
<td>6.6540</td>
<td>1.5639</td>
<td>5.2960</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>1.6120</td>
<td>6.6259</td>
<td>1.5029</td>
<td>5.1891</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>1.7388</td>
<td>6.8130</td>
<td>1.6217</td>
<td>5.5632</td>
</tr>
<tr>
<td>Explanation Generation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>1.1412</td>
<td>14.0329</td>
<td>2.1279</td>
<td>11.1894</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.0611</td>
<td>7.2892</td>
<td>0.9921</td>
<td>5.6923</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>1.2358</td>
<td>9.6405</td>
<td>2.8723</td>
<td>6.2824</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>0.9687</td>
<td>8.3097</td>
<td>2.1320</td>
<td>7.1427</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>1.3874</td>
<td>11.0487</td>
<td>3.0216</td>
<td>8.1146</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>1.1032</td>
<td>8.9895</td>
<td>2.3810</td>
<td>7.8419</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>1.3765</td>
<td>11.5749</td>
<td>2.8023</td>
<td>8.4256</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>1.5874</td>
<td>6.5531</td>
<td>1.4726</td>
<td>5.1530</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>1.6542</td>
<td>6.6540</td>
<td>1.5639</td>
<td>5.2960</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>1.6120</td>
<td>6.6259</td>
<td>1.5029</td>
<td>5.1891</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>1.7388</td>
<td>6.8130</td>
<td>1.6217</td>
<td>5.5632</td>
</tr>
<tr>
<td>Explanation Generation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>1.1412</td>
<td>14.0329</td>
<td>2.1279</td>
<td>11.1894</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.0611</td>
<td>7.2892</td>
<td>0.9921</td>
<td>5.6923</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>1.2358</td>
<td>9.6405</td>
<td>2.8723</td>
<td>6.2824</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>0.9687</td>
<td>8.3097</td>
<td>2.1320</td>
<td>7.1427</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>1.3874</td>
<td>11.0487</td>
<td>3.0216</td>
<td>8.1146</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>1.1032</td>
<td>8.9895</td>
<td>2.3810</td>
<td>7.8419</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>1.3765</td>
<td>11.5749</td>
<td>2.8023</td>
<td>8.4256</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>1.5874</td>
<td>6.5531</td>
<td>1.4726</td>
<td>5.1530</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>1.6542</td>
<td>6.6540</td>
<td>1.5639</td>
<td>5.2960</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>1.6120</td>
<td>6.6259</td>
<td>1.5029</td>
<td>5.1891</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>1.7388</td>
<td>6.8130</td>
<td>1.6217</td>
<td>5.5632</td>
</tr>
<tr>
<td>Explanation Generation</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>P5 (pre-trained expert,few-shot)</td>
<td>2.2850</td>
<td>15.0416</td>
<td>3.6798</td>
<td>12.1065</td>
</tr>
<tr>
<td>ChatGPT (zero-shot)</td>
<td>0.1379</td>
<td>9.7892</td>
<td>1.5416</td>
<td>5.3158</td>
</tr>
<tr>
<td>ChatGPT (few-shot)</td>
<td>2.0169</td>
<td>11.8905</td>
<td>3.2049</td>
<td>6.2689</td>
</tr>
<tr>
<td>RecMind-CoT (zero-shot)</td>
<td>2.1354</td>
<td>11.0597</td>
<td>2.1590</td>
<td>7.1445</td>
</tr>
<tr>
<td>RecMind-CoT (few-shot)</td>
<td>2.4079</td>
<td>12.7987</td>
<td>3.5146</td>
<td>7.4153</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, zero-shot)</td>
<td>2.1930</td>
<td>11.2874</td>
<td>2.1782</td>
<td>7.1854</td>
</tr>
<tr>
<td>RecMind-ToT (BFS, few-shot)</td>
<td>2.4565</td>
<td>12.8249</td>
<td>3.6327</td>
<td>7.6234</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, zero-shot)</td>
<td>2.1658</td>
<td>11.2802</td>
<td>2.1770</td>
<td>7.1809</td>
</tr>
<tr>
<td>RecMind-ToT (DFS, few-shot)</td>
<td>2.4152</td>
<td>12.8975</td>
<td>3.6079</td>
<td>7.7112</td>
</tr>
<tr>
<td>RecMind-SI (zero-shot)</td>
<td>2.2740</td>
<td>11.6794</td>
<td>2.2460</td>
<td>7.2536</td>
</tr>
<tr>
<td>RecMind-SI (few-shot)</td>
<td>2.4674</td>
<td>13.2560</td>
<td>3.6920</td>
<td>7.9987</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Work was done as an intern at Amazon Alexa AI.
${ }^{\dagger}$ Indicates equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
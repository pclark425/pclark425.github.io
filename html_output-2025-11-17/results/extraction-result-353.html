<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-353 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-353</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-353</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-247450642</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.acl-long.168.pdf" target="_blank">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals</a></p>
                <p><strong>Paper Abstract:</strong> Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge. Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning. Starting from the observation that images are more likely to exhibit spatial commonsense than texts, we explore whether models with visual signals learn more spatial commonsense than text-based PLMs. We propose a spatial commonsense benchmark that focuses on the relative scales of objects, and the positional relationship between people and objects under different actions.We probe PLMs and models with visual signals, including vision-language pretrained models and image synthesis models, on this benchmark, and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models. The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e353.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e353.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLMs (BERT / RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained Language Models (BERT-large, RoBERTa-large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-only pretrained transformer language models probed with prompt-based masked-word prediction to assess encoded spatial commonsense (object size/height comparisons and person-object positional relations). They operate only on text input and do not receive visual sensory input in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT / RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based masked language models pretrained on large text corpora (BERT and RoBERTa large variants). No multimodal inputs; probed via mask-filling prompts designed to elicit spatial knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial commonsense probing: Size/Height comparison & Positional relationship</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Two small probing tasks: (1) compare relative sizes or heights of object pairs ( prompts of form 'Oa is [MASK] than Ob' with answers {larger, smaller} or {taller, shorter}); (2) infer position of a person relative to an object given an action (prompts like 'A woman washes the car. She is [MASK] the car.' with answers {above, below, inside, beside}).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial reasoning / commonsense probing</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (object scales, relative positions); object-relational (person-object relations tied to actions)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (masked-language objective); prompt engineering/back-translation for selecting prompts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing via masked-word prediction (zero-shot / prompt-based probing with development-set prompt selection)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in model weights / contextual embeddings (no explicit spatial maps or symbolic structures)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and macro F1 (%) on the constructed probing datasets; also consistency metrics (symmetry and transitivity %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Size task: best PLM (RoBERTa) acc ~54.1% (F1 ~52.2%). Height task: best PLM acc ~50.8% (F1 ~50.3%). Positional relationship: best PLM acc ~49.0% (F1 ~43.7%). Consistency: symmetry ~25–37% (depending on size vs height), transitivity ~72–73%. These are close to random-chance baseline and substantially lower than vision-enabled models reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Occasional correct lexical/statistical associations where textual co-occurrence implies a spatial relation (e.g., 'sofa' more often associated with words indicating large in text corpora). Achieves moderate transitive consistency in some cases (transitivity ~72–73%).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Poor at explicit spatial scale reasoning and action-conditioned positional inference; often ignores the action context (RoBERTa chose the same relation for a ⟨person,object⟩ pair across different actions in 64% of cases). Symmetric consistency is low (violates A>B then B<A). Fails to generalize to spatial relations not well represented in text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Approximately at or slightly above random baselines (~50% for binary size/height, ~25% for four-way positional relation baseline by chance); significantly worse than VinVL and ISM-based methods (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation studies reported for PLMs; the paper only compares different prompt candidates and selects best via fold development sets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-only PLMs encode little reliable spatial commonsense: they perform near chance on object scale and person-object positional tasks, are biased by textual distributional signals, and often ignore action-conditioned positional cues. Their spatial knowledge is implicit and inconsistent (low symmetry).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Things not Written in Text: Exploring Spatial Commonsense from Visual Signals', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e353.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e353.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VinVL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VinVL (Vision-Language Pretrained Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language pretrained transformer that aligns image region features with text and can be probed with masked-word prediction or used for multimodal tasks; evaluated both in text-only probing and when paired with generated images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vinvl: Revisiting visual representations in vision-language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VinVL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based vision-language pretrained model trained on image-caption and VQA-style datasets; preserves masked-word prediction objective enabling prompt-based probing. Pretraining emphasizes aligning text with image regions, amplifying discriminative region features.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial commonsense probing: Size/Height comparison & Positional relationship (same benchmark as PLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Probing of object scale comparisons and person-object positional relations using masked prompts; additionally used as the vision-language reasoning backbone to answer yes/no spatial QA when provided with ISM-generated images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial reasoning / multimodal question answering</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (object scales, relative positions; mapping text to image regions and region features)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on multimodal (image-caption, VQA and other vision-language) datasets with region-text alignment objectives</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>masked-word prediction probing (text-only prompts) and multimodal inference when given images (including generated images from ISM)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>multimodal embeddings that align textual tokens with image region features; discriminative object-region features emphasized (implicit spatial knowledge in region locations and embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and macro F1 (%) on size/height and positional relation probing; consistency metrics (symmetry/transitivity %); QA accuracy/F1 for the toy QA task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Size task: acc ~61.8% (F1 ~54.4%). Height task: acc ~64.5% (F1 ~61.5%). Positional relationship: acc ~60.6% (F1 ~51.2%). Consistency: symmetry ~43–44%, transitivity very high (~93–95%). On QA with ISM-generated images (as reasoning model) it outperformed a text-only UnifiedQA baseline (exact per-task numbers reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Substantially better than PLMs at encoding spatial scale and positional relations, especially transitive consistency (high transitivity indicates coherent comparative ordering). Uses visual region alignment to pick up scene-layout cues when images are available.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Lower symmetric consistency than ISM-generated human judgments (some bias toward 'larger' or certain labels, likely from pretraining distributional biases); can miss non-discriminative spatial attributes because pretraining emphasis is on region-level discriminative features (e.g., size may be deemphasized).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms PLM baselines by ~7–14% absolute on scale/height tasks (e.g., VinVL acc ~61.8% vs best PLM ~54.1%), and by ~20% on positional relations versus PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No formal ablation reported dissecting VinVL components; comparison-style 'ablations' are implicit via comparisons to PLMs and ISM-based evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vision-language pretraining that aligns text with image regions provides better spatial commonsense than text-only pretraining: VinVL shows higher accuracy and transitive consistency, indicating multimodal pretraining meaningfully encodes spatial/object-relational knowledge, though some biases and lower symmetry persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Things not Written in Text: Exploring Spatial Commonsense from Visual Signals', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e353.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e353.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ISM (VQGAN+CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Image Synthesis Model (VQGAN + CLIP guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-image synthesis pipeline that optimizes a latent vector for VQGAN guided by CLIP text-image similarity to generate images from textual prompts; used as a probe for implicit spatial commonsense learned from visual data by evaluating generated images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Taming transformers for high-resolution image synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQGAN + CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image generator (VQGAN) generates images from optimized latent vectors; CLIP provides a multimodal embedding to align generated images to text prompts by iterative optimization of the latent vector (pretrained VQGAN and CLIP weights frozen). Used here to synthesize images for object-scale and person-object relation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial commonsense probing via image synthesis (Size/Height comparison & Positional relationship)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a text prompt describing two objects (for scale/height) or a person-object action scenario (for positional relation), ISM synthesizes an image; spatial relations are then extracted automatically via object detection + depth estimation on generated images (ISM(Box)) or via human annotation of generated images (ISM(Human)).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial reasoning via generative multimodal probing</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (object scales, relative heights, person-object positional relations conditioned on actions)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit knowledge learned in pretraining of image models on large image datasets and CLIP alignment (no explicit symbolic spatial KB); knowledge elicited by generating an image from text</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>text-to-image synthesis (optimize latent vector per prompt) followed by (a) automatic extraction via object detector + depth estimator on generated image (ISM(Box)), and (b) human judgment of generated image (ISM(Human))</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicitly encoded in generative model weights producing pixel-level images that instantiate spatial relations (thus knowledge is represented as generative priors and compositional image layouts rather than explicit symbolic maps)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and macro F1 (%) of extracted relations from generated images; recognition ratios (fraction of images where objects are detected); consistency metrics (symmetry/transitivity %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Size task: ISM(Box) acc 54.8% overall but 81.6% on subset where objects are auto-recognized; ISM(Human) acc ~72.7% (F1 ~72.6%), human recognition ~86% of images. Height task: ISM(Box) acc 52.5% overall but 68.1% on recognized subset; ISM(Human) acc ~78.9% (F1 ~78.8%), human recognition ~82%. Positional relationship: ISM(Box) acc ~58.5% overall (71.5% on recognized subset); ISM(Human) acc ~72.5% (F1 ~71.8%), human recognition ~93%. Consistency: ISM(Human) symmetry ~82–83% and transitivity ~81–85%, substantially higher than PLMs and VinVL.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>ISM-generated images manifest accurate and consistent object-scale and positional relations judged by humans (high human-recognition accuracy and high symmetry/transitivity); image synthesis captures action-conditioned positional relations and generalizes reasonably to uncommon scenarios via generative priors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Automatic extraction (ISM(Box)) limited by object detector and depth estimator failures on synthetic images (low recognition ratios ~14–40% depending on task); ISM sometimes generates fragmented or obstructed objects (hindering detection), struggles with very uncommon/unseen objects, and viewpoint/depth artifacts can confound automatic spatial measures.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ISM(Human) outperforms PLMs by ~15–30% absolute and outperforms VinVL by ~8–12% on many metrics (exact per-task comparisons in tables). ISM(Box) performance depends heavily on recognition ratio; on recognized subsets ISM(Box) often exceeds VinVL (e.g., size recognized subset: ISM(Box) 81.6% vs VinVL 61.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No formal ablations of the ISM architecture were reported; comparisons functionally act as ablations (ISM(Box) vs ISM(Human) highlights the impact of downstream perception tools and human judgment).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Image synthesis models implicitly learn and can instantiate accurate, consistent spatial commonsense (object scales and action-conditioned positional relations) even when operating from text-only prompts (no direct sensory input). Generated images serve as a rich intermediate representation that can be inspected (automatically or by humans) to extract spatial/object-relational knowledge; however, automatic extraction quality depends on downstream detectors and depth estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Things not Written in Text: Exploring Spatial Commonsense from Visual Signals', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e353.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e353.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ISM -> VinVL pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Image synthesis (VQGAN+CLIP) followed by vision-language reasoning (VinVL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline where ISM generates an image from a spatial question prompt, and a vision-language model (VinVL) consumes the generated image plus the question to answer spatial commonsense QA; demonstrates using generated visualizations to augment language-only reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQGAN+CLIP (image synthesis) + VinVL (vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative ISM produces images from textual question contexts; VinVL (fine-tuned on VQA) receives the synthesized image and the question to predict answers in a QA setting (yes/no / multi-choice). This pipeline is intended to supply visual spatial priors to a multimodal reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Toy spatial question answering (transformed probing dataset as QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transform probing instances into question forms (e.g., 'A boy is riding a bicycle. Is he on the bicycle?') — ISM generates an image from the question context; VinVL ingests the question and generated image to predict answer (yes/no or categorical).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multimodal question answering / spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (uses generated visual scene to ground spatial relations and answer QA)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>visual commonsense induced by ISM pretraining (image priors) + VinVL multimodal pretraining and VQA fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>generate image from text prompt (ISM) then feed image + text to a vision-language model (VinVL) to perform inference; no real-world sensory input is used (generated image stands in for perception)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>generated pixel-level image as an explicit intermediate representation encoding spatial relations; VinVL's region-text aligned multimodal embeddings used for reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) and macro F1 (%) on the toy QA tasks derived from the probing benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Pipeline (ISM w/ VinVL) outperformed a strong text-only QA baseline UnifiedQA across all subtasks in the paper's experiments (exact per-subtask numbers provided in paper tables; relative improvement reported though table values are in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Generated images provide spatial priors that VinVL can exploit to answer action-conditioned positional questions more accurately than text-only QA, demonstrating that synthesized visualizations can materially improve language understanding for spatial queries.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When generated images are not well-formed or objects are fragmented/occluded, VinVL's reasoning may degrade; quality of the synthesized image and the alignment between generated regions and object mentions are critical. The pipeline depends on the generative model producing recognizable, correctly arranged objects.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>UnifiedQA (text-only QA model) used as baseline; ISM+VinVL outperformed UnifiedQA on all subtasks (paper reports improvement but exact numeric deltas are in their Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No formal ablation isolating ISM vs VinVL components beyond comparing to UnifiedQA and to VinVL alone in probing; the comparison shows value added by ISM-generated images.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Synthesizing images from textual scenarios and feeding them into a vision-language model provides a practical route to inject spatial commonsense into language understanding tasks when direct sensory input is unavailable; generated visualizations act as an intermediate modality that encodes spatial and object-relational information usable by multimodal reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Things not Written in Text: Exploring Spatial Commonsense from Visual Signals', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Are elephants bigger than butterflies? reasoning about sizes of objects <em>(Rating: 2)</em></li>
                <li>Vinvl: Revisiting visual representations in vision-language models <em>(Rating: 2)</em></li>
                <li>Taming transformers for high-resolution image synthesis <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 1)</em></li>
                <li>Prost: Physical reasoning of objects through space and time <em>(Rating: 1)</em></li>
                <li>Stating the obvious: Extracting visual common sense knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-353",
    "paper_id": "paper-247450642",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "PLMs (BERT / RoBERTa)",
            "name_full": "Pretrained Language Models (BERT-large, RoBERTa-large)",
            "brief_description": "Text-only pretrained transformer language models probed with prompt-based masked-word prediction to assess encoded spatial commonsense (object size/height comparisons and person-object positional relations). They operate only on text input and do not receive visual sensory input in these experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT / RoBERTa",
            "model_size": null,
            "model_description": "Large transformer-based masked language models pretrained on large text corpora (BERT and RoBERTa large variants). No multimodal inputs; probed via mask-filling prompts designed to elicit spatial knowledge.",
            "task_name": "Spatial commonsense probing: Size/Height comparison & Positional relationship",
            "task_description": "Two small probing tasks: (1) compare relative sizes or heights of object pairs ( prompts of form 'Oa is [MASK] than Ob' with answers {larger, smaller} or {taller, shorter}); (2) infer position of a person relative to an object given an action (prompts like 'A woman washes the car. She is [MASK] the car.' with answers {above, below, inside, beside}).",
            "task_type": "spatial reasoning / commonsense probing",
            "knowledge_type": "spatial (object scales, relative positions); object-relational (person-object relations tied to actions)",
            "knowledge_source": "pre-training on large text corpora (masked-language objective); prompt engineering/back-translation for selecting prompts",
            "has_direct_sensory_input": false,
            "elicitation_method": "probing via masked-word prediction (zero-shot / prompt-based probing with development-set prompt selection)",
            "knowledge_representation": "implicit in model weights / contextual embeddings (no explicit spatial maps or symbolic structures)",
            "performance_metric": "Accuracy (%) and macro F1 (%) on the constructed probing datasets; also consistency metrics (symmetry and transitivity %)",
            "performance_result": "Size task: best PLM (RoBERTa) acc ~54.1% (F1 ~52.2%). Height task: best PLM acc ~50.8% (F1 ~50.3%). Positional relationship: best PLM acc ~49.0% (F1 ~43.7%). Consistency: symmetry ~25–37% (depending on size vs height), transitivity ~72–73%. These are close to random-chance baseline and substantially lower than vision-enabled models reported in the paper.",
            "success_patterns": "Occasional correct lexical/statistical associations where textual co-occurrence implies a spatial relation (e.g., 'sofa' more often associated with words indicating large in text corpora). Achieves moderate transitive consistency in some cases (transitivity ~72–73%).",
            "failure_patterns": "Poor at explicit spatial scale reasoning and action-conditioned positional inference; often ignores the action context (RoBERTa chose the same relation for a ⟨person,object⟩ pair across different actions in 64% of cases). Symmetric consistency is low (violates A&gt;B then B&lt;A). Fails to generalize to spatial relations not well represented in text.",
            "baseline_comparison": "Approximately at or slightly above random baselines (~50% for binary size/height, ~25% for four-way positional relation baseline by chance); significantly worse than VinVL and ISM-based methods (see other entries).",
            "ablation_results": "No ablation studies reported for PLMs; the paper only compares different prompt candidates and selects best via fold development sets.",
            "key_findings": "Text-only PLMs encode little reliable spatial commonsense: they perform near chance on object scale and person-object positional tasks, are biased by textual distributional signals, and often ignore action-conditioned positional cues. Their spatial knowledge is implicit and inconsistent (low symmetry).",
            "uuid": "e353.0",
            "source_info": {
                "paper_title": "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "VinVL",
            "name_full": "VinVL (Vision-Language Pretrained Model)",
            "brief_description": "A vision-language pretrained transformer that aligns image region features with text and can be probed with masked-word prediction or used for multimodal tasks; evaluated both in text-only probing and when paired with generated images.",
            "citation_title": "Vinvl: Revisiting visual representations in vision-language models",
            "mention_or_use": "use",
            "model_name": "VinVL",
            "model_size": null,
            "model_description": "Transformer-based vision-language pretrained model trained on image-caption and VQA-style datasets; preserves masked-word prediction objective enabling prompt-based probing. Pretraining emphasizes aligning text with image regions, amplifying discriminative region features.",
            "task_name": "Spatial commonsense probing: Size/Height comparison & Positional relationship (same benchmark as PLMs)",
            "task_description": "Probing of object scale comparisons and person-object positional relations using masked prompts; additionally used as the vision-language reasoning backbone to answer yes/no spatial QA when provided with ISM-generated images.",
            "task_type": "spatial reasoning / multimodal question answering",
            "knowledge_type": "spatial + object-relational (object scales, relative positions; mapping text to image regions and region features)",
            "knowledge_source": "pre-training on multimodal (image-caption, VQA and other vision-language) datasets with region-text alignment objectives",
            "has_direct_sensory_input": true,
            "elicitation_method": "masked-word prediction probing (text-only prompts) and multimodal inference when given images (including generated images from ISM)",
            "knowledge_representation": "multimodal embeddings that align textual tokens with image region features; discriminative object-region features emphasized (implicit spatial knowledge in region locations and embeddings)",
            "performance_metric": "Accuracy (%) and macro F1 (%) on size/height and positional relation probing; consistency metrics (symmetry/transitivity %); QA accuracy/F1 for the toy QA task",
            "performance_result": "Size task: acc ~61.8% (F1 ~54.4%). Height task: acc ~64.5% (F1 ~61.5%). Positional relationship: acc ~60.6% (F1 ~51.2%). Consistency: symmetry ~43–44%, transitivity very high (~93–95%). On QA with ISM-generated images (as reasoning model) it outperformed a text-only UnifiedQA baseline (exact per-task numbers reported in paper tables).",
            "success_patterns": "Substantially better than PLMs at encoding spatial scale and positional relations, especially transitive consistency (high transitivity indicates coherent comparative ordering). Uses visual region alignment to pick up scene-layout cues when images are available.",
            "failure_patterns": "Lower symmetric consistency than ISM-generated human judgments (some bias toward 'larger' or certain labels, likely from pretraining distributional biases); can miss non-discriminative spatial attributes because pretraining emphasis is on region-level discriminative features (e.g., size may be deemphasized).",
            "baseline_comparison": "Outperforms PLM baselines by ~7–14% absolute on scale/height tasks (e.g., VinVL acc ~61.8% vs best PLM ~54.1%), and by ~20% on positional relations versus PLMs.",
            "ablation_results": "No formal ablation reported dissecting VinVL components; comparison-style 'ablations' are implicit via comparisons to PLMs and ISM-based evaluations.",
            "key_findings": "Vision-language pretraining that aligns text with image regions provides better spatial commonsense than text-only pretraining: VinVL shows higher accuracy and transitive consistency, indicating multimodal pretraining meaningfully encodes spatial/object-relational knowledge, though some biases and lower symmetry persist.",
            "uuid": "e353.1",
            "source_info": {
                "paper_title": "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "ISM (VQGAN+CLIP)",
            "name_full": "Image Synthesis Model (VQGAN + CLIP guidance)",
            "brief_description": "A text-to-image synthesis pipeline that optimizes a latent vector for VQGAN guided by CLIP text-image similarity to generate images from textual prompts; used as a probe for implicit spatial commonsense learned from visual data by evaluating generated images.",
            "citation_title": "Taming transformers for high-resolution image synthesis",
            "mention_or_use": "use",
            "model_name": "VQGAN + CLIP",
            "model_size": null,
            "model_description": "Image generator (VQGAN) generates images from optimized latent vectors; CLIP provides a multimodal embedding to align generated images to text prompts by iterative optimization of the latent vector (pretrained VQGAN and CLIP weights frozen). Used here to synthesize images for object-scale and person-object relation prompts.",
            "task_name": "Spatial commonsense probing via image synthesis (Size/Height comparison & Positional relationship)",
            "task_description": "Given a text prompt describing two objects (for scale/height) or a person-object action scenario (for positional relation), ISM synthesizes an image; spatial relations are then extracted automatically via object detection + depth estimation on generated images (ISM(Box)) or via human annotation of generated images (ISM(Human)).",
            "task_type": "spatial reasoning via generative multimodal probing",
            "knowledge_type": "spatial + object-relational (object scales, relative heights, person-object positional relations conditioned on actions)",
            "knowledge_source": "implicit knowledge learned in pretraining of image models on large image datasets and CLIP alignment (no explicit symbolic spatial KB); knowledge elicited by generating an image from text",
            "has_direct_sensory_input": false,
            "elicitation_method": "text-to-image synthesis (optimize latent vector per prompt) followed by (a) automatic extraction via object detector + depth estimator on generated image (ISM(Box)), and (b) human judgment of generated image (ISM(Human))",
            "knowledge_representation": "implicitly encoded in generative model weights producing pixel-level images that instantiate spatial relations (thus knowledge is represented as generative priors and compositional image layouts rather than explicit symbolic maps)",
            "performance_metric": "Accuracy (%) and macro F1 (%) of extracted relations from generated images; recognition ratios (fraction of images where objects are detected); consistency metrics (symmetry/transitivity %)",
            "performance_result": "Size task: ISM(Box) acc 54.8% overall but 81.6% on subset where objects are auto-recognized; ISM(Human) acc ~72.7% (F1 ~72.6%), human recognition ~86% of images. Height task: ISM(Box) acc 52.5% overall but 68.1% on recognized subset; ISM(Human) acc ~78.9% (F1 ~78.8%), human recognition ~82%. Positional relationship: ISM(Box) acc ~58.5% overall (71.5% on recognized subset); ISM(Human) acc ~72.5% (F1 ~71.8%), human recognition ~93%. Consistency: ISM(Human) symmetry ~82–83% and transitivity ~81–85%, substantially higher than PLMs and VinVL.",
            "success_patterns": "ISM-generated images manifest accurate and consistent object-scale and positional relations judged by humans (high human-recognition accuracy and high symmetry/transitivity); image synthesis captures action-conditioned positional relations and generalizes reasonably to uncommon scenarios via generative priors.",
            "failure_patterns": "Automatic extraction (ISM(Box)) limited by object detector and depth estimator failures on synthetic images (low recognition ratios ~14–40% depending on task); ISM sometimes generates fragmented or obstructed objects (hindering detection), struggles with very uncommon/unseen objects, and viewpoint/depth artifacts can confound automatic spatial measures.",
            "baseline_comparison": "ISM(Human) outperforms PLMs by ~15–30% absolute and outperforms VinVL by ~8–12% on many metrics (exact per-task comparisons in tables). ISM(Box) performance depends heavily on recognition ratio; on recognized subsets ISM(Box) often exceeds VinVL (e.g., size recognized subset: ISM(Box) 81.6% vs VinVL 61.6%).",
            "ablation_results": "No formal ablations of the ISM architecture were reported; comparisons functionally act as ablations (ISM(Box) vs ISM(Human) highlights the impact of downstream perception tools and human judgment).",
            "key_findings": "Image synthesis models implicitly learn and can instantiate accurate, consistent spatial commonsense (object scales and action-conditioned positional relations) even when operating from text-only prompts (no direct sensory input). Generated images serve as a rich intermediate representation that can be inspected (automatically or by humans) to extract spatial/object-relational knowledge; however, automatic extraction quality depends on downstream detectors and depth estimators.",
            "uuid": "e353.2",
            "source_info": {
                "paper_title": "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "ISM -&gt; VinVL pipeline",
            "name_full": "Image synthesis (VQGAN+CLIP) followed by vision-language reasoning (VinVL)",
            "brief_description": "A two-stage pipeline where ISM generates an image from a spatial question prompt, and a vision-language model (VinVL) consumes the generated image plus the question to answer spatial commonsense QA; demonstrates using generated visualizations to augment language-only reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "VQGAN+CLIP (image synthesis) + VinVL (vision-language model)",
            "model_size": null,
            "model_description": "Generative ISM produces images from textual question contexts; VinVL (fine-tuned on VQA) receives the synthesized image and the question to predict answers in a QA setting (yes/no / multi-choice). This pipeline is intended to supply visual spatial priors to a multimodal reasoner.",
            "task_name": "Toy spatial question answering (transformed probing dataset as QA)",
            "task_description": "Transform probing instances into question forms (e.g., 'A boy is riding a bicycle. Is he on the bicycle?') — ISM generates an image from the question context; VinVL ingests the question and generated image to predict answer (yes/no or categorical).",
            "task_type": "multimodal question answering / spatial reasoning",
            "knowledge_type": "spatial + object-relational (uses generated visual scene to ground spatial relations and answer QA)",
            "knowledge_source": "visual commonsense induced by ISM pretraining (image priors) + VinVL multimodal pretraining and VQA fine-tuning",
            "has_direct_sensory_input": true,
            "elicitation_method": "generate image from text prompt (ISM) then feed image + text to a vision-language model (VinVL) to perform inference; no real-world sensory input is used (generated image stands in for perception)",
            "knowledge_representation": "generated pixel-level image as an explicit intermediate representation encoding spatial relations; VinVL's region-text aligned multimodal embeddings used for reasoning",
            "performance_metric": "Accuracy (%) and macro F1 (%) on the toy QA tasks derived from the probing benchmark",
            "performance_result": "Pipeline (ISM w/ VinVL) outperformed a strong text-only QA baseline UnifiedQA across all subtasks in the paper's experiments (exact per-subtask numbers provided in paper tables; relative improvement reported though table values are in paper).",
            "success_patterns": "Generated images provide spatial priors that VinVL can exploit to answer action-conditioned positional questions more accurately than text-only QA, demonstrating that synthesized visualizations can materially improve language understanding for spatial queries.",
            "failure_patterns": "When generated images are not well-formed or objects are fragmented/occluded, VinVL's reasoning may degrade; quality of the synthesized image and the alignment between generated regions and object mentions are critical. The pipeline depends on the generative model producing recognizable, correctly arranged objects.",
            "baseline_comparison": "UnifiedQA (text-only QA model) used as baseline; ISM+VinVL outperformed UnifiedQA on all subtasks (paper reports improvement but exact numeric deltas are in their Table 6).",
            "ablation_results": "No formal ablation isolating ISM vs VinVL components beyond comparing to UnifiedQA and to VinVL alone in probing; the comparison shows value added by ISM-generated images.",
            "key_findings": "Synthesizing images from textual scenarios and feeding them into a vision-language model provides a practical route to inject spatial commonsense into language understanding tasks when direct sensory input is unavailable; generated visualizations act as an intermediate modality that encodes spatial and object-relational information usable by multimodal reasoners.",
            "uuid": "e353.3",
            "source_info": {
                "paper_title": "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Are elephants bigger than butterflies? reasoning about sizes of objects",
            "rating": 2,
            "sanitized_title": "are_elephants_bigger_than_butterflies_reasoning_about_sizes_of_objects"
        },
        {
            "paper_title": "Vinvl: Revisiting visual representations in vision-language models",
            "rating": 2,
            "sanitized_title": "vinvl_revisiting_visual_representations_in_visionlanguage_models"
        },
        {
            "paper_title": "Taming transformers for high-resolution image synthesis",
            "rating": 2,
            "sanitized_title": "taming_transformers_for_highresolution_image_synthesis"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 1,
            "sanitized_title": "language_models_as_knowledge_bases"
        },
        {
            "paper_title": "Prost: Physical reasoning of objects through space and time",
            "rating": 1,
            "sanitized_title": "prost_physical_reasoning_of_objects_through_space_and_time"
        },
        {
            "paper_title": "Stating the obvious: Extracting visual common sense knowledge",
            "rating": 1,
            "sanitized_title": "stating_the_obvious_extracting_visual_common_sense_knowledge"
        }
    ],
    "cost": 0.01669675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Things not Written in Text: Exploring Spatial Commonsense from Visual Signals
Association for Computational LinguisticsCopyright Association for Computational LinguisticsMay 22-27, 2022 c 2022</p>
<p>Xiao Liu 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Da Yin 
Computer Science Department
University of California
Los Angeles</p>
<p>Yansong Feng 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>The MOE Key Laboratory of Computational Linguistics
Peking University</p>
<p>Dongyan Zhao zhaody@pku.edu.cnda.yin@cs.ucla.edu 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Artificial Intelligence Institute of Peking University</p>
<p>State Key Laboratory of Media Convergence Production Technology and Systems</p>
<p>Things not Written in Text: Exploring Spatial Commonsense from Visual Signals</p>
<p>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
the 60th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1May 22-27, 2022 c 2022
Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge. Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning. Starting from the observation that images are more likely to exhibit spatial commonsense than texts, we explore whether models with visual signals learn more spatial commonsense than text-based PLMs. We propose a spatial commonsense benchmark that focuses on the relative scales of objects, and the positional relationship between people and objects under different actions. We probe PLMs and models with visual signals, including visionlanguage pretrained models and image synthesis models, on this benchmark, and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models. The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense. Code and data are available at https://github.com/ xxxiaol/spatial-commonsense.</p>
<p>Introduction</p>
<p>Spatial perception, the ability to detect the spatial position and to infer the relationship between visual stimuli (Donnon et al., 2005;Saj and Barisnikov, 2015), is basic but important for human beings (Pellegrino et al., 1984). It is of everyday use, from understanding the surrounding environment, like when seeing a woman sitting in a car with her hands on the steering wheel, we know she is probably driving, to processing spatial information and performing reasoning, like navigating through a dense forest. We regard the knowledge needed in spatial perception as spatial commonsense. Humans start to develop spatial perception and acquire spatial commonsense from infancy, and apply the commonsense through lifetime (Kuipers et al., 1990;Poole et al., 2006).</p>
<p>Although text-based Pretrained Language Models (PLMs) achieve great performance on various commonsense reasoning tasks (Davison et al., 2019;Zhou et al., 2020), they are shown to be ineffective when dealing with spatial commonsense.  and Aroca-Ouellette et al. (2021) show that current PLMs lack the ability to reason about object scales. Bhagavatula et al. (2020) find that BERT (Devlin et al., 2019) underperforms on instances involving spatial locations. The struggle of PLMs with spatial commonsense is partly because spatial commonsense is rarely expressed explicitly in texts. We may write sentences like lions are big animals, but we seldom explicitly mention how big lions are; we also rarely write about the spatial relationship between a boy and a bicycle when he is cycling.</p>
<p>Spatial commonsense is exhibited in images more commonly . As shown in Figure 1, the two Wikipedia articles provide little spatial information, but a picture of a lion and a girl provides a reference to the size of a lion; and a painting of a boy riding a bicycle depicts that he sits on the bicycle. Hence, a natural idea is to elicit spatial knowledge from models with visual signals.</p>
<p>We first study whether models with visual signals learn more spatial knowledge than text-only models. We select Vision-Language PreTrained Models (VL-PTMs) and Image Synthesis Models (ISMs) for investigation. VL-PTMs encode texts and images together, fusing their features to deal with downstream tasks. ISMs take texts as input, and generate images based on the texts. To evaluate the spatial commonsense in PLMs and models with visual signals, we design a benchmark that involves two subtasks: 1) comparing sizes and heights of different objects (like a lion and a girl), and 2) determining the positional relationship between a person and an object when a certain action happens (like a boy's position when riding a bicycle). The subtasks are designed to examine the model's capability to master two kinds of spatial commonsense: understanding spatial scales, and the relationship between surrounding objects and ourselves.</p>
<p>As shown in Figure 2, we probe models with text prompts on this benchmark. We feed text prompts with masks to PLMs and VL-PTMs, and take the possible word with the highest probability as their prediction. We probe ISMs in a similar way: we first feed the text prompts to ISMs and then evaluate the generated images. We evaluate the images with two methods: automatically comparing bounding boxes of objects and conducting human evaluation. Results show that models with visual signals learn more accurate spatial commonsense than PLMs.</p>
<p>Besides the performance comparison, we are also interested in how is the quality of spatial commonsense learned by different models? We investigate how consistent the spatial knowledge learnt by a model is, like whether it can manifest a lion is larger than a girl and a girl is smaller than a lion simultaneously; and to what extent models can generalize the knowledge when uncommon scenarios like an enchantress lights the sparkler appear. We observe that ISMs are capable of generating consistent spatial knowledge and the performance is robust in uncommon scenarios.</p>
<p>The following problem is how to benefit natural language understanding tasks with the spatial knowledge from ISMs? We investigate this in the question answering scenario. Take a question like A boy is riding a bicycle. Is he on the bicycle? We generate an image about the question context a boy who is riding a bicycle with a text prompt using ISMs, and feed both the question and the generated image into vision-language models to predict an answer. This framework outperforms strong question answering models pretrained on texts only. While this is a simplified scenario of spatial commonsense reasoning, it manifests a possible way to employ the spatial knowledge learned by ISMs in natural language understanding.</p>
<p>Motivated by the observation that images contain more spatial commonsense than texts, we 1) design a framework, including the data and probing methods, to compare the spatial reasoning ability of models with different modalities; 2) propose methods to evaluate the quality of learned spatial commonsense, and find that models with visual signals, especially ISMs, learn more precise and robust spatial knowledge than PLMs; and 3) demonstrate the improvement in spatial commonsense question answering with the help of visual models.  (2021) design a physical reasoning dataset that requires not only spatial commonsense but also a complex reasoning process, which is extremely challenging for existing models. We choose the formulation of object comparison in pairs as this kind of knowledge is easy to be probed from different models. Spatial Relationship. Collell et al. (2018) introduce a dataset of spatial templates for objects under different relations, but the spatial relations are represented as relative positions of bounding boxes, which are hard to express in language. Yatskar et al. (2016) extract statements of spatial relationship from object co-occurrences in MS-COCO (Lin et al., 2014). Mirzaee et al. (2021) design a textual spatial reasoning benchmark, and Johnson et al. (2017) and Hudson and Manning (2019) involve spatial reasoning in images, but they focus on logical reasoning rather than commonsense. Contrast  Figure 2: The probing process. We take the size comparison between sof a and mountain as an example.</p>
<p>to them, we build a dataset to describe the spatial relationship between people and objects in certain actions with preposition words.</p>
<p>Knowledge Probing</p>
<p>Early attempts in probing PLMs (Liu et al., 2019a;Hewitt and Manning, 2019) mainly train a classifier on the task of interest with the encoded representations. However, the probing performance is highly influenced by the probe design (Pimentel et al., 2020), thus is hard to reflect the ability of PLMs.</p>
<p>Recently, prompt-based methods (Petroni et al., 2019;Zhou et al., 2020) become more prevalent to study what knowledge PLMs already encode. PLMs take a prompt as input, and generate the continuation (for generative PLMs) or predict masked words (for discriminative PLMs). This does not need additional training, and only a small development set is used to choose optimal prompts and answers (Jiang et al., 2020). In this work, we probe PLMs and VL-PTMs with prompts. Prompt-based methods are also used in model training (Schick and Schütze, 2021;Zhou et al., 2021), while we focus on the knowledge already learned by models. ;  try to apply the probing methods into the computer vision domain, but they focus on probing representations of visual models. In contrast, we probe ISMs by evaluating the generated images.</p>
<p>Benchmark Construction</p>
<p>Datasets</p>
<p>Size and Height. Inspired by the cognitive discovery (Hersh and Caramazza, 1976) that people tend to categorize objects scales into fuzzy sets, we select 25 common objects in daily life, and categorize them into 5 groups as shown in Table 1a   objects in the former group are smaller than those in the latter group. We form 250 pairs of objects from different groups, like ⟨ant, bird⟩, where the first object is smaller than the second in commonsense. Models are asked to compare the size of objects in pairs. To avoid an imbalance of answer distribution, we also consider the reversed pairs like ⟨bird, ant⟩, so there are 500 instances in total.</p>
<p>The dataset for comparing objects' heights is constructed similarly, as shown in Table 1b. We also form 500 instances with the objects. The comparison between objects is validated by 5 human annotators for both datasets.</p>
<p>Positional Relationship. The positional relationship dataset consists of human actions regarding objects and the most likely positional relation between the person and the object. We consider four types of positional relations: above, below, inside, beside, as they do not overlap with each other.</p>
<p>We select common objects, and write actions between people and the objects. The actions do not contain prepositions, like sit on the chair. Each ob-A man <verb> the car. He is the car.</p>
<p>A man washes the car. beside A man drives the car. inside ject is accompanied by two actions with different positional relations. Take Figure 3 as an example. The man is beside the car when washing the car, whereas he is inside the car when driving it. Therefore, the relation cannot be easily inferred from collocations between the person and the object. The dataset contains 224 instances, validated by 5 annotators.</p>
<p>Probing Tasks</p>
<p>We probe PLMs and VL-PTMs through masked word prediction. Given a text prompt with masks and a set of possible words, a model calculates the probability of each possible word filling the masked position. The word with the highest possibility is regarded as the prediction.</p>
<p>We also probe ISMs through text prompts. The input is a piece of descriptive text, and the output is the image generated by an ISM. We assess the image with two methods as described in 3.3.</p>
<p>PLMs are found to perform poorly in scenarios involving complex reasoning over spatial knowledge (Aroca-Ouellette et al., 2021), and we want to investigate whether they even fail in early stages, like whether they have learned spatial knowledge. So we probe models with simple tasks. In the subtask of size and height, the prompt for PLMs and
VL-PTMs is in the form of O a is [MASK] than O b , where ⟨O a , O b ⟩ is an object pair.
The possible answer set is {larger, smaller} for size and {taller, shorter} for height. The prompt for ISMs is in the form of O a and O b , and the objects in generated images are compared for size and height.</p>
<p>In the subtask of positional relationship, the prompt for PLMs and VL-PTMs contains an event scenario and a masked token for the positional relationship, like A woman washes the car. She is [MASK] the car. The possible answer set is {above, below, inside, beside}. The prompt for ISMs describes the scenario only, like A woman washes the car.</p>
<p>ISM Evaluation</p>
<p>We assess the images generated by ISMs with two methods. We first use the spatial information of bounding boxes (referred to as ISM (Box)). For each object mentioned in the prompt, we select the classified bounding box with the highest confidence. To mitigate the effect of viewpoint (an object closer to the camera may appear larger in the image), we compute the average depth of the box as the object's depth. We use the object detector from Zhang et al. (2021), and the depth estimator from Godard et al. (2019). When probing the relative size, we compare area × depth 2 of the two objects' boxes; and when probing the relative height, we compare height × depth. When classifying positional relations, we use the mapping rules between spatial relations and image regions from Visual Dependency Grammar (VDG) (Elliott and Keller, 2013). We list the rules in Appendix A.1.</p>
<p>Some generated images are vague while object detection models are trained to process clear pictures, so a number of objects are not recognized. To precisely assess the generated images, we conduct human evaluation on all images (referred to as ISM (Human)). Annotators are asked to compare the size/height of the objects in the images (for the first subtask) and classify the positional relationship between the person and the object (for the second subtask). Each image is evaluated by two annotators, and the average performance is reported. Specifically, we report the accuracy and macro F1 between models' predictions and correct answers. Besides the performance of ISMs on the subset of recognized instances, we also report the performance on the full dataset, giving the unrecognized instances a random guess.</p>
<p>Probing Spatial Commonsense</p>
<p>Models</p>
<p>We take BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) as examples of text-only PLMs. For VL-PTMs, we choose VinVL (Zhang et al., 2021), which performs well in various vision-language tasks. It uses a transformerbased backbone and is pretrained on various visionlanguage datasets including image caption datasets, visual QA datasets, etc. As it preserves the masked word prediction objective like PLMs, it can also be probed with prompts. We choose VQGAN+CLIP 1  As language models are sensitive to the expressions in probing  (like changing an answer choice from larger to bigger, the predictions of BERT may differ a lot), we generate new prompts and answers based on those originally designed in the benchmark, and search for the optimal ones for PLMs and VL-PTMs. Similar to Jiang et al. (2020), we use back-translation to generate 10 candidates for prompts and answers, and filter out the repeated ones. To select prompts and answers, we split the dataset into 5 folds, where different folds do not share the same objects. For each run, one fold is used as the development set to choose the best candidate, and the model is probed on other folds with the chosen prompt. We report average performance of 5 runs.</p>
<p>Probing Results</p>
<p>Size and Height. Table 2 reports the probing performance of comparing the scales of objects. We also demonstrate probing results on Relative-Size (Bagherinezhad et al., 2016)   best PLMs are slightly better than random guesses, indicating they are ineffective in predicting object scales. Although RoBERTa is trained on more texts and assumed to encode more knowledge, its performance is similar to BERT's. It shows that PLMs do not learn much spatial commonsense from texts even if the pretrained corpus greatly increases.</p>
<p>With the help of visual features in pretraining, VinVL greatly outperforms PLMs. ISM (Box), which simply compares bounding boxes in images generated by the ISM, also outperforms PLMs.</p>
<p>Since only a small portion of instances are recognized with bounding boxes, if we only consider the predictions on these instances, the gap between ISM (Box) and PLMs is more than 15%. These   indicate that models with visual signals learn accurate spatial commonsense knowledge from images. ISM (Box) outperforms VinVL on those recognizable instances (81.6 vs. 53.8), but the recognition ratio is admittedly low. We conduct human evaluation on the generated images for more precise assessment. More than 80% of images are recognized by humans and these images accurately reflect the spatial commonsense compared to PLMs and VinVL. 2 The gap between VinVL and ISM (Human) may be due to different ways of using visual signals in pretraining. A training objective of VinVL, and other VL-PTMs, is aligning text with image regions. The discriminative features of objects are amplified, while other features may not receive as much attention. For instance, the shape and color are the discriminative features of an apple, and its size is not that important in recognition. In image synthesis, models need to learn comprehensive knowledge of objects for reconstruction, and spatial knowledge may be learned implicitly in this process. Figure 4 demonstrates images generated by the ISM given the prompts of object pairs. ISM grasps the main characteristics of the objects, including their scales. Some objects (like theatre at the bottom of the middle column) can be identified by humans but are difficult for the object detection model because they are obstructed by objects in the foreground. And some objects are generated in multiple fragments (like plane and horse in the right column), therefore cannot be recognized by either the object detection model or humans. 2 The agreement between annotators is more than 90%.  Positional Relationship. The probing performance on positional relationship is shown in Table 3. VinVL outperforms PLMs more than 20%, and ISM (Human) outperforms PLMs more than 35%, suggesting that models with visual signals learn more knowledge of the scenarios, especially the positions of objects relative to people. The gap between PLMs and ISM (Box) is smaller compared to the gap in the subtask of size and height. One reason is that the rules defined in VDG cannot perfectly reflect the true positional relationship in images. For example, the man is beside the car in the left image of Figure 3, but he will be regarded as inside the car by the rules, as the region of car covers the region of man.</p>
<p>Text-based PLMs tend to lean towards certain positional relations between a person and an object, without referring to the action. In 64% cases, RoBERTa chooses the same option for a ⟨person, object⟩ pair with different actions, while the proportion is 21% for VinVL, and 28% for ISM (Human).</p>
<p>Quality of Spatial Knowledge</p>
<p>Consistency</p>
<p>Models that master better spatial knowledge should be able to infer the relative scale of two objects from intermediate references. For example, if a model knows a dog is larger than an ant and a sofa is larger than a dog, it may learn a sofa is larger than an ant, even if it has not seen sofa and ant together. We inspect models on how consistent their probing results are.</p>
<p>The consistency is measured in two aspects: symmetry and transitivity. Symmetry implies that if a model predicts A &gt; B, then it should also predict B &lt; A, and vice versa: A &lt; B =⇒ B &gt; A.</p>
<p>Here &gt; and &lt; are in terms of size or height. We enumerate the object pairs and count the percent- c is the current object and A is the set of all other comparable objects. #(c &gt; a)/|A| indicates the ratio of predicting the current object larger than others. As c &gt; a and a &gt; c should not appear simultaneously, the sum of the two solid bars is expected to be 1. age of predictions that meet the symmetry criterion. Transitivity implies that if a model predicts A &gt; B and B &gt; C, then it should predict A &gt; C. It also works for &lt;, A &lt; B ∧ B &lt; C =⇒ A &lt; C. We enumerate the triples ⟨A, B, C⟩ where the predicted relation between ⟨A, B⟩ is identical to the prediction between ⟨B, C⟩, and count the percentage that the prediction between ⟨A, C⟩ meets the transitivity criterion. Note that we only evaluate whether the predictions are consistent with each other, regardless of the gold answers.</p>
<p>We evaluate the consistency of predictions from PLMs that perform the best in the probing tasks (RoBERTa for size and BERT for height), VinVL, and ISM (Human). The results are in Table 4.</p>
<p>VinVL outperforms the best PLM in both metrics, and the characteristics of them are similar: the transitive consistency is high, while the symmetric consistency is low. To further analyze this phenomenon, we exhibit each object's size predictions from RoBERTa and VinVL in Figure 5. The models exhibit different behaviors in recognizing object scales. As the objects (X-axis of Figure 5) are roughly listed from smaller to larger groups, the bottom blue bars are expected to follow a nondescending order from left to right, and the solid orange bars should be non-ascending. The predictions of VinVL are generally in line with this trend, while RoBERTa's predictions are disordered. For example, ant is predicted to be larger than other objects with high probability, and cinema is larger than others is unlikely to happen. On the other hand, if the model predictions are consistent,  the two solid bars should sum to 1. However, the sum is far above 1 for most objects in VinVL's predictions. This bias towards words indicating the choice of large may come from the pretraining corpus. For example, sofa occurs twice as many times with words indicating large as with words indicating small in COCO (Lin et al., 2014), one of VinVL's pretraining datasets. ISM's predictions comply with the symmetry criterion, outperforming other models by 40%, while also having good transitive consistency. The knowledge probed from ISM is more consistent. Figure 6 exhibits the symmetric and transitive consistency of images generated by ISM. The consistency of scale knowledge makes the predictions more convincing, and gives models a chance to learn new comparisons between objects.</p>
<p>Generalizability</p>
<p>ISM may learn positional relations from training images directly. For example, a boy riding a bi-   cycle is a common scenario and may frequently exist in ISM's training set, so models can generate images more easily when being fed with the text prompts like a boy rides a bicycle. To further challenge ISM's capability, we make a generalized version of our original positional relationship dataset. It is designed to examine whether models are able to robustly reflect the spatial commonsense knowledge when facing uncommon scenarios.</p>
<p>A generalized scenario is built upon the original one by replacing the person and object in the text prompts. We select the new person and new object from the subterms of the original ones (those with IsA relation in ConceptNet (Speer et al., 2017), like enchantress is a woman). To ensure these newly constructed scenarios are not likely to appear in the training data of models, we search them in BookCorpus (Zhu et al., 2015) and remove the scenarios that have appeared. The newly generated scenarios are also validated by humans to ensure that they are reasonable.</p>
<p>Results of probing PLMs, VinVL, and ISM 3 on the generalized dataset are in Table 5. PLMs and VinVL achieve similar performance on both the generalized dataset and the original one, indicating that they behave robustly when facing uncommon scenarios. The performance gap between other models and ISM (Human) slightly narrows down, but ISM (Human) still outperforms VinVL more than 8%. Figure 7 exhibits images generated by ISM with the generalized prompts. Although it is A housefather is feeding the foal.</p>
<p>A schoolgirl climbs the cherry tree.</p>
<p>An enchantress lights the sparkler. difficult for ISM to generate unfamiliar objects, it is still capable of capturing the positional relations.</p>
<p>Solving Natural Language Questions</p>
<p>We investigate how to acquire spatial knowledge from ISMs and whether the knowledge is effective in natural language understanding scenarios. To our best knowledge, there is no appropriate task that focuses on spatial commonsense, so we create a toy task by transforming our probing benchmark into the form of question answering (QA).   (Raffel et al., 2019), Uni-fiedQA is continually trained on various QA tasks, including three yes/no datasets. We use UnifiedQAlarge, which is comparable with our synthesis and reasoning model (ISM w/ VinVL) in size. Table 6, ISM w/ VinVL outperforms UnifiedQA on all subtasks, showing that spatial knowledge from ISMs can be directly used by vision-language models without additional training. Although some images cannot be precisely recognized by object detection models, visionlanguage models may find regions that are related to the objects mentioned in questions, and make decisions based on the features of these regions. The results on the simple natural language task show that it is beneficial to tackle natural language tasks with vision-language methods, and ISMs can be a bridge between the two modalities. With the development of ISMs and object detection techniques, we believe the generated images will help more.</p>
<p>Results. As shown in</p>
<p>Conclusion</p>
<p>We propose a new spatial commonsense probing framework to investigate object scales and positional relationship knowledge in text-based pretrained models and models with visual signals. Experimental results show that models with visual signals, especially ISMs, learn more accurate and consistent spatial commonsense than text-only models. Integrating ISMs with visual reasoning models outperforms PLMs in answering spatial questions.</p>
<p>This manifests the potential of using spatial knowledge from ISMs in natural language understanding tasks. </p>
<p>A Implementation Details</p>
<p>Relation Definition X inside Y The entirety of region X overlaps with Y. X beside Y The angle between the centroid of X and the centroid of Y lies between 315 • and 45 • or 135 • and 225 • . X above Y The angle between X and Y lies between 225 • and 315 • . X below Y The angle between X and Y lies between 45 • and 135 • . </p>
<p>A.1 Spatial Relations in Visual Dependency Grammar</p>
<p>We use the rules defined in Visual Dependency Grammar (Elliott and Keller, 2013) to determine the positional relationship between bounding boxes. The rules used are listed in Table 7. If two bounding boxes meet the inside standard, they will be predicted as inside. Otherwise, the angle between the centers of the boxes is calculated to determine whether the prediction is above, below, or beside.</p>
<p>A.2 Image Synthesis</p>
<p>We generate images of 512 × 512 pixels with text prompts. We use 1) VQGAN (Esser et al., 2021), which takes in a vector, and outputs a highresolution image; and 2) CLIP (Radford et al., 2021), which can encode both text and images, and map them into a multi-modal embedding space. Image synthesis is the process of finding the optimal vector v inputted to VQGAN. In each iteration, the vector is fed into VQGAN to generate an image img = VQGAN(v). CLIP encodes the image into c = CLIP(img), and encodes the text prompt into t = CLIP(text), respectively. The optimization goal is to bring c and t, the representation of the image and text encoded by CLIP closer. The vector v is randomly initialized and optimized for 600 iterations. We use Adam optimizer with a learning rate of 0.5. This process looks like a normal model "training", but here both VQGAN and CLIP are pretrained and their parameters are frozen; only the vector v is optimized from randomness for every prompt.</p>
<p>A.3 Prompt Candidates Generation</p>
<p>When probing PLMs, we follow Jiang et al. (2020) to generate prompt and answer candidates with  back-translation. Manually designed prompts and answers are translated from English to German and then backward. It is used to construct candidates with similar meanings. We leverage the translation model designed in Ng et al. (2019).</p>
<p>A.4 Computing Infrastructure</p>
<p>Experiments are conducted on NVIDIA GeForce RTX 3090 GPU. It takes 8 hours to generate 500 images on one GPU, and all other experiments can be executed in a few minutes.</p>
<p>B Probing Results on RelativeSize</p>
<p>RelativeSize (Bagherinezhad et al., 2016) is another dataset for comparing objects' sizes. Table 8 demonstrates the probing results on it. The results are consistent with those on our datasets: ISM probing, both evaluated with bounding boxes and evaluated by humans, outperforms PLM probing. The methods used in Bagherinezhad et al. (2016) are all retrieval-based. They execute search engine queries and download images from Flickr to make the comparisons. So we do not compare with their results directly. However, it is worth noticing that our ISM probing is comparable to the image retrieval-based baseline (its accuracy is 72.4%). It exhibits that ISM learns sufficient knowledge from images.</p>
<p>Figure 1 :
1Texts and images related to lion and cycling. Images exhibit more explicit spatial knowledge than texts.</p>
<p>Figure 3 :
3Example of two positional relations between man and car.</p>
<p>Figure 4 :
4Images generated by ISM in scale comparison. ✓means objects are successfully recognized by the object detection model or humans, and × means not.</p>
<p>Acc (avg. / σ) F1 (avg. / σ)</p>
<p>Figure 5 :
5Predictions from RoBERTa and VinVL in the subtask of objects' sizes.</p>
<p>groups of generated images. Heights of objects meet the transitivity criterion.</p>
<p>Figure 6 :
6Examples of the symmetric and transitive consistency of images generated by ISM.</p>
<p>Figure 7 :
7Images generated by ISM with the generalized prompts.</p>
<p>2 Related Works 2.1 Spatial Commonsense Reasoning Object Scales. Bagherinezhad et al. (2016) build a dataset for objects' size comparison, and Elazar et al. (2019) provide distributional information about objects' lengths. Forbes and Choi (2017) also involve spatial comparison but are criticized for ill-defined comparison (Elazar et al., 2019). Aroca-Ouellette et al.</p>
<p>A sofa is [MASK] than a mountain.A sofa and a mountain. </p>
<p>PLM </p>
<p>larger, 7.2% 
smaller, 6.6% </p>
<p>Text prompt </p>
<p>sofa &lt; mountain </p>
<p>ISM ISM </p>
<p>VL-
PTM </p>
<p>larger, 6.7% 
smaller, 10.3% </p>
<p>to construct the dataset for size comparison. Typical Height 1 ant, insect, water drop, bullet, dice 2 bird, cup, shoe, bottle, mobile phone 3 table, chair, trash can, sofa, suitcase 4 human, horse, bookshelf, camel, door 5 apartment, theatre, giraffe, truck, street lamp (b) Objects of different levels of heights.Size </p>
<p>1 ant, coin, nut, bullet, dice 
2 bird, cup, shell, bottle, wallet 
3 tyre, chair, microwave, dog, suitcase 
4 human, sofa, bookshelf, tiger, bed 
5 house, cinema, mountain, truck, plane </p>
<p>(a) Objects of different levels of sizes. </p>
<p>Table 1 :
1The dataset of object scales.</p>
<p>ModelAcc (avg. / σ) F1 (avg. / σ) Comparing sizes of objects. Both objects are recognized by the object detection model in 15% images and are recognized by humans in 86% images. Comparing heights of objects. Both objects are recognized by the object detection model in 14% images and are recognized by humans in 82% images.BERT 
49.8 / 2.66 
47.7 / 2.48 
RoBERTa 
54.1 / 3.93 
52.2 / 6.92 
VinVL 
61.8 / 2.47 
54.4 / 3.06 </p>
<p>Model 
Acc 
F1 </p>
<p>Best PLM  § 
54.1 (52.2) 
52.2 (46.7) 
VinVL  § 
61.8 (61.6) 
54.4 (53.8) 
ISM (Box)  § 
54.8 (81.6) 
54.8 (81.6) </p>
<p>Best PLM  † 
54.1 (52.9) 
52.2 (51.0) 
VinVL  † 
61.8 (61.6) 
54.4 (54.3) 
ISM (Human)  † 
72.7 (76.5) 
72.6 (76.4) </p>
<p>(a) Model 
Acc (avg. / σ) F1 (avg. / σ) </p>
<p>BERT 
50.8 / 2.29 
50.3 / 0.25 
RoBERTa 
50.8 / 6.43 
49.2 / 7.45 
VinVL 
64.5 / 7.61 
61.5 / 10.5 </p>
<p>Model 
Acc 
F1 </p>
<p>Best PLM  § 
50.8 (48.6) 
50.3 (47.9) 
VinVL  § 
64.5 (69.3) 
61.5 (65.2) 
ISM (Box)  § 
52.5 (68.1) 
52.5 (68.1) </p>
<p>Best PLM  † 
50.8 (48.5) 
50.3 (47.5) 
VinVL  † 
64.5 (63.9) 
61.5 (60.6) 
ISM (Human)  † 
78.9 (85.4) 
78.8 (85.3) </p>
<p>(b) </p>
<p>Table 2 :
2Probing performance on object scales. The numbers are in percentages (%). The number before the slash (/) is the average performance of different folds, and the number after the slash is the standard deviation. The number out of parentheses is the performance on the whole dataset, and the number in parentheses indicates performance on the subset of instances where the generated images can be recognized by object detection models ( § ), and on the subset recognized by humans ( † ). as a representative of ISMs. It uses CLIP(Radford et al., 2021)  to guideVQGAN (Esser et al.,  2021)  to generate images that best match the given text. To make a fair comparison regarding model size, we select BERT-large, RoBERTa-large, and VinVL-large. We use VQGAN with codebook size Z = 16384 and downsampling factor f = 16, and CLIP with ViT-B/32 (Dosovitskiy et al., 2020) architecture. All four models are of similar sizes.</p>
<p>in Appendix B. We observe that PLMs perform similarly. Even theA house and a bird 
A bottle and a 
bookshelf </p>
<p>A plane and a 
bullet </p>
<p>Size </p>
<p>Height </p>
<p>A bird and a trash 
can </p>
<p>A trash can and a 
theatre </p>
<p>An apartment and 
a horse </p>
<p>Table 3 :
3Probing performance on positional relationship 
(%). The symbols are identical to those in Table 2. Both 
the person and the object are recognized with bounding 
boxes in 39% images and by humans in 93% images. </p>
<p>ModelSize Height Sym. Trans. Sym. Trans.Best PLM 
37.5 
71.9 
25.9 
73.1 
VinVL 
43.5 
95.0 
43.0 
93.2 </p>
<p>Best PLM  † 
36.6 
72.2 
26.1 
72.3 
VinVL  † 
44.4 
95.3 
32.2 
97.8 
ISM (Human)  † 
82.5 
81.1 
83.2 
85.2 </p>
<p>Table 4 :
4The percentage (%) of predictions that meet consistency. Sym and Trans indicate symmetry and transitivity. † indicates performance on the subset of images recognized by humans.</p>
<p>Table 5 :
5Probing models on the generalized dataset of positional relationship. The symbols are identical to those inTable 2. The human recognition ratio is 81%.</p>
<p>Table 6 :
6Performance of answering commonsense ques-
tions. Accuracy (%) and macro F1 (%) are reported. 
PosRel refers to positional relationship. </p>
<p>Models. We use VinVL-base together with our 
image synthesis model VQGAN+CLIP to answer 
spatial commonsense questions. The VinVL here 
is finetuned on the VQA (Goyal et al., 2017) task. 
It takes images generated from ISM with textual 
prompts from questions, and predicts the answer 
based on the question and image together. Note 
that the VQA training corpus does not contain com-
monsense reasoning questions. 
We choose UnifiedQA (Khashabi et al., 2020) as 
a text-based QA model for comparison. Based on 
the pretrained T5 model </p>
<p>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations. Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, and Dan Roth. 2019. How large are lions? inducing distributions over quantitative attributes. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3973-3983. Desmond Elliott and Frank Keller. 2013. Image description using visual dependency representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292-1302. Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12873-12883. Maxwell Forbes and Yejin Choi. 2017. Verb physics: Relative physical knowledge of actions and objects. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 266-276. Clément Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. 2019. Digging into selfsupervised monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3828-3838. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6904-6913. Harry M Hersh and Alfonso Caramazza. 1976. A fuzzy set approach to modifiers and vagueness in natural language. Journal of Experimental Psychology: General, 105(3):254. John Hewitt and Christopher D Manning. 2019. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129-4138. Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700-6709.Tyrone Donnon, Jean-Gaston DesCôteaux, and Claudio 
Violato. 2005. Impact of cognitive imaging and sex 
differences on the development of laparoscopic sutur-
ing skills. Canadian journal of surgery, 48(5):387. </p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander 
Kolesnikov, Yanai </p>
<p>Table 7 :
7Spatial relations between image regions in Visual Dependency Grammar (VDG).</p>
<p>ModelAcc (avg. / σ) F1 (avg. / σ)BERT 
49.0 / 4.11 
43.7 / 8.25 
RoBERTa 
48.9 / 1.71 
43.4 / 5.42 
VinVL 
60.6 / 1.47 
51.2 / 2.22 </p>
<p>Model 
Acc 
F1 </p>
<p>Best PLM 
49.0 (47.5) 
43.7 (40.5) 
VinVL 
60.6 (60.8) 
51.2 (49.8) 
ISM (Box) 
58.5 (71.5) 
58.5 (71.4) </p>
<p>Best PLM 
49.0 (48.5) 
43.7 (43.5) 
VinVL 
60.6 (65.5) 
51.2 (55.7) 
ISM (Human) 
72.5 (76.5) 
71.8 (75.7) </p>
<p>Table 8 :
8Probing performance on RelatizeSize. Accuracy and macro F1 are reported. The numbers are in percentages (%). In the last six lines, the first number is the performance on the whole dataset, and the number in parentheses indicates performance on the subset of instances where the generated images can be recognized by object detection models and humans, respectively. The standard deviation on different folds is represented with σ. Both objects are recognized with bounding boxes in 40% images and are recognized by humans in 85% images.
Originated by Ryan Murdoch, @advadnoun on Twitter. Implementation details are in Appendix A.2.
We do not consider ISM (Box) because many new objects we used are unfamiliar to object detection models. Only 17% of the objects are in the object detection classes.
AcknowledgmentsThis work is supported in part by National Key R&amp;D Program of China (No. 2020AAA0106600) and NSFC (62161160339). We would like to thank the anonymous reviewers and action editor for the helpful discussions and suggestions. Also, we would thank Quzhe Huang, Chen Zhang, Chen Henry Wu, Yuxuan Lai and Nan Hu for their detailed comments. For any correspondence, please contact Yansong Feng.
Stéphane Aroca-Ouellette, Cory Paik, Alessandro Roncone, Katharina Kann, arXiv:2106.03634Prost: Physical reasoning of objects through space and time. arXiv preprintStéphane Aroca-Ouellette, Cory Paik, Alessandro Ron- cone, and Katharina Kann. 2021. Prost: Physical reasoning of objects through space and time. arXiv preprint arXiv:2106.03634.</p>
<p>Are elephants bigger than butterflies? reasoning about sizes of objects. Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi, Thirtieth AAAI Conference on Artificial Intelligence. Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. 2016. Are elephants bigger than butterflies? reasoning about sizes of objects. In Thirtieth AAAI Conference on Artificial Intelligence.</p>
<p>Explaining self-supervised image representations with visual probing. Dominika Basaj, Witold Oleszkiewicz, Igor Sieradzki, Michał Górszczak, Rychalska, B Trzcinski, Zielinski, International Joint Conference on Artificial Intelligence. Dominika Basaj, Witold Oleszkiewicz, Igor Sieradzki, Michał Górszczak, B Rychalska, T Trzcinski, and B Zielinski. 2021. Explaining self-supervised image representations with visual probing. In International Joint Conference on Artificial Intelligence.</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Chaitanya Ronan Le Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, ICLR. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han- nah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In ICLR.</p>
<p>Acquiring common sense spatial knowledge through implicit spatial templates. Guillem Collell, Luc Van Gool, Marie-Francine Moens, Thirty-second AAAI conference on artificial intelligence. Guillem Collell, Luc Van Gool, and Marie-Francine Moens. 2018. Acquiring common sense spatial knowledge through implicit spatial templates. In Thirty-second AAAI conference on artificial intelli- gence.</p>
<p>Beyond language: Learning commonsense from images for reasoning. Wanqing Cui, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng, 10.18653/v1/2020.findings-emnlp.392Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational LinguisticsWanqing Cui, Yanyan Lan, Liang Pang, Jiafeng Guo, and Xueqi Cheng. 2020. Beyond language: Learn- ing commonsense from images for reasoning. In Findings of the Association for Computational Lin- guistics: EMNLP 2020, pages 4379-4389, Online. Association for Computational Linguistics.</p>
<p>Commonsense knowledge mining from pretrained models. Joe Davison, Joshua Feldman, Alexander M Rush, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)Joe Davison, Joshua Feldman, and Alexander M Rush. 2019. Commonsense knowledge mining from pre- trained models. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1173-1178.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In NAACL-HLT (1).</p>
<p>How can we know what language models know?. Zhengbao Jiang, F Frank, Jun Xu, Graham Araki, Neubig, Transactions of the Association for Computational Linguistics. 8Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901-2910.</p>
<p>Unifiedqa: Crossing format boundaries with a single qa system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab- harwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format bound- aries with a single qa system. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 1896-1907.</p>
<p>Commonsense knowledge of space: Learning from experience. Benjamin Kuipers, Advances in Spatial Reasoning. 2199Benjamin Kuipers et al. 1990. Commonsense knowl- edge of space: Learning from experience. Advances in Spatial Reasoning, 2:199.</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European confer- ence on computer vision, pages 740-755. Springer.</p>
<p>Linguistic knowledge and transferability of contextual representations. F Nelson, Matt Liu, Yonatan Gardner, Belinkov, E Matthew, Noah A Peters, Smith, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long and Short PapersNelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew E Peters, and Noah A Smith. 2019a. Lin- guistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Pa- pers), pages 1073-1094.</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, arXiv:2107.13586arXiv preprintPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.</p>
<p>Spartqa: A textual question answering benchmark for spatial reasoning. Roshanak Mirzaee, Qiang Hossein Rajaby Faghihi, Parisa Ning, Kordjamshidi, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesRoshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. Spartqa: A textual question answering benchmark for spatial rea- soning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 4582-4598.</p>
<p>Facebook fair's wmt19 news translation task submission. Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov, Proceedings of the Fourth Conference on Machine Translation. the Fourth Conference on Machine Translation2Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook fair's wmt19 news translation task submission. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 314-319.</p>
<p>Visual probing: Cognitive framework for explaining self-supervised image representations. Witold Oleszkiewicz, Dominika Basaj, Igor Sieradzki, Michał Górszczak, Barbara Rychalska, Koryna Lewandowska, Tomasz Trzciński, Bartosz Zieliński, arXiv:2106.11054arXiv preprintWitold Oleszkiewicz, Dominika Basaj, Igor Sier- adzki, Michał Górszczak, Barbara Rychalska, Ko- ryna Lewandowska, Tomasz Trzciński, and Bartosz Zieliński. 2021. Visual probing: Cognitive frame- work for explaining self-supervised image represen- tations. arXiv preprint arXiv:2106.11054.</p>
<p>Understanding spatial ability. W James, Pellegrino, Valerie J David L Alderton, Shute, Educational psychologist. 194James W Pellegrino, David L Alderton, and Valerie J Shute. 1984. Understanding spatial ability. Educa- tional psychologist, 19(4):239-253.</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 2463-2473.</p>
<p>Information-theoretic probing for linguistic structure. Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, Ryan Cotterell, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020. Information-theoretic probing for linguistic structure. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 4609-4622.</p>
<p>Development: Ages &amp; stages-spatial awareness. Carla Poole, A Susan, Ellen Booth Miller, Church, Early Childhood Today20Carla Poole, Susan A Miller, and Ellen Booth Church. 2006. Development: Ages &amp; stages-spatial aware- ness. Early Childhood Today, 20(6):25-30.</p>
<p>Girish Sastry. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, arXiv:2103.00020Amanda Askell, Pamela Mishkin, Jack ClarkarXiv preprintet al. 2021. Learning transferable visual models from natural language supervisionAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.</p>
<p>Influence of spatial perception abilities on reading in school-age children. Arnaud Saj, Koviljka Barisnikov, Cogent Psychology. 211049736Arnaud Saj and Koviljka Barisnikov. 2015. Influence of spatial perception abilities on reading in school-age children. Cogent Psychology, 2(1):1049736.</p>
<p>It's not just size that matters: Small language models are also fewshot learners. Timo Schick, Hinrich Schütze, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesTimo Schick and Hinrich Schütze. 2021. It's not just size that matters: Small language models are also few- shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of gen- eral knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Stating the obvious: Extracting visual common sense knowledge. Mark Yatskar, Vicente Ordonez, Ali Farhadi, 10.18653/v1/N16-1023Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsMark Yatskar, Vicente Ordonez, and Ali Farhadi. 2016. Stating the obvious: Extracting visual common sense knowledge. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 193-198, San Diego, California. Association for Computational Linguistics.</p>
<p>Vinvl: Revisiting visual representations in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian- feng Gao. 2021. Vinvl: Revisiting visual representa- tions in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5579-5588.</p>
<p>Do language embeddings capture scales?. Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPXikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Proceedings of the Third BlackboxNLP Workshop on Analyzing and In- terpreting Neural Networks for NLP, pages 292-299.</p>
<p>Learning to prompt for visionlanguage models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, arXiv:2109.01134arXiv preprintKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2021. Learning to prompt for vision- language models. arXiv preprint arXiv:2109.01134.</p>
<p>Evaluating commonsense in pretrained language models. Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Xuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. 2020. Evaluating commonsense in pre- trained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9733-9740.</p>
<p>Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE in- ternational conference on computer vision, pages 19-27.</p>            </div>
        </div>

    </div>
</body>
</html>
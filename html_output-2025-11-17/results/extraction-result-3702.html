<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3702 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3702</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3702</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-e9da2ce19846c5dd2497e323ebbacd991fbe1c20</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e9da2ce19846c5dd2497e323ebbacd991fbe1c20" target="_blank">TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work addresses the task of link forecasting on temporal knowledge graphs and introduces TLogic, an explainable framework that is based on temporal logical rules extracted via temporal random walks, and works well in the inductive setting where already learned rules are transferred to related datasets with a common vocabulary.</p>
                <p><strong>Paper Abstract:</strong> Conventional static knowledge graphs model entities in relational data as nodes, connected by edges of specific relation types. However, information and knowledge evolve continuously, and temporal dynamics emerge, which are expected to influence future situations. In temporal knowledge graphs, time information is integrated into the graph by equipping each edge with a timestamp or a time range. Embedding-based methods have been introduced for link prediction on temporal knowledge graphs, but they mostly lack explainability and comprehensible reasoning chains. Particularly, they are usually not designed to deal with link forecasting -- event prediction involving future timestamps. We address the task of link forecasting on temporal knowledge graphs and introduce TLogic, an explainable framework that is based on temporal logical rules extracted via temporal random walks. We compare TLogic with state-of-the-art baselines on three benchmark datasets and show better overall performance while our method also provides explanations that preserve time consistency. Furthermore, in contrast to most state-of-the-art embedding-based methods, TLogic works well in the inductive setting where already learned rules are transferred to related datasets with a common vocabulary.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3702",
    "paper_id": "paper-e9da2ce19846c5dd2497e323ebbacd991fbe1c20",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00398075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs</h1>
<p>Yushan Liu ${ }^{1,2}$, Yunpu Ma ${ }^{1,2}$, Marcel Hildebrandt ${ }^{1}$, Mitchell Joblin ${ }^{1}$, Volker Tresp ${ }^{1,2}$<br>${ }^{1}$ Siemens AG, Otto-Hahn-Ring 6, 81739 Munich, Germany<br>${ }^{2}$ Ludwig Maximilian University of Munich, Geschwister-Scholl-Platz 1, 80539 Munich, Germany<br>{yushan.liu, yunpu.ma, marcel.hildebrandt, mitchell.joblin, volker.tresp}@siemens.com</p>
<h4>Abstract</h4>
<p>Conventional static knowledge graphs model entities in relational data as nodes, connected by edges of specific relation types. However, information and knowledge evolve continuously, and temporal dynamics emerge, which are expected to influence future situations. In temporal knowledge graphs, time information is integrated into the graph by equipping each edge with a timestamp or a time range. Embeddingbased methods have been introduced for link prediction on temporal knowledge graphs, but they mostly lack explainability and comprehensible reasoning chains. Particularly, they are usually not designed to deal with link forecasting - event prediction involving future timestamps. We address the task of link forecasting on temporal knowledge graphs and introduce TLogic, an explainable framework that is based on temporal logical rules extracted via temporal random walks. We compare TLogic with state-of-the-art baselines on three benchmark datasets and show better overall performance while our method also provides explanations that preserve time consistency. Furthermore, in contrast to most state-of-the-art embedding-based methods, TLogic works well in the inductive setting where already learned rules are transferred to related datasets with a common vocabulary.</p>
<h2>Introduction</h2>
<p>Knowledge graphs (KGs) structure factual information in the form of triples $\left(e_{s}, r, e_{o}\right)$, where $e_{s}$ and $e_{o}$ correspond to entities in the real world and $r$ to a binary relation, e. g., (Anna, born in, Paris). This knowledge representation leads to an interpretation as a directed multigraph, where entities are identified with nodes and relations with edge types. Each edge $\left(e_{s}, r, e_{o}\right)$ in the KG encodes an observed fact, where the source node $e_{s}$ corresponds to the subject entity, the target node $e_{o}$ to the object entity, and the edge type $r$ to the predicate of the factual statement.</p>
<p>Some real-world information also includes a temporal dimension, e.g., the event (Anna, born in, Paris) happened on a specific date. To model the large amount of available event data that induce complex interactions between entities over time, temporal knowledge graphs ( tKGs ) have been introduced. Temporal KGs extend the triples to quadruples $\left(e_{s}, r, e_{o}, t\right)$ to integrate a timestamp or time range $t$, where $t$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A subgraph from the dataset ICEWS14 with the entities Angela Merkel, Barack Obama, France, and China. The timestamps are displayed in the format yy/mm/dd. The dotted blue line represents the correct answer to the query (Angela Merkel, consult, ?, 2014/08/09). Previous interactions between Angela Merkel and Barack Obama can be interpreted as an explanation for the prediction.
indicates the time validity of the static event $\left(e_{s}, r, e_{o}\right)$, e. g., (Angela Merkel, visit, China, 2014/07/04). Figure 1 visualizes a subgraph from the dataset ICEWS14 as an example of a tKG. In this work, we focus on tKGs where each edge is equipped with a single timestamp.</p>
<p>One of the common tasks on KGs is link prediction, which finds application in areas such as recommender systems (Hildebrandt et al. 2019), knowledge base completion (Nguyen et al. 2018a), and drug repurposing (Liu et al. 2021). Taking the additional temporal dimension into account, it is of special interest to forecast events for future timestamps based on past information. Notable real-world applications that rely on accurate event forecasting are, e. g., clinical decision support, supply chain management, and extreme events modeling. In this work, we address link forecasting on tKGs, where we consider queries $\left(e_{s}, r, ?, t\right)$ for a timestamp $t$ that has not been seen during training.</p>
<p>Several embedding-based methods have been introduced for tKGs to solve link prediction and forecasting (link prediction with future timestamps), e.g., TTransE (Leblay and Chekol 2018), TNTComplEx (Lacroix, Obozinski, and Usunier 2020), and RE-Net (Jin et al. 2019). The underlying principle is to project the entities and relations into a lowdimensional vector space while preserving the topology and temporal dynamics of the tKG. These methods can learn the complex patterns that lead to an event but often lack transparency and interpretability.</p>
<p>To increase the transparency and trustworthiness of the solutions, human-understandable explanations are necessary, which can be provided by logical rules. However, the manual creation of rules is often difficult due to the complex nature of events. Domain experts cannot articulate the conditions for the occurrence of an event sufficiently formally to express this knowledge as rules, which leads to a problem termed as the knowledge acquisition bottleneck. Generally, symbolic methods that make use of logical rules tend to suffer from scalability issues, which make them impractical for the application on large real-world datasets.</p>
<p>We propose TLogic that automatically mines cyclic temporal logical rules by extracting temporal random walks from the graph. We achieve both high predictive performance and time-consistent explanations in the form of temporal rules, which conform to the observation that the occurrence of an event is usually triggered by previous events. The main contributions of this work are summarized as follows:</p>
<ul>
<li>We introduce TLogic, a novel symbolic framework based on temporal random walks in temporal knowledge graphs. It is the first approach that directly learns temporal logical rules from tKGs and applies these rules to the link forecasting task.</li>
<li>Our approach provides explicit and human-readable explanations in the form of temporal logical rules and is scalable to large datasets.</li>
<li>We conduct experiments on three benchmark datasets (ICEWS14, ICEWS18, and ICEWS0515) and show better overall performance compared with state-of-the-art baselines.</li>
<li>We demonstrate the effectiveness of our method in the inductive setting where our learned rules are transferred to a related dataset with a common vocabulary.</li>
</ul>
<h2>Related Work</h2>
<p>Subsymbolic machine learning methods, e. g., embeddingbased algorithms, have achieved success for the link prediction task on static KGs. Well-known methods include RESCAL (Nickel, Tresp, and Kriegel 2011), TransE (Bordes et al. 2013), DistMult (Yang et al. 2015), and ComplEx (Trouillon et al. 2016) as well as the graph convolutional approaches R-GCN (Schlichtkrull et al. 2018) and CompGCN (Vashishth et al. 2020). Several approaches have been recently proposed to handle tKGs, such as TTransE (Leblay and Chekol 2018), TA-DistMult (GarcíaDurán, Dumančić, and Niepert 2018), DE-SimplE (Goel et al. 2020), TNTComplEx (Lacroix, Obozinski, and Usunier 2020), CyGNet (Zhu et al. 2021), RE-Net (Jin et al.</p>
<p>2019), and xERTE (Han et al. 2021). The main idea of these methods is to explicitly learn embeddings for timestamps or to integrate temporal information into the entity or relation embeddings. However, the black-box property of embeddings makes it difficult for humans to understand the predictions. Moreover, approaches with shallow embeddings are not suitable for an inductive setting with previously unseen entities, relations, or timestamps. From the above methods, only CyGNet, RE-Net, and xERTE are designed for the forecasting task. xERTE is also able to provide explanations by extracting relevant subgraphs around the query subject.</p>
<p>Symbolic approaches for link prediction on KGs like AMIE+ (Galárraga et al. 2015) and AnyBURL (Meilicke et al. 2019) mine logical rules from the dataset, which are then applied to predict new links. StreamLearner (Omran, Wang, and Wang 2019) is one of the first methods for learning temporal rules. It employs a static rule learner to generate rules, which are then generalized to the temporal domain. However, they only consider a rather restricted set of temporal rules, where all body atoms have the same timestamp.</p>
<p>Another class of approaches is based on random walks in the graph, where the walks can support an interpretable explanation for the predictions. For example, AnyBURL samples random walks for generating rules. The methods dynnode2vec (Mahdavi, Khoshraftar, and An 2018) and change2vec (Bian et al. 2019) alternately extract random walks on tKG snapshots and learn parameters for node embeddings, but they do not capture temporal patterns within the random walks. Nguyen et al. (2018b) extend the concept of random walks to temporal random walks on continuous-time dynamic networks for learning node embeddings, where the sequence of edges in the walk only moves forward in time.</p>
<h2>Preliminaries</h2>
<p>Let $[n]:={1,2, \ldots, n}$.
Temporal knowledge graph Let $\mathcal{E}$ denote the set of entities, $\mathcal{R}$ the set of relations, and $\mathcal{T}$ the set of timestamps.</p>
<p>A temporal knowledge graph (tKG) is a collection of facts $\mathcal{G} \subset \mathcal{E} \times \mathcal{R} \times \mathcal{E} \times \mathcal{T}$, where each fact is represented by a quadruple $\left(e_{s}, r, e_{o}, t\right)$. The quadruple $\left(e_{s}, r, e_{o}, t\right)$ is also called link or edge, and it indicates a connection between the subject entity $e_{s} \in \mathcal{E}$ and the object entity $e_{o} \in \mathcal{E}$ via the relation $r \in \mathcal{R}$. The timestamp $t \in \mathcal{T}$ implies the occurrence of the event $\left(e_{s}, r, e_{o}\right)$ at time $t$, where $t$ can be measured in units such as hour, day, and year.</p>
<p>For two timestamps $t$ and $\hat{t}$, we denote the fact that $t$ occurs earlier than $\hat{t}$ by $t&lt;\hat{t}$. If additionally, $t$ could represent the same time as $\hat{t}$, we write $t \leq \hat{t}$.</p>
<p>We define for each edge $\left(e_{s}, r, e_{o}, t\right)$ an inverse edge $\left(e_{o}, r^{-1}, e_{s}, t\right)$ that interchanges the positions of the subject and object entity to allow the random walker to move along the edge in both directions. The relation $r^{-1} \in \mathcal{R}$ is called the inverse relation of $r$.</p>
<p>Link forecasting The goal of the link forecasting task is to predict new links for future timestamps. Given a query with a previously unseen timestamp $\left(e_{s}, r, ?, t\right)$, we want to identify a ranked list of object candidates that are most likely</p>
<p>to complete the query. For subject prediction, we formulate the query as $\left(e_{o}, r^{-1}, ?, t\right)$.
Temporal random walk A non-increasing temporal random walk $W$ of length $l \in \mathbb{N}$ from entity $e_{l+1} \in \mathcal{E}$ to entity $e_{1} \in \mathcal{E}$ in the $\operatorname{tKG} \mathcal{G}$ is defined as a sequence of edges</p>
<p>$$
\begin{gathered}
\left(\left(e_{l+1}, r_{l}, e_{l}, t_{l}\right),\left(e_{l}, r_{l-1}, e_{l-1}, t_{l-1}\right), \ldots,\left(e_{2}, r_{1}, e_{1}, t_{1}\right)\right) \
\text { with } t_{l} \geq t_{l-1} \geq \cdots \geq t_{1}
\end{gathered}
$$</p>
<p>where $\left(e_{i+1}, r_{i}, e_{i}, t_{i}\right) \in \mathcal{G}$ for $i \in[l]$.
A non-increasing temporal random walk complies with time constraints so that the edges are traversed only backward in time, where it is also possible to walk along edges with the same timestamp.
Temporal logical rule Let $E_{i}$ and $T_{i}$ for $i \in[l+1]$ be variables that represent entities and timestamps, respectively. Further, let $r_{1}, r_{2}, \ldots, r_{l}, r_{h} \in \mathcal{R}$ be fixed.
A cyclic temporal logical rule $R$ of length $l \in \mathbb{N}$ is defined as</p>
<p>$$
\left(\left(E_{1}, r_{h}, E_{l+1}, T_{l+1}\right) \leftarrow \wedge_{i=1}^{l}\left(E_{i}, r_{i}, E_{i+1}, T_{i}\right)\right)
$$</p>
<p>with the temporal constraints</p>
<p>$$
T_{1} \leq T_{2} \leq \cdots \leq T_{l}&lt;T_{l+1}
$$</p>
<p>The left-hand side of $R$ is called the rule head, with $r_{h}$ being the head relation, while the right-hand side is called the rule body, which is represented by a conjunction of body atoms $\left(E_{i}, r_{i}, E_{i+1}, T_{i}\right)$. The rule is called cyclic because the rule head and the rule body constitute two different walks connecting the same two variables $E_{1}$ and $E_{l+1}$. A temporal rule implies that if the rule body holds with the temporal constraints given by (2), then the rule head is true as well for a future timestamp $T_{l+1}$.
The replacement of the variables $E_{i}$ and $T_{i}$ by constant terms is called grounding or instantiation. For example, a grounding of the temporal rule</p>
<p>$$
\left(\left(E_{1}, \text { consult }, E_{2}, T_{2}\right) \leftarrow\left(E_{1}, \text { discuss by telephone, } E_{2}, T_{1}\right)\right)
$$</p>
<p>is given by the edges (Angela Merkel, discuss by telephone, Barack Obama, 2014/07/22) and (Angela Merkel, consult, Barack Obama, 2014/08/09) in Figure 1. Let rule grounding refer to the replacement of the variables in the entire rule and body grounding refer to the replacement of the variables only in the body, where all groundings must comply with the temporal constraints in (2).
In many domains, logical rules are frequently violated so that confidence values are determined to estimate the probability of a rule's correctness. We adapt the standard confidence to take timestamp values into account. Let $\left(r_{1}, r_{2}, \ldots, r_{l}, r_{h}\right)$ be the relations in a rule $R$. The body support is defined as the number of body groundings, i. e., the number of tuples $\left(e_{1}, \ldots, e_{l+1}, t_{1}, \ldots, t_{l}\right)$ such that $\left(e_{i}, r_{i}, e_{i+1}, t_{i}\right) \in \mathcal{G}$ for $i \in[l]$ and $t_{i} \leq t_{i+1}$ for $i \in[l-1]$. The rule support is defined as the number of body groundings such that there exists a timestamp $t_{l+1}&gt;t_{l}$ with $\left(e_{1}, r_{h}, e_{l+1}, t_{l+1}\right) \in \mathcal{G}$. The confidence of the rule $R$, denoted by $\operatorname{conf}(R)$, can then be obtained by dividing the rule support by the body support.</p>
<p>Algorithm 1: Rule learning
Input: Temporal knowledge graph $\mathcal{G}$.
Parameters: Rule lengths $\mathcal{L} \subset \mathbb{N}$, number of temporal random walks $n \in \mathbb{N}$, transition distribution $d \in{\mathrm{unif}, \exp }$.
Output: Temporal logical rules $\mathcal{T} \mathcal{R}$.
for relation $r \in \mathcal{R}$ do
for $l \in \mathcal{L}$ do
for $i \in[n]$ do
$\mathcal{T} \mathcal{R}<em l_1="l+1">{r}^{l} \leftarrow \emptyset$
According to transition distribution $d$, sample a temporal random walk $W$ of length $l+1$ with $t</em> . \quad \triangleright$ See (4).
Transform walk $W$ to the corresponding temporal logical rule $R$. $\quad \triangleright$ See (5).
Estimate the confidence of rule $R$.
$\mathcal{T} \mathcal{R}}&gt;t_{l<em r="r">{r}^{l} \leftarrow \mathcal{T} \mathcal{R}</em>(R))}$
$\mathcal{T} \mathcal{R}}^{l} \cup{(R, \operatorname{conf<em _in="\in" _mathcal_L="\mathcal{L" l="l">{r} \leftarrow \cup</em>}} \mathcal{T} \mathcal{R<em _in="\in" _mathcal_R="\mathcal{R" r="r">{r}^{l}$
$\mathcal{T} \mathcal{R} \leftarrow \cup</em>$
return $\mathcal{T} \mathcal{R}$}} \mathcal{T} \mathcal{R}_{r</p>
<h2>Our Framework</h2>
<p>We introduce TLogic, a rule-based link forecasting framework for tKGs. TLogic first extracts temporal walks from the graph and then lifts these walks to a more abstract, semantic level to obtain temporal rules that generalize to new data. The application of these rules generates answer candidates, for which the body groundings in the graph serve as explicit and human-readable explanations. Our framework consists of the components rule learning and rule application. The pseudocode for rule learning is shown in Algorithm 1 and for rule application in Algorithm 2.</p>
<h2>Rule Learning</h2>
<p>As the first step of rule learning, temporal walks are extracted from the tKG $\mathcal{G}$. For a rule of length $l$, a walk of length $l+1$ is sampled, where the additional step corresponds to the rule head.</p>
<p>Let $r_{h}$ be a fixed relation, for which we want to learn rules. For the first sampling step $m=1$, we sample an edge $\left(e_{1}, r_{h}, e_{l+1}, t_{l+1}\right)$, which will serve as the rule head, uniformly from all edges with relation type $r_{h}$. A temporal random walker then samples iteratively edges adjacent to the current object until a walk of length $l+1$ is obtained.</p>
<p>For sampling step $m \in{2, \ldots, l+1}$, let $\left(e_{s}, \tilde{r}, e_{o}, t\right)$ denote the previously sampled edge and $\mathcal{A}\left(m, e_{o}, t\right)$ the set of feasible edges for the next transition. To fulfill the temporal constraints in (1) and (2), we define</p>
<p>$$
\begin{array}{ll}
\mathcal{A}\left(m, e_{o}, t\right):= &amp; \
&amp; \left{\begin{array}{l}
\left{\left(e_{o}, r, e, \tilde{t}\right) \mid\left(e_{o}, r, e, \tilde{t}\right) \in \mathcal{G}, \tilde{t}&lt;t\right} &amp; \text { if } m=2, \
\left{\left(e_{o}, r, e, \tilde{t}\right) \mid\left(e_{o}, r, e, \tilde{t}\right) \in \tilde{\mathcal{G}}, \tilde{t} \leq t\right} &amp; \text { if } m \in{3, \ldots, l}, \
\left{\left(e_{o}, r, e_{1}, \tilde{t}\right) \mid\left(e_{o}, r, e_{1}, \tilde{t}\right) \in \tilde{\mathcal{G}}, \tilde{t} \leq t\right} &amp; \text { if } m=l+1,
\end{array}\right.
\end{array}
$$</p>
<p>where $\tilde{\mathcal{G}}:=\mathcal{G} \backslash\left{\left(e_{o}, \tilde{r}^{-1}, e_{s}, t\right)\right}$ excludes the inverse edge to avoid redundant rules. For obtaining cyclic walks, we sample in the last step $m=l+1$ an edge that connects</p>
<p>the walk to the first entity $e_{1}$ if such edges exist. Otherwise, we sample the next walk.</p>
<p>The transition distribution for sampling the next edge can either be uniform or exponentially weighted. We define an index mapping $\hat{m}:=(l+1)-(m-2)$ to be consistent with the indices in (1). Then, the exponentially weighted probability for choosing edge $u \in \mathcal{A}\left(m, e_{\hat{m}}, t_{\hat{m}}\right)$ for $m \in{2, \ldots, l+1}$ is given by</p>
<p>$$
\mathbb{P}\left(u ; m, e_{\hat{m}}, t_{\hat{m}}\right)=\frac{\exp \left(t_{u}-t_{\hat{m}}\right)}{\sum_{\hat{u} \in \mathcal{A}\left(m, e_{\hat{m}}, t_{\hat{m}}\right)} \exp \left(t_{\hat{u}}-t_{\hat{m}}\right)}
$$</p>
<p>where $t_{u}$ denotes the timestamp of edge $u$. The exponential weighting favors edges with timestamps that are closer to the timestamp of the previous edge and probably more relevant for prediction.</p>
<p>The resulting temporal walk $W$ is given by</p>
<p>$$
\left(\left(e_{1}, r_{h}, e_{l+1}, t_{l+1}\right),\left(e_{l+1}, r_{l}, e_{l}, t_{l}\right), \ldots,\left(e_{2}, r_{1}, e_{1}, t_{1}\right)\right)
$$</p>
<p>$W$ can then be transformed to a temporal rule $R$ by replacing the entities and timestamps with variables. While the first edge in $W$ becomes the rule head $\left(E_{1}, r_{h}, E_{l+1}, T_{l+1}\right)$, the other edges are mapped to body atoms, where each edge $\left(e_{i+1}, r_{i}, e_{i}, t_{i}\right)$ is converted to the body atom $\left(E_{i}, r_{i}^{-1}, E_{i+1}, T_{i}\right)$. The final rule $R$ is denoted by</p>
<p>$$
\left(\left(E_{1}, r_{h}, E_{l+1}, T_{l+1}\right) \leftarrow \wedge_{i=1}^{l}\left(E_{i}, r_{i}^{-1}, E_{i+1}, T_{i}\right)\right)
$$</p>
<p>In addition, we impose the temporal consistency constraints $T_{1} \leq T_{2} \leq \cdots \leq T_{l}&lt;T_{l+1}$.</p>
<p>The entities $\left(e_{1}, \ldots, e_{l+1}\right)$ in $W$ do not need to be distinct since a pair of entities can have many interactions at different points in time. For example, Angela Merkel made several visits to China in 2014, which could constitute important information for the prediction. Repetitive occurrences of the same entity in $W$ are replaced with the same random variable in $R$ to maintain this knowledge.</p>
<p>For the confidence estimation of $R$, we sample from the graph a fixed number of body groundings, which have to match the body relations and the variable constraints mentioned in the last paragraph while satisfying the condition from (2). The number of unique bodies serves as the body support. The rule support is determined by counting the number of bodies for which an edge with relation type $r_{h}$ exists that connects $e_{1}$ and $e_{l+1}$ from the body. Moreover, the timestamp of this edge has to be greater than all body timestamps to fulfill (2).</p>
<p>For every relation $r \in \mathcal{R}$, we sample $n \in \mathbb{N}$ temporal walks for a set of prespecified lengths $\mathcal{L} \subset \mathbb{N}$. The set $\mathcal{T} \mathcal{R}<em r="r">{r}^{l}$ stands for all rules of length $l$ with head relation $r$ with their corresponding confidences. All rules for relation $r$ are included in $\mathcal{T} \mathcal{R}</em>}:=\cup_{l \in \mathcal{L}} \mathcal{T} \mathcal{R<em _in="\in" _mathcal_R="\mathcal{R" r="r">{r}^{l}$, and the complete set of learned temporal rules is given by $\mathcal{T} \mathcal{R}:=\cup</em>$.}} \mathcal{T} \mathcal{R}_{r</p>
<p>It is possible to learn rules only for a single relation or a set of specific relations of interest. Explicitly learning rules for all relations is especially effective for rare relations that would otherwise only be sampled with a small probability. The learned rules are not specific to the graph from which they have been extracted, but they could be employed in an</p>
<p>Algorithm 2: Rule application
Input: Test query $q=\left(e^{q}, r^{q}, ?, t^{q}\right)$, temporal logical rules $\mathcal{T} \mathcal{R}$, temporal knowledge graph $\mathcal{G}$.
Parameters: Time window $w \in \mathbb{N} \cup{\infty}$, minimum number of candidates $k$, score function $f$.
Output: Answer candidates $\mathcal{C}$.
$\mathcal{C} \leftarrow \emptyset$
$\triangleright$ Apply the rules in $\mathcal{T} \mathcal{R}$ by decreasing confidence.
if $\mathcal{T} \mathcal{R}<em r_q="r^{q">{r^{q}} \neq \emptyset$ then
for rule $R \in \mathcal{T} \mathcal{R}</em>$ do
Find all body groundings of $R$ in $\mathcal{S G} \subset \mathcal{G}$, where $\mathcal{S G}$ consists of the edges within the time window $\left[t^{q}-w, t^{q}\right)$.
Retrieve candidates $\mathcal{C}(R)$ from the target entities of the body groundings.
for $c \in \mathcal{C}(R)$ do
Calculate score $f(R, c)$.
$\triangleright$ See (6).
$\mathcal{C} \leftarrow \mathcal{C} \cup{(c, f(R, c))}$
if $|{c \mid \exists R:(c, f(R, c)) \in \mathcal{C}}| \geq k$ then
break
return $\mathcal{C}$
inductive setting where the rules are transferred to related datasets that share a common vocabulary for straightforward application.}</p>
<h2>Rule Application</h2>
<p>The learned temporal rules $\mathcal{T} \mathcal{R}$ are applied to answer queries of the form $q=\left(e^{q}, r^{q}, ?, t^{q}\right)$. The answer candidates are retrieved from the target entities of body groundings in the tKG $\mathcal{G}$. If there exist no rules $\mathcal{T} \mathcal{R}_{r^{q}}$ for the query relation $r^{q}$, or if there are no matching body groundings in the graph, then no answers are predicted for the given query.</p>
<p>To apply the rules on relevant data, a subgraph $\mathcal{S G} \subset \mathcal{G}$ dependent on a time window $w \in \mathbb{N} \cup{\infty}$ is retrieved. For $w \in \mathbb{N}$, the subgraph $\mathcal{S G}$ contains all edges from $\mathcal{G}$ that have timestamps $t \in\left[t^{q}-w, t^{q}\right)$. If $w=\infty$, then all edges with timestamps prior to the query timestamp $t^{q}$ are used for rule application, i. e., $\mathcal{S G}$ consists of all facts with $t \in\left[t_{\min }, t^{q}\right)$, where $t_{\min }$ is the minimum timestamp in the graph $\mathcal{G}$.</p>
<p>We apply the rules $\mathcal{T} \mathcal{R}<em r_q="r^{q">{r^{q}}$ by decreasing confidence, where each rule $R$ generates a set of answer candidates $\mathcal{C}(R)$. Each candidate $c \in \mathcal{C}(R)$ is then scored by a function $f: \mathcal{T} \mathcal{R}</em> \rightarrow[0,1]$ that reflects the probability of the candidate being the correct answer to the query.}} \times \mathcal{E</p>
<p>Let $\mathcal{B}(R, c)$ be the set of body groundings of rule $R$ that start at entity $e^{q}$ and end at entity $c$. We choose as score function $f$ a convex combination of the rule's confidence and a function that takes the time difference $t^{q}-t_{1}(\mathcal{B}(R, c))$ as input, where $t_{1}(\mathcal{B}(R, c))$ denotes the earliest timestamp $t_{1}$ in the body. If several body groundings exist, we take from all possible $t_{1}$ values the one that is closest to $t^{q}$. For candidate $c \in \mathcal{C}(R)$, the score function is defined as</p>
<p>$$
f(R, c)=a \cdot \operatorname{conf}(R)+(1-a) \cdot \exp \left(-\lambda\left(t^{q}-t_{1}(\mathcal{B}(R, c))\right)\right)
$$</p>
<p>with $\lambda&gt;0$ and $a \in[0,1]$.
The intuition for this choice of $f$ is that candidates generated by high-confidence rules should receive a higher score.</p>
<p>Adding a dependency on the timeframe of the rule grounding is based on the observation that the existence of edges in a rule become increasingly probable with decreasing time difference between the edges. We choose the exponential distribution since it is commonly used to model interarrival times of events. The time difference $t^{q}-t_{1}(\mathcal{B}(R, c))$ is always non-negative for a future timestamp value $t^{q}$, and with the assumption that there exists a fixed mean, the exponential distribution is also the maximum entropy distribution for such a time difference variable. The exponential distribution is rescaled so that both summands are in the range $[0,1]$.</p>
<p>All candidates are saved with their scores as $(c, f(R, c))$ in $\mathcal{C}$. We stop the rule application when the number of different answer candidates $\mid{c \mid \exists R:(c, f(R, c)) \in \mathcal{C}} \mid$ is at least $k$ so that there is no need to go through all rules.</p>
<h2>Candidate Ranking</h2>
<p>For the ranking of the answer candidates, all scores of each candidate $c$ are aggregated through a noisy-OR calculation, which produces the final score</p>
<p>$$
1-\Pi_{{s \mid(c, s) \in \mathcal{C}}}(1-s)
$$</p>
<p>The idea is to aggregate the scores to produce a probability, where candidates implied by more rules should have a higher score.</p>
<p>In case there are no rules for the query relation $r^{q}$, or if there are no matching body groundings in the graph, it might still be interesting to retrieve possible answer candidates. In the experiments, we apply a simple baseline where the scores for the candidates are obtained from the overall object distribution in the training data if $r^{q}$ is a new relation. If $r^{q}$ already exists in the training set, we take the object distribution of the edges with relation type $r^{q}$.</p>
<h2>Experiments</h2>
<h2>Datasets</h2>
<p>We conduct experiments on the dataset Integrated Crisis Early Warning System ${ }^{1}$ (ICEWS), which contains information about international events and is a commonly used benchmark dataset for link prediction on tKGs. We choose the subsets ICEWS14, ICEWS18, and ICEWS0515, which include data from the years 2014, 2018, and 2005 to 2015, respectively. Since we consider link forecasting, each dataset is split into training, validation, and test set so that the timestamps in the training set occur earlier than the timestamps in the validation set, which again occur earlier than the timestamps in the test set. To ensure a fair comparison, we use the split provided by Han et al. (2021) ${ }^{2}$. The statistics of the datasets are summarized in the supplementary material.</p>
<h2>Experimental Setup</h2>
<p>For each test instance $\left(e_{s}^{q}, r^{q}, e_{o}^{q}, t^{q}\right)$, we generate a list of candidates for both object prediction $\left(e_{s}^{q}, r^{q}, ?, t^{q}\right)$ and subject prediction $\left(e_{o}^{q},\left(r^{q}\right)^{-1}, ?, t^{q}\right)$. The candidates are ranked by decreasing scores, which are calculated according to (7).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The confidence for each rule is estimated by sampling 500 body groundings and counting the number of times the rule head holds. We learn rules of the lengths 1,2 , and 3 , and for application, we only consider the rules with a minimum confidence of 0.01 and minimum body support of 2 .</p>
<p>We compute the mean reciprocal rank (MRR) and hits@ $k$ for $k \in{1,3,10}$, which are standard metrics for link prediction on KGs. For a rank $x \in \mathbb{N}$, the reciprocal rank is defined as $\frac{1}{x}$, and the MRR is the average of all reciprocal ranks of the correct query answers across all queries. The metric hits@ $k$ (h@k) indicates the proportion of queries for which the correct entity appears under the top $k$ candidates.</p>
<p>Similar to Han et al. (2021), we perform time-aware filtering where all correct entities at the query timestamp except for the true query object are filtered out from the answers. In comparison to the alternative setting that filters out all other objects that appear together with the query subject and relation at any timestamp, time-aware filtering yields a more realistic performance estimate.</p>
<p>Baseline methods We compare TLogic ${ }^{3}$ with the state-of-the-art baselines for static link prediction DistMult (Yang et al. 2015), ComplEx (Trouillon et al. 2016), and AnyBURL (Meilicke et al. 2019, 2020) as well as for temporal link prediction TTransE (Leblay and Chekol 2018), TADistMult (García-Durán, Dumančić, and Niepert 2018), DESimplE (Goel et al. 2020), TNTComplEx (Lacroix, Obozinski, and Usunier 2020), CyGNet (Zhu et al. 2021), RENet (Jin et al. 2019), and xERTE (Han et al. 2021). All baseline results except for the results on AnyBURL are from Han et al. (2021). AnyBURL samples paths based on reinforcement learning and generalizes them to rules, where the rule space also includes, e. g., acyclic rules and rules with constants. A non-temporal variant of TLogic would sample paths randomly and only learn cyclic rules, which would presumably yield worse performance than AnyBURL. Therefore, we choose AnyBURL as a baseline to assess the effectiveness of adding temporal constraints.</p>
<h2>Results</h2>
<p>The results of the experiments are displayed in Table 1. TLogic outperforms all baseline methods with respect to the metrics MRR, hits@3, and hits@10. Only xERTE performs better than Tlogic for hits@1 on the datasets ICEWS18 and ICEWS0515.</p>
<p>Besides a list of possible answer candidates with corresponding scores, TLogic can also provide temporal rules and body groundings in form of walks from the graph that support the predictions. Table 2 presents three exemplary rules with high confidences that were learned from ICEWS14. For the query (Angela Merkel, consult, ?, 2014/08/09), two walks are shown in Table 2, which serve as time-consistent explanations for the correct answer Barack Obama.</p>
<p>Inductive setting One advantage of our learned logical rules is that they are applicable to any new dataset as long as the new dataset covers common relations. This might be relevant for cases where new entities appear. For example, Donald Trump, who served as president of the United States</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">ICEWS14</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ICEWS18</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ICEWS0515</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">h@1</td>
<td style="text-align: center;">h@3</td>
<td style="text-align: center;">h@10</td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">h@1</td>
<td style="text-align: center;">h@3</td>
<td style="text-align: center;">h@10</td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">h@1</td>
<td style="text-align: center;">h@3</td>
<td style="text-align: center;">h@10</td>
</tr>
<tr>
<td style="text-align: center;">DistMult</td>
<td style="text-align: center;">0.2767</td>
<td style="text-align: center;">0.1816</td>
<td style="text-align: center;">0.3115</td>
<td style="text-align: center;">0.4696</td>
<td style="text-align: center;">0.1017</td>
<td style="text-align: center;">0.0452</td>
<td style="text-align: center;">0.1033</td>
<td style="text-align: center;">0.2125</td>
<td style="text-align: center;">0.2873</td>
<td style="text-align: center;">0.1933</td>
<td style="text-align: center;">0.3219</td>
<td style="text-align: center;">0.4754</td>
</tr>
<tr>
<td style="text-align: center;">ComplEx</td>
<td style="text-align: center;">0.3084</td>
<td style="text-align: center;">0.2151</td>
<td style="text-align: center;">0.3448</td>
<td style="text-align: center;">0.4958</td>
<td style="text-align: center;">0.2101</td>
<td style="text-align: center;">0.1187</td>
<td style="text-align: center;">0.2347</td>
<td style="text-align: center;">0.3987</td>
<td style="text-align: center;">0.3169</td>
<td style="text-align: center;">0.2144</td>
<td style="text-align: center;">0.3574</td>
<td style="text-align: center;">0.5204</td>
</tr>
<tr>
<td style="text-align: center;">AnyBURL</td>
<td style="text-align: center;">0.2967</td>
<td style="text-align: center;">0.2126</td>
<td style="text-align: center;">0.3333</td>
<td style="text-align: center;">0.4673</td>
<td style="text-align: center;">0.2277</td>
<td style="text-align: center;">0.1510</td>
<td style="text-align: center;">0.2544</td>
<td style="text-align: center;">0.3891</td>
<td style="text-align: center;">0.3205</td>
<td style="text-align: center;">0.2372</td>
<td style="text-align: center;">0.3545</td>
<td style="text-align: center;">0.5046</td>
</tr>
<tr>
<td style="text-align: center;">TTransE</td>
<td style="text-align: center;">0.1343</td>
<td style="text-align: center;">0.0311</td>
<td style="text-align: center;">0.1732</td>
<td style="text-align: center;">0.3455</td>
<td style="text-align: center;">0.0831</td>
<td style="text-align: center;">0.0192</td>
<td style="text-align: center;">0.0856</td>
<td style="text-align: center;">0.2189</td>
<td style="text-align: center;">0.1571</td>
<td style="text-align: center;">0.0500</td>
<td style="text-align: center;">0.1972</td>
<td style="text-align: center;">0.3802</td>
</tr>
<tr>
<td style="text-align: center;">TA-DistMult</td>
<td style="text-align: center;">0.2647</td>
<td style="text-align: center;">0.1709</td>
<td style="text-align: center;">0.3022</td>
<td style="text-align: center;">0.4541</td>
<td style="text-align: center;">0.1675</td>
<td style="text-align: center;">0.0861</td>
<td style="text-align: center;">0.1841</td>
<td style="text-align: center;">0.3359</td>
<td style="text-align: center;">0.2431</td>
<td style="text-align: center;">0.1458</td>
<td style="text-align: center;">0.2792</td>
<td style="text-align: center;">0.4421</td>
</tr>
<tr>
<td style="text-align: center;">DE-SimplE</td>
<td style="text-align: center;">0.3267</td>
<td style="text-align: center;">0.2443</td>
<td style="text-align: center;">0.3569</td>
<td style="text-align: center;">0.4911</td>
<td style="text-align: center;">0.1930</td>
<td style="text-align: center;">0.1153</td>
<td style="text-align: center;">0.2186</td>
<td style="text-align: center;">0.3480</td>
<td style="text-align: center;">0.3502</td>
<td style="text-align: center;">0.2591</td>
<td style="text-align: center;">0.3899</td>
<td style="text-align: center;">0.5275</td>
</tr>
<tr>
<td style="text-align: center;">TNTComplEx</td>
<td style="text-align: center;">0.3212</td>
<td style="text-align: center;">0.2335</td>
<td style="text-align: center;">0.3603</td>
<td style="text-align: center;">0.4913</td>
<td style="text-align: center;">0.2123</td>
<td style="text-align: center;">0.1328</td>
<td style="text-align: center;">0.2402</td>
<td style="text-align: center;">0.3691</td>
<td style="text-align: center;">0.2754</td>
<td style="text-align: center;">0.1952</td>
<td style="text-align: center;">0.3080</td>
<td style="text-align: center;">0.4286</td>
</tr>
<tr>
<td style="text-align: center;">CyGNet</td>
<td style="text-align: center;">0.3273</td>
<td style="text-align: center;">0.2369</td>
<td style="text-align: center;">0.3631</td>
<td style="text-align: center;">0.5067</td>
<td style="text-align: center;">0.2493</td>
<td style="text-align: center;">0.1590</td>
<td style="text-align: center;">0.2828</td>
<td style="text-align: center;">0.4261</td>
<td style="text-align: center;">0.3497</td>
<td style="text-align: center;">0.2567</td>
<td style="text-align: center;">0.3909</td>
<td style="text-align: center;">0.5294</td>
</tr>
<tr>
<td style="text-align: center;">RE-Net</td>
<td style="text-align: center;">0.3828</td>
<td style="text-align: center;">0.2868</td>
<td style="text-align: center;">0.4134</td>
<td style="text-align: center;">0.5452</td>
<td style="text-align: center;">0.2881</td>
<td style="text-align: center;">0.1905</td>
<td style="text-align: center;">0.3244</td>
<td style="text-align: center;">0.4751</td>
<td style="text-align: center;">0.4297</td>
<td style="text-align: center;">0.3126</td>
<td style="text-align: center;">0.4685</td>
<td style="text-align: center;">0.6347</td>
</tr>
<tr>
<td style="text-align: center;">xERTE</td>
<td style="text-align: center;">0.4079</td>
<td style="text-align: center;">0.3270</td>
<td style="text-align: center;">0.4567</td>
<td style="text-align: center;">0.5730</td>
<td style="text-align: center;">0.2931</td>
<td style="text-align: center;">0.2103</td>
<td style="text-align: center;">0.3351</td>
<td style="text-align: center;">0.4648</td>
<td style="text-align: center;">0.4662</td>
<td style="text-align: center;">0.3784</td>
<td style="text-align: center;">0.5231</td>
<td style="text-align: center;">0.6392</td>
</tr>
<tr>
<td style="text-align: center;">TLogic</td>
<td style="text-align: center;">0.4304</td>
<td style="text-align: center;">0.3356</td>
<td style="text-align: center;">0.4827</td>
<td style="text-align: center;">0.6123</td>
<td style="text-align: center;">0.2982</td>
<td style="text-align: center;">0.2054</td>
<td style="text-align: center;">0.3395</td>
<td style="text-align: center;">0.4853</td>
<td style="text-align: center;">0.4697</td>
<td style="text-align: center;">0.3621</td>
<td style="text-align: center;">0.5313</td>
<td style="text-align: center;">0.6743</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of link forecasting on the datasets ICEWS14, ICEWS18, and ICEWS0515. All metrics are time-aware filtered. The best results among all models are displayed in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Confidence</th>
<th style="text-align: center;">Head</th>
<th style="text-align: center;">Body</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.963</td>
<td style="text-align: center;">$\left(E_{1}\right.$, demonstrate or rally, $\left.E_{2}, T_{4}\right)$</td>
<td style="text-align: center;">$\left(E_{1}\right.$, riot, $\left.E_{2}, T_{1}\right) \wedge\left(E_{2}\right.$, make statement, $\left.E_{1}, T_{2}\right) \wedge\left(E_{1}\right.$, riot, $\left.E_{2}, T_{3}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">$\left(E_{1}\right.$, share information, $\left.E_{2}, T_{2}\right)$</td>
<td style="text-align: center;">$\left(E_{1}\right.$, express intent to ease sanctions ${ }^{-1},\left.E_{2}, T_{1}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">$\left(E_{1}\right.$, provide military aid, $\left.E_{3}, T_{3}\right)$</td>
<td style="text-align: center;">$\left(E_{1}\right.$, provide military aid, $\left.E_{2}, T_{1}\right) \wedge\left(E_{2}\right.$, intend to protect ${ }^{-1},\left.E_{3}, T_{2}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">(Merkel, consult, Obama, 14/08/09)</td>
<td style="text-align: center;">(Merkel, discuss by telephone, Obama, 14/07/22)</td>
</tr>
<tr>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">(Merkel, consult, Obama, 14/08/09)</td>
<td style="text-align: center;">(Merkel, express intent to meet, Obama, 14/05/02) <br> $\wedge\left(\right.$ Obama, consult ${ }^{-1}$, Merkel, 14/07/18) $\wedge\left(\right.$ Merkel, consult ${ }^{-1}$, Obama, 14/07/29)</td>
</tr>
</tbody>
</table>
<p>Table 2: Three exemplary rules from the dataset ICEWS14 and two walks for the query (Angela Merkel, consult, ?, 2014/08/09) that lead to the correct answer Barack Obama. The timestamps are displayed in the format yy/mm/dd.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: MRR performance on the validation set of ICEWS14. The transition distribution is either uniform or exponentially weighted.
from 2017 to 2021, is included in the dataset ICEWS18 but not in ICEWS14. The logical rules are not tied to particular entities and would still be applicable, while embeddingbased methods have difficulties operating in this challenging setting. The models would need to be retrained to obtain embeddings for the new entities, where existing embeddings might also need to be adapted to the different time range.</p>
<p>For the two rule-based methods AnyBURL and TLogic,
we apply the rules learned on the training set of ICEWS0515 (with timestamps from 2005/01/01 to 2012/08/06) to the test set of ICEWS14 as well as the rules learned on the training set of ICEWS14 to the test set of ICEWS18 (see Table 3). The performance of TLogic in the inductive setting is for all metrics close to the results in Table 1, while for AnyBURL, especially the results on ICEWS18 drop significantly. It seems that the encoded temporal information in TLogic is essential for achieving correct predictions in the inductive setting. ICEWS14 has only 7,128 entities, while ICEWS18 contains 23,033 entities. The results confirm that temporal rules from TLogic can even be transferred to a dataset with a large number of new entities and timestamps and lead to a strong performance.</p>
<h2>Analysis</h2>
<p>The results in this section are obtained on the dataset ICEWS14, but the findings are similar for the other two datasets. More detailed results can be found in the supplementary material.</p>
<p>Number of walks Figure 2 shows the MRR performance on the validation set of ICEWS14 for different numbers of walks that were extracted during rule learning. We observe a performance increase with a growing number of walks. However, the performance gains saturate between 100 and 200 walks where rather small improvements are attainable.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\mathcal{G}_{\text {train }}$</th>
<th style="text-align: center;">$\mathcal{G}_{\text {test }}$</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MRR</th>
<th style="text-align: center;">h@1</th>
<th style="text-align: center;">h@3</th>
<th style="text-align: center;">h@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ICEWS0515</td>
<td style="text-align: center;">ICEWS14</td>
<td style="text-align: center;">AnyBURL</td>
<td style="text-align: center;">0.2664</td>
<td style="text-align: center;">0.1800</td>
<td style="text-align: center;">0.3024</td>
<td style="text-align: center;">0.4477</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TLogic</td>
<td style="text-align: center;">0.4253</td>
<td style="text-align: center;">0.3291</td>
<td style="text-align: center;">0.4780</td>
<td style="text-align: center;">0.6122</td>
</tr>
<tr>
<td style="text-align: center;">ICEWS14</td>
<td style="text-align: center;">ICEWS18</td>
<td style="text-align: center;">AnyBURL</td>
<td style="text-align: center;">0.1546</td>
<td style="text-align: center;">0.0907</td>
<td style="text-align: center;">0.1685</td>
<td style="text-align: center;">0.2958</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TLogic</td>
<td style="text-align: center;">0.2915</td>
<td style="text-align: center;">0.1987</td>
<td style="text-align: center;">0.3330</td>
<td style="text-align: center;">0.4795</td>
</tr>
</tbody>
</table>
<p>Table 3: Inductive setting where rules learned on $\mathcal{G}<em _test="{test" _text="\text">{\text {train }}$ are transferred and applied to $\mathcal{G}</em>$.}</p>
<p>Transition distribution We test two transition distributions for the extraction of temporal walks: uniform and exponentially weighted according to (3). The rationale behind using an exponentially weighted distribution is the observation that related events tend to happen within a short timeframe. The distribution of the first edge is always uniform to not restrict the variety of obtained walks. Overall, the performance of the exponential distribution consistently exceeds the uniform setting with respect to the MRR (see Figure 2).</p>
<p>We observe that the exponential distribution leads to more rules of length 3 than the uniform setting ( 11,718 compared to 8,550 rules for 200 walks), while it is the opposite for rules of length 1 ( 7,858 compared to 11,019 rules). The exponential setting leads to more successful longer walks because the timestamp differences between subsequent edges tend to be smaller. It is less likely that there are no feasible transitions anymore because of temporal constraints. The uniform setting, however, leads to a better exploration of the neighborhood around the start node for shorter walks.</p>
<p>Rule length We learn rules of lengths 1, 2, and 3. Using all rules for application results in the best performance (MRR on the validation set: 0.4373 ), followed by rules of only length 1 (0.4116), 3 (0.4097), and 2 (0.1563). The reason why rules of length 3 perform better than length 2 is that the temporal walks are allowed to transition back and forth between the same entities. Since we only learn cyclic rules, a rule body of length 2 must constitute a path with no recurring entities, resulting in fewer rules and rule groundings in the graph. Interestingly, simple rules of length 1 already yield very good performance.</p>
<p>Time window For rule application, we define a time window for retrieving the relevant data. The performance increases with the size of the time window, even though relevant events tend to be close to the query timestamp. The second summand of the score function $f$ in (6) takes the time difference between the query timestamp $t^{q}$ and the earliest body timestamp $t_{1}(\mathcal{B}(R, c))$ into account. In this case, earlier events with a large timestamp difference receive a lesser weight, while generally, as much information as possible is beneficial for prediction.</p>
<p>Score function We define the score function $f$ in (6) as a convex combination of the rule's confidence and a function that depends on the time difference $t^{q}-t_{1}(\mathcal{B}(R, c))$. The performance of only using the confidence (MRR: 0.3869 ) or only using the exponential function ( 0.4077 ) is worse than the combination (0.4373), which means that both the information from the rules' confidences and the time differences are important for prediction.</p>
<p>Variance The variance in the performance due to differ-
ent rules obtained from the rule learning component is quite small. Running the same model with the best hyperparameter settings for five different seeds results in a standard deviation of 0.0012 for the MRR. The rule application component is deterministic and always leads to the same candidates with corresponding scores for the same hyperparameter setting.</p>
<p>Training and inference time The worst-case time complexity for learning rules of length $l$ is $\mathcal{O}(|\mathcal{R}| n l D b)$, where $n$ is the number of walks, $D$ the maximum node degree in the training set, and $b$ the number of body samples for estimating the confidence. The worst-case time complexity for inference is given by $\mathcal{O}\left(|\mathcal{G}|+\left|\mathcal{T} \mathcal{R}<em r_q="r^{q">{r^{q}}\right| D^{L}|\mathcal{E}| \log (k)\right)$, where $L$ is the maximum rule length in $\mathcal{T} \mathcal{R}</em>| \log (k)\right)$ by only keeping a maximum of $K$ candidate walks during rule application.}}$ and $k$ the minimum number of candidates. For large graphs with high node degrees, it is possible to reduce the complexity to $\mathcal{O}\left(|\mathcal{G}|+\left|\mathcal{T} \mathcal{R}_{r^{q}}\right| K L D|\mathcal{E</p>
<p>Both training and application can be parallelized since the rule learning for each relation and the rule application for each test query are independent. Rule learning with 200 walks and exponentially weighted transition distribution for rule lengths ${1,2,3}$ on a machine with 8 CPUs takes 180 sec for ICEWS14, while the application on the validation set takes 2000 sec , with $w=\infty$ and $k=20$. For comparison, the best-performing baseline xERTE needs for training one epoch on the same machine already 5000 sec , where an MRR of 0.3953 can be obtained, while testing on the validation set takes 700 sec .</p>
<h2>Conclusion</h2>
<p>We have proposed TLogic, the first symbolic framework that directly learns temporal logical rules from temporal knowledge graphs and applies these rules for link forecasting. The framework generates answers by applying rules to observed events prior to the query timestamp and scores the answer candidates depending on the rules' confidences and time differences. Experiments on three datasets indicate that TLogic achieves better overall performance compared to state-of-the-art baselines. In addition, our approach also provides time-consistent, explicit, and human-readable explanations for the predictions in the form of temporal logical rules.</p>
<p>As future work, it would be interesting to integrate acyclic rules, which could also contain relevant information and might boost the performance for rules of length 2. Furthermore, the simple sampling mechanism for temporal walks could be replaced by a more sophisticated approach, which is able to effectively identify the most promising walks.</p>
<h2>Acknowledgements</h2>
<p>This work has been supported by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) as part of the project RAKI under grant number 01MD19012C and by the German Federal Ministry of Education and Research (BMBF) under grant number 01IS18036A. The authors of this work take full responsibility for its content.</p>
<h2>References</h2>
<p>Bian, R.; Koh, Y. S.; Dobbie, G.; and Divoli, A. 2019. Network embedding and change modeling in dynamic heterogeneous networks. In Proceedings of the Forty-Second International ACM SIGIR Conference on Research and Development in Information Retrieval.
Bordes, A.; Usunier, N.; García-Durán, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In Proceedings of the Twenty-Sixth International Conference on Neural Information Processing Systems.
Galárraga, L.; Teflioudi, C.; Hose, K.; and Suchanek, F. M. 2015. Fast rule mining in ontological knowledge bases with AMIE+. The VLDB Journal, 24: 707-730.
García-Durán, A.; Dumančić, S.; and Niepert, M. 2018. Learning sequence encoders for temporal knowledge graph completion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
Goel, R.; Kazemi, S. M.; Brubaker, M.; and Poupart, P. 2020. Diachronic embedding for temporal knowledge graph completion. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence.
Han, Z.; Chen, P.; Ma, Y.; and Tresp, V. 2021. Explainable subgraph reasoning for forecasting on temporal knowledge graphs. In Proceedings of the Ninth International Conference on Learning Representations.
Hildebrandt, M.; Sunder, S. S.; Mogoreanu, S.; Joblin, M.; Mehta, A.; Thon, I.; and Tresp, V. 2019. A recommender system for complex real-world applications with nonlinear dependencies and knowledge graph context. In Proceedings of the Sixteenth Extended Semantic Web Conference.
Jin, W.; Zhang, C.; Szekely, P.; and Ren, X. 2019. Recurrent event network for reasoning over temporal knowledge graphs. Workshop paper at the Seventh International Conference on Learning Representations.
Lacroix, T.; Obozinski, G.; and Usunier, N. 2020. Tensor decompositions for temporal knowledge base completion. In Proceedings of the Eighth International Conference on Learning Representations.
Leblay, J.; and Chekol, M. W. 2018. Deriving validity time in knowledge graph. In Companion Proceedings of the Web Conference 2018.
Liu, Y.; Hildebrandt, M.; Joblin, M.; Ringsquandl, M.; Raissouni, R.; and Tresp, V. 2021. Neural multi-hop reasoning with logical rules on biomedical knowledge graphs. In Proceedings of the Eighteenth Extended Semantic Web Conference.</p>
<p>Mahdavi, S.; Khoshraftar, S.; and An, A. 2018. dynnode2vec: scalable dynamic network embedding. In Proceedings of the 2018 IEEE International Conference on Big Data.
Meilicke, C.; Chekol, M. W.; Fink, M.; and Stuckenschmidt, H. 2020. Reinforced anytime bottom-up rule learning for knowledge graph completion. arXiv:2004.04412.
Meilicke, C.; Chekol, M. W.; Ruffinelli, D.; and Stuckenschmidt, H. 2019. Anytime bottom-up rule learning for knowledge graph completion. In Proceedings of the TwentyEighth International Joint Conference on Artificial Intelligence.
Nguyen, D. Q.; Nguyen, T. D.; Nguyen, D. Q.; and Phung, D. 2018a. A novel embedding model for knowledge base completion based on convolutional neural network. In Proceedings of the Sixteenth Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Nguyen, G. H.; Lee, J. B.; Rossi, R. A.; Ahmed, N. K.; Koh, E.; and Kim, S. 2018b. Dynamic network embeddings: from random walks to temporal random Walks. In Proceedings of the 2018 IEEE International Conference on Big Data.
Nickel, M.; Tresp, V.; and Kriegel, H.-P. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the Twenty-Eighth International Conference on Machine Learning.
Omran, P. G.; Wang, K.; and Wang, Z. 2019. Learning temporal rules from knowledge graph streams. In Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering.
Schlichtkrull, M.; Kipf, T. N.; Bloem, P.; Berg, R. v. d.; Titov, I.; and Welling, M. 2018. Modeling relational data with graph convolutional networks. In Proceedings of the Fifteenth Extended Semantic Web Conference.
Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier, É.; and Bouchard, G. 2016. Complex embeddings for simple link prediction. In Proceedings of the Thirty-Third International Conference on Machine Learning.
Vashishth, S.; Sanyal, S.; Nitin, V.; and Talukdar, P. 2020. Composition-based multi-relational graph convolutional networks. In Proceedings of the Eighth International Conference on Learning Representations.
Yang, B.; Yih, W.-T.; He, X.; Gao, J.; and Deng, L. 2015. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of the Third International Conference on Learning Representations.
Zhu, C.; Chen, M.; Fan, C.; Cheng, G.; and Zhang, Y. 2021. Learning from history: modeling temporal knowledge graphs with sequential copy-generation networks. In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://dataverse.harvard.edu/dataverse/icews
${ }^{2}$ https://github.com/TemporalKGTeam/xERTE&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Code available at https://github.com/liu-yushan/TLogic.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
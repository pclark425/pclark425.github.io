<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1752 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1752</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1752</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-e59b33aaab4de84819061d2184ae8d322f844d12</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e59b33aaab4de84819061d2184ae8d322f844d12" target="_blank">Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a joint simulation and real-world learning framework for mapping navigation instructions and raw first-person observations to continuous control and introduces Supervised Reinforcement Asynchronous Learning (SuReAL), which combines supervised learning for predicting positions to visit and reinforcement learning for continuous control.</p>
                <p><strong>Paper Abstract:</strong> We propose a joint simulation and real-world learning framework for mapping navigation instructions and raw first-person observations to continuous control. Our model estimates the need for environment exploration, predicts the likelihood of visiting environment positions during execution, and controls the agent to both explore and visit high-likelihood positions. We introduce Supervised Reinforcement Asynchronous Learning (SuReAL). Learning uses both simulation and real environments without requiring autonomous flight in the physical environment during training, and combines supervised learning for predicting positions to visit and reinforcement learning for continuous control. We evaluate our approach on a natural language instruction-following task with a physical quadcopter, and demonstrate effective execution and exploration behavior.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounded language learning in a simulated 3d world <em>(Rating: 2)</em></li>
                <li>Mapping instructions to actions in 3D environments with visual goal prediction <em>(Rating: 2)</em></li>
                <li>Mapping navigation instructions to continuous control actions with position-visitation prediction <em>(Rating: 2)</em></li>
                <li>Speaker-follower models for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1752",
    "paper_id": "paper-e59b33aaab4de84819061d2184ae8d322f844d12",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounded language learning in a simulated 3d world",
            "rating": 2
        },
        {
            "paper_title": "Mapping instructions to actions in 3D environments with visual goal prediction",
            "rating": 2
        },
        {
            "paper_title": "Mapping navigation instructions to continuous control actions with position-visitation prediction",
            "rating": 2
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
            "rating": 2
        }
    ],
    "cost": 0.0078065,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight</h1>
<p>Valts Blukis ${ }^{1}$ Yannick Terme ${ }^{2}$ Eyvind Niklasson ${ }^{3}$ Ross A. Knepper ${ }^{4}$ Yoav Artzi ${ }^{5}$<br>${ }^{1,4,5}$ Department of Computer Science, Cornell University, Ithaca, New York, USA<br>${ }^{1,2,3,5}$ Cornell Tech, Cornell University, New York, New York, USA<br>$\left{{ }^{1}\right.$ valts, ${ }^{4}$ rak, ${ }^{5}$ yoav}@cs.cornell.edu ${ }^{2}$ yannickterme@gmail.com<br>${ }^{3}$ een7@cornell.edu</p>
<h4>Abstract</h4>
<p>We propose a joint simulation and real-world learning framework for mapping navigation instructions and raw first-person observations to continuous control. Our model estimates the need for environment exploration, predicts the likelihood of visiting environment positions during execution, and controls the agent to both explore and visit high-likelihood positions. We introduce Supervised Reinforcement Asynchronous Learning (SuReAL). Learning uses both simulation and real environments without requiring autonomous flight in the physical environment during training, and combines supervised learning for predicting positions to visit and reinforcement learning for continuous control. We evaluate our approach on a natural language instruction-following task with a physical quadcopter, and demonstrate effective execution and exploration behavior.</p>
<p>Keywords: Natural language understanding; quadcopter; uav; reinforcement learning; instruction following; observability; simulation; exploration;</p>
<h2>1 Introduction</h2>
<p>Controlling robotic agents to execute natural language instructions requires addressing perception, language, planning, and control challenges. The majority of methods addressing this problem follow such a decomposition, where separate components are developed independently and are then combined together [e.g., $1,2,3,4,5,6]$. This requires a hard-to-scale engineering intensive process of designing and working with intermediate representations, including a formal language to represent natural language meaning. Recent work instead learns intermediate representations, and uses a single model to address all reasoning challenges [e.g., 7, 8, 9, 10]. So far, this line of work has mostly focused on pre-specified low-level tasks. In contrast, executing natural language instructions requires understanding sentence structure, grounding words and phrases to observations, reasoning about previously unseen tasks, and handling ambiguity and uncertainty.</p>
<p>In this paper, we address the problem of mapping natural language navigation instructions to continuous control of a quadcopter drone using representation learning. We present a neural network model to jointly reason about observations, natural language, and robot control, with explicit modeling of the agent's plan and exploration of the environment. For learning, we introduce Supervised and Reinforcement Asynchronous Learning (SuReAL), a method for joint training in simulated and physical environments. Figure 1 illustrates our task and model.
We design our model to reason about partial observability and incomplete knowledge of the environment in instruction following. We explicitly model observed and unobserved areas, and the agent's belief that the goal location implied by the instruction has been observed. During learning, we use an intrinsic reward to encourage behaviors that increase this belief, and penalize for indicating task completion while still believing the goal is unobserved.
SuReAL addresses two key learning challenges. First, flying in a physical environment at the scale needed for our complex learning task is both time-consuming and costly. We mitigate this problem using a simulated environment. However, in contrast to the common approach of domain transfer from simulated to physical environments [11, 12], we simultaneously train in both, while not requiring autonomous flight in the physical environment during training. Second, as each example requires a human instruction, it is prohibitively expensive to collect language data at the scale re-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of our task and model. Correct execution of the instruction requires recognizing objects (e.g., <em>blue bale</em>), inferring a path (e.g., to turn to the right <em>after the blue bale</em>), and generating the commands to steer the quadcopter and stop at the goal location. The model input at time $t$ is the instruction $u$, a first-person RGB observation $I_{t}$, and a pose estimate $P_{t}$. The model has two stages: predicting the probability of visiting positions during execution and generating actions.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Predicted visitation distributions as the instruction execution progresses (left-to-right), trajectory in red and goal in green, for the example from Figure 1. The green bar shows the agent's belief the goal has not been observed yet. A video of the execution and intermediate representations is available at https://youtu.be/PLdsNPE4Gz4.</p>
<p>quired for representation learning [13, 14]. This is unlike tasks where data can be collected without human interaction. We combine supervised and reinforcement learning (RL); the first to best use the limited language data, and the second to effectively leverage experience.</p>
<p>We evaluate our approach with a navigation task, where a quadcopter drone flies between landmarks following natural language instructions. We modify an existing natural language dataset [15] to create a new benchmark with long instructions, complex trajectories, and observability challenges. We evaluate using both automated metrics and human judgements of semantic correctness. To the best of our knowledge, this is the first demonstration of a physical quadcopter system that follows natural language instructions by mapping raw first-person images and pose estimates to continuous control. Our code, data, and demo videos are available at https://github.com/lil-lab/drif.</p>
<h2>2 Technical Overview</h2>
<p><strong>Task</strong> Our goal is to map natural language navigation instructions to continuous control of a quadcopter drone. The agent behavior is determined by a velocity controller setpoint $\rho = (v, \omega)$, where $v \in \mathbb{R}$ is a forward velocity and $\omega \in \mathbb{R}$ is a yaw rate. The model generates actions at fixed intervals. An action is either the task completion action STOP or a setpoint update $(v, \omega) \in \mathbb{R}^2$. Given a setpoint update $a_t = (v_t, \omega_t)$ at time $t$, we fix the controller setpoint $\rho = (v_t, \omega_t)$ that is maintained between actions. Given a start state $s_1$ and an instruction $u$, an execution $\Xi$ of length $T$ is a sequence $\langle (s_1, a_1), \ldots, (s_T, a_T)\rangle$, where $s_t$ is the state at time $t$, $a_{t&lt;T} \in \mathbb{R}^2$ are setpoint updates, and $a_T = \text{STOP}$. The state includes the quadcopter pose, internal state, and all landmark locations.</p>
<p><strong>Model</strong> We assume access to raw first-person monocular observations and pose estimates. The agent does not have access to the world state. At time $t$, the agent observes the <em>agent context</em> $c_t = (u, I_1, \cdots, I_t, P_1, \cdots P_t)$, where $u$ is the instruction and $I_t$ and $P_t$ are monocular first-person RGB images and 6-DOF agent poses observed at time $t$. We base our model on the Position Visitation Network [PVN; 16] architecture, and introduce mechanisms for reasoning about observability and exploration and learning across simulated and real environments. The model operates in two stages: casting planning as predicting distributions over world positions indicating the probability of visiting a position during execution, and generating actions to visit high probability positions.</p>
<p><strong>Learning</strong> We train jointly in simulated and physical environments. We assume access to a simulator and demonstration sets in both environments, $\mathcal{D}^{\mathbb{R}}$ in the physical environment and $\mathcal{D}^{\mathbb{S}}$ in the simulation. We do not interact with the physical environment during training. Each dataset includes $N^D$ examples ${(u^{(i)}, \Xi^{(i)}}_{i=1}^N^D$, where $D \in {\mathbb{R}, \mathbb{S}}$, $u^{(i)}$ is an instruction, and $\Xi^{(i)}$ is a demonstration execution. We do not require the datasets to be aligned or provide demonstrations for the same set of instructions. We propose SuReAL, a learning approach that concurrently trains the two model</p>
<p>stages in two separate processes. The planning stage is trained with supervised learning, while the action generation stage is trained with RL. The two processes exchange data and parameters. The trajectories collected during RL are added to the dataset used for supervised learning, and the planning stage parameters are periodically transferred to the RL process training the action generation stage. This allows the action generator to learn to execute the plans predicted by the planning stage, which itself is trained using on-policy observations collected from the action generator.</p>
<p>Evaluation We evaluate on a test set of $M$ examples $\left{\left(u^{(i)},s_{1}^{(i)},{\Xi^{(i)}}\right)\right}<em 1="1">{i=1}^{M}$, where $u^{(i)}$ is an instruction, $s</em>$ and executed trajectories.}^{(i)}$ is a start state, and $\Xi^{(i)}$ is a human demonstration. We use human evaluation to verify the generated trajectories are semantically correct with regard to the instruction. We also use automated metrics. We consider the task successful if the agent stops within a predefined Euclidean distance of the final position in $\Xi^{(i)}$. We evaluate the quality of generating the trajectory following the instruction using earth mover's distance between $\Xi^{(i)</p>
<h1>3 Related Work</h1>
<p>Natural language instruction following has been extensively studied using hand-engineered symbolic intermediate representations of world state or instruction semantics with physical robots [1, 2, $3,4,17,5,18,6,19]$ and simulated agents [20, 21, 22, 23, 24, 25]. In contrast, we study trading off the symbolic representation design with representation learning from demonstrations.</p>
<p>Representation learning has been studied for executing specific tasks such as grasping [7, 8, 10], dexterous manipulation [26, 27], or continuous flight [9]. Our aim is to execute navigation tasks specified in natural language, including new tasks at test time. This problem was addressed with representation learning in discrete simulated environments [28, 29, 30, 15, 31, 32], and more recently with continuous simulations [16]. However, these methods were not demonstrated on physical robots. A host of problems combine to make this challenging, including grounding natural language to constantly changing observations, robustly bridging the gap between relatively highlevel instructions to continuous control, and learning with limited language data and the high costs of robot usage.</p>
<p>Our model is based on the Position Visitation Network [16] architecture that incorporates geometric computation to represent language and observations in learned spatial maps. This approach is related to neural network models that construct maps [33, 34, 35, 36] or perform planning [37].</p>
<p>Our approach is aimed at a partial observability scenario and does not assume access to the complete system state. Understanding the instruction often requires identifying mentioned entities that are not initially visible. This requires exploration during task execution. Nyga et al. [38] studied modeling incomplete information in instructions with a modular approach. In contrast, we jointly learn to infer the absence of necessary information and to remedy it via exploration.</p>
<h2>4 Model</h2>
<p>We model the policy $\pi$ with a neural network. At time $t$, given the agent context $c_{t}$, the policy outputs a stopping probability $p_{t}^{\text {STOP }}$, a forward velocity $v_{t}$, and an angular velocity $\omega_{t}$. We decompose the architecture to two stages $\pi\left(c_{t}\right)=g\left(f\left(c_{t}\right)\right)$, where $f$ predicts the probability of visiting positions in the environment and $g$ generates the actions to visit high probability positions. The position visitation probabilities are continuously updated during execution to incorporate the most recent observations, and past actions directly affect the information available for future decisions. Our model is based on the PVN architecture [16]. We introduce several improvements, including explicit modeling of observability in both stages. Appendix B contains further model implementation details, including a detailed list of our improvements. Figure 3 illustrates our model for an example input.</p>
<p>Stage 1: Visitation Distribution Prediction At time $t$, the first stage $f(\cdot)$ generates two probability distributions: a trajectory-visitation distribution $d_{t}^{p}$ and a goal-visitation distribution $d_{t}^{g}$. Both distributions assign probabilities to positions $\mathcal{P}^{\text {obs }} \cup\left{p^{\text {sub }}\right}$, where $\mathcal{P}^{\text {obs }}$ is the set of positions observed up to time $t$ and $p^{\text {sub }}$ represents all yet-unobserved positions. The set $\mathcal{P}^{\text {obs }}$ is a discretized approximation of the continuous environment. This approximation enables efficient computation of the visitation distributions [16]. The trajectory-visitation distribution $d^{p}$ assigns high probability to positions the agent should go through during execution, and the goal-visitation distribution $d^{g}$ puts high probability on positions where the agent should STOP to complete its execution. We add</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Model architecture illustration. The first stage generates a semantic map <strong>S</strong><sup>W</sup>, a grounding map <strong>R</strong><sup>W</sup>, observability masks <strong>M</strong><sup>W</sup><sub>t</sub> and <strong>B</strong><sup>W</sup><sub>t</sub>, and visitation distributions <em>d</em><sup>p</sup><sub>t</sub> and <em>d</em><sup>g</sup><sub>t</sub>. The red and yellow arrows indicate the rock and banana locations. We show all intermediate representations at timestep 17 out of 37, after most of the environment has been observed. Figure 2 shows the visitation distributions for other timesteps, and Figure 8 in the appendix shows all timesteps. An animated version of this figure is available at https://youtu.be/UuZtSI6ckTk.</p>
<p>The special position <em>p</em><sup>sub</sup> to PVN to capture probability mass that should otherwise be assigned to positions not yet observed, for example when the goal position has not been observed yet.</p>
<p>The first stage combines a learned neural network and differentiable deterministic computations. The input instruction <em>u</em> is mapped to a vector <strong>u</strong> using a recurrent neural network (RNN). The input image <em>I</em><sub><em>t</em></sub> is mapped to a feature map <strong>F</strong><sup><em>C</em></sup><sub><em>t</em></sub> that captures spatial and semantic information using a convolutional neural network (CNN). The feature map <strong>F</strong><sup><em>C</em></sup><sub><em>t</em></sub> is processed using a deterministic semantic mapping process [34] using a pinhole camera model and the agent pose <em>P</em><sub><em>t</em></sub> to project <strong>F</strong><sup><em>C</em></sup><sub><em>t</em></sub> onto the environment ground at zero elevation. The projected features are deterministically accumulated from previous timesteps to create a semantic map <strong>S</strong><sup>W</sup><sub><em>t</em></sub>. <strong>S</strong><sup>W</sup><sub><em>t</em></sub> represents each position with a learned feature vector aggregated from all past observations of that position. For example, in Figure 3, the <em>banana</em> and the <em>white bush</em> can be identified in the raw image features <strong>F</strong><sup><em>C</em></sup><sub><em>t</em></sub> and the projected semantic map <strong>S</strong><sup>W</sup><sub><em>t</em></sub>, where their representations are identical. We generate a language-conditioned grounding map <strong>R</strong><sup>W</sup><sub><em>t</em></sub> by creating convolutional filters using the text representation <strong>u</strong> and filtering <strong>S</strong><sup>W</sup><sub><em>t</em></sub>. The two maps aim to provide different representation: <strong>S</strong><sup>W</sup><sub><em>t</em></sub> aims for a language-agnostic environment representation and <strong>R</strong><sup>W</sup><sub><em>t</em></sub> is intended to focus on the objects mentioned in the instruction. We use auxiliary objectives (Appendix C.4) to optimize each map to contain the intended information. We predict the two distributions using LINGUNET [15], a language-conditioned variant of the UNET image reconstruction architecture [39], which takes as input the learned maps, <strong>S</strong><sup>W</sup><sub><em>t</em></sub> and <strong>R</strong><sup>W</sup><sub><em>t</em></sub>, and the instruction representation <strong>u</strong>. Appendix B.1 provides a detailed description of this architecture.</p>
<p>We add two outputs to the original PVN design: an observability mask <strong>M</strong><sup>W</sup><sub><em>t</em></sub> and a boundary mask <strong>B</strong><sup>W</sup><sub><em>t</em></sub>. Both are computed deterministically given the agent pose estimate and the camera parameters, and are intended to aid exploration of the environment during instruction execution. <strong>M</strong><sup>W</sup><sub><em>t</em></sub> assigns 1 to each position <em>p</em> ∈ <strong>P</strong> in the environment if <em>p</em> has been observed by the agent's first-person camera by time <em>t</em>, or 0 otherwise. <strong>B</strong><sup>W</sup><sub><em>t</em></sub> assigns 1 to environment boundaries and 0 to other positions. Together, the masks provide information about what parts of the environment remain to be explored.</p>
<p><strong>Stage 2: Action Generation</strong> The second stage <em>g</em>(·) is a <em>control network</em> that receives four inputs: a trajectory-visitation distribution <em>d</em><sup>p</sup><sub>t</sub>, a goal-visitation visitation distribution <em>d</em><sup>g</sup><sub>t</sub>, an observability mask <strong>M</strong><sup>W</sup><sub><em>t</em></sub>, and a boundary mask <strong>B</strong><sup>W</sup><sub><em>t</em></sub>. The four inputs are rotated to the current egocentric agent reference frame, and used to generate the output velocities using a learned neural network. Appendix B.2 describes the network architecture. The velocities are generated to visit high probability positions according to <em>d</em><sup>p</sup>, and the STOP probability is predicted to stop in a likely position according to <em>d</em><sup>g</sup>. In the figure, <em>d</em><sup>p</sup><sub>t</sub> shows the curved flight path, and <em>d</em><sup>g</sup><sub>t</sub> identifies the goal <em>right of the banana</em>. Our addition of the two masks and the special position <em>p</em><sup>sub</sup> enables generating actions to explore the environment to reduce <em>d</em><sup>p</sup>(<em>p</em><sup>sub</sup>) and <em>d</em><sup>g</sup>(<em>p</em><sup>sub</sup>). Figure 2 visualizes <em>d</em><sup>g</sup>(<em>p</em><sup>sub</sup>) with a green bar, showing how it starts high, and decreases once the goal is observed at step <em>t</em> = 15.</p>
<h3>5 Learning</h3>
<p>We learn two sets of parameters: θ for the first stage <em>f</em>(·) and φ for the second stage <em>g</em>(·). We use a simulation for all autonomous flight during learning, and jointly train for both the simulation and the physical environment. This includes training a simulation-specific first stage <em>f</em><sub>S</sub>(·) with additional parameters θ<sub>S</sub>. The second stage model <em>g</em>(·) is used in both environments. We assume access to</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning architecture. Stages 1 and 2 of our model are concurrently trained in processes A and B. The blue and orange blocks, and red arrows, represent modules, computation, and loss functions during training only. The white blocks form the final learned policy. We learn from inputs from simulation and real world environments, by switching between the two CNN modules.</p>
<p>sets of training examples $\mathcal{D}^{\mathcal{S}}={[(u^{(i)}, \Xi^{(i)})}]<em i="1">{i=1}^{N^{\mathcal{S}}}$ for the simulation and $\mathcal{D}^{\mathcal{R}}={[(u^{(i)}, \Xi^{(i)})}]</em>$ are demonstration executions. The training examples are not spatially or temporally aligned between domains.}^{N^{\mathcal{R}}}$ for the physical environment, where $u^{(i)}$ are instructions and $\Xi^{(i)</p>
<p>Our learning algorithm, Supervised and Reinforcement Asynchronous Learning (SuREAL), uses two concurrent asynchronous processes. Each process only updates the parameters of one stage. Process A uses supervised learning to estimate Stage 1 parameters for both environments: $\theta$ for the physical environment model $f(\cdot)$ and $\theta_{\mathcal{S}}$ for $f_{\mathcal{S}}(\cdot)$. We use both $\mathcal{D}^{\mathcal{R}}$ and $\mathcal{D}^{\mathcal{S}}$ to update the model parameters. We use RL in Process B to learn the parameters $\phi$ of the second stage $g(\cdot)$ using an intrinsic reward function. We start learning using the provided demonstrations in $\mathcal{D}^{\mathcal{S}}$ and periodically replace execution trajectories with RL rollouts from Process B, keeping a single execution per instruction at any time. We warm-start by running Process A for $K_{\text{iter}}^B$ iterations before launching Process B to make sure that Process B always receives as input sensible visitation predictions instead of noise. The model parameters are periodically synchronized by copying the simulation parameters of Stage 1 from Process A to B. For learning, we use a learning architecture (Figure 4) that extends our model to process simulation observations and adds a discriminator that encourages learning representations that are invariant to the type of visual input.</p>
<p>Process A: Supervised Learning for Visitation Prediction We train $f(\cdot)$ and $f_{\mathcal{S}}(\cdot)$ to: (a) minimize the KL-divergence between the predicted visitation distributions and reference distributions generated from the demonstrations, and (b) learn domain invariant visual representations that allow sharing of instruction grounding and execution between the two environments. We use demonstration executions in the real environment $\mathcal{D}^{\mathcal{R}}$ and in the simulated environment $\mathcal{D}^{\mathcal{S}}$. The loss for executions from the physical environment $\Xi^{\mathcal{R}}$ and the simulation $\Xi^{\mathcal{S}}$ is:</p>
<p>$$
\mathcal{L}<em _in="\in" _mathcal_C="\mathcal{C" c="c">{\mathrm{SL}}(\Xi^{\mathcal{R}}, \Xi^{\mathcal{S}}) = \frac{1}{|\Xi^{\mathcal{R}}|} \sum</em>(f(c) | f^}(\Xi^{\mathcal{R}})} D_{\mathrm{KL}<em> (c)) + \frac{1}{|\Xi^{\mathcal{R}}|} \sum_{c \in \mathcal{C}(\Xi^{\mathcal{R}})} D_{\mathrm{KL}}(f_{\mathcal{S}}(c) | f^</em> (c)) + \mathcal{L}_W(\Xi^{\mathcal{R}}, \Xi^{\mathcal{S}}) \quad , \tag{1}
$$</p>
<p>where $\mathcal{C}(\Xi)$ is the sequence of contexts observed by the agent during an execution $\Xi$ and $f^* (c)$ creates the gold-standard visitation distribution examples (i.e., Stage 1 outputs) for a context $c$ from the training data. The term $\mathcal{L}_W(\Xi^{\mathcal{R}}, \Xi^{\mathcal{S}})$ aims to make the feature representation $\mathbf{F}^{C}$ indistinguishable between real and simulated images. This allows the rest of the model to use either simulated or real observations interchangeably. $\mathcal{L}_W(\Xi^{\mathcal{R}}, \Xi^{\mathcal{S}})$ is the approximated empirical Wasserstein distance between the visual feature distributions extracted from simulated and real agent contexts:</p>
<p>$$
\mathcal{L}<em _Xi_mathcal_R="\Xi^{\mathcal{R" _in="\in" c_t="c_t">W(\Xi^{\mathcal{R}}, \Xi^{\mathcal{S}}) = \frac{1}{|\Xi^{\mathcal{R}}|} \sum</em>(I_t)) \quad ,
$$}}} h(\text{CNN}^{\mathcal{S}}(I_t)) - \frac{1}{|\Xi^{\mathcal{R}}|} \sum_{c_t \in \Xi^{\mathcal{R}}} h(\text{CNN</p>
<p>where $h$ is a Lipschitz continuous neural network discriminator with parameters $\psi$ that we train to output high values for simulated features and low values for real features [40, 41]. $I_t$ is the $t$-th image in the agent context $c_t$. The discriminator architecture is described in Appendix C.1.</p>
<p>Algorithm 1 shows the supervised optimization procedure. We alternate between updating the discriminator parameters $\psi$, and the first stage model parameters $\theta$ and $\theta_{\mathcal{S}}$. At every iteration, we perform $K_{\text{discr}}^{\text{SL}}$ gradient updates of $\psi$ to maximize the Wasserstein loss $\mathcal{L}<em _mathcal_S="\mathcal{S">W$ (lines 3–7), and then perform a single gradient update to $\theta$ and $\theta</em>}}$ to minimize supervised learning loss $\mathcal{L<em _mathcal_S="\mathcal{S">{\text{SL}}$ (lines 8–10). We send the simulation-specific parameters $\theta</em>$ iterations (line 12).}}$ to the RL process every $K_{\text{iter}}^{\text{SL}</p>
<p>Process B: Reinforcement Learning for Action Generation We train the action generator $g(\cdot)$ using RL with an intrinsic reward. We use Proximal Policy Optimization [PPO; 42] to maximize the expected return. The learner has no access to an external task reward, but instead computes a reward $r(\cdot)$ from how well the agent follows the visitation distributions generated by the first stage:</p>
<p>$$
r(c_t, a_t) = \lambda_v r_v(c_t, a_t) + \lambda_s r_s(c_t, a_t) + \lambda_s r_v(c_t, a_t) - \lambda_a r_a(a_t) - \lambda_{step} \quad ,
$$</p>
<p>Input: First stage models $f$ and $f_{\theta}$ with parameters $\theta$ and $\theta_{\theta}$, discriminator $h$ with parameters $\psi$, datasets of simulated and physical environment trajectories $\mathcal{D}^{\theta}$ and $\mathcal{D}^{\theta}$.
Definitions: $\mathcal{D}^{\theta}$ and $f_{\theta}^{D}$ are shared with Process B.
1: while Process B has not finished do
2: for $i=1, \ldots, K_{\text {sizer }}^{\text {NL }}$ do
3: for $j=1, \ldots, K_{\text {sizer }}^{\text {NL }}$ do
4: $\quad$ Sample trajectories
5: $\quad \Xi^{\theta} \sim \mathcal{D}^{\theta}$ and $\Xi^{\theta} \sim \mathcal{D}^{\theta}$
6: $\quad$ Update discriminator to maximize Wasserstein distance
7: $\quad \psi \leftarrow \operatorname{ADAM}\left(\nabla_{\psi}-\mathcal{L}<em _theta="\theta">{W}\left(\Xi^{\theta}, \Xi^{\theta}\right)\right)$
8: $\quad \Xi^{\theta} \sim \mathcal{D}^{\theta}$ and $\Xi^{\theta} \sim \mathcal{D}^{\theta}$
9: $\quad$ Update first stage parameters
10: $\quad\left(\theta</em>}, \theta\right) \leftarrow \operatorname{ADAM}\left(\nabla_{\theta_{\theta}, \theta} \mathcal{L<em _theta="\theta">{\mathrm{RL}}\left(\Xi^{\theta}, \Xi^{\theta}\right)\right)$
11: Send $f</em>$ to Process B if it is running
12: $f_{\theta}^{B} \leftarrow f_{\theta_{B}}$
13: if $i=K_{\text {sizer }}^{B}$ then
14: Launch Process B (Algorithm 2)
15: return $f$</p>
<p>Algorithm 2 Process B: Reinforcement Learning</p>
<p>Input: Simulation dataset $\mathcal{D}^{\theta}$, second-stage model $g$ with parameters $\phi$, value function $V$ with parameters $v$, first-stage simulation model $f_{\theta}$.
Definitions: $\operatorname{MERGE}\left(\mathcal{D}, E\right)$ is a set of sentence-execution pairs including all instructions from $\mathcal{D}$, where each instruction is paired with an execution from $E$, or $\mathcal{D}$ if not in $E . \mathcal{D}^{\theta}$ and $f_{\theta}^{B}$ are shared with Process A.
1: for $e=1, \ldots, K_{\text {epoch }}^{\mathrm{NL}}$ do
2: $\quad$ Get the most recent update from Process A
3: $f_{\theta} \leftarrow f_{\theta}^{B}$
4: for $i=1, \ldots, K_{\text {sizer }}^{\mathrm{HL}}$ do
5: Sample simulator executions of $N$ instructions
6: $\quad \hat{\Xi}^{(1)}, \ldots, \hat{\Xi}^{(N)} \sim g\left(f_{\theta}(\cdot)\right)$
7: for $j=1, \ldots, K_{\text {steps }}^{\mathrm{HL}}$ do
8: Sample state-action-return tuples and update
9: $\quad X \sim \hat{\Xi}<em N="N">{1}, \ldots, \hat{\Xi}</em>$
10: $\quad \phi, v \leftarrow \operatorname{ADAM}\left(\nabla_{\phi, v} \mathcal{L}<em 1="1">{P P O}(X, V)\right)$
11: Update executions to share with Process A
12: $\quad \mathcal{D}^{\theta} \leftarrow \operatorname{MERGE}\left(\mathcal{D}^{\theta},\left{\hat{\Xi}</em>}, \ldots, \hat{\Xi<em t="t">{N}\right}\right)$
13: return $g$
where $c</em>$ is negative per-step reward to encourage efficient execution. We provide the reward implementation details in Appendix C.3.
Algorithm 2 shows the RL procedure. At every iteration, we collect $N$ simulation executions $\hat{\Xi}^{(1)}, \ldots, \hat{\Xi}^{(N)}$ using the policy $g\left(f_{\theta}(\cdot)\right)$ (line 6). To sample setpoint updates we treat the existing output as the mean of a normal distribution, add variance prediction, and sample the two velocities from the predicted normal distributions. We perform $K_{\text {steps }}^{\mathrm{HL}}$ PPO updates using the return and value estimates (lines 7-10). For every update, we sample state-action-return triplets from the collected trajectories, compute the PPO loss $\mathcal{L}_{P P O}(X, V)$, update parameters $\phi$, and update the parameters $v$ of the value function $V$. We pass the sampled executions to Process A to allow the model to learn to predict the visitation distributions in a way that is robust to the agent actions (line 12).}$ is the agent context at time $t$ and $a_{t}$ is the action. The reward $r(\cdot)$ is a weighted combination of five terms. The visitation reward $r_{v}(\cdot)$ is the per-timestep reduction in earth mover's distance between the predicted distribution $d_{t}^{p}$ and an empirical distribution that assigns equal probability to every position visited until time $t$. This smooth and dense reward encourages $g(\cdot)$ to follow the visitation distributions predicted by $f(\cdot)$. The stop reward $r_{s}(\cdot)$ is only non-zero for STOP, when it is the earth mover's distance between the predicted goal distribution $d_{t}^{g}$ and an empirical stopping distribution that assigns the full probability mass to the stop position in the policy execution. The exploration reward $r_{e}(\cdot)$ combines a positive reward for reducing the belief that the goal has not been observed yet (i.e., reducing $d_{t}^{g}\left(p^{\text {web }}\right)$ ) and a negative reward proportional to the probability that the goal position is unobserved according to $d_{t}^{g}\left(p^{\text {web }}\right)$. The action reward $r_{a}(\cdot)$ penalizes actions outside of the controller range. Finally, $\lambda_{\text {step }</p>
<h1>6 Experimental Setup</h1>
<p>We provide the complete implementation and experimental setup details in Appendix E.
Environment and Data We use an Intel Aero quadcopter with a PX4 flight controller, and a Vicon motion capture system for pose estimates. For simulation, we use the quadcopter simulator from Blukis et al. [34] that is based on Microsoft AirSim [43]. The environment size is $4.7 \times 4.7 \mathrm{~m}$. We randomly create environments with 5-8 landmarks, selected randomly out of a set of 15 . Figure 1 shows the real environment. We follow the crowdsourcing setup of Misra et al. [15] to collect 997 paragraphs with 4,557 segments. We use 3,245/640/672 for training, development, and testing. We expand this data by concatenating consecutive segments to create more challenging instructions, including with exploration challenges [32]. Figure 1 shows an instruction made of two consecutive segments. The simulator dataset $\mathcal{D}^{\theta}$ includes oracle demonstrations of all instructions, while the real-world dataset $\mathcal{D}^{\theta}$ includes only 402 single-segment demonstrations. For evaluation in the physical environment, we sample 20 test paragraphs consisting of 93 single-segment and 73 two-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Human evaluation results on the physical quadcopter on two-segment data. We plot the Likert scores using Gantt charts of score frequencies. The black numbers indicate average scores. segment instructions. We use both single and concatenated instructions for training, and test on each set separately. We also use the original Misra et al. [15] data as additional simulation training data. Appendix D provides data statistics and further details.</p>
<p><strong>Evaluation</strong> We use human judgements to evaluate if the agent's final position is correct with regard to the instruction (goal score) and how well the agent followed the path described by the instruction (path score). We present MTurk workers with an instruction and a top-down animation of the agent behavior, and ask for a 5-point Likert-scale score for the final position and trajectory correctness. We obtain five judgements per example per system. We also automatically measure (a) SR: success rate of stopping within 47cm of the correct position; and (b) EMD: earth mover's distance in meters between the agent and demonstration trajectories. Appendix F provides more evaluation details.</p>
<p><strong>Systems</strong> We compare our approach, PVN2-SuREAL, with three non-learning and two learning baselines: (1) STOP: output the STOP action without movement; (2) AVERAGE: take the average action for the average number of steps; (3) ORACLE: a hand-crafted upper-bound expert policy that has access to the ground truth human demonstration; (4) PVN-BC the Blukis et al. [16] PVN model trained with behavior cloning; and (5) PVN2-BC: our model trained with behavior cloning. The two behavior cloning systems require access to an oracle that provides velocity command output labels, in addition to demonstration data. SuREAL uses demonstrations, but does not require the oracle during learning. None of the learned systems use any oracle data at test time. All learned systems use the same training data $$ \mathcal{D}^b $$ and $$ \mathcal{D}^h $$, and include the domain-adaptation loss (Equation 1).</p>
<h1>7 Results</h1>
<p>Figure 5 shows human evaluation Likert scores. Our model receives five-point scores 39.72% of the time for getting the goal right, and 37.8% of the time for the path. This is a 34.9% and 24.8% relative improvement compared to PVN2-BC, the next best system. This demonstrates the benefits of modeling observability, using SuREAL for training-time exploration, and using a reward function that trades-off task performance and test-time exploration. The AVERAGE baseline received only 15.8% 5-point ratings in both <em>path score</em> and <em>goal score</em>, demonstrating the task difficulty.</p>
<p>We study how well our model addresses observability and exploration challenges. Figure 6 shows human path score judgements split to tasks where the goal is visible from the agent's first-person view at start position (34 examples) and those where it is not and exploration is required (38 examples). Our approach outperforms the baselines in cases requiring exploration, but it is slightly outperformed by PVN2-BC in simple examples. This could be explained by our agent attempting to explore the environment in cases where it is not necessary.</p>
<p>Table 1 shows the automated metrics for both environments. We observe that the success rate (SR) measure is sensitive to the threshold selection, and correct executions are often considered as wrong; PVN2-SuREAL gets 30.6% SR compared to a perfect human score 39.72% of the time. This highlights the need for human evaluation, and must be considered when interpreting the SR results. We generally find EMD more reliable, although it also does not account for semantic correctness.</p>
<p>Comparing to PVN2-BC, our approach performs better on the real environment demonstrating the benefit of SuREAL. In simulation, we observe better EMD, but worse SR. Qualitatively, we observe our approach often recovers the correct overall trajectory, with a slightly imprecise stopping location due to instruction ambiguity or control challenges. Such partial correctness is not captured by SR. Comparing PVN2-BC and PVN-BC, we see the benefit of modeling observability. SuREAL further improves upon PVN2-BC, by learning to explore unobserved locations at test-time.</p>
<p>Comparing our approach between simulated and real environments, we see an absolute performance degradation of 2.7% SR and 0.1 EMD from simulation to the real environment. This highlights the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">1-segment</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2-segment</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;">Test Results</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AVERAGE</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN-BC</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-BC</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AVERAGE</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN-BC</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-BC</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;">Development Results</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL-NOU</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL50real</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL ${ }_{10}$ real</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL-NOU</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PVN2-SuREAL-NOI</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">0.45</td>
</tr>
</tbody>
</table>
<p>Table 1: Automated evaluation test and development results. SR: success rate (\%) and EMD: earth-mover's distance in meters between agent and demonstration trajectories.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Human evaluation path score frequencies (Figure 5) decomposed by initially unobservable (top) or observable (bottom) goal location.
remaining challenges of visual domain transfer and complex flight dynamics. The flight dynamics challenges are also visible in the ORACLE performance degradation between the two environments.</p>
<p>We study several ablations. First, we quantify the effect of using a smaller number of real-world training demonstrations. We randomly select subsets of demonstrations, with the constraint that all objects are visually represented. We find that using only half (200) of the physical demonstrations does not appear to reduce performance ( $\mathrm{PVN} 2-\mathrm{SUREAL}<em 10="10" _="{" _text="\text" real="real">{50 \text { real }}$ ), while using only $10 \%$ (40), drastically hurts real-world performance ( $\mathrm{PVN} 2-\mathrm{SUREAL}</em>$ ). This shows that the learning method is successfully leveraging real-world data to improve performance, while requiring relatively modest amounts of data. We also study performance without access to the instruction (PVN2-SuReAL-NOU), and with using a blank input image (PVN2-SuReAL-NOI). The relatively high SR of these ablations on 1-segment instructions highlights the inherent bias in simple trajectories. The 2-segment data, which is our main focus, is much more robust to such biases. Appendix G provides more automatic evaluation results, including additional ablations and results on the original data of Misra et al. [15].}</p>
<h1>8 Discussion</h1>
<p>We study the problem of mapping natural language instructions to continuous control of a physical quadcopter drone. Our two-stage model decomposition allows some level of re-use and modularity. For example, a trained Stage 1 can be re-used with different robot types. This decomposition and the interpretability it enables also create limitations, including limited sensory input for deciding about control actions given the visitation distributions. These are both important topics for future study.</p>
<p>Our learning method, SuREAL, uses both annotated demonstration trajectories and a reward function. In this work, we assume demonstration trajectories were generated with an expert policy. However, SuREAL does not necessarily require the initial demonstrations to come from a reasonable policy, as long as we have access to the gold visitation distributions, which are easier to get compared to oracle actions. For example, given an initial policy that immediately stops instead of demonstrations, we will train Stage 1 to predict the given visitation distributions and Stage 2 using the intrinsic reward. Studying this empirically is an important direction for future work.</p>
<p>Finally, our environment provides a strong testbed for our system-building effort and the transition from the simulation to the real world. However, various problems are not well represented, such as reasoning about obstacles, raising important directions for future work. While we do not require the simulation to accurately reflect the real world, studying scenarios with stronger difference between the two is another important future direction. Our work also points towards the need for better automatic evaluation for instruction following, or, alternatively, wide adoption of human evaluation.</p>
<h1>Acknowledgments</h1>
<p>This research was supported by the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program, a Google Faculty Award, NSF CAREER-1750499, AFOSR FA9550-17-1-0109, an Amazon Research Award, and cloud computing credits from Amazon. We thank Dipendra Misra, Alane Suhr, and the anonymous reviewers for their helpful comments.</p>
<h2>References</h2>
<p>[1] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. Gopal Banerjee, S. Teller, and N. Roy. Approaching the Symbol Grounding Problem with Probabilistic Graphical Models. AI Magazine, 2011.
[2] C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. A Joint Model of Language and Perception for Grounded Attribute Learning. In ICML, 2012.
[3] F. Duvallet, T. Kollar, and A. Stentz. Imitation learning for natural language direction following through unknown environments. In ICRA, 2013.
[4] M. R. Walter, S. Hemachandra, B. Homberg, S. Tellex, and S. Teller. Learning Semantic Maps from Natural Language Descriptions. In RSS, 2013.
[5] S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter. Learning models for following natural language directions in unknown environments. In ICRA, 2015.
[6] N. Gopalan, D. Arumugam, L. L. Wong, and S. Tellex. Sequence-to-sequence language grounding of non-markovian task specifications. In RSS, 2018.
[7] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. IJRR, 2015.
[8] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen. Learning hand-eye coordination for robotic grasping with large-scale data collection. In ISER, 2016.
[9] F. Sadeghi and S. Levine. Cad2rl: Real single-image flight without a single real image. In RSS, 2017.
[10] D. Quillen, E. Jang, O. Nachum, C. Finn, J. Ibarz, and S. Levine. Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of off-policy methods. ICRA, 2018.
[11] A. A. Rusu, M. Večerík, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell. Sim-to-real robot learning from pixels with progressive nets. In CoRL, 2017.
[12] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. ICRA, 2018.
[13] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017.
[14] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov. Gated-attention architectures for task-oriented language grounding. AAAI, 2018.
[15] D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkin, and Y. Artzi. Mapping instructions to actions in 3D environments with visual goal prediction. In EMNLP, 2018.
[16] V. Blukis, D. Misra, R. A. Knepper, and Y. Artzi. Mapping navigation instructions to continuous control actions with position-visitation prediction. In CoRL, 2018.
[17] D. K. Misra, J. Sung, K. Lee, and A. Saxena. Tell me dave: Context-sensitive grounding of natural language to mobile manipulation instructions. In RSS, 2014.
[18] J. Thomason, S. Zhang, R. J. Mooney, and P. Stone. Learning to interpret natural language commands through human-robot dialog. In International Joint Conferences on Artificial Intelligence, 2015.
[19] E. C. Williams, N. Gopalan, M. Rhee, and S. Tellex. Learning to parse natural language to grounded reward functions with weak supervision. In ICRA, 2018.
[20] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.</p>
<p>[21] S. R. K. Branavan, L. S. Zettlemoyer, and R. Barzilay. Reading between the lines: Learning to map high-level instructions to commands. In ACL, 2010.
[22] C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. Learning to parse natural language commands to a robot control system. In ISER, 2012.
[23] Y. Artzi and L. Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. TACL, 2013.
[24] Y. Artzi, D. Das, and S. Petrov. Learning compact lexicons for CCG semantic parsing. In EMNLP, 2014.
[25] A. Suhr and Y. Artzi. Situated mapping of sequential instructions to actions with single-step reward observation. In $A C L, 2018$.
[26] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. JMLR, 2016.
[27] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine. Combining selfsupervised learning and imitation for vision-based rope manipulation. In ICRA, 2017.
[28] D. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017.
[29] P. Shah, M. Fiser, A. Faust, J. C. Kew, and D. Hakkani-Tur. Follownet: Robot navigation by following natural language directions with deep reinforcement learning. arXiv preprint arXiv:1805.06150, 2018.
[30] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018.
[31] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navigation. In Advances in Neural Information Processing Systems, 2018.
[32] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. In ACL, 2019.
[33] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In CVPR, 2017.
[34] V. Blukis, N. Brukhim, A. Bennet, R. Knepper, and Y. Artzi. Following high-level navigation instructions on a simulated quadcopter with imitation learning. In RSS, 2018.
[35] A. Khan, C. Zhang, N. Atanasov, K. Karydis, V. Kumar, and D. D. Lee. Memory augmented control networks. In ICLR, 2018.
[36] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee. Chasing ghosts: Instruction following as bayesian state tracking. 2019.
[37] A. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn. Universal planning networks. ICML, 2018.
[38] D. Nyga, S. Roy, R. Paul, D. Park, M. Pomarlan, M. Beetz, and N. Roy. Grounding robot plans from natural language instructions with incomplete world knowledge. In CoRL, 2018.
[39] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, 2015.
[40] J. Shen, Y. Qu, W. Zhang, and Y. Yu. Wasserstein distance guided representation learning for domain adaptation. In AAAI, 2018.
[41] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, 2017.
[42] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[43] S. Shah, D. Dey, C. Lovett, and A. Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics, 2017.
[44] A. Suhr, C. Yan, J. Schluger, S. Yu, H. Khader, M. Mouallem, I. Zhang, and Y. Artzi. Executing instructions in situated collaborative interactions. In EMNLP, 2019.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Instruction executions from the development set on the physical quadcopter. For each example, the figure shows (from the left) the input instruction, the initial image that the agent observes, the initial visitation distributions overlaid on the top-down view, visitation distributions at the midpoint of the trajectory, and the final visitation distributions when outputting the STOP action. The green bar on the lower-right corner of each distribution plot shows the predicted probability that the goal is not yet observed. The blue arrow indicates the agent pose.</p>
<h1>A Execution Examples on Real Quadcopter</h1>
<p>Examples of Different Instruction Executions Figure 7 shows a number of instruction-following executions collected on the real drone, showing successes and some typical failures.</p>
<p>Visualization of Intermediate Representations Figure 8 shows the intermediate representations and visitation predictions over time during instruction execution for the examples used in Figures 13 , illustrating the model reasoning. The model is able to output the STOP action to stop on the right side of the banana, even after the banana has disappeared from the first-person view. This demon-</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Illustration of changes in the intermediate representations during an instruction execution, showing how information is accumulated in the semantic maps over time, and how that affects the predicted visitation distributions. We show the instruction from Figures 1-3. From top to bottom, representations are shown for timesteps $0,10,15,20,25,30$, and 37 (the final timestep). From left to right, we show the input image $I_{t}$, first-person features $\mathbf{F}<em t="t">{t}^{C}$, semantic map $\mathbf{S}</em>}^{W}$, grounding map $\mathbf{R<em t="t">{t}^{W}$, goal and position visitation distributions $d</em>}^{g}$ and $d_{t}^{g}$, observability mask $\mathbf{M<em t="t">{t}^{W}$ and boundary mask $\mathbf{B}</em>$, and the overhead view of the environment. The agent position is indicated with a blue arrow in the overhead view. The agent does not have access to the overhead view, which is provided for illustration purposes only.
strates the advantages of using an explicit spatial map aggregated over time instead, for example, a learned hidden state vector representing the agent state.}^{W</p>
<h1>B Model Details</h1>
<h2>B. 1 Stage 1: Visitation Distribution Prediction</h2>
<p>The first-stage model is largely based on the Position Visitation Network, except for several improvements we introduce:</p>
<ul>
<li>Computing observability and boundary masks $\mathbf{M}^{W}$ and $\mathbf{B}^{W}$ that are used to track unexplored space and environment boundaries.</li>
</ul>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: The 13-layer ResNet architecture used in PVN and PVN2 networks (figure adapted from Blukis et al. [34]).</p>
<ul>
<li>Introducing a placeholder position $p^{\text {nob }}$ that represents all unobserved positions in the environment for use in the visitation distributions.</li>
<li>Modification to the LingUNet architecture to support outputting a probability score for the unobserved placeholder position, in addition to the 2D distributions over environment positions.</li>
<li>Predicting 2D probability distributions only over observed locations in the environment.</li>
<li>Minor hyperparameter changes to better support the longer instructions.</li>
</ul>
<p>The description in Section B. 1 has been taken from Blukis et al. [16]. We present it here for reference and completeness, with minor modifications to highlight technical differences.</p>
<h1>B.1.1 Instruction Representation</h1>
<p>We represent the instruction $u=\left\langle u_{1}, \cdots u_{l}\right\rangle$ as an embedded vector $\mathbf{u}$. We generate a series of hidden states $\mathbf{h}<em i="i">{i}=\operatorname{LSTM}\left(\phi\left(u</em>}\right), \mathbf{h<em l="l">{i-1}\right), i=1 \ldots l$, where LSTM is a Long-Short Term Memory recurrent neural network (RNN) and $\phi$ is a learned word-embedding function. The instruction embedding is the last hidden state $\mathbf{u}=\mathbf{h}</em>$. This part is replicated as is from Blukis et al. [16].</p>
<h2>B.1.2 Semantic Mapping</h2>
<p>We construct the semantic map using the method of Blukis et al. [34]. The full details of the process are specified in the original paper. Roughly speaking, the semantic mapping process includes three steps: feature extraction, projection, and accumulation. At timestep $t$, we process the currently observed image $I_{t}$ using a 13-layer residual neural network CNN (Figure 9) to generate a feature map $\mathbf{F}<em t="t">{t}^{C}=\operatorname{CNN}\left(I</em>}\right)$ of size $W_{f} \times H_{f} \times C$. We compute a feature map in the world coordinate frame $\mathbf{F<em t="t">{t}^{W}$ by projecting $\mathbf{F}</em>$ with a pinhole camera model onto the ground plane at elevation zero.
The semantic map of the environment $\mathbf{S}}^{C<em t="t">{t}^{W}$ at time $t$ is an integration of $\mathbf{F}</em>}^{W}$ and $\mathbf{S<em t="t">{t-1}^{W}$, the map from the previous timestep. The integration equation is given in Section 4c in Blukis et al. [34]. This process generates a tensor $\mathbf{S}</em>}^{W}$ of size $W_{w} \times H_{w} \times C$ that represents a map, where each location $\left[\mathbf{S<em _x_="(x," y_="y)">{t}^{W}\right]</em>}$ is a $C$-dimensional feature vector computed from all past observations $I_{&lt;t}$, each processed to learned features $\mathbf{F<em 1="1">{&lt;t}^{C}$ and projected onto the environment ground in the world frame at coordinates $(x, y)$. This map maintains a learned high-level representation for every world location $(x, y)$ that has been visible in any of the previously observed images. We define the world coordinate frame using the agent starting pose $P</em>$; the agent start position is the coordinates $(0,0)$, and the positive direction of the $x$-axis is along the agent heading. This gives consistent meaning to spatial language, such as turn left or pass on the left side of.</p>
<h2>B.1.3 Grounding</h2>
<p>We create the grounding map $\mathbf{R}<em t="t">{t}^{W}$ with a $1 \times 1$ convolution $\mathbf{R}</em>}^{W}=\mathbf{S<em G="G">{t}^{W} \circledast \mathbf{K}</em>}$. The kernel $\mathbf{K<em G="G">{G}$ is computed using a learned linear transformation $\mathbf{K}</em>}=\mathbf{W<em G="G">{G} \mathbf{u}+\mathbf{b}</em>}$, where $\mathbf{u}$ is the instruction embedding. The grounding map $\mathbf{R<em t="t">{t}^{W}$ has the same height and width as $\mathbf{S}</em>$, and during training we optimize the parameters so it captures the objects mentioned in the instruction $u$ (Section C.4).}^{W</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: The LingUNET architecture, showing the additional output head that was added as part of the PVN2 model. LingUNET outputs raw scores, which we normalize over the domain of each distribution. This figure is adapted from Blukis et al. [16].</p>
<h1>B.1.4 LingUNET and Visitation Distributions</h1>
<p>The following paragraphs are adapted from Blukis et al. [16] and formally define the LingUNET architecture with our modifications. Figure 10 illustrates the architecture.
LINGUNET uses a series of convolution and scaling operations. The input map $\mathbf{F}<em t="t">{0}=\left[\mathbf{S}</em>}^{W}, \mathbf{R<em k="k">{t}^{W}\right]$ is processed through $L$ cascaded convolutional layers to generate a sequence of feature maps $\mathbf{F}</em>}=$ $\operatorname{CNN<em k-1="k-1">{k}^{D}\left(\mathbf{F}</em>}\right), k=1 \ldots L .{ }^{1}$ Each $\mathbf{F<em k="k">{k}$ is filtered with a $1 \times 1$ convolution with weights $\mathbf{K}</em>}$. The kernels $\mathbf{K<em k="k">{k}$ are computed from the instruction embedding $\mathbf{u}$ using a learned linear transformation $\mathbf{K}</em>}=\mathbf{W<em k="k">{k}^{u} \mathbf{u}+\mathbf{b}</em>}^{u}$. This generates $l$ language-conditioned feature maps $\mathbf{G<em k="k">{k}=\mathbf{F}</em>, k=1 \ldots L$. A series of $L$ upscale and convolution operations computes $L$ feature maps of increasing size:} \circledast \mathbf{K}_{k</p>
<p>$$
\mathbf{H}<em k="k">{k}= \begin{cases}\operatorname{Upscale}\left(\operatorname{CNN}</em>}^{U}\left(\left[\mathbf{H<em k="k">{k+1}, \mathbf{G}</em>}\right]\right)\right), &amp; \text { if } 1 \leq k \leq L-1 \ \operatorname{Upscale}\left(\operatorname{CNN<em k="k">{k}^{U}\left(\mathbf{G}</em>
$$}\right)\right), &amp; \text { if } k=L\end{cases</p>
<p>We modify the original LingUNET design by adding an output head that outputs a vector $\mathbf{h}$ :</p>
<p>$$
\mathbf{h}=\operatorname{AvGPool}\left(\operatorname{CNN}^{\mathbf{h}}\left(\mathbf{H}_{2}\right)\right)
$$</p>
<p>where AvGPool takes averages across the dimensions.
The output of LingUNET is a tuple ( $\mathbf{H}<em 1="1">{1}, \mathbf{h}$ ), where $\mathbf{H}</em>\right)$.}$ is of size $W_{w} \times H_{w} \times 2$ and $\mathbf{h}$ is a vector of length 2. This output is used to compute two distributions, and can be increased if more distribution are predicted, such as in Suhr et al. [44]. We use an additional normalization step to produce the position visitation and goal visitation distributions given $\left(\mathbf{H}_{1}, \mathbf{h</p>
<h2>B. 2 Control Network: Action Generation and Value Function</h2>
<p>Figure 11 shows the architecture of the control network for the second action generation stage of the model. The value function architecture is identical to the action generator and also uses the control network, except that it has only a single output. The value function does not share the parameters with the action generator.
The control network takes as input the trajectory and stopping visitation distributions $d_{t}^{p}$ and $d_{t}^{g}$, as well as the observability and boundary masks $\mathbf{M}<em t="t">{t}^{W}$ and $\mathbf{B}</em>\right)$ define the probability mass outside of any observed environment location.
The visitation distributions $d_{t}^{p}$ and $d_{t}^{g}$ are first rotated to the agent's current ego-centric reference frame, concatenated along the channel dimension, and then processed with a convolutional neural network. The output is flattened into a vector. The masks $\mathbf{B}}^{W}$. The distributions $d_{t}^{p}$ and $d_{t}^{g}$ are represented as 2D square-shaped images over environment locations, where unobserved locations have a probability of zero. Additional scalars $d^{p}\left(p^{\text {sub }}\right)$ and $d^{g}\left(p^{\text {sub }<em t="t">{t}^{W}$ and $\mathbf{M}</em>\right)$ are embedded into fixed-length vectors:}^{W}$ are processed in an analogous way to the visitation distributions $d_{t}^{p}$ and $d_{t}^{g}$, and the output is also flattened into a vector. The scalars $d^{p}\left(p^{\text {sub }}\right)$ and $d^{g}\left(p^{\text {sub }</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Control network architecture.</p>
<p>$$
\begin{aligned}
\operatorname{EMBED}<em 1="1">{d^{p}\left(p^{\text {web }}\right)} &amp; =\mathbf{q}</em>} \cdot d^{q}\left(p^{\text {web }}\right)-\mathbf{q<em d_p="d^{p">{1} \cdot\left(1-d^{q}\left(p^{\text {web }}\right)\right) \
\operatorname{EMBED}</em>}\left(p^{\text {web }}\right)} &amp; =\mathbf{q<em 2="2">{2} \cdot d^{p}\left(p^{\text {web }}\right)-\mathbf{q}</em>\right)\right)
\end{aligned}
$$} \cdot\left(1-d^{p}\left(p^{\text {web }</p>
<p>where $\mathbf{q}<em _cdot_="(\cdot)">{(\cdot)}$ are random fixed-length vectors. We do not tune $\mathbf{q}</em>$.
The resulting vector representations for the visitation distributions, out-of-bounds probabilities, and masks are concatenated and processed with a three-layer multi-layer perceptron (MLP). The output are five scalars. Two of the scalars are predicted forward and angular velocities $v_{t}$ and $\omega_{t}$, one scalar is the logit of the stopping probability, and two scalars are standard deviations used during PPO training to define a Gaussian probability distribution over actions.</p>
<h1>B. 3 Coordinate Frames of Reference</h1>
<p>At the start of each task, we define the world reference frame according to the agent's starting position, with x and y axis pointing forward and left respectively, according to the agent's position. The maps are represented with the origin at the center. Throughout the instruction execution, this reference frame remains fixed. Within the first model stage, the semantic and grounding maps, observability and boundary masks, and visitation distributions are all represented in the world reference frame. At the input to second stage, we transform the visitation distributions, and observability and boundary masks to the agent's current egocentric frame of reference. This allows the model to focus on generating velocities to follow the high probability regions, without having to reason about coordinate transformations.</p>
<h2>C Additional Learning Details</h2>
<h2>C. 1 Discriminator Architecture and Training</h2>
<p>Figure 12 shows the neural network architecture of our discriminator $h$. The Wasserstein distance estimation procedure from Shen et al. [40] requires a discriminator that is K-Lipschitz continuous. We guarantee that our discriminator meets this requirement by clamping the discriminator parameters $\psi$ to a range of $\left[-T_{\psi} ; T_{\psi}\right]$ after every gradient update [40].</p>
<h2>C. 2 Return Definition</h2>
<p>The expected return $R_{t}(\hat{\Xi})$ is defined as:</p>
<p>$$
R_{t}(\hat{\Xi})=\sum_{i \geq t,\left(s_{i}, a_{i}\right) \in \hat{\Xi},} \gamma^{i-t} r\left(c_{i}, a_{i}\right)
$$</p>
<p>where $\hat{\Xi}$ is a policy execution, $\mathcal{C}\left(s_{i}\right)$ is the agent context observed at state $s_{i}, \gamma$ is a discount factor, and $r(\cdot)$ is the intrinsic reward. The reward does not depend on any external state information, but only on the agent context and visitation predictions.</p>
<h2>C. 3 Reward Function</h2>
<p>Section 5 provides the high level description and motivation of the intrinsic reward function.
Visitation Reward We design the visitation reward to reward policy executions $\hat{\Xi}$ that closely match the predicted visitation distribution $d^{p}$. An obvious choice would be the probability of the trajectory under independence assumptions $P(\hat{\Xi}) \approx \prod_{p \in \hat{\Xi}} d_{t}^{p}(p)$. According to this measurement, if $d_{t}^{p}(p)=0$ for any $p \in \hat{\Xi}$, then $P(\hat{\Xi})=0$. This would lead to a reward of zero as soon as the pol-</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Our discriminator architecture. The discriminator takes as input a 3D feature map with two spatial dimensions and one feature dimension. It processes the feature map with a cascade of four convolutional neural networks with LeakyReLU non-linearities, before processing the output with a linear neural network layer. The discriminator is trained to output a scalar score that assigns high values to feature maps from the simulated domain, and low values from the real domain. The discriminator is used as component in our Wasserstein domain loss $\mathcal{L}<em v="v">{W}$.
icy makes a mistake, resulting in sparse rewards and slow learning. Instead, we define the visitation reward in terms of earth mover's distance that provides a smooth and dense reward. The visitation reward $r</em>$ is:</p>
<p>$$
r_{v}\left(c_{t}, a_{t}\right)=\phi_{v}\left(c_{t}, a_{t}\right)-\phi_{v}\left(c_{t-1}, a_{t-1}\right)
$$</p>
<p>where $\phi_{v}$ is a reward shaping potential:</p>
<p>$$
\phi_{v}\left(c_{t}, a_{t}\right)=-\operatorname{EMD}\left(\mathbb{1}<em t="t">{p \in \Xi}, d</em>\right)\right)
$$}^{p}\left(p_{t} \mid p_{t} \in \mathcal{P}^{\text {obs }</p>
<p>EMD is the earth mover's distance in Euclidean $\mathbb{R}^{2}$ space, $\mathbb{1}<em t="t">{p \in \Xi}$ is a probability distribution that assigns equal probability to all positions visited thus far, $\mathcal{P}^{\text {obs }}$ is the set of positions the agent has observed so far, ${ }^{2}$ and $d</em>$ rewards per-timestep reduction in earth mover's distance between the predicted visitation distribution and the empirical distribution derived from the agent's trajectory.
Stop Reward Similar to $r_{v}$, the stop reward $r_{s}$ is the negative earth mover's distance between the conditional predicted goal distribution over all observed environment locations, and the empirical stop distribution $\mathbb{1}}^{p}\left(p_{t} \mid p_{t} \in \mathcal{P}^{\text {obs }}\right)$ is the position visitation distribution over all observed positions. Intuitively, $r_{v<em -1="-1">{p=\bar{\Xi}</em>$ that assigns unit probability to the agent's final stopping position.}</p>
<p>$$
r_{s}\left(c_{t}, a_{t}\right)=-\mathbb{1}<em t="t">{a</em>}=\operatorname{STOP}} \cdot \operatorname{EMD}\left(\mathbb{1<em -1="-1">{p=\bar{\Xi}</em>\right)\right)
$$}}, d_{t}^{g}\left(p_{t} \mid p_{t} \in \mathcal{P}^{\text {obs }</p>
<p>Exploration Reward The exploration reward $r_{e}$ is:</p>
<p>$$
r_{e}\left(c_{t}, a_{t}\right)=\left(\phi_{e}\left(c_{t}, a_{t}\right)-\phi_{e}\left(c_{t-1}, a_{t-1}\right)\right)-\mathbb{1}<em t="t">{a</em>\right)
$$}=\operatorname{STOP}} \cdot d_{t}^{g}\left(p^{\text {sob }</p>
<p>where:</p>
<p>$$
\phi_{e}\left(c_{t}, a_{t}\right)=\max <em t_prime="t^{\prime">{t^{\prime}&lt;t}\left[1-d</em>\right)\right]
$$}}^{g}\left(p^{\text {sob }</p>
<p>The term $\phi_{e}$ reflects the agent's belief that it has observed the goal location $p_{g} .1-d_{t^{\prime}}^{g}\left(p^{\text {sob }}\right)$ is the probability that the goal has been observed before time $t^{\prime}$. We take the maximum over past timesteps to reduce effects of noise from the model output. The second term in Equation 2 penalizes the agent for stopping while it predicts that the goal is not yet observed.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C. 4 Auxiliary Objectives</h1>
<p>During training, we add an additional auxiliary loss $\mathcal{L}<em L="L" S="S">{\text {aux }}$ to the supervised learning loss $\mathcal{L}</em>$ to ensure that the different modules in the PVN model specialize according to their function. The auxiliary loss is:</p>
<p>$$
\mathcal{L}<em t="t">{\text {aux }}\left(c</em>}\right)=\mathcal{L<em t="t">{\text {percept }}\left(c</em>}\right)+\mathcal{L<em t="t">{\text {ground }}\left(c</em>}\right)+\mathcal{L<em t="t">{\text {lang }}\left(c</em>\right)
$$</p>
<p>The text in the remainder of Section C. 4 has been taken from Blukis et al. [16]. We present it here for reference and completeness.</p>
<p>Object Recognition Loss The object-recognition loss $\mathcal{L}<em t="t">{\text {percept }}$ ensures the semantic map $\mathbf{S}</em>$ corresponding to the object location in the world. We use a linear softmax classifier to predict the object identity given the feature vector. At a given timestep $t$ the classifier loss is:}^{W}$ stores information about locations and identities of objects. At timestep $t$, for every object $o$ that is visible in the first person image $I_{t}$, we classify the feature vector in the position in the semantic map $\mathbf{S}_{t}^{W</p>
<p>$$
\mathcal{L}<em 1="1">{\text {percept }}\left(\theta</em>}\right)=\frac{-1}{\left|O_{\mathrm{FPV}}\right|} \sum_{o \in O_{\mathrm{FPV}}}\left[\hat{y<em o="o">{o} \log \left(y</em>\right)\right]
$$</p>
<p>where $\hat{y}<em o="o">{o}$ is the true class label of the object $o$ and $y</em>$.
Grounding Loss For every object $o$ visible in the first-person image $I_{t}$, we use the feature vector from the grounding map $\mathbf{R}_{t}^{W}$ corresponding to the object location in the world with a linear softmax classifier to predict whether the object was mentioned in the instruction $u$. The objective is:}$ is the predicted probability. $O_{\mathrm{FPV}}$ is the set of objects visible in the image $I_{t</p>
<p>$$
\mathcal{L}<em 1="1">{\text {ground }}\left(\theta</em>}\right)=\frac{-1}{\left|O_{\mathrm{FPV}}\right|} \sum_{o \in O_{\mathrm{FPV}}}\left[\hat{y<em o="o">{o} \log \left(y</em>}\right)+\left(1-\hat{y<em o="o">{o}\right) \log \left(1-y</em>\right)\right]
$$</p>
<p>where $\hat{y}<em o="o">{o}$ is a $0 / 1$-valued label indicating whether the object o was mentioned in the instruction and $y</em>$.
Language Loss The instruction-mention auxiliary objective uses a similar classifier to the grounding loss. Given the instruction embedding $\mathbf{u}$, we predict for each of the possible objects whether it was mentioned in the instruction $u$. The objective is:}$ is the corresponding model prediction. $O_{\mathrm{FPV}}$ is the set of objects visible in the image $I_{t</p>
<p>$$
\mathcal{L}<em 1="1">{\text {lang }}\left(\theta</em>}\right)=\frac{-1}{|O|} \sum_{o \in O_{\mathrm{FPV}}}\left[\hat{y<em o="o">{o} \log \left(y</em>}\right)+\left(1-\hat{y<em o="o">{o}\right) \log \left(1-y</em>\right)\right]
$$</p>
<p>where $\hat{y}_{o}$ is a $0 / 1$-valued label, same as above.
Automatic Word-object Alignment Extraction In order to infer whether an object $o$ was mentioned in the instruction $u$, we use automatically extracted word-object alignments from the dataset. Let $E(o)$ be the event that an object $o$ occurs within 15 meters of the human-demonstration trajectory $\Xi$, let $E(\tau)$ be the event that a word type $\tau$ occurs in the instruction $u$, and let $E(o, \tau)$ be the event that both $E(o)$ and $E(\tau)$ occur simultaneously. The pointwise mutual information between events $E(o)$ and $E(\tau)$ over the training set is:</p>
<p>$$
\operatorname{PMI}(o, \tau)=P(E(o, \tau)) \log \frac{P(E(o, \tau))}{P(E(o)) P(E(\tau))}
$$</p>
<p>where the probabilities are estimated from counts over training examples $\left{\left(u^{(i)}, s_{1}^{(i)}, \Xi^{(i)}\right)\right}_{i=1}^{N}$. The output set of word-object alignments is:</p>
<p>$$
\left{(o, \tau) \mid \operatorname{PMI}(o, \tau)&gt;T_{\mathrm{PMI}} \wedge P(\tau)&lt;T_{\tau}\right}
$$</p>
<p>where $T_{P M I}=0.008$ and $T_{\tau}=0.1$ are threshold hyperparameters.</p>
<h2>D Dataset Details</h2>
<p>Natural Language and Demonstration Data Table 2 provides statistics on the natural language instruction datasets.</p>
<p>LANI Dataset Collection Details The crowdsourcing process includes two stages. First, a Mechanical Turk worker is shown a long, random trajectory in the overhead view and writes an instruction paragraph for a first-person agent. The trajectories were generated with a sampling process biased towards moving around the landmarks. Given the instruction paragraph, a second worker follows the instructions by controlling a discrete simple agent, simultaneously segmenting the instruction and trajectory into shorter segments. The output are pairs of instruction segments and discrete</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Split</th>
<th style="text-align: center;"># Paragraphs</th>
<th style="text-align: center;"># Instr.</th>
<th style="text-align: center;">Avg. Instr. Len. (tokens)</th>
<th style="text-align: center;">Avg. Path Len. (m)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LANI</td>
<td style="text-align: center;">1-segment</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">4200</td>
<td style="text-align: center;">19762</td>
<td style="text-align: center;">11.04</td>
<td style="text-align: center;">1.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">898</td>
<td style="text-align: center;">4182</td>
<td style="text-align: center;">10.94</td>
<td style="text-align: center;">1.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">902</td>
<td style="text-align: center;">4260</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">1.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-segment</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">4200</td>
<td style="text-align: center;">15919</td>
<td style="text-align: center;">21.84</td>
<td style="text-align: center;">3.07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">898</td>
<td style="text-align: center;">3366</td>
<td style="text-align: center;">21.65</td>
<td style="text-align: center;">3.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">902</td>
<td style="text-align: center;">3432</td>
<td style="text-align: center;">22.26</td>
<td style="text-align: center;">3.07</td>
</tr>
<tr>
<td style="text-align: center;">REAL</td>
<td style="text-align: center;">1-segment</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">698</td>
<td style="text-align: center;">3245</td>
<td style="text-align: center;">11.10</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">640</td>
<td style="text-align: center;">11.47</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">672</td>
<td style="text-align: center;">11.31</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-segment</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">698</td>
<td style="text-align: center;">2582</td>
<td style="text-align: center;">20.97</td>
<td style="text-align: center;">1.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">501</td>
<td style="text-align: center;">21.42</td>
<td style="text-align: center;">2.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">531</td>
<td style="text-align: center;">21.28</td>
<td style="text-align: center;">1.99</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset and split sizes. LANI was introduced by Misra et al. [15]. Each layout in LANI consists of 6-13 landmarks out of a total of 64. REAL is the additional data we collected for use on the physical drone. In REAL, each layout has 5-8 landmarks from a set of 15 that is a subset of the landmarks in LANI.
ground-truth trajectories. We restrict to a pool of workers who had previously qualified for our other instruction-writing tasks.
Demonstration Data We collect the demonstration datasets $\mathcal{D}^{\mathbb{B}}$ and $\mathcal{D}^{\mathcal{S}}$ by executing a handengineered oracle policy that has access to the ground truth human demonstrations, and collect observation data. The oracle is described in Appendix E.2. $\mathcal{D}^{\mathcal{S}}$ includes all instructions from original LANI data, as well as the additional instructions we collect for our smaller environment. Due to the high cost of data collection on the physical quadcopter, $\mathcal{D}^{\mathbb{B}}$ includes demonstrations on only 100 paragraphs of single-segment instructions, approximately $1.5 \%$ of the data available in simulation.
Data Augmentation To improve generalization, we perform two types of data augmentation. First, we train on the combined dataset that includes both single-segment and two-segment instructions. Two-segment data consists of instructions and executions that combine two consecutive segments. This increases the mean instruction length from 11.10 tokens to 20.97 , and the mean trajectory length by a factor of 1.91 . Second, we randomly rotate the semantic map $\mathbf{S}^{W}$ and the gold visitation distributions by a random angle $\alpha \sim N(0,0.5 r a d)$ to counter the bias of always flying forward, which is especially present in the single-segment data because of how humans segmented the original paragraphs.</p>
<h1>E Additional Experimental Setup Details</h1>
<h2>E. 1 Computing hardware and training time</h2>
<p>Training took approximately three days on an Intel i9 7980X CPU with three Nvidia 1080Ti GPUs. We used one GPU for the supervised learning process, one GPU for the RL process for both gradient updates and roll-outs, and one GPU for rendering simulator visuals. We ran five simulator processes in parallel, each at 7 x real-time speed. We used a total of 400 k RL simulation rollouts.</p>
<h2>E. 2 Oracle implementation</h2>
<p>The oracle uses the ground truth trajectory, and follows it with a control rule. It adjusts its angular velocity with a P -control rule to fly towards a dynamic goal on the ground truth trajectory. The dynamic goal is always 0.5 m in front of the quadcopter's current position on the trajectory, until it overlaps the goal position. The forward speed is a constant maximum minus a multiple of angular velocity.</p>
<h2>E. 3 Environment and Quadcopter Parameters</h2>
<p>Environment Scaling The original LANI dataset includes a unique 50x50m environment for each paragraph. Each environment includes 6-13 landmarks. Because the original data is for a larger</p>
<p>environment, we scale it down to the same dimension as ours. We use the original data split, where environments are not shared between the splits.
Action Range We clip the forward velocity to the range $[0,0.7] \mathrm{m} / \mathrm{s}$ and the yaw rate to $[-1.0,1.0] \mathrm{rad} / \mathrm{s}$. During training, we give a small negative reward for sampling an action outside the intervals $[-0.5,1.7] \mathrm{m} / \mathrm{s}$ for forward velocity and $[-2.0,2.0] \mathrm{rad} / \mathrm{s}$ for yaw-rate. This reduces the chance of action predictions diverging, and empirically ensures they stay mostly within the permitted range.
Quadcopter Safety We prevent the quadcopter from crashing into environment bounds through a safety interface that modifies the controller setpoint $\rho$. The safety mechanism performs a forwardsimulation for one second and checks whether setting $\rho$ as the setpoint would cause a collision. If it would, the forward velocity is reduced, possibly to standstill, until it would no longer pose a threat. Angular velocity is left unmodified. This mechanism is only used when collecting demonstration trajectories and during test-time. No autonomous flight in the physical environment is done during learning.
First-person Camera We use a front-facing camera on the Intel Aero drone, tilted at a 15 degree pitch. The camera has a horizontal field-of-view of 84 degrees, which is less than the 90-degree horizontal FOV of the camera used in simulated experiments of Blukis et al. [16].</p>
<h1>F Evaluation Details</h1>
<h2>F. 1 Automated Evaluation Details</h2>
<p>We report two automated metrics: success rate and earth mover's distance (EMD). The success rate is the frequency of executions in which the quadcopter stopped within 0.47 m of the human demonstration stopping location. To compute EMD, we convert the trajectories executed by the quadcopter and the human demonstration trajectories to probability distributions with uniformly distributed mass across all positions on the trajectory. EMD is then the earth mover's distance between these two distributions, using Euclidean distance as the distance metric. EMD has a number of favorable properties, including: (a) taking into account the entire trajectory and not only the goal location, (b) giving partial credit for trajectories that are very close but do not overlap the human demonstrations, and (c) is smooth in that a slight change in the executed trajectory corresponds to at most a slight change in the metric.</p>
<h2>F. 2 Human Evaluation Details</h2>
<p>Navigation Instruction Quality One out of 73 navigation instructions that the majority of workers identified as unclear is excluded from the human evaluation analysis. The remaining instructions were judged by majority of workers not to contain mistakes, or issues with clarity or feasibility.
Mechanical Turk Evaluation Task Figure 13 shows the instructions given to workers for the human evaluation task. Figure 14 shows an example human evaluation task. We use the simulated environment to visualize the quadcopter behavior to the workers because it is usually simpler to observe. However, we use this interface to evaluate performance on the physical environment, and use trajectories generated in the physical environment. We avoid using language descriptions to describe objects to avoid biasing the workers, and allowing them to judge for themselves how well the instruction matches the objects. We observed that a large number of workers were not able to reliably judge the efficiency of agent behavior, since they generally considered correct behavior efficient and incorrect behavior inefficient. Therefore, we do not report efficiency scores. Figure 15 shows examples of human judgements for different instruction executions.</p>
<p>We need your help to understand how well our drone follows instructions.
Your task: Read the navigation instruction below, and watch the animation of the drone trying to follow the instruction. Then answer three questions. Consider the guidelines below:</p>
<ul>
<li>Looking around: The drone can only see what's directly in front of it as indicated by the highlighted region. Depending on the instruction, it may have to look for certain objects to understand where to go, and looking around for them is the right way to go and is efficient.</li>
<li>Bad instructions: If the instruction is unclear, impossible, or full of confusing mistakes, please indicate it by checking the checkbox. You must still answer all the questions consider if the drone's behavior was reasonable given the bad instruction.</li>
<li>Objects: The drone observes the environment from a first-person view. The numbered images illustrate how each object would look like to the drone. Consider the appearance of objects from the first-person perspective in relation to the instructions.</li>
<li>Note: Try to answer each question in isolation, focusing on the specific behavior. For example, if the drone reached the goal correctly, but took the wrong path, you should answer "Yes, perfectly" for question 2, while giving a worse score for question 1. Similarly, if the drone went straight for the goal, that would be considered "efficient" behavior, even though it may have taken the wrong path to get there</li>
<li>The drone might sometimes decide not to do anything at all, maybe because it thinks it's already where it was instructed to go. If that happens you won't see any movement in the animation.</li>
<li>The drone always "stops" at the end, even if the motion appears to end abruptly.</li>
<li>The field is surrounded by a red fence on top, white fence on the right, blue fence on the bottom, and yellow fence on the left. The colorful borders are there to help you better distinguish between these colors.</li>
</ul>
<p>Figure 13: The instructions given to the crowdsourcing workers during human evaluation.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Human evaluation task. The images on top show the various objects in the environment from a reasonable agent point of view, and numbers indicate correspondence to objects in the topdown view animation. The animation shows the trajectory that the agent took to complete the instruction. Because efficiency scores are unreliable, we do not report them.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We restrict the metric to observed locations on the map, because as discussed in Section 4, all unobserved locations are represented by a dummy location $p^{\text {sob }} \notin \mathbb{R}^{2}$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-430 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-430</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-430</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-265157609</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07759v1.pdf" target="_blank">Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems</a></p>
                <p><strong>Paper Abstract:</strong> High-level reasoning can be defined as the capability to generalize over knowledge acquired via experience, and to exhibit robust behavior in novel situations. Such form of reasoning is a basic skill in humans, who seamlessly use it in a broad spectrum of tasks, from language communication to decision making in complex situations. When it manifests itself in understanding and manipulating the everyday world of objects and their interactions, we talk about common sense or commonsense reasoning. State-of-the-art AI systems don't possess such capability: for instance, Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence; on a different level, performance degradation outside training data prevents self-driving vehicles to safely adapt to unseen scenarios, a serious and unsolved problem that limits the adoption of such technology. In this paper we propose to enable high-level reasoning in AI systems by integrating cognitive architectures with external neuro-symbolic components. We illustrate a hybrid framework centered on ACT-R and we discuss the role of generative models in recent and future applications.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e430.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e430.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT-R Neuro-Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACT-R centered Cognitive Neuro-Symbolic Reasoning System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A loosely-coupled hybrid architecture that integrates the ACT-R cognitive architecture with external symbolic knowledge resources and neural modules to support high-level, commonsense reasoning by combining production-rule procedural control, declarative memory linked to symbolic inference, and neural perception/generative components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACT-R centered Cognitive Neuro-Symbolic System</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid framework that places the ACT-R cognitive architecture at the center and couples it with two external neuro-symbolic modules: (1) a symbolic/background knowledge module composed of knowledge graphs, lexical resources, rule-bases and an inference engine linked bidirectionally to ACT-R's declarative memory; and (2) a neural module (CNNs, RNNs/LSTMs, generative models / LLMs) that supplies perceptual/imaginal data to ACT-R and is itself infused with symbolic knowledge via embedding mechanisms. The ACT-R procedural module (production rules with utility-based selection) orchestrates processing using limited-capacity buffers; the symbolic inference engine provides both asserted and inferred knowledge to declarative memory and to knowledge-infusion pipelines for the neural module. The architecture is explicitly described as loosely-coupled modular integration rather than an end-to-end differentiable single model.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit symbolic knowledge resources: background/domain Knowledge Graphs (KGs), lexical resources, rule bases (production-style rule representations held in the symbolic module), and a classical inference engine that derives asserted + inferred assertions; these symbolic artifacts are linked to ACT-R's declarative memory representation.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural control via ACT-R's production system (utility-based production selection) and neural modules including convolutional networks, recurrent/LSTM networks, and generative large language models (LLMs) used for perception, pattern extraction, generation and task decomposition. Also instance-based learning / reinforcement learning can be plugged into ACT-R for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, loosely-coupled integration along three explicit channels: (a) knowledge ↔ memory: two-way link between external symbolic module (KGs, RBs, inference engine) and ACT-R declarative memory (read/write); (b) neural ⇝ perception: neural models provide processed perceptual/imaginal inputs to ACT-R perceptual/imaginal buffers (bypassing direct environment→perceptual path); (c) knowledge ⇝ neural: knowledge-infusion via embedding mechanisms (Knowledge Graph Embedding, concatenation, non-linear mapping, attention/gating) to condition/fine-tune/prompt neural modules. The symbolic inference engine also supplies derived knowledge for both ACT-R and for pretraining/fine-tuning/prompting neural models. Integration is not described as end-to-end differentiable; instead it is an orchestration of modules with clear interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>According to the paper, emergent capabilities include: improved high-level (System-2-like) reasoning and commonsense generalization by combining symbolic inference with procedural cognitive control; robustness to out-of-distribution and contextual perturbations via knowledge-based contextualization of neural percepts; decoupling of logical/temporal/spatial inference (handled by symbolic inference engine) from procedural decision policies (handled by ACT-R productions) enabling efficiency and clearer role separation; traceability and introspection of decision steps (cognitive trace) that can be used to ground LLMs and reduce certain hallucinations; and the potential to scale cognitive models using generative models while preserving cognitive interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>No single experimental benchmark is presented for the integrated system in this paper; the proposal is motivated by tasks such as commonsense question answering, visual/contextual object recognition, temporal/spatial reasoning tasks (e.g., bAbI), activity recognition, decision support in troubleshooting, and autonomous driving robustness scenarios discussed in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claimed improved generalization and robustness relative to purely neural approaches via symbolic contextualization (e.g., reducing incorrect out-of-context labels, improving commonsense inferences), and better handling of novel situations by leveraging structured, inferred knowledge, but the paper provides conceptual arguments and references rather than quantitative OOD or compositional generalization measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability relative to black-box neural models is asserted: ACT-R's internal mechanisms (productions, buffers, declarative memory) provide explicit traces of reasoning steps that can be inspected; the symbolic inference engine yields explicit asserted and derived facts that can be presented as explanations. The paper emphasizes transparency of cognitive architectures versus LLMs' black-box nature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No empirical evaluation is provided in this paper; identified challenges include: production rules are not well-suited for heavy logical deduction (hence need for an external inference engine), manual cognitive model construction lacks scalability (motivation to use LLMs for scaling), LLMs used as orchestrators suffer from hallucinations and lack of grounding, and the framework requires careful selection of embedding/infusion mechanisms. The paper notes practical engineering challenges rather than demonstrated failure modes on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>A complementary division-of-labor theoretical stance: cognitive architecture (ACT-R) provides procedural control and a transparent computational-level model of human cognition (linked to the Standard Model of the Mind), while external symbolic inference handles logic-like deduction and neural modules provide perceptual/generalization power. The paper advocates loosely-coupled neuro-symbolic integration guided by cognitive principles rather than attempting to reduce all reasoning to subsymbolic learning (contrasting with tightly-coupled neurosymbolic approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e430.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e430.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM + Cognitive Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model augmented Cognitive Modeling (scaling and grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed modality of hybrid reasoning where large generative language models are used to (a) scale/auto-generate cognitive model code (e.g., PyACTR implementations) and (b) be prompt-engineered with cognitive-model traces to ground LLM reasoning, thereby creating a hybrid where LLMs provide flexible neural computation and cognitive artifacts provide structured procedural traces and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-augmented Cognitive Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two complementary proposals: (1) 'scaling cognitive models via LLMs' — using LLMs' code-generation abilities to auto-produce implementations of cognitive models (reducing manual effort); (2) 'prompt-engineering LLMs with cognitive models' — supplying LLMs with cognitive-model traces (stepwise introspective reasoning) as prompts or fine-tuning data so that LLM outputs are constrained by principled cognitive-decisional steps, with the aim of reducing hallucination of inferential mechanisms as well as factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>The cognitive-model traces, production-rule steps, and symbolic knowledge resources (e.g., traces of ACT-R productions, declarative memory items, KG-derived assertions) used as structured prompts or as explicit constraints on LLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Large language models and generative neural architectures (e.g., GPT-family, PaLM) used for code generation, chain-of-thought reasoning, and for producing perceptual/agent behavior; procedural cognitive models generated or augmented by these LLM outputs are then executed in ACT-R or similar architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Prompting and fine-tuning: LLMs are prompted with instructions and cognitive-model specifications to generate code or reasoning traces; chain-of-thought prompting and 'trace injection' are explicitly mentioned as mechanisms; the LLMs are not claimed to replace the cognitive architecture but to be orchestrated or constrained by it. This is a modular interaction (LLM ↔ cognitive architecture) rather than end-to-end integration.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Potential emergent benefits include rapid prototyping and scaling of cognitive models, richer and more detailed cognitive traces that can be used to constrain LLM outputs (reducing certain hallucinations), and improved plausibility/realism of simulated agents when cognitive constraints are combined with LLM-generated behaviors. The paper also cautions that LLM flexibility is limited to patterns in text data and grounding remains an issue.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>No concrete benchmark reported; discussed applications include automatic generation of PyACTR cognitive models, improving LLM performance on decision-support tasks via trace grounding, and building believable agents/simulacra in sandbox environments.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Argued to potentially improve scalability of cognitive models (alleviating manual engineering bottlenecks) and reduce some forms of LLM inference hallucination via cognitive grounding, but also noted that LLMs' generalization is constrained to distributional patterns in text and lacks true grounding—no quantitative claims provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Grounding LLMs with cognitive traces is proposed to increase interpretability by providing explicit stepwise reasoning that can be inspected; cognitive-model outputs serve as an interpretable scaffold for otherwise opaque LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Identified limitations include LLM hallucinations, lack of grounding, inconsistent chain-of-thought reliability across model versions, and that prompting alone does not always produce consistent or correct stepwise reasoning. Also, generated cognitive models by LLMs tended to underuse ACT-R modules unless explicitly instructed.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Pragmatic complementarity: LLMs provide scalable neural generation and pattern completion, while procedural cognitive models provide transparent constraints and structured traces; the combination aims to exploit strengths of both (scalability vs. interpretability/grounding) rather than subsuming one within the other.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e430.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e430.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGE-based Knowledge-Infusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Embedding for Knowledge-Infusion into Neural Modules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of methods referenced for transforming symbolic knowledge (KG triples) into dense sub-symbolic vectors (KGE) which are then infused into neural models via concatenation, non-linear mapping, attention-like and gating mechanisms to contextualize neural perception and language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-Infused Neural Modules via KGE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a common neuro-symbolic practice: KGs are embedded into low-dimensional vector spaces (via geometric, tensor, or deep learning KGE methods) to produce dense representations that augment neural models. Integration approaches include concatenation of embeddings with neural inputs, non-linear mapping layers, attention-conditioning with KG vectors, and gating mechanisms to modulate neural activations; these infused neural modules then feed ACT-R perceptual/imaginal buffers or are used for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Knowledge Graphs (CONCEPTNET and other domain/background KGs), triples and explicit asserted facts; also rule-bases and lexical resources as structured symbolic inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural embedding models and downstream neural architectures (transformers, CNNs, RNN/LSTM) that accept infused KG embeddings as additional inputs or conditioning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Knowledge embedding followed by infusion into neural models using concatenation, non-linear mapping, attention-based conditioning, and gating; the symbolic module may be read to produce embeddings and/or to produce additional derived facts via an inference engine that are also embedded. This is a modular conditioning/feature-injection approach rather than joint differentiable symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Reported/claimed benefits include improved disambiguation in language tasks, contextual robustness in visual classification (e.g., reducing out-of-context mislabels), and better labeling of unseen entities in domains like autonomous driving when KGE-based contextual signals are combined with neural perception.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Discussed in relation to image classification contextualization, entity labeling in autonomous driving datasets, and improvements in neural language models for disambiguation/QA tasks (citations given to task-specific works but no experimental numbers provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claimed to improve robustness to test-time distribution shifts and to help neural models generalize better in contextualized recognition tasks, but the paper only references prior empirical works rather than reporting new quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides some interpretability benefits by making symbolic content influence neural activations; however, once embedded the symbolic facts become sub-symbolic and less directly interpretable unless mapped back to symbolic form. The paper emphasizes that infusion alone does not endow procedural knowledge about how to reason with the knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Embedding compresses symbolic structure into dense vectors, which can lose explicit reasoning mechanisms and does not by itself instruct neural models on reasoning procedures; thus KGE infusion improves contextual signals but is insufficient by itself for high-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Practical knowledge-infusion rationale: symbolic knowledge supplies structured priors and contextual constraints that, when embedded, act as additional conditioning features for neural learners—complementary strengths rather than a unified symbolic/neural reasoning theory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural-symbolic learning and reasoning: A survey and interpretation <em>(Rating: 2)</em></li>
                <li>Neurosymbolic AI: The 3rd wave <em>(Rating: 2)</em></li>
                <li>Knowledge Graph Embedding: A Survey of Approaches and Applications <em>(Rating: 2)</em></li>
                <li>The more you know: Using knowledge graphs for image classification <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>PALM: Scaling language modeling with pathways <em>(Rating: 1)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 1)</em></li>
                <li>Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-430",
    "paper_id": "paper-265157609",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "ACT-R Neuro-Symbolic",
            "name_full": "ACT-R centered Cognitive Neuro-Symbolic Reasoning System",
            "brief_description": "A loosely-coupled hybrid architecture that integrates the ACT-R cognitive architecture with external symbolic knowledge resources and neural modules to support high-level, commonsense reasoning by combining production-rule procedural control, declarative memory linked to symbolic inference, and neural perception/generative components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ACT-R centered Cognitive Neuro-Symbolic System",
            "system_description": "A hybrid framework that places the ACT-R cognitive architecture at the center and couples it with two external neuro-symbolic modules: (1) a symbolic/background knowledge module composed of knowledge graphs, lexical resources, rule-bases and an inference engine linked bidirectionally to ACT-R's declarative memory; and (2) a neural module (CNNs, RNNs/LSTMs, generative models / LLMs) that supplies perceptual/imaginal data to ACT-R and is itself infused with symbolic knowledge via embedding mechanisms. The ACT-R procedural module (production rules with utility-based selection) orchestrates processing using limited-capacity buffers; the symbolic inference engine provides both asserted and inferred knowledge to declarative memory and to knowledge-infusion pipelines for the neural module. The architecture is explicitly described as loosely-coupled modular integration rather than an end-to-end differentiable single model.",
            "declarative_component": "Explicit symbolic knowledge resources: background/domain Knowledge Graphs (KGs), lexical resources, rule bases (production-style rule representations held in the symbolic module), and a classical inference engine that derives asserted + inferred assertions; these symbolic artifacts are linked to ACT-R's declarative memory representation.",
            "imperative_component": "Procedural control via ACT-R's production system (utility-based production selection) and neural modules including convolutional networks, recurrent/LSTM networks, and generative large language models (LLMs) used for perception, pattern extraction, generation and task decomposition. Also instance-based learning / reinforcement learning can be plugged into ACT-R for learning.",
            "integration_method": "Modular, loosely-coupled integration along three explicit channels: (a) knowledge ↔ memory: two-way link between external symbolic module (KGs, RBs, inference engine) and ACT-R declarative memory (read/write); (b) neural ⇝ perception: neural models provide processed perceptual/imaginal inputs to ACT-R perceptual/imaginal buffers (bypassing direct environment→perceptual path); (c) knowledge ⇝ neural: knowledge-infusion via embedding mechanisms (Knowledge Graph Embedding, concatenation, non-linear mapping, attention/gating) to condition/fine-tune/prompt neural modules. The symbolic inference engine also supplies derived knowledge for both ACT-R and for pretraining/fine-tuning/prompting neural models. Integration is not described as end-to-end differentiable; instead it is an orchestration of modules with clear interfaces.",
            "emergent_properties": "According to the paper, emergent capabilities include: improved high-level (System-2-like) reasoning and commonsense generalization by combining symbolic inference with procedural cognitive control; robustness to out-of-distribution and contextual perturbations via knowledge-based contextualization of neural percepts; decoupling of logical/temporal/spatial inference (handled by symbolic inference engine) from procedural decision policies (handled by ACT-R productions) enabling efficiency and clearer role separation; traceability and introspection of decision steps (cognitive trace) that can be used to ground LLMs and reduce certain hallucinations; and the potential to scale cognitive models using generative models while preserving cognitive interpretability.",
            "task_or_benchmark": "No single experimental benchmark is presented for the integrated system in this paper; the proposal is motivated by tasks such as commonsense question answering, visual/contextual object recognition, temporal/spatial reasoning tasks (e.g., bAbI), activity recognition, decision support in troubleshooting, and autonomous driving robustness scenarios discussed in the text.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claimed improved generalization and robustness relative to purely neural approaches via symbolic contextualization (e.g., reducing incorrect out-of-context labels, improving commonsense inferences), and better handling of novel situations by leveraging structured, inferred knowledge, but the paper provides conceptual arguments and references rather than quantitative OOD or compositional generalization measurements.",
            "interpretability_properties": "High interpretability relative to black-box neural models is asserted: ACT-R's internal mechanisms (productions, buffers, declarative memory) provide explicit traces of reasoning steps that can be inspected; the symbolic inference engine yields explicit asserted and derived facts that can be presented as explanations. The paper emphasizes transparency of cognitive architectures versus LLMs' black-box nature.",
            "limitations_or_failures": "No empirical evaluation is provided in this paper; identified challenges include: production rules are not well-suited for heavy logical deduction (hence need for an external inference engine), manual cognitive model construction lacks scalability (motivation to use LLMs for scaling), LLMs used as orchestrators suffer from hallucinations and lack of grounding, and the framework requires careful selection of embedding/infusion mechanisms. The paper notes practical engineering challenges rather than demonstrated failure modes on benchmarks.",
            "theoretical_framework": "A complementary division-of-labor theoretical stance: cognitive architecture (ACT-R) provides procedural control and a transparent computational-level model of human cognition (linked to the Standard Model of the Mind), while external symbolic inference handles logic-like deduction and neural modules provide perceptual/generalization power. The paper advocates loosely-coupled neuro-symbolic integration guided by cognitive principles rather than attempting to reduce all reasoning to subsymbolic learning (contrasting with tightly-coupled neurosymbolic approaches).",
            "uuid": "e430.0",
            "source_info": {
                "paper_title": "Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM + Cognitive Models",
            "name_full": "Large Language Model augmented Cognitive Modeling (scaling and grounding)",
            "brief_description": "A proposed modality of hybrid reasoning where large generative language models are used to (a) scale/auto-generate cognitive model code (e.g., PyACTR implementations) and (b) be prompt-engineered with cognitive-model traces to ground LLM reasoning, thereby creating a hybrid where LLMs provide flexible neural computation and cognitive artifacts provide structured procedural traces and constraints.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "LLM-augmented Cognitive Modeling",
            "system_description": "Two complementary proposals: (1) 'scaling cognitive models via LLMs' — using LLMs' code-generation abilities to auto-produce implementations of cognitive models (reducing manual effort); (2) 'prompt-engineering LLMs with cognitive models' — supplying LLMs with cognitive-model traces (stepwise introspective reasoning) as prompts or fine-tuning data so that LLM outputs are constrained by principled cognitive-decisional steps, with the aim of reducing hallucination of inferential mechanisms as well as factual errors.",
            "declarative_component": "The cognitive-model traces, production-rule steps, and symbolic knowledge resources (e.g., traces of ACT-R productions, declarative memory items, KG-derived assertions) used as structured prompts or as explicit constraints on LLM behavior.",
            "imperative_component": "Large language models and generative neural architectures (e.g., GPT-family, PaLM) used for code generation, chain-of-thought reasoning, and for producing perceptual/agent behavior; procedural cognitive models generated or augmented by these LLM outputs are then executed in ACT-R or similar architectures.",
            "integration_method": "Prompting and fine-tuning: LLMs are prompted with instructions and cognitive-model specifications to generate code or reasoning traces; chain-of-thought prompting and 'trace injection' are explicitly mentioned as mechanisms; the LLMs are not claimed to replace the cognitive architecture but to be orchestrated or constrained by it. This is a modular interaction (LLM ↔ cognitive architecture) rather than end-to-end integration.",
            "emergent_properties": "Potential emergent benefits include rapid prototyping and scaling of cognitive models, richer and more detailed cognitive traces that can be used to constrain LLM outputs (reducing certain hallucinations), and improved plausibility/realism of simulated agents when cognitive constraints are combined with LLM-generated behaviors. The paper also cautions that LLM flexibility is limited to patterns in text data and grounding remains an issue.",
            "task_or_benchmark": "No concrete benchmark reported; discussed applications include automatic generation of PyACTR cognitive models, improving LLM performance on decision-support tasks via trace grounding, and building believable agents/simulacra in sandbox environments.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Argued to potentially improve scalability of cognitive models (alleviating manual engineering bottlenecks) and reduce some forms of LLM inference hallucination via cognitive grounding, but also noted that LLMs' generalization is constrained to distributional patterns in text and lacks true grounding—no quantitative claims provided.",
            "interpretability_properties": "Grounding LLMs with cognitive traces is proposed to increase interpretability by providing explicit stepwise reasoning that can be inspected; cognitive-model outputs serve as an interpretable scaffold for otherwise opaque LLM outputs.",
            "limitations_or_failures": "Identified limitations include LLM hallucinations, lack of grounding, inconsistent chain-of-thought reliability across model versions, and that prompting alone does not always produce consistent or correct stepwise reasoning. Also, generated cognitive models by LLMs tended to underuse ACT-R modules unless explicitly instructed.",
            "theoretical_framework": "Pragmatic complementarity: LLMs provide scalable neural generation and pattern completion, while procedural cognitive models provide transparent constraints and structured traces; the combination aims to exploit strengths of both (scalability vs. interpretability/grounding) rather than subsuming one within the other.",
            "uuid": "e430.1",
            "source_info": {
                "paper_title": "Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "KGE-based Knowledge-Infusion",
            "name_full": "Knowledge Graph Embedding for Knowledge-Infusion into Neural Modules",
            "brief_description": "A set of methods referenced for transforming symbolic knowledge (KG triples) into dense sub-symbolic vectors (KGE) which are then infused into neural models via concatenation, non-linear mapping, attention-like and gating mechanisms to contextualize neural perception and language models.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Knowledge-Infused Neural Modules via KGE",
            "system_description": "Described as a common neuro-symbolic practice: KGs are embedded into low-dimensional vector spaces (via geometric, tensor, or deep learning KGE methods) to produce dense representations that augment neural models. Integration approaches include concatenation of embeddings with neural inputs, non-linear mapping layers, attention-conditioning with KG vectors, and gating mechanisms to modulate neural activations; these infused neural modules then feed ACT-R perceptual/imaginal buffers or are used for downstream tasks.",
            "declarative_component": "Knowledge Graphs (CONCEPTNET and other domain/background KGs), triples and explicit asserted facts; also rule-bases and lexical resources as structured symbolic inputs.",
            "imperative_component": "Neural embedding models and downstream neural architectures (transformers, CNNs, RNN/LSTM) that accept infused KG embeddings as additional inputs or conditioning signals.",
            "integration_method": "Knowledge embedding followed by infusion into neural models using concatenation, non-linear mapping, attention-based conditioning, and gating; the symbolic module may be read to produce embeddings and/or to produce additional derived facts via an inference engine that are also embedded. This is a modular conditioning/feature-injection approach rather than joint differentiable symbolic reasoning.",
            "emergent_properties": "Reported/claimed benefits include improved disambiguation in language tasks, contextual robustness in visual classification (e.g., reducing out-of-context mislabels), and better labeling of unseen entities in domains like autonomous driving when KGE-based contextual signals are combined with neural perception.",
            "task_or_benchmark": "Discussed in relation to image classification contextualization, entity labeling in autonomous driving datasets, and improvements in neural language models for disambiguation/QA tasks (citations given to task-specific works but no experimental numbers provided in this paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claimed to improve robustness to test-time distribution shifts and to help neural models generalize better in contextualized recognition tasks, but the paper only references prior empirical works rather than reporting new quantitative results.",
            "interpretability_properties": "Provides some interpretability benefits by making symbolic content influence neural activations; however, once embedded the symbolic facts become sub-symbolic and less directly interpretable unless mapped back to symbolic form. The paper emphasizes that infusion alone does not endow procedural knowledge about how to reason with the knowledge.",
            "limitations_or_failures": "Embedding compresses symbolic structure into dense vectors, which can lose explicit reasoning mechanisms and does not by itself instruct neural models on reasoning procedures; thus KGE infusion improves contextual signals but is insufficient by itself for high-level reasoning.",
            "theoretical_framework": "Practical knowledge-infusion rationale: symbolic knowledge supplies structured priors and contextual constraints that, when embedded, act as additional conditioning features for neural learners—complementary strengths rather than a unified symbolic/neural reasoning theory.",
            "uuid": "e430.2",
            "source_info": {
                "paper_title": "Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural-symbolic learning and reasoning: A survey and interpretation",
            "rating": 2,
            "sanitized_title": "neuralsymbolic_learning_and_reasoning_a_survey_and_interpretation"
        },
        {
            "paper_title": "Neurosymbolic AI: The 3rd wave",
            "rating": 2,
            "sanitized_title": "neurosymbolic_ai_the_3rd_wave"
        },
        {
            "paper_title": "Knowledge Graph Embedding: A Survey of Approaches and Applications",
            "rating": 2,
            "sanitized_title": "knowledge_graph_embedding_a_survey_of_approaches_and_applications"
        },
        {
            "paper_title": "The more you know: Using knowledge graphs for image classification",
            "rating": 2,
            "sanitized_title": "the_more_you_know_using_knowledge_graphs_for_image_classification"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "PALM: Scaling language modeling with pathways",
            "rating": 1,
            "sanitized_title": "palm_scaling_language_modeling_with_pathways"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering",
            "rating": 2,
            "sanitized_title": "towards_generalizable_neurosymbolic_systems_for_commonsense_question_answering"
        }
    ],
    "cost": 0.012495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems</p>
<p>Alessandro Oltramari alessandro.oltramari@us.bosch.com 
Bosch Center for Artificial Intelligence
2555 Smallman Street, Suite 302 Pittsburgh15222PennsylvaniaUSA</p>
<p>Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems
EA2D1FEEDE42F02A10E63F39C85D3456
High-level reasoning can be defined as the capability to generalize over knowledge acquired via experience, and to exhibit robust behavior in novel situations.Such form of reasoning is a basic skill in humans, who seamlessly use it in a broad spectrum of tasks, from language communication to decision making in complex situations.When it manifests itself in understanding and manipulating the everyday world of objects and their interactions, we talk about common sense or commonsense reasoning.State-of-the-art AI systems don't possess such capability: for instance, Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence; on a different level, performance degradation outside training data prevents self-driving vehicles to safely adapt to unseen scenarios, a serious and unsolved problem that limits the adoption of such technology.In this paper we propose to enable high-level reasoning in AI systems by integrating cognitive architectures with external neuro-symbolic components.We illustrate a hybrid framework centered on ACT-R, and we discuss the role of generative models in recent and future applications.</p>
<p>Introduction</p>
<p>A large part of neuro-symbolic systems is based on transforming symbolic knowledge into sub-symbolic representations that are suitable for infusion in data-driven learning algorithms: Knowledge Graph Embedding (KGE), among the others, is a prominent approach to reduce knowledge graph (KG) triples to latent vectors (Wang et al. 2017).Such transformation is instrumental to efficient computability of KG properties, as well as to application in a variety of downstream tasks: for instance, in (Wickramarachchi, Henson, and Sheth 2023) the authors leverage KGE methods to label unseen entities in autonomous driving datasets.Whether the KGE process is realized by geometric, tensor or deep learning models, the purpose is to compress KG structures into a low-dimensional space, where symbolic statements are replaced with dense, sub-symbolic expressions.Concatenation, non-linear mapping, attention-like mechanisms, gating mechanisms, are further methods to adapt knowledge structures to neural computations -e.g., (Peters et al. 2017;Strub et al. 2018;Margatina, Baziotis, and Potamianos 2019).While knowledge-infusion can improve neural models, it is not sufficient to enable high-level reasoning, which is typically required by complex tasks such as natural language understanding, activity recognition, decision making in complex scenarios: latent, sub-symbolic expressions can only augment training signals with features derived from explicit semantic content, but this infusion process does neither carry any information about the reasoning mechanisms needed to process the learned knowledge, nor instruct the neural models on how those should unfold.</p>
<p>But, what do we mean with high-level reasoning and why is it important to endow artificial intelligent systems with such feature?</p>
<p>Problem statement</p>
<p>We can define high-level reasoning as the capability to generalize over knowledge acquired via direct or mediated experience, and to exhibit robust behavior in novel situations.This definition is inspired by Kahneman's SYSTEM 2 mode of thought (Kahneman 2011).When high-level reasoning manifests itself in understanding and manipulating the everyday world of objects and their interactions, we talk about common sense or commonsense reasoning.State-of-the-art AI systems don't possess such capability: for instance, Large Language Models (LLMs) have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence (see next section); on a different level, one of the motivations why the promise of autonomous cars hasn't panned out yet concerns performance degradation outside training data, which prevents self-driving vehicles to safely adapt to unseen scenarios. 1Humans, on the opposite, are very good at generalizing from a few examples, and at filling the gaps in experience with reasoning: for in-stance, when asked about what happens after a bottle of red wine is thrown against a concrete wall, even children can answer with the utmost certainty that the bottle will shatter and the wall will be wet and red-stained -they can also easily infer that the impact between any fragile material and any hard surface typically ends with the former being substantially altered, if not destroyed; analogously, student drivers only need limited training to learn how to safely maneuver a car, adapting their knowledge and skills to novel situations.Compared to current AI systems based on GPU accelerated computing, human reasoning capabilities are impressive, even more so when we factor in what Herbert Simon used to call 'bounded rationality' (Simon 1955), i.e., the notion that human cognition operates with limited knowledge and is subject to time constraints -a heritage of evolution (Santos and Rosati 2015).As these arguments suggest, a cognitive stance toward designing AI systems (Lieto 2021) seems to be key to enable high-level reasoning capabilities at the computational level: accordingly, we propose to complement cognitive architectures (Kotseruba and Tsotsos 2020;Langley, Laird, and Rogers 2009) with neuro-symbolic methods.In this paper we illustrate the blueprints of a cognitive neuro-symbolic reasoning system centered on the ACT-R2 cognitive architecture (Anderson 1996), whose hybrid (symbolic and subsymbolic) mechanisms are well-suited for integration with neuro-symbolic algorithms and resources.Note that the proposed approach is applicable to any cognitive architecture whose properties are compatible with ACT-R, such as SOAR (Laird 2019) and SIGMA (Rosenbloom, Demski, and Ustun 2016): in fact, these three architectures have been grouped into the so-called 'Standard Model of the Mind' (Laird, Lebiere, and Rosenbloom 2017), an idea that has its roots in physics. 3Note that the Standard Model of the Mind doesn't prescribe how to implement cognitively-inspired AI systems; rather, it aims to play the role of a conceptual framework of reference for developing them.</p>
<p>Motivations</p>
<p>Over the last decade, deep learning has yielded tremendous advancements in many AI fields, such as computer vision.For instance, neural models can achieve high accuracy in object detection when training and testing domains originate from the same data distribution.However, recent work shows that minimal/regional modifications implanted in the data at test time cause significant drop in accuracy (Eykholt et al. 2018;Rosenfeld, Zemel, and Tsotsos 2018).The examples documented in (Rosenfeld, Zemel, and Tsotsos 2018) are of particular interest, as they indicate how commonsense contextualization, by means of incorporating a priori structured knowledge into deep networks, can mitigate the effect of those perturbations, resulting in more robust performance (Marino, Salakhutdinov, and Gupta 2016).In general, a visual model suitably infused with knowledge extracted from semantic resources like CONCEPTNET (Speer, Chin, and Havasi 2017) can strengthen the connections holding within instances of the same conceptual domain (e.g., couch, television, table, lamp are located in living rooms) and discard out-of-context interpretations (e.g., no real elephants are located in living rooms, but photographs of elephant may be -figure 1 depicts such case).</p>
<p>When shifting to natural language, and to tasks like automated question answering, the key role played by knowledge-based contextualization for neural language models stands evident. 4For instance, it has been demonstrated that using KG triples to disambiguate textual elements in a sentence, and embed the corresponding concepts and relations in neural language models (Devlin et al. 2018), significantly improves performance (Ma et al. 2021).In fact, despite of the impressive results that LLMs are producing in Natural Language Processing (Ma et al. 2019;Bauer and Bansal 2021;Shwartz et al. 2020), basic reasoning capabilities are still largely missing.This is also the reason why it's not appropriate to use 'Natural Language Understanding' to denote these tasks, because it would entail that robust and comprehensive reasoning capabilities are present (McShane 2017).Let's expand on this argument and consider a few representative examples.In ProtoQA (Boratko et al. 2020), GPT-2 (Dale 2021) fails to select options like 'pumpkin', 'cauliflower', 'cabbage' as top candidates, for the question 'one vegetable that is about as big as your head is?': instead, 'broccoli', 'cucumber', 'beet', 'carrot' are predicted.In this case, the different models learn some essential properties of vegetables from the training data, but do not seem to acquire the capability of comparing their size to that of other types of objects, revealing a substantial lack of analogical reasoning (Ushio et al. 2021).The same issues are observed when CHATGPT, a recent popular version of GPT-3 optimized for conversations, is considered: the main difference is that CHATGPT is capable of generating plausible answers when the question is submitted literally, but often fails to do so when the verbal expression 'about as big as' is paraphrased with alternative forms like 'about the same size', 'about the same shape', 'comparable to', etc.This 'hypersensitivity' to surface-level linguistic features -an epiphenomenon of the model's incapability to generalize over textual variations of the same content -seem to indicate that the model cannot perform the necessary (analogical) reasoning steps needed to correctly answer to the question.Along these lines, recent work (Ettinger 2020) has shown that lack of complex inferences, role-based event prediction, and understanding the conceptual impact of negation, are some of the weaknesses diagnosed when BERT (Devlin et al. 2018), one of promi-Figure 1: The Elephant in the Room: the probability that a label assigned by an object detection system is correct increases when the context is factored in: in this example, the label 'elephant' could plausibly denote a picture of the pachyderm, but not the pachyderm itself.</p>
<p>nent open source language models, is applied to benchmark datasets.ProtoQA again provides good examples of these deficiencies: in general, neural models struggle to correctly interpret the scope of modifiers like 'not' (reasoning under negation), 'often' and 'seldom' (temporal reasoning).Regarding the latter, in task 14 of bAbI (Weston et al. 2015), a comprehensive benchmark challenge designed by Facebook Research, neural language systems exhibit variable accuracy in grasping temporal ordering entailed by prepositions like 'before' and 'after'.Similarly, in bAbI task 17, which concerns spatial reasoning, LLM-based systems fail to infer basic positional information that require interpreting the semantics of 'to the left/right of', 'above/below', etc.If such systems are inaccurate when dealing with common characteristics of the physical world, their performance doesn't improve when sentiments are considered: for instance, in SocialIQA (Sap et al. 2019), given a context like 'in the school play, Robin played a hero in the struggle to death with the angry villain', models are unable to consistently select 'hopeful that Robin will succeed' over 'sorry for the villain' when required to pick the correct answer to 'how would others feel afterwards?'.It's not surprising that reasoning about emotional reactions represents a difficult task for pure learning systems, when we consider that such form of inference is deeply rooted in the sphere of human experiences and social life, which involves a 'layered' understanding of mental attitudes, intentions, motivations, emotions, and of the events that trigger them.The qualitative analysis presented above suggests that neural models struggle to perform well in tasks that require highlevel reasoning.But, are neuro-symbolic approaches sufficient to overcome these limitation?Latent expressions can augment training signals with sub-symbolic features derived from explicit semantic content, but knowledge infusion per se doesn't determine how inference processes are conducted.Relevant work in this space shows how deep neural models can replicate logical reasoning (Ebrahimi, Eberhart, and Hitzler 2021; Garcez et al. 2022), but it doesn't follow that any form of logical reasoning that is provably reducible to learning algorithms, should also be systematically reduced to it -this would be a requirement only for tightly-coupled neurosymbolic systems (Kautz 2022; Garcez and Lamb 2023).Accordingly, in the next section we make the case for developing an AI framework where the ACT-R architecture is loosely-coupled with neuro-symbolic components, to enable high-level reasoning.</p>
<p>Method</p>
<p>Cognitive architectures attempt to capture at the computational level the invariant mechanisms of human cognition, including those underlying the functions of control, learning, memory, adaptivity, perception and action.ACT-R (Anderson 1996), in particular, is designed as a hybrid modular framework including perceptual, motor and memory components, synchronized by a procedural module through limited capacity buffers.Over the years, ACT-R has accounted for a broad range of tasks at a high level of fidelity, reproducing aspects of complex human behavior, from everyday activities like event planning (Somers, Oltramari, and Lebiere 2020) and car driving (Cina and Rad 2023), to highly technical tasks such as piloting an airplane (Chen et al. 2021), and monitoring a network to prevent cyber-attacks (Ben-Asher et al. 2015).ACT-R has been used as a component in pipelines that include learning algorithms (e.g., biologicallyinspired neural networks (Jilk et al. 2008)) and external semantic resources (e.g., (Oltramari and Lebiere 2012;Emond 2006)): along this line of research, we claim that integrating ACT-R -or any compatible cognitive architecture -with neuro-symbolic components is instrumental to enable highlevel machine reasoning.</p>
<p>Figure 2 provides a compact visualization of our proposed framework: the boxes in blue, enclosed in the grey rectangle, represent the default components of ACT-R, those in green the neuro-symbolic extensions.</p>
<p>The integration would occur along three main directions:</p>
<p>• knowledge ↭ memory: the external symbolic module, which can include background/domain knowledge graphs (KG), lexical resources (LR), rule bases (RB), and a suitable inference engine, is linked to the declarative memory.This is a two-way integration: the symbolic module can be read or written by ACT-R, where the latter operation is triggered when populating or pruning world knowledge is needed as part of task execution.• neural ⇝ perception: the neural module, which can include convolutional, recurrent, long-short-term memory networks, generative models, etc., is trained, fine-tuned, or prompted with data processed from the environment, providing relevant patterns of information to the perceptual or imaginal module.This integration bypasses the direct connection holding -in standard ACT-R -between the perceptual module and the environment.5• knowledge ⇝ neural: adequately-selected embedding mechanisms govern knowledge-infusion in the neural module, enabling knowledge-based contextualization of patterns of information distilled from the environment, which are subsequently channeled into ACT-R buffers.</p>
<p>If the mutual connections between the two intertwined neuro-symbolic modules and ACT-R can be used to combine rich semantic contents with scalable learning functionalities, they don't per se bring about high-level reasoning: this capability also requires two features of the integrated framework, namely the cognitive architecture's own procedural module and a proper inference engine in the external symbolic module.The procedural module matches the content of the other module buffers and coordinates their activity using produc-tion rules, which are 'condition-action' pairs tied to the task at hand.Productions use an utility-based computation to select, from a set of task-specific plausible rules, the single rule that is executed at any point in time.For instance, when building a recommendation system to support a mechanic in troubleshooting a car engine, a relevant situation that needs to be covered is a vehicle that doesn't start but has power; in this example, a high-utility production rule should capture the following heuristic: if the engine holds compression well, and the fuel system is working correctly, then the spark plugs should be checked.The variables in these rule conditions would need to be filled with actual empirical observations and measurements, as it is often the case when cognitive architectures are applied in real-world scenarios: in our example, such evidence could be actually gathered by a real technician using the recommendation system in a humanmachine-teaming fashion, a type of approach that falls under the 'cognitive model as oracle' paradigm (Lebiere et al. 2022).The inference engine in the symbolic module is used to derive knowledge from assertions in the semantic resource of reference, a well-known feature of symbolic AI systems.What is important to stress here, is that -in our proposalthis form of logic-based reasoning would realize two functions: 1) provide a combination of asserted and inferred knowledge that ACT-R declarative memory can process and pass to the production system; 2) support knowledgeinfusion into neural modules.The first functionality would help to decouple basic forms of reasoning, e.g.temporal and spatial 6 , from cognitive assessments performed by the pro-duction system on conditional actions.Such feature makes our proposed system efficient, as ACT-R productions are not well-suited for logical reasoning.The second functionality would allow pre-training, fine-tuning, or prompting a neuralmodel on both asserted and inferred knowledge: this can provide ACT-R perceptual model with more informative patterns than just those obtained by processing raw data.It's worth making a final consideration here: the framework introduced in this section is complemental to the body of work that investigates how neuro-symbolic systems can be leveraged to realize human-like cognitive reasoning (Garcez, Lamb, and Gabbay 2008): in our proposal, ACT-R is interfaced with neuro-symbolic components, whereas -in the approaches reviewed by Garcez et al. -neuro-symbolic frameworks are used to solve cognitive tasks.The difference lies on whether cognitive processes are considered first class citizens or not.</p>
<p>Discussion: The Role of Generative AI in Cognitive Neuro-Symbolic Reasoning</p>
<p>As seen in the previous section, our proposed framework doesn't require or commit on a specific neural architecture.However, generative AI, and specifically large language models, will play an increasingly relevant role in enabling high-level reasoning based on cognitive neuro-symbolic systems.In the next two sections we will briefly outline present research in this field, and sketch what we think are promising developments.</p>
<p>Related Work</p>
<p>The importance of integrating cognitive mechanisms into data-driven AI systems has been recently acknowledged by one of the key figures in deep learning, Yann LeCun: in a position paper published in 2022 (LeCun 2022), he described a biologically-inspired cognitive architecture, where a so-called configurator orchestrates information provided by different modules, such as the perception module and the world model module, which replicate the functions emerging from prefrontal-cortical processes.Furthermore, a motivation model -designed to mimic the role of the amygdala in producing basic emotional states like pain and pleasure -is used to compute intrinsic costs associated with current and future actions, a mechanism that is instrumental to inform predictive capabilities.It's relevant to point out that there has been extensive research on mapping cognitive architectures to brain areas/processes -e.g., (Borst et al. 2015) and that an established scientific community has been working on biologically-inspired approaches to cognitive architectures since the early 2000's (the BICA international conference has reached its 14 th edition 7 ).</p>
<p>In line with the current trend of investigating computational models of cognition in the context of large-scale neural networks, a recent blog (Weng 2023) provides an overview of tial reasoning, Allen's axioms for temporal reasoning (Allen and  Ferguson 1994).  See: https://bica2023.org/cfp/how LLMs could be used to control autonomous agents.It goes beyond the scope of our contribution to review in detail the papers mentioned in the blog, but it's beneficial to highlight some of the most interesting topics.</p>
<p>In (Wei et al. 2022) the authors leverage chain-of-thought prompting with PALM 540B (Chowdhery et al. 2022) for task-decomposition: despite of their reported success, using prompting to generate fine-grained reasoning steps does not always yield consistent results, as shown by (Chen, Zaharia, and Zou 2023) for different versions of GPT-4 (Ope-nAI 2023).The same work also indicates that, even when reasoning steps are correctly reproduced, they don't always match with the model selecting the correct solution/answer to a problem/question.Another paper surveyed in the blog (Park et al. 2023) focuses on using GPT-4 to build believable agents for a sandbox environment8 .According to the authors, cognitive architectures would not have the same degree of flexibility (and scalability) that modern generative models provide when building AI agents, as the former depend on hand-crafting rules, thus applicable only to narrow, closed-world contexts.However, this is a partial account of the state of the art: for instance, production compilation, ACT-R's rule learning mechanism, allows to learn new, taskspecific production rules that directly implement the relevant action(s) for a particular state (Taatgen, Huss, and Anderson 2006); moreover, to assess which stimuli from an environment are relevant for an agent to act upon, researchers have developed mechanisms like instance-based learning, a type of reinforcement learning (Sutton and Barto 2018), which can be plugged into ACT-R (Gonzalez, Lerch, and Lebiere 2003).One may also question the claim on generative models' flexibility: in fact, the scope of such capability is not the real world, with its ever-changing situations, but rather some emerging patterns in the text-based training data, which are biased interpretations of the real world.Incidentally, this lack of 'grounding' is also at the origin of LLMs' widelydocumented hallucination problem -for an introduction to this phenomenon, see (Ji et al. 2023).9</p>
<p>Future Work</p>
<p>As the overview in the previous section suggests, there are intrinsic limitations in utilizing a LLM as orchestrator for intelligent agents.In this regard, we can distill two main reasons for selecting a cognitive architecture over a LLM: a) the inner functioning of the former is transparent, whereas the latter is a 'black-box' (Castelvecchi 2016); b) the former is designed to replicate the invariant mechanisms of human cognition, the latter is engineered to produce human-grade linguistic behavior, which cognitive properties can only be ascribed to.By and large, what the state-of-the-art suggests is that a synergistic integration of these cognitive architectures and LLMs can help to maximize their relative strengths Figure 3: Without adequate instructions, CHAT-GPT opts for a compact model of counting from 1 to 10, which is not substantially distinct from a simple for loop (left-side).Interestingly, the chatbot suggests to include additional cognitive processes and mechanisms to make the model's behavior more realistic and accurate (bottom-left): this is what actually happens when the LLM is instructed to use all ACT-R modules (right-side).Far from being exhaustive, this example provides some evidence of the feasibility of scaling cognitive models via LLMs.</p>
<p>and mitigate their weaknesses, fostering the creation of more advanced AI systems, capable of high-level reasoning.In particular, (1) scaling cognitive models via LLMs and (2) prompt-engineering LLMs with cognitive models can be seen as novel approaches in this direction; they would actually be complemental, as (1) is a method to automatize the creation of cognitive models using generative AI, whereas (2) is a method to ground generative AI on computational artifacts that reflect principled cognitive theories.1. Scaling cognitive models via LLMs.A cognitive architecture is a generic framework to develop cognitive models, which are, conversely, tied to specific tasks and domains: the process of developing cognitive models is still largely manual, and thus affected by lack of scalability.Because LLMs have proven to be effective in generating code across a variety of programming languages (Gozalo-Brizuela and Garrido-Merchan 2023), they could also be leveraged to produce software implementations of cognitive models.Initial experiments performed by asking CHAT-GPT to generate basic cognitive models using a novel library, i.e., PyACT-R10 , show that the OpenAI's signature LLM learns to correctly generate compact Python snippets, although it only makes marginal use of ACT-R modules and buffers.In order to achieve such level of sophistication in cognitive model design, CHAT-GPT needs to be prompted with relevant instructions about which mechanisms of a cognitive architecture it should use (see figure 3). 2. Prompt-engineering LLMs with cognitive models.Using LLMs in domain-specific applications requires either fine-tuning on a target dataset, or prompt-engineering with adequate contextual knowledge.In many use cases, well-curated data are either unavailable or too timeconsuming to collect at scale, making the latter more convenient and efficient.When the goal is to turn a LLM into a reliable decision support system, the 'grounding' problem mentioned earlier also extends to the cognitive dimension: that is, such system would need to be based on shared interpretations of reality as well as on sound reasoning steps, from a cognitive-decisional standpoint.In fact, it'd be difficult to conceive such a system as trustworthy if hallucinations on both factual knowledge and on inferential mechanisms were widespread.To this end, prompting a LLM with key steps of a cognitive model's reasoning process, the so-called trace, would be instrumental to mitigate the second type of hallucinations.Such steps de facto represent the introspective stages of a cognitive model, and of a cognitive neuro-symbolic reasoning system based on it.</p>
<p>Conclusion</p>
<p>In the current debate on the limits of deep neural networks, the split is oftentimes between those who think that more data is the panacea, and those who support designing systems that integrate learning approaches with other processing elements, such as knowledge representation and reasoning, statistical algorithms, human-in-the-loop methods.In</p>
<p>Figure 2 :
2
Figure 2: ACT-R integrated with neuro-symbolic modules.</p>
<p>A main weakness of deep learning approaches, as stated in a recent article(Bengio et al.<br />
), is that 'current methods seem weak when they are required to generalize beyond the training distribution, which is what is often needed in practice', such as in safely maneuvering a vehicle.
Abbreviation of 'Adaptive Control of Thought, Rational'.
For a brief introduction to the Standard Model of Particle Physics, see this resource from the U.S. Department of Energy: https://www.energy.gov/science/doe-explainsthe-standard-modelparticle-physics
We use 'language model', 'neural language model' and 'large language model' as interchangeable terms, as they commonly refer to the same neural architecture based on multi-headed selfattention mechanisms(Vaswani et al. 2017); however, computational power significantly differs as function of the specific implementations (e.g., BERT has 6 blocks with 12 heads, GPT-3 has 24 blocks and 48 heads), and of the size of training datasets (CHAT-GPT has been trained on a massive corpus -570 GB -of text data).
Such connection assumes symbolic representations of visual and auditory signals being available to the architecture through preprocessing.
E.g., Region-Connection-Calculus(Cohn et al. 1997) for spa-
Inspired by the video-game 'The Sims': https://www.ea.com/ games/the-sims
There is an interesting analogy between Plato's Cave myth(Jowett et al. 1873), where shadows projected on a blank wall were all that prisoners could use to understand reality, thus misinterpreting it, and LLMs's generating inaccurate statements about the world, based on biased data patterns.
https://github.com/jakdot/pyactr this paper, which echoes the second category, we made the case for adopting a cognitive approach to perform that integration, inspired by the results that architectures like ACT-R have produced, over the last decades, in replicating complex human tasks at the machine level. We described the main components of a cognitive neuro-symbolic reasoning system, outlined their respective functionalities, and discussed related and future work in the area of generative AI. At the end, to paraphrase Yoshua Bengio (Yoshua Bengio 2022), we don't assume or prove that using cognitive architectures is the only possibility to equip machines with highlevel, human-like reasoning: however, through a diversity of scientific explorations, we'll increase our chances to find the ingredients we are missing.</p>
<p>Actions and events in interval temporal logic. J F Allen, G Ferguson, Journal of logic and computation. 451994</p>
<p>ACT: A simple theory of complex cognition. J R Anderson, American psychologist. 5143551996</p>
<p>Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks. L Bauer, M Bansal, Proc. of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline: Association for Computational Linguistics2021</p>
<p>Ontology-based Adaptive Systems of Cyber Defense. N Ben-Asher, A Oltramari, R F Erbacher, C Gonzalez, STIDS. 2015</p>
<p>A metatransfer objective for learning to disentangle causal mechanisms. Y Bengio, T Deleu, N Rahaman, R Ke, S Lachapelle, O Bilaniuk, A Goyal, C Pal, arXiv:1901.109122019arXiv preprint</p>
<p>ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning. M Boratko, X L Li, R Das, T O'gorman, D Le, A Mccallum, arXiv:2005.007712020arXiv preprint</p>
<p>Using data-driven model-brain mappings to constrain formal models of cognition. J P Borst, M Nijboer, N A Taatgen, H Van Rijn, J R Anderson, PLoS One. 103e01196732015</p>
<p>Can we open the black box of AI?. D Castelvecchi, Nature News. 5387623202016</p>
<p>Developing an improved ACT-R model for pilot situation awareness measurement. H Chen, S Liu, L Pang, X Wanyan, Y Fang, IEEE Access. 92021</p>
<p>L Chen, M Zaharia, J Zou, arXiv:2307.09009How is Chat-GPT's behavior changing over time?. 2023arXiv preprint</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Categorized review of drive simulators and driver behavior analysis focusing on ACT-R architecture in autonomous vehicles. M Cina, A B Rad, Sustainable Energy Technologies and Assessments. 561030442023</p>
<p>Qualitative spatial representation and reasoning with the region connection calculus. geoinformatica. A G Cohn, B Bennett, J Gooday, N M Gotts, 19971</p>
<p>GPT-3: What's it good for?. R Dale, Natural Language Engineering. 2712021</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, M Ebrahimi, A Eberhart, P Hitzler, arXiv:1810.04805arXiv:2106.09225Proceedings of the Seventh International Conference on Cognitive Modeling. the Seventh International Conference on Cognitive Modeling2018. 2021. 2006arXiv preprintOn the Capabilities of Pointer Networks for Deep Deductive Reasoning</p>
<p>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. A Ettinger, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Robust physical-world attacks on deep learning visual classification. K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, C Xiao, A Prakash, T Kohno, D Song, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Neural-symbolic learning and reasoning: A survey and interpretation. Neuro-Symbolic Artificial Intelligence: The State of the Art. A D Garcez, S Bader, H Bowman, L C Lamb, L De Penning, B Illuminoo, H Poon, Gerson Zaverucha, C , 20223421</p>
<p>A D Garcez, L C Lamb, Neurosymbolic AI: The 3 rd wave. 2023</p>
<p>Neural-symbolic cognitive reasoning. A S Garcez, L C Lamb, D M Gabbay, 2008Springer Science &amp; Business Media</p>
<p>Instancebased learning in dynamic decision making. C Gonzalez, J F Lerch, C Lebiere, Cognitive Science. 2742003</p>
<p>ChatGPT is not all you need. A State of the Art Review of large Generative AI models. R Gozalo-Brizuela, E C Garrido-Merchan, arXiv:2301.046552023arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>. D J Jilk, C Lebiere, R C O'reilly, J R Anderson, </p>
<p>SAL: An explicitly pluralistic cognitive architecture. Journal of Experimental and Theoretical Artificial Intelligence. 203</p>
<p>B Jowett, The dialogues of Plato. Scribner, Armstrong18734</p>
<p>Thinking, fast and slow. macmillan. Kautz, H. 2022. The third ai summer: Aaai robert s. engelmore memorial lecture. D Kahneman, AI Magazine. 4312011</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. I Kotseruba, J K Tsotsos, Artificial Intelligence Review. 5312020</p>
<p>The Soar cognitive architecture. J E Laird, 2019MIT press</p>
<p>A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. J E Laird, C Lebiere, P S Rosenbloom, P Langley, J E Laird, S Rogers, 2017. 2009Cognitive Systems Research38Ai MagazineCognitive architectures: Research issues and challenges</p>
<p>Cognitive architectures and their applications. C Lebiere, E Cranford, M Martin, D Morrison, A Stocco, Proceedings of IEEE CIC. IEEE CIC2022</p>
<p>A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62. Lieto, A. 2021. Cognitive design for artificial minds. Y Lecun, 2022Routledge</p>
<p>Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering. K Ma, J Francis, Q Lu, E Nyberg, A Oltramari, Proc. of the First Workshop on Commonsense Inference in Natural Language Processing. of the First Workshop on Commonsense Inference in Natural Language essing2019</p>
<p>Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. K Ma, F Ilievski, J Francis, Y Bisk, E Nyberg, A Oltramari, arXiv:1906.03674Proc. of 35th AAAI Conference on Artificial Intelligence. Margatina, K.; Baziotis, C.; and Potamianos, A. 2019. Attention-based conditioning methods for external knowledge integration. of 35th AAAI Conference on Artificial Intelligence. Margatina, K.; Baziotis, C.; and Potamianos, A. 2019. Attention-based conditioning methods for external knowledge integration2021arXiv preprint</p>
<p>The more you know: Using knowledge graphs for image classification. K Marino, R Salakhutdinov, A Gupta, arXiv:1612.04844AI Magazine. 3842016. 2017arXiv preprintNatural language understanding (NLU, not NLP) in cognitive systems</p>
<p>Using ontologies in a cognitive-grounded system: automatic action recognition in video surveillance. A Oltramari, C Lebiere, Proceedings of the 7th International Conference on Semantic Technology for Intelligence, Defense, and Security. the 7th International Conference on Semantic Technology for Intelligence, Defense, and SecurityCiteseer2012</p>
<p>R Openai, GPT-4 technical report. arXiv. 2023</p>
<p>J S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Semi-supervised sequence tagging with bidirectional language models. M E Peters, W Ammar, C Bhagavatula, R Power, arXiv:1705.001082017arXiv preprint</p>
<p>The Sigma cognitive architecture and system: Towards functionally elegant grand unification. P S Rosenbloom, A Demski, V Ustun, Journal of Artificial General Intelligence. 7112016</p>
<p>A Rosenfeld, R Zemel, J K Tsotsos, arXiv:1808.03305The elephant in the room. 2018arXiv preprint</p>
<p>The evolutionary roots of human decision making. L R Santos, A G Rosati, Annual review of psychology. 662015</p>
<p>Social IQa: Commonsense Reasoning about Social Interactions. M Sap, H Rashkin, D Chen, R Le Bras, Y Choi, Proc. of EMNLP-IJCNLP. of EMNLP-IJCNLP2019</p>
<p>Unsupervised Commonsense Question Answering with Self-Talk. V Shwartz, P West, R Le Bras, C Bhagavatula, Y Choi, Proc. of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). of the 2020 Conference on Empirical Methods in Natural Language essing (EMNLP)Online: Association for Computational Linguistics2020</p>
<p>A behavioral model of rational choice. H A Simon, The quarterly journal of economics. 1955</p>
<p>Cognitive Twin: A Cognitive Approach to Personalized Assistants. S Somers, A Oltramari, C Lebiere, AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering. 2020</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. R Speer, J Chin, C Havasi, Thirtyfirst AAAI conference on artificial intelligence. 2017</p>
<p>Visual reasoning with multi-hop feature modulation. F Strub, M Seurin, E Perez, H De Vries, J Mary, P Preux, A C Pietquin, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>How cognitive models can inform the design of instructions. N A Taatgen, D Huss, J R Anderson, Citeseer, A Ushio, L Espinosa-Anke, S Schockaert, J Camacho-Collados, arXiv:2105.04949BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?. 2006. 2021arXiv preprintProceedings of the seventh international conference on cognitive modeling</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730</p>
<p>Knowledge Graph Embedding: A Survey of Approaches and Applications. Q Wang, Z Mao, B Wang, L Guo, IEEE Transactions on Knowledge and Data Engineering. 29122017</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>CLUE-AD: A Context-Based Method for Labeling Unobserved Entities in Autonomous Driving Data. L Weng, J Weston, A Bordes, S Chopra, A M Rush, B Van Merriënboer, A Joulin, T Mikolov, R Wickramarachchi, C Henson, A Sheth, arXiv:1502.05698Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023. 2015. 202337arXiv preprintLLM-powered Autonomous Agents. lilianweng</p>
<p>Yoshua Bengio. 2022. DeepLearning.AI. </p>            </div>
        </div>

    </div>
</body>
</html>
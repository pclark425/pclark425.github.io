<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4473 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4473</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4473</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-278959434</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.21935v2.pdf" target="_blank">From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors''into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4473.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4473.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token-similarity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level similarity metrics (BLEU / ROUGE / METEOR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated string-overlap metrics originally developed for MT and summarization (BLEU, ROUGE, METEOR) that are used to compare generated natural-language hypotheses to reference hypotheses by n-gram overlap or alignment heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Token-level similarity evaluation (BLEU / ROUGE / METEOR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram overlap or alignment-based similarity between an LLM-generated natural-language hypothesis and a human-written reference hypothesis; scores are aggregated (e.g., BLEU precision, ROUGE recall, METEOR alignment) to produce an automated quality estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Surface similarity to reference (lexical overlap, fluency proxies); implicitly assumed correctness/coherence relative to the reference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / cross-domain (used for natural-language hypothesis outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Natural-language explanatory hypotheses (open-ended)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey notes these metrics are commonly used (e.g., Qi et al. use BLEU/ROUGE; DEER uses METEOR) but are inadequate for open-ended hypothesis generation: they reward surface similarity and penalize novel but valid formulations. No reliable numerical correlation with substantive hypothesis quality reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric only (string overlap); often complemented by human evaluation in practice because of metric shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Indirect: validated historically for MT/summarization via correlation with human judgments, but survey highlights lack of domain-specific validation for hypothesis-generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Fails to capture novelty, explanatory power, or alternative-but-valid wordings; assumes single reference; poor for open-ended scientific hypotheses; can penalize creative/abstractive valid hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used in DEER (expert-written rules) and Qi et al. biomedical benchmark as the automated similarity metric for natural-language hypothesis outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4473.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implicit-prediction evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit prediction-based evaluation (QA as proxy for abduction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluate hypothesis generation indirectly by testing whether the model can answer downstream questions that require forming an implicit hypothesis; correctness of the answer is used as a proxy for successful hypothesis formation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Implicit prediction / QA-proxy evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Pose a question whose correct answer depends on an implicit explanatory hypothesis (e.g., derive color of a swan). If the model answers correctly, infer that it formed an appropriate hypothesis; sometimes models are prompted to produce an intermediate hypothesis explicitly and then answer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Downstream task correctness (QA accuracy), implicit use of abductive inference, sometimes intermediate-hypothesis plausibility if produced.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning / commonsense / domain-specific when QA is domain-specific</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Implicit explanatory hypotheses used to support answers</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey emphasizes this is widely used historically (CLUTRR, early QA tasks) and that CoT + intermediate-hypothesis prompting can improve QA, but warns that correct answers may be reached via memorization or spurious heuristics rather than genuine abductive reasoning; thus success on QA does not reliably indicate true hypothesis discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (closed-answer correctness); sometimes supplemented by inspection of intermediate hypotheses by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to ground-truth answers; critique in survey notes lack of validation that success implies correct abductive process.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Conflates answer correctness with quality of hypothesis generation; cannot distinguish correct guesses from genuine explanatory invention; not suitable for open-ended hypothesis evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Implicitly associated with older QA benchmarks (e.g., CLUTRR) and with newer work that prompts intermediate hypotheses (Balepur et al., Shi et al., Wang et al. referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4473.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ground-truth NL evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ground-truth based evaluation for natural-language hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compare LLM-generated natural-language hypotheses against expert-written gold hypotheses using reference-based scoring or human adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Ground-truth reference comparison (NL hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Construct a dataset of paired observation→gold-hypothesis examples (written by experts). Evaluate generated hypotheses via automated token-level metrics (BLEU/ROUGE/METEOR) or via human raters who judge validity, novelty, and usefulness against the gold.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Similarity to gold (via metrics), human judgments of correctness, novelty, and applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Domains with curated corpora (e.g., biomedicine, zoology, geology, astronomy, history, physics as in DEER or Qi et al.'s biomedical set)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Natural-language domain hypotheses / research-idea style hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports example datasets: DEER with 1,200 fact-rule pairs (six topics) evaluated with METEOR; Qi et al.'s biomedical benchmark (seen: 2,700 pairs; unseen: 200 pairs) evaluated with BLEU/ROUGE. Reported behavior: increasing prompt examples reduced novelty and increased correctness; token metrics do not capture open-ended hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid — automated metrics used where gold references exist; human evaluation used for subjective aspects and to validate novelty/utility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ground-truth correctness on held-out examples; human expert assessment used to validate metric outputs when available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Gold references are expensive and often single-reference; token metrics fail to capture multiple valid hypotheses; human evaluations are expensive, subjective, and low-reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>DEER (1,200 expert fact-rule pairs across six topics); Qi et al. biomedical dataset (seen/unseen splits: 2,700 / 200).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4473.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formal procedural evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Procedural / execution-based evaluation of formal hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When hypotheses are represented in formal languages (FOL or executable code), evaluate them by running provers or executing candidate programs on held-out inputs and checking deterministic outputs against ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Formal solver / execution-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Translate generated hypotheses into formal representations (FOL, code, or functions), use solvers or interpreters to deduce consequences or execute programs on test cases, and measure exact-match correctness on held-out examples (deterministic verification).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Deterministic correctness on test inputs, logical entailment of observations, completeness/soundness with respect to formal semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Tasks amenable to formalization (toy algorithmic tasks, symbolic domains, some programmatic scientific tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Formal rules, executable hypothesis functions, first-order logic rules</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey cites Bowen et al. (2024), RuleBench, StringGame, and list-function tasks where generated formal hypotheses are executed and judged correct on held-out examples; execution-based evaluation gives objective, deterministic correctness but is typically limited to simplified or synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (program execution or logical provers); human people validate benchmarks and test cases during dataset creation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ground-truth verification by solvers/execution on held-out cases; procedural guarantees (soundness) when using formal provers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Formal tasks often oversimplify real-world complexity; translation from NL to formal representation is itself brittle; benchmarks risk being solvable via memorization rather than genuine discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>StringGame, RuleBench, list-function tasks, ARC (as analogous example), and synthetic formal grouping tasks referenced in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4473.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using LLMs to evaluate hypotheses (LLM-based evaluation / self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use an LLM to judge or critique candidate hypotheses, either by direct scoring, consistency checks, or meta-evaluation modules (e.g., generate critiques, judge novelty, or filter plagiarized hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based hypothesis evaluation / self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an LLM (or a chain of modules) to assess candidate hypotheses on criteria like deductive consistency, novelty, generalizability, or copying; can be used iteratively in a generate-then-evaluate loop to rank or refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Deductive consistency, novelty/originality, generalizability, non-triviality, evidence coverage, potential falsifiability (as encoded in prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General; used in scientific idea generation pipelines (e.g., literature-grounded hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Natural-language research hypotheses and claims</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey describes pipelines that include modules for checking deductive consistency, plagiarism, generalizability, and triviality (e.g., Yang et al.'s five-module pipeline) and RAG-based iterative re-scoring pipelines; LLM evaluators can improve fluency and surface-level consistency but risk reinforcing model biases and can be circular without external grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM scoring) often combined with human review for final assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Sometimes validated by downstream acceptance (e.g., human experts selecting final ideas) or by proxy metrics (novelty scores), but the survey flags lack of robust validation of LLM-as-evaluator reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Evaluator and generator share the same parametric biases (risk of self-reinforcing errors); difficulty establishing calibration or correlation with expert judgments; vulnerable to circular validation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used in RAG-based hypothesis generation pipelines (Hu et al., Yang et al.) and novelty-guided loops (Chai et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4473.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human/expert evaluation of hypothesis quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Have human experts or crowd annotators rate generated hypotheses along dimensions such as plausibility, novelty, correctness, and usefulness; often used where automated references are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human/expert assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Recruit domain experts or multiple human annotators to judge candidate hypotheses on predefined scales (binary plausibility, Likert scales for novelty/usefulness, pairwise preference), possibly with adjudication and inter-annotator agreement measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Plausibility, novelty/originality, scientific value, falsifiability, clarity, potential for follow-up experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Domain-specific evaluations (e.g., biomedicine, chemistry) and general scientific idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Open-ended natural-language hypotheses and research ideas</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports human evaluation is informative but costly and subjective; example: Zhao et al. found annotator disagreement of 62.34% on plausibility judgments for 1,365 explanations, indicating low inter-annotator agreement and high subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based; sometimes combined with automated pre-filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-annotator agreement statistics and expert adjudication; survey notes lack of consistent validation protocols across studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Expensive, low reproducibility, subjective variance across annotators and domains, sensitive to annotation instructions and rater expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to DEER, Qi et al.'s biomedical set, and human studies evaluating novelty/correctness of LLM-generated research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4473.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Strengthen/Weaken protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strengthening vs. weakening evidence classification (Rudinger et al. approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transform validation into a consistency-update task where models decide whether a new observation strengthens or weakens a given hypothesis, reducing subjective variability in direct plausibility judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Thinking like a skeptic: Defeasible inference in natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Strengthener/Weakener classification (defeasible inference evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sample observation–hypothesis pairs and craft paired sentences that are explicit 'strengtheners' and 'weakeners' for the hypothesis. The model's task is to classify whether a new observation increases or decreases belief in the hypothesis, operationalizing induction as defeasible update rather than absolute plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correct identification of evidence polarity (strengthening vs weakening); robustness of judgments across annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural-language hypothesis validation (general)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis validation / inductive update judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey highlights Rudinger et al.'s method as producing consistent annotator judgments and being better aligned with modeling defeasible inference; used as a proxy for induction in NL settings. Exact numeric model performance not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated classification task with human-crafted labeled examples; human annotation used to create labels and measure consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human annotator agreement on crafted strengthen/ weaken sentences; consistency across annotators reported as good relative to direct plausibility labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires manual crafting of strengthening/weakening sentences; may not capture graded confidence updates or complex contextual reasoning; still sensitive to wording.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Approach applied in Rudinger et al. (2020) and extended by Zhang et al. (2025) to visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4473.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multiple-choice abductive benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple-choice abductive explanation benchmarks (ART / BRAINTEASER / True Detective)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks that provide multiple-choice explanatory options for narratives or puzzles and measure whether models can select the most plausible explanation among distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>True detective: A deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multiple-choice explanatory selection</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Present a narrative (observations) and several candidate explanations (one plausible, others less so); the model must choose the best explanation. Datasets include human-generated distractors and sometimes chain-of-thought gold explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Selection accuracy (percentage of correct choices), consistency across formulations, ability to reconstruct gold chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Narrative abductive reasoning, lateral thinking, general commonsense</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanatory hypotheses (narrative-style)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey cites ART (~20k contexts), BRAINTEASER (~1.1k puzzles), and True Detective (191 long-form puzzles) where humans average 47% on True Detective (top solvers >80%); models struggle on deep abductive puzzles and on chain-of-thought reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring (accuracy) against human-labeled correct choice; human baselines reported for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Gold labels created by human authors; chain-of-thought gold explanations used to validate reasoning steps where provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Multiple-choice format constrains expressivity to provided options; can still be solved by surface cues; long-form puzzles are hard for models but annotation is expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ART, BRAINTEASER, True Detective (191 puzzles with gold chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4473.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-following benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-following / deductive evaluation benchmarks (TURTLEBENCH / RULES / Holiday Puzzle / RuleBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks designed to test whether models can apply explicit rules (including counterfactual ones) to produce correct deductions or classifications, frequently with programmatic evaluation functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rule-following / deduction outcome evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given explicit hypotheses/rules and contexts, ask model to derive consequences or answer yes/no/true-false queries; evaluate outputs against ground-truth labels or programmatic test functions (True/False/Not Relevant or numerical answers). Some benchmarks include counterfactual rules to test faithful rule-following.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Prediction correctness under explicit rules, fidelity to counterfactual rules, robustness across unfamiliar contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General deductive reasoning / rule application (applies across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Application of formal or natural-language rules to derive consequences</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey cites TURTLEBENCH (1,532 QA pairs) and RULES (14 rule-following scenarios with programmatic evaluation). Sun et al.'s RuleBench shows models achieve near-perfect accuracy on factual rules but performance degrades substantially under counterfactual rules, revealing poor counterfactual rule-following.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated programmatic checks or ground-truth comparators; sometimes human-annotated question labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Programmatic evaluation functions and curated labels; survey reports that these are objective for given rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Natural-language deduction paths are ambiguous; correct outcomes can hide incorrect reasoning trajectories; lack of annotated intermediate reasoning steps limits trajectory-level evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TURTLEBENCH, RULES, Holiday Puzzle, RuleBench.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4473.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proactive iterative evaluation (APEx)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated iterative experiment programming (APEx) and proactive benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated frameworks that iteratively design, run, and refine experiments to test hypotheses (often multimodal), producing a cycle of evidence collection and hypothesis evaluation without human-in-the-loop for each step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic benchmarking of large multimodal models via iterative experiment programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Automated iterative experiment-based evaluation (APEx)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given a hypothesis about a model or phenomenon, the framework programmatically generates test cases (e.g., images), applies transformations/augmentations, executes model evaluations, analyzes results, and refines tests in an iterative loop to robustly evaluate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Empirical robustness across augmentations, reproducibility, automated evidence gathering, statistical significance of observed effects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multimodal model behavior evaluation (e.g., image recognition properties)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Behavioral hypotheses about model capabilities (e.g., sensitivity to graffiti style)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey states APEx can build tailored testsets and iteratively stress-test models; gives an automated, repeatable evaluation pipeline though example performance numbers are from APEx paper rather than the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated experimental loop; human experts validate initial hypotheses and interpret results but the test generation and execution are automated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical testing and iterative refinement; statistical analysis of experiment outcomes validates or refutes hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Automation depends on quality of generated tests and transformations; may miss nuanced domain knowledge accessible only to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>APEx is itself a framework rather than a fixed dataset; used to generate datasets per-hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4473.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4473.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complete-loop simulation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end discovery simulation environments (ScienceWorld / DiscoveryWorld / puzzle envs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Interactive simulated environments that require agents to plan experiments, act to collect evidence, form and revise hypotheses, and are evaluated on successful discovery and interpretability of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Complete-loop simulation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Place an agent in a simulated environment (virtual lab, Minecraft-like world, puzzle environment) where it must proactively design and execute actions/experiments to gather data, form hypotheses, deduce consequences, and update beliefs; evaluation includes task completion, correctness of discovered hypothesis vs ground-truth rules, execution of key experimental steps, and human judgments of reasoning clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task completion, correctness of discovered rules/hypotheses, experimental procedure fidelity, efficiency of evidence gathering, interpretability and clarity of reasoning chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Simulated scientific tasks spanning physics, chemistry, archaeology, and general scientific problem-solving curricula</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Quantitative rule-learning, mechanistic rules, experimental hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey cites ScienceWorld and DiscoveryWorld style tasks (Wang et al., Jansen et al.), puzzle environments by He et al. (2024) where success judged by solution and human judgments of clarity/rigor; results show agents can perform qualitative inference but action spaces are often too coarse for fine-grained scientific rule learning. Specific numeric scores vary by environment and are reported in those original works.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated task-success and programmatic checks plus human evaluation of reasoning steps and clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to ground-truth hidden rules in simulation; human evaluation for chain-of-thought and experimental fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Simulations can be oversimplified and permit hypothesis leakage from pretraining; building realistic, expressive action spaces is expensive; current sims often evaluate evidence-interpretation rather than full agent-driven experiment design.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ScienceWorld (fifth-grade science tasks), DiscoveryWorld experimental suites, puzzle environments (He et al. 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments <em>(Rating: 2)</em></li>
                <li>Bleu: a method for automatic evaluation of machine translation <em>(Rating: 2)</em></li>
                <li>ROUGE: A package for automatic evaluation of summaries <em>(Rating: 2)</em></li>
                <li>True detective: A deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4 <em>(Rating: 2)</em></li>
                <li>Thinking like a skeptic: Defeasible inference in natural language <em>(Rating: 2)</em></li>
                <li>Automatic benchmarking of large multimodal models via iterative experiment programming <em>(Rating: 2)</em></li>
                <li>Turtlebench: Evaluating top language models via real-world yes/no puzzles <em>(Rating: 2)</em></li>
                <li>Beyond instruction following: Evaluating inferential rule following of large language models <em>(Rating: 2)</em></li>
                <li>Causejudger: Identifying the cause with llms for abductive logical reasoning <em>(Rating: 2)</em></li>
                <li>Large language models as biomedical hypothesis generators: A comprehensive evaluation <em>(Rating: 2)</em></li>
                <li>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents <em>(Rating: 2)</em></li>
                <li>Scienceworld: Is your agent smarter than a 5th grader? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4473",
    "paper_id": "paper-278959434",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Token-similarity metrics",
            "name_full": "Token-level similarity metrics (BLEU / ROUGE / METEOR)",
            "brief_description": "Automated string-overlap metrics originally developed for MT and summarization (BLEU, ROUGE, METEOR) that are used to compare generated natural-language hypotheses to reference hypotheses by n-gram overlap or alignment heuristics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Token-level similarity evaluation (BLEU / ROUGE / METEOR)",
            "evaluation_method_description": "Compute n-gram overlap or alignment-based similarity between an LLM-generated natural-language hypothesis and a human-written reference hypothesis; scores are aggregated (e.g., BLEU precision, ROUGE recall, METEOR alignment) to produce an automated quality estimate.",
            "evaluation_criteria": "Surface similarity to reference (lexical overlap, fluency proxies); implicitly assumed correctness/coherence relative to the reference.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General / cross-domain (used for natural-language hypothesis outputs)",
            "theory_type": "Natural-language explanatory hypotheses (open-ended)",
            "human_comparison": true,
            "evaluation_results": "Survey notes these metrics are commonly used (e.g., Qi et al. use BLEU/ROUGE; DEER uses METEOR) but are inadequate for open-ended hypothesis generation: they reward surface similarity and penalize novel but valid formulations. No reliable numerical correlation with substantive hypothesis quality reported in the survey.",
            "automated_vs_human_evaluation": "Automated metric only (string overlap); often complemented by human evaluation in practice because of metric shortcomings.",
            "validation_method": "Indirect: validated historically for MT/summarization via correlation with human judgments, but survey highlights lack of domain-specific validation for hypothesis-generation tasks.",
            "limitations_challenges": "Fails to capture novelty, explanatory power, or alternative-but-valid wordings; assumes single reference; poor for open-ended scientific hypotheses; can penalize creative/abstractive valid hypotheses.",
            "benchmark_dataset": "Used in DEER (expert-written rules) and Qi et al. biomedical benchmark as the automated similarity metric for natural-language hypothesis outputs.",
            "uuid": "e4473.0",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Implicit-prediction evaluation",
            "name_full": "Implicit prediction-based evaluation (QA as proxy for abduction)",
            "brief_description": "Evaluate hypothesis generation indirectly by testing whether the model can answer downstream questions that require forming an implicit hypothesis; correctness of the answer is used as a proxy for successful hypothesis formation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Implicit prediction / QA-proxy evaluation",
            "evaluation_method_description": "Pose a question whose correct answer depends on an implicit explanatory hypothesis (e.g., derive color of a swan). If the model answers correctly, infer that it formed an appropriate hypothesis; sometimes models are prompted to produce an intermediate hypothesis explicitly and then answer.",
            "evaluation_criteria": "Downstream task correctness (QA accuracy), implicit use of abductive inference, sometimes intermediate-hypothesis plausibility if produced.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General reasoning / commonsense / domain-specific when QA is domain-specific",
            "theory_type": "Implicit explanatory hypotheses used to support answers",
            "human_comparison": false,
            "evaluation_results": "Survey emphasizes this is widely used historically (CLUTRR, early QA tasks) and that CoT + intermediate-hypothesis prompting can improve QA, but warns that correct answers may be reached via memorization or spurious heuristics rather than genuine abductive reasoning; thus success on QA does not reliably indicate true hypothesis discovery.",
            "automated_vs_human_evaluation": "Automated (closed-answer correctness); sometimes supplemented by inspection of intermediate hypotheses by humans.",
            "validation_method": "Comparison to ground-truth answers; critique in survey notes lack of validation that success implies correct abductive process.",
            "limitations_challenges": "Conflates answer correctness with quality of hypothesis generation; cannot distinguish correct guesses from genuine explanatory invention; not suitable for open-ended hypothesis evaluation.",
            "benchmark_dataset": "Implicitly associated with older QA benchmarks (e.g., CLUTRR) and with newer work that prompts intermediate hypotheses (Balepur et al., Shi et al., Wang et al. referenced).",
            "uuid": "e4473.1",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Ground-truth NL evaluation",
            "name_full": "Ground-truth based evaluation for natural-language hypotheses",
            "brief_description": "Compare LLM-generated natural-language hypotheses against expert-written gold hypotheses using reference-based scoring or human adjudication.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Ground-truth reference comparison (NL hypotheses)",
            "evaluation_method_description": "Construct a dataset of paired observation→gold-hypothesis examples (written by experts). Evaluate generated hypotheses via automated token-level metrics (BLEU/ROUGE/METEOR) or via human raters who judge validity, novelty, and usefulness against the gold.",
            "evaluation_criteria": "Similarity to gold (via metrics), human judgments of correctness, novelty, and applicability.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Domains with curated corpora (e.g., biomedicine, zoology, geology, astronomy, history, physics as in DEER or Qi et al.'s biomedical set)",
            "theory_type": "Natural-language domain hypotheses / research-idea style hypotheses",
            "human_comparison": true,
            "evaluation_results": "Survey reports example datasets: DEER with 1,200 fact-rule pairs (six topics) evaluated with METEOR; Qi et al.'s biomedical benchmark (seen: 2,700 pairs; unseen: 200 pairs) evaluated with BLEU/ROUGE. Reported behavior: increasing prompt examples reduced novelty and increased correctness; token metrics do not capture open-ended hypothesis quality.",
            "automated_vs_human_evaluation": "Hybrid — automated metrics used where gold references exist; human evaluation used for subjective aspects and to validate novelty/utility.",
            "validation_method": "Ground-truth correctness on held-out examples; human expert assessment used to validate metric outputs when available.",
            "limitations_challenges": "Gold references are expensive and often single-reference; token metrics fail to capture multiple valid hypotheses; human evaluations are expensive, subjective, and low-reproducibility.",
            "benchmark_dataset": "DEER (1,200 expert fact-rule pairs across six topics); Qi et al. biomedical dataset (seen/unseen splits: 2,700 / 200).",
            "uuid": "e4473.2",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Formal procedural evaluation",
            "name_full": "Procedural / execution-based evaluation of formal hypotheses",
            "brief_description": "When hypotheses are represented in formal languages (FOL or executable code), evaluate them by running provers or executing candidate programs on held-out inputs and checking deterministic outputs against ground-truth.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Formal solver / execution-based evaluation",
            "evaluation_method_description": "Translate generated hypotheses into formal representations (FOL, code, or functions), use solvers or interpreters to deduce consequences or execute programs on test cases, and measure exact-match correctness on held-out examples (deterministic verification).",
            "evaluation_criteria": "Deterministic correctness on test inputs, logical entailment of observations, completeness/soundness with respect to formal semantics.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Tasks amenable to formalization (toy algorithmic tasks, symbolic domains, some programmatic scientific tasks)",
            "theory_type": "Formal rules, executable hypothesis functions, first-order logic rules",
            "human_comparison": false,
            "evaluation_results": "Survey cites Bowen et al. (2024), RuleBench, StringGame, and list-function tasks where generated formal hypotheses are executed and judged correct on held-out examples; execution-based evaluation gives objective, deterministic correctness but is typically limited to simplified or synthetic tasks.",
            "automated_vs_human_evaluation": "Automated (program execution or logical provers); human people validate benchmarks and test cases during dataset creation.",
            "validation_method": "Ground-truth verification by solvers/execution on held-out cases; procedural guarantees (soundness) when using formal provers.",
            "limitations_challenges": "Formal tasks often oversimplify real-world complexity; translation from NL to formal representation is itself brittle; benchmarks risk being solvable via memorization rather than genuine discovery.",
            "benchmark_dataset": "StringGame, RuleBench, list-function tasks, ARC (as analogous example), and synthetic formal grouping tasks referenced in the survey.",
            "uuid": "e4473.3",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM-as-evaluator",
            "name_full": "Using LLMs to evaluate hypotheses (LLM-based evaluation / self-critique)",
            "brief_description": "Use an LLM to judge or critique candidate hypotheses, either by direct scoring, consistency checks, or meta-evaluation modules (e.g., generate critiques, judge novelty, or filter plagiarized hypotheses).",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "LLM-based hypothesis evaluation / self-critique",
            "evaluation_method_description": "Prompt an LLM (or a chain of modules) to assess candidate hypotheses on criteria like deductive consistency, novelty, generalizability, or copying; can be used iteratively in a generate-then-evaluate loop to rank or refine hypotheses.",
            "evaluation_criteria": "Deductive consistency, novelty/originality, generalizability, non-triviality, evidence coverage, potential falsifiability (as encoded in prompt).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General; used in scientific idea generation pipelines (e.g., literature-grounded hypothesis generation)",
            "theory_type": "Natural-language research hypotheses and claims",
            "human_comparison": false,
            "evaluation_results": "Survey describes pipelines that include modules for checking deductive consistency, plagiarism, generalizability, and triviality (e.g., Yang et al.'s five-module pipeline) and RAG-based iterative re-scoring pipelines; LLM evaluators can improve fluency and surface-level consistency but risk reinforcing model biases and can be circular without external grounding.",
            "automated_vs_human_evaluation": "Automated (LLM scoring) often combined with human review for final assessment.",
            "validation_method": "Sometimes validated by downstream acceptance (e.g., human experts selecting final ideas) or by proxy metrics (novelty scores), but the survey flags lack of robust validation of LLM-as-evaluator reliability.",
            "limitations_challenges": "Evaluator and generator share the same parametric biases (risk of self-reinforcing errors); difficulty establishing calibration or correlation with expert judgments; vulnerable to circular validation.",
            "benchmark_dataset": "Used in RAG-based hypothesis generation pipelines (Hu et al., Yang et al.) and novelty-guided loops (Chai et al.).",
            "uuid": "e4473.4",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Human expert evaluation",
            "name_full": "Human/expert evaluation of hypothesis quality",
            "brief_description": "Have human experts or crowd annotators rate generated hypotheses along dimensions such as plausibility, novelty, correctness, and usefulness; often used where automated references are insufficient.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Human/expert assessment",
            "evaluation_method_description": "Recruit domain experts or multiple human annotators to judge candidate hypotheses on predefined scales (binary plausibility, Likert scales for novelty/usefulness, pairwise preference), possibly with adjudication and inter-annotator agreement measurement.",
            "evaluation_criteria": "Plausibility, novelty/originality, scientific value, falsifiability, clarity, potential for follow-up experiments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Domain-specific evaluations (e.g., biomedicine, chemistry) and general scientific idea generation",
            "theory_type": "Open-ended natural-language hypotheses and research ideas",
            "human_comparison": true,
            "evaluation_results": "Survey reports human evaluation is informative but costly and subjective; example: Zhao et al. found annotator disagreement of 62.34% on plausibility judgments for 1,365 explanations, indicating low inter-annotator agreement and high subjectivity.",
            "automated_vs_human_evaluation": "Human-based; sometimes combined with automated pre-filtering.",
            "validation_method": "Inter-annotator agreement statistics and expert adjudication; survey notes lack of consistent validation protocols across studies.",
            "limitations_challenges": "Expensive, low reproducibility, subjective variance across annotators and domains, sensitive to annotation instructions and rater expertise.",
            "benchmark_dataset": "Applied to DEER, Qi et al.'s biomedical set, and human studies evaluating novelty/correctness of LLM-generated research ideas.",
            "uuid": "e4473.5",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Strengthen/Weaken protocol",
            "name_full": "Strengthening vs. weakening evidence classification (Rudinger et al. approach)",
            "brief_description": "Transform validation into a consistency-update task where models decide whether a new observation strengthens or weakens a given hypothesis, reducing subjective variability in direct plausibility judgments.",
            "citation_title": "Thinking like a skeptic: Defeasible inference in natural language",
            "mention_or_use": "use",
            "evaluation_method_name": "Strengthener/Weakener classification (defeasible inference evaluation)",
            "evaluation_method_description": "Sample observation–hypothesis pairs and craft paired sentences that are explicit 'strengtheners' and 'weakeners' for the hypothesis. The model's task is to classify whether a new observation increases or decreases belief in the hypothesis, operationalizing induction as defeasible update rather than absolute plausibility.",
            "evaluation_criteria": "Correct identification of evidence polarity (strengthening vs weakening); robustness of judgments across annotators.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural-language hypothesis validation (general)",
            "theory_type": "Hypothesis validation / inductive update judgments",
            "human_comparison": true,
            "evaluation_results": "Survey highlights Rudinger et al.'s method as producing consistent annotator judgments and being better aligned with modeling defeasible inference; used as a proxy for induction in NL settings. Exact numeric model performance not reported in survey.",
            "automated_vs_human_evaluation": "Automated classification task with human-crafted labeled examples; human annotation used to create labels and measure consistency.",
            "validation_method": "Human annotator agreement on crafted strengthen/ weaken sentences; consistency across annotators reported as good relative to direct plausibility labels.",
            "limitations_challenges": "Requires manual crafting of strengthening/weakening sentences; may not capture graded confidence updates or complex contextual reasoning; still sensitive to wording.",
            "benchmark_dataset": "Approach applied in Rudinger et al. (2020) and extended by Zhang et al. (2025) to visual observations.",
            "uuid": "e4473.6",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Multiple-choice abductive benchmarks",
            "name_full": "Multiple-choice abductive explanation benchmarks (ART / BRAINTEASER / True Detective)",
            "brief_description": "Benchmarks that provide multiple-choice explanatory options for narratives or puzzles and measure whether models can select the most plausible explanation among distractors.",
            "citation_title": "True detective: A deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4",
            "mention_or_use": "use",
            "evaluation_method_name": "Multiple-choice explanatory selection",
            "evaluation_method_description": "Present a narrative (observations) and several candidate explanations (one plausible, others less so); the model must choose the best explanation. Datasets include human-generated distractors and sometimes chain-of-thought gold explanations.",
            "evaluation_criteria": "Selection accuracy (percentage of correct choices), consistency across formulations, ability to reconstruct gold chain-of-thought.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Narrative abductive reasoning, lateral thinking, general commonsense",
            "theory_type": "Explanatory hypotheses (narrative-style)",
            "human_comparison": true,
            "evaluation_results": "Survey cites ART (~20k contexts), BRAINTEASER (~1.1k puzzles), and True Detective (191 long-form puzzles) where humans average 47% on True Detective (top solvers &gt;80%); models struggle on deep abductive puzzles and on chain-of-thought reconstruction.",
            "automated_vs_human_evaluation": "Automated scoring (accuracy) against human-labeled correct choice; human baselines reported for comparison.",
            "validation_method": "Gold labels created by human authors; chain-of-thought gold explanations used to validate reasoning steps where provided.",
            "limitations_challenges": "Multiple-choice format constrains expressivity to provided options; can still be solved by surface cues; long-form puzzles are hard for models but annotation is expensive.",
            "benchmark_dataset": "ART, BRAINTEASER, True Detective (191 puzzles with gold chain-of-thought).",
            "uuid": "e4473.7",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Rule-following benchmarks",
            "name_full": "Rule-following / deductive evaluation benchmarks (TURTLEBENCH / RULES / Holiday Puzzle / RuleBench)",
            "brief_description": "Benchmarks designed to test whether models can apply explicit rules (including counterfactual ones) to produce correct deductions or classifications, frequently with programmatic evaluation functions.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Rule-following / deduction outcome evaluation",
            "evaluation_method_description": "Given explicit hypotheses/rules and contexts, ask model to derive consequences or answer yes/no/true-false queries; evaluate outputs against ground-truth labels or programmatic test functions (True/False/Not Relevant or numerical answers). Some benchmarks include counterfactual rules to test faithful rule-following.",
            "evaluation_criteria": "Prediction correctness under explicit rules, fidelity to counterfactual rules, robustness across unfamiliar contexts.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General deductive reasoning / rule application (applies across domains)",
            "theory_type": "Application of formal or natural-language rules to derive consequences",
            "human_comparison": false,
            "evaluation_results": "Survey cites TURTLEBENCH (1,532 QA pairs) and RULES (14 rule-following scenarios with programmatic evaluation). Sun et al.'s RuleBench shows models achieve near-perfect accuracy on factual rules but performance degrades substantially under counterfactual rules, revealing poor counterfactual rule-following.",
            "automated_vs_human_evaluation": "Automated programmatic checks or ground-truth comparators; sometimes human-annotated question labels.",
            "validation_method": "Programmatic evaluation functions and curated labels; survey reports that these are objective for given rules.",
            "limitations_challenges": "Natural-language deduction paths are ambiguous; correct outcomes can hide incorrect reasoning trajectories; lack of annotated intermediate reasoning steps limits trajectory-level evaluation.",
            "benchmark_dataset": "TURTLEBENCH, RULES, Holiday Puzzle, RuleBench.",
            "uuid": "e4473.8",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Proactive iterative evaluation (APEx)",
            "name_full": "Automated iterative experiment programming (APEx) and proactive benchmarking",
            "brief_description": "Automated frameworks that iteratively design, run, and refine experiments to test hypotheses (often multimodal), producing a cycle of evidence collection and hypothesis evaluation without human-in-the-loop for each step.",
            "citation_title": "Automatic benchmarking of large multimodal models via iterative experiment programming",
            "mention_or_use": "mention",
            "evaluation_method_name": "Automated iterative experiment-based evaluation (APEx)",
            "evaluation_method_description": "Given a hypothesis about a model or phenomenon, the framework programmatically generates test cases (e.g., images), applies transformations/augmentations, executes model evaluations, analyzes results, and refines tests in an iterative loop to robustly evaluate hypotheses.",
            "evaluation_criteria": "Empirical robustness across augmentations, reproducibility, automated evidence gathering, statistical significance of observed effects.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Multimodal model behavior evaluation (e.g., image recognition properties)",
            "theory_type": "Behavioral hypotheses about model capabilities (e.g., sensitivity to graffiti style)",
            "human_comparison": false,
            "evaluation_results": "Survey states APEx can build tailored testsets and iteratively stress-test models; gives an automated, repeatable evaluation pipeline though example performance numbers are from APEx paper rather than the survey.",
            "automated_vs_human_evaluation": "Automated experimental loop; human experts validate initial hypotheses and interpret results but the test generation and execution are automated.",
            "validation_method": "Empirical testing and iterative refinement; statistical analysis of experiment outcomes validates or refutes hypotheses.",
            "limitations_challenges": "Automation depends on quality of generated tests and transformations; may miss nuanced domain knowledge accessible only to humans.",
            "benchmark_dataset": "APEx is itself a framework rather than a fixed dataset; used to generate datasets per-hypothesis.",
            "uuid": "e4473.9",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Complete-loop simulation benchmarks",
            "name_full": "End-to-end discovery simulation environments (ScienceWorld / DiscoveryWorld / puzzle envs)",
            "brief_description": "Interactive simulated environments that require agents to plan experiments, act to collect evidence, form and revise hypotheses, and are evaluated on successful discovery and interpretability of reasoning.",
            "citation_title": "Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents",
            "mention_or_use": "mention",
            "evaluation_method_name": "Complete-loop simulation evaluation",
            "evaluation_method_description": "Place an agent in a simulated environment (virtual lab, Minecraft-like world, puzzle environment) where it must proactively design and execute actions/experiments to gather data, form hypotheses, deduce consequences, and update beliefs; evaluation includes task completion, correctness of discovered hypothesis vs ground-truth rules, execution of key experimental steps, and human judgments of reasoning clarity.",
            "evaluation_criteria": "Task completion, correctness of discovered rules/hypotheses, experimental procedure fidelity, efficiency of evidence gathering, interpretability and clarity of reasoning chain-of-thought.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Simulated scientific tasks spanning physics, chemistry, archaeology, and general scientific problem-solving curricula",
            "theory_type": "Quantitative rule-learning, mechanistic rules, experimental hypotheses",
            "human_comparison": true,
            "evaluation_results": "Survey cites ScienceWorld and DiscoveryWorld style tasks (Wang et al., Jansen et al.), puzzle environments by He et al. (2024) where success judged by solution and human judgments of clarity/rigor; results show agents can perform qualitative inference but action spaces are often too coarse for fine-grained scientific rule learning. Specific numeric scores vary by environment and are reported in those original works.",
            "automated_vs_human_evaluation": "Hybrid: automated task-success and programmatic checks plus human evaluation of reasoning steps and clarity.",
            "validation_method": "Comparison to ground-truth hidden rules in simulation; human evaluation for chain-of-thought and experimental fidelity.",
            "limitations_challenges": "Simulations can be oversimplified and permit hypothesis leakage from pretraining; building realistic, expressive action spaces is expensive; current sims often evaluate evidence-interpretation rather than full agent-driven experiment design.",
            "benchmark_dataset": "ScienceWorld (fifth-grade science tasks), DiscoveryWorld experimental suites, puzzle environments (He et al. 2024).",
            "uuid": "e4473.10",
            "source_info": {
                "paper_title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "rating": 2,
            "sanitized_title": "meteor_an_automatic_metric_for_mt_evaluation_with_improved_correlation_with_human_judgments"
        },
        {
            "paper_title": "Bleu: a method for automatic evaluation of machine translation",
            "rating": 2,
            "sanitized_title": "bleu_a_method_for_automatic_evaluation_of_machine_translation"
        },
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries",
            "rating": 2,
            "sanitized_title": "rouge_a_package_for_automatic_evaluation_of_summaries"
        },
        {
            "paper_title": "True detective: A deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4",
            "rating": 2,
            "sanitized_title": "true_detective_a_deep_abductive_reasoning_benchmark_undoable_for_gpt3_and_challenging_for_gpt4"
        },
        {
            "paper_title": "Thinking like a skeptic: Defeasible inference in natural language",
            "rating": 2,
            "sanitized_title": "thinking_like_a_skeptic_defeasible_inference_in_natural_language"
        },
        {
            "paper_title": "Automatic benchmarking of large multimodal models via iterative experiment programming",
            "rating": 2,
            "sanitized_title": "automatic_benchmarking_of_large_multimodal_models_via_iterative_experiment_programming"
        },
        {
            "paper_title": "Turtlebench: Evaluating top language models via real-world yes/no puzzles",
            "rating": 2,
            "sanitized_title": "turtlebench_evaluating_top_language_models_via_realworld_yesno_puzzles"
        },
        {
            "paper_title": "Beyond instruction following: Evaluating inferential rule following of large language models",
            "rating": 2,
            "sanitized_title": "beyond_instruction_following_evaluating_inferential_rule_following_of_large_language_models"
        },
        {
            "paper_title": "Causejudger: Identifying the cause with llms for abductive logical reasoning",
            "rating": 2,
            "sanitized_title": "causejudger_identifying_the_cause_with_llms_for_abductive_logical_reasoning"
        },
        {
            "paper_title": "Large language models as biomedical hypothesis generators: A comprehensive evaluation",
            "rating": 2,
            "sanitized_title": "large_language_models_as_biomedical_hypothesis_generators_a_comprehensive_evaluation"
        },
        {
            "paper_title": "Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents",
            "rating": 2,
            "sanitized_title": "discoveryworld_a_virtual_environment_for_developing_and_evaluating_automated_scientific_discovery_agents"
        },
        {
            "paper_title": "Scienceworld: Is your agent smarter than a 5th grader?",
            "rating": 1,
            "sanitized_title": "scienceworld_is_your_agent_smarter_than_a_5th_grader"
        }
    ],
    "cost": 0.02044425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models
24 Aug 2025</p>
<p>Kaiyu He kaiyu.he@utdallas.edu 
Department of Computer Science
University of Texas at Dallas</p>
<p>Zhiyu Chen zhiyu.chen2@utdallas.edu 
Department of Computer Science
University of Texas at Dallas</p>
<p>From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models
24 Aug 20258F37A326FD3062241B8C2D95CF9FF566arXiv:2505.21935v2[cs.AI]https:openreview. netforum? id= d7W38UzUg0
Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge.In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world.Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery.We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps.By unifying these threads, we illuminate how LLMs might evolve from mere "information executors" into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.</p>
<p>Introduction</p>
<p>One major pillar of human intelligence is the capacity to discover hypotheses and learning rules.We call this capability hypothesis discovery (or rule learning).Earlier AI systems struggled with it because formal symbolic methods lacked the commonsense background needed for inventive rule formation (Yu et al., 2024a).Recent advances in natural language processing (NLP) have produced LLMs pretrained on extensive text corpora that embed substantial commonsense knowledge.These models now enable tasks that demand rich background knowledge, such as formulating new hypotheses and deriving novel conclusions.</p>
<p>Hypothesis discovery inherently relies on a blend of reasoning that includes abduction, induction, and deduction, each defined differently by various scholars.For instance, Gilbert H. Harman considers induction to be a special case of abduction, describing it as "inference to the best explanation" (IBE) (Harman, 1965;Douven, 2021).However, while this definition is easy to understand, it oversimplifies key aspects of hypothesis discovery.In particular, the notion of the "best" explanation is ambiguous and often requires additional assumptions that vary by context.Moreover, this framework does not fully capture real-world scenarios, where a "best" explanation is rarely reached immediately; rather, we continually experiment, gather new observations, and refine our hypotheses.Based on these considerations, we adopt Charles Peirce's definition of hypothesis discovery and reasoning, which posits that hypothesis discovery begins with forming an explanatory hypothesis to explain observations through abduction, proceeds with iteratively apply hypothesis to solve problem or derive new knowledge with deduction, and validate hypothesis through induction (Frankfurt, 1958;Peirce, 1974;Burks, 1946;Minnameier, 2004) (See explanation in Figure 2).Figure 1: Taxonomy for Hypothesis Discovery with LLMs.Our survey categorizes work into four topics based on Peirce's definition of hypothesis discovery: Generation (creating hypotheses that explain given observations with abduction), Application (deducing new observations from established hypotheses with deduction), Validation (verifying and refining hypotheses against new evidence with induction), and Integrated Hypothesis Discovery (examining the dynamic interdependencies among these components in a continuous, iterative process).</p>
<p>The rest of the survey is organized as follows.Section 2 presents background knowledge on hypothesis discovery using LLMs, including different forms of reasoning and representations involved in the process.Section 3 examines prior surveys on LLM reasoning and hypothesis discovery, highlighting their narrow emphasis on deductive tasks or application-specific methods.Section 4 reviews methods for forming hypotheses (Abduction).Section 5 then covers approaches for applying these hypotheses (Deduction), and Section 6 focuses on techniques for validating given hypotheses with new observations (Induction).Finally, Section 7 explores the entire hypothesis-discovery cycle by examining the interdependencies among these reasoning steps and showing how abduction, deduction, and induction can be iteratively used to refine more robust hypotheses.For each stage, we discuss methods, benchmarks, evaluations, and identify limitations and future directions.A high-level taxonomy guiding this survey is shown in Figure 1.</p>
<p>Figure 2: On the left-hand side, a) illustrates Peirce's framework for hypothesis discovery through abduction, deduction, and induction.The process begins with abduction, which generates explanatory hypotheses based on an initial set of observations.Deduction is then used to apply these hypotheses and derive predictions.Induction evaluates how well the predicted observations align with actual outcomes, updating the confidence of the hypotheses or rejecting those that are no longer valid.This process is iterative: validated hypotheses may be refined through further rounds of abduction using updated observations, gradually leading to more robust theories.On the right-hand side, b) provides a simple example that illustrates this process.</p>
<p>Background</p>
<p>Before LLMs, most AI systems stored knowledge as handcrafted symbols and rules.That format works well for deduction, because most of the problems we need to solve with symbolic AI systems work with limited premises and countable task-specific knowledge; for example, questions in the ProofWriter (Tafjord et al., 2021) and FOLIO (Han et al., 2024) benchmarks are limited to fewer than a hundred premises.However, abduction and induction are different: they call for generating and validating many tentative explanations inspired by vast commonsense or expert domain knowledge (such as weather patterns, social norms, or physics) and for updating beliefs as new observations arrive.Handling these reasoning tasks with symbolic AI meant writing and maintaining vast, interlocking rule bases, an effort so costly that few projects moved beyond toy domains (Yang et al., 2024c).Consequently, the research landscape remained dominated by deductive tasks (Yu et al., 2024a;Liu et al., 2025;Huang &amp; Chang, 2023).</p>
<p>LLMs have transformed this landscape.Trained on vast corpora, they implicitly absorb broad commonsense and domain knowledge, exhibiting strong reasoning abilities on complex, natural-language tasks (Yang et al., 2024c).With a simple text prompt, we can now ask them to carry out abduction or induction and even inspect their intermediate reasoning steps (Li et al., 2024;Jung et al., 2022), exposing the latent information they rely on.This advancement has made it practical to study and deploy defeasible reasoning (Yang et al., 2024c).Defeasible reasoning refers to forms of reasoning, such as abduction and induction, that yield probable conclusions that remain open to revision as new evidence emerges.This shift has fueled a wave of NLP research that places such flexible reasoning at the heart of AI progress (Liu et al., 2025;Huang &amp; Chang, 2023).</p>
<p>Hypothesis Discovery</p>
<p>Hypothesis discovery or Rule learning, the cyclical process of formulating hypotheses, gathering evidence, validating or refuting them, and ultimately establishing robust theories, lies at the heart of scientific progress (Eger et al., 2025).Early humans, for example, hypothesized that the Earth was flat based on everyday observations.Later, Eratosthenes measured shadow angles at different locations, obtaining evidence that suggested the Earth's surface was curved.This evidence challenged the flat Earth hypothesis, and subsequent findings, notably Magellan's circumnavigation, conclusively confirmed the Earth's roundness.Even with to-day's sophisticated instruments, researchers continue to iterate this loop in new domains, validating and refining theories as new data emerges.Today, there is growing interest in whether LLMs can autonomously generate, apply, and validate hypotheses from natural language represented observations, mirroring this iterative process to achieve interpretable and adaptive hypothesis discovery.Although many studies have explored individual steps of hypothesis discovery, their efforts tend to be scattered across abduction, deduction, and induction, with insufficient attention to how these forms of reasoning interconnect to drive genuinely iterative, hypothesis-driven discovery.</p>
<p>Reasoning</p>
<p>Reasoning is central to hypothesis discovery.Researchers have historically debated how best to categorize reasoning into clear, operational types.Different frameworks each have strengths and limitations (Harman, 1965;Douven, 2021;Bacon, 1878;Laudan, 1971;Mill, 2024;Stadler, 2011;Popper, 2005;Okoli, 2023).In this survey, we adopt Charles Peirce's definition of reasoning (Peirce, 1974;Burks, 1946;Minnameier, 2004), emphasizing abduction, deduction, and induction as separate but interrelated processes.We choose Peirce's framework for three main reasons.First, clarity: Unlike many other approaches, Peirce explicitly differentiates among the three reasoning types, preventing confusion, such as the common conflation of abduction and induction.Second, practicality: Peirce's model aligns each form of reasoning directly with a distinct phase in the hypothesis discovery cycle-abduction for generating hypotheses, deduction for applying these hypotheses, and induction for validating them.This clear mapping makes his framework particularly suitable for systematically studying the entire process of hypothesis discovery, rather than isolated reasoning components.Finally, granularity: Peirce's framework breaks down the scientific discovery process into welldefined, finer-grained steps, facilitating detailed analysis and enabling more structured evaluation.</p>
<p>Abductive Reasoning is the process of forming explanatory hypotheses to make sense of observed phenomena.It is the only form of reasoning that generates entirely new ideas or explanations (Peirce, 1974;Frankfurt, 1958).Given a set of observations, one uses creative thinking and recalls necessary knowledge to come up with hypotheses that plausibly explain these observations.Importantly, a single set of observations can lead to multiple possible explanations.For instance, if you come home and find the floor wet, you might form several possible explanations: perhaps a pipe leaked, or someone spilled water accidentally.Without additional evidence or testing, you can't know for sure which explanation is correct.This illustrates how abduction helps generate potential explanations, which then must be tested further.</p>
<p>Inductive Reasoning is the process of testing whether the hypothesis and deduced consequences really obtain and evaluating to what extent they obtain (Minnameier, 2004;Peirce, 1974).In practice, induction updates a hypothesis's confidence based on new observations, including rejecting it outright, or selects the most convincing candidate from a set of competing hypotheses.Consider the claim "Swans are (100%) white," or linguistically, "All swans are white," formed after observing 99 white swans in Texas.Encountering a black swan in New York contradicts that hypothesis.Through induction, we recognize this contradiction and lower our confidence in the original claim, adjusting it to "Swans are (99%) white," linguistically expressed as "Almost all swans are white."In this example, although the hypothesis appears "revised," the change is limited to its confidence level; no new explanatory perspective is introduced, and we do not actually form a new hypothesis.By contrast, abduction can lead us to a fresh explanatory hypothesis with new observations, e.g., "Swans' color depends on their habitat," or "All swans in Texas are white," which introduces new ideas and is not a case of induction.Thus, inductive reasoning verifies or refines existing hypotheses (in terms of confidence) based on accumulating evidence.</p>
<p>Deductive Reasoning is the process of logically deriving specific conclusions from general hypotheses or rules.If the initial hypotheses are true, deduction guarantees that the derived conclusions must also be true.For instance, from the general rule "All swans are white" and the observation "This bird is a swan," we logically conclude "This bird must be white".While traditional deductive reasoning tasks, such as instruction-following and standard problem-solving, have been extensively studied with LLMs (Pan et al., 2023;Wei et al., 2022;Liu et al., 2025;Huang &amp; Chang, 2023), deductive reasoning in the context of hypothesis discovery poses unique challenges.Specifically, it emphasizes inferential rule-following, requiring models to consistently apply hypotheses or rules to derive new and potentially unfamiliar conclusions, even when these hypotheses are counterfactual, unfamiliar, or incorrect.For example, when a flawed hypothesis is introduced in an unfamiliar domain, inferential rule-following requires us to strictly derive its predicted consequence, even if that consequence itself is incorrect.By comparing this consequence with experimental data, we can directly assess the hypothesis's validity and guide its revision.Conversely, if the deductive process is unreliable, we may overlook real contradictions and thus retain invalid hypotheses or discard valid ones.Indeed, recent work shows that although LLMs can demonstrate strong deductive performance on indistribution tasks, they rely heavily on surface-level pattern matching and fail to generalize their inferential rule-following to novel or counterfactual scenarios (Pu et al., 2025;Mirzadeh et al., 2024;Kang et al., 2024;Yan et al., 2025).</p>
<p>There are also other types of reasoning, such as analogical reasoning (Yuan et al., 2023;Jiayang et al., 2023).However, their function in hypothesis discovery is generally covered by abduction and induction.We will include these additional forms when we encounter a relevant case in the following section.</p>
<p>Rule Representation: Formal Language vs Natural Language</p>
<p>Table 1: Comparison of Natural vs. Formal Language Representations for the Hypothesis "Sam is a dragon".In natural language, commonsense knowledge is implicitly embedded, and derived knowledge relies on extensive commonsense, potentially resulting in different interpretations depending on background knowledge and context.In formal languages (e.g., FOL or code), the knowledge base must be defined explicitly and cannot fully capture all commonsense knowledge, however, the derived conclusions are deterministic and precise. . . .</p>
<p>Representation</p>
<p>Sam.fly()</p>
<p>There are many ways to represent hypotheses and rules, which we broadly divide into two categories: formal languages (FL) and natural languages (NL).Formal languages, such as first-order logic and programming languages, are systematic and rule-bound.After real-world entities are encoded as explicit literals, precise inference rules yield provably correct and sound conclusions, making these systems well suited to deductive reasoning.Yet the encoding process strips away many subtle semantic relationships and commonsense knowledge, limiting the system's ability to handle the creative, defeasible reasoning required for abduction and induction (McCarthy &amp; Hayes, 1981;Reiter, 1980;Hanks &amp; McDermott, 1987;Liu et al., 2025;Yu et al., 2024a;Huang &amp; Chang, 2023).Natural language preserves those nuances and aligns more closely with human cognition, so it is better suited to abductive and inductive tasks.However, its meanings are implicit and context-dependent, making it difficult to define a deterministic reasoning pipeline and reducing the reliability of the resulting inferences (See in Table 1).Accordingly, the following sections treat formal-language and natural-language approaches separately, emphasizing how their reasoning methods and evaluation protocols differ.</p>
<p>Related Surveys</p>
<p>Most existing work assessing LLM reasoning, both survey syntheses and popular benchmarks such as GSM8K (Cobbe et al., 2021), centres almost exclusively on multi-step deductive tasks, leaving abduction and induction, the engines of hypothesis discovery, largely unexplored.Surveys of the field Yu et al. (2024a); Liu et al. (2025); Huang &amp; Chang (2023) highlight the absence of systematic study and clear analytical frameworks for these modes, while benchmark analyses likewise show that abductive and inductive inference receive limited attention (Plaat et al., 2024;Li et al., 2025b).This imbalance obscures our understand-ing of whether, and to what extent, LLMs can perform the creative, evidence-based reasoning required for hypothesis-driven discovery.</p>
<p>On the other hand, research in the AI for Science domain takes a distinctly horizontal, application-driven approach.This body of work emphasizes practical tasks such as generating research ideas, conducting experiments, and synthesizing reports, often employing domain-specific pipelines tailored to individual scientific fields.However, these studies usually lack a generalizable reasoning framework applicable across different scientific contexts.Furthermore, their evaluation metrics, typically novelty, creativity, or consistency, tend to be subjective, human-centric, and thus difficult to generalize, offering limited theoretical insight into the underlying reasoning mechanisms involved in scientific discovery (Movva et al., 2025;Alkan et al., 2025;Reddy &amp; Shojaee, 2025;Gridach et al., 2025;Bazgir et al., 2025).</p>
<p>Our survey adopts a vertical, reasoning-centered perspective grounded in Peirce's classical framework.</p>
<p>It integrates three modes of reasoning into a unified view of hypothesis discovery: abduction for hypothesis generation, deduction for hypothesis application, and induction for hypothesis validation.Unlike prior surveys that emphasize primarily deductive tasks, we concentrate on the entire reasoning process involved in hypothesis discovery, explicitly covering both defeasible reasoning (abduction and induction) and deductive reasoning.By clearly defining each reasoning mode and explaining its role within each stage of the discovery process, we provide a structured basis for designing principled, model-agnostic benchmarks and evaluation tasks.Compared to existing application-oriented surveys, our framework thus offers a more abstract, systematic, and theoretically informed approach to understanding and enhancing the role of LLMs in automated scientific discovery.</p>
<p>Hypothesis Generation</p>
<p>Every scientific discovery begins with a set of observations, denoted as O = {o 1 , o 2 , . . ., o n }, that we aim to explain.Let h represent the generated explanation or hypothesis.The hypothesis generation task can be defined as generating an h such that:
h |= (o 1 ∧ o 2 ∧ • • • ∧ o n )
This notation means that h logically entails the observations.In other words, assuming h holds, it guarantees that all observations
o 1 ∧ o 2 ∧ • • • ∧ o n follow.
In this survey, we follow Peirce's definitions for reasoning.Accordingly, the primary process used in hypothesis generation is abduction, the method of formulating explanatory hypotheses to account for observed phenomena.</p>
<p>Method</p>
<p>Despite LLMs' demonstrated prowess in tasks like summarization or code generation, devising robust methods to guide them in hypothesis generation remains an active area of research.Recent work has sought to leverage LLMs' in-context learning and natural language understanding to produce novel or domain-specific hypotheses, spurring the development of new techniques aimed at improving both the quality and applicability of generated hypotheses (Yang et al., 2024c).In this section, we review these methods, spanning approaches that rely solely on prompting, those that integrate external knowledge sources, and those that incorporate human expertise in the loop.</p>
<p>Natural Language Hypothesis Generation with LLMs</p>
<p>Prompt-Based Methods: Due to the lack of large-scale, domain-specific data for hypothesis generation, most abduction approaches rely on prompt-based methods that are easy to deploy and don't require extensive additional data.For instance, when provided with observations expressed in natural language and asked to generate a plausible hypothesis that explains them, both Wiegreffe et al. (2022) and Qi et al. (2024) employ few-shot prompting to guide LLMs in generating hypotheses.Specifically, Wiegreffe et al. (2022) constructs few-shot examples using a triplet format (question, answer, explanation).In solving a task of generating biomedical hypotheses with given observations, Qi et al. (2024) embeds a small set of independent observation-to-hypothesis pairs in the prompt.By showing how each block of biomedical background observations maps to its corresponding hypothesis, the model learns to extract relevant domain cues and generate novel biomedical hypotheses.Their findings indicate that including more examples in the prompt tends to reduce the novelty of the generated hypotheses while increasing their correctness.Furthermore, Yang et al. (2024a) propose a pipeline for hypothesis generation that involves five prompt-based modules: one to generate hypotheses, one to test deductive consistency, one to verify that the hypothesis is not merely a copy of the given context, one to assess its generalizability, and one to determine whether the hypothesis is trivial.</p>
<p>RAG-Based Methods:</p>
<p>Labeling massive corpora for pre-training is costly, but assembling a small or medium dataset for Retrieval-Augmented Generation (RAG) is practical, and several studies follow a similar iterative three-step pattern: (i) retrieve task-specific documents, (ii) let an LLM generate or refine hypotheses, and (iii) iterate with LLM feedback.For instance, after a user supplies a seed paper and asks the LLM to generate a worthwhile hypothesis to pursue in research, Hu et al. (2024) query the Scholar API for related work, then repeatedly generate and critique hypotheses, gradually expanding a web of novel ideas.Yang et al. (2025) apply the same loop to 51 top-tier chemistry papers from 2024: experts first segment each paper into background, inspiration, and hypothesis; an LLM-based multi-agent system (MOOSE-Chem) then retrieves relevant snippets, drafts hypotheses, and scores them for originality.A similar pipeline appears in Yang et al. (2024b), where 50 conference papers are annotated in the same three fields, augmented with thematically similar web documents and 14 survey papers so that the LLM can judge both relevance and novelty.</p>
<p>Two variants enrich the retrieval step with structured or fine-tuned knowledge.Xiong et al. (2024) ground each hypothesis in a domain knowledge graph: entities mentioned during generation are checked against graph relations, ensuring the final claims remain fact-consistent.In contrast, Chai et al. (2024) fine-tune a T5 model (Raffel et al., 2020) on curated scientific abstracts and, during inference, retrieve citation contexts and related data; a novelty-guided loop then re-generates until the candidate is both coherent and inventive, outperforming standard transformer baselines.</p>
<p>Human-in-the-loop Hypothesis Generation with LLM:</p>
<p>Recent studies show that combining humans with LLM support yields higher-quality, more novel hypotheses than either party working alone.The quality of natural-language hypothesis generation largely depends on the inherent capabilities of LLMs.Because these models excel at in-context learning, prompt strategies such as Chain-of-Thought (CoT) and Reflexion (Wei et al., 2022;Shinn et al., 2023) can be applied directly to this task.However, unlike computervision research, which gained rapid momentum from the ImageNet benchmark, hypothesis generation lacks a comparable, widely recognized task set.The main challenge is therefore the absence of a reliable evaluation task and benchmark for natural-language hypotheses, an issue examined further in Section 4.2.</p>
<p>Formal Language Hypothesis Generation with LLM</p>
<p>One major advantage of formal hypotheses is that once a formal language hypothesis is obtained, we can directly perform inference on it with guarantees of soundness and correctness.Depending on whether observations are represented in formal or natural language, methods for proposing a formal language hypothesis need to be discussed separately.</p>
<p>Formal Language Observations:</p>
<p>When observations are encoded in a formal language, dedicated formal language solvers typically yield clear, white-box solutions that outperform language models.Consequently, using an LLM for these tasks is generally not preferred.Nevertheless, a few early studies in the LLM era have explored this approach.For example, Young et al. (2022) 2025) first ask the model for a single-word "main concept," then use that concept to steer subsequent code generation, avoiding the similarity of low-temperature outputs and the degeneration of high-temperature sampling while still producing coherent hypotheses.</p>
<p>A complementary line of work probes the model's internal representations.Using sparse autoencoders (SAE) (Bricken et al., 2023), Movva et al. (2025) isolate neurons activated when the LLM predicts the click rate of Twitter posts and discover that neurons associated with "surprise" or "shock" positively influence the score, supporting the hypothesis that surprising or shocking content tends to receive more clicks.</p>
<p>Evaluation for Hypothesis Generation</p>
<p>Due to LLMs' strong reasoning abilities and natural language interface, many methods have been proposed for hypothesis generation, and numerous ideas based on everyday human reasoning can be adapted for this purpose (Niu et al., 2024).However, a major challenge remains in establishing a grounded and convincing way to evaluate the quality of the generated hypotheses.</p>
<p>Natural Language Hypothesis Evaluation</p>
<p>Although prompting LLMs to generate natural language hypotheses is straightforward, evaluating the quality of these hypotheses is challenging due to the ambiguity inherent in natural language representations.Consequently, a common evaluation method involves either human evaluation or using an LLM to assess the generated hypotheses' validity (Zhao et al., 2024;Yang et al., 2024b;Hu et al., 2024;Qi et al., 2024;Yang et al., 2025).While human evaluation can provide valuable insights without relying on predefined answers, it is inherently subjective, less reproducible, expensive, and sometimes not entirely convincing.Therefore, alternative evaluation strategies are needed.</p>
<p>Implicit Prediction-based Evaluation:</p>
<p>Early benchmarks often relied on question-answering (QA) tasks that required the model to implicitly form a hypothesis to answer a question (Sinha et al., 2019;Weston et al., 2015).For example, consider the observation: "Lily is a swan, Lily is white, Bernhard is green, Gerg is a swan.What color is Greg?"To answer correctly, one must infer an implicit hypothesis, such as "All swans are white" or "Most swans are white," based on the fact that Lily is both a swan and white.Thus, the correct answer is "white."By verifying whether the model's answer is "white," one can indirectly assess its ability to form an appropriate hypothesis and perform reasoning.Similarly, recent work shows that prompting LLMs to generate an intermediate hypothesis and then using that hypothesis for inference yields higher performance on complex tasks (Balepur et al., 2024;Shi et al., 2023;Wang et al., 2025).However, this approach is problematic: the hypothesis may be formed incorrectly, the subsequent inference could be flawed, and the model might arrive at the correct answer through memorization or random guessing rather than proper abductive reasoning.Therefore, success in these tasks does not directly imply that the model possesses superior abductive capabilities, making them unsuitable for reliably evaluating hypothesis generation.</p>
<p>Ground Truth-based Evaluation: Some studies build benchmarks with labeled hypotheses so that outputs of LLM can be matched directly against references.DEER (Yang et al., 2024a) supplies 1,200 fact-rule pairs, all written in natural language by experts across six topics-zoology, botany, geology, astronomy, history, and physics.Generated hypotheses are compared with the gold rules using token-level mapping metrics like METEOR (Banerjee &amp; Lavie, 2005).In biomedicine, Qi et al. ( 2024) curate a benchmark with both seen and unseen samples: the seen split contains 2,700 background-hypothesis pairs collected before January 2023, whereas the unseen split has 200 pairs collected after that date.Outputs are evaluated against the ground truth with BLEU and ROUGE (Papineni et al., 2002;Lin, 2004)</p>
<p>Formal Language Hypothesis Evaluation</p>
<p>Unlike natural language hypotheses, formal hypotheses evaluations are more grounded due to their clarity and unambiguous semantics.</p>
<p>Ground Truth-based Evaluation: Generated formal hypotheses can be evaluated against pre-defined ground truth hypotheses.Unlike natural language evaluation, where ground truth is often written by domain experts and evaluated using token-level metrics like BLEU or ROUGE, formal hypotheses can be evaluated procedurally using solvers.This allows us to verify correctness deterministically.For example, Bowen et al. (2024) designed formal representations for synthetic grouping tasks to evaluate formal language hypothesis generation.Hua et al. (2025)</p>
<p>Discussion and Future Directions in Hypothesis Generation</p>
<p>There exists a significant gap between formal and natural language approaches to hypothesis generation.</p>
<p>In natural language hypothesis generation, observations typically originate from recent research papers, and generated hypotheses can potentially inspire novel research ideas with tangible real-world impacts (Eger et al., 2025).However, rigorous and reliable evaluation methods for such hypotheses remain underdeveloped.Token-based metrics, such as BLEU or ROUGE, do not effectively capture the qualitative aspects of openended hypothesis generation (Yang et al., 2024b).Meanwhile, alternative approaches involving human or LLM-based evaluations are costly, subjective, and prone to inconsistencies.</p>
<p>Conversely, formal language hypothesis generation benefits from grounded, objective evaluation methods.Nevertheless, existing formal tasks often involve simplified or artificial scenarios that fail to reflect the complexity and nuance inherent in real-world applications.Consequently, the field faces a trade-off: formal representations facilitate robust evaluation but risk omitting critical real-world nuances, while natural language representations capture real-world complexity yet lack rigorous evaluation mechanisms.</p>
<p>To address this challenge, future research in hypothesis generation could focus on two key directions.Firstly, there is an urgent need to develop novel evaluation methodologies tailored specifically for natural language hypothesis generation.Current implicit prediction-based evaluations suffer from inherent limitations, and ground truth-based evaluations remain inadequate due to reliance on token-level similarity metrics.Alternative evaluation strategies, potentially involving multi-dimensional human assessments, structured feedback mechanisms, or hybrid evaluation frameworks integrating automated and expert evaluations, merit exploration.Secondly, bridging the gap between formal and natural language hypothesis generation is crucial.</p>
<p>Leveraging code as an intermediate representation offers a promising path forward, combining evaluative rigor with expressive capability.However, existing code-based hypothesis generation benchmarks tend to focus on oversimplified problems that lack relevance to practical scenarios.Thus, developing realistic, code-based hypothesis-generation tasks grounded in established research papers, real-world datasets, and open-source repositories presents a compelling and valuable direction for future research (Chen et al., 2024).</p>
<p>Hypothesis Application</p>
<p>Given a hypothesis h, hypothesis application is defined as the derivation of a new observation o new such that:
h |= o new
In some cases, the hypothesis may depend on a context c, so that h can be viewed as a function of c.In this context-dependent formulation, hypothesis application is defined as deriving a new observation o new such that:
h(c) = o new
In our work, we follow Peirce's definitions for reasoning.Accordingly, the primary process used in hypothesis application is deduction, the method of deriving necessary consequences from a given hypothesis.</p>
<p>Notably, when a hypothesis is expressed in a formal language, directly applying it with a deterministic solver yields a correct and sound prediction.Therefore, there is little motivation to leverage LLMs for deductive reasoning on formal hypotheses.This section, consequently, focuses on the natural language hypothesis application and evaluation.2024) treat LLMs as formal language parsers, using them to translate natural language hypotheses into formal representations like FOL and code before applying a formal inference procedure.This translation significantly improves deductive correctness.However, these methods have primarily been evaluated on benchmarks such as ProofWriter (Tafjord et al., 2021) and FOLIO (Han et al., 2024), where the questions are already closely aligned with formal language.For example, given the input "Fact1: Eric is young, Fact2: Dave is white, Rule 10: if someone is young and not kind then they are big", translating this into FOL is relatively straightforward.It remains unclear whether LLMs can reliably parse more complex, everyday natural language into formal representations.</p>
<p>Method</p>
<p>LLM as Formal</p>
<p>Fine-Tuning-Based Method: Fine-tuning is a common approach to improve model performance when corresponding training data is available.Sun et al. (2024) proposed a synthetic "StringGame" task in which ground truth hypotheses and answers are provided.Leveraging the CoT approach, a LLM is prompted to generate multiple candidate hypothesis application trajectories along with their results.By comparing these results with the ground truth, the trajectories that produce correct outcomes are identified as correct and stored for fine-tuning.The resulting fine-tuned model then demonstrates improved performance in both hypothesis application and instruction following.</p>
<p>Prompt-Based Method: Although CoT prompting has improved performance on multi-hop question answering tasks, Sun et al. (2024) found that it does not directly enhance performance in hypothesis application.Therefore, new prompting methods have been designed specifically for this purpose.Inspired by mathematical induction, Cai et al. (2025) propose quantifying the difficulty of a question so that the LLM can solve it incrementally, from simpler versions to more complex ones, ultimately arriving at the correct answer.In another approach, Ling et al. ( 2023) design a pipeline that supervise the correctness of each reasoning step during hypothesis application.First, the LLM indexes all premises; then it is asked to label the minimal set of premises required to derive new facts.This pipeline generates multiple candidate hypothesis application trajectories, and by having the LLM vote on each step, the most convincing deductive trajectory is selected.</p>
<p>Evaluation for Hypothesis Application</p>
<p>Although many benchmarks and evaluation methods exist for general deductive reasoning, such as question answering and mathematical tasks like GSM-8k (Cobbe et al., 2021), these question types do not explicitly test the formation of new facts based on given hypotheses or rules.Evaluating the correctness of a naturallanguage deductive trajectory is challenging because annotated reasoning paths for hypothesis application are scarce, and the same result can follow from different reasoning paths.As a result, most evaluations use prediction-based checks.We assume that, given a correct hypothesis and a known ground-truth result, a valid deduction will reproduce that result.By comparing the model's deduced outcome with the ground truth, we can judge whether its deduction is correct.For example, take the hypothesis "Coin flips are independent and identically distributed (i.i.d.) with a 50 percent chance of heads."When asked, "After three consecutive heads, what is the probability of a tail on the fourth flip?," a flawed model might claim the chance of a tail has increased.In fact, under the i.i.d.assumption, the probability remains 50 percent.Supplying the correct hypothesis and comparing the model's answer to the true result lets us evaluate whether its deductive reasoning is valid.</p>
<p>Building on this idea, Yu et al. (2024b) create the TURTLEBENCH benchmark, inspired by the "Turtle Soup" game in which players deduce a story's hidden explanation by asking yes/no questions; in TURTLEBENCH, the LLM instead answers human-annotated questions with "True," "False," or "Not Relevant," across 1,532 high-quality question-answer pairs sourced from an online platform to test whether it can fully follow a story and provide accurate answers.Similarly, Mu et al. (2024) introduced the RULES benchmark, comprising 14 rule-following scenarios, each paired with concise test cases and programmatic evaluation functions that objectively assess adherence to specified rules.In addition, Cai et al. (2025) presented the Holiday Puzzle benchmark, which features multiple holiday schedule scenarios ranging from simple single-week planning to multi-phase arrangements and complex date arithmetic tasks, again using test cases and evaluation functions to verify correct computation of extra holiday rest days under provided rules.Moreover, Sun et al. (2024) constructed RuleBench to evaluate not only whether models can produce correct answers based on factual rules but also their ability to apply counterfactual rules, designed to yield incorrect outcomes, and experiments show that while LLMs achieve near-perfect accuracy on factual rules, their performance drops dramatically under counterfactual rules, revealing a significant gap in counterfactual rule-following capability.</p>
<p>Discussion and Future Directions in Hypothesis Application</p>
<p>While traditional deductive reasoning tasks (e.g., question answering, problem solving) in LLMs have been widely studied, the capability for hypothesis application remains significantly underexplored.According to Sun et al. (2024), hypothesis application involves inferential rule-following, requiring models to consistently apply given hypotheses to derive novel knowledge in unfamiliar domains.Robust hypothesis application is critical to hypothesis discovery, as hypotheses must generalize to scenarios with unseen observations.However, existing LLMs frequently struggle to extend hypotheses beyond familiar contexts, thus limiting the evaluation of hypothesis generation.</p>
<p>Future research could therefore focus on rigorously evaluating LLMs' hypothesis application, both factual and counterfactual, in novel scenarios.Developing benchmarks explicitly designed for hypothesis-driven inference in unfamiliar domains could reveal important insights into model adaptability and generalization.</p>
<p>Additionally, current evaluations of hypothesis application mainly rely on outcome-based correctness, comparing predicted results to ground truth given correct hypotheses.However, incorrect reasoning may still lead to correct predictions in natural-language contexts.Although Ling et al. (2023) propose improving hypothesis application by intervening in reasoning trajectories, a large-scale benchmark specifically designed to evaluate trajectory-based hypothesis application remains absent.</p>
<p>Hypothesis Validation</p>
<p>According to Peirce, induction validates a hypothesis by updating its confidence when new evidence appears.However, in studies that focus exclusively on induction, tasks are typically one-off: a hypothesis (or set of hypotheses) and a collection of observations are provided, and there is no iterative updating of confidence.</p>
<p>A simplified framework for hypothesis validation treats it as a multiple-choice problem: given observations O = {o 1 , o 2 , . . ., o n } and a set of hypothesis H = {h 1 , h 2 , . . ., h m }, the model selects the most possible hypothesis.In simpler scenarios, where only one hypothesis is provided, the model determines whether the hypothesis correctly explains the observations.In the next section, when combined with deduction and abduction, induction can subsequently be used to iteratively update the confidence in the hypothesis.</p>
<p>Natural language representations add significant complexity to induction.In formal language settings, all necessary information is explicitly provided, and reasoning follows rigorous, well-defined steps.In contrast, validating a natural language hypothesis often requires commonsense knowledge and interpretation of nuanced language.For example, consider the observations "Neil wanted to see the mountains of Asia" and "Neil loved being so close to the mountains in Nepal," with candidate hypotheses "Neil booked a trip online" and "Neil took a trip to see the Rocky Mountains instead."Here, the nuanced meaning of the term "instead" and the geographic relationships require careful analysis and may lead to different conclusions.Indeed, Zhao et al. (2023) reports that, when verifying their dataset where five annotators judged the plausibility of handwritten hypotheses, disagreements occurred in 62.34% of 1,365 explanations, underscoring the challenge of natural language hypothesis validation.</p>
<p>Method</p>
<p>Formal Language Hypothesis Validation</p>
<p>He &amp; Lu (2024) introduce the CauseJudger framework, which leverages LLMs at every stage to validate candidate hypotheses.First, an LLM transforms the natural language inputs into an FOL-based representation by integrating each candidate hypothesis into the premises.Next, an LLM filters out irrelevant premises and rules.Finally, another LLM performs forward reasoning to decide which hypothesis explains the observations.</p>
<p>Natural Language Hypothesis Validation</p>
<p>Prompt-Based Method: Lampinen et al. (2022); Sun et al. (2024) employ a few-shot prompting approach for hypothesis validation.In this method, case triplets, consisting of an observation, a hypothesis, and its corresponding validity, are provided to the model, which then answers a hypothesis validation question.</p>
<p>Although this approach improves performance, Sun et al. (2024) reports that the performance boost is limited.Their experiments further indicate that fine-tuning outperforms few-shot prompting.</p>
<p>Fine-Tuning-Based Method: Since hypothesis validation essentially constitutes a classification problem, many Natural Language Inference (NLI) datasets can be adapted into hypothesis validation tasks.Consequently, fine-tuning is a popular method in this context.For example, Zhao et al. (2023); Chan et al. (2023); Sun et al. (2024) fine-tune models to select the correct hypothesis from a set of hypotheses based on new observations.</p>
<p>Evaluation for Hypothesis Validation</p>
<p>Formal Language Evaluation</p>
<p>Along with the CauseJudger framework, He &amp; Lu (2024) also proposed the CauseLogics dataset.Based on the required formal reasoning depth, the dataset is divided into four difficulty levels for hypothesis validation tasks, with 50,000 samples per level.Each hypothesis is assigned a binary ground-truth label indicating whether it correctly explains the observations.</p>
<p>Natural language Evaluation</p>
<p>Binary-Classification-Based Evaluation: Lampinen et al. ( 2022) chose a subset of 40 tasks from the crowd-sourced benchmark BIG-bench (bench authors, 2023) and constructed their own benchmark specifically for hypothesis validation.Each data sample consists of an observation, its corresponding hypothesis, and a ground truth label indicating whether the hypothesis truly explains the observation.</p>
<p>Hypothesis validation using natural language is inherently challenging because the implicit information and required common-sense background are not explicitly stated.This often leads different individuals to draw different conclusions when validating a hypothesis based solely on recalled information.Rudinger et al. (2020) mitigate this issue by adopting a different strategy.Instead of asking annotators to directly judge whether an observation explains a hypothesis, they ask the model to determine if a given observation weakens or strengthens the hypothesis.Specifically, they sample observation-hypothesis pairs from existing datasets and then manually craft two types of sentences: one that acts as a "strengthener" (increasing the likelihood of the hypothesis) and one that acts as a "weakener" (decreasing the likelihood of the hypothesis).</p>
<p>Their validation process showed that the strengthening and weakening effects are consistent across different annotators.During evaluation, the model is required to decide whether a new observation strengthens or weakens the hypothesis.This approach aligns with the paper's goal of modeling defeasible inference by leveraging explicit contextual updates rather than relying on potentially variable human interpretations of implicit information.Furthermore, Zhang et al. (2025) extended this task to include visual observations.In their extension, given a visual observation and a natural language hypothesis, an LLM is tasked to determine whether the provided sentence serves as a strengthener or a weakener.</p>
<p>Multiple-Choice-Based Evaluation</p>
<p>Bhagavatula et al. ( 2020) introduce the ART benchmark, comprising roughly 20k narrative contexts where each sample includes two time-ordered observations, one depicting a story's start (o 1 ) and the other its outcome (o 2 ), alongside two hypotheses: a plausible explanation (h + ) and a less plausible one (h − ), challenging models to choose the best explanatory hypothesis and enabling adaptation to hypothesis-generation tasks evaluated against ground-truth explanations.Similarly, Jiang et al. (2023) present the BRAINTEASER benchmark of about 1.1k lateral-thinking puzzles, each offering a question with multiple-choice answers, one that defies commonsense and several conventional distractors, in both sentence (narrative) and word (meaning-alteration) formats to test creative reasoning, with additional semantic and context reconstruction variants assessing reasoning consistency and robustness across formulations.Moreover, Del &amp; Fishel (2023) introduced the True Detective benchmark for deep hypothesis validation, featuring 191 long-form detective puzzles (≈ 1200 words each) from the "5 Minute Mystery" platform, where models (and humans) select the correct explanation from 4-5 options, human accuracy averages 47%, top solvers exceed 80%, and each puzzle includes golden chain-of-thought explanations detailing the reasoning steps that lead to the correct answer.</p>
<p>Discussion and Future Directions in Hypothesis Validation</p>
<p>Previous literature often conflates hypothesis generation and hypothesis validation, primarily due to ambiguity inherent in the IBE paradigm.Within IBE-based approaches, hypothesis validation typically appears as an implicit intermediate step, where selecting the "best" hypothesis is frequently based on unclear or subjective criteria without dedicated, independent evaluation.However, adopting Peirce's explicit distinction between abduction, deduction, and induction clearly separates validation from generation, underscoring the need for dedicated research on validating hypotheses against newly observed evidence.</p>
<p>Current validation methodologies predominantly adopt end-to-end metrics that only assess final correctness, neglecting the reasoning processes and commonsense knowledge required to validate hypotheses in realistic settings.The subjective nature of natural language, coupled with different interpretations of observations, highlights the necessity for richer evaluative frameworks.Future benchmarks should incorporate detailed intermediate Chain-of-Thought data, capturing explicit reasoning steps humans take when validating hypotheses, such as recalling relevant commonsense knowledge and performing nuanced inference.Evaluations should then emphasize consistency between the reasoning process and available commonsense context rather than relying solely on superficial similarity to reference answers.Such benchmarks would greatly enhance our understanding of hypothesis validation and better reflect the complexities of human-like reasoning.</p>
<p>Hypothesis Discovery</p>
<p>Although many works introduced in the previous sections propose methods and evaluation metrics, they mainly focus on individual phases of Hypothesis Discovery-Hypothesis generation (Abduction 4), Hypothesis application (Deduction 5), and Hypothesis validation (Induction 6).However, in real-life Hypothesis Discovery, these reasoning stages are not independent and must be treated holistically.Initially, we form hypotheses based on limited observations using abduction, which subsequently informs the application of these hypotheses through deduction, enabling the collection of further evidence.Concurrently, induction continuously evaluates and resolves inconsistencies arising between newly obtained observations and earlier hypotheses.This iterative interplay means that each hypothesis formulated, action taken, observation gathered, and inconsistency identified dynamically shapes and reshapes our evolving understanding, influencing subsequent reasoning steps and contributing to diverse interpretations of the world.Treating any single reasoning phase in isolation oversimplifies hypothesis discovery.For example, although Bowen et al. (2024) evaluated every reasoning, they handled each step separately and thus failed to assess the true rule-learning capability of LLMs.Consequently, integrating abduction, deduction, and induction into a unified learning loop remains both challenging and largely understudied, yet it is the ultimate goal for constructing end-to-end agents capable of scientific discovery.</p>
<p>Despite a few studies that acknowledge the interdependence among reasoning types and allow models to refine hypotheses iteratively, they still overlook two decisive aspects of real-world hypothesis discovery.First, most benchmarks remain static and passive: they hand agents a fixed set of observations deemed sufficient to reach the correct hypothesis, whereas real-life hypothesis discovery requires actively seeking additional evidence.Second, even in settings that allow proactive information gathering, the granularity of the action space is still too coarse: agents fetch observations via one-shot "recall" or "web-search" commands, whereas real scientists must strategically plan and carry out precisely staged experiments-often designing specialized equipment at each step.Recognizing these limitations, we categorize existing hypothesis-discovery research into three classes (see Fig. 3).</p>
<p>Passive Hypothesis Discovery</p>
<p>In this type of study, LLMs generate, apply, and validate hypotheses iteratively.However, the observations are provided by a fixed dataset.The LLM does not need to worry about which observations it will receive.Instead, it simply reasons based on the given data, passively receiving and processing the information provided.2024) proposed the Hypotheses-to-Theories (HtT) Framework to generate formal hypotheses (e.g., "if A then B") by leveraging existing benchmarks (Sinha et al., 2019;Wang et al., 2022b;Rule, 2020).</p>
<p>In HtT, LLMs generate a hypothesis and propose learned rules to each question.When a new question is received, the model first formulates a preliminary hypothesis based on the context.It then proposes candidate rules that might lead to the correct answer.These candidate rules are applied to the problem and verified against the ground truth.Rules that consistently yield correct predictions are retained and added to the rule library, while ineffective ones are discarded.Iteratively, after processing all questions in the benchmark, the LLM builds a rule library containing effective rules for solving the questions.et al., 2021), and Tweet Popularity (Tan et al., 2014).Due to the complexity of real-world data, the generated hypotheses are more nuanced and expressed in natural language.Similar to HtT, HypoGeniC begins by generating a set of candidate hypotheses from a small number of examples.As new observations are processed, each hypothesis is used to make predictions and is assigned a reward based on its accuracy.The system dynamically updates the confidence of each hypothesis; those that consistently perform poorly are removed from the hypothesis bank.New hypotheses are generated from examples that existing hypotheses fail to explain, allowing the model to refine and expand its understanding over time.</p>
<p>Both HypoGeniC and HtT simplify hypothesis discovery by relying on benchmark questions that include ground-truth answers.This configuration allows an external algorithm, not the LLMs themselves, to validate generated hypotheses and update their confidence based on the correctness of predictions.In real-world scenarios, where no ground-truth answers are available, these frameworks become inapplicable and would require substantial adaptation.</p>
<p>Proactive Hypothesis Discovery</p>
<p>In real-life hypothesis discovery, we do not start with a predefined set of observations that continuously propose new insights.Instead, once an initial hypothesis is formed, we proactively recall our memories or explore further to gather new observations that either strengthen or weaken the hypothesis, allowing us to verify and refine our ideas.</p>
<p>Given a hypothesis, Li et al. (2024) and Jung et al. (2022) propose two proactive methods for hypothesis discovery that both leverage the LLM's parametric memory to generate evidence that either strengthens or weakens the hypothesis.In Hypothesis Testing Prompting, the model directly uses its internal reasoning to evaluate the generated evidence, determining which pieces are more convincing, and then decides whether the hypothesis is correct based on the balance of evidence that strengthens or weakens it.In contrast, Maieutic Prompting iteratively constructs a tree of evidence by generating both strengthening and weakening explanations.It then employs the LLM to assign a belief score (reflecting the model's confidence in the evidence) and a consistency score (measuring how well the evidence aligns with the hypothesis).Finally, a MAX-SAT solver is applied to select the subset of evidence that maximizes the overall scores, thereby determining whether to accept or reject the hypothesis.</p>
<p>Different from relying solely on an LLM's parametric memory to generate new evidence, Seals &amp; Shalin (2024) propose a minimal setting for proactive hypothesis discovery.Inspired by the Wason Task from cognitive science, this task challenges LLMs to prove a formal language hypothesis of the form "if p then q." Here, both p and q are objects described in natural language, for example, "if a person is a man, then he drinks alcohol."The task provides four cards, each with two sides representing different attributes.Initially, one side of each card is shown, displaying p, q, ¬p, and ¬q, while the other side reveals the state of another attribute.To rigorously validate the hypothesis "if p then q," one must flip the p card to confirm that its hidden side is q (modus ponens) and flip the ¬q card to check that its hidden side is ¬p (modus tollens).</p>
<p>Flipping only these two cards provides sufficient evidence for the hypothesis, while the other two cards do not offer the necessary information.Thus, in this benchmark, by proactively flipping two cards, we can determine whether the LLM can correctly identify natural language expressions of p and q and validate the hypothesis using a minimal action space.</p>
<p>Moreover, Conti et al. (2024) propose APEx, a multimodal automatic benchmarking framework that evaluates hypotheses about large multimodal models in a fully automated and iterative fashion.For example, to test a hypothesis such as "a model is able to identify graffiti-styled images," APEx first leverages textto-image retrieval and generation tools to create a tailored set of test images.It then employs a range of transformation tools to perform image augmentation, introducing variations that challenge the models' robustness.In an iterative experimental loop, the framework executes these experiments on a library of models, analyzes the results, and refines the testing protocol accordingly.</p>
<p>Complete Loop: Real-World Discovery Simulation</p>
<p>Other works equip LLM agents with interactive environments that more closely mirror the complexity of real-world hypothesis discovery by combining planning, acting, and evidence collection.For example, Xu et al. ( 2023) construct a Minecraft-like world in which a "vandal" agent performs up to 26 types of actions (e.g., moving, eating, crafting) to achieve a hidden goal (such as collecting lava or crafting a particular item) and leaves behind tracks as evidence.A detective agent-driven by reinforcement learning to maximize information gain-then gathers those tracks and presents them to an LLM, which must answer a multiplechoice question about the vandal's original objective.Because evidence collection relies on an RL policy rather than LLM planning, however, this setup evaluates only the model's capacity to interpret evidence, not its ability to proactively generate and test hypotheses in a dynamic setting.</p>
<p>Building on this approach, Wang et al. (2022a) introduce 30 scientific tasks drawn from five topics in fifthgrade curricula, ranging from measuring the friction coefficient of an inclined plane to testing electrical conductivity.Here, agents must execute long action sequences and apply deductive reasoning grounded in established theories and definitions to complete each task.Likewise, Jansen et al. (2024) design 120 experiments across eight subjects (e.g., Chemistry, Archaeology), each with three difficulty levels, and allow 14 coarse-grained actions (such as "take," "put," and "move").Agents are evaluated on (1) task completion, (2) execution of key experimental steps, and (3) accurate hypothesis discovery compared to a ground truth.While these virtual labs simulate multi-step procedures and test hypothesis application, their restricted action spaces support only qualitative inference and preclude the fine-grained interventions needed for quantitative rule-learning.</p>
<p>To address these limitations, He et al. (2024) propose puzzle environments in which agents can input arbitrary integers or letters and receive tailored feedback based on a hidden rule.In this framework, an LLM must iteratively probe the environment, uncover the underlying quantitative rule, and solve the puzzle.Performance is assessed not only by whether the agent solves the puzzle but also by human judgments of the clarity and rigor of its reasoning steps, thereby offering a finer-grained evaluation of both quantitative hypothesis generation and the quality of the model's deductive process.</p>
<p>Discussion and Future Directions in Hypothesis Discovery</p>
<p>Hypothesis discovery fundamentally differs from isolated reasoning tasks by requiring iterative learning and continuous refinement of hypotheses within dynamic, evolving contexts.Particularly in Real-World simulation scenarios, the decisions and actions taken by an LLM may lead to entirely different trajectories of observation collection, varied learning efficiencies, and alternative hypotheses.</p>
<p>Building effective benchmarks for hypothesis discovery requires constructing rich, realistic environments capable of simulating real-world complexities.These environments should contain diverse, comprehensive action spaces and varied observational feedback mechanisms.Compared to traditional static, label-based datasets, creating such benchmarks is significantly more labor-intensive, demanding at least two key components: 1, A set of rules unknown to the LLM that can be learned within the environment.2, A sufficiently expressive action space that allows the LLM to interact with the environment, receive feedback, and gather new information.</p>
<p>Given that current LLMs are trained on vast quantities of data, there is a risk of hypothesis leakage, where underlying rules might already be implicitly embedded in their parametric memory.For instance, benchmarks such as those introduced by Wang et al. (2022a) often rely on relatively straightforward tasks that do not genuinely necessitate novel hypothesis formation.Conversely, tasks proposed by He et al. (2024), despite aiming to encourage creative hypothesis formation, often yield simplistic, toy-like hypotheses with limited applicability to realistic scenarios.Therefore, future research should aim to develop environments with greater complexity and realism, fostering diverse and genuinely novel hypotheses.Benchmarks should be explicitly designed to push LLMs beyond their pretrained knowledge boundaries, and must provide practical tools for validating newly generated hypotheses.Such realistic simulation environments would address critical challenges such as hypothesis leakage and task oversimplification, ultimately fostering more robust and practical hypothesis discovery capabilities within LLMs.</p>
<p>Summary</p>
<p>In this survey, we have presented a comprehensive and structured framework for hypothesis discovery using LLMs, guided by Peirce's reasoning paradigm of abduction, deduction, and induction.Specifically, we systematically explored current methods and benchmarks across the three core components: hypothesis generation, hypothesis application, and hypothesis validation.</p>
<p>Our analysis identifies a significant gap between formal and natural language representations.While formal representations enable rigorous and objective evaluations, they often remain restricted to simplified, artificial scenarios lacking real-world complexity.Conversely, natural language representations effectively capture the nuanced complexities inherent in real-world reasoning tasks, yet suffer from a lack of reliable, rigorous evaluation metrics due to their inherently open-ended nature.</p>
<p>Existing methods, including prompt-based and fine-tuning approaches, demonstrate considerable potential but frequently isolate individual reasoning components.To move forward, we advocate for the development of integrated benchmarks and realistic, dynamic environments that more closely mimic real-world scientific inquiry and hypothesis discovery processes.Such benchmarks should provide rich intermediate Chain-of-Thought data, detailed commonsense reasoning steps, and comprehensive action spaces, thereby bridging the current divide between formal and informal reasoning representations.</p>
<p>Ultimately, establishing environments that demand proactive hypothesis generation, robust application to novel contexts, and rigorous validation against evolving evidence will be crucial.By addressing these challenges, future research will significantly advance the ability of LLMs to not merely execute instructions but to autonomously generate, refine, and validate hypotheses, thus realizing their potential as true engines of discovery and innovation.</p>
<p>Language Parser: Since formal symbolic solvers yield sound and correct predictions, Pan et al. (2023); Olausson et al. (2023); Kalyanpur et al. (</p>
<p>Figure 3 :
3
Figure 3: Differences and similarities among different types of hypothesis discovery tasks</p>
<p>Hypothesis Knowledge Base Derived Knowledge
Natural LanguageEnglish: 'Sam is a dragon"English commonsense of 'dragon"Sam is dangerousChinese: 'Sam is a dragon"Chinese commonsense of 'dragon"Sam brings good fortune and a bountiful harvestFormal LanguageFOL: Dragon(Sam)∀x(Dragon(x) → Fly(x)) . . .Fly(Sam)Class Dragon:Code: Sam = Dragon()def fly(self):</p>
<p>Qiu et al. (2024))))r model on FOL abduction tasks, demonstrating that the model can generate FOL hypotheses from formal observations.Similarly,Nguyen et al. (2023)fine-tuned state-of-the-art legal transformers on FOL abduction tasks and found that models pre-trained on natural language legal abduction tasks do not show any performance improvements on FOL hypothesis generation problems.When observations are represented in natural language, traditional symbolic solvers struggle to extract the key information needed for hypothesis generation.With LLMs, however, we can directly generate formal hypotheses.A popular formal language for this purpose is code, as it offers greater flexibility than other symbolic representations like FOL, and LLMs excel at coding.The simplest variant prompts an LLM with an observation set and asks it to produce executable functions as hypotheses that match the input-output pairs;Cheng et al. (2024)follow this pattern, treating each observation as an (x, y) example and evaluating the generated function by execution.Extending this idea,Wang et al. (2024);Qiu et al. (2024)have the LLM create multiple executable hypotheses, run them on the observations, feed the results back to the model, and iterate, discarding weak candidates and refining promising ones until one covers all examples.To encourage diversity, il Lee et al. (
Natural Language Observations:</p>
<p>Yang et al. (2024b)ora such asWIKIZhong et al., 2024) al., 2024;Zhong et al., 2024),Movva et al. (2025)treat hypothesis generation as identifying the key features that drive a prediction.The model proposes feature sets, which are judged by how well they match and cover the ground-truth features, thereby quantifying the LLM's ability to isolate causal signals.Despite these efforts,Yang et al. (2024b)note that reference-based metrics such as BLEU, ROUGE, and ME-TEOR assume a single correct answer and therefore struggle to capture the open-ended nature of hypothesis generation; developing fair, reliable metrics remains an open challenge.</p>
<p>Li et al. (2025a)2)enchmark based on deterministic regular functions, providing a procedural framework for evaluating formal hypotheses.Similarly,Young et al. (2022)used first-order logic (FOL) representations, where LLMs were tasked with generating FOL hypotheses to explain given facts, and the outputs were evaluated by comparing them against ground truth hypotheses verified by solvers.Since inference on formal hypotheses is deterministic, a common evaluation method is to test whether the generated hypothesis produces correct outcomes on held-out examples.For instance, Rule (2020) propose the list function task, where LLMs generate a hypothesis function from observed (x, y) pairs, and evaluation is based on how well the hypothesis predicts hidden pairs.Similarly,Chollet (2019)introduces the Abstract Reasoning Corpus (ARC), where tasks involve transforming input grids of colored cells into output grids.The generated function is executed on test inputs, and correctness is determined by exact matches with the target output grids, including grid dimensions.Liu et al. (2024)further propose a benchmark consisting of arithmetic calculations, color token mapping, and Kalamang vocabulary tasks, all evaluated in the same way.Additionally,Li et al. (2025a)construct diverse application
Prediction-based Evaluation:
Chen et al. (2024)ng list transformations, real-world problems, code generation, and string transformations, where the generated hypothesis is executed on both seen and test observations and the final score aggregates performance across both sets.In a more realistic setting,Chen et al. (2024)extract 102 tasks from 44 peer-reviewed publications, unifying the target output for every task into a self-contained Python program file, accompanied by a set of test cases validated by human experts.LLMs are then asked to read the paper and reproduce the tasks in code, and the generated code is directly evaluated on the prepared test cases.</p>
<p>Published in Transactions on Machine LearningResearch (04/2025)   </p>
<p>Kevin Schawinski, and Ioana Ciucă. A survey on hypothesis generation for scientific discovery in the era of large language models. Shashwat Atilla Kaan Alkan, Maja Sourav, Simone Jablonska, Rishabh Astarita, Nikhil Chakrabarty, Pranav Garuda, Maciej Khetarpal, Dimitrios Pióro, Tanoglidis, G Kartheik, Mugdha S Iyer, Michael J Polimera, Tirthankar Smith, Marc Ghosal, Sandor Huertas-Company, Kruk, 2025</p>
<p>Francis Bacon, Novum organum. Clarendon press1878</p>
<p>Artifacts or abduction: How do LLMs answer multiple-choice questions without the question?. Nishant Balepur, Abhilasha Ravichander, Rachel Rudinger, 10.18653/v1/2024.acl-long.555Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAugust 20241Association for Computational Linguistics</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. Jade Goldstein, Alon Lavie, Chin-Yew Lin, Clare Voss, the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, MichiganAssociation for Computational LinguisticsJune 2005</p>
<p>BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Adib Bazgir, Rama Chandra Praneeth, Yuwen Madugula, Zhang, Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation. 2025. 2023Agentichypothesis: A survey on hypothesis generation using LLM systems</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Yejin Wen Tau Yih, Choi, 2020</p>
<p>A comprehensive evaluation of inductive reasoning capabilities and problem solving in large language models. Chen Bowen, Rune Saetre, Yusuke Miyao, Findings of the Association for Computational Linguistics: EACL 2024. Yvette Graham, Matthew Purver, St. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden Mclean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Christopher Olah, 2023</p>
<p>Peirce's theory of abduction. Arthur W Burks, Philosophy of Science. 003182481341946</p>
<p>Chengkun Cai, Xu Zhao, Haoliang Liu, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Serge Belongie, Lei Li, The role of deductive and inductive reasoning in large language models. 2025</p>
<p>Exploring scientific hypothesis generation with mamba. Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal, 10.18653/v1/2024.nlp4science-1.17Proceedings of the 1st Workshop on NLP for Science (NLP4Science). Lotem Peled-Cohen, Nitay Calderon, Shir Lissak, Roi Reichart, the 1st Workshop on NLP for Science (NLP4Science)Miami, FL, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Selfconsistent narrative prompts on abductive natural language inference. Chunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang Cheng, Yangqiu Song, Ginny Wong, Simon See, 10.18653/v1/2023.ijcnlp-main.67Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. Jong C Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, Adila Alfa Krisnadhi, the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliAssociation for Computational LinguisticsNovember 20231</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, 2024</p>
<p>. Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, 2024Bing Yin, and Yizhou SunInductive or deductive? rethinking the fundamental reasoning abilities of llms</p>
<p>On the measure of intelligence. François Chollet, 2019</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>Automatic benchmarking of large multimodal models via iterative experiment programming. Alessandro Conti, Enrico Fini, Paolo Rota, Yiming Wang, Massimiliano Mancini, Elisa Ricci, 2024</p>
<p>True detective: A deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4. Maksym Del, Mark Fishel, 10.18653/v1/2023.starsem-1.28Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (<em>SEM 2023). Alexis Palmer, Jose Camacho-Collados, the 12th Joint Conference on Lexical and Computational Semantics (</em>SEM 2023)Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Igor Douven, The Stanford Encyclopedia of Philosophy. Edward N Zalta, 2021Metaphysics Research Lab, Stanford UniversitySummer 2021 edition</p>
<p>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation. Steffen Eger, Yong Cao, D' Jennifer, Andreas Souza, Christian Geiger, Stephanie Greisinger, Yufang Gross, Brigitte Hou, Anne Krenn, Yizhi Lauscher, Chenghua Li, Nafise Sadat Lin, Wei Moosavi, Tristan Zhao, Miller, 2025</p>
<p>Peirce's notion of abduction. G Harry, Frankfurt, The Journal of Philosophy. 0022362X55141958</p>
<p>Pär Anders Granhag and Aldert Vrij. Deception detection. Psychology and law: An empirical perspective. 2005</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, 2025</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alex Wardle-Solano, Hannah Szabo, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander R Fabbri, Wojciech Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, Arman Cohan, and Dragomir Radev. Folio: Natural language reasoning with first-order logic. 2024</p>
<p>Nonmonotonic logic and temporal projection. Steve Hanks, Drew Mcdermott, Artificial intelligence. 3331987</p>
<p>The inference to the best explanation. Gilbert H Harman, The Philosophical Review. 00318108, 155814707411965</p>
<p>Causejudger: Identifying the cause with llms for abductive logical reasoning. Jinwei He, Feng Lu, 2024</p>
<p>Idea: Enhancing the rule learning ability of large language model agent through induction, deduction, and abduction. Kaiyu He, Mian Zhang, Shuo Yan, Peilin Wu, Zhiyu Zoey, Chen , 2024</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, 2024</p>
<p>Inductionbench: Llms fail in the simplest complexity class. Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang, Wang , 2025</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaJuly 2023Association for Computational Linguistics</p>
<p>Generating diverse hypotheses for inductive reasoning. Hyukhun Kang Il Lee, Dongryeol Koh, Seunghyun Lee, Minsung Yoon, Kyomin Kim, Jung, 2025</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, Advances in Neural Information Processing Systems. 202437</p>
<p>Brainteaser: Lateral thinking puzzles for large language models. Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati, 2023</p>
<p>StoryAnalogy: Deriving story-level analogies from large language models to unlock analogical understanding. Cheng Jiayang, Lin Qiu, Tsz Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang, 10.18653/v1/2023.emnlp-main.706Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, 10.18653/v1/2022.emnlp-main.82Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Llm-arc: Enhancing llms with an automated reasoning critic. Aditya Kalyanpur, Karthik Kailash, Victor Saravanakumar, Jennifer Barres, David Chu-Carroll, David Melville, Ferrucci, 2024</p>
<p>How far is video generation from world model: A physical law perspective. Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng, arXiv:2411.023852024arXiv preprint</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, 10.18653/v1/2022.findings-emnlp.38Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Mirage: Evaluating and explaining inductive reasoning process in language models. Larry Laudan, ; Jiachun Li, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao, 1971. 2025aWilliam whewell on the consilience of inductions. The Monist</p>
<p>Hypothesis testing prompting improves deductive reasoning in large language models. Yitian Li, Jidong Tian, Hao He, Yaohui Jin, 2024</p>
<p>Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu, From system 1 to system 2: A survey of reasoning large language models. 2025b</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJuly 2004</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>An incomplete loop: Instruction inference, instruction following, and in-context learning in language models. Emmy Liu, Graham Neubig, Jacob Andreas, 2024</p>
<p>Logical reasoning in large language models: A survey. Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, Yue Zhang, 2025</p>
<p>The upworthy research archive. Jorge Nathan, Matias , Kevin Munger, Marianne Aubin Le Quere, Charles R Ebersole, 487 experiments in U.S. media. Scientific Data. 322021</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. John Mccarthy, Patrick J Hayes, Readings in artificial intelligence. Elsevier1981</p>
<p>John Stuart, Mill , A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Principles of Evidence, and Methods of Scientific Investigation. BoD-Books on Demand2024I</p>
<p>Peirce-suit of truth -why inference to the best explanation and abduction ought not to be confused. Gerhard Minnameier, 10.1023/B:ERKE.0000005162.52052.7fErkenntnis. 6012004</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. 2024arXiv preprint</p>
<p>Sparse autoencoders for hypothesis generation. Rajiv Movva, Kenny Peng, Nikhil Garg, Jon Kleinberg, Emma Pierson, 2025</p>
<p>Can llms follow simple rules?. Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, David Wagner, 2024</p>
<p>How well do sota legal reasoning models support abductive reasoning?. Ha-Thanh Nguyen, Randy Goebel, Francesca Toni, Kostas Stathis, Ken Satoh, 2023</p>
<p>Large language models and cognitive science: A comprehensive review of similarities, differences, and challenges. Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, Ming Li, Yichao Lawrence Kq Yan, Caitlyn Zhang, Cheng Heqi Yin, Tianyang Fei, Yunze Wang, Silin Wang, Ming Chen, Liu, 2024</p>
<p>Inductive, abductive and deductive theorising. Chitu Okoli, International Journal of Management Concepts and Philosophy. 1632023</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, doi: 10.18653Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>URL. </p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Pierre Isabelle, Eugene Charniak, Dekang Lin, the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsJuly 2002</p>
<p>Collected papers of charles sanders peirce. Charles Sanders, Peirce , 1974Harvard University Press5</p>
<p>TopicGPT: A promptbased topic modeling framework. Minh Chau, Alexander Pham, Simeng Hoyle, Philip Sun, Mohit Resnik, Iyyer, 10.18653/v1/2024.naacl-long.164Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Reasoning with large language models, a survey. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki Van Stein, Thomas Back, 2024</p>
<p>The logic of scientific discovery. Karl Popper, 2005Routledge</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, Kevin Kj, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, 2024</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. K Chandan, Parshin Reddy, Shojaee, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>A logic for default reasoning. Raymond Reiter, Artificial intelligence. 131-21980</p>
<p>Thinking like a skeptic: Defeasible inference in natural language. Rachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Le Ronan, Noah A Bras, Yejin Smith, Choi, 10.18653/v1/2020.findings-emnlp.418Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Association for Computational LinguisticsNovember 2020</p>
<p>The child as hacker: building more human-like models of learning. Joshua Stewart, Rule , 2020Massachusetts Institute of TechnologyPhD thesis</p>
<p>Evaluating the deductive competence of large language models. S Seals, Valerie Shalin, 10.18653/v1/2024.naacl-long.476Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Language models can improve event prediction by few-shot abductive reasoning. Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Zhang, Jun Zhou, Chenhao Tan, Hongyuan Mei, Advances in Neural Information Processing Systems. 202336</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2024</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>The road to experience and prediction from within: Hans reichenbach's scientific correspondence from berlin to istanbul. Friedrich Stadler, Synthese. 1812011</p>
<p>Beyond instruction following: Evaluating inferential rule following of large language models. Wangtao Sun, Chenxiang Zhang, Xueyou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, Kang Liu, 2024</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, Association for Computational LinguisticsAugust 2021</p>
<p>The effect of wording on message propagation: Topic-and authorcontrolled natural experiments on twitter. Chenhao Tan, Lillian Lee, Bo Pang, Proceedings of ACL. ACL2014</p>
<p>Llm assists hypothesis generation and testing for deliberative questions. Fuchun Wang, Xian Zhou, Wenpeng Hu, Zhunchen Luo, Wei Luo, Xiaoying Bai, Natural Language Processing and Chinese Computing. Derek F Wong, Zhongyu Wei, Muyun Yang, Singapore; SingaporeSpringer Nature2025</p>
<p>Hypothesis search: Inductive reasoning with language models. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, 2024</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 2022a</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 2022b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, 2015</p>
<p>Reframing human-AI collaboration for generating free-text explanations. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi, 10.18653/v1/2022.naacl-main.47Proceedings of the 2022 Conference of the North American Chapter. Marine Carpuat, Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, 2024</p>
<p>Active reasoning in an open-world environment. Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu, Advances in Neural Information Processing Systems. 202336</p>
<p>Do phd-level llms truly grasp elementary addition? probing rule learning vs. memorization in large language models. Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan, arXiv:2504.052622025arXiv preprint</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024a1</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, 10.18653/v1/2024.findings-acl.804Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024b</p>
<p>Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria, 2024c</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, 2025</p>
<p>AbductionRules: Training transformers to explain unexpected inputs. Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock, 10.18653/v1/2022.findings-acl.19Findings of the Association for Computational Linguistics: ACL 2022. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, 10.1145/3664194ACM Comput. Surv. 0360-03005612October 2024a</p>
<p>Turtlebench: Evaluating top language models via real-world yes/no puzzles. Qingchen Yu, Shichao Song, Ke Fang, Yunfeng Shi, Zifan Zheng, Hanyu Wang, Simin Niu, Zhiyu Li, 2024b</p>
<p>Beneath surface similarity: Large language models make reasonable scientific analogies after structure abduction. Siyu Yuan, Jiangjie Chen, Xuyang Ge, Yanghua Xiao, Deqing Yang, 10.18653/v1/2023.findings-emnlp.160Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Defeasible visual entailment: Benchmark, evaluator, and reward-driven optimization. Yue Zhang, Liqiang Jing, Vibhav Gogate, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Abductive commonsense reasoning exploiting mutually exclusive explanations. Wenting Zhao, Justin T Chiu, Claire Cardie, Alexander M Rush, 2023</p>
<p>UNcommonsense reasoning: Abductive reasoning about uncommon situations. Wenting Zhao, Justin Chiu, Jena Hwang, Faeze Brahman, Jack Hessel, Sanjiban Choudhury, Yejin Choi, Xiang Li, Alane Suhr, 10.18653/v1/2024.naacl-long.469Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Explaining datasets in words: Statistical models with natural language parameters. Ruiqi Zhong, Heng Wang, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202437</p>
<p>Hypothesis generation with large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024</p>
<p>Large language models can learn rules. Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
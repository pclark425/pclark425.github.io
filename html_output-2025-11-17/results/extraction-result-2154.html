<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2154 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2154</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2154</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-278636629</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.10012v1.pdf" target="_blank">Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in artificial intelligence (AI) and quantum computing are accelerating automation in scientific and engineering processes, fundamentally reshaping research methodologies. This perspective highlights parallels between scientific automation and established Computer-Aided Engineering (CAE) practices, introducing Quantum CAE as a framework that leverages quantum algorithms for simulation, optimization, and machine learning within engineering design. Practical implementations of Quantum CAE are illustrated through case studies for combinatorial optimization problems. Further discussions include advancements toward higher automation levels, highlighting the critical role of specialized AI agents proficient in quantum algorithm design. The integration of quantum computing with AI raises significant questions about the collaborative dynamics among human scientists and engineers, AI systems, and quantum computational resources, underscoring a transformative future for automated discovery and innovation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2154.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2154.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>digital scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>digital scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual class of AI/robotic agents designed to autonomously perform scientific research tasks within computational environments, including hypothesis generation, experiment (simulation) execution, data analysis, and manuscript drafting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>digital scientist (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid AI-automation framework (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific research / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>research topics, hypotheses, experimental designs, simulation code, manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>simulation-based experiments, iterative data acquisition, human oversight and peer review (paper emphasizes simulations and standard scientific validation pipelines rather than a single formal verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>implicitly measured by distance from existing knowledge bases and by iterative improvement of predictive models as more data is acquired; no formal numeric measure given</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>qualitatively presented as capable of Level 3 automation (full autonomy under well-defined conditions); no quantitative metrics provided, and higher-level adaptability (Levels 4–5) noted as currently out of reach</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>described qualitatively: validation improves with iterative data acquisition and simulations but specific accuracy metrics are not provided</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>paper states validation becomes more difficult for higher automation levels and for novel/uncontrollable conditions; iterative simulation/data acquisition improves validation for initially unfamiliar problems</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>supports an asymmetry: generation (hypothesis/idea creation) can be automated more readily in defined domains, while reliable validation—especially for novel/unfamiliar problems—remains more demanding and often requires human oversight</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified; paper notes Levels 4–5 require adaptability to dynamically changing/novel conditions that current systems lack</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not specified; uncertainty quantification and robust calibration are described as open challenges for higher automation levels</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>validation via simulation/experiments can be computationally expensive relative to generation (e.g., building prediction models, running simulations), and cost increases for higher fidelity/novel validations; no numeric values provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>iterative data acquisition, Bayesian optimization, incorporation of domain-specific simulators, human-in-the-loop oversight, and development of specialized agents</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper articulates the concept of digital scientists and argues that while automated generation of hypotheses and designs is feasible (Level 3 in many computational domains), reliable validation—especially for novel, out-of-distribution problems—remains a bottleneck requiring iterative simulation, human oversight, and improved methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2154.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>robot scientist (King et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A historically significant implementation of an automated system that generates hypotheses and executes wet-lab experiments to test them, demonstrating feasibility of automated scientific cycles in genomics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>robot scientist (King et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>rule-based AI + laboratory automation (robotics + symbolic/inference systems)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>functional genomics / life sciences</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates biological hypotheses and experimental plans</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>physical experiments (wet-lab execution) and measurement of experimental outcomes to confirm or reject hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>implicitly novelty assessed by whether hypotheses extend known knowledge and by experimental novelty; no formal numeric novelty metric provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>historically demonstrated successful hypothesis generation and experimental throughput sufficient to enable discovery in genomics; this paper does not provide further numerical performance data</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>successful use-case reported historically (King et al.) where experiments validated hypotheses; no error rates provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>wet-lab experimental validation is treated as ground truth and thus robust across novelty, but resource demands and experimental design complexity increase for more novel hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>paper highlights that experimental validation (especially wet-lab) is more expensive and slower than hypothesis generation, creating a practical asymmetry</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not specified here; robot scientist approach is domain-limited (genomics) and generalization beyond domain requires redesign</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not described</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>high (real-world experiments), substantially larger than purely computational generation; specific costs not provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>automation of experiments coupled with closed-loop hypothesis testing (robot+AI cycle), but human oversight remains important</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Robot scientists demonstrate closed-loop automated generation-and-validation cycles with physical experiments, but the cost and complexity of experimental validation create a throughput and resource asymmetry relative to automated generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2154.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system that predicts protein 3D structures from sequence data with high accuracy, widely cited as a landmark of AI enabling scientific discovery in structural biology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep neural network (large-scale supervised learning model)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>structural biology / protein structure prediction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates 3D protein structure predictions from amino acid sequences</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>benchmarking against experimentally determined structures (e.g., CASP evaluations), comparison to known results, and downstream utility in scientific work (cited as leading to Nobel-recognized impact)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>performance in CASP benchmarks and predictive distance/errors relative to experimental structures; paper references AlphaFold's high accuracy but does not reproduce numeric benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>described in the literature as 'highly accurate' (AlphaFold achieved top performance in structural prediction competitions), but no numeric metrics are provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validated by comparison to experimental structures; this paper does not report numeric validation statistics</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>AlphaFold's benchmarks indicate strong generalization to many sequences, but this paper does not report specifics on out-of-distribution or highly novel proteins</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>implied: generation (structure prediction) is automated and high-quality, but experimental determination remains the definitive validation; resource asymmetry exists between computational generation and experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified here; AlphaFold shown to generalize well in practice per cited works but paper provides no numeric OOD measures</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not discussed in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>computational prediction is far cheaper/quicker than experimental structure determination; exact costs not provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>benchmarking against experimental datasets and community evaluation (CASP); integration into human workflows for final verification</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AlphaFold exemplifies how AI can generate scientifically useful outputs at scale, but community-standard experimental benchmarks and downstream human validation remain essential for trust and adoption.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2154.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOCS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization of Combinatorial Structures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A black-box optimization method for discrete/combinatorial design problems that constructs probabilistic prediction models by sampling from posterior distributions over model parameters to guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian optimization of combinatorial structures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BOCS (Bayesian optimization of combinatorial structures)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>probabilistic modeling + Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>combinatorial design optimization (materials, electronics, manufacturing)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates candidate discrete designs/hypotheses (e.g., circuit patterns, mounting point configurations) to optimize objective functions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>evaluated via simulations or domain-specific solvers; probabilistic posterior sampling guides exploration and validation relies on simulated/empirical cost function evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>ensures exploratory search via posterior distributions to discover novel combinations; novelty implicitly measured by search distance from initial samples and exploration-exploitation tradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>described as enabling broader exploratory search than point-estimate methods and successfully applied in cited case studies; no numeric success rates provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation performed through simulation/evaluation of cost functions; performance improves with more data acquisitions, as shown qualitatively in the cited case study</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>paper indicates that iterative acquisition improves model accuracy and that exploratory approaches like BOCS are better suited to find novel, high-quality solutions than purely exploitative methods</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>some asymmetry exists: model-based generation can propose many candidates quickly, but verifying them via simulation/experimental evaluation is more costly; BOCS' probabilistic modeling mitigates premature convergence to familiar solutions</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified; BOCS designed to explore beyond local optima, suggesting improved OOD exploration relative to point-estimate methods</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>uses Bayesian posterior distributions which provide uncertainty quantification; calibration quality not numerically reported here</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>validation requires running simulations for each candidate and grows with the number of acquisitions; no numeric costs provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>posterior-based uncertainty modeling, exploration-driven acquisition, and integration with domain simulators</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BOCS provides a probabilistic framework that favors exploratory search and uncertainty-aware candidate generation, which helps discover novel combinatorial designs, but validation cost remains a practical bottleneck addressed by iterative data acquisition.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2154.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FMQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factorization Machine Quantum Annealing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method that uses Factorization Machines as fast prediction models combined with quantum annealing for combinatorial optimization to accelerate design search with fewer data points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Designing metamaterials with quantum annealing and factorization machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FMQA (Factorization Machine + Quantum Annealing)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid classical ML (factorization machine) + quantum optimization (quantum annealer)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>combinatorial materials/design optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates discrete design candidates (e.g., metamaterial patterns) guided by fast surrogate models and optimized via quantum annealing</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>candidates evaluated by domain-specific simulations; surrogate model trained via point estimation from limited data and validated by simulation/experiment</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not formally defined; FMQA prioritizes fast convergence from limited data which can risk converging to local (possibly familiar) optima rather than exploring highly novel regions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>described as achieving faster convergence with fewer data than Gaussian-process-based methods but with higher risk of local optima; no numeric metrics given in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validated via simulations in cited studies and produced practical designs in case studies; quantitative validation metrics not provided</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>FMQA's faster convergence from limited data can reduce exploration and thereby potentially miss novel high-quality solutions, affecting validation outcomes for novel tasks negatively</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>there is an implied asymmetry: FMQA efficiently generates candidates but may under-explore novelty, making validation more likely to confirm familiar/local solutions than uncover truly novel ones</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified; design choices (surrogate bias) suggest weaker OOD exploration compared to fully Bayesian approaches</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>point-estimate surrogate models provide limited uncertainty quantification compared to Bayesian alternatives</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>surrogate-assisted approach reduces the number of expensive validation simulations, lowering validation cost per candidate overall; exact numbers not provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>combining surrogates with quantum annealing and designing cost functions (QUBO) to exclude impractical candidates, iterative data acquisition to improve models</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FMQA trades exploratory breadth for faster convergence—useful in data-limited settings but more likely to converge to local/familiar optima, indicating a generation-validation tradeoff when seeking novel solutions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2154.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GQE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The generative quantum eigensolver (GQE) and its application for ground state search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative-model-based approach that uses a decoder-only Transformer to automatically design quantum circuits aimed at preparing molecular ground states, demonstrating automated quantum circuit generation for scientific computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The generative quantum eigensolver (gqe) and its application for ground state search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative Quantum Eigensolver (GQE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative model (decoder-only Transformer) for quantum circuit design</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>quantum chemistry / quantum algorithm design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates quantum-circuit ansätze tailored to target quantum states (e.g., molecular ground states)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>validation via quantum circuit simulation and evaluation of resulting state energies against reference values; performance assessments use simulators to guide training</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty implicitly judged by ability to find circuit forms not previously hand-designed and by achieved ground-state energies compared to baselines</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>paper cites successful automated circuit generation in referenced work; no numeric performance values included here</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validated by simulation-based energy comparisons, with reinforcement learning or optimization guiding improvements; no aggregate metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>validation via simulators remains reliable for circuits within simulator fidelity; generating highly novel circuits may require more simulation budget and can reveal mismatches between simulator predictions and real hardware</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>generation of candidate circuits is fast via the model, but validating their actual utility (e.g., low energy on real hardware) is more expensive, producing an asymmetry between creative generation and costly validation</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified; novelty in circuit space may reduce simulator predictive fidelity for real hardware outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; uncertainty quantification for generated circuits not discussed in depth</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>simulation-based validation can be computationally expensive relative to generation; cost increases with circuit complexity and fidelity requirements</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>use of simulators for iterative assessment, reinforcement learning guided by simulator feedback, and combining generative models with performance-driven reward signals</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative models can autonomously produce quantum circuits that perform useful scientific computations, but simulator-based validation and resource costs create practical constraints, especially for novel circuit architectures and real-device transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2154.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minami agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI agent that autonomously generates quantum circuits for combinatorial optimization tasks using an encoder-decoder architecture (GNN encoder, Transformer decoder) and reinforcement learning guided by circuit simulator performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Minami et al. circuit-generation agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>graph neural network encoder + Transformer decoder; reinforcement-learning-guided generative agent</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>quantum optimization / combinatorial optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates quantum circuits tailored to specific QUBO / graph-structured optimization problems</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>performance assessment via quantum circuit simulators; reinforcement learning uses simulator feedback (e.g., solution quality, objective values) to improve generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty judged by the generated circuit architecture relative to prior designs and by achieved optimization performance on unseen problem instances</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>paper reports that simulator-guided RL demonstrates effectiveness in generating circuits; no numeric success rates are provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation limited to simulator evaluations which measure optimization performance; real-device validation not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>simulator-based validation is reliable for in-distribution problems but may not accurately predict performance on novel or real-hardware instances; paper notes performance assessments guided RL but does not quantify OOD gaps</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>generation via learned models is rapid, while validation (simulator runs, potentially many episodes for RL) is costlier, producing an operational asymmetry</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified; generalization to out-of-distribution problem graphs is an open concern implied in discussion</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not discussed</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>validation requires simulator evaluations for many generated circuits during RL training, incurring significant computational cost relative to a single-generation pass</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>reinforcement learning guided by simulator rewards, encoder-decoder architectures leveraging graph structure, and iterative improvement through performance feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI agents can learn to generate quantum circuits for optimization by using simulator-based validation signals, but simulator cost and potential simulator-to-hardware transfer gaps limit direct applicability to novel/unseen tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2154.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sakka LLM automation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automating quantum feature map design via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An example where large language models (LLMs) are used to automate the generation, implementation, and validation of quantum feature map designs, demonstrating an AI scientist workflow that writes and tests code and analyses results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automating quantum feature map design via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sakka et al. LLM-driven quantum feature map automation</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (LLM) integrated with experiment/code execution pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>quantum machine learning / quantum algorithm design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates design ideas, implementation code (feature maps), and experimental protocols for quantum ML tasks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>implements generated code, runs simulations or experiments, analyzes results algorithmically (and possibly via human inspection), and iteratively refines designs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty assessed by deviation from existing human-designed feature maps and by performance improvements in downstream tasks; no formal metric provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>demonstrated practical examples of LLM-generated ideas leading to working implementations; no aggregate quantitative metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation performed by implementation and empirical evaluation (simulations/experiments) with analysis of results; quantitative validation metrics not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>LLM-generated novel designs require empirical/simulation validation; paper emphasizes that automated generation can produce viable novel designs but their validation depends on computational/experimental resources</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>LLM can quickly generate many candidate designs but thorough validation (running code, simulations, experiments) is the rate-limiting step, producing an asymmetry</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified; generation of out-of-distribution novel designs is possible but validation fidelity and reliability may decline without additional resources</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>LLM confidence calibration not discussed; robustness and uncertainty in generated design efficacy are open challenges</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>validation (implementing/running and analyzing experiments) is computationally and time intensive relative to text/code generation; no numeric costs provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>automated code execution pipelines, iterative implement-test-analyze loops, human-in-the-loop checks, and integration of simulators with LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can act as 'AI scientists' to generate and implement algorithmic ideas, but reliable scientific validation requires running simulations/experiments and human oversight, particularly for novel designs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2154.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2154.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-authored papers passing peer review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-authored papers passing peer review (cited examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports and demonstrations that AI-generated manuscripts have in some cases passed peer review, indicating AI's capability to produce publishable scientific text and analysis in specific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-authored papers / AI manuscript generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models / generative models for text and analysis</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific writing / automated analysis</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates manuscripts, drafts, analyses, and sometimes code or experimental protocols</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>peer review and editorial evaluation, sometimes supplemented by reproducibility checks and experimental/simulation replication</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>assessed by peer reviewers and editors for originality and contribution; no automated novelty metric cited in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>qualitatively noted that AI-authored papers have succeeded in passing peer review in some instances; no statistics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>peer review functions as a filter for quality and correctness, but the paper notes concerns about trust and the degree to which peer review suffices for validating AI-generated scientific claims</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>peer review may be less reliable for detecting subtle errors in novel AI-generated claims, and the paper stresses continued human oversight</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>text generation can be high-quality while substantive scientific validation (experimental replication, domain-expert scrutiny) remains necessary, highlighting an asymmetry</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not quantified</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not discussed</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>peer review and reproducibility checks are human- and resource-intensive compared to automated text generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>human peer review, reproduction attempts, and integrated automated experiment/simulation pipelines to check claims</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI can generate publishable manuscripts in some cases, but passing peer review does not fully substitute for rigorous empirical validation—especially for novel scientific claims—so human oversight and reproducibility remain essential.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Functional genomic hypothesis generation and experimentation by a robot scientist <em>(Rating: 2)</em></li>
                <li>Bayesian optimization of combinatorial structures <em>(Rating: 2)</em></li>
                <li>Designing metamaterials with quantum annealing and factorization machines <em>(Rating: 2)</em></li>
                <li>The generative quantum eigensolver (gqe) and its application for ground state search <em>(Rating: 2)</em></li>
                <li>Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver <em>(Rating: 2)</em></li>
                <li>Automating quantum feature map design via large language models <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with alphafold <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated openended scientific discovery <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2154",
    "paper_id": "paper-278636629",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "digital scientist",
            "name_full": "digital scientist",
            "brief_description": "A conceptual class of AI/robotic agents designed to autonomously perform scientific research tasks within computational environments, including hypothesis generation, experiment (simulation) execution, data analysis, and manuscript drafting.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "digital scientist (concept)",
            "system_type": "hybrid AI-automation framework (conceptual)",
            "domain": "general scientific research / computational science",
            "generation_capability": "research topics, hypotheses, experimental designs, simulation code, manuscripts",
            "validation_method": "simulation-based experiments, iterative data acquisition, human oversight and peer review (paper emphasizes simulations and standard scientific validation pipelines rather than a single formal verifier)",
            "novelty_measure": "implicitly measured by distance from existing knowledge bases and by iterative improvement of predictive models as more data is acquired; no formal numeric measure given",
            "generation_performance": "qualitatively presented as capable of Level 3 automation (full autonomy under well-defined conditions); no quantitative metrics provided, and higher-level adaptability (Levels 4–5) noted as currently out of reach",
            "validation_performance": "described qualitatively: validation improves with iterative data acquisition and simulations but specific accuracy metrics are not provided",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "paper states validation becomes more difficult for higher automation levels and for novel/uncontrollable conditions; iterative simulation/data acquisition improves validation for initially unfamiliar problems",
            "generation_validation_asymmetry": "supports an asymmetry: generation (hypothesis/idea creation) can be automated more readily in defined domains, while reliable validation—especially for novel/unfamiliar problems—remains more demanding and often requires human oversight",
            "out_of_distribution_performance": "not quantified; paper notes Levels 4–5 require adaptability to dynamically changing/novel conditions that current systems lack",
            "calibration_quality": "not specified; uncertainty quantification and robust calibration are described as open challenges for higher automation levels",
            "validation_computational_cost": "validation via simulation/experiments can be computationally expensive relative to generation (e.g., building prediction models, running simulations), and cost increases for higher fidelity/novel validations; no numeric values provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "iterative data acquisition, Bayesian optimization, incorporation of domain-specific simulators, human-in-the-loop oversight, and development of specialized agents",
            "evidence_type": "supports",
            "key_findings": "The paper articulates the concept of digital scientists and argues that while automated generation of hypotheses and designs is feasible (Level 3 in many computational domains), reliable validation—especially for novel, out-of-distribution problems—remains a bottleneck requiring iterative simulation, human oversight, and improved methods.",
            "uuid": "e2154.0"
        },
        {
            "name_short": "robot scientist (King et al.)",
            "name_full": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "brief_description": "A historically significant implementation of an automated system that generates hypotheses and executes wet-lab experiments to test them, demonstrating feasibility of automated scientific cycles in genomics.",
            "citation_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "mention_or_use": "mention",
            "system_name": "robot scientist (King et al.)",
            "system_type": "rule-based AI + laboratory automation (robotics + symbolic/inference systems)",
            "domain": "functional genomics / life sciences",
            "generation_capability": "generates biological hypotheses and experimental plans",
            "validation_method": "physical experiments (wet-lab execution) and measurement of experimental outcomes to confirm or reject hypotheses",
            "novelty_measure": "implicitly novelty assessed by whether hypotheses extend known knowledge and by experimental novelty; no formal numeric novelty metric provided in this paper",
            "generation_performance": "historically demonstrated successful hypothesis generation and experimental throughput sufficient to enable discovery in genomics; this paper does not provide further numerical performance data",
            "validation_performance": "successful use-case reported historically (King et al.) where experiments validated hypotheses; no error rates provided in this paper",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "wet-lab experimental validation is treated as ground truth and thus robust across novelty, but resource demands and experimental design complexity increase for more novel hypotheses",
            "generation_validation_asymmetry": "paper highlights that experimental validation (especially wet-lab) is more expensive and slower than hypothesis generation, creating a practical asymmetry",
            "out_of_distribution_performance": "not specified here; robot scientist approach is domain-limited (genomics) and generalization beyond domain requires redesign",
            "calibration_quality": "not described",
            "validation_computational_cost": "high (real-world experiments), substantially larger than purely computational generation; specific costs not provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "automation of experiments coupled with closed-loop hypothesis testing (robot+AI cycle), but human oversight remains important",
            "evidence_type": "supports",
            "key_findings": "Robot scientists demonstrate closed-loop automated generation-and-validation cycles with physical experiments, but the cost and complexity of experimental validation create a throughput and resource asymmetry relative to automated generation.",
            "uuid": "e2154.1"
        },
        {
            "name_short": "AlphaFold",
            "name_full": "Highly accurate protein structure prediction with AlphaFold",
            "brief_description": "A deep learning system that predicts protein 3D structures from sequence data with high accuracy, widely cited as a landmark of AI enabling scientific discovery in structural biology.",
            "citation_title": "Highly accurate protein structure prediction with alphafold",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_type": "deep neural network (large-scale supervised learning model)",
            "domain": "structural biology / protein structure prediction",
            "generation_capability": "generates 3D protein structure predictions from amino acid sequences",
            "validation_method": "benchmarking against experimentally determined structures (e.g., CASP evaluations), comparison to known results, and downstream utility in scientific work (cited as leading to Nobel-recognized impact)",
            "novelty_measure": "performance in CASP benchmarks and predictive distance/errors relative to experimental structures; paper references AlphaFold's high accuracy but does not reproduce numeric benchmarks",
            "generation_performance": "described in the literature as 'highly accurate' (AlphaFold achieved top performance in structural prediction competitions), but no numeric metrics are provided in this paper",
            "validation_performance": "validated by comparison to experimental structures; this paper does not report numeric validation statistics",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "AlphaFold's benchmarks indicate strong generalization to many sequences, but this paper does not report specifics on out-of-distribution or highly novel proteins",
            "generation_validation_asymmetry": "implied: generation (structure prediction) is automated and high-quality, but experimental determination remains the definitive validation; resource asymmetry exists between computational generation and experimental validation",
            "out_of_distribution_performance": "not quantified here; AlphaFold shown to generalize well in practice per cited works but paper provides no numeric OOD measures",
            "calibration_quality": "not discussed in this paper",
            "validation_computational_cost": "computational prediction is far cheaper/quicker than experimental structure determination; exact costs not provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "benchmarking against experimental datasets and community evaluation (CASP); integration into human workflows for final verification",
            "evidence_type": "supports",
            "key_findings": "AlphaFold exemplifies how AI can generate scientifically useful outputs at scale, but community-standard experimental benchmarks and downstream human validation remain essential for trust and adoption.",
            "uuid": "e2154.2"
        },
        {
            "name_short": "BOCS",
            "name_full": "Bayesian Optimization of Combinatorial Structures",
            "brief_description": "A black-box optimization method for discrete/combinatorial design problems that constructs probabilistic prediction models by sampling from posterior distributions over model parameters to guide exploration.",
            "citation_title": "Bayesian optimization of combinatorial structures",
            "mention_or_use": "mention",
            "system_name": "BOCS (Bayesian optimization of combinatorial structures)",
            "system_type": "probabilistic modeling + Bayesian optimization",
            "domain": "combinatorial design optimization (materials, electronics, manufacturing)",
            "generation_capability": "generates candidate discrete designs/hypotheses (e.g., circuit patterns, mounting point configurations) to optimize objective functions",
            "validation_method": "evaluated via simulations or domain-specific solvers; probabilistic posterior sampling guides exploration and validation relies on simulated/empirical cost function evaluations",
            "novelty_measure": "ensures exploratory search via posterior distributions to discover novel combinations; novelty implicitly measured by search distance from initial samples and exploration-exploitation tradeoff",
            "generation_performance": "described as enabling broader exploratory search than point-estimate methods and successfully applied in cited case studies; no numeric success rates provided in this paper",
            "validation_performance": "validation performed through simulation/evaluation of cost functions; performance improves with more data acquisitions, as shown qualitatively in the cited case study",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "paper indicates that iterative acquisition improves model accuracy and that exploratory approaches like BOCS are better suited to find novel, high-quality solutions than purely exploitative methods",
            "generation_validation_asymmetry": "some asymmetry exists: model-based generation can propose many candidates quickly, but verifying them via simulation/experimental evaluation is more costly; BOCS' probabilistic modeling mitigates premature convergence to familiar solutions",
            "out_of_distribution_performance": "not quantified; BOCS designed to explore beyond local optima, suggesting improved OOD exploration relative to point-estimate methods",
            "calibration_quality": "uses Bayesian posterior distributions which provide uncertainty quantification; calibration quality not numerically reported here",
            "validation_computational_cost": "validation requires running simulations for each candidate and grows with the number of acquisitions; no numeric costs provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "posterior-based uncertainty modeling, exploration-driven acquisition, and integration with domain simulators",
            "evidence_type": "supports",
            "key_findings": "BOCS provides a probabilistic framework that favors exploratory search and uncertainty-aware candidate generation, which helps discover novel combinatorial designs, but validation cost remains a practical bottleneck addressed by iterative data acquisition.",
            "uuid": "e2154.3"
        },
        {
            "name_short": "FMQA",
            "name_full": "Factorization Machine Quantum Annealing",
            "brief_description": "A hybrid method that uses Factorization Machines as fast prediction models combined with quantum annealing for combinatorial optimization to accelerate design search with fewer data points.",
            "citation_title": "Designing metamaterials with quantum annealing and factorization machines",
            "mention_or_use": "mention",
            "system_name": "FMQA (Factorization Machine + Quantum Annealing)",
            "system_type": "hybrid classical ML (factorization machine) + quantum optimization (quantum annealer)",
            "domain": "combinatorial materials/design optimization",
            "generation_capability": "generates discrete design candidates (e.g., metamaterial patterns) guided by fast surrogate models and optimized via quantum annealing",
            "validation_method": "candidates evaluated by domain-specific simulations; surrogate model trained via point estimation from limited data and validated by simulation/experiment",
            "novelty_measure": "not formally defined; FMQA prioritizes fast convergence from limited data which can risk converging to local (possibly familiar) optima rather than exploring highly novel regions",
            "generation_performance": "described as achieving faster convergence with fewer data than Gaussian-process-based methods but with higher risk of local optima; no numeric metrics given in this paper",
            "validation_performance": "validated via simulations in cited studies and produced practical designs in case studies; quantitative validation metrics not provided",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "FMQA's faster convergence from limited data can reduce exploration and thereby potentially miss novel high-quality solutions, affecting validation outcomes for novel tasks negatively",
            "generation_validation_asymmetry": "there is an implied asymmetry: FMQA efficiently generates candidates but may under-explore novelty, making validation more likely to confirm familiar/local solutions than uncover truly novel ones",
            "out_of_distribution_performance": "not quantified; design choices (surrogate bias) suggest weaker OOD exploration compared to fully Bayesian approaches",
            "calibration_quality": "point-estimate surrogate models provide limited uncertainty quantification compared to Bayesian alternatives",
            "validation_computational_cost": "surrogate-assisted approach reduces the number of expensive validation simulations, lowering validation cost per candidate overall; exact numbers not provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "combining surrogates with quantum annealing and designing cost functions (QUBO) to exclude impractical candidates, iterative data acquisition to improve models",
            "evidence_type": "mixed",
            "key_findings": "FMQA trades exploratory breadth for faster convergence—useful in data-limited settings but more likely to converge to local/familiar optima, indicating a generation-validation tradeoff when seeking novel solutions.",
            "uuid": "e2154.4"
        },
        {
            "name_short": "GQE",
            "name_full": "The generative quantum eigensolver (GQE) and its application for ground state search",
            "brief_description": "A generative-model-based approach that uses a decoder-only Transformer to automatically design quantum circuits aimed at preparing molecular ground states, demonstrating automated quantum circuit generation for scientific computation.",
            "citation_title": "The generative quantum eigensolver (gqe) and its application for ground state search",
            "mention_or_use": "mention",
            "system_name": "Generative Quantum Eigensolver (GQE)",
            "system_type": "generative model (decoder-only Transformer) for quantum circuit design",
            "domain": "quantum chemistry / quantum algorithm design",
            "generation_capability": "generates quantum-circuit ansätze tailored to target quantum states (e.g., molecular ground states)",
            "validation_method": "validation via quantum circuit simulation and evaluation of resulting state energies against reference values; performance assessments use simulators to guide training",
            "novelty_measure": "novelty implicitly judged by ability to find circuit forms not previously hand-designed and by achieved ground-state energies compared to baselines",
            "generation_performance": "paper cites successful automated circuit generation in referenced work; no numeric performance values included here",
            "validation_performance": "validated by simulation-based energy comparisons, with reinforcement learning or optimization guiding improvements; no aggregate metrics provided",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "validation via simulators remains reliable for circuits within simulator fidelity; generating highly novel circuits may require more simulation budget and can reveal mismatches between simulator predictions and real hardware",
            "generation_validation_asymmetry": "generation of candidate circuits is fast via the model, but validating their actual utility (e.g., low energy on real hardware) is more expensive, producing an asymmetry between creative generation and costly validation",
            "out_of_distribution_performance": "not quantified; novelty in circuit space may reduce simulator predictive fidelity for real hardware outcomes",
            "calibration_quality": "not reported; uncertainty quantification for generated circuits not discussed in depth",
            "validation_computational_cost": "simulation-based validation can be computationally expensive relative to generation; cost increases with circuit complexity and fidelity requirements",
            "human_validation_required": true,
            "gap_closing_mechanisms": "use of simulators for iterative assessment, reinforcement learning guided by simulator feedback, and combining generative models with performance-driven reward signals",
            "evidence_type": "supports",
            "key_findings": "Generative models can autonomously produce quantum circuits that perform useful scientific computations, but simulator-based validation and resource costs create practical constraints, especially for novel circuit architectures and real-device transfer.",
            "uuid": "e2154.5"
        },
        {
            "name_short": "Minami agent",
            "name_full": "Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver",
            "brief_description": "An AI agent that autonomously generates quantum circuits for combinatorial optimization tasks using an encoder-decoder architecture (GNN encoder, Transformer decoder) and reinforcement learning guided by circuit simulator performance.",
            "citation_title": "Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver",
            "mention_or_use": "mention",
            "system_name": "Minami et al. circuit-generation agent",
            "system_type": "graph neural network encoder + Transformer decoder; reinforcement-learning-guided generative agent",
            "domain": "quantum optimization / combinatorial optimization",
            "generation_capability": "generates quantum circuits tailored to specific QUBO / graph-structured optimization problems",
            "validation_method": "performance assessment via quantum circuit simulators; reinforcement learning uses simulator feedback (e.g., solution quality, objective values) to improve generation",
            "novelty_measure": "novelty judged by the generated circuit architecture relative to prior designs and by achieved optimization performance on unseen problem instances",
            "generation_performance": "paper reports that simulator-guided RL demonstrates effectiveness in generating circuits; no numeric success rates are provided in this paper",
            "validation_performance": "validation limited to simulator evaluations which measure optimization performance; real-device validation not reported here",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "simulator-based validation is reliable for in-distribution problems but may not accurately predict performance on novel or real-hardware instances; paper notes performance assessments guided RL but does not quantify OOD gaps",
            "generation_validation_asymmetry": "generation via learned models is rapid, while validation (simulator runs, potentially many episodes for RL) is costlier, producing an operational asymmetry",
            "out_of_distribution_performance": "not quantified; generalization to out-of-distribution problem graphs is an open concern implied in discussion",
            "calibration_quality": "not discussed",
            "validation_computational_cost": "validation requires simulator evaluations for many generated circuits during RL training, incurring significant computational cost relative to a single-generation pass",
            "human_validation_required": true,
            "gap_closing_mechanisms": "reinforcement learning guided by simulator rewards, encoder-decoder architectures leveraging graph structure, and iterative improvement through performance feedback",
            "evidence_type": "supports",
            "key_findings": "AI agents can learn to generate quantum circuits for optimization by using simulator-based validation signals, but simulator cost and potential simulator-to-hardware transfer gaps limit direct applicability to novel/unseen tasks.",
            "uuid": "e2154.6"
        },
        {
            "name_short": "Sakka LLM automation",
            "name_full": "Automating quantum feature map design via large language models",
            "brief_description": "An example where large language models (LLMs) are used to automate the generation, implementation, and validation of quantum feature map designs, demonstrating an AI scientist workflow that writes and tests code and analyses results.",
            "citation_title": "Automating quantum feature map design via large language models",
            "mention_or_use": "mention",
            "system_name": "Sakka et al. LLM-driven quantum feature map automation",
            "system_type": "large language model (LLM) integrated with experiment/code execution pipeline",
            "domain": "quantum machine learning / quantum algorithm design",
            "generation_capability": "generates design ideas, implementation code (feature maps), and experimental protocols for quantum ML tasks",
            "validation_method": "implements generated code, runs simulations or experiments, analyzes results algorithmically (and possibly via human inspection), and iteratively refines designs",
            "novelty_measure": "novelty assessed by deviation from existing human-designed feature maps and by performance improvements in downstream tasks; no formal metric provided in this paper",
            "generation_performance": "demonstrated practical examples of LLM-generated ideas leading to working implementations; no aggregate quantitative metrics provided",
            "validation_performance": "validation performed by implementation and empirical evaluation (simulations/experiments) with analysis of results; quantitative validation metrics not reported in this paper",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "LLM-generated novel designs require empirical/simulation validation; paper emphasizes that automated generation can produce viable novel designs but their validation depends on computational/experimental resources",
            "generation_validation_asymmetry": "LLM can quickly generate many candidate designs but thorough validation (running code, simulations, experiments) is the rate-limiting step, producing an asymmetry",
            "out_of_distribution_performance": "not quantified; generation of out-of-distribution novel designs is possible but validation fidelity and reliability may decline without additional resources",
            "calibration_quality": "LLM confidence calibration not discussed; robustness and uncertainty in generated design efficacy are open challenges",
            "validation_computational_cost": "validation (implementing/running and analyzing experiments) is computationally and time intensive relative to text/code generation; no numeric costs provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "automated code execution pipelines, iterative implement-test-analyze loops, human-in-the-loop checks, and integration of simulators with LLM outputs",
            "evidence_type": "supports",
            "key_findings": "LLMs can act as 'AI scientists' to generate and implement algorithmic ideas, but reliable scientific validation requires running simulations/experiments and human oversight, particularly for novel designs.",
            "uuid": "e2154.7"
        },
        {
            "name_short": "AI-authored papers passing peer review",
            "name_full": "AI-authored papers passing peer review (cited examples)",
            "brief_description": "Reports and demonstrations that AI-generated manuscripts have in some cases passed peer review, indicating AI's capability to produce publishable scientific text and analysis in specific contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AI-authored papers / AI manuscript generation",
            "system_type": "large language models / generative models for text and analysis",
            "domain": "scientific writing / automated analysis",
            "generation_capability": "generates manuscripts, drafts, analyses, and sometimes code or experimental protocols",
            "validation_method": "peer review and editorial evaluation, sometimes supplemented by reproducibility checks and experimental/simulation replication",
            "novelty_measure": "assessed by peer reviewers and editors for originality and contribution; no automated novelty metric cited in this paper",
            "generation_performance": "qualitatively noted that AI-authored papers have succeeded in passing peer review in some instances; no statistics provided",
            "validation_performance": "peer review functions as a filter for quality and correctness, but the paper notes concerns about trust and the degree to which peer review suffices for validating AI-generated scientific claims",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "peer review may be less reliable for detecting subtle errors in novel AI-generated claims, and the paper stresses continued human oversight",
            "generation_validation_asymmetry": "text generation can be high-quality while substantive scientific validation (experimental replication, domain-expert scrutiny) remains necessary, highlighting an asymmetry",
            "out_of_distribution_performance": "not quantified",
            "calibration_quality": "not discussed",
            "validation_computational_cost": "peer review and reproducibility checks are human- and resource-intensive compared to automated text generation",
            "human_validation_required": true,
            "gap_closing_mechanisms": "human peer review, reproduction attempts, and integrated automated experiment/simulation pipelines to check claims",
            "evidence_type": "mixed",
            "key_findings": "AI can generate publishable manuscripts in some cases, but passing peer review does not fully substitute for rigorous empirical validation—especially for novel scientific claims—so human oversight and reproducibility remain essential.",
            "uuid": "e2154.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "rating": 2
        },
        {
            "paper_title": "Bayesian optimization of combinatorial structures",
            "rating": 2
        },
        {
            "paper_title": "Designing metamaterials with quantum annealing and factorization machines",
            "rating": 2
        },
        {
            "paper_title": "The generative quantum eigensolver (gqe) and its application for ground state search",
            "rating": 2
        },
        {
            "paper_title": "Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver",
            "rating": 2
        },
        {
            "paper_title": "Automating quantum feature map design via large language models",
            "rating": 2
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated openended scientific discovery",
            "rating": 2
        }
    ],
    "cost": 0.0165205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering
15 May 2025</p>
<p>Tadashi Kadowaki 
Global R&amp;D Center for Business by Quantum-AI Technology
National Institute of Advanced Industrial Science and Technology
IbarakiJapan</p>
<p>DENSO CORPORATION
TokyoJapan</p>
<p>Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering
15 May 2025B5AE5C65A7CB6CC2A24FE003C03B9510arXiv:2505.10012v1[quant-ph]AI4ScienceQuantum CAEComputer-Aided EngineeringDigital scientists
Recent advances in artificial intelligence (AI) and quantum computing are accelerating automation in scientific and engineering processes, fundamentally reshaping research methodologies.This perspective highlights parallels between scientific automation and established Computer-Aided Engineering (CAE) practices, introducing Quantum CAE as a framework that leverages quantum algorithms for simulation, optimization, and machine learning within engineering design.Practical implementations of Quantum CAE are illustrated through case studies for combinatorial optimization problems.Further discussions include advancements toward higher automation levels, highlighting the critical role of specialized AI agents proficient in quantum algorithm design.The integration of quantum computing with AI raises significant questions about the collaborative dynamics among human scientists and engineers, AI systems, and quantum computational resources, underscoring a transformative future for automated discovery and innovation.</p>
<p>I. INTRODUCTION</p>
<p>Machine learning traces its origins to the perceptron introduced by Rosenblatt in 1958, marking the inception of its development within artificial intelligence [1].Initially constrained by the simplicity of linear classifiers, progress stagnated until breakthroughs in multilayer neural networks and the popularization of the backpropagation algorithm in the late 1980s [2].Subsequent innovations, including support vector machines (SVM) by Cortes and Vapnik [3], decision trees [4], and ensemble methods such as random forests [5] and gradient boosting [6], significantly enhanced performance in practical applications.</p>
<p>The 2010s witnessed an explosive growth in deep learning, driven by massive datasets and increased computational capabilities provided by GPUs, which led to outstanding successes in image and speech recognition tasks [7].Architectures like convolutional neural networks (CNNs) demonstrated by Krizhevsky et al. [8], recurrent neural networks (RNNs) by Hochreiter and Schmidhuber [9], and Transformers by Vaswani et al. [10] have substantially outperformed previous methods.Particularly in natural language processing (NLP), Transformers facilitated major advancements exemplified by large language models such as GPT [11] and BERT [12].Utilizing self-supervised learning and enormous text datasets, these models excel in text generation, dialogue systems, and other diverse applications, reshaping the landscape of AI research.</p>
<p>Additionally, advancements in deep reinforcement learning enabled AI to surpass human experts in highly complex games like Go [13].Despite its enormous combinational complexity, Go is a complete-information game.Current AI technologies now surpass top human players in even more complex scenarios, such as competitive racing games [14].</p>
<p>Practical industrial applications of AI include advanced driver-assistance systems that enhance automotive safety through onboard cameras and sophisticated feedback mechanisms.Research and development in selfdriving technology actively involve automotive manufacturers and IT companies alike, integrating cutting-edge AI techniques, including deep reinforcement learning and large language models.</p>
<p>The ability of AI to exceed human capabilities in specialized tasks suggests transformative potential across scientific and industrial domains.Recognizing this significance, organizations such as the Japan Science and Technology Agency (JST), the National Academies of Sciences (NAS), and the Organisation for Economic Co-operation and Development (OECD) have released strategic proposals concerning AI's integration into scientific discovery and research methodologies [15][16][17].These initiatives, labeled as "Novel Turing Challenge," "Accelerated Discovery," or "AI for Science (AI4Science)," emphasize automating scientific discovery processes, wherein humans define objectives, and AI autonomously formulates hypotheses, performs experiments, and iteratively verifies outcomes.</p>
<p>The notion of scientific automation questions which research activities can be delegated to AI and robotics and explores the resulting efficiencies.AI endows systems with sensory and cognitive capabilities, whereas robotics provides mechanical movement and physical interaction abilities.This technological convergence could ultimately lead to AI-enabled robots assuming roles traditionally reserved for human scientists.Such possibilities have long been explored in science fiction, exemplified by The Greatest Robot on Earth in Osamu Tezuka's Astro Boy, in which a robot scientist surpasses human scientists in both intellect and capability [18].</p>
<p>As AI and robotics technologies mature, their realworld application in scientific research is anticipated to accelerate, influencing both technical methodologies and societal interactions.Currently, many scientists and engineers operate under corporate or national interests, driving increased demands for productivity and discovery.Integrating AI and robotics to enhance research efficiency thus becomes increasingly essential.The 2024 Nobel Prize in Chemistry awarded to scientists involved in AlphaFold further underscores this emerging trend [19].</p>
<p>Scientific discovery involves extracting novel insights from an expansive space of hypotheses, analogous to mathematician Paul Erdős's metaphor of discovering theorems recorded in "The Book" [20].Similar to mining gold, the ease of discovery diminishes over time, necessitating innovation to sustain productivity.As long as scientific discoveries promise economic returns exceeding their investment, research will continue, fostering advances in supporting technologies.</p>
<p>King et al. introduced the concept of robot scientists, demonstrating their effectiveness in functional genomics research [21].Automation leveraging AI and robotics has become prevalent, especially in resource-intensive research areas like genome sequencing and compound library screening.</p>
<p>Similar to the disruptive impact IT had on service sectors, recent AI advancements are poised to extend automation into various scientific fields beyond life sciences, making automation indispensable for scientists and engineers across disciplines and scales.</p>
<p>This paper specifically discusses research activities entirely executable within computational environments for two reasons: firstly, improvements in computational power and simulation algorithms have increased the feasibility of replacing physical experiments; secondly, the practical realization of quantum computing promises to further enhance these capabilities.</p>
<p>Fully automated computational research processesspanning topic selection, programming and debugging, conducting computational experiments, data analysis, manuscript drafting, and even peer review -are already underway [22].Subsequent studies have demonstrated instances of AI-authored papers successfully passing peer review, highlighting the potential for AI agents to efficiently handle demanding research tasks within projects.</p>
<p>In this paper, we overview the concept of scientific automation, explore its parallels with widely adopted Computer-Aided Engineering (CAE), introduce strategies and examples of quantum computing applications within CAE, discuss advancements towards higher automation levels, and conclude with future perspectives.</p>
<p>II. AUTOMATION IN SCIENCE AND ENGINEERING</p>
<p>As generative AI approaches human-like capabilities, the concept of "digital humans" emerges.Analogously, AI systems conducting research and development through computer simulations can be termed "dig-ital scientists."Unlike digital humans, digital scientists do not require communication skills.Instead, they must possess the capability for rapid logical decision-making grounded in scientific knowledge.Consequently, digital scientists neither require distinct personalities nor can autonomously undertake undesired research beyond the scope of human oversight.</p>
<p>Using the term "digital scientist" strategically positions AI as a supportive tool rather than a replacement for human scientists, thereby mitigating social concerns.The automation of scientific research thus fosters collaborative frameworks between human scientists and digital scientists.</p>
<p>But what capabilities must digital scientists possess?The strategic report from JST categorizes scientific automation into six levels (from Level 0 to Level 5), analogous to the autonomy levels for self-driving vehicles defined by SAE International.Level 3 automation corresponds to full autonomy under well-defined conditions, often referred to as "perfect information."</p>
<p>The scientific discovery process, illustrated by the work of King et al., typically involves (1) hypothesis generation based on existing knowledge, (2) experimental design and execution to verify these hypotheses, and (3) data analysis to validate hypotheses.This cyclic process continuously updates knowledge and generates new hypotheses, constituting scientific automation.</p>
<p>At Level 3 in self-driving technology, human oversight ensures AI properly manages steering, acceleration, and braking, intervening only when necessary.Similarly, scientific automation requires human scientists to provide sufficient information and evaluation criteria to AI, automating routine tasks while humans interpret the purpose and implications of results.</p>
<p>Levels 4 and 5 automation require complete adaptability to dynamically changing conditions, capabilities currently beyond practical realization.Analogously, in science, comprehensive automation remains in foundational research stages.Nevertheless, pioneering studies aim at fully automated scientific processes from topic selection to manuscript preparation, as described in the previous section.</p>
<p>King et al. demonstrated Level 3 scientific automation in life sciences, a field inherently reliant on experiments.Conversely, disciplines like materials science employ firstprinciples calculations enabling complete computational workflow, broadly facilitating scientific automation.</p>
<p>This automated methodology closely parallels Computer-Aided Engineering (CAE), widely employed in manufacturing and product design.CAE employs numerical techniques -such as the finite element method (FEM), boundary element method (BEM), and computational fluid dynamics (CFD) -to predict product characteristics based on design parameters.Widely adopted across automotive, aerospace, electronics, material industries, CAE optimizes designs, reduces prototype costs, shortens development cycles, ensures quality control, and supports digital transformation (DX).Here, automation in product design will be termed "design automation."</p>
<p>In design automation, numerical parameters termed "design variables (x)" are simulated to determine their relationship with "product characteristics (y)."Machine learning then models these characteristics from data, creating predictive models analogous to scientific knowledge.Solving inverse problems identifies optimal values of design variables (x * ) that yield desired (maximized or minimized) product characteristics (y * ).Iteratively refining this optimization process drives design automation forward.</p>
<p>Comparatively, scientific automation parallels design automation: (1) Optimization corresponds directly to hypothesis generation, (2) simulation parallels experimental verification, and (3) machine learning aligns with the analysis of resulting data.Initially limited data yields low model accuracy, which progressively improves with iterative simulations and model updates.Figure 1 illustrates this cyclic workflow.</p>
<p>This optimization method, "black-box optimization," addresses unknown input-output relations, with Bayesian optimization often applied for continuous variables.Conversely, combinatorial optimization problems involving discrete variables pose greater challenges and have primarily relied on relaxation techniques until recently.</p>
<p>The subsequent chapters will explore quantum computing applications within CAE and examples of their implementation, advancing the discourse on automation in science and engineering.</p>
<p>III. QUANTUM CAE</p>
<p>Quantum computing presents significant opportunities for enhancing CAE by leveraging quantum algorithms [23] in simulation, machine learning, and optimization, which together are termed Quantum CAE.Integrating quantum computing into product design automation could significantly reduce development lead times, resulting in improved productivity, cost-efficiency, and market responsiveness.</p>
<p>Implementing Quantum CAE requires the development of quantum algorithms tailored to typical CAE simulations, as well as quantum machine learning models and quantum optimization methods to process simulation results efficiently.Quantum optimization is particularly promising for handling discrete design variables commonly encountered in engineering design.Furthermore, seamlessly integrating these quantum tasks through coherent quantum information processing technologies is critical.</p>
<p>Initial stages of Quantum CAE involve executing one of the key tasks (simulation, machine learning, or optimization) using quantum computers.For example, employing quantum annealing for optimization tasks to identify candidate designs (hypotheses) represents a practical starting point.Leveraging existing technologies such as quantum gate simulators or Ising solvers can demonstrate short-term, practical benefits on small-scale problems.</p>
<p>Quantum-inspired classical algorithms, such as those developed by Tang derived from quantum recommendation techniques, demonstrate efficient performance on conventional computers, illustrating how quantum research advances classical computing [24].</p>
<p>The long-term goal of Quantum CAE is fully quantumintegrated CAE processes.However, significant challenges persist, especially the exponential computational cost associated with encoding classical data into quantum states and extracting useful information back into classical form in the worst case.Overcoming these limitations necessitates methods facilitating efficient quantum state information exchange among tasks.Current research efforts focus on facilitating direct quantum-state exchanges between quantum simulations and quantum machine learning models, and developing quantum machine learning techniques capable of learning from limited quantum data [25].Additionally, methods integrating quantum annealing outputs with quantum gate processing to improve optimization effectiveness have been demonstrated [26].</p>
<p>Quantum CAE algorithms fundamentally solve complex optimization problems, aligned with general quantum search algorithms such as Grover adaptive search [27] and quantum walks [28].Unlike abstract quantum oracle-based algorithms, Quantum CAE emphasizes direct integration of concrete physical simulations and evaluation of product characteristics, creating practical, efficient quantum circuits suitable for real-world applications.Such developments highlight the practical realization of foundational quantum computing research, potentially driving entirely new algorithmic approaches.</p>
<p>Quantum CAE at Level 3 automation can address highly complex design challenges, including optimizing fusion reactor designs through advanced plasma simulations and comprehensive design optimizations for inter-stellar spacecraft.Higher automation levels (Levels 4 and 5) will require tackling complex, uncontrollable societal challenges and investigating scientific frontiers beyond current human cognitive capacities.Quantum computing could become indispensable, propelled by Quantum CAE-derived knowledge and technologies.</p>
<p>IV. QUANTUM CAE FOR MANUFACTURING</p>
<p>This chapter presents practical examples of Quantum CAE applications at Level 3 automation, focusing specifically on discrete-variable design problems solved using black-box optimization methods.</p>
<p>Baptista and Poloczek introduced Bayesian Optimization of Combinatorial Structures (BOCS), a method that constructs probabilistic prediction models by sampling from the posterior distribution over model parameters [29].Kitai et al. proposed Factorization Machine Quantum Annealing (FMQA), employing Factorization Machines (FM) as prediction models and quantum annealing for optimization [30].FM differs from Gaussian process methods used in continuous optimization by employing point estimation techniques, enabling faster convergence with fewer data points but increasing the risk of convergence to local optima.Conversely, BOCS utilizes distribution-based modeling similar to Gaussian processes, facilitating broader exploratory searches.</p>
<p>These methods rely on discrete design variables and evaluation approaches, such as simulations, to assess product characteristics.Design optimization problems involving discrete variables are typically NP-hard, complicating the task of obtaining high-quality solutions.Both BOCS and FMQA efficiently derive feasible solutions from limited iterations.</p>
<p>Two case studies illustrate the practical application of these methods:</p>
<p>First, Matsumori et al. addressed mounting-point optimization in automotive electronic control boards [31].To mitigate vibration-related risks in automotive engine rooms, the board's lowest natural frequency must be maximized to avoid resonance and minimize displacement.Increasing the lowest natural frequency typically necessitates additional mounting points, creating trade-offs between improved stability, design flexibility, and manufacturing cost.Formulating this as a multiobjective optimization problem, BOCS and FMQA efficiently identified optimal number of mounting points and their locations.</p>
<p>Next, Okada et al. considered the design of highfrequency noise filters [32].High-frequency circuits require evaluation as distributed-element circuits, accounting for interactions between printed circuit patterns and electronic components.Traditional rule-based methods, AI-based approaches, and topology optimization [33] techniques have been previously applied.Multiple candidates of printed circuit patterns between components were prepared, formulating the design as a combinatorial FIG. 2. Optimization results for a noise filter.Similar solutions were obtained using topology optimization (left, reproduced from [33] with permission) and black-box optimization (right, reproduced from [32]).optimization problem.A cost function employing constraints aligned with Quadratic Unconstrained Binary Optimization (QUBO) was developed to efficiently exclude impractical printed circuit patterns.As a result, practical and optimal printed circuit patterns were effectively identified.Figure 2 shows that the solutions obtained by topology optimization and black-box optimization methods are essentially similar.Figure 3 demonstrates that iterative data acquisition leads to an exploration shift from impractical to practical printed circuit patterns.</p>
<p>Beyond these case studies, potential applications have been explored in various fields, including pharmaceuticals [34][35][36], chemistry [37], optics [38][39][40], magnetic materials [41,42], electronics [43,44], and data science [45,46].These case studies highlight the effectiveness of quantum annealing as well as Ising solvers in enhancing automation frameworks in both manufacturing and scientific fields.If larger-scale quantum annealers become available, prediction model construction could become a bottleneck.Therefore, while quantum algorithms represent a long-term research goal, improving conventional computing algorithms remains an important short-term objective [46,47].</p>
<p>Furthermore, Quantum CAE realization also demands quantum algorithm implementations for simulations.Inspired by quantum algorithms for the Boltzmann equa-tion [48], Igarashi et al. developed a quantum algorithm for solving radiative transfer equations, a class of linear differential equations, which allows efficient implementation via quantum circuits [49].Nonlinear equations, frequently encountered in real-world simulations, require tailored quantum algorithm development [50,51].Additionally, research into implementations using quantum circuits [52] and quantum annealers [53] to generate proposal states with reduced rejection rates for accelerating Monte Carlo simulations exemplifies ongoing advancements in quantum-enhanced computational methods.</p>
<p>V. PERSPECTIVES FOR ADVANCED AUTOMATION</p>
<p>The previous chapters focused primarily on Level 3 automation within scientific and engineering contexts.Advancing to higher automation levels (Levels 4 and 5) necessitates developing sophisticated teams of AI agents capable of autonomously conducting comprehensive research activities -from formulating research topics and conducting experiments to data analysis and manuscript writing.Achieving this goal requires enhancing individual agent capabilities and developing advanced technologies for effective collaboration and communication among agents within an integrated framework.</p>
<p>Developing practical AI agents requires a combination of versatile, general-purpose agents and highly specialized agents equipped with expert knowledge or specialized skills.Quantum computing's role in accelerating scientific and design processes underscores the importance of developing specialized agents proficient in quantum algorithms and quantum circuit design.Recent research has explored generative models that autonomously design quantum circuits tailored to specific objectives.For instance, Nakaji et al. introduced the Generative Quantum Eigensolver (GQE), employing a decoder-only Transformer to compute molecular ground states through automated quantum circuit generation [54].Extending this concept further, Minami et al. introduced an AI agent capable of autonomously generating quantum circuits specifically designed for solving optimization problems [55].</p>
<p>Discrete optimization problems are commonly formulated using Quadratic Unconstrained Binary Optimization (QUBO).Since QUBO matrices generalize adjacency matrices used in graph theory, these problems naturally lend themselves to graphical representations.Capitalizing on this graphical representation, an AI agent employing an encoder-decoder neural network architecture was developed.Specifically, the encoder uses a Graph Neural Network (GNN) to process input graph data, while the decoder employs a Transformer to generate quantum circuits for solving optimization problems.Performance assessments using quantum circuit simulators guided reinforcement learning strategies, demonstrating the AI agent's effectiveness in generating quantum cir-... cuits (Fig. 4).Sakka et al. provide a practical example of an AI scientist applying automated quantum machine learning for feature map design by generating ideas, implementing and validating code, and analyzing results, thereby demonstrating the potential of AI-driven quantum algorithm development in practical settings [56].</p>
<p>Circuit Generation
Output State Transformer Decoder Feed Forward Graph Encoder
Developing specialized agents for quantum computing increasingly demands interdisciplinary collaboration between quantum computing specialists and AI experts, alongside integrating advancements from broader AI research.Quantum computing and AI are both developing technologies, and advancements in each independently contribute to automating scientific and engineering processes.Additionally, AI applications are actively being employed to facilitate advancements in quantum computing itself [57].</p>
<p>VI. SUMMARY AND FUTURE PERSPECTIVES</p>
<p>Rapid advancements in AI technology have significantly accelerated automation across various scientific and engineering domains.This paper highlighted how current technological capabilities enable Level 3 automation, comparable to widely established CAE practices in industry.Furthermore, integrating quantum computing into these automated processes offers the potential to significantly enhance their efficiency.Specific examples were presented illustrating how Quantum CAE can deliver tangible benefits even at small scales.</p>
<p>Future research should prioritize two main strategies: First, practical application of existing quantum technologies to specific product development and engineering problems.Second, further development of advanced quantum algorithms in anticipation of future improvements in quantum computing capabilities.Insights and technologies gained through Quantum CAE will not only facilitate advanced scientific research automation but also expand the role of quantum computing toward achieving higher automation levels (Levels 4 and 5).</p>
<p>Central to this vision is the development of advanced AI agents, or "digital scientists," designed to handle specialized tasks and actively collaborate with human scientists.To advance this direction, it is essential to establish integrated infrastructure supporting joint quantum computing and AI research.The current adoption of quantum computing at supercomputing centers worldwide presents an opportunity to build robust, integrated research communities focused on quantum computing and AI.</p>
<p>Lastly, we are approaching an era in which AI surpasses human capabilities in various research domains.Concurrently, AI is gaining access to computational resources that provide quantum acceleration.However, fundamental questions remain regarding how scientists and engineers, businesses, nations, and society will establish effective collaboration frameworks with AI.Answers to these questions will progressively emerge through continuous dialogue between humans and AI systems.Such interactions will leverage AI to efficiently navigate hu-manity's extensive knowledge base, stimulate innovative discoveries, foster deeper intellectual engagement, and support ethical approaches to scientific and technological development.</p>
<p>FIG. 1 .
1
FIG.1.Process of automation in science and product design.Hypotheses are automatically generated based on existing knowledge and validated through experiments or simulations, producing new findings.These findings are integrated into the knowledge base, enabling iterative hypothesis generation.Parentheses indicate specific information processing tasks involved in the automation process.This process parallels design automation workflows such as those found in Computer-Aided Engineering (CAE).</p>
<p>FIG. 3 .
3
FIG. 3. Relationship between the number of data acquisitions (x-axis) and the cost function (y-axis).Feasible solutions emerge as the number of data acquisitions increases.(reproduced from[32])</p>
<p>[</p>
<p>FIG.4.Encoder-decoder model for generating quantum circuits to solve optimization problems.The QUBO matrix, which encodes an optimization problem, is first converted into a graph and then processed using a graph neural network as the encoder and a transformer as the decoder.(reproduced from[55] with modifications)</p>
<p>ACKNOWLEDGMENTSThe author gratefully acknowledges Mitsuru Ambai, Tadayoshi Matsumori, Masato Taki, Akihisa Okada, Asuka Igarashi, Shiro Kawabata, Shunya Minami, Kohei Nakaji, Yoichi Suzuki, Alán Aspuru-Guzuk, Shunta Arai, and Tatsuya Ishigaki for fruitful collaboration and insightful discussions in joint research activities.This work was partly performed for Council for Science, Technology and Innovation (CSTI), Cross-ministerial Strategic Innovation Promotion Program (SIP), "Promoting the application of advanced quantum technology platforms to social issues"(Funding agency: QST).This paper was partly based on results obtained from a project, JPNP16007, commissioned by the New Energy and Industrial Technology Development Organization (NEDO), Japan.
The perceptron: A probabilistic model for information storage and organization in the brain. F Rosenblatt, 10.1037/h0042519Psychological Review. 653861958</p>
<p>Learning representations by back-propagating errors. D E Rumelhart, G E Hinton, R J Williams, 10.1038/323533A0;KWRD=SCIENCENature. 3235331986</p>
<p>Support-vector networks. C Cortes, V Vapnik, 10.1023/A:1022627411411/METRICSMachine Learning. 202731995</p>
<p>Induction of decision trees. J R Quinlan, 10.1007/BF00116251Machine Learning. 1811986</p>
<p>Random forests. L Breiman, 10.1023/A:1010933404324/METRICSMachine Learning. 452001</p>
<p>Greedy function approximation: A gradient boosting machine. J H Friedman, The Annals of Statistics. 2911892001</p>
<p>Y Lecun, Y Bengio, G Hinton, 10.1038/NATURE14539;SUBJMETA=117,639,705;KWRD=COMPUTER+SCIENCE,MATHEMATICS+AND+COMPUTINGDeep learning. 2015521436</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. 252012</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, 10.1162/NECO.1997.9.8.1735Neural Computation. 917351997</p>
<p>Polosukhin, Attention is all you need. A Vaswani, G Brain, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Lukasz Kaiser, I , Advances in Neural Information Processing Systems. 302017</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. 3318772020</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K T Google, A I Language, 10.18653/V1/N19-1423Proceedings of the 2019 Conference of the North. the 2019 Conference of the North20194171</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G V D Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, 10.1038/NATURE16961;SUBJMETA=1042,117,1788,378,631,639,705;KWRD=COMPUTATIONAL+SCIENCE,COMPUTER+SCIENCE,REWARDNature. 5294842016</p>
<p>Outracing champion gran turismo drivers with deep reinforcement learning. P R Wurman, S Barrett, K Kawamoto, J Mac-Glashan, K Subramanian, T J Walsh, R Capobianco, A Devlic, F Eckert, F Fuchs, L Gilpin, P Khandelwal, V Kompella, H C Lin, P Macalpine, D Oller, T Seno, C Sherstan, M D Thomure, H Aghabozorgi, L Barrett, R Douglas, D Whitehead, P Dürr, P Stone, M Spranger, H Kitano, 10.1038/S41586-021-04357-7;SUBJMETA=1041,117,639,705;KWRD=APPLIED+MATHEMATICS,COMPUTER+SCIENCENature. 6022232022</p>
<p>Artificial Intelligence and Science -Toward discovery and understanding by AI-driven science. Research Center For, Development Strategy, 2021Japan Science and Technology Agency</p>
<p>. 10.17226/265322022National Academies PressNational Academies of Sciences, Engineering, and Medicine, Automated Research Workflows for Accelerated Discovery</p>
<p>10.1787/a8d820bd-enArtificial Intelligence in Science: Challenges, Opportunities and the Future of Research. OECD Publishing2023</p>
<p>O Tezuka, Astro boy: The greatest robot on earth. Shonen1964</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-2Nature. 5965832021. 2021</p>
<p>P Hoffman, The Man Who Loved Only Numbers. Hyperion Books1998</p>
<p>Functional genomic hypothesis generation and experimentation by a robot scientist. R D King, K E Whelan, F M Jones, P G Reiser, C H Bryant, S H Muggleton, D B Kell, S G Oliver, 10.1038/NATURE02236;KWRD=SCIENCENature. 4272472004</p>
<p>The ai scientist: Towards fully automated openended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024</p>
<p>A M Dalzell, S Mcardle, M Berta, P Bienias, C.-F Chen, A Gilyén, C T Hann, M J Kastoryano, E T Khabiboulline, A Kubica, G Salton, S Wang, F G S L Brandão, Quantum Algorithms: A Survey of Applications and End-to-end Complexities. Cambridge University Press2025</p>
<p>A quantum-inspired classical algorithm for recommendation systems. E Tang, 10.1145/3313276.3316310;PAGE:STRING:ARTICLE/CHAPTERProceedings of the Annual ACM Symposium on Theory of Computing. the Annual ACM Symposium on Theory of Computing2019217</p>
<p>H Y Huang, M Broughton, J Cotler, S Chen, J Li, M Mohseni, H Neven, R Babbush, R Kueng, J Preskill, J R Mcclean, 10.1126/SCIENCE.ABN7293/SUPPL_FILE/SCIENCE.ABN7293_SM.PDFQuantum advantage in learning from experiments. 20223761182</p>
<p>Enhancing quantum annealing in digital-analog quantum computing. T Kadowaki, 10.1063/5.0179540/3280427APL Quantum. 1261012024</p>
<p>Grover adaptive search for constrained polynomial binary optimization. A Gilliam, S Woerner, C Gonciulea, 10.22331/q-2021-04-08-42820215428</p>
<p>Combinatorial optimization via highly efficient quantum walks. S Marsh, J B Wang, 10.1103/PHYSREVRESEARCH.2.023302/FIGURES/8/MEDIUMPhysical Review Research. 2233022020</p>
<p>R Baptista, M Poloczek, Bayesian optimization of combinatorial structures, 35th International Conference on Machine Learning, ICML. 2018. 20182782</p>
<p>Designing metamaterials with quantum annealing and factorization machines. K Kitai, J Guo, S Ju, S Tanaka, K Tsuda, J Shiomi, R Tamura, 10.1103/PhysRevResearch.2.013319Physical Review Research. 2133192020</p>
<p>Application of qubo solver using black-box optimization to structural design for resonance avoidance. T Matsumori, M Taki, T Kadowaki, 10.1038/s41598-022-16149-8Scientific Reports. 1212022</p>
<p>Design optimization of noise filter using quantum annealer. A Okada, H Yoshida, K Kidono, T Matsumori, T Takeno, T Kadowaki, 10.1109/ACCESS.2023.3271969IEEE Access. 11443432023</p>
<p>Layout design of components and conductors in noise filter by integrative optimization of structural topology and external variables. S Maruyama, S Yamasaki, K Nomura, K Yaji, K Fujita, 10.11421/JSCES.2021.20210007Transactions of the Japan Society for Computational Engineering and Science. 2021202100072021</p>
<p>Toward realworld automated antibody design with combinatorial bayesian optimization. A Khan, A I Cowen-Rivers, A Grosnit, D.-G.-X Deik, P A Robert, V Greiff, E Smorodina, P Rawat, R Akbar, K Dreczkowski, R Tutunov, D Bou-Ammar, J Wang, A Storkey, H Bou-Ammar, 10.1016/j.crmeth.2022.100374Cell Reports Methods. 31003742023</p>
<p>Quantum annealing designs nonhemolytic antimicrobial peptides in a discrete latent space. A Tučs, F Berenger, A Yumoto, R Tamura, T Uzawa, K Tsuda, 10.1021/ACSMEDCHEMLETT.2C00487/ASSET/IMAGES/LARGE/ML2C00487_0003.JPEGACS Medicinal Chemistry Letters. 145772023</p>
<p>Chemical design with gpu-based ising machines. Z Mao, Y Matsuda, R Tamura, K Tsuda, 10.1039/D3DD00047HDigital Discovery. 210982023</p>
<p>Quantum-classical computational molecular design of deuterated high-efficiency oled emitters. Q Gao, G O Jones, T Kobayashi, M Sugawara, H Yamashita, H Kawaguchi, S Tanaka, N Yamamoto, 10.34133/icomputing.0037Intelligent Computing. 22023</p>
<p>High-performance transparent radiative cooler designed by quantum computing. S Kim, W Shang, S Moon, T Pastega, E Lee, T Luo, 10.1021/ACSENERGYLETT.2C01969/SUPPL_FILE/NZ2C01969_SI_001.PDFACS Energy Letters. 741342022</p>
<p>Designing thermal radiation metamaterials via a hybrid adversarial autoencoder and bayesian optimization. D Zhu, J Guo, G Yu, C Y Zhao, H Wang, S Ju, S Ju, 10.1364/OL.453442Optics Letters. 471433952022</p>
<p>Towards optimization of photonic-crystal surface-emitting lasers via quantum annealing. T Inoue, Y Seki, K Ishizaki, S Noda, N Togawa, S Tanaka, 10.1364/OE.476839Optics Express. 30435032022</p>
<p>Quantum annealing optimization method for the design of barrier materials in magnetic tunnel junctions. K Nawa, T Suzuki, K Masuda, S Tanaka, Y Miura, 10.1103/PHYSREVAPPLIED.20.024044/FIGURES/11/MEDIUMPhysical Review Applied. 20240442023</p>
<p>Topology optimization of electromagnetic devices using digital annealer. A Maruo, T Soeda, H Igarashi, 10.1109/TMAG.2022.3184325IEEE Transactions on Magnetics. 582022</p>
<p>Method for analyzing bit error rates (bers) of nonlinear circuits and systems for high-performance signaling. Y Dou, D Jiao, J Yan, J Zhu, 10.1109/TMTT.2021.3125022IEEE Transactions on Microwave Theory and Techniques. 707322022</p>
<p>. C Oh, R Bondesan, D Kianfar, R Ahmed, R Khurana, P Agarwal, R Lepert, M Sriram, M Welling, 2022Bayesian optimization for macro placement</p>
<p>Surrogate-assisted asynchronous multiobjective algorithm for nuclear power plant operations. V Drouet, S Verel, J M Do, 10.1145/3377930.3390206;TOPIC:TOPIC:CONFERENCE-COLLECTIONS&gt;GECCO;PAGE:STRING:ARTICLE/CHAPTERGECCO 2020 -Proceedings of the 2020 Genetic and Evolutionary Computation Conference. 20201073</p>
<p>Lossy compression of matrices by black box optimisation of mixed integer nonlinear programming. T Kadowaki, M Ambai, 10.1038/S41598-022-19763-8;SUBJMETA=117,481,483,639,705,766;KWRD=COMPUTER+SCIENCE,QUANTUM+INFORMATIONScientific Reports. 1212022</p>
<p>A black-box optimization method with polynomial-based kernels and quadratic-optimization annealing. Y Minamoto, Y Sakamoto, arXiv:2501.042252025</p>
<p>Quantum algorithm for the advection-diffusion equation simulated with the lattice boltzmann method. L Budinski, 10.1007/S11128-021-02996-3/FIGURES/10Quantum Information Processing. 2012021</p>
<p>Quantum algorithm for the radiative-transfer equation. A Igarashi, T Kadowaki, S Kawabata, 10.1103/PHYSREVAPPLIED.21.034010/FIGURES/9/MEDIUMPhysical Review Applied. 21340102024</p>
<p>Efficient quantum algorithm for dissipative nonlinear differential equations, Proceedings of the National Academy of. J P Liu, H Øie Kolden, H K Krovi, N F Loureiro, K Trivisa, A M Childs, 10.1073/PNAS.2026805118/SUPPL_FILE/PNAS.2026805118.SAPP.PDFSciences of the United States of America. 118e20268051182021</p>
<p>Koopman-von neumann approach to quantum simulation of nonlinear classical dynamics. I Joseph, 10.1103/PHYSREVRESEARCH.2.043102/FIGURES/3/MEDIUMPhysical Review Research. 2431022020</p>
<p>Quantumenhanced markov chain monte carlo. D Layden, G Mazzola, R V Mishmash, M Motta, P Wocjan, J S Kim, S Sheldon, 10.1038/s41586-023-06095-4Nature. 6192822023. 2023</p>
<p>Quantum annealing enhanced markov-chain monte carlo. S Arai, T Kadowaki, arXiv:2502.080602025</p>
<p>The generative quantum eigensolver (gqe) and its application for ground state search. K Nakaji, L B Kristensen, J A Campos-Gonzalez-Angulo, M G Vakili, H Huang, M Bagherimehrab, C Gorgulla, F Wong, A Mccaskey, J.-S Kim, T Nguyen, P Rao, A Aspuru-Guzik, arXiv:2401.092532024</p>
<p>Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver. S Minami, K Nakaji, Y Suzuki, A Aspuru-Guzik, T Kadowaki, arXiv:2501.169862025</p>
<p>Automating quantum feature map design via large language models. K Sakka, K Mitarai, K Fujii, arXiv:2504.073962025</p>
<p>. Y Alexeev, M H Farag, T L Patti, M E Wolf, N Ares, A Aspuru-Guzik, S C Benjamin, Z Cai, Z Chandani, F Fedele, N Harrigan, J.-S Kim, E Kyoseva, J G Lietz, T Lubowe, A Mccaskey, R G Melko, K Nakaji, A Peruzzo, S Stanwyck, N M Tubman, H Wang, T Costa, arXiv:2411.09131Artificial intelligence for quantum computing. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
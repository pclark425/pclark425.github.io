<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-213 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-213</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-213</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-179d3928ceda469af35f95d9b366989ced3d3476</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/179d3928ceda469af35f95d9b366989ced3d3476" target="_blank">Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A novel Debate and Reflect (D&R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback to guide student models, and introduces Tree-structured Direct Preference Optimization to efficiently leverage these debate logs.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques--such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection--struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e213.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e213.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT-from-MAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning dataset extracted from Multi-Agent Interaction Graphs (MAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SFT training instances constructed by extracting problems and their correct responses from debate MAGs; each instance contains an instruction, a problem, and the gold/correct answer (structured information from previous rounds is excluded). Used as the first distillation stage to bootstrap the student model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / multiple-choice QA (MMLU Pro) and step-by-step math QA (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>SFT instances: (instruction I, problem x, correct answer y) extracted from debate MAGs (teacher+student debates); no structured previous-round info included</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Per Table 5: MATH: 6957 SFT instances; Physics: 6246; Biology: 2827; Computer Science: 1280 (constructed from 1000/974/478/205 debates respectively)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Gold final answers only; distilled from multi-agent debates (teacher-derived solutions); contains Chain-of-Thought style reasoning in source debates though removed from SFT instances; high-quality teacher-correct responses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (MMLU Pro), exact match (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>D&R_SFT (SFT-only distillation using MAG-extracted correct responses) yields per-table: CS 32.68, Physics 29.23, Biology 64.44, MATH exact match 17.64; average 36.00</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>No distillation Mistral-7B-Instruct baseline: CS 20.49, Physics 18.46, Biology 48.95, MATH 8.02; average 23.98</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Absolute lift vs baseline: CS +12.19, Physics +10.77, Biology +15.49, MATH +9.62; average lift +12.02 accuracy points</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>SFT on MAG-extracted correct responses provides the majority of the distillation gains (≈+12 percentage points average), effectively transferring teacher answers into the student before preference optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e213.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e213.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T-DPO-preference-trees</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-structured Direct Preference Optimization (T-DPO) training data (preference trees)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hierarchical preference trees constructed from debate logs: root contains prompt+previous structured information, children are paired chosen (correct) and rejected (incorrect) responses; used to train via a DPO-like objective that compares chosen vs rejected completions conditioned on the root.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct (SFT-initialized student)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / multiple-choice QA (MMLU Pro) and step-by-step math QA (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Preference pairs/trees derived from debates: root prompt (I, x, structured info SI), chosen response y_w (correct), rejected response y_l (incorrect); contains self-reflection and teacher feedback in SI</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Per Table 5 (# T-DPO trees): MATH: 7744; Physics: 4238; Biology: 1214; Computer Science: 670 (constructed from the debates)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Hierarchical reasoning trajectories, contrasts correct vs incorrect reasoning, includes intermediate debate turns, self-reflection (SR) and teacher feedback (TF) in root structured info, captures 'why' an answer is preferred</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (MMLU Pro), exact match (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Full D&R (SFT + T-DPO) yields per-table: CS 33.17, Physics 34.77, Biology 67.36, MATH 17.32; average 38.16</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>D&R_SFT (SFT-only) average 36.00 (CS 32.68, Phys 29.23, Biol 64.44, MATH 17.64)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>T-DPO added ≈+2.16 average absolute accuracy points over SFT-only; per-category delta: CS +0.49, Physics +5.54, Biology +2.92, MATH −0.32 (the MATH drop attributed to student capacity limits)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Preference trees + T-DPO provide additional structured preference learning on top of SFT, yielding modest but consistent average gains (~+2.16 points) and especially helping categories like Physics; effectiveness can depend on student model capacity (MATH exception).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e213.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e213.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Debate-logs-(SR+TF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-round debate logs with Self-Reflection (SR) and Teacher Feedback (TF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training debate data containing multi-turn agent responses plus explicit self-reflection from incorrect agents and corrective feedback from teacher agents; used both to generate SFT instances and as structured info in preference-tree roots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / QA (MMLU Pro) and mathematical reasoning (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Debate transcripts including Responses (R), Self-Reflection (SR) for agents that erred, and Teacher Feedback (TF) from correct teachers; multi-round (up to 4 rounds) MAG format</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Number of debates: MATH 1000 debates; Physics 974; Biology 478; Computer Science 205 (see Table 5). These debates produced 2.5–7.7× more SFT/T-DPO instances than debates themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Contains explicit error analysis, corrective strategies, chain-of-thought reasoning, and teacher-supplied guidance; higher-quality, targeted feedback vs traditional passive distillation data</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (MMLU Pro)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>With SR and TF included in training debates, D&R achieves higher accuracy; see D&R numbers (avg 38.16 with T-DPO and SR/TF present).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Ablations removing SR and/or TF (trained without these components) produce lower accuracy: without SR: −3.07 to −4.92 points depending on setup; without TF: −4.61 to −5.54; without both SR & TF: −5.54 to −6.46 (absolute drops reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Including SR and TF yields up to ≈+5.54 absolute accuracy points (ablation max) compared to removing them; they are critical for transferring self-correction abilities and improving final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Explicit SR and TF content in training debates materially improves distillation: ablations show removal of SR and/or TF causes up to ≈5.5 percentage-point drops, and SR/TF enable the student to self-correct at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e213.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e213.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-teacher-debates-vs-single</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-teacher debate-generated data (multiple strong teachers) versus single-teacher distillation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training data generated by multi-teacher debates (student + multiple strong teachers) yields richer training signals (targeted feedback and iterative corrections) compared to SFT from a single teacher's outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / QA (MMLU Pro) and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multi-teacher debate outputs (GPT-4o, Claude 3.5, Gemini 1.5 Pro used as teachers), producing MAGs, SR/TF and preference trees; compared to single-teacher SFT where only one teacher's outputs are used</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Debates counts as above (MATH 1000, Physics 974, Biology 478, CS 205); single-teacher SFT used analogous datasets but from one teacher</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Richer diversity of insights, corrective feedback directed at student errors, multiple correct answers cross-verified; enables construction of preference trees</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (MMLU Pro), exact match (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Best single-teacher (Claude 3.5) average: 35.21; D&R (multi-teacher + SFT + T-DPO) average: 38.16; per-table comparisons show D&R surpasses all single-teacher baselines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Single-Teacher Distillation (examples): Single Teacher_{Claude 3.5} average 35.21; Single Teacher_{GPT-Ao} avg 33.92; Single Teacher_{Gemini 1.5 Pro} avg 31.83</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>D&R improves at least +2.95 average points over best single-teacher distillation (35.21 -> 38.16); larger gains in some categories</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Involving multiple teacher models and the student in debate produces higher-quality training data (targeted feedback and SR), yielding clear gains (~+3 points avg) over single-teacher SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e213.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e213.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-scale-sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of debate training data scale on distillation (small to full datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of D&R performance when training with different numbers of debates (e.g., 100, 300, 600, 974 debates for Physics) showing D&R remains effective at small scales and improves with more debates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / QA (MMLU Pro - Physics category experimented)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Subsampled debate datasets (varying numbers of debates): 100, 300, 600, and full 974 debates (Physics); corresponding SFT/T-DPO instances produced from each debate</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Explicit experiment sizes for Physics: 100 debates, 300 debates, 600 debates, 974 debates (full set). Each debate yields multiple SFT and preference-tree instances (2.5–7.7× expansion factor)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Data-efficiency due to extracting multiple training instances per debate; structured SR/TF and preference trees amplify signal from few debates</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (MMLU Pro - Physics)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Figure 4 (described in text): D&R outperforms MAGD1 at all scales; performance of D&R monotonically improves with data size (no exact numeric points provided in text for each subsample but qualitative improvement reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>MAGD1 struggles on smaller datasets and fails to distill effectively at small scales</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Qualitative: D&R yields robust performance even with limited debates and continues to improve as debate count increases; efficient utilization yields multiple SFT/T-DPO examples per debate, increasing effective training data by 2.5–7.7×</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>D&R (with T-DPO and debate-structured data) is data-efficient: it provides strong distillation even with small numbers of debates and scales positively with more debates, whereas some baselines (MAGD1) fail at small scales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e213.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e213.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Student-capacity-effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of student model capacity on gains from T-DPO and debate data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that student model capability limits how much additional benefit T-DPO yields; stronger student models (Llama-3.1-8B) gain more from T-DPO on hard tasks (MATH) than weaker students (Mistral-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>comparison: Mistral-7B-Instruct vs Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Mistral), 8B (Llama-3.1)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical reasoning (MATH) and broader reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Same D&R debates / SFT / T-DPO training data distilled into students of different capacity</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Same debate-derived datasets as above (MATH: 1000 debates -> 7744 T-DPO trees etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Effectiveness depends on student capacity to absorb complex reasoning signals; T-DPO benefits require sufficient model representational capacity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact match (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Mistral-7B: baseline MATH 8.02 -> D&R_SFT 17.64 -> D&R (with T-DPO) 17.32 (no further improvement). Llama-3.1-8B: baseline MATH 45.78 -> D&R_SFT 47.06 (+1.28) -> D&R 48.02 (+2.24)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Student-specific baselines (Mistral baseline 8.02; Llama baseline 45.78)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>For stronger student (Llama-3.1-8B): T-DPO provided additional +0.98 absolute beyond SFT (47.06->48.02) and +2.24 vs baseline; for weaker student (Mistral) T-DPO gave no improvement on MATH, indicating capacity bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>The same debate + T-DPO data yields larger incremental gains for more capable student models; student capacity can bottleneck the benefits of preference-structured training on complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models <em>(Rating: 2)</em></li>
                <li>Direct Preference Optimization: Your Language Model is Secretly a Reward Model <em>(Rating: 2)</em></li>
                <li>Iterative Reasoning Preference Optimization <em>(Rating: 2)</em></li>
                <li>Advancing LLM Reasoning Generalists with Preference Trees <em>(Rating: 2)</em></li>
                <li>Self-Rewarding Language Models <em>(Rating: 1)</em></li>
                <li>Step-DPO: Stepwise Preference Optimization for Long-chain Reasoning of LLMs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-213",
    "paper_id": "paper-179d3928ceda469af35f95d9b366989ced3d3476",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "SFT-from-MAG",
            "name_full": "Supervised Fine-Tuning dataset extracted from Multi-Agent Interaction Graphs (MAGs)",
            "brief_description": "SFT training instances constructed by extracting problems and their correct responses from debate MAGs; each instance contains an instruction, a problem, and the gold/correct answer (structured information from previous rounds is excluded). Used as the first distillation stage to bootstrap the student model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_size": "7B",
            "training_stage": "SFT",
            "task_type": "reasoning / multiple-choice QA (MMLU Pro) and step-by-step math QA (MATH)",
            "is_scientific_domain": false,
            "data_type": "SFT instances: (instruction I, problem x, correct answer y) extracted from debate MAGs (teacher+student debates); no structured previous-round info included",
            "data_size": "Per Table 5: MATH: 6957 SFT instances; Physics: 6246; Biology: 2827; Computer Science: 1280 (constructed from 1000/974/478/205 debates respectively)",
            "data_properties": "Gold final answers only; distilled from multi-agent debates (teacher-derived solutions); contains Chain-of-Thought style reasoning in source debates though removed from SFT instances; high-quality teacher-correct responses",
            "performance_metric": "accuracy (MMLU Pro), exact match (MATH)",
            "performance_with_data": "D&R_SFT (SFT-only distillation using MAG-extracted correct responses) yields per-table: CS 32.68, Physics 29.23, Biology 64.44, MATH exact match 17.64; average 36.00",
            "performance_baseline": "No distillation Mistral-7B-Instruct baseline: CS 20.49, Physics 18.46, Biology 48.95, MATH 8.02; average 23.98",
            "performance_lift": "Absolute lift vs baseline: CS +12.19, Physics +10.77, Biology +15.49, MATH +9.62; average lift +12.02 accuracy points",
            "compares_data_types": true,
            "key_finding": "SFT on MAG-extracted correct responses provides the majority of the distillation gains (≈+12 percentage points average), effectively transferring teacher answers into the student before preference optimization.",
            "uuid": "e213.0",
            "source_info": {
                "paper_title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "T-DPO-preference-trees",
            "name_full": "Tree-structured Direct Preference Optimization (T-DPO) training data (preference trees)",
            "brief_description": "Hierarchical preference trees constructed from debate logs: root contains prompt+previous structured information, children are paired chosen (correct) and rejected (incorrect) responses; used to train via a DPO-like objective that compares chosen vs rejected completions conditioned on the root.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct (SFT-initialized student)",
            "model_size": "7B",
            "training_stage": "DPO",
            "task_type": "reasoning / multiple-choice QA (MMLU Pro) and step-by-step math QA (MATH)",
            "is_scientific_domain": false,
            "data_type": "Preference pairs/trees derived from debates: root prompt (I, x, structured info SI), chosen response y_w (correct), rejected response y_l (incorrect); contains self-reflection and teacher feedback in SI",
            "data_size": "Per Table 5 (# T-DPO trees): MATH: 7744; Physics: 4238; Biology: 1214; Computer Science: 670 (constructed from the debates)",
            "data_properties": "Hierarchical reasoning trajectories, contrasts correct vs incorrect reasoning, includes intermediate debate turns, self-reflection (SR) and teacher feedback (TF) in root structured info, captures 'why' an answer is preferred",
            "performance_metric": "accuracy (MMLU Pro), exact match (MATH)",
            "performance_with_data": "Full D&R (SFT + T-DPO) yields per-table: CS 33.17, Physics 34.77, Biology 67.36, MATH 17.32; average 38.16",
            "performance_baseline": "D&R_SFT (SFT-only) average 36.00 (CS 32.68, Phys 29.23, Biol 64.44, MATH 17.64)",
            "performance_lift": "T-DPO added ≈+2.16 average absolute accuracy points over SFT-only; per-category delta: CS +0.49, Physics +5.54, Biology +2.92, MATH −0.32 (the MATH drop attributed to student capacity limits)",
            "compares_data_types": true,
            "key_finding": "Preference trees + T-DPO provide additional structured preference learning on top of SFT, yielding modest but consistent average gains (~+2.16 points) and especially helping categories like Physics; effectiveness can depend on student model capacity (MATH exception).",
            "uuid": "e213.1",
            "source_info": {
                "paper_title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Debate-logs-(SR+TF)",
            "name_full": "Multi-round debate logs with Self-Reflection (SR) and Teacher Feedback (TF)",
            "brief_description": "Training debate data containing multi-turn agent responses plus explicit self-reflection from incorrect agents and corrective feedback from teacher agents; used both to generate SFT instances and as structured info in preference-tree roots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_size": "7B",
            "training_stage": "multiple",
            "task_type": "reasoning / QA (MMLU Pro) and mathematical reasoning (MATH)",
            "is_scientific_domain": false,
            "data_type": "Debate transcripts including Responses (R), Self-Reflection (SR) for agents that erred, and Teacher Feedback (TF) from correct teachers; multi-round (up to 4 rounds) MAG format",
            "data_size": "Number of debates: MATH 1000 debates; Physics 974; Biology 478; Computer Science 205 (see Table 5). These debates produced 2.5–7.7× more SFT/T-DPO instances than debates themselves.",
            "data_properties": "Contains explicit error analysis, corrective strategies, chain-of-thought reasoning, and teacher-supplied guidance; higher-quality, targeted feedback vs traditional passive distillation data",
            "performance_metric": "accuracy (MMLU Pro)",
            "performance_with_data": "With SR and TF included in training debates, D&R achieves higher accuracy; see D&R numbers (avg 38.16 with T-DPO and SR/TF present).",
            "performance_baseline": "Ablations removing SR and/or TF (trained without these components) produce lower accuracy: without SR: −3.07 to −4.92 points depending on setup; without TF: −4.61 to −5.54; without both SR & TF: −5.54 to −6.46 (absolute drops reported in Table 2)",
            "performance_lift": "Including SR and TF yields up to ≈+5.54 absolute accuracy points (ablation max) compared to removing them; they are critical for transferring self-correction abilities and improving final performance.",
            "compares_data_types": true,
            "key_finding": "Explicit SR and TF content in training debates materially improves distillation: ablations show removal of SR and/or TF causes up to ≈5.5 percentage-point drops, and SR/TF enable the student to self-correct at inference.",
            "uuid": "e213.2",
            "source_info": {
                "paper_title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Multi-teacher-debates-vs-single",
            "name_full": "Multi-teacher debate-generated data (multiple strong teachers) versus single-teacher distillation",
            "brief_description": "Training data generated by multi-teacher debates (student + multiple strong teachers) yields richer training signals (targeted feedback and iterative corrections) compared to SFT from a single teacher's outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_size": "7B",
            "training_stage": "multiple",
            "task_type": "reasoning / QA (MMLU Pro) and MATH",
            "is_scientific_domain": false,
            "data_type": "Multi-teacher debate outputs (GPT-4o, Claude 3.5, Gemini 1.5 Pro used as teachers), producing MAGs, SR/TF and preference trees; compared to single-teacher SFT where only one teacher's outputs are used",
            "data_size": "Debates counts as above (MATH 1000, Physics 974, Biology 478, CS 205); single-teacher SFT used analogous datasets but from one teacher",
            "data_properties": "Richer diversity of insights, corrective feedback directed at student errors, multiple correct answers cross-verified; enables construction of preference trees",
            "performance_metric": "accuracy (MMLU Pro), exact match (MATH)",
            "performance_with_data": "Best single-teacher (Claude 3.5) average: 35.21; D&R (multi-teacher + SFT + T-DPO) average: 38.16; per-table comparisons show D&R surpasses all single-teacher baselines",
            "performance_baseline": "Single-Teacher Distillation (examples): Single Teacher_{Claude 3.5} average 35.21; Single Teacher_{GPT-Ao} avg 33.92; Single Teacher_{Gemini 1.5 Pro} avg 31.83",
            "performance_lift": "D&R improves at least +2.95 average points over best single-teacher distillation (35.21 -&gt; 38.16); larger gains in some categories",
            "compares_data_types": true,
            "key_finding": "Involving multiple teacher models and the student in debate produces higher-quality training data (targeted feedback and SR), yielding clear gains (~+3 points avg) over single-teacher SFT.",
            "uuid": "e213.3",
            "source_info": {
                "paper_title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Data-scale-sensitivity",
            "name_full": "Effect of debate training data scale on distillation (small to full datasets)",
            "brief_description": "Analysis of D&R performance when training with different numbers of debates (e.g., 100, 300, 600, 974 debates for Physics) showing D&R remains effective at small scales and improves with more debates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_size": "7B",
            "training_stage": "multiple",
            "task_type": "reasoning / QA (MMLU Pro - Physics category experimented)",
            "is_scientific_domain": false,
            "data_type": "Subsampled debate datasets (varying numbers of debates): 100, 300, 600, and full 974 debates (Physics); corresponding SFT/T-DPO instances produced from each debate",
            "data_size": "Explicit experiment sizes for Physics: 100 debates, 300 debates, 600 debates, 974 debates (full set). Each debate yields multiple SFT and preference-tree instances (2.5–7.7× expansion factor)",
            "data_properties": "Data-efficiency due to extracting multiple training instances per debate; structured SR/TF and preference trees amplify signal from few debates",
            "performance_metric": "accuracy (MMLU Pro - Physics)",
            "performance_with_data": "Figure 4 (described in text): D&R outperforms MAGD1 at all scales; performance of D&R monotonically improves with data size (no exact numeric points provided in text for each subsample but qualitative improvement reported)",
            "performance_baseline": "MAGD1 struggles on smaller datasets and fails to distill effectively at small scales",
            "performance_lift": "Qualitative: D&R yields robust performance even with limited debates and continues to improve as debate count increases; efficient utilization yields multiple SFT/T-DPO examples per debate, increasing effective training data by 2.5–7.7×",
            "compares_data_types": true,
            "key_finding": "D&R (with T-DPO and debate-structured data) is data-efficient: it provides strong distillation even with small numbers of debates and scales positively with more debates, whereas some baselines (MAGD1) fail at small scales.",
            "uuid": "e213.4",
            "source_info": {
                "paper_title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Student-capacity-effect",
            "name_full": "Impact of student model capacity on gains from T-DPO and debate data",
            "brief_description": "Observation that student model capability limits how much additional benefit T-DPO yields; stronger student models (Llama-3.1-8B) gain more from T-DPO on hard tasks (MATH) than weaker students (Mistral-7B).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "comparison: Mistral-7B-Instruct vs Llama-3.1-8B-Instruct",
            "model_size": "7B (Mistral), 8B (Llama-3.1)",
            "training_stage": "multiple",
            "task_type": "mathematical reasoning (MATH) and broader reasoning",
            "is_scientific_domain": false,
            "data_type": "Same D&R debates / SFT / T-DPO training data distilled into students of different capacity",
            "data_size": "Same debate-derived datasets as above (MATH: 1000 debates -&gt; 7744 T-DPO trees etc.)",
            "data_properties": "Effectiveness depends on student capacity to absorb complex reasoning signals; T-DPO benefits require sufficient model representational capacity",
            "performance_metric": "exact match (MATH)",
            "performance_with_data": "Mistral-7B: baseline MATH 8.02 -&gt; D&R_SFT 17.64 -&gt; D&R (with T-DPO) 17.32 (no further improvement). Llama-3.1-8B: baseline MATH 45.78 -&gt; D&R_SFT 47.06 (+1.28) -&gt; D&R 48.02 (+2.24)",
            "performance_baseline": "Student-specific baselines (Mistral baseline 8.02; Llama baseline 45.78)",
            "performance_lift": "For stronger student (Llama-3.1-8B): T-DPO provided additional +0.98 absolute beyond SFT (47.06-&gt;48.02) and +2.24 vs baseline; for weaker student (Mistral) T-DPO gave no improvement on MATH, indicating capacity bottleneck",
            "compares_data_types": true,
            "key_finding": "The same debate + T-DPO data yields larger incremental gains for more capable student models; student capacity can bottleneck the benefits of preference-structured training on complex reasoning tasks.",
            "uuid": "e213.5",
            "source_info": {
                "paper_title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
            "rating": 2,
            "sanitized_title": "magdi_structured_distillation_of_multiagent_interaction_graphs_improves_reasoning_in_smaller_language_models"
        },
        {
            "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Iterative Reasoning Preference Optimization",
            "rating": 2,
            "sanitized_title": "iterative_reasoning_preference_optimization"
        },
        {
            "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
            "rating": 2,
            "sanitized_title": "advancing_llm_reasoning_generalists_with_preference_trees"
        },
        {
            "paper_title": "Self-Rewarding Language Models",
            "rating": 1,
            "sanitized_title": "selfrewarding_language_models"
        },
        {
            "paper_title": "Step-DPO: Stepwise Preference Optimization for Long-chain Reasoning of LLMs",
            "rating": 1,
            "sanitized_title": "stepdpo_stepwise_preference_optimization_for_longchain_reasoning_of_llms"
        }
    ],
    "cost": 0.01979325,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement</h1>
<p>Xiaofeng Zhou ${ }^{1 *}$, Heyan Huang ${ }^{1}$, Lizi Liao ${ }^{2}$<br>${ }^{1}$ Beijing Institute of Technology, ${ }^{2}$ Singapore Management University<br>{zhouxiaofeng, hhy63}@bit.edu.cn, lzliao@smu.edu.sg</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques-such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection-struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D\&amp;R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Treestructured Direct Preference Optimization (TDPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.</p>
<h2>1 Introduction</h2>
<p>LLMs have emerged as powerful tools for tackling complex reasoning tasks (Zelikman et al., 2024; Qwen et al., 2025; He et al., 2024). Their ability to process and generate coherent, context-aware outputs has set a new standard in artificial intelligence (AI). However, the computational demands of training and deploying these models make them inaccessible to many users, highlighting the need for more efficient alternatives. Developing smaller models capable of achieving similar knowledge and reasoning performance is an essential step toward making advanced AI technologies more accessible and sustainable.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of D\&amp;R. The student model debates with teacher models, receiving self-reflection and teacher feedback. A Multi-Agent Interaction Graph (MAG) records debate content, from which preference trees are extracted. All information is then distilled into the student, enhancing its knowledge and reasoning.</p>
<p>Existing strategies for enhancing smaller models generally fall into three categories. First, knowledge distillation transfers knowledge from a larger teacher to a smaller student model (Hinton et al., 2015; Shridhar et al., 2023). While effective for reducing model size, traditional distillation procedures are typically one-off processes that lack targeted guidance from the teacher. That is, the teacher's rich insights into how the student can improve are not fully leveraged, and the student often receives only generic training signals. Second, reinforcement learning from human feedback (RLHF) aligns model behavior with human preferences and can substantially boost reasoning (Stiennon et al., 2020; Liang et al., 2024c). However, it requires extensive human intervention, raising concerns about scalability and cost. Third, multiagent systems and self-reflection frameworks enable models to exchange critiques or engage in introspective refinement during inference (Liang et al., 2024b; Madaan et al., 2023; Wang et al., 2024a). Although these methods show promise in improving model reasoning, they often apply feedback only transiently-without permanently</p>
<p>updating model weights-or use simplistic finetuning schemes that do not fully exploit iterative feedback.</p>
<p>In this paper, we introduce a novel Debate and Reflect (D\&amp;R) framework that combines the strengths of multi-agent debate and actionable teacher feedback to robustly enhance smaller models. Concretely, a student model engages in multiturn debates with one or more stronger teacher models, collecting detailed feedback in the form of error analyses, corrective strategies, and best practices. Crucially, the feedback is not limited to a single inference; rather, we incorporate both self-reflection and teacher feedback mechanisms that help the student to internalize these insights for lasting, parameter-level improvements (Yuan et al., 2024; Ying et al., 2024; Lan et al., 2024). To maximize the efficiency of this training signal, we further propose Tree-structured Direct Preference Optimization (T-DPO) to transform debate logs into hierarchical preference trees (Yuan et al., 2025). By systematically organizing intermediate debate turns and teacher guidance, T-DPO enables the student to learn not just what the best response is, but why certain answers are preferred. Through extensive empirical evaluations, we show that D\&amp;R with T-DPO significantly outperforms conventional methods in accuracy, robustness, and generalization, offering a promising route for achieving high performance in smaller, more accessible language models.</p>
<p>In summary, our contributions are threefold:</p>
<ul>
<li>We introduce a Debate and Reflect paradigm that combines the strengths of multi-agent interaction and targeted teacher feedback to continuously improve smaller models.</li>
<li>We propose a Tree-structured Direct Preference Optimization (T-DPO) to better leverages debate data by structuring it into preference trees for efficient and robust training.</li>
<li>Extensive experiments across diverse NLP tasks, demonstrating that our method significantly outperforms standard knowledge distillation and supervised fine-tuning.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Knowledge Distillation</h3>
<p>Knowledge distillation (Hinton et al., 2015) is a widely used technique for transferring knowledge
from large and high-performing teacher models to smaller student models, enhancing performance while reducing inference costs. Given the varying parameter sizes, capabilities, and computational requirements of LLMs (Touvron et al., 2023; Jiang et al., 2023; Li et al., 2024; Zhang et al., 2024b; Wang et al., 2023; Ren et al., 2022; Zhou et al., 2024; Bai et al., 2024), distilling knowledge from powerful teacher models is a common strategy for optimizing the trade-off between performance and efficiency. Typically, knowledge distillation involves querying teacher models with a set of seed data to extract relevant knowledge, which is then used to train student models through various distillation algorithms (Xu et al., 2024; Gu et al., 2024). In the context of reasoning tasks, Chain of Thought (CoT) reasoning serves as a crucial learning signal, allowing student models to learn structured reasoning steps from their teachers (Mukherjee et al., 2023). Beyond CoT, feedback has also been employed to provide process supervision, further refining student models' reasoning capabilities (Luo et al., 2023). However, conventional knowledge distillation methods are typically static, meaning they lack iterative interactions between teacher and student models. To address this, our work utilizes CoT and feedback from debate-based interactions, providing more targeted and dynamic guidance for student model improvement.</p>
<h3>2.2 Multi-Agent Debate</h3>
<p>Multi-agent debate refines LLM reasoning and decision-making based on LLMs' powerful conversational ability (Chu et al., 2024; Liang et al., 2024a; Yang et al., 2024) and agents' collaborative ability (Li et al., 2023; Tu et al., 2024), by exploring diverse reasoning paths and cross-verifying claims, thereby reducing hallucinations and errors (Du et al., 2024). It also serves as an evaluation mechanism for tasks like question answering and summarization (Chan et al., 2024). Debate formats vary-from collaborative (Xiong et al., 2023) to adversarial (Cohen et al., 2023)—with different communication protocols (Chan et al., 2024). Recent studies show that such frameworks can even drive self-improvement without human supervision (Fu et al., 2023). For instance, MAGDi (Chen et al., 2024) uses multi-teacher debates to build interaction graphs that yield richer learning signals than single-teacher setups. In our work, we leverage multi-agent debates with stronger models to generate richer reasoning and actionable feedback,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: A debate example of our proposed D&amp;R. The left part demonstrates the MAG of a debate involving the teacher models $T_{1}, T_{2}, \cdots$ and the student model $S$. Each node corresponds to an agent in a round, including responses (R), self-reflection (SR), and teacher feedback (TF). The highlighted nodes are extracted to construct a preference tree for T-DPO. In its root node, there is the structured information representing the previous debate.
integrating these insights into our Tree-structured Direct Preference Optimization pipeline.</p>
<h3>2.3 DPO and Reasoning</h3>
<p>Direct Preference Optimization (DPO) [Rafailov et al., 2023] offers a stable and computationally efficient way to align LLMs with specific objectives, making it valuable for refining both behavior and reasoning quality. Several extensions illustrate how DPO can be harnessed to improve reasoning. For instance, RPO [Pang et al., 2024] iteratively applies DPO alongside negative log-likelihood losses to fine-tune models on complex tasks. CPO [Zhang et al., 2024a] and Eurus [Yuan et al., 2025] take a tree-structured approach, building hierarchical reasoning paths to guide DPO-based alignment. Further refinements like Step-DPO [Lai et al., 2024] and Step-Controlled DPO [Lu et al., 2024] apply alignment at each individual reasoning step, ensuring that the model learns correct transitions between steps. In addition, M-DPO [Xiong et al., 2025] shows how DPO can be extended to multi-turn dialogues, incorporating tool usage for solving math problems. In our work, we extend DPO with a Tree-structured approach (T-DPO) that turns debate logs into hierarchical preference trees, enabling the student model to learn from both final answers and the intermediate reasoning in multi-turn debates.</p>
<h2>3 Debate \&amp; Reflect Method</h2>
<p>To improve small models using Debate \&amp; Reflect (D\&amp;R), we construct debates where the small student model engages in discussions with their teacher models, receiving feedback from them (Section 3.1). From debates, we construct preference trees in Multi-Agent Interaction Graphs (MAGs) including debate context for T-DPO (Section 3.2). Finally, we distill these structured insights into the student model with T-DPO, enabling more effective learning (Section 3.3).</p>
<h3>3.1 Small Models Engage in Debates</h3>
<p>To enable iterative interaction and targeted feedback, we design a debate framework where a student model engages in discussions with stronger teacher models. Figure 2 shows a debate example. We use training sets as our data sources to obtain gold answers. During the debates, multiple agents engage in several rounds of discussion on the same problem, exchanging their insights, and ultimately achieving a consensus answer. In each debate round, the teacher models $\mathcal{T}=\left{T_{i}\right}<em i="i">{i=1}^{n}$ and the student model $S$ analyze the structured information from the previous debate round. This information includes prior responses, along with the corresponding self-reflection and feedback. Based on this analysis, each agent generates a new response that includes both a reasoning process and a final answer. To evaluate performance, we compare each agent's response against the gold answer. Agents who get an incorrect answer (such as the student model $S$ ) are prompted to self-reflect, while agents with correct answers (such as a teacher model $T</em>$ ) provide constructive feedback to incorrect agents. Teacher models actively highlight student errors and suggest corrections, while the student model attempts to recognize and address its own mistakes. Then the newly generated responses, including reasoning processes, self-reflections, and feedback, are passed to the next round as updated structured information. In the next round, the models also analyze the strengths and weaknesses of their pre-</p>
<p>vious responses again, identifying potential errors and incorporating corrective feedback to generate next-round responses. This iterative process enables the student model to progressively refine its reasoning and responses. Debates continue until all agents reach a consensus or the maximum number of rounds is reached. Following Chen et al. (2024), we construct MAGs for debates, which record debate processes as structured graphs for distillation.</p>
<h3>3.2 Construct Preference Trees for T-DPO</h3>
<p>To effectively train models on reasoning trajectories, we construct preference trees as structured representations of debate interactions. Inspired by previous studies (Yuan et al., 2025; Zhang et al., 2024a), we propose Tree-structured DPO (T-DPO) for debate distillation, which constructs preference trees from graphs for preference optimization.</p>
<p>The right hand side of Figure 2 shows an example of our preference tree. Preference trees capture small, specific reasoning cases from debates. In the debates mentioned above, agents receive structured information from the previous round and generate new responses. Each agent is given the same input prompt in the same round, containing the problem and the previous structured information. However, due to varying capabilities, agents produce different responses; some are correct, and others are incorrect. Therefore, we choose an input prompt as the root node of a preference tree. A correct response and an incorrect response derived from this prompt become child nodes. The root node stores not only the previous round's responses but also self-reflection and feedback on the incorrect answers. This allows agents to integrate insights from past correct responses while addressing mistakes through refinement. Responses that evolve into correct answers (chosen nodes) illustrate how effective reasoning develops over multiple debate rounds. Conversely, responses that remain incorrect (rejected nodes) highlight lessons that should not be reinforced in the distillation process. Through this hierarchical structure, preference trees systematically map out reasoning trajectories, helping models internalize both successful strategies and critical learning points from past errors.</p>
<h3>3.3 Distillation</h3>
<p>Our distillation pipeline consists of two key stages: Supervised Fine-Tuning (SFT) and Tree-structured DPO (T-DPO). First, we extract problems and their corresponding correct responses from MAGs to construct the SFT dataset. Each data instance in this dataset includes an instruction $I$, a problem $x_{i}$, and its correct answer $y_{i}$, but excludes the structured information from the previous debate round. The student model is optimized with maximum likelihood estimation:</p>
<p>$$
\mathcal{L}<em _I_="(I," _mathcal_D="\mathcal{D" _sim="\sim" x_="x," y_="y)">{S F T}=-\mathbb{E}</em><em _theta="\theta">{S F T}} \log \pi</em>(y \mid I, x)
$$</p>
<p>where $\mathcal{D}<em _theta="\theta">{S F T}$ is the SFT training dataset and the student model $\pi</em>$ is initialized as the original small model before distillation.</p>
<p>In the second stage, we apply T-DPO training to refine the student model's reasoning. The root node of each preference tree serves as the input prompt, incorporating the instruction $I$, the problem $x$, and the previous structured information $S I$. The correct and incorrect child nodes act as the chosen and rejected responses, $y_{w}$ and $y_{l}$, respectively. The student model is optimized with the T-DPO objective:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em I_="I," S="S" _left_I_="\left(I," x_="x," y__w="y_{w">{T-D P O}= &amp; -\mathbb{E}</em>}, y_{l}\right) \sim \mathcal{D<em _theta="\theta">{T-D P O}} \
&amp; {\left[\log \sigma\left(\beta \log \frac{\pi</em> \
&amp; \left.\left.-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid I, x, S I\right)}{\pi_{r e f}\left(y_{l} \mid I, x, S I\right)}\right)\right]
\end{aligned}
$$}\left(y_{w} \mid I, x, S I\right)}{\pi_{r e f}\left(y_{w} \mid I, x, S I\right)}\right.\right.</p>
<p>where $\mathcal{D}<em _theta="\theta">{T-D P O}$ is all preference trees in the TDPO training dataset, $\sigma$ is the logistic function, $\pi</em>$ are initialized with the above SFT-trained student model.}$ is the student model policy, $\pi_{r e f}$ is the base reference policy, and $\beta$ is the hyperparameter controlling the deviation from the base reference policy. Both $\pi_{\theta}$ and $\pi_{r e f</p>
<p>After undergoing SFT and T-DPO, the student model acquires the ability to construct correct reasoning paths while effectively avoiding past errors. During inference, the model processes problems step by step, systematically generating thoughts and analyses, ultimately leading to final answers.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Settings</h3>
<p>This section outlines the major settings. More setting details in Appendix A.</p>
<p>Datasets. The MMLU Pro benchmark (Wang et al., 2024b) is a multi-discipline language understanding and reasoning benchmark, extending the MMLU benchmark (Hendrycks et al., 2021a) with more challenging problems. The dataset is</p>
<p>partially derived from MMLU, with additional samples sourced from the Internet and other datasets. It not only evaluates the models’ knowledge but also emphasizes reasoning capabilities. Notably, it expands the number of answer choices to 10. For our experiments, we choose three categories of MMLU Pro: computer science, physics, and biology. These categories vary in data size and complexity, making them ideal for evaluating both knowledge retention and logical reasoning. Since MMLU Pro does not provide a dedicated training set, we split the original test set into separate training and test sets. Statistical information of datasets is shown in Table 5.</p>
<p>The MATH dataset <em>Hendrycks et al. (2021b)</em> is a benchmark designed to assess step-by-step mathematical reasoning in LLMs. It requires LLMs to think about the problems step by step and generate a final answer, such as a mathematical expression. In our experiments, we adopt the training data in <em>Chen et al. (2024)</em> but reconstruct the debates and MAGs with our teacher and student models.</p>
<p>Baselines. To evaluate our method, we compare it against various distillation baselines. (1) No Distillation: the original Mistral-7B-Instruct model without any distillation. For MMLU Pro, it is tested in a zero-shot setting. For MATH, we provide only a few simple demonstrations from <em>Hendrycks et al. (2021b)</em> to illustrate the expected output format. (2) Single-Teacher Distillation: The student model is distilled using SFT with knowledge from only one teacher model at a time. (3) Multi-Teacher Distillation: MAGD1 <em>Chen et al. (2024)</em> uses both correct and incorrect answers of multiple teacher models for distillation. But it does not involve the student interacting with teachers and lacks reflection or feedback mechanisms. Following the official code of MAGD1, we reimplement it with our debate data. For all baseline models and D&amp;R, We select GPT-4o, Claude 3.5, and Gemini 1.5 Pro as our teacher models, while the student model is Mistral-7B-Instruct <em>Jiang et al. (2023)</em>.</p>
<p>Evaluation Metrics. For MMLU Pro, we use accuracy as the evaluation metric. For MATH, we follow <em>Hendrycks et al. (2021b)</em> to use exact match after normalizing answers to get accuracy.</p>
<p>Implement Details. To ensure high-quality distillation while controlling computational costs, we impose the following settings. When constructing debates, we limit debates to a maximum of 4 rounds to balance cost-efficiency and data richness. When collecting root node data for preference trees, we control the length of structured information since it is impractical to train the student model with too long sequences. We aim to include as much structured information in root nodes as possible and place the remainder into a new preference tree. For the first round, the preference trees have no structured information from the previous round. During distillation, we implement our distillation with LoRA <em>Hu et al. (2022)</em>. The prompt templates used in our experiments are shown in Appendix F.</p>
<h3>4.2 Main Results.</h3>
<p>We show our experiment results in Table 1. The student model’s knowledge and reasoning capability across three disciplines, computer science, physics, and biology, are evaluated by MMLU Pro. The complex mathematic reasoning capability is evaluated by MATH.</p>
<p>D&amp;R significantly enhances the student model through distillation. Before distillation, the original Mistral-7B-Instruct demonstrates moderate performance. On MMLU Pro, it surpasses the random guessing performance (10.00) by a meaningful margin, highlighting its basic knowledge and reasoning capability. However, on MATH, it achieves only 8.02, indicating that it can handle complex reasoning for only a small subset of MATH problems. All distillation methods lead to notable improvements over the original Mistral-7B-Instruct. D&amp;R gets an average improvement of 14.18 (up to 18.41 in the biology category of MMLU Pro). This strong improvement underscores D&amp;R’s ability to capture the valuable insights of debates and recognize the preference between the correct and incorrect answers.</p>
<p>D&amp;R outperforms Single-Teacher Distillation. As baseline models, single-teacher distillation results vary across different teacher models. Among them, Single Teacher_{Claude 3.5} achieves the best average results of single-teacher distillation, while Single Teacher_{Gemini 1.5 Pro} lags behind, with an average performance gap of 3.38. Despite these variations, all single-teacher distillation methods</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table] 1</p>
<p>[table</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>MMLU Pro</th>
<th></th>
<th></th>
<th>MATH</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Computer Science (CS)</td>
<td>Physics</td>
<td>Biology</td>
<td></td>
<td></td>
</tr>
<tr>
<td>No Distillation</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>20.49</td>
<td>18.46</td>
<td>48.95</td>
<td>8.02</td>
<td>23.98</td>
</tr>
<tr>
<td>Single-Teacher Distillation</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Single Teacher_{GPT-Ao}</td>
<td>27.80</td>
<td>28.63</td>
<td>62.34</td>
<td>16.92</td>
<td>33.92</td>
</tr>
<tr>
<td>Single Teacher_{Claude 3.5}</td>
<td>30.73</td>
<td>30.77</td>
<td>62.76</td>
<td>16.56</td>
<td>35.21</td>
</tr>
<tr>
<td>Single Teacher_{Gemini 1.5 Pro}</td>
<td>25.85</td>
<td>24.92</td>
<td>60.67</td>
<td>15.86</td>
<td>31.83</td>
</tr>
<tr>
<td>Multi-Teaher Distillation</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MAGDI</td>
<td>22.93</td>
<td>20.00</td>
<td>54.39</td>
<td>11.26</td>
<td>27.15</td>
</tr>
<tr>
<td>D&amp;R_{SFT}</td>
<td>32.68</td>
<td>29.23</td>
<td>64.44</td>
<td>17.64</td>
<td>36.00</td>
</tr>
<tr>
<td>D&amp;R</td>
<td>33.17</td>
<td>34.77</td>
<td>67.36</td>
<td>17.32</td>
<td>38.16</td>
</tr>
</tbody>
</table>
<p>Table 1: The main results of our experiments on three categories of MMLU Pro and MATH. We compare our D&amp;R with three types of distillation baselines and SFT-only D&amp;R_{SFT}. MAGDI are reimplemented with its official code and our debate data. Bold figures denote the best results.
lead to meaningful improvements, proving that knowledge and reasoning can be effectively transferred from teacher models.</p>
<p>Compared to the single-teacher distillation, D&amp;R achieves better distillation results. On the average performance, D&amp;R improves at least 2.95 (from 35.21 of Single Teacher ${ }_{\text {Claude } 3.5}$ to 38.16 of D&amp;R). The key advantages of D&amp;R include multiple teacher models for richer insights, debate mechanisms for iterative interactions, and T-DPO for structured reasoning preference learning. These factors collectively enable more effective knowledge and reasoning acquisition. The debate process enhances training data quality, focusing on the student model's weaknesses, while T-DPO refines learning by emphasizing the contrast between correct and incorrect responses.</p>
<p>D&amp;R achieves the best performance in MultiTeacher Distillation. D&amp;R ${ }<em F="F" S="S" T="T">{S F T}$ already achieves strong performance, surpassing most baseline models. T-DPO further enhances it, leading to an additional 2.16 improvement in the average performance. On the MATH dataset, the result of D&amp;R is slightly lower than the one of D&amp;R ${ }</em>$ but still higher than all other baseline models. We will conduct further exploration in Section 4.3.4. Compared to MAGDI, our D&amp;R demonstrates a substantial average gain of 11.01 (up to 14.77 in the physics category of MMLU Pro). While both D&amp;R and MAGDI utilize multiple teachers and debates, D&amp;R generates richer debate content. Involving the student model in the debates can lead to higher-quality training data. The teacher models are aware of errors in the student model and can actively provide explicit feedback. The student model
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Trade-off between performance and efficiency. Inspired by Chen et al. (2024), we define efficiency as $1 / \operatorname{avg}($ tokens $)$. D&amp;R improves both performance and efficiency compared to previous distillation methods, surpassing their Pareto frontier. We choose Single Teacher $<em _Claude="{Claude" _text="\text">{\text {Claude } 3.5}\left(\mathrm{ST}</em>\right)$ to represent the SingleTeacher Distillation.
learns not only from teacher responses but also from the feedback and their own self-reflection to know where and why it makes errors. This process makes D&amp;R's distillation approach superior to passively learning from teacher responses without explicit error analysis, as done in MAGDI. Moreover, T-DPO provides a more structured and effective optimization framework than the margin-based objective (Cortes and Vapnik, 1995) and GCN (Kipf and Welling, 2017) of MAGDI. MAGDI underperforms compared to the single-teacher distillation, suggesting its joint learning of three training objectives may not model the debate data effectively.}</p>
<h3>4.3 Detailed Analysis</h3>
<h3>4.3.1 D&amp;R Improves Inference Efficiency</h3>
<p>D&amp;R effectively balance efficiency and performance. Although LLMs and debate can provide</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Additional</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Correction</td>
<td></td>
</tr>
<tr>
<td>D&amp;R</td>
<td>None</td>
<td>34.77</td>
</tr>
<tr>
<td>D&amp;R</td>
<td></td>
<td>36.92</td>
</tr>
<tr>
<td>without SR data</td>
<td>Self-Reflect</td>
<td>33.85 (-3.07)</td>
</tr>
<tr>
<td>without TF data</td>
<td></td>
<td>32.31 (-4.61)</td>
</tr>
<tr>
<td>without SR &amp; TF data</td>
<td></td>
<td>31.38 (-5.54)</td>
</tr>
<tr>
<td>D&amp;R</td>
<td></td>
<td>37.54</td>
</tr>
<tr>
<td>without SR data</td>
<td>Teacher^{∗}</td>
<td>32.62 (-4.92)</td>
</tr>
<tr>
<td>without TF data</td>
<td>Feedback</td>
<td>32.00 (-5.54)</td>
</tr>
<tr>
<td>without SR &amp; TF data</td>
<td></td>
<td>31.08 (-6.46)</td>
</tr>
</tbody>
</table>
<ul>
<li>As during inference, we do not allow teacher models to involve, we let the student model role-play a teacher.
Table 2: Effect of self-reflection (SR) and teacher feedback (TF) in D&amp;R. We also show D&amp;R with additional self-reflection and teacher feedback during inference.</li>
</ul>
<p>strong knowledge and reasoning, they come at a high inference cost. Distillation improves efficiency, making reasoning models more accessible and sustainable. Inspired by <em>Chen et al. (2024)</em>, we measure inference efficiency by calculating token cost per problem across three categories in MMLU Pro (excluding MATH due to API cost). We report our detailed token cost in Appendix C. Figure 3 illustrates the trade-off between performance and efficiency. Debate incurs the highest token cost, making it the least efficient. After distillation, MAGD1 and Single Teacher${}_{\text{Claude 3.5}}$ can generate better reasoning paths, improving their performance with high efficiency. D&amp;R surpasses the boundaries of previous distillation methods, not only improving performance but also effectively enhancing efficiency. It becomes a new optimal balance of performance and inference efficiency.</p>
<h4>4.3.2 D&amp;R Can Correct Itself</h4>
<p>D&amp;R not only improves reasoning but also learns to identify and correct its own mistakes. In our debates, agents are explicitly required to generate self-reflection and feedback to identify and correct errors. During distillation, the student model learns this correction process. To evaluate its error correction ability, we conduct an additional correction step during inference. In correspondence with the self-reflection and feedback in the training debates, we design two additional correction methods. The first one is self-reflection. After generating an initial response, the student model self-reflects on its own response and updates its answer. The second one is teacher feedback simulation. The student model roleplays a teacher model, critiques its own response as if reviewing another agent’s answer, and then corrects errors (both the teacher and stu-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance with different training debates on MMLU Pro physics. The dashed line refers to the original Mistral-7B-Instruct without any distillation.</p>
<p>dent models are actually the same model). Table 2 shows how self-reflection and teacher feedback in training debates improve the student model’s performance. With these additional correction methods, D&amp;R can leverage its learned error correction ability to refine its reasoning. However, without the self-reflection and feedback data from the training debates for distillation, the student model fails to effectively self-correct or simulate teacher feedback during inference. This highlights that beyond reasoning, self-reflection and teacher feedback in training debates are essential for transferring critical correction abilities to the student model, enhancing its performance through additional correction at inference.</p>
<h4>4.3.3 D&amp;R Works on Various Data Scales</h4>
<p>To further investigate D&amp;R’s adaptability to different training data size, we train D&amp;R on the physics category of MMLU Pro using different debate scales: 100, 300, 600, and 974 debates (full set). The results, reported in Figure 4, reveal the following key findings.</p>
<p>At first, our D&amp;R achieves effective distillation learning across all dataset sizes, while MAGD1 struggles with smaller datasets and fails to distill knowledge and reasoning capabilities effectively. Then D&amp;R outperforms MAGD1 with all data scales. And T-DPO always enhances the SFT models, proving that self-reflection and teacher feedback in T-DPO are crucial. Without them, performance declines. The performance of D&amp;R also improves as the data scale increases. Therefore, D&amp;R has better utilization of debate data and still can capture preference with limited data. It can collect multiple SFT data instances and preference trees from a single debate MAG, improving data efficiency. The structured information in our pref-</p>
<table>
<thead>
<tr>
<th>Distillation</th>
<th>Student Models</th>
<th>MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data</td>
<td></td>
<td></td>
</tr>
<tr>
<td>The Same</td>
<td>Mistral-7B-Instruct</td>
<td>8.02</td>
</tr>
<tr>
<td>Training</td>
<td>with D&amp;R_{SFT}</td>
<td>17.64 (+9.62)</td>
</tr>
<tr>
<td>Debates</td>
<td>with D&amp;R</td>
<td>17.32 (+9.30)</td>
</tr>
<tr>
<td>for D&amp;R</td>
<td>Llama-3.1-8B-Instruct</td>
<td>45.78</td>
</tr>
<tr>
<td></td>
<td>with D&amp;R_{SFT}</td>
<td>47.06 (+1.28)</td>
</tr>
<tr>
<td></td>
<td>with D&amp;R</td>
<td>48.02 (+2.24)</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between different student models. The performance of distillation can be affected by the capability of student models. Llama-3.1-8B-Instruct is able to learn more during D&amp;R distillation.
erence trees provides rich knowledge, while selfreflection and feedback data can help the student model deeply understand complex reasoning preferences. Hence D&amp;R can be applied in scenarios with limited data resources, achieving effective improvements with only a small number of debates. When given access to more data, it scales effectively, leading to further performance gains.</p>
<h3>4.3.4 D&amp;R with Different Student Models</h3>
<p>D&amp;R achieves improvements across student models of varying capabilities. The effectiveness of distillation depends not only on teacher models and algorithms but also on the capability of our student model. MATH is a challenging dataset that requires complex reasoning. And the basic performance of Mistral-7B-Instruct on MATH is a little weak. As shown in Table 1, while D&amp;R $R_{S F T}$ achieves a strong distillation performance on the MATH dataset, D&amp;R does not provide further improvements. To investigate whether this limitation stems from the student model's capacity, we distill the same training debates into a more powerful student model, Llama-3.1-8B-Instruct. The results, presented in Table 3, confirm this hypothesis. On the MATH dataset, where Mistral may reaches a distillation bottleneck, Llama-3.1-8B-Instruct continues to improve with both SFT and T-DPO in D&amp;R. The complex reasoning capacity of Mistral-7B-Instruct might not be sufficient for full distillation. It can absorb teacher models’ reasoning capabilities during SFT but reaches its capacity limit, preventing further gains from T-DPO. On the contrary, Llama-3.1-8B-Instruct is stronger and has a much better basic MATH performance than Mistral-7B-Instruct. It has much potential for distillation. Therefore, during the T-DPO, it can still learn more complex reasoning capabilities from our training debates, demonstrating that D&amp;R can work well on the MATH dataset and fit different student</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Variants</th>
<th style="text-align: center;">MMLU Pro</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">Phys.</td>
<td style="text-align: center;">Biol.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ SFT</td>
<td style="text-align: center;">32.68</td>
<td style="text-align: center;">29.23</td>
<td style="text-align: center;">64.44</td>
<td style="text-align: center;">17.64</td>
<td style="text-align: center;">36.00</td>
</tr>
<tr>
<td style="text-align: center;">w/ RPO</td>
<td style="text-align: center;">36.59</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">66.95</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">38.14</td>
</tr>
<tr>
<td style="text-align: center;">w/ T-DPO</td>
<td style="text-align: center;">33.17</td>
<td style="text-align: center;">34.77</td>
<td style="text-align: center;">67.36</td>
<td style="text-align: center;">17.32</td>
<td style="text-align: center;">38.16</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison among D&amp;R variants. Although $w / R P O$ is better than $w / S F T$, it generally performs worse than $w / T$-DPO version.
models. That proves D&amp;R's robust effectiveness across diverse student models.</p>
<h3>4.3.5 The Influence of Different Objectives</h3>
<p>Our proposed T-DPO objective is well-suited for distilling reasoning abilities from debates. In reasoning tasks, various preference-learning objectives exist (Xiong et al., 2025; Zhang et al., 2024a). In this section, we investigate whether T-DPO is sufficient and if other training objectives could replace T-DPO in D&amp;R. RPO (Pang et al., 2024) is a DPO-based training objective designed for reasoning tasks. Previous experiments (Pang et al., 2024) suggest that the sequence-level log probabilities for chosen completions with RPO are increasing, while the standard DPO suffers a decrease.</p>
<p>To compare these objectives, we apply RPO to our preference trees and report the results in Table 4. Following Pang et al. (2024), we set the hyperparameter $\alpha$ in the RPO loss as 1 . We observe that $\mathrm{D} \&amp; \mathrm{R}<em F="F" S="S" T="T">{R P O}$ performs better than $\mathrm{D} \&amp; \mathrm{R}</em>$ in average performance. However, according to our experimental logs, the log probability does not show a consistent trend across different data categories. In some categories, such as physics, RPO increases log probabilities beyond TDPO, yet performance does not improve. In others like computer science, log probabilities show little difference from T-DPO, yet RPO performs well. There is no clear correlation between log probability trends and model performance, suggesting that RPO's effectiveness may be task-dependent. In conclusion, based on preference trees from debate graphs, T-DPO remains the most suitable objective for debate distillation.}$ and close to $\mathrm{D} \&amp; \mathrm{R}_{T-D P O</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed D&amp;R, a novel distillation framework to enhance small models through debates. The debate process, involving teacher and student models, fully exposed the student model's weaknesses, providing self-reflection and feedback. Then, we used T-DPO with preference trees from</p>
<p>debate MAGs, capturing structured information and preference relationships between different reasoning paths. D\&amp;R effectively distilled the teacher models' knowledge and reasoning capabilities into the student model through SFT and T-DPO. Our experiments demonstrate that D\&amp;R achieves superior distillation compared to baseline methods, bridging the gap between smaller student models and larger teacher models. Compared to online debate everytime, D\&amp;R significantly improved inference efficiency. Beyond reasoning, it also strengthened the student model's ability to self-correct. In future, we will explore long-term learning where the student model continues to improve over time through continuous debates and iterative feedback.</p>
<h2>Acknowledgements</h2>
<p>This research was supported by the National Research Foundation, Singapore under its National Large Language Models Funding Initiative (AISG Award No: AISG-NMLP-2024-002). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. It was also supported by the National Natural Science Foundation of China (No. U21B2009) and the Lee Kong Chian Fellowship. This work was done during a visit to Singapore Management University funded by the China Scholarship Council.</p>
<h2>Limitations</h2>
<p>Our study has limitations in two aspects. First, while multi-agent debate and preference learning are explored across a wide range of tasks, such as evaluation and model alignment, our work focuses solely on the knowledge and reasoning tasks. Investigating D\&amp;R in other tasks would be valuable for assessing its generalizability. Second, our evaluation primarily measures the correctness of final answers rather than the validity of reasoning paths, which may lead to some false positive results. Process verification remains an open challenge, and we anticipate future studies that incorporate comprehensive evaluations of both reasoning paths and final outcomes.</p>
<h2>References</h2>
<p>Yu Bai, Xiyuan Zou, Heyan Huang, Sanxing Chen, Marc-Antoine Rondeau, Yang Gao, and Jackie CK</p>
<p>Cheung. 2024. CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59085930, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. In The Twelfth International Conference on Learning Representations.</p>
<p>Justin Chen, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. 2024. MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 7220-7235. PMLR.</p>
<p>Yuqi Chu, Lizi Liao, Zhiyuan Zhou, Chong-Wah Ngo, and Richang Hong. 2024. Towards Multimodal Emotional Support Conversation Systems. arXiv preprint.</p>
<p>Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs LM: Detecting Factual Errors via Cross Examination. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12621-12640, Singapore. Association for Computational Linguistics.</p>
<p>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine Learning, 20(3):273-297.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024. Improving Factuality and Reasoning in Language Models through Multiagent Debate. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 11733-11763. PMLR.</p>
<p>Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. arXiv preprint.</p>
<p>Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge Distillation of Large Language Models. In The Twelfth International Conference on Learning Representations.</p>
<p>Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Ming Liu, Zerui Chen, and Bing Qin. 2024. Planning Like Human: A Dual-process Framework for Dialogue Planning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4768-4791, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring Mathematical Problem Solving With the MATH Dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1.</p>
<p>Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowledge in a Neural Network. CoRR, abs/1503.02531.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv preprint.</p>
<p>Thomas N. Kipf and Max Welling. 2017. SemiSupervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations.</p>
<p>Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2024. Step-DPO: Stepwise Preference Optimization for Long-chain Reasoning of LLMs. arXiv preprint.</p>
<p>Tian Lan, Wenwei Zhang, Chengqi Lyu, Shuaibin Li, Chen Xu, Heyan Huang, Dahua Lin, Xian-Ling Mao, and Kai Chen. 2024. Training Language Models to Critique With Multi-agent Feedback. arXiv preprint.</p>
<p>Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society. In Advances in Neural Information Processing Systems, volume 36, pages 51991-52008. Curran Associates, Inc.</p>
<p>Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, Yiguan Lin, Bin Xu, Ren Bowen, Chong Feng, Yang Gao, and Heyan Huang. 2024. Fundamental Capabilities of Large Language Models and their Applications in Domain Scenarios: A Survey. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11116-11141, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Jinggui Liang, Yuxia Wu, Yuan Fang, Hao Fei, and Lizi Liao. 2024a. A Survey of Ontology Expansion for Conversational Understanding. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 18111-18127,</p>
<p>Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024b. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 17889-17904, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Xinyue Liang, Jiawei Li, Yizhe Yang, and Yang Gao. 2024c. Bit_numeval at SemEval-2024 Task 7: Enhance Numerical Sensitivity and Reasoning Completeness for Quantitative Understanding. In Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024), pages 18301841, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. 2024. Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning. arXiv preprint.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, and Dongmei Zhang. 2023. WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. arXiv preprint.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. In Advances in Neural Information Processing Systems, volume 36, pages 46534-46594. Curran Associates, Inc.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive Learning from Complex Explanation Traces of GPT-4. arXiv preprint.</p>
<p>Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason E. Weston. 2024. Iterative Reasoning Preference Optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.</p>
<p>Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang</p>
<p>Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv preprint.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In Advances in Neural Information Processing Systems, volume 36, pages 53728-53741. Curran Associates, Inc.</p>
<p>Mucheng Ren, Heyan Huang, Yuxiang Zhou, Qianwen Cao, Yuan Bu, and Yang Gao. 2022. TCM-SD: A Benchmark for Probing Syndrome Differentiation via Natural Language Processing. In Chinese Computational Linguistics, pages 247-263, Cham. Springer International Publishing.</p>
<p>Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling Reasoning Capabilities into Smaller Language Models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7059-7073, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and FineTuned Chat Models. arXiv preprint.</p>
<p>Rong-Cheng Tu, Wenhao Sun, Zhao Jin, Jingyi Liao, Jiaxing Huang, and Dacheng Tao. 2024. SPAgent: Adaptive Task Decomposition and Model Selection for General Video Generation and Editing. arXiv preprint.</p>
<p>Bo Wang, Heyan Huang, Yixin Cao, Jiahao Ying, Wei Tang, and Chong Feng. 2024a. QRMeM: Unleash the</p>
<p>Length Limitation through Question then Reflection Memory Mechanism. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 4837-4851, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Bo Wang, Heyan Huang, Xiaochi Wei, Ge Shi, Xiao Liu, Chong Feng, Tong Zhou, Shuaiqiang Wang, and Dawei Yin. 2023. Boosting Event Extraction with Denoised Structure-to-Text Augmentation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11267-11281, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024b. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track.</p>
<p>Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023. Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7572-7590, Singapore. Association for Computational Linguistics.</p>
<p>Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Pior, Mohammad Saleh, Chi Jin, Tong Zhang, and Tianqi Liu. 2025. Building Math Agents with Multi-Turn Iterative Preference Learning. In The Thirteenth International Conference on Learning Representations.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions. In The Twelfth International Conference on Learning Representations.</p>
<p>Yizhe Yang, Heyan Huang, Yang Gao, and Jiawei Li. 2024. Building knowledge-grounded dialogue systems with graph-based semantic modelling. Knowledge-Based Systems, 298:111943.</p>
<p>Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, and Shuicheng Yan. 2024. LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1118511208, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Boji Shan, Zeyuan Liu, Jia Deng, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and</p>
<p>Maosong Sun. 2025. Advancing LLM Reasoning Generalists with Preference Trees. In The Thirteenth International Conference on Learning Representations.</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E Weston. 2024. Self-Rewarding Language Models. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 57905-57923. PMLR.</p>
<p>Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. 2024. Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking. In First Conference on Language Modeling.</p>
<p>Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. 2024a. Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.</p>
<p>Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, and Tat-Seng Chua. 2024b. Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15881606, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Xiaofeng Zhou, Heyan Huang, Zewen Chi, Mucheng Ren, and Yang Gao. 2024. RS-BERT: Pre-training radical enhanced sense embedding for Chinese word sense disambiguation. Information Processing \&amp; Management, 61(4):103740.</p>
<p>A Experimental Setting Details</p>
<p>Datasets The data scale varies significantly across different categories in MMLU Pro. To ensure a sufficient amount of test data, we adopt different training-to-test split ratios: 1:1 for computer science, 3:1 for physics, and 2:1 for biology. When structuring information in the root nodes of preference trees, we filter out failed responses where the content is missing.</p>
<p>Table 5 presents the detailed statistics of our dataset. Most of our debates can proceed to the second round, resulting in meaningful interactions between the agents. In contrast, prior debate datasets contain a large proportion of debates that conclude after the initial round without further interaction. For the statistics of the previous debate data, please refer to Chen et al. (2024). With self-reflection and teacher feedback, agents in our framework can promptly correct errors after the second round, ensuring more consensual correct answers and reducing instances where debates terminate without reaching a valid solution. Furthermore, the number of SFT and T-DPO training instances generated from our debates is 2.5-7.7 times the number of debates, highlighting D&amp;R's efficient utilization of debate data for distillation.</p>
<p>Models To ensure the reproducibility of our work, we use the following versions of the teacher and student models: gpt-40-2024-08-06 for GPT-4o, claude-3-5-sonnet-20241022 for Claude 3.5, gemini-1.5-pro-002 for Gemini 1.5 Pro, and Mistral-7B-Instruct-v0.2 for Mistral-7BInstruct.</p>
<p>Implement Details For debate generation, we use the following generation configurations: temperature is 0.3 , the hard limit max tokens is 700 ( 1000 for MATH and we also specify a soft limit in the prompt templates). For SFT, we train the student model on all datasets for 2 epochs with learning rate as $2 \mathrm{e}-4$ and batch size as 16 .</p>
<p>For T-DPO, we train the student model on MMLU Pro for 3 epochs with learning rate as 5e-6 and batch size as 16. On MATH, we train the student model for 1 epoch with learning rate as le-6 and batch size as 16. We limit the total sequence length to 2000 and the prompt sequence length to 1400 . For LoRA, we set the LoRA adapter rank as 16 and the alpha as 32.</p>
<p>For evaluation, our generation configurations are temperature as 0.3 , top_p as 0.9 , and the hard limit max tokens as 700 (1000 for MATH). For the reimplementation of MAGDi, we follow its original training hyperparameters and generation configurations.</p>
<h2>B Quality Analysis of Debate Data</h2>
<p>Our debate data exhibits higher quality than traditional debate data, providing clearer reasoning structures and more targeted feedback. In D\&amp;R, we introduce an improved debate data generation method that actively involves the student model and explicitly incorporates self-reflection and feedback from teacher models. As a result, our training debate data is enriched with detailed reasoning and targeted guidance. To illustrate the advantages of our approach, Figure 10 presents a comparison between our debate data and traditional debate data from MATH.</p>
<p>For the problem of finding the greatest common divisor, the teacher model in our debate data clearly outlines the use of the Euclidean algorithm, systematically performs the calculations, and verifies the result. In contrast, the debate data from Chen et al. (2024), constructed using a traditional approach, lacks an explicit mention of the prime factorization method and presents a less structured response.</p>
<p>In our debate, the student model initially produces an incorrect response. While it introduces the Euclidean algorithm, it actually applies prime factorization, making multiple calculation errors in the process. However, the inclusion of self-reflection and teacher feedback highlights these mistakes and provides explicit recommendations for correctly applying the Euclidean algorithm. This guidance plays a crucial role in shaping subsequent debate rounds. Traditional multi-agent debate frameworks provide agents only with prior responses, requiring them to independently compare answers and identify correct and incorrect elements. This process adds cognitive complexity, making it harder for the agents to reason effectively. In contrast, our method streamlines error correction by integrating self-reflection and feedback, offering clear suggestions for improvement. Consequently, in the following debate rounds, agents can more efficiently rectify previous mistakes and generate more accurate responses, leading to a more effective learning process.</p>
<h2>C Token Cost</h2>
<p>We report averaged per-problem token cost on the test sets of three categories of MMLU Pro in Table</p>
<p>| Datasets | # Training | Round | # R | # SR | # TF | # SFT | # T-DPO | # Test |
| | Debates | (1/2/3/4) | | | | | | Problems |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| MATH | 1000 | 85/606/149/160 | 9524 | 2579 | 4111 | 6957 | 7744 | 5000 |
| Physics | 974 | 136/696/104/38 | 7968 | 1722 | 3016 | 6246 | 4238 | 325 |
| Biology | 478 | 151/295/24/8 | 3380 | 553 | 1061 | 2827 | 1214 | 239 |
| Computer Science | 205 | 32/151/17/5 | 1620 | 340 | 554 | 1280 | 670 | 205 |</p>
<p>Table 5: Detailed statistics of our datasets. Round (1,2,3,4) shows the number of debates with 1/2/3/4 rounds. R, SR, and TF represent Responses, Self-Reflection, and Teacher Feedback, respectively. # SFT is the number of SFT data constructed from debates. # T-DPO is the number of preference trees constructed from debates for T-DPO.</p>
<p>| Models | MMLU Pro | | | Avg. |
| | CS | Physics | Biology | |
| --- | --- | --- | --- | --- |
| Mistral | 601.87 | 692.51 | 586.92 | 627.10 |
| MAGDI | 578.05 | 619.64 | 533.54 | 577.08 |
| ST${}_{\text{Claude}}$ | 555.38 | 550.27 | 556.16 | 553.94 |
| D&amp;R | 520.24 | 584.17 | 482.07 | 528.83 |
| Debate | 17993.04 | 21789.26 | 10526.94 | 16769.75 |</p>
<p>Table 6: Token cost on the three categories of MMLU Pro. For each category, we report the averaged perproblem token cost on test sets.</p>
<p>| Models | MMLU Pro | | |
| | CS | Physics | Biology |
| --- | --- | --- | --- |
| GPT-4o | 75.12 | 74.77 | 88.70 |
| Claude 3.5 | 77.07 | 77.54 | 87.87 |
| Gemini 1.5 Pro | 78.05 | 76.31 | 84.94 |
| Debate | 84.29 | 85.23 | 90.38 |
| Llama-3.1-8B-Instruct | 46.83 | 39.38 | 65.27 |
| with D&amp;R_{SFT} | 50.24 | 44.92 | 71.97 |
| with D&amp;R | 52.68 | 46.46 | 74.06 |</p>
<p>Table 7: Results of teacher models, debates, and a more powerful student model, Llama-3.1-8B-Instruct, on the three categories of MMLU Pro.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The schematic diagram showing how to construct preference trees from debates.
6, which is used to calculate efficiency in Section 4.3.1. D&amp;R learns to reach answers with better reasoning paths and reduce failures during generation, hence has a better efficiency than Mistral, MAGDI, and ST${}_{\text{Claude}}$. Since knowledge and reasoning capabilities have been distilled into the student model, there is no need to use multiple large models for debate. The token cost of D&amp;R is much lower than that of debate.</p>
<h2>D More Results on MMLU Pro</h2>
<p>In Table 7, we report more results on the three categories of MMLU Pro, including three teacher models, debates, and Llama-3.1-8B-Instrct. The teacher models and debates achieve strong performance, ensuring the quality of distillation data. Besides MATH (as shown in Section 4.3.4), Llama-3.1-8BInstruct also gain improvements on MMLU Pro. It demonstrates the broad applicability and robustness of D&amp;R.</p>
<h2>E The Detailed Schematic Diagram for Constructing Preference Trees</h2>
<p>In Figure 5, we present a schematic diagram to complement Figure 2, illustrating the steps to construct preference trees:</p>
<ol>
<li>Select a correct node, an incorrect node, and several nodes from the previous round.</li>
<li>The correct and incorrect nodes are chosen and rejected, respectively. From them, we trace back their debate history nodes as structured information.</li>
<li>Repeat steps 1-2 to construct more preference trees.</li>
</ol>
<h2>F Designed Prompt Templates in D&amp;R</h2>
<p>In this section, we provide the prompt templates that are used in our D&amp;R for generating debates and distillation in Figure 6, 7, 8, and 9.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">### Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">You are a helpful assistant aiming to answer the following question with reasoning.</td>
</tr>
<tr>
<td style="text-align: left;">You are debating the input question with other agents. You are provided with all answers from the last round, along with</td>
</tr>
<tr>
<td style="text-align: left;">their self-reflections and the feedback received. Please carefully consider these answers and output a new answer.</td>
</tr>
<tr>
<td style="text-align: left;">Please output in the following format:</td>
</tr>
<tr>
<td style="text-align: left;">---</td>
</tr>
<tr>
<td style="text-align: left;">### Answer</td>
</tr>
<tr>
<td style="text-align: left;"><your step-by-step reasoning></td>
</tr>
<tr>
<td style="text-align: left;">Final answer: <your final answer></td>
</tr>
<tr>
<td style="text-align: left;"><choice prompt> Please keep your output concise and limit it to 500 words. Use the most short reasoning to get the</td>
</tr>
<tr>
<td style="text-align: left;">correct answer. Do not output unrelated content! Do not explicitly mention other agents; instead, internally consider their</td>
</tr>
<tr>
<td style="text-align: left;">answers.</td>
</tr>
<tr>
<td style="text-align: left;">### Question</td>
</tr>
<tr>
<td style="text-align: left;"><question></td>
</tr>
<tr>
<td style="text-align: left;">### Last Round Answers</td>
</tr>
<tr>
<td style="text-align: left;"><structured information from last round></td>
</tr>
</tbody>
</table>
<p>Figure 6: The prompt template for generating debates.</p>
<div class="codehilite"><pre><span></span><code>###<span class="w"> </span><span class="nv">Instruction</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">helpful</span><span class="w"> </span><span class="nv">assistant</span><span class="w"> </span><span class="nv">aiming</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">feedback</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">yourself</span>.
<span class="nv">Unfortunately</span>,<span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">previous</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">question</span><span class="w"> </span><span class="nv">was</span><span class="w"> </span><span class="nv">incorrect</span>.<span class="w"> </span><span class="nv">Given</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">final</span><span class="w"> </span><span class="nv">answer</span>,<span class="w"> </span><span class="nv">please</span><span class="w"> </span><span class="nv">reflect</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">it</span><span class="w"> </span><span class="nv">and</span>
<span class="nv">provide</span><span class="w"> </span><span class="nv">feedback</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">response</span>.<span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">identify</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">potential</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">knowledge</span>,<span class="w"> </span><span class="nv">faulty</span><span class="w"> </span><span class="nv">reasoning</span><span class="w"> </span><span class="nv">or</span>
<span class="nv">format</span><span class="w"> </span><span class="nv">errors</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">response</span>.<span class="w"> </span><span class="nv">Please</span><span class="w"> </span><span class="nv">check</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">final</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">any</span><span class="w"> </span><span class="nv">formatting</span><span class="w"> </span><span class="nv">errors</span><span class="w"> </span><span class="nv">compared</span>
<span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">answer</span>.
<span class="nv">Please</span><span class="w"> </span><span class="nv">output</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">format</span>:
###<span class="w"> </span><span class="nv">Feedback</span>
<span class="o">&lt;</span><span class="nv">your</span><span class="w"> </span><span class="nv">feedback</span><span class="o">&gt;</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">don</span><span class="err">&#39;t need to apologize.</span>
<span class="err">Only output your feedback. Please keep your output concise and limit it to 500 words. Provide your most concise</span>
<span class="err">feedback. Do not output a new answer.</span>
<span class="err">### Question</span>
<span class="err">&lt;question&gt;</span>
<span class="err">### Correct Final Answer</span>
<span class="err">&lt;gold answer&gt;</span>
<span class="err">### Your Incorrect Answer</span>
<span class="err">&lt;incorrect reasoning&gt;</span>
<span class="err">Fianl answer: &lt;incorrect answer&gt;</span>
</code></pre></div>

<p>Figure 7: The prompt template for self-reflecting in debates.</p>
<div class="codehilite"><pre><span></span><code>###<span class="w"> </span><span class="nv">Instruction</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">helpful</span><span class="w"> </span><span class="nv">assistant</span><span class="w"> </span><span class="nv">aiming</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">feedback</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">other</span><span class="w"> </span><span class="nv">agents</span>.
<span class="nv">Congratulation</span><span class="o">!</span><span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">answered</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">question</span><span class="w"> </span><span class="nv">correctly</span>.<span class="w"> </span><span class="nv">Based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">answer</span>,<span class="w"> </span><span class="nv">please</span><span class="w"> </span><span class="nv">help</span><span class="w"> </span><span class="nv">another</span><span class="w"> </span><span class="nv">agent</span><span class="w"> </span><span class="nv">who</span>
<span class="nv">answered</span><span class="w"> </span><span class="nv">incorrectly</span>.<span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">feedback</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">pointing</span><span class="w"> </span><span class="nv">out</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">potential</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">knowledge</span>,<span class="w"> </span><span class="nv">faulty</span><span class="w"> </span><span class="nv">reasoning</span><span class="w"> </span><span class="nv">or</span>
<span class="nv">format</span><span class="w"> </span><span class="nv">errors</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">its</span><span class="w"> </span><span class="nv">response</span>.<span class="w"> </span><span class="nv">Please</span><span class="w"> </span><span class="nv">check</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">its</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">final</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">any</span><span class="w"> </span><span class="nv">formatting</span><span class="w"> </span><span class="nv">errors</span><span class="w"> </span><span class="nv">compared</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">correct</span>
<span class="nv">answer</span>.
<span class="nv">Please</span><span class="w"> </span><span class="nv">output</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">format</span>:
###<span class="w"> </span><span class="nv">Feedback</span>
<span class="o">&lt;</span><span class="nv">your</span><span class="w"> </span><span class="nv">feedback</span><span class="o">&gt;</span>
</code></pre></div>

<p>Only output your feedback. Please keep your output concise and limit it to 500 words. Provide your most concise feedback. Do not output a new answer.
### Question
<question>
### Your Correct Answer
<correct reasoning>
Final answer: <correct answer>
### Incorrect Answer from Another Agent
<incorrect reasoning>
Final Answer: <incorrect answer>
Figure 8: The prompt template for generating feedback in debates.</p>
<div class="codehilite"><pre><span></span><code><span class="err">###</span><span class="w"> </span><span class="nx">Instruction</span>
<span class="nx">You</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">helpful</span><span class="w"> </span><span class="nx">assistant</span><span class="w"> </span><span class="nx">aiming</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">question</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">step</span><span class="o">-</span><span class="nx">by</span><span class="o">-</span><span class="nx">step</span><span class="w"> </span><span class="nx">reasoning</span><span class="p">.</span>
<span class="nx">Please</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">format</span><span class="p">:</span>
<span class="err">###</span><span class="w"> </span><span class="nx">Answer</span>
<span class="p">&lt;</span><span class="nx">your</span><span class="w"> </span><span class="nx">step</span><span class="o">-</span><span class="nx">by</span><span class="o">-</span><span class="nx">step</span><span class="w"> </span><span class="nx">reasoning</span><span class="p">&gt;</span>
<span class="nx">Final</span><span class="w"> </span><span class="nx">answer</span><span class="p">:</span><span class="w"> </span><span class="p">&lt;</span><span class="nx">your</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="nx">answer</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="kd">choice</span><span class="w"> </span><span class="nx">prompt</span><span class="p">&gt;</span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">unrelated</span><span class="w"> </span><span class="nx">content</span><span class="p">!</span>
<span class="err">###</span><span class="w"> </span><span class="nx">Question</span>
<span class="p">&lt;</span><span class="nx">question</span><span class="p">&gt;</span>
</code></pre></div>

<p>Figure 9: The prompt template for D\&amp;R distillation.</p>
<h1>Question:</h1>
<p>What is the greatest common divisor of 128,144 and 480 ?</p>
<h2>Teacher response in traditional debates:</h2>
<p>To find the greatest common divisor (GCD) of three numbers:</p>
<ol>
<li>Find the GCD of the first two numbers</li>
<li>Then find the GCD of that number and the third number
$\operatorname{GCD}(128,144) 128=2^{\wedge} 7144=2^{\wedge} 4 * 3^{\wedge} 2$
The prime factors are 2 and 3 . The greatest power of 2 that divides both numbers is $2^{\wedge} 4 . \mathrm{GCD}(128,144)=16$
$\mathrm{GCD}(16,480) 480=2^{\wedge} 5 * 3 * 516$ and 480 are both divisible by 16 .
Therefore, GCD $(128,144,480)=16$</li>
</ol>
<h2>Teacher response in D\&amp;R training debates:</h2>
<p>Let's solve this step by step using the Euclidean algorithm:</p>
<p>1) First, let's find $\operatorname{GCD}(128,144)$</p>
<ul>
<li>$144=1 \times 128+16$</li>
<li>$128=8 \times 16+0$</li>
<li>So, GCD $(128,144)=16$
2) Now, let's find $\operatorname{GCD}(16,480)$</li>
<li>$480=30 \times 16+0$</li>
<li>So, GCD $(16,480)=16$
3) Therefore, GCD $(128,144,480)=16$</li>
<li>We can verify this:</li>
<li>$128=16 \times 8$</li>
<li>$144=16 \times 9$</li>
<li>$480=16 \times 30$</li>
</ul>
<p>Final answer: 16</p>
<h2>Student response in D\&amp;R training debates:</h2>
<ol>
<li>Find the greatest common divisors of 128 and 144 using the Euclidean algorithm.</li>
<li>Set up the equation: $\mathrm{a}=\mathrm{bq}+\mathrm{r}$, where a is the larger number, b is the smaller number, q is the quotient, and r is the remainder.</li>
<li>Repeat step 1 with the new pair of numbers $(b, r)$ until $r$ is zero.</li>
<li>The greatest common divisor (gcd) is the last non-zero value of $b$.</li>
<li>Apply the same process to find the gcd of the gcd of 128 and 144 , and 480 .</li>
</ol>
<p>Find answer: $\$ 128=4$ 'cdot $32^{\wedge} 2$ 'quad $144=4$ 'cdot $36^{\wedge} 2$ 'quad $480=2^{\wedge} 4$ 'cdot $5^{\wedge} 2$ 'cdot $36^{\wedge} 2 \$$ Using the Euclidean algorithm: $\$ 128=4$ 'cdot $144+112$ 'quad $144=1$ 'cdot $112+32$ 'quad $112=3$ 'cdot $36+40$ 'quad $36=1$ 'cdot $40+0 \$$</p>
<p>Therefore, $\operatorname{gcd}(128,144,480)=36$.</p>
<h2>Self-reflection in D\&amp;R training debates:</h2>
<p>.... The prime factorization was then compared to find a common factor, but this approach is not necessary and can lead to incorrect results when dealing with larger numbers. The correct method is to use the Euclidean algorithm directly to find the greatest common divisor of 128,144 , and 480 .</p>
<h2>Feedback in D\&amp;R training debates:</h2>
<p>The incorrect answer has several issues:</p>
<ol>
<li>The agent listed steps of the Euclidean algorithm but didn't actually execute them. Instead, they attempted to use prime factorization, which was unnecessary and led to incorrect calculations.</li>
<li>The prime factorizations provided are incorrect:</li>
<li>$128=2^{\wedge} 7($ not $4 \cdot 32^{\wedge} 2)$</li>
<li>$144=2^{\wedge} 4 \cdot 3^{\wedge} 2($ not $4 \cdot 36^{\wedge} 2)$</li>
<li>$480=2^{\wedge} 5 \cdot 3 \cdot 5\left(\right.$ not $\left.2^{\wedge} 4 \cdot 5^{\wedge} 2 \cdot 36^{\wedge} 2\right)$</li>
</ol>
<p>The agent should have followed through with the Euclidean algorithm steps they outlined and provided a single number as the GCD.</p>
<p>Figure 10: An example from the MATH dataset. Given the question, we show a response from Chen et al. (2024), which is generated with a traditional debate method without any self-reflection, feedback, and student models, and then two responses, a self-reflection, and feedback from our D\&amp;R training debates. Our debate data has a better quality with detailed responses, self-reflection, and feedback.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*This work was done during an internship at SMU.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
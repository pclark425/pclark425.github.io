<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1221 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1221</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1221</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-236087473</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2107.08241v1.pdf" target="_blank">High-Accuracy Model-Based Reinforcement Learning, a Survey</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning has shown remarkable success in the past few years. Highly complex sequential decision making problems from game playing and robotics have been solved with deep model-free methods. Unfortunately, the sample complexity of model-free methods is often high. To reduce the number of environment samples, model-based reinforcement learning creates an explicit model of the environment dynamics. Achieving high model accuracy is a challenge in high-dimensional problems. In recent years, a diverse landscape of model-based methods has been introduced to improve model accuracy, using methods such as uncertainty modeling, model-predictive control, latent models, and end-to-end learning and planning. Some of these methods succeed in achieving high accuracy at low sample complexity, most do so either in a robotics or in a games context. In this paper, we survey these methods; we explain in detail how they work and what their strengths and weaknesses are. We conclude with a research agenda for future work to make the methods more robust and more widely applicable to other applications.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1221.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1221.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative recurrent latent models that learn a compressed spatial-temporal representation of an environment (vision model + memory model + controller) trained (largely) unsupervised to support policy learning and planning in the compact latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative recurrent world model: VAE-based visual encoder/decoder (observation model), recurrent memory (RNN/LSTM) as a temporal latent state, and a lightweight controller (policy) that uses latent features. The world model generates imagined trajectories in latent space for policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (generative RNN + VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>CarRacing / low-dimensional video games; general video-based control</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss of VAE (ELBO), predictive likelihood in latent/RNN rollout, reward/value prediction when used for planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitative: reported to produce compact, temporally-coherent latent predictions sufficient to train simple controllers on car-racing tasks; no quantitative MSE/score numbers provided in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low-to-moderate: latent dimensions provide compressed features that can sometimes be visualized (decoded frames), but internal recurrent dynamics are largely black-box</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>VAE latent visualization (decoded frames) and inspecting generated rollouts; no symbolic interpretations reported</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Moderate-to-high: requires training VAE + recurrent model; survey notes world models are elaborate and computationally demanding but no exact training-time or parameter counts given</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sample-efficient vs naive pixel predictive models for some tasks (enables training small controllers); not directly compared numerically to model-free baselines in survey</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables compact controllers to solve CarRacing and similar tasks; used to train policies that perform competitively for the evaluated domains (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>World models prioritize learning compressed spatio-temporal features useful for control; high-fidelity pixel reconstruction is not required as long as latent captures task-relevant dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Compressing observations into latent space improves sample efficiency and planning tractability but increases architectural complexity and reduces direct interpretability; high computational cost for training vs simpler models.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use VAE for visual encoder/decoder, RNN (LSTM) for temporal dynamics, separate lightweight controller; emphasis on unsupervised learning of latent dynamics and then policy learning using latent rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared favorably to raw-pixel planning for some small video tasks; less interpretable than physics-based simulators; more sample-efficient than naive model-free in domains tested but more complex to train.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends compact latent representations and separating vision/memory/controller; suggests further standardization and simplification of latent/world-model architectures to balance fidelity, compute, and utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1221.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VPN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Prediction Network (VPN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-model that directly predicts future rewards/values in an abstract latent space instead of reconstructing observations, enabling planning in a value-focused compressed representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value prediction network</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Prediction Network (VPN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture with four networks: encoder (observations -> latent), latent transition model (next latent given action), reward model, and value model; planning/backups are performed in latent/value space rather than by predicting full next observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / value-centric latent model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and discrete planning domains (Sokoban/mazes) where value prediction suffices</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Value-prediction accuracy (e.g., TD/bootstrapped value error), reward-prediction error; planning success rate on downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported (in survey) to outperform model-free DQN on certain Atari games (Pacman, Seaquest) and to handle stochastic domains better than observation-based planning; no precise numeric errors provided in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: latent states are explicitly trained to be value-relevant, making them more interpretable in terms of predicted rewards/values than raw-pixel models, but latent features remain neural encodings</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of predicted values/rewards and latent rollouts; no formal disentanglement or symbolic mapping described in survey</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Relatively lower inference cost than full pixel predictors because planning works in low-dimensional latent space; training still requires multiple networks but cheaper than full observation predictors in many cases (survey qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More sample-efficient than pixel-based models for Atari in cited work; offers improved planning horizon utility because predictions focus on value-relevant aspects</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improved game scores over DQN on selected Atari titles per cited work; effective in stochastic domains where pixel prediction is brittle (qualitative report)</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>By prioritizing value-relevant aspects, VPN often translates improved prediction fidelity in latent-value terms into better policy performance; avoids wasteful modeling of irrelevant visual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Sacrifices full observation fidelity for value-relevant fidelity; leads to improved task utility but may reduce generality if tasks require precise observation reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Train encoder/transition/reward/value jointly with value-based RL losses; limit planning depth to control roll-out error accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms observation-predictive models for several Atari games in cited work; compared favorably to DQN in some domains due to focusing on value rather than pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests value-focused latent representations and limited rollout depth as robust choices; encourages using value-based objectives to shape latent representations for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1221.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (Recurrent State Space Model for latent dynamics planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent dynamics model (RSSM) combining an encoder, variational observation model, stochastic and deterministic transition components, and a reward model used with MPC (CEM) to plan in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent State Space Model (RSSM): an encoder/VAE to map pixels to latent, an RNN-based transition combining stochastic and deterministic latents, an observation model, and a reward predictor; planning performed via model-predictive control (CEM) in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (probabilistic RSSM with VAE encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control tasks (DeepMind Control Suite / MuJoCo) and visual control from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Latent predictive log-likelihood (ELBO), reward-prediction error, downstream task return/episode return</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported to reach performance close to strong model-free baselines on continuous control tasks; survey gives qualitative statement (no numeric fidelity metrics included)</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low: latent representations compress task-relevant dynamics but internal stochastic/deterministic latent behavior remains neural and not directly interpretable</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualizing decoded reconstructions from latent states; no systematic interpretability tools described in survey</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: training VAE + RSSM and running CEM-based MPC each step is computationally intensive; survey notes computational cost is substantial though exact GPU/duration not given</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>High sample efficiency compared to model-free baselines (fewer environment steps needed) but larger wall-clock/training compute per collected sample in some cases (survey cites benchmarking caveats)</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Achieves close-to-model-free performance on many DeepMind Control tasks and enables planning from pixels; Dreamer later builds on PlaNet with improved performance</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Latent RSSM provides latent states sufficient for MPC planning; high latent predictive fidelity tends to translate into good control performance when planning horizon is tuned appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latent compression reduces sample complexity but increases model/training complexity and runtime; probabilistic/stochastic latents provide uncertainty modeling at cost of added complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use stochastic + deterministic latent components (RSSM), VAE encoder for pixels, reward predictor, and CEM-based MPC with short re-planning horizon to limit accumulation of model error.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More sample-efficient than many model-free methods on specified tasks; compared to ensembles/MPC approaches, PlaNet trades ensemble uncertainty modeling for structured latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends short planning horizons with MPC, careful encoder/transition design (RSSM) and tuning of stochastic/deterministic components to balance fidelity and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1221.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (Dream to Control / latent imagination-based control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-imagination actor-critic method that backpropagates value gradients through imagined latent trajectories produced by an RSSM-like model to learn policies end-to-end in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model (RSSM-style): VAE encoder + recurrent latent dynamics; imaginations (short latent rollouts) are used to compute gradients for an actor-critic policy that is trained using imagined returns; planning is implicit via latent imagination and gradient backprop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with latent imagination (model-based actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>DeepMind Control Suite (continuous control), Atari (discrete) in follow-up work</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality of imagined latent rollouts measured by predicted reward/value accuracy, downstream policy return</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported to solve a diverse collection of continuous problems and, in extensions, to reach human-level performance on many Atari games; survey relays qualitative success without tabulated fidelity numbers</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low: learned latent dynamics are neural and not readily interpretable, though decoded reconstructions can be visualized</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of decoded latent rollouts and predicted rewards; no formal interpretability pipeline reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: joint training of RSSM, actor, critic with backprop through imagined trajectories increases compute; survey notes end-to-end/backpropagation planning has large computational effort</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Highly sample-efficient compared to model-free baselines (fewer environment interactions); wall-clock training time may be larger due to model complexity</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Competitive or better than strong model-free baselines on several control benchmarks; later Dreamer variants achieved strong Atari results (survey qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-quality latent imagination (fidelity in predicted rewards/values) directly improves policy learning because gradients are backpropagated through latent rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>End-to-end latent imagination improves sample efficiency but raises computational cost and sensitivity to hyperparameters; complex architectures can be brittle across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use RSSM-like latent with stochastic/deterministic components, short imagined rollouts for stable gradients, actor-critic objective backpropagated through model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Better sample efficiency than many model-free approaches; compared to PlaNet, Dreamer adds explicit policy/value networks trained via backprop through latent predictions for stronger performance on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests limiting imagination horizon, careful architecture tuning, and improving computational efficiency of end-to-end planning as priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1221.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PETS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Ensembles with Trajectory Sampling (PETS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble of probabilistic neural network dynamics models used with sampling-based uncertainty propagation and CEM/MPC planning to achieve sample-efficient continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning in a handful of trials using probabilistic dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PETS (Probabilistic Ensembles with Trajectory Sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learn an ensemble of probabilistic neural dynamics models (each predicts next-state distribution); propagate particles/trajectories sampled from ensemble (TS); use CEM to select action sequences and execute first action in MPC loop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic ensemble latent/explicit dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MuJoCo continuous control tasks (pusher, reacher, half-cheetah, hopper, walker)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predictive uncertainty calibration, next-state prediction error (MSE), downstream task return under MPC</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey reports that PETS approaches asymptotic model-free baselines on several MuJoCo tasks and is sample-efficient; precise numeric fidelity metrics are not provided in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: ensemble variance provides an interpretable uncertainty estimate (aleatoric/epistemic separation to some degree), but base models are neural black boxes</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Use ensemble disagreement/variance as an uncertainty proxy and to guide planning (short rollouts); no latent visualization described</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High at inference: propagating many particles across ensemble for CEM sampling is compute-intensive; training multiple models increases training compute</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More sample-efficient than model-free (survey cites benchmarking where model-based used ~200k time steps vs ~1M for model-free), but wall-clock time and compute per step may be higher</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Matches or approaches model-free asymptotic performance on many MuJoCo tasks while using far fewer environment interactions (qualitative reported results)</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Ensemble predictive uncertainty reduces harmful effects of model error during planning; short MPC horizons mitigate compounding errors leading to strong control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Ensembles give robustness/uncertainty at the cost of increased training/inference compute; shorter rollouts reduce accumulated error but may limit long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use probabilistic neural nets in ensemble, trajectory sampling (TS) for propagation, CEM/MPC with re-planning each timestep and short horizons to reduce model-bias.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Benchmarked favorably against single deterministic models and many model-free baselines in terms of sample efficiency; compared to latent RSSMs, PETS uses explicit ensembles to quantify uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends combining ensembles with MPC and using short, re-planned horizons (MPC) to balance fidelity, robustness, and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1221.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PILCO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PILCO: Probabilistic Inference for Learning COntrol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gaussian process based model-based RL system that achieves strong sample efficiency on low-dimensional continuous control by explicitly modeling uncertainty and performing analytic inference for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pilco: A model-based and data-efficient approach to policy search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PILCO (GP-based dynamics model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses Gaussian processes to model transition dynamics probabilistically, computes analytic moment-matching predictions for multi-step propagation, and uses gradient-based policy optimization under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic explicit (Gaussian process) world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Low-dimensional continuous control (pendulum, cart-pole, mountain car)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predictive posterior mean and variance (GP predictive MSE/likelihood), policy return (expected cumulative reward)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Highly sample-efficient on small benchmarks (cartpole, mountain car) in original work; survey notes PILCO scales poorly to high-dimensional problems</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability for small problems: GP predictive mean/variance are transparent and uncertainty is explicit</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Analytic predictive distributions from GP; uncertainty quantified directly by GP variance</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Moderate-to-high scaling: GP training/inference scales poorly with dataset size (cubic in samples), limiting applicability to low-dimensional/small-data settings</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Extremely sample-efficient relative to many methods on small problems, but not tractable for high-dimensional pixel-based tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Excellent on classic small control tasks (pendulum, cart-pole); not applied to complex/high-dimensional benchmarks in survey</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High fidelity + explicit uncertainty leads to strong control performance in small domains; inability to scale limits broader utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>GP fidelity and interpretability come at computational scaling costs; cannot match deep latent model approaches for high-dimensional domains.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use Gaussian processes for dynamics and analytic moment-matching for multi-step prediction; favor exact uncertainty over model-capacity scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More sample-efficient than deep models on small tasks, but deep latent models and ensembles scale to larger domains where PILCO fails.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey implies PILCO is optimal for small, low-dimensional problems where explicit uncertainty and analytic inference are viable, but recommends other approaches for high-dimensional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1221.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (learning a model for planning without modelling observations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end model-based system that learns an abstract (latent) model of environment dynamics and reward/policy/value predictions, and uses an explicit MCTS planner to plan in the learned latent space without predicting raw observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero (latent abstract model + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Abstract representation network maps observations to latent; learned latent 'dynamics' (transition) that outputs next latent and reward/policy/value predictions; uses conventional MCTS externally to plan using network-provided priors and value estimates (no pixel reconstruction).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (abstract dynamics + prediction heads)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games (Go, Chess, Shogi) and Atari (discrete high-dimensional games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Policy/value prediction accuracy, MCTS-assisted game-play Elo/score, planning success rate</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Achieved state-of-the-art and superhuman performance across Atari, Go, Chess and Shogi in cited work (survey reports this high task performance qualitatively); exact numeric benchmarks available in original MuZero paper (not reproduced in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low: latent model is learned and not directly interpretable; provides policy/value predictions but internal latent semantics are opaque</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>No explicit interpretability methods noted in survey; model outputs (policy/value) and search tree can be inspected but latent structure is black-box</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Very high: training combined with large MCTS search is computationally expensive (MuZero-style search + network training), survey remarks end-to-end planning is computationally heavy though no precise resource counts given</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Extremely strong task performance; sample efficiency relative to alternatives depends on domain and compute budget  survey notes model-based methods can have lower sample complexity but sometimes higher wall-clock compute</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>State-of-the-art / superhuman on multiple games per the MuZero paper (survey highlights as a major success story for learned abstract models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Learning an abstract model sufficient for planning (policy/value/reward) can yield top-level performance even without reconstructing observations; high latent fidelity in prediction heads leads directly to strong planning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Omitting observation reconstruction reduces model complexity but makes model less interpretable; combining learned model with powerful MCTS yields top performance but at very high compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Learn abstract representation + transition + prediction heads; use external MCTS that leverages network priors/values; do not train decoder to reconstruct observations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms many model-free and earlier model-based approaches on large discrete game benchmarks; trades interpretability and computational efficiency for top-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests abstract latent models used with strong search (MCTS) and well-tuned training produce best results in discrete games; recommends further work on efficiency and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1221.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predictron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Predictron (end-to-end learning and planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end differentiable latent planning architecture that learns a representation, internal multi-step model, and internal value rollouts (the predictron) for value prediction and planning without explicit environment rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The predictron: End-to-end learning and planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Predictron</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Internal abstract model with representation, transition, reward and value prediction modules; performs limited-horizon internal rollouts to accumulate multi-step returns (internal planning) and is trained end-to-end to predict values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent end-to-end planning model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Procedurally generated mazes and value-prediction tasks; general discrete planning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Value-prediction error, downstream policy performance on planning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported to perform well on limited-horizon planning/value prediction tasks in cited work; survey does not provide numeric metrics</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: internal rollouts are explicit and can be inspected as internal value predictions, but latent representations are learned and not directly human-interpretable</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspect internal accumulated returns and rollout steps; no symbolic extraction described</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher than simple model-free value networks due to internal rollouts and additional modules, but lower than full external planning in some settings since rollouts are internal and differentiable</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides improved value-prediction accuracy in some tasks and can be more sample-efficient than naive model-free baselines for value prediction problems (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Effective for value-prediction and some planning tasks (maze navigation) in the cited work; survey presents it as a success in end-to-end planning research</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Internal multi-step rollouts focus on value-relevant dynamics so improved internal fidelity generally improves downstream value estimates; practical gains depend on task horizon and representation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>End-to-end internal planning increases model complexity and compute, and may be sensitive to hyperparameters; however it reduces need for explicit external planning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Learn representation + internal transition + reward/value, limit internal rollout depth for stability, train end-to-end with value losses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to external planners, Predictron trades explicit environment rollouts for differentiable internal rollouts, yielding advantages in tasks where value prediction is primary objective.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests limited-horizon internal rollouts and strong representation learning to focus on value-relevant latent features as effective design principles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1221.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1221.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Foresight</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Foresight (video-prediction + MPC for robotic manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Video-prediction-based latent model trained on vision data that generates future frames conditioned on action sequences; used inside an MPC loop to select action sequences with least predicted cost for robotic manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual foresight: Model-based deep reinforcement learning for vision-based robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Visual Foresight (video-prediction model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Action-conditioned video prediction model (typically convolutional sequence model) that predicts future frames from current image + candidate action sequences; planning uses MPC to choose actions minimizing predicted cost (e.g., object position errors).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>observation-predictive latent/video model (action-conditioned pixel predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Robotic manipulation from raw camera images (multiple-object pushing, picking/placing, cloth-folding)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel-level reconstruction/prediction loss (e.g., MSE), task success rate (manipulation success), planning cost on predicted frames</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey reports success on multi-object manipulation and cloth folding tasks in cited work; quantitative pixel-loss numbers not provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: predicted frames can be visualized directly, enabling intuitive inspection of predicted outcomes, but internal latent dynamics are neural and not symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of predicted image sequences and comparison to real futures; use of least-cost predicted trajectories to justify actions</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High for high-resolution video prediction; MPC over predicted frames requires sampling action sequences and evaluating predictions online (compute-intensive during inference)</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sample-efficient relative to naive model-free learning for vision-based manipulation; compute per decision step is higher due to online prediction and sampling</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Effective for a variety of manipulation tasks including pushing and cloth-folding; survey notes practical success in robotics contexts</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High fidelity in predicting task-relevant object motions (even if pixel-perfect prediction is not achieved) translates into effective MPC action selection; background pixels may be ignored by planner cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Predicting full pixels gives intuitive visual predictions but is computationally heavy and can waste capacity on irrelevant visual details; latent/value-focused models can be more efficient for purely reward-driven tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Action-conditioned video predictor trained with data sampled by a distribution geared toward task cost; combine with MPC and short planning horizons to reduce compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to latent RSSM approaches, visual-foresight directly predicts pixels enabling intuitive planning but at higher compute; latent models may be more compact and faster for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey indicates using MPC with short horizons, focusing training data sampling on task-relevant distributions, and using video prediction models tailored to object motions yields best practical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'High-Accuracy Model-Based Reinforcement Learning, a Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Value prediction network <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 2)</em></li>
                <li>Pilco: A model-based and data-efficient approach to policy search <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>The predictron: End-to-end learning and planning <em>(Rating: 2)</em></li>
                <li>Visual foresight: Model-based deep reinforcement learning for vision-based robotic control <em>(Rating: 2)</em></li>
                <li>Model-based reinforcement learning for Atari <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1221",
    "paper_id": "paper-236087473",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "World Models",
            "name_full": "World models (Ha & Schmidhuber)",
            "brief_description": "Generative recurrent latent models that learn a compressed spatial-temporal representation of an environment (vision model + memory model + controller) trained (largely) unsupervised to support policy learning and planning in the compact latent space.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models",
            "model_description": "A generative recurrent world model: VAE-based visual encoder/decoder (observation model), recurrent memory (RNN/LSTM) as a temporal latent state, and a lightweight controller (policy) that uses latent features. The world model generates imagined trajectories in latent space for policy training.",
            "model_type": "latent world model (generative RNN + VAE)",
            "task_domain": "CarRacing / low-dimensional video games; general video-based control",
            "fidelity_metric": "Reconstruction loss of VAE (ELBO), predictive likelihood in latent/RNN rollout, reward/value prediction when used for planning",
            "fidelity_performance": "Qualitative: reported to produce compact, temporally-coherent latent predictions sufficient to train simple controllers on car-racing tasks; no quantitative MSE/score numbers provided in the survey",
            "interpretability_assessment": "Low-to-moderate: latent dimensions provide compressed features that can sometimes be visualized (decoded frames), but internal recurrent dynamics are largely black-box",
            "interpretability_method": "VAE latent visualization (decoded frames) and inspecting generated rollouts; no symbolic interpretations reported",
            "computational_cost": "Moderate-to-high: requires training VAE + recurrent model; survey notes world models are elaborate and computationally demanding but no exact training-time or parameter counts given",
            "efficiency_comparison": "Sample-efficient vs naive pixel predictive models for some tasks (enables training small controllers); not directly compared numerically to model-free baselines in survey",
            "task_performance": "Enables compact controllers to solve CarRacing and similar tasks; used to train policies that perform competitively for the evaluated domains (qualitative)",
            "task_utility_analysis": "World models prioritize learning compressed spatio-temporal features useful for control; high-fidelity pixel reconstruction is not required as long as latent captures task-relevant dynamics.",
            "tradeoffs_observed": "Compressing observations into latent space improves sample efficiency and planning tractability but increases architectural complexity and reduces direct interpretability; high computational cost for training vs simpler models.",
            "design_choices": "Use VAE for visual encoder/decoder, RNN (LSTM) for temporal dynamics, separate lightweight controller; emphasis on unsupervised learning of latent dynamics and then policy learning using latent rollouts.",
            "comparison_to_alternatives": "Compared favorably to raw-pixel planning for some small video tasks; less interpretable than physics-based simulators; more sample-efficient than naive model-free in domains tested but more complex to train.",
            "optimal_configuration": "Survey recommends compact latent representations and separating vision/memory/controller; suggests further standardization and simplification of latent/world-model architectures to balance fidelity, compute, and utility.",
            "uuid": "e1221.0",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "VPN",
            "name_full": "Value Prediction Network (VPN)",
            "brief_description": "A latent-model that directly predicts future rewards/values in an abstract latent space instead of reconstructing observations, enabling planning in a value-focused compressed representation.",
            "citation_title": "Value prediction network",
            "mention_or_use": "mention",
            "model_name": "Value Prediction Network (VPN)",
            "model_description": "Architecture with four networks: encoder (observations -&gt; latent), latent transition model (next latent given action), reward model, and value model; planning/backups are performed in latent/value space rather than by predicting full next observations.",
            "model_type": "latent world model / value-centric latent model",
            "task_domain": "Atari games and discrete planning domains (Sokoban/mazes) where value prediction suffices",
            "fidelity_metric": "Value-prediction accuracy (e.g., TD/bootstrapped value error), reward-prediction error; planning success rate on downstream tasks",
            "fidelity_performance": "Reported (in survey) to outperform model-free DQN on certain Atari games (Pacman, Seaquest) and to handle stochastic domains better than observation-based planning; no precise numeric errors provided in the survey",
            "interpretability_assessment": "Moderate: latent states are explicitly trained to be value-relevant, making them more interpretable in terms of predicted rewards/values than raw-pixel models, but latent features remain neural encodings",
            "interpretability_method": "Inspection of predicted values/rewards and latent rollouts; no formal disentanglement or symbolic mapping described in survey",
            "computational_cost": "Relatively lower inference cost than full pixel predictors because planning works in low-dimensional latent space; training still requires multiple networks but cheaper than full observation predictors in many cases (survey qualitative)",
            "efficiency_comparison": "More sample-efficient than pixel-based models for Atari in cited work; offers improved planning horizon utility because predictions focus on value-relevant aspects",
            "task_performance": "Improved game scores over DQN on selected Atari titles per cited work; effective in stochastic domains where pixel prediction is brittle (qualitative report)",
            "task_utility_analysis": "By prioritizing value-relevant aspects, VPN often translates improved prediction fidelity in latent-value terms into better policy performance; avoids wasteful modeling of irrelevant visual detail.",
            "tradeoffs_observed": "Sacrifices full observation fidelity for value-relevant fidelity; leads to improved task utility but may reduce generality if tasks require precise observation reconstruction.",
            "design_choices": "Train encoder/transition/reward/value jointly with value-based RL losses; limit planning depth to control roll-out error accumulation.",
            "comparison_to_alternatives": "Outperforms observation-predictive models for several Atari games in cited work; compared favorably to DQN in some domains due to focusing on value rather than pixels.",
            "optimal_configuration": "Survey suggests value-focused latent representations and limited rollout depth as robust choices; encourages using value-based objectives to shape latent representations for planning.",
            "uuid": "e1221.1",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "PlaNet (RSSM)",
            "name_full": "PlaNet (Recurrent State Space Model for latent dynamics planning)",
            "brief_description": "A latent dynamics model (RSSM) combining an encoder, variational observation model, stochastic and deterministic transition components, and a reward model used with MPC (CEM) to plan in latent space.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "mention",
            "model_name": "PlaNet (RSSM)",
            "model_description": "Recurrent State Space Model (RSSM): an encoder/VAE to map pixels to latent, an RNN-based transition combining stochastic and deterministic latents, an observation model, and a reward predictor; planning performed via model-predictive control (CEM) in latent space.",
            "model_type": "latent world model (probabilistic RSSM with VAE encoder)",
            "task_domain": "Continuous control tasks (DeepMind Control Suite / MuJoCo) and visual control from pixels",
            "fidelity_metric": "Latent predictive log-likelihood (ELBO), reward-prediction error, downstream task return/episode return",
            "fidelity_performance": "Reported to reach performance close to strong model-free baselines on continuous control tasks; survey gives qualitative statement (no numeric fidelity metrics included)",
            "interpretability_assessment": "Low: latent representations compress task-relevant dynamics but internal stochastic/deterministic latent behavior remains neural and not directly interpretable",
            "interpretability_method": "Visualizing decoded reconstructions from latent states; no systematic interpretability tools described in survey",
            "computational_cost": "High: training VAE + RSSM and running CEM-based MPC each step is computationally intensive; survey notes computational cost is substantial though exact GPU/duration not given",
            "efficiency_comparison": "High sample efficiency compared to model-free baselines (fewer environment steps needed) but larger wall-clock/training compute per collected sample in some cases (survey cites benchmarking caveats)",
            "task_performance": "Achieves close-to-model-free performance on many DeepMind Control tasks and enables planning from pixels; Dreamer later builds on PlaNet with improved performance",
            "task_utility_analysis": "Latent RSSM provides latent states sufficient for MPC planning; high latent predictive fidelity tends to translate into good control performance when planning horizon is tuned appropriately.",
            "tradeoffs_observed": "Latent compression reduces sample complexity but increases model/training complexity and runtime; probabilistic/stochastic latents provide uncertainty modeling at cost of added complexity.",
            "design_choices": "Use stochastic + deterministic latent components (RSSM), VAE encoder for pixels, reward predictor, and CEM-based MPC with short re-planning horizon to limit accumulation of model error.",
            "comparison_to_alternatives": "More sample-efficient than many model-free methods on specified tasks; compared to ensembles/MPC approaches, PlaNet trades ensemble uncertainty modeling for structured latent dynamics.",
            "optimal_configuration": "Survey recommends short planning horizons with MPC, careful encoder/transition design (RSSM) and tuning of stochastic/deterministic components to balance fidelity and computational cost.",
            "uuid": "e1221.2",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dreamer (Dream to Control / latent imagination-based control)",
            "brief_description": "A latent-imagination actor-critic method that backpropagates value gradients through imagined latent trajectories produced by an RSSM-like model to learn policies end-to-end in latent space.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer",
            "model_description": "Latent world model (RSSM-style): VAE encoder + recurrent latent dynamics; imaginations (short latent rollouts) are used to compute gradients for an actor-critic policy that is trained using imagined returns; planning is implicit via latent imagination and gradient backprop.",
            "model_type": "latent world model with latent imagination (model-based actor-critic)",
            "task_domain": "DeepMind Control Suite (continuous control), Atari (discrete) in follow-up work",
            "fidelity_metric": "Quality of imagined latent rollouts measured by predicted reward/value accuracy, downstream policy return",
            "fidelity_performance": "Reported to solve a diverse collection of continuous problems and, in extensions, to reach human-level performance on many Atari games; survey relays qualitative success without tabulated fidelity numbers",
            "interpretability_assessment": "Low: learned latent dynamics are neural and not readily interpretable, though decoded reconstructions can be visualized",
            "interpretability_method": "Visual inspection of decoded latent rollouts and predicted rewards; no formal interpretability pipeline reported in survey",
            "computational_cost": "High: joint training of RSSM, actor, critic with backprop through imagined trajectories increases compute; survey notes end-to-end/backpropagation planning has large computational effort",
            "efficiency_comparison": "Highly sample-efficient compared to model-free baselines (fewer environment interactions); wall-clock training time may be larger due to model complexity",
            "task_performance": "Competitive or better than strong model-free baselines on several control benchmarks; later Dreamer variants achieved strong Atari results (survey qualitative)",
            "task_utility_analysis": "High-quality latent imagination (fidelity in predicted rewards/values) directly improves policy learning because gradients are backpropagated through latent rollouts.",
            "tradeoffs_observed": "End-to-end latent imagination improves sample efficiency but raises computational cost and sensitivity to hyperparameters; complex architectures can be brittle across tasks.",
            "design_choices": "Use RSSM-like latent with stochastic/deterministic components, short imagined rollouts for stable gradients, actor-critic objective backpropagated through model predictions.",
            "comparison_to_alternatives": "Better sample efficiency than many model-free approaches; compared to PlaNet, Dreamer adds explicit policy/value networks trained via backprop through latent predictions for stronger performance on some tasks.",
            "optimal_configuration": "Survey suggests limiting imagination horizon, careful architecture tuning, and improving computational efficiency of end-to-end planning as priorities.",
            "uuid": "e1221.3",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "PETS",
            "name_full": "Probabilistic Ensembles with Trajectory Sampling (PETS)",
            "brief_description": "An ensemble of probabilistic neural network dynamics models used with sampling-based uncertainty propagation and CEM/MPC planning to achieve sample-efficient continuous control.",
            "citation_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "mention_or_use": "mention",
            "model_name": "PETS (Probabilistic Ensembles with Trajectory Sampling)",
            "model_description": "Learn an ensemble of probabilistic neural dynamics models (each predicts next-state distribution); propagate particles/trajectories sampled from ensemble (TS); use CEM to select action sequences and execute first action in MPC loop.",
            "model_type": "probabilistic ensemble latent/explicit dynamics model",
            "task_domain": "MuJoCo continuous control tasks (pusher, reacher, half-cheetah, hopper, walker)",
            "fidelity_metric": "Predictive uncertainty calibration, next-state prediction error (MSE), downstream task return under MPC",
            "fidelity_performance": "Survey reports that PETS approaches asymptotic model-free baselines on several MuJoCo tasks and is sample-efficient; precise numeric fidelity metrics are not provided in the survey",
            "interpretability_assessment": "Moderate: ensemble variance provides an interpretable uncertainty estimate (aleatoric/epistemic separation to some degree), but base models are neural black boxes",
            "interpretability_method": "Use ensemble disagreement/variance as an uncertainty proxy and to guide planning (short rollouts); no latent visualization described",
            "computational_cost": "High at inference: propagating many particles across ensemble for CEM sampling is compute-intensive; training multiple models increases training compute",
            "efficiency_comparison": "More sample-efficient than model-free (survey cites benchmarking where model-based used ~200k time steps vs ~1M for model-free), but wall-clock time and compute per step may be higher",
            "task_performance": "Matches or approaches model-free asymptotic performance on many MuJoCo tasks while using far fewer environment interactions (qualitative reported results)",
            "task_utility_analysis": "Ensemble predictive uncertainty reduces harmful effects of model error during planning; short MPC horizons mitigate compounding errors leading to strong control performance.",
            "tradeoffs_observed": "Ensembles give robustness/uncertainty at the cost of increased training/inference compute; shorter rollouts reduce accumulated error but may limit long-horizon planning.",
            "design_choices": "Use probabilistic neural nets in ensemble, trajectory sampling (TS) for propagation, CEM/MPC with re-planning each timestep and short horizons to reduce model-bias.",
            "comparison_to_alternatives": "Benchmarked favorably against single deterministic models and many model-free baselines in terms of sample efficiency; compared to latent RSSMs, PETS uses explicit ensembles to quantify uncertainty.",
            "optimal_configuration": "Survey recommends combining ensembles with MPC and using short, re-planned horizons (MPC) to balance fidelity, robustness, and computational cost.",
            "uuid": "e1221.4",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "PILCO",
            "name_full": "PILCO: Probabilistic Inference for Learning COntrol",
            "brief_description": "A Gaussian process based model-based RL system that achieves strong sample efficiency on low-dimensional continuous control by explicitly modeling uncertainty and performing analytic inference for planning.",
            "citation_title": "Pilco: A model-based and data-efficient approach to policy search",
            "mention_or_use": "mention",
            "model_name": "PILCO (GP-based dynamics model)",
            "model_description": "Uses Gaussian processes to model transition dynamics probabilistically, computes analytic moment-matching predictions for multi-step propagation, and uses gradient-based policy optimization under uncertainty.",
            "model_type": "probabilistic explicit (Gaussian process) world model",
            "task_domain": "Low-dimensional continuous control (pendulum, cart-pole, mountain car)",
            "fidelity_metric": "Predictive posterior mean and variance (GP predictive MSE/likelihood), policy return (expected cumulative reward)",
            "fidelity_performance": "Highly sample-efficient on small benchmarks (cartpole, mountain car) in original work; survey notes PILCO scales poorly to high-dimensional problems",
            "interpretability_assessment": "High interpretability for small problems: GP predictive mean/variance are transparent and uncertainty is explicit",
            "interpretability_method": "Analytic predictive distributions from GP; uncertainty quantified directly by GP variance",
            "computational_cost": "Moderate-to-high scaling: GP training/inference scales poorly with dataset size (cubic in samples), limiting applicability to low-dimensional/small-data settings",
            "efficiency_comparison": "Extremely sample-efficient relative to many methods on small problems, but not tractable for high-dimensional pixel-based tasks",
            "task_performance": "Excellent on classic small control tasks (pendulum, cart-pole); not applied to complex/high-dimensional benchmarks in survey",
            "task_utility_analysis": "High fidelity + explicit uncertainty leads to strong control performance in small domains; inability to scale limits broader utility.",
            "tradeoffs_observed": "GP fidelity and interpretability come at computational scaling costs; cannot match deep latent model approaches for high-dimensional domains.",
            "design_choices": "Use Gaussian processes for dynamics and analytic moment-matching for multi-step prediction; favor exact uncertainty over model-capacity scaling.",
            "comparison_to_alternatives": "More sample-efficient than deep models on small tasks, but deep latent models and ensembles scale to larger domains where PILCO fails.",
            "optimal_configuration": "Survey implies PILCO is optimal for small, low-dimensional problems where explicit uncertainty and analytic inference are viable, but recommends other approaches for high-dimensional settings.",
            "uuid": "e1221.5",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero (learning a model for planning without modelling observations)",
            "brief_description": "An end-to-end model-based system that learns an abstract (latent) model of environment dynamics and reward/policy/value predictions, and uses an explicit MCTS planner to plan in the learned latent space without predicting raw observations.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero (latent abstract model + MCTS)",
            "model_description": "Abstract representation network maps observations to latent; learned latent 'dynamics' (transition) that outputs next latent and reward/policy/value predictions; uses conventional MCTS externally to plan using network-provided priors and value estimates (no pixel reconstruction).",
            "model_type": "latent world model (abstract dynamics + prediction heads)",
            "task_domain": "Board games (Go, Chess, Shogi) and Atari (discrete high-dimensional games)",
            "fidelity_metric": "Policy/value prediction accuracy, MCTS-assisted game-play Elo/score, planning success rate",
            "fidelity_performance": "Achieved state-of-the-art and superhuman performance across Atari, Go, Chess and Shogi in cited work (survey reports this high task performance qualitatively); exact numeric benchmarks available in original MuZero paper (not reproduced in survey)",
            "interpretability_assessment": "Low: latent model is learned and not directly interpretable; provides policy/value predictions but internal latent semantics are opaque",
            "interpretability_method": "No explicit interpretability methods noted in survey; model outputs (policy/value) and search tree can be inspected but latent structure is black-box",
            "computational_cost": "Very high: training combined with large MCTS search is computationally expensive (MuZero-style search + network training), survey remarks end-to-end planning is computationally heavy though no precise resource counts given",
            "efficiency_comparison": "Extremely strong task performance; sample efficiency relative to alternatives depends on domain and compute budget  survey notes model-based methods can have lower sample complexity but sometimes higher wall-clock compute",
            "task_performance": "State-of-the-art / superhuman on multiple games per the MuZero paper (survey highlights as a major success story for learned abstract models)",
            "task_utility_analysis": "Learning an abstract model sufficient for planning (policy/value/reward) can yield top-level performance even without reconstructing observations; high latent fidelity in prediction heads leads directly to strong planning outcomes.",
            "tradeoffs_observed": "Omitting observation reconstruction reduces model complexity but makes model less interpretable; combining learned model with powerful MCTS yields top performance but at very high compute cost.",
            "design_choices": "Learn abstract representation + transition + prediction heads; use external MCTS that leverages network priors/values; do not train decoder to reconstruct observations.",
            "comparison_to_alternatives": "Outperforms many model-free and earlier model-based approaches on large discrete game benchmarks; trades interpretability and computational efficiency for top-level performance.",
            "optimal_configuration": "Survey suggests abstract latent models used with strong search (MCTS) and well-tuned training produce best results in discrete games; recommends further work on efficiency and generalization.",
            "uuid": "e1221.6",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Predictron",
            "name_full": "The Predictron (end-to-end learning and planning)",
            "brief_description": "An end-to-end differentiable latent planning architecture that learns a representation, internal multi-step model, and internal value rollouts (the predictron) for value prediction and planning without explicit environment rollouts.",
            "citation_title": "The predictron: End-to-end learning and planning",
            "mention_or_use": "mention",
            "model_name": "Predictron",
            "model_description": "Internal abstract model with representation, transition, reward and value prediction modules; performs limited-horizon internal rollouts to accumulate multi-step returns (internal planning) and is trained end-to-end to predict values.",
            "model_type": "latent end-to-end planning model",
            "task_domain": "Procedurally generated mazes and value-prediction tasks; general discrete planning benchmarks",
            "fidelity_metric": "Value-prediction error, downstream policy performance on planning tasks",
            "fidelity_performance": "Reported to perform well on limited-horizon planning/value prediction tasks in cited work; survey does not provide numeric metrics",
            "interpretability_assessment": "Moderate: internal rollouts are explicit and can be inspected as internal value predictions, but latent representations are learned and not directly human-interpretable",
            "interpretability_method": "Inspect internal accumulated returns and rollout steps; no symbolic extraction described",
            "computational_cost": "Higher than simple model-free value networks due to internal rollouts and additional modules, but lower than full external planning in some settings since rollouts are internal and differentiable",
            "efficiency_comparison": "Provides improved value-prediction accuracy in some tasks and can be more sample-efficient than naive model-free baselines for value prediction problems (qualitative)",
            "task_performance": "Effective for value-prediction and some planning tasks (maze navigation) in the cited work; survey presents it as a success in end-to-end planning research",
            "task_utility_analysis": "Internal multi-step rollouts focus on value-relevant dynamics so improved internal fidelity generally improves downstream value estimates; practical gains depend on task horizon and representation quality.",
            "tradeoffs_observed": "End-to-end internal planning increases model complexity and compute, and may be sensitive to hyperparameters; however it reduces need for explicit external planning pipelines.",
            "design_choices": "Learn representation + internal transition + reward/value, limit internal rollout depth for stability, train end-to-end with value losses.",
            "comparison_to_alternatives": "Compared to external planners, Predictron trades explicit environment rollouts for differentiable internal rollouts, yielding advantages in tasks where value prediction is primary objective.",
            "optimal_configuration": "Survey suggests limited-horizon internal rollouts and strong representation learning to focus on value-relevant latent features as effective design principles.",
            "uuid": "e1221.7",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Visual Foresight",
            "name_full": "Visual Foresight (video-prediction + MPC for robotic manipulation)",
            "brief_description": "Video-prediction-based latent model trained on vision data that generates future frames conditioned on action sequences; used inside an MPC loop to select action sequences with least predicted cost for robotic manipulation tasks.",
            "citation_title": "Visual foresight: Model-based deep reinforcement learning for vision-based robotic control",
            "mention_or_use": "mention",
            "model_name": "Visual Foresight (video-prediction model)",
            "model_description": "Action-conditioned video prediction model (typically convolutional sequence model) that predicts future frames from current image + candidate action sequences; planning uses MPC to choose actions minimizing predicted cost (e.g., object position errors).",
            "model_type": "observation-predictive latent/video model (action-conditioned pixel predictor)",
            "task_domain": "Robotic manipulation from raw camera images (multiple-object pushing, picking/placing, cloth-folding)",
            "fidelity_metric": "Pixel-level reconstruction/prediction loss (e.g., MSE), task success rate (manipulation success), planning cost on predicted frames",
            "fidelity_performance": "Survey reports success on multi-object manipulation and cloth folding tasks in cited work; quantitative pixel-loss numbers not provided in survey",
            "interpretability_assessment": "Moderate: predicted frames can be visualized directly, enabling intuitive inspection of predicted outcomes, but internal latent dynamics are neural and not symbolic",
            "interpretability_method": "Visual inspection of predicted image sequences and comparison to real futures; use of least-cost predicted trajectories to justify actions",
            "computational_cost": "High for high-resolution video prediction; MPC over predicted frames requires sampling action sequences and evaluating predictions online (compute-intensive during inference)",
            "efficiency_comparison": "Sample-efficient relative to naive model-free learning for vision-based manipulation; compute per decision step is higher due to online prediction and sampling",
            "task_performance": "Effective for a variety of manipulation tasks including pushing and cloth-folding; survey notes practical success in robotics contexts",
            "task_utility_analysis": "High fidelity in predicting task-relevant object motions (even if pixel-perfect prediction is not achieved) translates into effective MPC action selection; background pixels may be ignored by planner cost.",
            "tradeoffs_observed": "Predicting full pixels gives intuitive visual predictions but is computationally heavy and can waste capacity on irrelevant visual details; latent/value-focused models can be more efficient for purely reward-driven tasks.",
            "design_choices": "Action-conditioned video predictor trained with data sampled by a distribution geared toward task cost; combine with MPC and short planning horizons to reduce compounding errors.",
            "comparison_to_alternatives": "Compared to latent RSSM approaches, visual-foresight directly predicts pixels enabling intuitive planning but at higher compute; latent models may be more compact and faster for planning.",
            "optimal_configuration": "Survey indicates using MPC with short horizons, focusing training data sampling on task-relevant distributions, and using video prediction models tailored to object motions yields best practical performance.",
            "uuid": "e1221.8",
            "source_info": {
                "paper_title": "High-Accuracy Model-Based Reinforcement Learning, a Survey",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Value prediction network",
            "rating": 2,
            "sanitized_title": "value_prediction_network"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_in_a_handful_of_trials_using_probabilistic_dynamics_models"
        },
        {
            "paper_title": "Pilco: A model-based and data-efficient approach to policy search",
            "rating": 2,
            "sanitized_title": "pilco_a_modelbased_and_dataefficient_approach_to_policy_search"
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "The predictron: End-to-end learning and planning",
            "rating": 2,
            "sanitized_title": "the_predictron_endtoend_learning_and_planning"
        },
        {
            "paper_title": "Visual foresight: Model-based deep reinforcement learning for vision-based robotic control",
            "rating": 2,
            "sanitized_title": "visual_foresight_modelbased_deep_reinforcement_learning_for_visionbased_robotic_control"
        },
        {
            "paper_title": "Model-based reinforcement learning for Atari",
            "rating": 1,
            "sanitized_title": "modelbased_reinforcement_learning_for_atari"
        }
    ],
    "cost": 0.022538499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>High-Accuracy Model-Based Reinforcement Learning, a Survey
July 20, 2021</p>
<p>Aske Plaat 
Walter Kosters 
Mike Preuss 
High-Accuracy Model-Based Reinforcement Learning, a Survey
July 20, 2021Noname manuscript No. (will be inserted by the editor)Model-based reinforcement learning  Latent models  Deep learning  Machine learning  Planning
Deep reinforcement learning has shown remarkable success in the past few years. Highly complex sequential decision making problems from game playing and robotics have been solved with deep model-free methods. Unfortunately, the sample complexity of modelfree methods is often high. To reduce the number of environment samples, model-based reinforcement learning creates an explicit model of the environment dynamics.Achieving high model accuracy is a challenge in high-dimensional problems. In recent years, a diverse landscape of model-based methods has been introduced to improve model accuracy, using methods such as uncertainty modeling, model-predictive control, latent models, and end-to-end learning and planning. Some of these methods succeed in achieving high accuracy at low sample complexity, most do so either in a robotics or in a games context. In this paper, we survey these methods; we explain in detail how they work and what their strengths and weaknesses are. We conclude with a research agenda for future work to make the methods more robust and more widely applicable to other applications.Keywords Model-based reinforcement learning  Latent models  Deep learning  Machine learning  PlanningIntroductionRecent breakthroughs in game playing and robotics have shown the power of deep reinforcement learning, for example, by learning to play Atari and Go from scratch or by learning to fly an acrobatic model helicopter(Mnih et al., 2015;Abbeel et al., 2007). Unfortunately, for most applications the sample efficiency is lowLeCun et al., 2015), and achieving faster learning is a major topic in current research. Model-based methods can achieve faster learning by making an internal dynamics model of the environment. By then using this dynamics model for policy updates, the number of necessary environment samples can be reduced substantially(Sutton, 1991).</p>
<p>The success of the model-based approach hinges on the accuracy of this dynamics model-there is a trade-off between accuracy and sample complexity, especially in models with many parameters that require many samples to prevent overfitting (LeCun et al., 2015;Talvitie, 2015). The challenge for the methods in this survey is to train a high-accuracy dynamics model for high-dimensional problems with low sample complexity.</p>
<p>Reinforcement learning finds solutions for sequential decision problems (see Figure 2). Where model-free methods learn the best action in each state, model-based methods go a step further: the next-state transition function in each state is learned. Model-based methods capture the core of complex decision sequences, and models may also be applicable to related environments (Risi and Preuss, 2020;Torrado et al., 2018), for transfer learning.</p>
<p>The contribution of this survey is to give an in-depth overview of recent high-accuracy model-based methods for high-dimensional problems. We present a taxonomy based on application type, learning method, and planning method, and while improving model accuracy is difficult, successful methods are reported for game playing and visuo-motor control (although rarely both at the same time). We describe how and why the methods work-we do note, however, that the computational cost is still high, and that the outcomes of experiments are often sensitive to the choice of hyperparameters. We close with a research agenda to improve reproducibility, to further improve accuracy, and to make methods more widely applicable.</p>
<p>The field of deep model-based reinforcement learning is quite active. Excellent works with background information exist for reinforcement learning (Sutton and Barto, 2018) and deep learning (Goodfellow et al., 2016). Previous surveys provide an overview of the uses of classic (tabular) model-based methods (Deisenroth et al., 2013;Kober et al., 2013;Kaelbling et al., 1996). The purpose of the current survey is to focus on deep learning methods.</p>
<p>Other relevant surveys into model-based reinforcement learning are (Justesen et al., 2019;Polydoros and Nalpantidis, 2017;Hui, 2018;Wang et al., 2019;alr and Pehlivanolu, 2019;Moerland et al., 2020b).</p>
<p>Section 2 provides necessary background and the MDP formalism for reinforcement learning. Section 3 then surveys the field. Section 4 provides a discussion reflecting on the different approaches and provides open problems and research directions for future work. Section 5 concludes the survey.  Barto, 2018). Maximizing the reward for state is performed by following the transition function to find the next state . Note that the policy ( , ) tells the first half of this transition, going from  ; the transition function ( , ) completes the transition, going from  (via ).</p>
<p>Background</p>
<p>The goal of reinforcement learning is to learn optimal behavior for a certain environment, maximizing expected cumulative future reward. This goal is reached after a sequence of decisions is taken, actions that can be different for each state; the best sequence of actions solves a sequential decision making problem. Reinforcement learning draws inspiration from human and animal learning (Hamrick, 2019;Kahneman, 2011;Anthony et al., 2017;Duan et al., 2016), where behavioral adaptation by reward and punishment is studied.</p>
<p>In contrast, supervised learning methods can be used when a dataset of labeled training pairs ( , ) is present. The reinforcement learning paradigm, however, does not assume the presence of such a dataset, but derives the ground truths from an external environment, see Figure 1. The environment provides a new state and its reward for every action that the agent tries in a certain state (Sutton and Barto, 2018). In this way, as many (state-action, reward) pairs can be generated as needed.</p>
<p>Formalizing Model-Based Reinforcement Learning</p>
<p>Reinforcement learning problems are often modeled as a Markov decision process (Bishop, 2006;Sutton and Barto, 2018), MDP. First we introduce state, action, transition and reward. Then we introduce trajectory, policy and value. Finally, we discuss model-based and modelfree solution approaches.</p>
<p>A Markov decision process is a 4-tuple ( , , , ) where is a finite set of states, is a finite set of actions;</p>
<p> is the set of actions available from state . Furthermore, is the transition function: ( , ) is the probability that action in state at time will lead to state at time + 1. Finally, ( , ) is the immediate reward received after transitioning from state to state due to action .</p>
<p>A policy is a stochastic or deterministic function mapping states to actions. The goal of the agent is to learn a policy that maximizes the value function, the expected cumulative discounted sum of rewards ( ) = E =0 in a trajectory , with a discount parameter in an episode with steps. The optimal policy  contains the solution to a sequential decision problem: a prescription of which action must be taken in each state.</p>
<p>In model-free reinforcement learning only the environment knows the transition model () that computes the next state distribution; the policy is learned directly from environment feedback (Figure 3). In contrast, in model-based reinforcement learning, the A dataset is static. In reinforcement learning the choice of actions may depend on the rewards that are returned during the learning process, giving rise to a dynamic, potentially unstable, learning process.</p>
<p>Environment</p>
<p>Policy/Value learning acting Fig. 3 Model-Free Learning agent constructs its own version of the transition model, and the policy can be learned from the environment feedback and with the help of the local transition model. Figure 2 shows a visual diagram of states, actions, and transitions. The function ( ) is called the state-value function. Some algorithms compute the policy directly, and some first compute the function ( ). For continuous or stochastic problems often direct policy-based methods work best, for discrete or deterministic problems the value-based methods are most often used (Kaelbling et al., 1996). A third approach combines the best of value and policy methods: actor-critic (Sutton and Barto, 2018;Konda and Tsitsiklis, 2000;Mnih et al., 2016). In the remainder of the paper we will see that the distinction between continuous/discrete action spaces and policy-based/value-based algorithms plays an important role (see also Table 2).</p>
<p>Closely related to the value function is the state-action-value function ( , ). This -function gives the expected sum of discounted rewards for action in state , and then afterwards following policy . The optimal policy can be found by recursively choosing the argmax action with ( , ) =  ( ) in each state. This relationship is given by
* = max ( ) = max , ( , ).
There are many algorithms to find optimal policies (Hessel et al., 2017). Algorithms that use the agent's transition function directly to find the next state are called planning algorithms, algorithms that use the environment to find the next state are called learning algorithms. We now briefly discuss classical model-free learning and planning approaches, before we continue to survey model-based algorithms in more depth in the next section.</p>
<p>In deep learning, functions such as the policy function are approximated by the parameters (or weights) of a deep neural network, and are written as , to distinguish them from classical tabular policies.</p>
<p>Model-Free Learning</p>
<p>When the agent does not have access to the transition or reward model, then the policy function has to be learned by querying the environment to find the reward for the action in a certain state. Learning the policy or value function in this way is called model-free learning, see Figure 3.</p>
<p>Recall that the policy is a mapping of states to (best) actions. Each time when a new reward is returned by the environment the policy can be improved: the best action for the state is updated to reflect the new information. Algorithm 1 shows high-level steps of model-free reinforcement learning (later on the algorithms become more elaborate).</p>
<p>Model-free reinforcement learning is the most basic form of reinforcement learning (Kaelbling et al., 1996;Deisenroth et al., 2013;Kober et al., 2013). It has been successfully applied to a range of challenging problems (Mnih et al., 2015;Abbeel et al., 2007). In model-free</p>
<p>Algorithm 1 Model-Free Learning repeat</p>
<p>Sample env to generate data = ( , , , ) Use to update policy ( , ) until converges reinforcement learning a policy is learned from the ground up through interactions with the environment.</p>
<p>The goal of classic model-free learning is to find the optimal policy for the environment; the goal of deep model-free learning is to find a policy function that generalizes well to states from the environment that have not been seen during training. A secondary goal is to do so with good sample efficiency: to use as few environment samples as possible.</p>
<p>Model-free learning follows the current behavior policy in selecting the action to try, deciding between exploration of new actions and exploitation of known good actions with a selection rule such as -greedy. Exploration is essentially blind, and learning the policy and value often takes many samples, millions in current experiments (Mnih et al., 2015;Wang et al., 2019).</p>
<p>A well-known model-free reinforcement learning algorithm is Q-learning (Watkins, 1989). Algorithms such as Q-learning were developed in a classical tabular setting. Deep neural networks have been used with success in model-free learning, in domains in which samples can be generated cheaply and quickly, such as in Atari video games (Mnih et al., 2015). Deep model-free algorithms such as Deep Q-Network (DQN) (Mnih et al., 2013) and Proximal Policy Optimization, PPO (Schulman et al., 2017) have become quite popular. PPO is an algorithm that computes the policy directly, DQN finds the value function first (Section 2.1).</p>
<p>Model-free methods select actions in a straightforward manner, without using a separately learned local transition model. An advantage of the straightforward action selection is that they can find global optima without suffering from selection bias from model imperfections. Model-based methods may not always be able to find as good policies as model-free can.</p>
<p>A disadvantage is that interaction with the environment can be costly. Especially when the environment involves the real world, such as in real-world robot-interaction, then sampling should be minimized, for reasons of cost, and to prevent wear of the robot arm. In virtual environments on the other hand, model-free approaches have been quite successful (Mnih et al., 2015). An overview of model-free reinforcement learning can be found in (Sutton and Barto, 2018;alr and Pehlivanolu, 2019;Kaelbling et al., 1996).</p>
<p>Planning</p>
<p>When an agent has an internal transition model, then planning algorithms can use it to find the optimal policy, by selecting actions in states, looking ahead, and backing up reward values, see Figure 2 and Figure 4. Planning algorithms require access to an explicit model. In the deterministic case the transition model provides the next state for each of the possible actions in the states, it is a function = ( ). In the stochastic case, it provides the probability distribution ( , ). The reward model provides the immediate reward for transitioning from state to state after taking action , backing up the value from the child state to the parent state (see the backup diagram in Figure 2). The policy function ( , ) concerns the top layer of the diagram, from to . The transition function ( , ) covers both layers, from to . The transition function defines a space of states in which the planning algorithm can search for the optimal policy  and value  .</p>
<p>A basic planning approach is Bellman's dynamic programming (Bellman, 1957(Bellman, , 2013, a recursive traversal method of the state and action space. Value iteration is a straightforward dynamic programming algorithm. The pseudo-code for value iteration is shown in Algorithm 2 (Alpaydin, 2020). It traverses all actions in all states, computing the value of the entire state space, until the value function converges.</p>
<p>When the agent has an accurate model, planning algorithms can be used to find the best policy. This approach is sample efficient since a policy is found without further interaction with the environment. A sampling action performed in an environment is irreversible, since state changes of the environment can not be undone by the agent in the reinforcement learning paradigm. In contrast, a planning action taken in the agent's local transition model is reversible (Moerland et al., 2020a(Moerland et al., , 2018. A planning agent can backtrack, a sampling agent cannot. The ability to backtrack is especially useful to try alternatives to further improve local solutions-local solutions can be found easily by sampling; for finding global optima efficiently the ability to backtrack out of a local optimum improves efficiency.</p>
<p>Model-Based Learning</p>
<p>It is now time to look at model-based reinforcement learning, where the policy and value function are learned by both sampling and planning. Recall that the environment samples return ( , ) pairs when the agent selects action in state . This means that we can learn the transition model ( , ) and the reward model ( , ), for example by supervised learning, since all information is present. When the transition and reward model are present in the agent, they can then be used with planning to update the policy and value functions as often as we like using the local functions without any further sampling of the environment (although we might want to continue sampling to further improve our models). This alternative approach Why would we want to go this convoluted learning-and-planning route, which may even introduce model-bias, if the environment samples can teach us the optimal policy and value directly? The reason is that the convoluted route may be more sample efficient. In model-free learning a sample is used once to optimize the policy, and then thrown away, in model-based learning the sample is used to learn a transition model, which can then be used many times in planning to optimize the policy. The sample is used more efficiently by the agent.</p>
<p>A well-known classic model-based approach is imagination, which was introduced by Sutton (1990Sutton ( , 1991 in the Dyna system, long before deep learning was used widely. Dyna uses the samples to update the policy function directly (model-free learning) and also uses the samples to learn a transition model, to augment the model-free environment-samples with the model-based imagined "samples." Imagination is a hybrid algorithm that uses both model-based planning and model-free learning to improve the behavior policy. Figure 6 illustrates the working of the Dyna approach. Algorithm 4 shows the steps of the algorithm (compared to Algorithm 3, the line in italics is new, from Algorithm 1). Note how the policy is updated twice in each iteration, by environment sampling, and by transition planning. More details are shown in Algorithm 5 (Sutton, 1990).</p>
<p>After these introductory words, we are now ready to take a deeper look into recent concrete deep model-based reinforcement learning methods.</p>
<p>Algorithm 4 Dyna's Model-Based Imagination repeat</p>
<p>Sample env to generate data = ( , , , ) Use to update policy ( , ) Use to learn ( , ) and ( , ) Use , to update policy ( , ) by planning until converges Algorithm 5 Dyna-Q: Classic learning and planning with a Q-function-based dynamics model (Sutton, 1990) 
Initialize ( , )  R randomly Initialize ( , )  R  randomly  Model repeat Select  randomly  ( )  ( ) can be -greedy( ) based on ( , )  ( , )
 Learn new state and reward from environment 
( , )  ( , ) +  [ +  max ( , )  ( , ) ] ( , )  ( , ) for = 1, . . . , do Selectandrandomly ( , )  (,) </p>
<p>Survey of Model-Based Deep Reinforcement Learning</p>
<p>The success of model-based reinforcement learning in high-dimensional problems depends on the accuracy of the dynamics model. The model is typically used by planning algorithms for multiple sequential predictions, and errors in predictions accumulate quickly with each step. Model-based reinforcement learning is an active field, and many papers have been published that document progress towards improving model-accuracy. The methods that are proposed in the papers are quite diverse.</p>
<p>We will now present our taxonomy. The taxonomy distinghuishes three aspects: (1) application type, (2) learning method, and (3) planning method. Table 1 summarizes the taxonomy, which is the basis of the remainder of this survey. We will now describe the methods, explaining how they fit together by going through the learning, planning, and application that they use.</p>
<p>First, we will describe the way in which the model is learned, and how the accuracy of the model is improved. Among the approaches are uncertainty modeling such as Gaussian processes and ensembles, and convolutional neural networks or latent models (sometimes integrated in end-to-end learning and planning).</p>
<p>Name</p>
<p>Learning Planning Application PILCO (Deisenroth and Rasmussen, 2011) Uncertainty Trajectory Pendulum iLQG  Uncertainty MPC Small GPS (Levine and Abbeel, 2014) Uncertainty Trajectory Small SVG (Heess et al., 2015) Uncertainty Trajectory Small Local Model (Gu et al., 2016) Uncertainty Trajectory MuJoCo Visual Foresight  Video Prediction MPC Manipulation PETS (Chua et al., 2018) Ensemble MPC MuJoCo MVE (Feinberg et al., 2018) Ensemble Trajectory MuJoCo Meta Policy (Clavera et al., 2018) Ensemble Trajectory MuJoCo Policy Optim (Janner et al., 2019) Ensemble Trajectory MuJoCo PlaNet (Hafner et al., 2018) Latent MPC MuJoCo Dreamer (Hafner et al., 2019) Latent Trajectory MuJoCo Plan2Explore (Sekar et al., 2020) Latent Trajectory MuJoCo Video-prediction (Oh et al., 2015) Latent Trajectory Atari VPN (Oh et al., 2017) Latent  Second, we will describe the way in which the model is subsequently used by the planner to improve the behavior policy ( Figure 6). These methods aim to reduce the impact of planning with inaccurate models. Among the methods are planning with (short) trajectories, model-predictive control, and end-to-end learning and planning.</p>
<p>Finally, we will describe applications on which model-based methods have been successful. The effectiveness of model-based methods depends on whether they fit the application domain in which they are used, and on further aspects of the application. There are two main types of applications, those with continuous action spaces, and those with discrete action spaces. For continuous action spaces, simulated physics robotics in MuJoCo is a favorite test bed . For discrete action spaces many researchers use mazes or blocks puzzles. For large, high dimensional, problems the Arcade Learning Environment is used, where the input consists of the screen pixels, and the output actions are the joystick movements (Bellemare et al., 2013). We will use this taxonomy to categorize and understand the recent literature on high-accuracy model-based reinforcement learning. We list some of the papers in Table 2, which provides an overview of many of the methods that we discuss in this survey. We will explain the main issues and challenges in the field step by step, using the taxonomy as guideline, illustrating solutions to these issues and challenges with approaches from the papers from the table.  Table 2.</p>
<p>Let us now start with the taxonomy. We begin with learning, next is planning, and finally applications.</p>
<p>Learning</p>
<p>The transition model is what gives model-based reinforcement learning its name. The accuracy of the model is of great importance, planning with inaccurate models will not improve the policy much, planning with a biased model will even harm the policy, and performance of model-based methods will be worse than the model-free baseline (Gu et al., 2016). Getting high-accuracy models with few samples is challenging when the model has many parameters, since, in order to prevent overfitting, we would need many environment observations (high sample complexity).</p>
<p>In this section we will describe techniques that have been developed to improve model accuracy. We will discuss: 1. uncertainty modeling, 2. ensemble methods, 3. latent models.</p>
<p>For environments with continuous action spaces and non-determinism, such as robotics, uncertainty modeling and ensembles have shown progress. Latent models were developed in both continuous and discrete action spaces.</p>
<p>Uncertainty Modeling</p>
<p>One of the shortcomings of conventional reinforcement learning methods is that they only focus on expected value, ignoring the variance of values. This is problematic when few samples are taken for each trajectory . Uncertainty modeling methods have been developed to counter this problem. Gausian processes can learn simple processes with good sample efficiency, although for high-dimensional problems they need many samples. They have been used for probabilistic inference to learn control (Deisenroth and Rasmussen, 2011) in the PILCO system. This system was effective on Cartpole and Mountain car ( Figure 11), but does not scale to larger problems.</p>
<p>A related method uses nonlinear least-squares optimization . Here the model learner uses quadratic approximation on the reward function, which is then used with linear approximation of the transition function. With further enhancements this method was able to teach a humanoid robot how to stand up (see Figure 9).</p>
<p>We can also sample from a trajectory distribution optimized for cost, and to use that to train the policy, with a policy-based method (Levine and Koltun, 2013). Then we can optimize policies with the aid of locally-linear models and a stochastic trajectory optimizer. This approach, called Guided policy search (GPS), has been shown to train complex policies with thousands of parameters learning tasks in MuJoCo such as swimming, hopping and walking. Alternatively, we can compute value gradients along the real environment trajectories, instead of planned ones, and re-parameterize the trajectory through sampling, to mitigate learned model inaccuracy (Heess et al., 2015). This was done by Stochastic value gradients (SVG) with global neural network value function approximators.</p>
<p>Learning arm and hand manipulation directly from video camera input is a challenging problem in robotics. The camera image provides a high dimensional input and increases problem size and complexity of the subsequent manipulation task substantially. Both Finn and Levine (2017); Ebert et al. (2018) introduce a method called Visual foresight. This system uses a training procedure where data is sampled according to a probability distribution. Concurrently, a video prediction model is trained. This model generates a sequence of future frames based on an image and a sequence of actions, as in GPS. At test time, the leastcost sequence of actions is selected in a model-predictive control planning framework (see Section 3.2.2). This approach is able to perform multi-object manipulation, pushing, picking and placing, and cloth-folding tasks (which adds the difficulty of material that changes shape as it is being manipulated).</p>
<p>Ensembles</p>
<p>Ensemble methods, such as a random forest of decision trees (Ho, 1995), are widely used in machine learning (Bishop, 2006), and they are also used in controlling uncertainty in high dimensional modeling. Ensemble methods mitigate variance and improve performance by running algorithms multiple times. They are used with success in model-based deep reinforcement learning as well. Chua et al. (2018) combine uncertainty-aware modeling with sampling-based uncertainty propagation, creating a method called Probabilistic ensembles with trajectory sampling, PETS. (This approach is described in the next section, see Algorithm 6). An ensemble of probabilistic neural network models is used by Nagabandi et al. (2018). Ensembles perform well; performance on pusher, reacher, and half-cheetah (see Figure 9) is reported to approach asymptotic model-free baselines such as PPO (Schulman et al., 2017). Ensembles of probabilistic networks (Chua et al., 2018) are also used with short rollouts, where the model horizon is shorter than the task horizon (Janner et al., 2019). Results have been reported for hopper, walker, and half-cheetah, again matching the performamce of model-free approaches.</p>
<p>The ensemble approach is related to meta learning, where we try to speed up learning a new task by learning from previous, related, tasks (Brazdil et al., 2008;Hospedales et al., 2020;Huisman et al., 2021). MAML is a popular meta learning approach ,  (Oh et al., 2017) that attempts to learn a network initialization such that for any task the policy attains maximum performance after one policy gradient step. The MAML approach can be used to improve model accuracy by learning an ensemble of dynamics models and by then metaoptimizing the policy for adaptation in each of the learned models (Clavera et al., 2018). Results indicate that such meta-learning of a policy over an ensemble of learned models indeed approaches the level of performance of model-free methods with substantially better sample complexity.</p>
<p>Latent Models</p>
<p>The next group of methods that we describe are the latent models. Central to all our approaches is the need for improvement of model accuracy in complex, high-dimensional, problems. The main challenge to achieve high accuracy is to overcome the size of the highdimensional state space. The idea behind latent models is that in most high-dimensional environments there are elements that are less important, such as background trees that never move, that have little or no relation with the reward of the agent's actions. The goal of latent models is to abstract away these unimportant elements of the input space, reducing the effective dimensionality of the space. They do so by learning the relation between the elements of the input and the reward. When we focus our learning mechanism on the changes in observations that are correlated with changes in these values, then we can improve the efficiency of learning high-dimensional problems greatly. Latent models thus learn a smaller representation, smaller than the observation space. Planning takes place in this smaller representation space.</p>
<p>The value prediction network (VPN) was introduced by Oh et al. (2015Oh et al. ( , 2017 to achieve this goal. They ask the question in their paper: "What if we could predict future rewards and values directly without predicting future observations?" and describe a nework architecture and learning method for such focused value prediction models. The core idea is not to learn directly in actual observation space, but first to transform the actual state respresentation to a smaller latent representation model, also known as abstract model. The other functions, such as value, reward, and next-state, then work with the smaller latent representations, instead of the actual high-dimensional states. By training all functions based on the values (Grimm et al., 2020), planning and learning occur in a space where states are encouraged only to contain the elements that influence value changes. In VPN the latent model consists of four networks: an encoding function, a reward function, a value function, and a transition function. All functions are parameterized with their own set of parameters (Figure 8). Latent space is lower-dimensional, and training and planning become more efficient. The figure shows a single step rollout, planning one step ahead, as in Dyna-Q (Algorithm 4).</p>
<p>The training of the networks can in principle be performed with any value-based reinforcement learning algorithm. Oh et al. (2017) report results with -step Q-learning and temporal difference search (Silver et al., 2012).</p>
<p>VPN (Oh et al., 2017) showed impressive results on Atari games such as Pacman and Seaquest, outperforming model-free DQN (Mnih et al., 2015), and outperforming observation-based planning in stochastic domains. Subsequently, many other works have been published that further improved results (Kaiser et al., 2019;Hafner et al., 2018Hafner et al., , 2019Sekar et al., 2020;Silver et al., 2017b;Ha and Schmidhuber, 2018b). Many of these latent-model approaches are complicated designs, with multiple neural networks, and different learning and planning algorithms.</p>
<p>The latent-model approach is related to world models, a term used by Ha and Schmidhuber (2018a,b). World models are inspired by the manner in which humans are thought to contruct a mental model of the world in which we live. World models are often generative recurrent neural networks that are trained unsupervised using a variational autoencoder Welling, 2013, 2019;Goodfellow et al., 2014) and a recurrent network. They learn a compressed spatial and temporal representation of the environment. In world models multiple neural networks are used, for a vision model, a memory model, and a controller (Ha and Schmidhuber, 2018b). By using features extracted from the world model as inputs to the agent, a compact and simple policy can be trained to solve a task, and planning occurs in the compressed or simplified world. World models have been applied by Ha and Schmidhuber (2018a,b) on a car racing game (Kempka et al., 2016). The term world model actually goes back to 1990, where it was used by Schmidhuber (1990b). Latent models are also related to dimensionality reduction (Van Der Maaten et al., 2009).</p>
<p>The architecture of latent models, or world models, is elaborate. The dynamics model typically includes an observation model, a representation model, a transition model, and a value or reward model (Karl et al., 2016;Buesing et al., 2018;Doerr et al., 2018). The task of the observation model is to reduce the high-dimensional world into a lower-dimensional world, to allow more efficient planning. Often a variational autoencoder or LSTM is used.</p>
<p>The Arcade learning environment is one of the main benchmarks in reinforcement learning. The high-dimensionality of Atari video input has long been problematic for model-based reinforcement learning. Latent models were instrumental in reducing the dimensionality of Atari, producing the first successes for model-based approaches on this major benchmark.</p>
<p>Related to the VPN approach (Oh et al., 2015(Oh et al., , 2017 is other work that uses latent models on Atari, such as Kaiser et al. (2019), that is aimed at video prediction, outperforming modelfree baselines (Hessel et al., 2017), reaching comparable accuracy with up to an order of magnitude better sample efficiency. The approach by Kaiser et al. (2019) uses a variational autoencoder (VAE) to process input frames, conditioned on the actions of the agent, to learn the world model, using PPO (Schulman et al., 2017). The policy is then improved by planning inside the reduced world model, with short rollouts. This behavior policy then determines the actions to be used for learning from the environment.</p>
<p>Latent models are also used on continuous MuJoCo problems. Here the work by Hafner et al. (2018Hafner et al. ( , 2019 on the PlaNet and Dreamer systems is noteworthy including the application of their work back to Atari , which achieved human-level performance. PlaNet uses a Recurrent state space model (RSSM) that consists of a transition model, an observation model, a variational encoder and a reward model (Karl et al., 2016;Buesing et al., 2018;Doerr et al., 2018). Based on these models a Model-predictive control agent is used to adapt its plan, replanning each step (Richards, 2005). The RSSM is used by a Cross entropy method search (Botev et al. (2013), CEM) for the best action sequence. In contrast to model-free approaches, no explicit policy or value function network is used; the policy is implemented as MPC planning with the best sequence of future actions. PlaNet is tested on continuous tasks and reaches performance that is close to strong model-free algorithms. A further system, called Dreamer (Hafner et al., 2019), builds on PlaNet. Using an actor critic approach (Mnih et al., 2016) and backpropagating value gradients through predicted sequences of compact model states, the improved system solves a diverse collection of continuous problems from the Deepmind control suite (Tassa et al., 2018), see Figure 9. Dreamer is also applied to discrete problems from the Arcade learning environment, and to few-shot learning (Sekar et al., 2020). A further improvement achieved human-level performance on 55 Atari games, a first for a model-based approach , showing that the latent model approach is well-suited for high-dimensional problems.</p>
<p>Planning</p>
<p>After the transition model has been learned, it will be used with a planning algorithm so that the behavior policy can be improved. Since the transition model will contain some inaccuracy, the challenge is to find a planning algorithm that performs well despite the inaccuracies. We describe three groups of methods that have been developed for planning algorithms to cope with inaccurate models. These are:</p>
<ol>
<li>trajectory rollouts, 2. model-predictive control, 3. end-to-end learning and planning. Trajectory rollouts and model-predictive control have been shown to work for both continuous and discrete action spaces; end-to-end learning and planning has been developed in the context of discrete action spaces (mazes and games).</li>
</ol>
<p>Of the three planning methods, we will start with these trajectory rollouts.</p>
<p>Trajectory Rollouts</p>
<p>As we saw in Section 2.1, methods for continuous action spaces typically sample full trajectory rollouts to get stable actions. At each planning step, the transition model ( )  computes the new state, using the reward to update the policy. Due to the inaccuracies of the internal model, planning algorithms that perform many steps will quickly accumulate model errors (Gu et al., 2016). Full rollouts of long and inaccurate trajectories are therefore problematic. We can reduce the impact of accumulated model errors by not planning too far ahead. For example, Gu et al. (2016) perform experiments with locally linear models that roll out planning trajectories of length 5 to 10. This reportedly works well for MuJoCo tasks gripper and reacher.</p>
<p>In their work on model-based value expansion (MVE), Feinberg et al. (2018) also allow imagination to fixed depth, value estimates are split into a near-future model-based component and a distant future model-free component. They experiment with model horizons of 1, 2, and 10. They find that 10 generally performs best on typical MuJoCo tasks such as swimmer, walker, and cheetah. The sample complexity in their experiments is better than model-free methods such as DDPG (Silver et al., 2014). Similarly good results are reported by Janner et al. (2019); Kalweit and Boedecker (2017), both approaches use a model horizon that is much shorter than the task horizon.</p>
<p>Algorithm 6 PETS MPC (Chua et al., 2018) Initialize data with a random controller for one trial for Trial = 1 to do Train a PE dynamics model given for Time = 0 to TaskHorizon do for Actions sampled  : +  CEM( ), 1 to NSamples do Propagate state particles  using TS and | { ,  : + } Evaluate actions as + = 1 =1 (  ,  ) Update CEM( ) distribution end for Execute first action  * (only) from optimal actions  * : + Record outcome:   {  ,  * ,  +1 }. end for end for</p>
<p>Model-Predictive Control</p>
<p>Taking the idea of shorter trajectories for planning than for learning further, we arrive at Model-predictive control (MPC) (Kwon et al., 1983;Garcia et al., 1989). Model-predictive control is a well-known approach in process engineering, to control complex processes with frequent re-planning of a limited time horizon. Model-predictive control uses the fact that while many real-world processes are not linear, they are approximately linear over a small operating range. Applications are found in the automotive industry and in aerospace, for example for terrain-following and obstacle-avoidance algorithms (Kamyar and Taheri, 2014). In optimal control, four MPC approaches are identified: linear model MPC, nonlinear prediction model, explicit control law MPC, and robust MPC to deal with disturbances (Garcia et al., 1989). In this survey, we focus on how the principle of continuous replannning with a rolling planning horizon performs in nonlinear model-based reinforcement learning.</p>
<p>It is instructive to compare the MPC and linear quadratic regulators (LQR) approach, since both methods come from the field of optimal control. MPC computes the target function with a small time window that rolls forward as new information comes in; it is dynamic. LQR computes the target function in a single episode, using all available information; it is static. We observe that in model-based reinforcement learning MPC is used in the planning part with the behavior policy being the target and the transition function () the input; for LQR the transition function () is the target, and the environment samples ( , ) are the input. Thus, one could conceivably use both MPC and LQR, the first as planning and the second as learning algorithm, in a model-based approach.</p>
<p>An iterative form of LQG has indeed been used together with MPC on a smaller Mu-JoCo problem , achieving good results. MPC used step-by-step real-time local optimization;  used many further improvements to the trajectory optimization, physics engine, and cost function to achieve good performance.</p>
<p>MPC has also been used in other model learning approaches. Both Finn and Levine (2017); Ebert et al. (2018) use a form of MPC in the planning for their Visual foresight robotic manipulation system (that we have seen in a previous section). The MPC part uses a model that generates the corresponding sequence of future frames based on an image to select the least-cost sequence of actions.</p>
<p>Another approach uses ensemble models for learning the transition model, while using MPC for planning. PETS (Chua et al., 2018) uses probabilistic ensembles (Lakshminarayanan et al., 2017) for learning. In MPC fashion only the first action from the CEM-optimized sequence is used, re-planning at every time-step (see Algorithm 6).</p>
<p>MPC is a simple and effective planning method that is well-suited for model inaccuracy, by restricting the planning horizon. MPC is used with success in model-based reinforcement learning, with high-variance or complex transition models. MPC has also been used with success in combination with latent models (Hafner et al., 2018;Kaiser et al., 2019).</p>
<p>End-to-End Learning and Planning</p>
<p>Our third planning approach is different, it integrates planning with learning in a fully differentiable algorithm. Let us see how this works.</p>
<p>Model-based reinforcement learning consists of two distinct functions: learning the transition model and planning with the model to improve the behavior policy ( Figure 5). In classical, tabular, reinforcement learning, both learning and planning functions are designed by hand (Sutton and Barto, 2018). In deep reinforcement learning, one of these functions is approximated by deep learning-the model learning-while the planner is still handwritten. End-to-end learning and planning breaks this classical planning barrier. End-toend approaches integrate the planning into deep learning, using differentiable planning algorithms, extending the backpropagation fully from reward to observation in all parts of the model-based approach.</p>
<p>How can a neural network learn to plan? While conceptually exciting and appealing, there are challenges to overcome. Among them are finding suitable differentiable planning algorithms and the increase in computational training complexity, since now the planner must also be learned.</p>
<p>The idea of planning by gradient descent exists for some time, several authors explored learning approximations of state transition dynamics in neural networks (Kelley, 1960;Schmidhuber, 1990a;Ilin et al., 2007). Neural networks are typically used to transform and filter, to learn selection and classification tasks. A planner unrolls a state, computes values, using selection and value aggregation, and backtracks to try another state. Although counterintuitive at first, these operations are not that different from what classic neural networks are performing. A progression of papers has published methods on how this can be achieved.</p>
<p>We will start at the beginning, with convolutional neural networks (CNN) and value iteration. We will see how the iterations of value iteration can be implemented in the layers of a convolutional neural network (CNN). Next, two variations of this method are presented, and a way to implement planning with convolutional LSTM modules. All these approaches implement differentiable, trainable, planning algorithms, that can generalize to different inputs. The later methods use elaborate schemes with latent models so that the learning can be applied to different application domains.</p>
<p>Let us start to see what is possible with a CNN. A CNN can be used to implement value iteration. This was first shown by Tamar et al. (2016), who introduced value iteration networks (VIN). The core idea is that value iteration (VI, see Algorithm 2) can be implemented stepby-step by a multi-layer convolutional network: each layer does a step of lookahead. In this way VI is implemented in a CNN. The VI iterations for the Q-action-value-function are rolled out in the network layers with channels. Through backpropagation the model learns the value function. The aim is to learn a general model, that can navigate in unseen environments.</p>
<p>VIN can be used for discrete and continuous path planning, and has been tried in grid world problems and natural language tasks. VIN has achieved generalization of finding shortest paths in unseen mazes. However, a limitation of VIN is that the number of layers of the CNN restricts the number of planning steps, restricting VINs to small and lowdimensional domains. Follow-up studies focus on making end-to-end learning and planning more generally applicable. Schleich et al. (2019) extend VINs by adding abstraction, and Srinivas et al. (2018) introduce universal planning networks, UPN, which generalize to modified robot morphologies. Value propagation (Nardelli et al., 2018) uses a hierarchical structure to generalize end-to-end methods to large problems. TreeQN (Farquhar et al., 2018) incorporates a recursive tree structure in the network, modeling the different functions of an MDP explicitly. TreeQN is applied to Sokoban and nine Atari games.</p>
<p>A further step is to model more complex planning algorithms, such as Monte Carlo Tree Search (MCTS), a successful planning algorithm (Coulom, 2006;Browne et al., 2012). This has been achieved to a certain extent by  who implement many elements of MCTS in MCTSnets and (Guez et al., 2019). In this method planning is learned with a general recurrent architecture consisting of LSTMs and a convolutional network (Schmidhuber, 1990b) in the form of a stack of ConvLSTM modules (Xingjian et al., 2015). The architecture was used on Sokoban and boxworld (Zambaldi et al., 2018), and was able to perform full planning steps. Future work should investigate how to achieve sample-efficiency with this architecture.</p>
<p>The question whether model-based planning can be learned by a neural network has been studied by , who showed that imagination-based planning steps can indeed be learned for a small game with an LSTM. Related to this, imagination-augmented agents (I2A) has been designed as a fully end-to-end differentiable architecture for modelbased imagination and model-free reinforcement learning . It consists of an LSTM-based encoder (Chiappa et al., 2017;Buesing et al., 2018), a ConvLSTM rollout module, and a standard CNN-based model-free path. The policy improvement algorithm is A3C.  report that on Sokoban and Pacman I2A performs better than modelfree learning and MCTS. I2A has been specifically designed to handle model imperfections well and uses a manager or meta-controller to choose between rolling out actions in the environment or by imagination (Hamrick et al., 2017).</p>
<p>In VIN there is a tight connection between the network architecture and the application structure. One way to remedy this restriction is with a latent model, such as the ones that were discussed earlier. One of the first attempts is the Predictron (Silver et al., 2017b), where the familiar four elements appear: a representation model, a transition model, a reward model, and a value model. The goal of the latent model is to perform value prediction (not state prediction), including being able to encode special events such as "staying alive" or "reaching the next room." Predictron performs limited-horizon rollouts, and has been applied to procedurally generated mazes. One of the main success stories of model-based reinforcement learning is AlphaZero (Silver et al., 2017a. AlphaZero combines planning and learning in a highly successful way. Inspired by this approach, a fully differentiable version of this architecture has been introduced by Schrittwieser et al. (2020) which is named MuZero. This system is able to learn the rules and to learn to play games as different as Atari, chess, and Go, purely from the environment, with end-to-end learning and planning. The MuZero architecture is based on Predictron, with an abstract model consisting of a representation, transition, reward, and prediction function (policy and value). For planning, MuZero uses an explicitly coded version of MCTS that uses policy and value input form the network (Rosin, 2011), but that is executed separately from the network.</p>
<p>End-to-end planning and learning has shown impressive results, but there are still open questions concerning the applicability to different applications, and especially the scalability to larger problems. </p>
<p>Applications</p>
<p>After we have discussed in some depth the learning and planning methods, we must look at the third element of the taxonomy: the applications. We will see that the type of application plays an important role in the success of the learning and planning methods.</p>
<p>Model-based reinforcement learning is applied to sequential decision problems. Which types of sequential decision problems can we distinguish? Two main application areas are robotics and games. The actions in robotics are continuous, and the environment is nondeterministic. The actions in games are typically discrete and the environment is often deterministic. We will describe four application areas:</p>
<ol>
<li>continuous action space (a) small tasks (b) large tasks 2. discrete action space (a) low-dimensional input (b) high-dimensional input</li>
</ol>
<p>Continuous Actions</p>
<p>Sequential decision problems are well-suited to model robotic actions, such as how to move the joints in a robotic arm to pour a cup of tea, how to move the joints of a humanoid figure to stand up when lying down, and how to develop gaits of a four-legged animal. The action space of such problems is continuous since the angles over which robotic joints move span a continuous range of values. Furthermore, the environment in which robots operate mimics the real world, and is non-deterministic. Things move, objects do not always respond in a predictable fashion, and unexpected situations arise. In reinforcement learning, where agent algorithms are trained by the feedback on their many actions, working with real robots would get prohibitively expensive due to wear. Most reinforcement learning systems use physics simulations such as offered by MuJoCo . MuJoCo allows the creation of experiments that provide environments for an agent. Tasks can range form small to large. </p>
<p>Small</p>
<p>MuJoCo tasks differ in difficulty, depending on how many joints or degrees of freedom are modeled, and which task is being learned. Figure 9 shows some of the tasks that have been modeled in MuJoCo as part of the DeepMind Control Suite (Tassa et al., 2018). Some of the small tasks are ball-in-cup and reacher. The iterative quadratic non-linear optimization method iLQG  is able to teach a humanoid to stand up, and Guided policy search (Levine and Koltun, 2013) and Stochastic value gradients (Heess et al., 2015) can learn tasks such as swimmer, reacher, half-cheetah ( Figure 10) and walker. Also ensemble methods such as PETS, MVE, and meta ensembles achieve good results on these applications (Gu et al., 2016;Chua et al., 2018;Feinberg et al., 2018;Clavera et al., 2018;Janner et al., 2019).</p>
<p>Large</p>
<p>MuJoCo has enabled progress in model-based reinforcement learning in small continuous tasks, and also in larger tasks, such as how to develop gaits of a four-legged robotic animal, or how to scale an obstacle course. PlaNet (Hafner et al., 2018), Dreamer (Hafner et al., 2019) and MBPO (Janner et al., 2019) achieve good results on more complicated MuJoCo tasks using latent models to reduce the dimensionality.</p>
<p>Discrete Actions</p>
<p>There is a long tradition in reinforcement learning to see if we can teach a computer to play complicated games and puzzles (Plaat, 2020). Games and puzzles are often played on a board with discrete squares. Actions in such games are discrete, a move to square e3 is not a move to square e4. The environments are also deterministic, we assume that pieces do not move by itself.</p>
<p>Most games that are used in deep model-based reinforcement learning papers fall into this category. More complex games, such as partial information (card games such as poker (Brown and Sandholm, 2019)) or games with multiple actors (real-time strategy video games such as StarCraft (Vinyals et al., 2019;Ontann et al., 2013;Wong et al., 2021)) are not used in the approaches that we survey here.  (Dietterich, 1998;Kansal and Martin, 2018) </p>
<p>Low-Dimensional</p>
<p>Among low-dimensional applications that are used in model-based reinforcement learning are simple pendulum problems, Cartpole and Mountain car, where the challenge is to reverse engineer the laws of impulse and gravity ( Figure 11). The action space consists of two discrete actions, push left or push right, the environment is continuous and deterministic. PILCO (Deisenroth and Rasmussen, 2011) achieves good results with Gaussian process modeling and gradient based planning on the pendulum task.</p>
<p>Perhaps the most frequently used low-dimensional application area is grid-world, where various navigation tasks are tested ( Figure 12). VIN (Tamar et al., 2016), VProp (Nardelli et al., 2018) and the Predictron (Silver et al., 2017b) that use maze navigation to test their approaches to integrating end-to-end learning and planning. Grid worlds and mazes can be designed and scaled in different forms and sizes, making them well suited for testing new ideas.</p>
<p>Other low-dimensional games are board games such as chess and Go. These games are low-dimensional (their input has few atttributes, in contrast to a mega-pixel image) but they nevertheless have a large state space. Finding good policies for chess and Go was one of the most challenging feats in reinforcement learning (Campbell et al., 2002;Plaat, 2020). Block puzzles such as Sokoban (Figure 13), are also often used to test reinforcement learning methods. Sokoban is a block-pushing puzzle that derives much of its complexity from the facft that the agent can push a box, but cannot pull (undo) a mistake, giving rise to many dead-ends that are hard to detect. It has been used by I2A  and MCTS network planning (Guez et al., 2019) approaches that implement planning by unrolling steps within a neural network.</p>
<p>High-dimensional</p>
<p>Most recent success in model-free reinforcement learning has been achieved in highdimensional problems, such as the Arcade Learning Environment (Bellemare et al., 2013), see for example (Mnih et al., 2015;Hessel et al., 2017). Atari games were popular video games in the 1980s in game arcades. Figure 14 shows a screenshot of Q*bert, a typical Atari arcade game.</p>
<p>Deep learning methods are well suited to process high-dimensional inputs. The challenge for model-based methods is to learn accurate models with a low number of observations. Latent model approaches have been tried for Atari with some success (Oh et al., 2015(Oh et al., , 2017Kaiser et al., 2019;. The MuZero approach was even able to learn the rules of very different games: Atari, Go, shogi (Japanese chess) and chess (Schrittwieser et al., 2020).</p>
<p>Discussion and Outlook</p>
<p>We have now discussed in depth our taxonomy of application, learning method, and planning method. We have seen different innovative approaches, all aiming to achieve similar goals. Let us discuss how well they succeed.</p>
<p>Model-based reinforcement learning promises high accuracy and low sample complexity. Sutton's work on imagination, where a transition model is created with environment samples that are then used to create extra "imagined" samples for the policy for free, clearly suggests this aspect of model-based reinforcement learning. The transition model acts as a multiplier on the amount of information that is used from each environment sample, as the agent builds up its own model of the environment.</p>
<p>Another, and perhaps more important aspect, is generalization performance. Modelbased reinforcement learning builds a dynamics model of the environment. This model can be used multiple times, not only for the same problem instance, but also for new problem instances, and for variations. By learning the state-to-state transition model and the reward model, model-based reinforcement learning captures the essence of a domain, where modelfree methods only learn best response actions. Model-based reinforcement learning may thus be better suited for solving transfer learning problems, and for solving long sequential decision making problems. It is the difference between learning how to respond to certain actions of a difficult boss, and knowing the boss.</p>
<p>The Challenge</p>
<p>Classical tabular approaches and Gaussian process approaches have been quite succesful in achieving low sample complexity for problems of small complexity (Sutton and Barto, 2018;Deisenroth et al., 2013;Kober et al., 2013). However, the topic of the current survey is deep models, for large, high dimensional, problems with complex, non-linear, and discontinuous functions. These application domains pose a problem for classical approaches.</p>
<p>The main challenge that the model-based reinforcement learning algorithms in this survey address is the following. For high-dimensional tasks the curse of dimensionality causes data to be sparse and variance to be high. Deep methods tend to overfit on small sample sizes, and model-free methods use many millions of environment samples. For high-dimensional problems, the accuracy of transition models is under pressure. Model-based methods that use poor models make poor planning predictions far into the future (Talvitie, 2015).</p>
<p>The challenge is therefore to learn deep, high-dimensional transition functions from limited data that are accurate-or that account for model uncertainty-and plan over these models to achieve policy and value functions that are as accurate or better than model-free methods.</p>
<p>Taxonomy</p>
<p>In discussing our survey, we first summarize the taxonomy. Model-based methods use two main algorithms: (1) an algorithm that is used to learn the model, and (2) an algorithm that is used to plan the policy based on the model ( Figure 5). Our taxonomy is based on application characteristics and algorithm characteristics-  Table 3 Taxonomy: Application, Learning, Planning models, uncertainty modeling and ensembles for continuous action spaces, and for discrete action spaces we look at latent and end-to-end learning and planning. For the planning algorithms we look at trajectory rollouts and model-predictive control for continuous action spaces, and for discrete action spaces we look at end-to-end learning and planning and trajectory rollouts.</p>
<p>Outcome</p>
<p>We can now consider the question if finding methods to achieve high-accuracy models with low sample complexity has been solved. This is the central research question that many researchers have worked on. Unfortunately, authors have used different tasks within benchmark suites-or even different benchmarks-making comparisons between different publications challenging. A study by Wang et al. (2019) reimplemented many methods for continuous problems to perform a fair comparison. They found that ensemble methods and model-predictive control indeed achieve good results on MuJoCo tasks, and do so in significantly fewer time steps than model-free methods. Typcially model-based used 200k time steps versus 1 million for model-free. However, they also found that although the sample complexity is less, the wall-clock time may be different, with model-free methods such as PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018a,b) being much faster for some problems. The accuracy for the policy varies greatly for different problems, as does the sensitivity to different hyperparameter values; i.e., results are brittle.</p>
<p>Taking these caveats into consideration, in general we conclude that the papers that we survey report that, for high-dimensional problems, model-based methods do indeed approach an accuracy as high as model-free baselines, with substantially fewer environment samples. Therefore we conclude that the methods that we survey overcome the difficulties posed by overfitting. Indeed, we have seen that new classes of applications have become possible, both in continuous action spaces-learning to perform complex robotic behaviors-and in discrete action spaces-learning the rules of Atari, chess, shogi, and Go.</p>
<p>Further Challenges</p>
<p>Although in some important applications high accuracy at lower sample complexity has been achieved, and although quite a few results are impressive, challenges remain. To start, the algorithms that have been developed are quite complex. Latent models, end-to-end learning and planning, and uncertainty modeling, are complex algorithms, that require effort to understand, and even more to implement correctly in new applications. In addition, the learned transition models are used in single problems only. Few results are reported where they are used in a transfer learning or meta learning setting (Brazdil et al., 2008;Hospedales et al., 2020;Huisman et al., 2021), with the exception of Sekar et al. (2020).</p>
<p>Furthermore, reproducibility is a challenge due to the use of different benchmarks. Also, high sensitivity to differences in hyperparameter values leads to brittleness in results, making reprodicibility difficult. Finally, we note that continuous problems that are solved appear to be of lower dimensionality than some discrete problems.</p>
<p>Research Agenda</p>
<p>The good results are promising for future work. Based on the results that have been achieved, and the challenges that remain, we come to the following research agenda.</p>
<p>We note that different approaches were developed for different applications. Reproducibility of results is a challenge, different hyperparameters can have a large influence on performance. Furthermore, there are many different benchmarks in use in the field, and the complexity of some continuous benchmarks is less than for discrete. Reproducibility and benchmarking are the first item on our research agenda.</p>
<p>We also note that end-to-end learning and planning is a complex algorithm, that does not work on all applications, and that requires large computational effort. Applying end-to-end to large problems is still a challenge. As second item on our research agenda is to develop end-to-end learning and planning further, to become more efficient, and to be applicable to more and different applications.</p>
<p>We further note that latent models and world models are also complex. Different approaches use difference types of modules, sometimes consisting of submodules. As third item on our research agenda is to integrate latent models with end-to-end learning and planning, as fourth item, we would like to simplify latent models and standardize them, if possible for different applications, and, as fifth item, we wish to apply latent models to higher-dimensional continuous problems.</p>
<p>Finally, we would like to enter on our research agenda meta and transfer learning experiments for model-based reinforcement learning, as sixth item.</p>
<p>In summary, to improve the accuracy and applicability of model-based methods we suggest to work on the following:</p>
<ol>
<li>Improve reproducibility of model-based reinforcement learning, standardize benchmarking, and improve robustness (hyperparameters) 2. Improve efficiency and of integrated end-to-end learning and planning; improve applicability to more and larger applications 3. Integrate latent models and end-to-end learning and planning 4. Simplify the latent model architecture across different applications 5. Apply latent models to more higher-dimensional continuous problems 6. Use model-based reinforcement learning transition models for meta and transfer learning</li>
</ol>
<p>Conclusion</p>
<p>Deep learning has revolutionized reinforcement learning. The new methods allow us to approach more complicated problems than before. Control and decision making tasks involving high dimensional visual input have come within reach.</p>
<p>Model-based methods offer the advantage of lower sample complexity than model-free methods, because agents learn their own transition model of the environment. However, traditional methods, such as Gaussian processes, that work well on moderately complex problems with few samples, do not perform well on high-dimensional problems. Highcapacity models may have high sample complexity to create high-accuracy models, and finding methods that generalize well with low sample complexity has been difficult.</p>
<p>In the last five years many new methods have been devised, and great success has been achieved in model-based deep reinforcement learning. This survey summarizes the main ideas of recent papers in a taxonomy based on applications and algorithms. Latent models condense complex problems into compact latent representations that are easier to learn, improving the accuracy of the model; limited horizon planning reduces the impact of lowaccuracy models; end-to-end methods have been devised to integrate learning and planning in one fully differentiable approach. The Arcade Learning Environment has been one of the main benchmarks in model-free reinforcement learning, starting off the recent interest in the field with the work by Mnih et al. (2013Mnih et al. ( , 2015. The high-dimensionality of Atari video input has long been problematic for model-based reinforcement learning. Latent models were instrumental in reducing the dimensionality of Atari, producing the first successes for model-based approaches on this major benchmark. Despite this success, challenges remain. In the discussion we mentioned open problems for each of the approaches, where we expect worthwhile future work to occur. Impressive results have been reported; future work can be expected in transfer learning with latent models, and the interplay of latent models, in combination with end-to-end learning of larger problems. Benchmarks in the field have also had to keep up. Benchmarks have progressed from single-agent grid worlds to highdimensional games and complicated camera-arm manipulation tasks. Reproducibility and benchmarking studies are of great importance for real progress. In real-time strategy games model-based methods are being combined with multi-agent, hierarchical and evolutionary approaches, allowing the study of collaboration, competation and negotiation.</p>
<p>Model-based deep reinforcement learning is a vibrant field of AI with a long history before deep learning. The field is blessed with a high degree of activity, an open culture, clear benchmarks, shared code-bases (Bellemare et al., 2013;Brockman et al., 2016;Tassa et al., 2018) and a quick turnaround of ideas. We hope that this survey will contribute to the low barrier of entry.</p>
<p>Fig. 1
1Reinforcement learning: agent performs action on environment, which provides new state and reward</p>
<p>Fig. 2
2Backup Diagram (Sutton and</p>
<p>Fig. 6
6Dyna's Model-Based Imagination Algorithm 3 Model-Based Reinforcement Learning repeat Sample env to generate data = ( , , , ) Use to learn ( , ) and ( , ) Use , to update policy ( , ) by planning until converges of finding the policy and the value is called model-based learning, see Algorithm 3 and Figure 5.</p>
<p>Figure 7 Fig. 7
77illustrates how the approaches of the papers influence each other. Note that, as is often the case in reinforcement learning, the influence has two origins: policy-based methods for continuous action spaces (robotics, upper part), and value-Influence of Model-Based Deep Reinforcement Learning Approaches; Top: Continuous/Policy-based (MuJoCo), Bottom: Discrete/Value-based (Mazes, Atari); Red: Uncertainty, Blue: Ensemble, Green: Latent Models, Dashed: end-to-end, Bold: Large Problems discrete action spaces (games, lower part). The colors in the figure refer to approaches that are also listed in</p>
<p>Fig. 8
8Architecture of latent model</p>
<p>Fig. 9
9DeepMind Control Suite. Top: Acrobot, Ball-in-cup, Cart-pole, Cheetah, Finger, Fish, Hopper. Bottom: Humanoid, Manipulator, Pendulum, Point-mass, Reacher, Swimmer (6 and 15 links), Walker</p>
<p>Fig. 10
10Half-Cheetah</p>
<p>Fig. 11
11Cart Pole and Mountain Car Fig. 12 Taxi in a Grid world</p>
<p>Fig. 13
13Sokoban Puzzle (Chao, 2013) Fig. 14 Q*bert, Example Game from the Arcade Learning Environment</p>
<p>Table 1 Taxonomy: Application, Learning, PlanningPlan imagined state and reward from model 
(,)  (,) +  [ +  max 
( , )  (,) ] 
end for 
until converges </p>
<p>Application Learning 
Planning 
Discrete 
CNN/LSTM 
End-to-end learning and planning 
Latent models 
Trajectory rollouts 
Continuous 
Latent Models 
Trajectory rollouts 
Uncertainty modeling Model-predictive control 
Ensemble models </p>
<p>Table 2
2Overview of High-Accuracy Model-Based Reinforcement Learning Methods; Top: Continuous/Policy-based, Bottom: Discrete/Value-based</p>
<p>Table 3 :
3application, learning, planning. First of all, we distinguish continuous versus discrete action spaces (robotics versus games). Second, we look at the algorithms. For the model-learning algorithms we look at latentApplication Learning 
Planning 
Discrete 
CNN/LSTM 
End-to-end learning and planning 
Latent models 
Trajectory rollouts 
Continuous 
Latent models 
Trajectory rollouts 
Uncertainty modeling Model-predictive control 
Ensemble models </p>
<p>Aske Plaat et al.
AcknowledgmentsWe thank the members of the Leiden Reinforcement Learning Group, and especially Thomas Moerland and Mike Huisman, for many discussions and insights.
An application of reinforcement learning to aerobatic helicopter flight. Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Y Ng, Advances in Neural Information Processing Systems. Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of re- inforcement learning to aerobatic helicopter flight. In Advances in Neural Information Processing Systems, pages 1-8, 2007.</p>
<p>Introduction to machine learning. Ethem Alpaydin, MIT PressThird editionEthem Alpaydin. Introduction to machine learning, Third edition. MIT Press, 2020.</p>
<p>Thinking fast and slow with deep learning and tree search. Thomas Anthony, Zheng Tian, David Barber, Advances in Neural Information Processing Systems. Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Advances in Neural Information Processing Systems, pages 5360-5370, 2017.</p>
<p>The Arcade Learning Environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.</p>
<p>Dynamic programming. Courier Corporation. Richard Bellman, Richard Bellman. Dynamic programming. Courier Corporation, 1957, 2013.</p>
<p>Pattern recognition and machine learning. Information science and statistics. M Christopher, Bishop, Springer VerlagHeidelbergChristopher M Bishop. Pattern recognition and machine learning. Information science and statistics. Springer Verlag, Heidelberg, 2006.</p>
<p>The crossentropy method for optimization. I Zdravko, Dirk P Botev, Reuven Y Kroese, Pierre L&apos; Rubinstein, Ecuyer, Handbook of statistics. Elsevier31Zdravko I Botev, Dirk P Kroese, Reuven Y Rubinstein, and Pierre L'Ecuyer. The cross- entropy method for optimization. In Handbook of statistics, volume 31, pages 35-59. Elsevier, 2013.</p>
<p>Metalearning: Applications to data mining. Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, Ricardo Vilalta, Springer Science &amp; Business MediaPavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning: Applications to data mining. Springer Science &amp; Business Media, 2008.</p>
<p>Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.01540Science. 3656456OpenAI Gym. arXiv preprintGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365 (6456):885-890, 2019.</p>
<p>A survey of Monte Carlo Tree Search methods. B Cameron, Edward Browne, Daniel Powley, Whitehouse, M Simon, Lucas, I Peter, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in Games. 41Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of Monte Carlo Tree Search methods. IEEE Transactions on Computa- tional Intelligence and AI in Games, 4(1):1-43, 2012.</p>
<p>Demis Hassabis, et al. Learning and querying fast generative models for reinforcement learning. Lars Buesing, Theophane Weber, Sbastien Racaniere, Danilo Sm Eslami, Rezende, P David, Fabio Reichert, Frederic Viola, Karol Besse, Gregor, arXiv:1802.03006arXiv preprintLars Buesing, Theophane Weber, Sbastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, et al. Learn- ing and querying fast generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.</p>
<p>Model-free reinforcement learning algorithms: A survey. Sinan alr, Meltem Kurt Pehlivanolu, 27th Signal Processing and Communications Applications Conference (SIU). Sinan alr and Meltem Kurt Pehlivanolu. Model-free reinforcement learning algorithms: A survey. In 2019 27th Signal Processing and Communications Applications Conference (SIU), pages 1-4, 2019.</p>
<p>. Murray Campbell, Feng-Hsiung Joseph HoaneJr, Hsu, Deep Blue. Artificial Intelligence. 1341-2Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. Deep Blue. Artificial Intelli- gence, 134(1-2):57-83, 2002.</p>
<p>. Yang Chao, Sokoban.org. Yang Chao. Sokoban.org, 2013.</p>
<p>. Silvia Chiappa, Sbastien Racaniere, Daan Wierstra, Shakir Mohamed, arXiv:1704.02254Recurrent environment simulators. arXiv preprintSilvia Chiappa, Sbastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent envi- ronment simulators. arXiv preprint arXiv:1704.02254, 2017.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in Neural Information Processing Systems. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforce- ment learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages 4754-4765, 2018.</p>
<p>Model-based reinforcement learning via meta-policy optimization. Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, Pieter Abbeel, arXiv:1809.05214arXiv preprintIgnasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. arXiv preprint arXiv:1809.05214, 2018.</p>
<p>Efficient selectivity and backup operators in Monte-Carlo Tree Search. Rmi Coulom, International Conference on Computers and Games. SpringerRmi Coulom. Efficient selectivity and backup operators in Monte-Carlo Tree Search. In International Conference on Computers and Games, pages 72-83. Springer, 2006.</p>
<p>Pilco: A model-based and data-efficient approach to policy search. Marc Deisenroth, Carl E Rasmussen, Proceedings of the 28th International Conference on Machine Learning (ICML-11). the 28th International Conference on Machine Learning (ICML-11)Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 465-472, 2011.</p>
<p>A survey on policy search for robotics. Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, rFoundations and Trends in Robotics 2. Now publishersMarc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. In rFoundations and Trends in Robotics 2, pages 1-142. Now publishers, 2013.</p>
<p>The maxq method for hierarchical reinforcement learning. G Thomas, Dietterich, ICML. Citeseer98Thomas G Dietterich. The maxq method for hierarchical reinforcement learning. In ICML, volume 98, pages 118-126. Citeseer, 1998.</p>
<p>Probabilistic recurrent state-space models. Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Toussaint, Sebastian Trimpe, arXiv:1801.10395arXiv preprintAndreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Toussaint, and Sebastian Trimpe. Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018.</p>
<p>RL 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, L Peter, Ilya Bartlett, Pieter Sutskever, Abbeel, arXiv:1611.02779arXiv preprintYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL 2 : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.</p>
<p>Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, Sergey Levine, arXiv:1812.00568arXiv preprintFrederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018.</p>
<p>Gregory Farquhar, Tim Rocktschel, Maximilian Igl, S A Whiteson, TreeQN and ATreeC: Differentiable tree planning for deep reinforcement learning. International Conference on Learning Representations. Gregory Farquhar, Tim Rocktschel, Maximilian Igl, and SA Whiteson. TreeQN and ATreeC: Differentiable tree planning for deep reinforcement learning. International Conference on Learning Representations, 2018.</p>
<p>Model-based value estimation for efficient model-free reinforcement learning. Vladimir Feinberg, Alvin Wan, Ion Stoica, I Michael, Joseph E Jordan, Sergey Gonzalez, Levine, arXiv:1803.00101arXiv preprintVladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.</p>
<p>Deep visual foresight for planning robot motion. Chelsea Finn, Sergey Levine, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEEChelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.</p>
<p>Model-Agnostic Meta-Learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, arXiv:1703.03400arXiv preprintChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.</p>
<p>Model predictive control: Theory and practice-a survey. E Carlos, Garcia, M David, Manfred Prett, Morari, Automatica. 253Carlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: Theory and practice-a survey. Automatica, 25(3):335-348, 1989.</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672-2680, 2014.</p>
<p>Deep learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT PressCambridgeIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT Press, Cam- bridge, 2016.</p>
<p>The value equivalence principle for model-based reinforcement learning. Christopher Grimm, Andr Barreto, Satinder Singh, David Silver, arXiv:2011.03506International Conference on Machine Learning. Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey LevinearXiv preprintContinuous deep Qlearning with model-based accelerationChristopher Grimm, Andr Barreto, Satinder Singh, and David Silver. The value equivalence principle for model-based reinforcement learning. arXiv preprint arXiv:2011.03506, 2020. Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep Q- learning with model-based acceleration. In International Conference on Machine Learn- ing, pages 2829-2838, 2016.</p>
<p>Learning to search with MCTSnets. Arthur Guez, Thophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Rmi Munos, David Silver, arXiv:1802.04697arXiv preprintArthur Guez, Thophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Rmi Munos, and David Silver. Learning to search with MCTSnets. arXiv preprint arXiv:1802.04697, 2018.</p>
<p>Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sbastien Racanire, Thophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, arXiv:1901.03559An investigation of model-free planning. arXiv preprintArthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sbastien Racanire, Thophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, et al. An investigation of model-free planning. arXiv preprint arXiv:1901.03559, 2019.</p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jrgen Schmidhuber, Advances in Neural Information Processing Systems. David Ha and Jrgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pages 2450-2462, 2018a.</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. David Ha, Jrgen Schmidhuber, arXiv:1803.10122Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. PMLRWorld models. arXiv preprintInternational conference on machine learningDavid Ha and Jrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018b. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor. In Interna- tional conference on machine learning, pages 1861-1870. PMLR, 2018a.</p>
<p>Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, arXiv:1812.05905Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprintTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algo- rithms and applications. arXiv preprint arXiv:1812.05905, 2018b.</p>
<p>Learning latent dynamics for planning from pixels. Danar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, arXiv:1811.04551arXiv preprintDanar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.</p>
<p>Dream to control: Learning behaviors by latent imagination. Danar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, arXiv:1912.01603arXiv preprintDanar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</p>
<p>Mastering atari with discrete world models. Danar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, arXiv:2010.02193arXiv preprintDanar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.</p>
<p>Analogues of mental simulation and imagination in deep learning. B Jessica, Hamrick, Current Opinion in Behavioral Sciences. 29Jessica B Hamrick. Analogues of mental simulation and imagination in deep learning. Current Opinion in Behavioral Sciences, 29:8-16, 2019.</p>
<p>B Jessica, Andrew J Hamrick, Razvan Ballard, Oriol Pascanu, Nicolas Vinyals, Peter W Heess, Battaglia, arXiv:1705.02670Metacontrol for adaptive imagination-based optimization. arXiv preprintJessica B Hamrick, Andrew J Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas Heess, and Peter W Battaglia. Metacontrol for adaptive imagination-based optimization. arXiv preprint arXiv:1705.02670, 2017.</p>
<p>Learning continuous control policies by stochastic value gradients. Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, Yuval Tassa, Advances in Neural Information Processing Systems. Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pages 2944-2952, 2015.</p>
<p>Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver, arXiv:1710.02298Rainbow: Combining improvements in deep reinforcement learning. arXiv preprintMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.</p>
<p>Random decision forests. Kam Tin, Ho, Proceedings of 3rd international conference on document analysis and recognition. 3rd international conference on document analysis and recognitionIEEE1Tin Kam Ho. Random decision forests. In Proceedings of 3rd international conference on document analysis and recognition, volume 1, pages 278-282. IEEE, 1995.</p>
<p>Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey, arXiv:2004.05439Meta-learning in neural networks: A survey. arXiv preprintTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020.</p>
<p>Model-based reinforcement learning. Jonathan Hui, Jonathan Hui. Model-based reinforcement learning https://medium.com/@jonathan_ hui/rl-model-based-reinforcement-learning-3c2b6f0aa323. Medium post, 2018.</p>
<p>A survey of deep meta-learning. Mike Huisman, Jan N Van Rn, Aske Plaat, Artificial Intelligence Review. Mike Huisman, Jan N. van Rn, and Aske Plaat. A survey of deep meta-learning. Artificial Intelligence Review, 2021.</p>
<p>Efficient learning in cellular simultaneous recurrent neural networks-the case of maze navigation problem. Roman Ilin, Robert Kozma, Paul J Werbos, IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning. Roman Ilin, Robert Kozma, and Paul J Werbos. Efficient learning in cellular simultaneous recurrent neural networks-the case of maze navigation problem. In 2007 IEEE Interna- tional Symposium on Approximate Dynamic Programming and Reinforcement Learning, pages 324-329, 2007.</p>
<p>When to trust your model: Model-based policy optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, Advances in Neural Information Processing Systems. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, pages 12498-12509, 2019.</p>
<p>Deep learning for video game playing. Niels Justesen, Philip Bontrager, Julian Togelius, Sebastian Risi, IEEE Transactions on Games. 121Niels Justesen, Philip Bontrager, Julian Togelius, and Sebastian Risi. Deep learning for video game playing. IEEE Transactions on Games, 12(1):1-20, 2019.</p>
<p>Reinforcement learning: A survey. Leslie Pack Kaelbling, Andrew W Michael L Littman, Moore, Journal of Artificial Intelligence Research. 4Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of Artificial Intelligence Research, 4:237-285, 1996.</p>
<p>Daniel Kahneman, Thinking, Farrar, Straus, Giroux, Mohammad Kaiser, Piotr Babaeizadeh, Blazej Milos, Osinski, H Roy, Konrad Campbell, Dumitru Czechowski, Chelsea Erhan, Piotr Finn, Sergey Kozakowski, Levine, arXiv:1903.00374Model-based reinforcement learning for Atari. arXiv preprintDaniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for Atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>Uncertainty-driven imagination for continuous deep reinforcement learning. Gabriel Kalweit, Joschka Boedecker, Conference on Robot Learning. Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep reinforcement learning. In Conference on Robot Learning, pages 195-206, 2017.</p>
<p>Aircraft optimal terrain/threat-based trajectory planning and control. Reza Kamyar, Ehsan Taheri, Journal of Guidance, Control, and Dynamics. 372Satwik Kansal and Brendan Martin. Learn data science webpage.Reza Kamyar and Ehsan Taheri. Aircraft optimal terrain/threat-based trajectory planning and control. Journal of Guidance, Control, and Dynamics, 37(2):466-483, 2014. Satwik Kansal and Brendan Martin. Learn data science web- page., 2018. URL https://www.learndatasci.com/tutorials/ reinforcement-q-learning-scratch-python-openai-gym/.</p>
<p>Deep variational Bayes filters: Unsupervised learning of state space models from raw data. Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick Van Der, Smagt, arXiv:1605.06432arXiv preprintMaximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep variational Bayes filters: Unsupervised learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.</p>
<p>Gradient theory of optimal flight paths. J Henry, Kelley, American Rocket Society Journal. 3010Henry J Kelley. Gradient theory of optimal flight paths. American Rocket Society Journal, 30(10):947-954, 1960.</p>
<p>VizDoom: A Doom-based AI research platform for visual reinforcement learning. Micha Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jakowski, 2016 IEEE Conference on Computational Intelligence and Games. Micha Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jakowski. VizDoom: A Doom-based AI research platform for visual reinforcement learning. In 2016 IEEE Conference on Computational Intelligence and Games, pages 1-8, 2016.</p>
<p>. P Diederik, Max Kingma, Welling, arXiv:1312.6114Auto-encoding variational Bayes. arXiv preprintDiederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>An introduction to variational autoencoders. P Diederik, Max Kingma, Welling, arXiv:1906.02691arXiv preprintDiederik P Kingma and Max Welling. An introduction to variational autoencoders. arXiv preprint arXiv:1906.02691, 2019.</p>
<p>Reinforcement learning in robotics: A survey. Jens Kober, Andrew Bagnell, Jan Peters, The International Journal of Robotics Research. 3211Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238-1274, 2013.</p>
<p>Actor-critic algorithms. R Vay, John N Konda, Tsitsiklis, Advances in Neural Information Processing Systems. Vay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information Processing Systems, pages 1008-1014, 2000.</p>
<p>Stabilizing state-feedback design via the moving horizon method. W Hi Kwon, T Bruckstein, Kailath, International Journal of Control. 373W Hi Kwon, AM Bruckstein, and T Kailath. Stabilizing state-feedback design via the moving horizon method. International Journal of Control, 37(3):631-643, 1983.</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, Advances in Neural Information Processing Systems. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre- dictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6402-6413, 2017.</p>
<p>Deep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, Nature. 5217553436Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.</p>
<p>Learning neural network policies with guided policy search under unknown dynamics. Sergey Levine, Pieter Abbeel, Advances in Neural Information Processing Systems. Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pages 1071-1079, 2014.</p>
<p>Guided policy search. Sergey Levine, Vladlen Koltun, International Conference on Machine Learning. Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pages 1-9, 2013.</p>
<p>Playing Atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.5602arXiv preprintVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540529Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International Conference on Machine Learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928- 1937, 2016.</p>
<p>A0c: Alpha zero in continuous action space. Joost Thomas M Moerland, Aske Broekens, Catholn M Jonker Plaat, arXiv:1805.09613arXiv preprintThomas M Moerland, Joost Broekens, Aske Plaat, and Catholn M Jonker. A0c: Alpha zero in continuous action space. arXiv preprint arXiv:1805.09613, 2018.</p>
<p>A framework for reinforcement learning and planning. Joost Thomas M Moerland, Catholn M Jonker Broekens, arXiv:2006.15009arXiv preprintThomas M Moerland, Joost Broekens, and Catholn M Jonker. A framework for reinforce- ment learning and planning. arXiv preprint arXiv:2006.15009, 2020a.</p>
<p>Model-based reinforcement learning: A survey. Joost Thomas M Moerland, Catholn M Jonker Broekens, arXiv:2006.16712arXiv preprintThomas M Moerland, Joost Broekens, and Catholn M Jonker. Model-based reinforcement learning: A survey. arXiv preprint arXiv:2006.16712, 2020b.</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Anusha Nagabandi, Gregory Kahn, S Ronald, Sergey Fearing, Levine, 2018 IEEE International Conference on Robotics and Automation (ICRA). Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559- 7566, 2018.</p>
<p>Actionconditional video prediction using deep networks in Atari games. Nantas Nardelli, Gabriel Synnaeve, Zeming Lin, Pushmeet Kohli, H S Philip, Nicolas Usunier ; Junhyuk Torr, Xiaoxiao Oh, Honglak Guo, Lee, L Richard, Satinder Lewis, Singh, arXiv:1805.11199Advances in Neural Information Processing Systems. arXiv preprintValue propagation networksNantas Nardelli, Gabriel Synnaeve, Zeming Lin, Pushmeet Kohli, Philip HS Torr, and Nicolas Usunier. Value propagation networks. arXiv preprint arXiv:1805.11199, 2018. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action- conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems, pages 2863-2871, 2015.</p>
<p>Value prediction network. Junhyuk Oh, Satinder Singh, Honglak Lee, Advances in Neural Information Processing Systems. Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information Processing Systems, pages 6118-6128, 2017.</p>
<p>A survey of real-time strategy game AI research and competition in StarCraft. Santiago Ontann, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, Mike Preuss, IEEE Transactions on Computational Intelligence and AI in Games. 54Santiago Ontann, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and Mike Preuss. A survey of real-time strategy game AI research and competition in StarCraft. IEEE Transactions on Computational Intelligence and AI in Games, 5(4): 293-311, 2013.</p>
<p>Learning modelbased planning from scratch. Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanire, David Reichert, Thophane Weber, Daan Wierstra, Peter Battaglia, arXiv:1707.06170arXiv preprintRazvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanire, David Reichert, Thophane Weber, Daan Wierstra, and Peter Battaglia. Learning model- based planning from scratch. arXiv preprint arXiv:1707.06170, 2017.</p>
<p>Learning to Play: Reinforcement Learning and Games. Aske Plaat, Springer VerlagSeeHeidelbergAske Plaat. Learning to Play: Reinforcement Learning and Games. Springer Verlag, Hei- delberg, See https://learningtoplay.net, 2020.</p>
<p>Survey of model-based reinforcement learning: Applications on robotics. S Athanasios, Lazaros Polydoros, Nalpantidis, Journal of Intelligent &amp; Robotic Systems. 862Athanasios S Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning: Applications on robotics. Journal of Intelligent &amp; Robotic Systems, 86(2): 153-173, 2017.</p>
<p>Robust constrained model predictive control. Arthur George Richards, Massachusetts Institute of TechnologyPhD thesisArthur George Richards. Robust constrained model predictive control. PhD thesis, Mas- sachusetts Institute of Technology, 2005.</p>
<p>From Chess and Atari to StarCraft and Beyond: How Game AI is Driving the World of AI. Sebastian Risi, Mike Preuss, KI-Knstliche IntelligenzSebastian Risi and Mike Preuss. From Chess and Atari to StarCraft and Beyond: How Game AI is Driving the World of AI. KI-Knstliche Intelligenz, pages 1-11, 2020.</p>
<p>Multi-armed bandits with episode context. Christopher D Rosin, Annals of Mathematics and Artificial Intelligence. 613Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence, 61(3):203-230, 2011.</p>
<p>Value iteration networks on multiple levels of abstraction. Daniel Schleich, Tobias Klamt, Sven Behnke, arXiv:1905.11068arXiv preprintDaniel Schleich, Tobias Klamt, and Sven Behnke. Value iteration networks on multiple levels of abstraction. arXiv preprint arXiv:1905.11068, 2019.</p>
<p>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. Jrgen Schmidhuber, CNN International Joint Conference on Neural Networks. IEEEJrgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In 1990 CNN International Joint Conference on Neural Networks, pages 253-258. IEEE, 1990a.</p>
<p>Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Jrgen Schmidhuber, Jrgen Schmidhuber. Making the world differentiable: On using self-supervised fully recur- rent neural networks for dynamic reinforcement learning and planning in non-stationary environments. 1990b.</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 5887839Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839): 604-609, 2020.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Planning to explore via self-supervised world models. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danar Hafner, Deepak Pathak, arXiv:2005.05960arXiv preprintRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. arXiv preprint arXiv:2005.05960, 2020.</p>
<p>Temporal-difference search in computer Go. David Silver, S Richard, Martin Sutton, Mller, Machine Learning. 87David Silver, Richard S Sutton, and Martin Mller. Temporal-difference search in computer Go. Machine Learning, 87(2):183-219, 2012.</p>
<p>Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. David Silver, Guy Lever, David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried- miller. Deterministic policy gradient algorithms. 2014.</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 529484David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587): 484, 2016.</p>
<p>George van den Driessche, Thore Graepel, and Demis Hassabis. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, Nature. 5507676354Mastering the game of Go without human knowledgeDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354, 2017a.</p>
<p>The predictron: End-to-end learning and planning. David Silver, Matteo Hado Van Hasselt, Tom Hessel, Arthur Schaul, Tim Guez, Gabriel Harley, David Dulac-Arnold, Neil Reichert, Andre Rabinowitz, Barreto, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningDavid Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The pre- dictron: End-to-end learning and planning. In Proceedings of the 34th International Conference on Machine Learning, pages 3191-3199, 2017b.</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis, Science. 3626419David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419):1140-1144, 2018.</p>
<p>Universal planning networks. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea Finn, arXiv:1804.00645arXiv preprintAravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018.</p>
<p>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. S Richard, Sutton, Machine learning proceedings. ElsevierRichard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine learning proceedings 1990, pages 216-224. Elsevier, 1990.</p>
<p>Dyna, an integrated architecture for learning, planning, and reacting. S Richard, Sutton, ACM Sigart Bulletin. 24Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160-163, 1991.</p>
<p>Reinforcement learning, An Introduction, Second Edition. S Richard, Andrew G Sutton, Barto, MIT PressRichard S Sutton and Andrew G Barto. Reinforcement learning, An Introduction, Second Edition. MIT Press, 2018.</p>
<p>Agnostic system identification for monte carlo planning. Erik Talvitie, Twenty-Ninth AAAI Conference on Artificial Intelligence. Erik Talvitie. Agnostic system identification for monte carlo planning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.</p>
<p>Value iteration networks. Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, Pieter Abbeel, Adv. in Neural Information Processing Systems. Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Adv. in Neural Information Processing Systems, pages 2154-2162, 2016.</p>
<p>Synthesis and stabilization of complex behaviors through online trajectory optimization. Yuval Tassa, Tom Erez, Emanuel Todorov, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behav- iors through online trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4906-4913, 2012.</p>
<p>. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego De Las, David Casas, Budden, arXiv:1801.00690Abbas Abdolmaleki. Andrew LefrancqarXiv preprintJosh Merel. et al. Deepmind control suiteYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>MuJoCo: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5026-5033, 2012.</p>
<p>Deep reinforcement learning for general video game ai. Ruben Rodriguez Torrado, Philip Bontrager, Julian Togelius, Jialin Liu, Diego Perez-Liebana, 2018 IEEE Conference on Computational Intelligence and Games (CIG). IEEERuben Rodriguez Torrado, Philip Bontrager, Julian Togelius, Jialin Liu, and Diego Perez- Liebana. Deep reinforcement learning for general video game ai. In 2018 IEEE Conference on Computational Intelligence and Games (CIG), pages 1-8. IEEE, 2018.</p>
<p>Jaap Van den Herik, et al. Dimensionality reduction: a comparative. Laurens Van Der Maaten, Eric Postma, J Mach Learn Res. 1013Laurens Van Der Maaten, Eric Postma, Jaap Van den Herik, et al. Dimensionality reduction: a comparative. J Mach Learn Res, 10(66-71):13, 2009.</p>
<p>Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Kttler, John Agapiou, Julian Schrittwieser, arXiv:1708.04782Starcraft II: A new challenge for reinforcement learning. arXiv preprintOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhn- evets, Michelle Yeo, Alireza Makhzani, Heinrich Kttler, John Agapiou, Julian Schrit- twieser, et al. Starcraft II: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michal Mathieu, Andrew Dudzik, Junyoung Chung, H David, Richard Choi, Timo Powell, Petko Ewalds, Georgiev, Nature. 5757782Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michal Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350-354, 2019.</p>
<p>Benchmarking modelbased reinforcement learning. Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba, arXiv:1907.02057preprintTingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model- based reinforcement learning. preprint arXiv:1907.02057, 2019.</p>
<p>Learning from delayed rewards. Jch Christopher, Watkins, King's College, CambridgePhD thesisChristopher JCH Watkins. Learning from delayed rewards. PhD thesis, King's College, Cambridge, 1989.</p>
<p>Imagination-augmented agents for deep reinforcement learning. Thophane Weber, Sbastien Racanire, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Advances in Neural Information Processing Systems. Thophane Weber, Sbastien Racanire, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 5690-5701, 2017.</p>
<p>Multiagent deep reinforcement learning: Challenges and directions towards human-like approaches. Annie Wong, Thomas Bck, Anna V Kononova, Aske Plaat, Artificial Intelligence Review. Annie Wong, Thomas Bck, Anna V. Kononova, and Aske Plaat. Multiagent deep rein- forcement learning: Challenges and directions towards human-like approaches. Artificial Intelligence Review, 2021.</p>
<p>Convolutional LSTM network: A machine learning approach for precipitation nowcasting. Zhourong Shi Xingjian, Hao Chen, Dit-Yan Wang, Wai-Kin Yeung, Wangchun Wong, Woo, arXiv:1806.01830Advances in Neural Information Processing Systems. Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward LockhartarXiv preprintet al. Relational deep reinforcement learningSHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang- chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems, pages 802-810, 2015. Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
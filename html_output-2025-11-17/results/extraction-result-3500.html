<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3500 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3500</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3500</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-fe257027193ea4a74fdab99d7509ce4002ad7de6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fe257027193ea4a74fdab99d7509ce4002ad7de6" target="_blank">Learning a SAT Solver from Single-Bit Supervision</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations.</p>
                <p><strong>Paper Abstract:</strong> We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3500.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3500.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuroSAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural SAT Solver (NeuroSAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A message-passing neural network (MPNN) that is trained end-to-end with a single bit of supervision per problem (satisfiable / unsatisfiable) to perform search for satisfying assignments on CNF SAT instances; it operates on a bipartite literal-clause graph with complementary-literal edges and can be run for variable numbers of iterations at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NeuroSAT (message-passing neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An MPNN operating on a graph with nodes for literals and clauses and edges for literal-in-clause and complementary-literal pairs. Each node has a d-dimensional embedding updated by learned MLP message functions and layer-norm LSTMs; architecture enforces permutation and negation invariances. After T iterations, literal embeddings are projected to scalar votes and averaged to predict satisfiability. Trained end-to-end with sigmoid cross-entropy on a dataset of SAT pairs differing by a single literal flip.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Propositional SAT solving (predict satisfiable / decode satisfying assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decide satisfiability of CNF formulas and, for satisfiable instances, produce a satisfying assignment; strict combinatorial logical reasoning / discrete search over boolean assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train MPNN (NeuroSAT) with only single-bit supervision (satisfiable/unsat) on SR(U(10,40)); architecture: literal+clause nodes, Flip operator for complements, MLPs for messages and vote projection, layer-norm LSTMs; run for T=26 iterations during training but allowed to run for many more iterations at test time; decode assignments by 2‑clustering literal embeddings (k-means) and trying both labelings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On SR(40) test after training on SR(U(10,40)): overall accuracy 85%; accuracy on unsat problems 96%; accuracy on sat problems 73%; percent of satisfiable problems for which a satisfying assignment can be decoded 70%. Extrapolation: on SR(200) NeuroSAT solves ~25% of satisfiable problems when run for ~4× training iterations; with only training-time iterations it solves <10% of SR(200). On 4,888 satisfiable SAT encodings of graph tasks (coloring/clique/dominating set/vertex cover) NeuroSAT (run for 512 iterations) decoded solutions for 85% of instances.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to Survey Propagation (SP) on the graph-encoded problems: SP did not converge to a satisfying assignment on any of those problems (0%); compared qualitatively to MiniSat used as a ground-truth solver: MiniSat needed on average ~10 backjumps and >100 unit propagations to solve SR(40) problems (i.e., a strong classical solver baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NeuroSAT achieved nontrivial end-to-end solving capability on random and structurally different SAT encodings where a canonical learning-free message-passing algorithm (Survey Propagation) failed to produce satisfying assignments (85% vs 0% on those graph-encoded problems). It is nevertheless not competitive with state-of-the-art SAT solvers like MiniSat for reliability and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not competitive with modern engineered SAT solvers; on unsatisfiable problems (when trained on SR) it does not produce proofs and instead searches indefinitely without systematic proof construction; decoding a satisfying assignment from its scalar 'votes' is sometimes unreliable (the direct vote projection only encodes the assignment by chance), requiring explicit clustering of high-dimensional embeddings; extrapolation success depends on running many more iterations (i.e., slower); may not generalize to arbitrary unsat-core-based reasoning unless explicitly trained on such distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Observed a sharp phase transition in literal-vote dynamics at the moment a solution is found; PCA visualizations show literals cluster by final assignment only after phase transition. Reliable decoding achieved by 2‑clustering the high-dimensional literal embeddings (k-means). Trained for T=26 iterations but empirical analysis shows learned dynamical system can be run for hundreds/thousands of iterations at test time to solve harder problems. Authors also trained a separate variant to predict per-literal satisfiability bits (omitted in detail) which behaved differently (no phase transition, earlier predictions but with Type I/II errors), indicating objectives/data strongly affect learned procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a SAT Solver from Single-Bit Supervision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3500.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3500.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuroUNSAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Unsat-core Detector (NeuroUNSAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same NeuroSAT architecture retrained on a dataset where unsatisfiable examples contain a small unsat core; the network learns to detect and localize the unsat core rather than search for satisfying assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NeuroUNSAT (NeuroSAT retrained on SRC data)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identical MPNN architecture to NeuroSAT but trained on SR C(n,u) datasets in which every unsatisfiable problem contains a small unsat core; the model learns to recognize patterns corresponding to these cores and signals unsatisfiability based on internal activations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Unsatisfiable-core detection and localization (specialized SAT unsat-certificate recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Detect whether a CNF contains a small, localized unsat core and identify literals/clauses participating in that contradiction (helps produce proofs like resolution proofs when core is small).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train NeuroSAT architecture on SR C(n,u) where unsat examples include a small known unsat core (u); decoding unsat-core literals via 2‑clustering of literal embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On the test set from SR C(40,u) NeuroUNSAT achieved 100% accuracy predicting satisfiability; literals in the unsat core end up isolated into their own cluster 98% of the time (i.e., reliable localization).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Compared to the NeuroSAT model trained on SR (which searches for satisfying assignments and cannot construct proofs), NeuroUNSAT reliably recognizes the supplied small unsat cores and yields perfect test accuracy on that specialized dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Authors note NeuroUNSAT likely memorizes specific subgraph patterns (the particular cores) from the training set and is not shown to generalize to arbitrary unsat cores; no evidence it learned a generic proof procedure for arbitrary unsatisfiable formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Literal embeddings for core literals cluster tightly and are readily separable after training; success is contingent on the unsat core being small relative to the problem and included in training distribution; the paper explicitly warns against assuming generalization to arbitrary unsat cores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a SAT Solver from Single-Bit Supervision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3500.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3500.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniSat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniSat (conflict-driven clause-learning SAT solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A canonical, engineered conflict-driven clause-learning SAT solver used as the ground-truth solver to label datasets and as a classical baseline for difficulty comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minisat v1.13-a sat solver with conflict-clause minimization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniSat (classical SAT solver)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A highly optimized CDCL (conflict-driven clause learning) SAT solver used by the authors to compute satisfiability labels for training and to characterize problem difficulty (number of backjumps and unit propagations required).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Propositional SAT solving (classical solver baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deterministic search algorithm using unit propagation, branching heuristics, backjumping, and clause learning to decide satisfiability and produce proofs/assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>N/A (classical symbolic algorithm rather than learned intervention); used to (1) generate ground-truth labels for training data and (2) report problem-solving costs (backjumps, unit propagations) as a measure of instance difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as an accuracy number in the paper, but reported empirical costs on SR(40): MiniSat typically backjumps ~10 times on average and performs >100 unit propagations to solve each SR(40) problem; classical solvers can solve much larger problems in practice (millions of variables) though not quantified in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NeuroSAT is not competitive with MiniSat in reliability or scale; MiniSat is used to provide ground truth and to demonstrate that SR problems are nontrivial (MiniSat still requires nontrivial search effort).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not applicable to the learned-method evaluation except that MiniSat's performance motivates problem difficulty; paper concedes NeuroSAT is 'vastly less reliable' than state-of-the-art solvers like MiniSat.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors report MiniSat's backjump and unit-propagation counts as evidence that SR(n) problems require nontrivial logical inference, motivating the need for learned search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a SAT Solver from Single-Bit Supervision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3500.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3500.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Survey Propagation (SP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Survey Propagation algorithm for satisfiability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A canonical, learning-free message-passing algorithm from statistical physics for satisfiability problems; used as a non-learned message-passing baseline in the paper's experiments on graph-encoded NP instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Survey propagation: An algorithm for satisfiability.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Survey Propagation (SP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A heuristic, iterative message-passing algorithm derived from belief-propagation / cavity-method ideas in statistical physics, previously used to attack random SAT ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Propositional SAT solving via message-passing heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Attempt to find satisfying assignments via iterative, local message passing (non-learned) across factor-graph representations of CNF formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard SP algorithm with reinforcement messages as implemented by authors (and a numerical trick from Knuth). Used directly (no learning) as a baseline on graph-encoded SAT instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On the 4,888 satisfiable SAT encodings of graph tasks, Survey Propagation did not converge to a satisfying assignment on any of these problems (0% success).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NeuroSAT (learning-based MPNN) solved 85% of those same graph-encoded instances (with 512 iterations), whereas SP solved 0%, indicating NeuroSAT learned an algorithm qualitatively different from SP and substantially more effective on these instances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>SP failed to converge to satisfying assignments on the graph-encoded benchmarks used in the paper; this highlights that SP's heuristics are not universally effective across problem distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors implemented an SP variant with reinforcement messages and a numerical stabilization trick, but still observed non-convergence—used to argue NeuroSAT is not simply approximating SP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a SAT Solver from Single-Bit Supervision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3500.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3500.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM many-hot baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM trained on many-hot clause encodings (attempted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional sequential LSTM baseline trained on many-hot encodings of clauses specialized to 40-variable problems; it failed to learn to predict satisfiability (>50% accuracy) on the training set, demonstrating limits of naive sequence encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (many-hot encoding baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LSTM-based sequence model trained on a many-hot encoding of clauses (specialized to problems with exactly 40 variables) attempted as a simple learned baseline for satisfiability classification; details: conventional RNN architecture (not permutation/negation invariant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Propositional SAT classification (learned baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict satisfiable vs unsatisfiable from a many-hot encoding of CNF clauses (sequence input to LSTM), intended as a straightforward neural baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard LSTM training on many-hot clause encodings (no graph invariance or message passing), trained on SR(40)-type problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Failed to achieve >50% accuracy on its training set (i.e., learning did not succeed for this encoding and architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NeuroSAT (MPNN) substantially outperformed this naive LSTM baseline, indicating the importance of architecture that respects SAT instance symmetries (permutation and negation invariances) and graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>LSTM many-hot encoding ignored key invariances (permutation, negation) and could not learn to classify the SR dataset; demonstrates that naive sequence models may fail on strict logical reasoning tasks without appropriate inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors report inability to train this LSTM baseline as evidence motivating the graph-structured MPNN architecture; no detailed ablations provided for the LSTM itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a SAT Solver from Single-Bit Supervision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural message passing for quantum chemistry <em>(Rating: 2)</em></li>
                <li>Survey propagation: An algorithm for satisfiability. <em>(Rating: 2)</em></li>
                <li>Recurrent relational networks for complex relational reasoning <em>(Rating: 2)</em></li>
                <li>Can neural networks understand logical entailment? <em>(Rating: 2)</em></li>
                <li>Minisat v1.13-a sat solver with conflict-clause minimization. <em>(Rating: 1)</em></li>
                <li>The art of computer programming, volume 4, fascicle 6: Satisfiability <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3500",
    "paper_id": "paper-fe257027193ea4a74fdab99d7509ce4002ad7de6",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "NeuroSAT",
            "name_full": "Neural SAT Solver (NeuroSAT)",
            "brief_description": "A message-passing neural network (MPNN) that is trained end-to-end with a single bit of supervision per problem (satisfiable / unsatisfiable) to perform search for satisfying assignments on CNF SAT instances; it operates on a bipartite literal-clause graph with complementary-literal edges and can be run for variable numbers of iterations at test time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NeuroSAT (message-passing neural network)",
            "model_description": "An MPNN operating on a graph with nodes for literals and clauses and edges for literal-in-clause and complementary-literal pairs. Each node has a d-dimensional embedding updated by learned MLP message functions and layer-norm LSTMs; architecture enforces permutation and negation invariances. After T iterations, literal embeddings are projected to scalar votes and averaged to predict satisfiability. Trained end-to-end with sigmoid cross-entropy on a dataset of SAT pairs differing by a single literal flip.",
            "model_size": null,
            "reasoning_task_name": "Propositional SAT solving (predict satisfiable / decode satisfying assignment)",
            "reasoning_task_description": "Decide satisfiability of CNF formulas and, for satisfiable instances, produce a satisfying assignment; strict combinatorial logical reasoning / discrete search over boolean assignments.",
            "method_or_intervention": "Train MPNN (NeuroSAT) with only single-bit supervision (satisfiable/unsat) on SR(U(10,40)); architecture: literal+clause nodes, Flip operator for complements, MLPs for messages and vote projection, layer-norm LSTMs; run for T=26 iterations during training but allowed to run for many more iterations at test time; decode assignments by 2‑clustering literal embeddings (k-means) and trying both labelings.",
            "performance": "On SR(40) test after training on SR(U(10,40)): overall accuracy 85%; accuracy on unsat problems 96%; accuracy on sat problems 73%; percent of satisfiable problems for which a satisfying assignment can be decoded 70%. Extrapolation: on SR(200) NeuroSAT solves ~25% of satisfiable problems when run for ~4× training iterations; with only training-time iterations it solves &lt;10% of SR(200). On 4,888 satisfiable SAT encodings of graph tasks (coloring/clique/dominating set/vertex cover) NeuroSAT (run for 512 iterations) decoded solutions for 85% of instances.",
            "baseline_performance": "Compared to Survey Propagation (SP) on the graph-encoded problems: SP did not converge to a satisfying assignment on any of those problems (0%); compared qualitatively to MiniSat used as a ground-truth solver: MiniSat needed on average ~10 backjumps and &gt;100 unit propagations to solve SR(40) problems (i.e., a strong classical solver baseline).",
            "improvement_over_baseline": "NeuroSAT achieved nontrivial end-to-end solving capability on random and structurally different SAT encodings where a canonical learning-free message-passing algorithm (Survey Propagation) failed to produce satisfying assignments (85% vs 0% on those graph-encoded problems). It is nevertheless not competitive with state-of-the-art SAT solvers like MiniSat for reliability and scalability.",
            "limitations_or_failures": "Not competitive with modern engineered SAT solvers; on unsatisfiable problems (when trained on SR) it does not produce proofs and instead searches indefinitely without systematic proof construction; decoding a satisfying assignment from its scalar 'votes' is sometimes unreliable (the direct vote projection only encodes the assignment by chance), requiring explicit clustering of high-dimensional embeddings; extrapolation success depends on running many more iterations (i.e., slower); may not generalize to arbitrary unsat-core-based reasoning unless explicitly trained on such distributions.",
            "ablation_or_analysis": "Observed a sharp phase transition in literal-vote dynamics at the moment a solution is found; PCA visualizations show literals cluster by final assignment only after phase transition. Reliable decoding achieved by 2‑clustering the high-dimensional literal embeddings (k-means). Trained for T=26 iterations but empirical analysis shows learned dynamical system can be run for hundreds/thousands of iterations at test time to solve harder problems. Authors also trained a separate variant to predict per-literal satisfiability bits (omitted in detail) which behaved differently (no phase transition, earlier predictions but with Type I/II errors), indicating objectives/data strongly affect learned procedures.",
            "uuid": "e3500.0",
            "source_info": {
                "paper_title": "Learning a SAT Solver from Single-Bit Supervision",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "NeuroUNSAT",
            "name_full": "Neural Unsat-core Detector (NeuroUNSAT)",
            "brief_description": "The same NeuroSAT architecture retrained on a dataset where unsatisfiable examples contain a small unsat core; the network learns to detect and localize the unsat core rather than search for satisfying assignments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NeuroUNSAT (NeuroSAT retrained on SRC data)",
            "model_description": "Identical MPNN architecture to NeuroSAT but trained on SR C(n,u) datasets in which every unsatisfiable problem contains a small unsat core; the model learns to recognize patterns corresponding to these cores and signals unsatisfiability based on internal activations.",
            "model_size": null,
            "reasoning_task_name": "Unsatisfiable-core detection and localization (specialized SAT unsat-certificate recognition)",
            "reasoning_task_description": "Detect whether a CNF contains a small, localized unsat core and identify literals/clauses participating in that contradiction (helps produce proofs like resolution proofs when core is small).",
            "method_or_intervention": "Train NeuroSAT architecture on SR C(n,u) where unsat examples include a small known unsat core (u); decoding unsat-core literals via 2‑clustering of literal embeddings.",
            "performance": "On the test set from SR C(40,u) NeuroUNSAT achieved 100% accuracy predicting satisfiability; literals in the unsat core end up isolated into their own cluster 98% of the time (i.e., reliable localization).",
            "baseline_performance": null,
            "improvement_over_baseline": "Compared to the NeuroSAT model trained on SR (which searches for satisfying assignments and cannot construct proofs), NeuroUNSAT reliably recognizes the supplied small unsat cores and yields perfect test accuracy on that specialized dataset.",
            "limitations_or_failures": "Authors note NeuroUNSAT likely memorizes specific subgraph patterns (the particular cores) from the training set and is not shown to generalize to arbitrary unsat cores; no evidence it learned a generic proof procedure for arbitrary unsatisfiable formulas.",
            "ablation_or_analysis": "Literal embeddings for core literals cluster tightly and are readily separable after training; success is contingent on the unsat core being small relative to the problem and included in training distribution; the paper explicitly warns against assuming generalization to arbitrary unsat cores.",
            "uuid": "e3500.1",
            "source_info": {
                "paper_title": "Learning a SAT Solver from Single-Bit Supervision",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "MiniSat",
            "name_full": "MiniSat (conflict-driven clause-learning SAT solver)",
            "brief_description": "A canonical, engineered conflict-driven clause-learning SAT solver used as the ground-truth solver to label datasets and as a classical baseline for difficulty comparisons.",
            "citation_title": "Minisat v1.13-a sat solver with conflict-clause minimization.",
            "mention_or_use": "use",
            "model_name": "MiniSat (classical SAT solver)",
            "model_description": "A highly optimized CDCL (conflict-driven clause learning) SAT solver used by the authors to compute satisfiability labels for training and to characterize problem difficulty (number of backjumps and unit propagations required).",
            "model_size": null,
            "reasoning_task_name": "Propositional SAT solving (classical solver baseline)",
            "reasoning_task_description": "Deterministic search algorithm using unit propagation, branching heuristics, backjumping, and clause learning to decide satisfiability and produce proofs/assignments.",
            "method_or_intervention": "N/A (classical symbolic algorithm rather than learned intervention); used to (1) generate ground-truth labels for training data and (2) report problem-solving costs (backjumps, unit propagations) as a measure of instance difficulty.",
            "performance": "Not reported as an accuracy number in the paper, but reported empirical costs on SR(40): MiniSat typically backjumps ~10 times on average and performs &gt;100 unit propagations to solve each SR(40) problem; classical solvers can solve much larger problems in practice (millions of variables) though not quantified in these experiments.",
            "baseline_performance": null,
            "improvement_over_baseline": "NeuroSAT is not competitive with MiniSat in reliability or scale; MiniSat is used to provide ground truth and to demonstrate that SR problems are nontrivial (MiniSat still requires nontrivial search effort).",
            "limitations_or_failures": "Not applicable to the learned-method evaluation except that MiniSat's performance motivates problem difficulty; paper concedes NeuroSAT is 'vastly less reliable' than state-of-the-art solvers like MiniSat.",
            "ablation_or_analysis": "Authors report MiniSat's backjump and unit-propagation counts as evidence that SR(n) problems require nontrivial logical inference, motivating the need for learned search.",
            "uuid": "e3500.2",
            "source_info": {
                "paper_title": "Learning a SAT Solver from Single-Bit Supervision",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Survey Propagation (SP)",
            "name_full": "Survey Propagation algorithm for satisfiability",
            "brief_description": "A canonical, learning-free message-passing algorithm from statistical physics for satisfiability problems; used as a non-learned message-passing baseline in the paper's experiments on graph-encoded NP instances.",
            "citation_title": "Survey propagation: An algorithm for satisfiability.",
            "mention_or_use": "use",
            "model_name": "Survey Propagation (SP)",
            "model_description": "A heuristic, iterative message-passing algorithm derived from belief-propagation / cavity-method ideas in statistical physics, previously used to attack random SAT ensembles.",
            "model_size": null,
            "reasoning_task_name": "Propositional SAT solving via message-passing heuristics",
            "reasoning_task_description": "Attempt to find satisfying assignments via iterative, local message passing (non-learned) across factor-graph representations of CNF formulas.",
            "method_or_intervention": "Standard SP algorithm with reinforcement messages as implemented by authors (and a numerical trick from Knuth). Used directly (no learning) as a baseline on graph-encoded SAT instances.",
            "performance": "On the 4,888 satisfiable SAT encodings of graph tasks, Survey Propagation did not converge to a satisfying assignment on any of these problems (0% success).",
            "baseline_performance": null,
            "improvement_over_baseline": "NeuroSAT (learning-based MPNN) solved 85% of those same graph-encoded instances (with 512 iterations), whereas SP solved 0%, indicating NeuroSAT learned an algorithm qualitatively different from SP and substantially more effective on these instances.",
            "limitations_or_failures": "SP failed to converge to satisfying assignments on the graph-encoded benchmarks used in the paper; this highlights that SP's heuristics are not universally effective across problem distributions.",
            "ablation_or_analysis": "Authors implemented an SP variant with reinforcement messages and a numerical stabilization trick, but still observed non-convergence—used to argue NeuroSAT is not simply approximating SP.",
            "uuid": "e3500.3",
            "source_info": {
                "paper_title": "Learning a SAT Solver from Single-Bit Supervision",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "LSTM many-hot baseline",
            "name_full": "LSTM trained on many-hot clause encodings (attempted baseline)",
            "brief_description": "A traditional sequential LSTM baseline trained on many-hot encodings of clauses specialized to 40-variable problems; it failed to learn to predict satisfiability (&gt;50% accuracy) on the training set, demonstrating limits of naive sequence encodings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM (many-hot encoding baseline)",
            "model_description": "An LSTM-based sequence model trained on a many-hot encoding of clauses (specialized to problems with exactly 40 variables) attempted as a simple learned baseline for satisfiability classification; details: conventional RNN architecture (not permutation/negation invariant).",
            "model_size": null,
            "reasoning_task_name": "Propositional SAT classification (learned baseline)",
            "reasoning_task_description": "Predict satisfiable vs unsatisfiable from a many-hot encoding of CNF clauses (sequence input to LSTM), intended as a straightforward neural baseline.",
            "method_or_intervention": "Standard LSTM training on many-hot clause encodings (no graph invariance or message passing), trained on SR(40)-type problems.",
            "performance": "Failed to achieve &gt;50% accuracy on its training set (i.e., learning did not succeed for this encoding and architecture).",
            "baseline_performance": null,
            "improvement_over_baseline": "NeuroSAT (MPNN) substantially outperformed this naive LSTM baseline, indicating the importance of architecture that respects SAT instance symmetries (permutation and negation invariances) and graph structure.",
            "limitations_or_failures": "LSTM many-hot encoding ignored key invariances (permutation, negation) and could not learn to classify the SR dataset; demonstrates that naive sequence models may fail on strict logical reasoning tasks without appropriate inductive bias.",
            "ablation_or_analysis": "Authors report inability to train this LSTM baseline as evidence motivating the graph-structured MPNN architecture; no detailed ablations provided for the LSTM itself.",
            "uuid": "e3500.4",
            "source_info": {
                "paper_title": "Learning a SAT Solver from Single-Bit Supervision",
                "publication_date_yy_mm": "2018-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural message passing for quantum chemistry",
            "rating": 2
        },
        {
            "paper_title": "Survey propagation: An algorithm for satisfiability.",
            "rating": 2
        },
        {
            "paper_title": "Recurrent relational networks for complex relational reasoning",
            "rating": 2
        },
        {
            "paper_title": "Can neural networks understand logical entailment?",
            "rating": 2
        },
        {
            "paper_title": "Minisat v1.13-a sat solver with conflict-clause minimization.",
            "rating": 1
        },
        {
            "paper_title": "The art of computer programming, volume 4, fascicle 6: Satisfiability",
            "rating": 1
        }
    ],
    "cost": 0.01310075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LEARNING A SAT SOLVER FROM SINGLE-BIT SUPERVISION</h1>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, David L. Dill<br>Department of Computer Science<br>Stanford University<br>Stanford, CA 94305<br>{dselsam,mlamm,buenz,pliang,dill}@cs.stanford.edu</p>
<p>Leonardo de Moura<br>Microsoft Research<br>Redmond, WA 98052<br>leonardo@microsoft.com</p>
<h4>Abstract</h4>
<p>We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.</p>
<h2>1 INTRODUCTION</h2>
<p>The propositional satisfiability problem (SAT) is one of the most fundamental problems of computer science. Cook (1971) showed that the problem is NP-complete, which means that searching for any kind of efficiently-checkable certificate in any context can be reduced to finding a satisfying assignment of a propositional formula. In practice, search problems arising from a wide range of domains such as hardware and software verification, test pattern generation, planning, scheduling, and combinatorics are all routinely solved by constructing an appropriate SAT problem and then calling a SAT solver (Gomes et al., 2008). Modern SAT solvers based on backtracking search are extremely well-engineered and have been able to solve problems of practical interest with millions of variables (Biere et al., 2009).</p>
<p>We consider the question: can a neural network learn to solve SAT problems? To answer, we develop a novel message passing neural network (MPNN) (Scarselli et al., 2009; Li et al., 2015; Gilmer et al., 2017), NeuroSAT, and train it as a classifier to predict satisfiability on a dataset of random SAT problems. We provide NeuroSAT with only a single bit of supervision for each SAT problem that indicates whether or not the problem is satisfiable. When making a prediction about a new SAT problem, we find that NeuroSAT guesses unsatisfiable with low confidence until it finds a solution, at which point it converges and guesses satisfiable with very high confidence. The solution itself can almost always be automatically decoded from the network's activations, making NeuroSAT an end-to-end SAT solver. See Figure 1 for an illustration of the train and test regimes.</p>
<p>Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve SAT problems that are substantially larger and more difficult than it ever saw during training by simply performing more iterations of message passing. Despite only running for a few dozen iterations during training, at test time NeuroSAT continues to find solutions to harder problems after hundreds and even thousands of iterations. The learning process has yielded not a traditional classifier but rather a procedure that can be run indefinitely to search for solutions to problems of varying difficulty.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We train NeuroSAT to predict whether SAT problems are satisfiable, providing only a single bit of supervision for each problem. At test time, when NeuroSAT predicts satisfiable, we can almost always extract a satisfying assignment from the network's activations. The problems at test time can also be substantially larger, more difficult, and even from entirely different domains than the problems seen during training.</p>
<p>Moreover, NeuroSAT generalizes to entirely new domains. Since NeuroSAT operates on SAT problems and since SAT is NP-complete, NeuroSAT can be queried on SAT problems encoding any kind of search problem for which certificates can be checked in polynomial time. Although we train it using only problems from a single random problem generator, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.</p>
<p>The same neural network architecture can also be used to help construct proofs for unsatisfiable problems. When we train it on a different dataset in which every unsatisfiable problem contains a small contradiction (call this trained model NeuroUNSAT), it learns to detect these contradictions instead of searching for satisfying assignments. Just as we can extract solutions from NeuroSAT's activations, we can extract the variables involved in the contradiction from NeuroUNSAT's activations. When the number of variables involved in the contradiction is small relative to the total number of variables, knowing which variables are involved in the contradiction can enable constructing a resolution proof more efficiently.</p>
<h1>2 Problem Setup</h1>
<p>Background. A formula of propositional logic is a boolean expression built using the constants true (1) and false (0), variables, negations, conjunctions, and disjunctions. A formula is satisfiable provided there exists an assignment of boolean values to its variables such that the formula evaluates to 1 . For example, the formula $\left(x_{1} \vee x_{2} \vee x_{3}\right) \wedge \neg\left(x_{1} \wedge x_{2} \wedge x_{3}\right)$ is satisfiable because it will evaluate to 1 under every assignment that does not map $x_{1}, x_{2}$ and $x_{3}$ to the same value. For every formula, there exists an equisatisfiable formula in conjunctive normal form (CNF), expressed as a conjunction of disjunctions of (possibly negated) variables. ${ }^{1}$ Each conjunct of a formula in CNF is called a clause, and each (possibly negated) variable within a clause is called a literal. The formula above is equivalent to the CNF formula $\left(x_{1} \vee x_{2} \vee x_{3}\right) \wedge\left(\neg x_{1} \vee \neg x_{2} \vee \neg x_{3}\right)$, which we can represent more concisely as ${1 \mid 2 \mid 3, \overline{1} \mid \overline{2} \mid \overline{3}}$. A formula in CNF has a satisfying assignment if and only if it has an assignment such that every clause has at least one literal mapped to 1. A SAT problem is a formula in CNF, where the goal is to determine if the formula is satisfiable, and if so, to produce a satisfying assignment of truth values to variables. We use $n$ to denote the number of of variables in a SAT problem, and $m$ to denote the number of clauses.
Classification task. For a SAT problem $P$, we define $\phi(P)$ to be true if and only if $P$ is satisfiable. Our first goal is to learn a classifier that approximates $\phi$. Given a distribution $\Psi$ over SAT problems, we can construct datasets $\mathcal{D}<em _test="{test" _text="\text">{\text {train }}$ and $\mathcal{D}</em>$ with examples of the form $(P, \phi(P))$ by sampling problems $P \sim \Psi$ and computing $\phi(P)$ using an existing SAT solver. At test time, we get only the problem $P$ and the goal is to predict $\phi(P)$, i.e. to determine if $P$ is satisfiable. Ultimately we care about the solving task, which also includes finding solutions to satisfiable problems.}</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>3 MODEL</p>
<p>A SAT problem has a simple syntactic structure and therefore could be encoded into a vector space using standard methods such as an RNN. However, the semantics of propositional logic induce rich invariances that such a syntactic method would ignore, such as permutation invariance and negation invariance. Specifically, the satisfiability of a formula is not affected by permuting the variables (e.g. swapping $x_{1}$ and $x_{2}$ throughout the formula), by permuting the clauses (e.g. swapping the first clause with the second clause), or by permuting the literals within a clause (e.g. replacing the clause $1|\overline{2}$ with $\overline{2}|1$. The satisfiability of a formula is also not affected by negating every literal corresponding to a given variable (e.g. negating all occurrences of $x_{1}$ in ${1|\overline{2},\overline{1}|\overline{3}}$ to yield ${\overline{1}|\overline{2},1|\overline{3}}$).</p>
<p>We now describe our neural network architecture, NeuroSAT, that enforces both permutation invariance and negation invariance. We encode a SAT problem as an undirected graph with one node for every literal, one node for every clause, an edge between every literal and every clause it appears in, and a different type of edge between each pair of complementary literals (e.g. between $x_{i}$ and $\overline{x_{i}}$ ). NeuroSAT iteratively refines a vector space embedding for each node by passing “messages” back and forth along the edges of the graph as described in <em>Gilmer et al. (2017)</em>. At every time step, we have an embedding for every literal and every clause. An iteration consists of two stages. First, each clause receives messages from its neighboring literals and updates its embedding accordingly. Next, each literal receives messages from its neighboring clauses as well as from its complement, then updates its embedding accordingly. Figure 2 provides a high-level illustration of the architecture.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: High-level illustration of NeuroSAT operating on the graph representation of ${1|2,\overline{1}|\overline{2}}$. On the top of both figures are nodes for each of the four literals, and on the bottom are nodes for each of the two clauses. At every time step $t$, we have an embedding for every literal and every clause. An iteration consists of two stages. First, each clause receives messages from its neighboring literals and updates it embedding accordingly (Figure 2a). Next, each literal receives messages from its neighboring clause as well as from its complement, and updates its embedding accordingly (Figure 2b).</p>
<p>More formally, our model is parameterized by two vectors ($L_{\text{init}},C_{\text{init}}$), three multilayer perceptrons ($L_{\text{msg}},C_{\text{msg}},L_{\text{vote}}$) and two layer-norm LSTMs ( [Ba et al., 2016; Hochreiter &amp; Schmidhuber, 1997]) ($L_{u},C_{u}$). At every time step $t$, we have a matrix $L^{(t)}\in\mathbb{R}^{2n\times d}$ whose $i$th row contains the embedding for the literal $\ell_{i}$ and a matrix $C^{(t)}\in\mathbb{R}^{m\times d}$ whose $j$th row contains the embedding for the clause $c_{j}$, which we initialize by tiling $L_{\text{init}}$ and $C_{\text{init}}$ respectively. We also have hidden states $L_{h}^{(t)}\in\mathbb{R}^{2n\times d}$ and $C_{h}^{(t)}\in\mathbb{R}^{m\times d}$ for $L_{u}$ and $C_{u}$ respectively, both initialized to zero matrices. Let $M$ be the (bipartite) adjacency matrix defined by $M(i,j)=\mathbb{1}\left{\ell_{i}\in c_{j}\right}$ and let Flip be the operator that takes a matrix $L$ and swaps each row of $L$ with the row corresponding to the literal’s negation. A single iteration consists of applying the following two updates:</p>
<p>$$
\begin{aligned}
(C^{(t+1)},C_{h}^{(t+1)}) &amp; \leftarrow C_{u}\left(\left[C_{h}^{(t)},M^{\top} L_{\text{msg}}\left(L^{(t)}\right)\right]\right) \
\left(L^{(t+1)},L_{h}^{(t+1)}\right) &amp; \leftarrow L_{u}\left(\left[L_{h}^{(t)}, \operatorname{Flip}\left(L^{(t)}\right), M C_{\text{msg}}\left(C^{(t+1)}\right)\right]\right)
\end{aligned}
$$</p>
<p>After $T$ iterations, we compute $L_{<em>}^{(T)} \leftarrow \mathbf{L}<em>{\text {vote }}\left(L^{(T)}\right) \in \mathbb{R}^{2 n}$, which contains a single scalar for each literal (the literal’s vote), and then we compute the average of the literal votes $y^{(T)} \leftarrow \operatorname{mean}\left(L</em>{</em>}^{(T)}\right) \in$. $\mathbb{R}$. We train the network to minimize the sigmoid cross-entropy loss between the logit $y^{(T)}$ and the true label $\phi(P)$.</p>
<p>Our architecture enforces permutation invariance by operating on nodes and edges according to the topology of the graph without any additional ordering over nodes or edges. Likewise, it enforces negation invariance by treating all literals the same no matter whether they originated as a positive or negative occurrence of a variable.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The sequence of literal votes $L_{<em>}^{(1)}$ to $L_{</em>}^{(24)}$ as NeuroSAT runs on a satisfiable problem from $\mathbf{S R}(20)$. For clarity, we reshape each $L_{*}^{(t)}$ to be an $\mathbb{R}^{n \times 2}$ matrix so that each literal is paired with its complement; specifically, the $i$ th row contains the scalar votes for $x_{i}$ and $\overline{x_{i}}$. Here white represents zero, blue negative and red positive. For several iterations, almost every literal is voting unsat with low confidence (light blue). Then a few scattered literals start voting sat for the next few iterations, but not enough to affect the mean vote. Suddenly there is a phase transition and all the literals (and hence the network as a whole) start to vote sat with very high confidence (dark red). After the phase transition, the vote for each literal converges and the network stops evolving.</p>
<p>We stress that none of the learned parameters depend on the size of the SAT problem and that a single model can be trained and tested on problems of arbitrary and varying sizes. At both train and test time, the input to the model is simply any bipartite adjacency matrix $M$ over any number of literals and clauses. The learned parameters only determine how each individual literal and clause behaves in terms of its neighbors in the graph. Variation in problem size is handled by the aggregation operators: we sum the outgoing messages of each of a node's neighbors to form the incoming message, and we take the mean of the literal votes at the end of message passing to form the logit $y^{(T)}$.</p>
<h1>4 Training data</h1>
<p>We want our neural network to be able to classify (and ultimately solve) SAT problems from a variety of domains that it never trained on. One can easily construct distributions over SAT problems for which it would be possible to predict satisfiability with perfect accuracy based only on crude statistics; however, a neural network trained on such a distribution would be unlikely to generalize to problems from other domains. To force our network to learn something substantive, we create a distribution $\mathbf{S R}(n)$ over pairs of random SAT problems on $n$ variables with the following property: one element of the pair is satisfiable, the other is unsatisfiable, and the two differ by negating only a single literal occurrence in a single clause. To generate a random clause on $n$ variables, $\mathbf{S R}(n)$ first samples a small integer $k$ (with mean a little over 4) ${ }^{2}$ then samples $k$ variables uniformly at random without replacement, and finally negates each one with independent probability $50 \%$. It continues to generate clauses $c_{i}$ in this fashion, adding them to the SAT problem, and then querying a traditional SAT solver (we used Minisat <em>Sorensson &amp; Een (2005)</em>), until adding the clause $c_{m}$ finally makes the problem unsatisfiable. Since $\left{c_{1}, \ldots, c_{m-1}\right}$ had a satisfying assignment, negating a single literal in $c_{m}$ must yield a satisfiable problem $\left{c_{1}, \ldots, c_{m-1}, c_{m}^{\prime}\right}$. The pair $\left(\left{c_{1}, \ldots, c_{m-1}, c_{m}\right},\left{c_{1}, \ldots, c_{m-1}, c_{m}^{\prime}\right}\right)$ are a sample from $\mathbf{S R}(n)$.</p>
<h2>5 Predicting Satisfiability</h2>
<p>Although our ultimate goal is to solve SAT problems arising from a variety of domains, we begin by training NeuroSAT as a classifier to predict satisfiability on $\mathbf{S R}(40)$. Problems in $\mathbf{S R}(40)$ are small enough to be solved efficiently by modern SAT solvers-a fact we rely on to generate the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>problems—but the classification problem is highly non-trivial from a machine learning perspective. Each problem has 40 variables and almost 200 clauses on average, and the positive and negative examples differ by negating only a single literal occurrence out of a thousand. We were unable to train an LSTM on a many-hot encoding of clauses (specialized to problems with 40 variables) to predict with $&gt;50\%$ accuracy on its training set. Even the canonical SAT solver MiniSAT (Sorensson \&amp; Een, 2005) needs to backjump ${ }^{3}$ almost ten times on average, and needs to perform over a hundred primitive logical inferences (i.e. unit propagations) to solve each problem.</p>
<p>We instantiated the NeuroSAT architecture described in $\S 3$ with $d=128$ dimensions for the literal embeddings, the clause embeddings, and all the hidden units; 3 hidden layers and a linear output layer for each of the MLPs $\mathbf{L}<em _msg="{msg" _text="\text">{\text {msg }}$, $\mathbf{C}</em>}}$, and $\mathbf{L<em 2="2">{\text {vote }}$; and rectified linear units for all non-linearities. We regularized by the $\ell</em>(40)$. We trained on millions of problems.}$ norm of the parameters scaled by $10^{-10}$, and performed $T=26$ iterations of message passing on every problem. We trained our model using the ADAM optimizer (Kingma $\&amp; \mathrm{Ba}, 2014$ ) with a learning rate of $2 \times 10^{-5}$, clipping the gradients by global norm with clipping ratio 0.65 (Pascanu et al., 2012). We batched multiple problems together, with each batch containing up to 12,000 nodes (i.e. literals plus clauses). To accelerate the learning, we sampled the number of variables $n$ uniformly from between 10 and 40 during training (i.e. we trained on $\mathbf{S R}(\mathbf{U}(10,40))$ ), though we only evaluate on $\mathbf{S R</p>
<p>After training, NeuroSAT is able to classify the test set correctly with 85\% accuracy. In the next section, we examine how NeuroSAT manages to do so and show how we can decode solutions to satisfiable problems from its activations. Note: for the entire rest of the paper, NeuroSAT refers to the specific trained model that has only been trained on $\mathbf{S R}(\mathbf{U}(10,40))$.</p>
<h1>6 DECODING SATISFYING ASSIGNMENTS</h1>
<p>Let us try to understand what NeuroSAT (trained on $\mathbf{S R}(\mathbf{U}(10,40))$ ) is computing as it runs on new problems at test time. For a given run, we can compute and visualize the $2 n$-dimensional vector of literal votes $L_{<em>}^{(t)} \leftarrow \mathbf{L}<em>{\text {vote }}\left(L^{(t)}\right)$ at every iteration $t$. Figure 3 illustrates the sequence of literal votes $L</em>{</em>}^{(1)}$ to $L_{<em>}^{(24)}$ as NeuroSAT runs on a satisfiable problem from $\mathbf{S R}(20)$. For clarity, we reshape each $L_{</em>}^{(t)}$ to be an $\mathbb{R}^{n \times 2}$ matrix so that each literal is paired with its complement; specifically, the $i$ th row contains the scalar votes for $x_{i}$ and $\overline{x_{i}}$. Here white represents zero, blue negative and red positive. For several iterations, almost every literal is voting unsat with low confidence (light blue). Then a few scattered literals start voting sat for the next few iterations, but not enough to affect the mean vote. Suddenly, there is a phase transition and all the literals (and hence the network as a whole) start to vote sat with very high confidence (dark red). After the phase transition, the vote for each literal converges and the network stops evolving.</p>
<p>NeuroSAT seems to exhibit qualitatively similar behavior on every satisfiable problem that it predicts correctly. The problems for which NeuroSAT guesses unsat are similar except without the phase change: it continues to guess unsat with low-confidence for as many iterations as NeuroSAT runs for. NeuroSAT never becomes highly confident that a problem is unsat, and it almost never guesses sat on an unsat problem. These results suggest that NeuroSAT searches for a certificate of satisfiability, and that it only guesses sat once it has found one.</p>
<p>Let us look more carefully at the literal votes $L_{<em>}^{(24)}$ from Figure 3 after convergence. Note that most of the variables have one literal vote distinctly darker than the other. Moreover, the dark votes are all approximately equal to each other, and the light votes are all approximately equal to each other as well. Thus the votes seem to encode one bit for each variable. It turns out that these bits encode a satisfying assignment in this case, but they do not do so reliably in general. Recall from $\S 3$ that NeuroSAT projects the higher dimensional literal embeddings $L^{(T)} \in \mathbb{R}^{2 n \times d}$ to the literal votes $L_{</em>}^{(T)}$ using the MLP $\mathbf{L}_{\text {vote }}$. Figure 4 illustrates the two-dimensional PCA embeddings for $L^{(12)}$ to $L^{(26)}$ (skipping every other time step) as NeuroSAT runs on a satisfiable problem from $\mathbf{S R}(40)$. Blue and red dots indicate literals that are set to 0 and 1 in the satisfying assignment that it eventually finds, respectively. The blue and red dots cannot be linearly separated until the phase transition at the end, at which point they form two distinct clusters according to the satisfying assignment. We</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: right;">Trained on:</th>
<th style="text-align: left;">$\mathbf{S R}(\mathbf{U}(10,40))$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Trained with:</td>
<td style="text-align: left;">26 iterations</td>
</tr>
<tr>
<td style="text-align: right;">Tested on:</td>
<td style="text-align: left;">$\mathbf{S R}(40)$</td>
</tr>
<tr>
<td style="text-align: right;">Tested with:</td>
<td style="text-align: left;">26 iterations</td>
</tr>
<tr>
<td style="text-align: right;">Overall test accuracy:</td>
<td style="text-align: left;">$85 \%$</td>
</tr>
<tr>
<td style="text-align: right;">Accuracy on unsat problems:</td>
<td style="text-align: left;">$96 \%$</td>
</tr>
<tr>
<td style="text-align: right;">Accuracy on sat problems:</td>
<td style="text-align: left;">$73 \%$</td>
</tr>
<tr>
<td style="text-align: right;">Percent of sat problems solved:</td>
<td style="text-align: left;">$\mathbf{7 0 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 1: NeuroSAT's performance at test time on $\mathbf{S R}(40)$ after training on $\mathbf{S R}(\mathbf{U}(10,40))$. It almost never guesses sat on unsatisfiable problems. On satisfiable problems, it correctly guesses sat $73 \%$ of the time, and we can decode a satisfying assignment for $70 \%$ of the satisfiable problems by clustering the literal embeddings $L^{(T)}$ as described in $\S 6$.
observe a similar clustering almost every time the network guesses sat. Thus the literal votes $L_{*}^{(T)}$ only ever encode the satisfying assignment by chance, when the projection $\mathbf{L}_{\text {vote }}$ happens to preserve this clustering.</p>
<p>Our analysis suggests a more reliable way to decode solutions from NeuroSAT's internal activations: 2-cluster $L^{(T)}$ to get cluster centers $\Delta_{1}$ and $\Delta_{2}$, partition the variables according to the predicate $\left|x_{i}-\Delta_{1}\right|^{2}+\left|\overline{x_{i}}-\Delta_{2}\right|^{2}&lt;\left|x_{i}-\Delta_{2}\right|^{2}+\left|\overline{x_{i}}-\Delta_{1}\right|^{2}$, and then try both candidate assignments that result from mapping the partitions to truth values. This decoding procedure (using $k$-means to find the two cluster centers) successfully decodes a satisfying assignment for over $70 \%$ of the satisfiable problems in the $\mathbf{S R}(40)$ test set. Table 1 summarizes the results when training on $\mathbf{S R}(\mathbf{U}(10,40))$ and testing on $\mathbf{S R}(40)$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: PCA projections for the high-dimensional literal embeddings $L^{(12)}$ to $L^{(26)}$ (skipping every other time step) as NeuroSAT runs on a satisfiable problem from $\mathbf{S R}(40)$. Blue and red dots indicate literals that are set to 0 and 1 in the satisfying assignment that it eventually finds, respectively. We see that the blue and red dots are mixed up and cannot be linearly separated until the phase transition at the end, at which point they form two distinct clusters according to the satisfying assignment.</p>
<p>Recall that at training time, NeuroSAT is only given a single bit of supervision for each SAT problem. Moreover, the positive and negative examples in the dataset differ only by the placement of a single edge. NeuroSAT has learned to search for satisfying assignments solely to explain that single bit of supervision.</p>
<h1>7 EXTRAPOLATING TO OTHER PROBLEM DISTRIBUTIONS</h1>
<h3>7.1 BIGGER PROBLEMS</h3>
<p>Even though we only train NeuroSAT on $\mathbf{S R}(\mathbf{U}(10,40))$, it is able to solve SAT problems sampled from $\mathbf{S R}(n)$ for $n$ much larger than 40 by simply running for more iterations of message passing. Figure 5 shows NeuroSAT's success rate on $\mathbf{S R}(n)$ for a range of $n$ as a function of the number of iterations $T$. For $n=200$, there are $2^{160}$ times more possible assignments to the variables than any problem it saw during training, and yet it can solve $25 \%$ of the satisfiable problems in $\mathbf{S R}(200)$ by running for four times more iterations than it performed during training. On the other hand, when restricted to the number of iterations it was trained with, it solves under $10 \%$ of them. Thus we see that its ability to solve bigger and harder problems depends on the fact that the dynamical system it has learned encodes generic procedural knowledge that can operate effectively over a wide range of time frames.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: NeuroSAT's success rate on $\mathbf{S R}(n)$ for a range of $n$ as a function of the number of iterations $T$. Even though we only train NeuroSAT on $\mathbf{S R}(40)$ and below, it is able to solve SAT problems sampled from $\mathbf{S R}(n)$ for $n$ much larger than 40 by simply running for more iterations.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Example graph from the Forest-Fire distribution. The graph has a coloring for $k \geq 5$, a clique for $k \leq 3$, a dominating set for $k \geq 3$, and a vertex cover for $k \geq 6$. However, these properties are not perceptually obvious and require deliberate computation to determine.</p>
<h1>7.2 DIFFERENT PROBLEMS</h1>
<p>Every problem in NP can be reduced to SAT in polynomial time, and SAT problems arising from different domains may have radically different structural and statistical properties. Even though NeuroSAT has learned to search for satisfying assignments on problems from $\mathbf{S R}(n)$, we may still find that the dynamical system it has learned only works properly on problems similar to those it was trained on.</p>
<p>To assess NeuroSAT's ability to extrapolate to different classes of problems, we generated problems in several other domains and then encoded them all into SAT problems (using standard encodings). In particular, we started by generating one hundred graphs from each of six different random graph distributions (Barabasi, Erdös-Renyi, Forest-Fire, Random- $k$-Regular, Random-Static-Power-Law, and Random-Geometric). ${ }^{4}$ We found parameters for the random graph generators such that each graph has ten nodes and seventeen edges on average. For each graph in each collection, we generated graph coloring problems ( $3 \leq k \leq 5$ ), dominating-set problems ( $2 \leq k \leq 4$ )), clique-detection problems ( $3 \leq k \leq 5$ ), and vertex cover problems ( $4 \leq k \leq 6$ ). ${ }^{5}$ We chose the range of $k$ for each problem to include the threshold for most of the graphs while avoiding trivial problems such as 2-clique. As before, we used Minisat Sorensson \&amp; Een (2005) to determine satisfiability. Figure 6 shows an example graph from the distribution. Note that the trained network does not know anything a priori about these tasks; the generated SAT problems need to encode not only the graphs themselves but also formal descriptions of the tasks to be solved.</p>
<p>Out of the 7,200 generated problems, we kept only the 4,888 satisfiable problems. On average these problems contained over two and a half times as many clauses as the problems in $\mathbf{S R}(40)$. We ran NeuroSAT for 512 iterations on each of them and found that we could successfully decode solutions for $85 \%$ of them. In contrast, Survey Propagation (SP) (Braunstein et al., 2005), the canonical (learning-free) message passing algorithm for satisfiability, does not on its own converge to a satisfying assignment on any of these problems. ${ }^{6}$ This suggests that NeuroSAT has not simply found a way to approximate SP, but rather has synthesized a qualitatively different algorithm.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>8 Finding UnSat Cores</h1>
<p>NeuroSAT (trained on $\mathbf{S R}(\mathbf{U}(10,40))$ ) can find satisfying assignments but is not helpful in constructing proofs of unsatisfiability. When it runs on an unsatisfiable problem, it keeps searching for a satisfying assignment indefinitely and non-systematically. However, when we train the same architecture on a dataset in which each unsatisfiable problem has a small subset of clauses that are already unsatisfiable (called an unsat core), it learns to detect these unsat cores instead of searching for satisfying assignments. The literals involved in the unsat core can be decoded from its internal activations. When the number of literals involved in the unsat core is small relative to the total number of literals, knowing the literals involved in the unsat core can enable constructing a resolution proof more efficiently.</p>
<p>We generated a new distribution $\mathbf{S R C}(n, u)$ that is similar to $\mathbf{S R}(n)$ except that every unsatisfiable problem contains a small unsat core. Here $n$ is the number of variables as before, and $u$ is an unsat core over $x_{1}, \ldots, x_{k}(k&lt;n)$ that can be made into a satisfiable set of clauses $u^{\prime}$ by negating a single literal. We sample a pair from $\mathbf{S R C}(n, u)$ as follows. First, we initialize a problem with $u^{\prime}$, and then we sample clauses (over $x_{1}$ to $x_{n}$ ) just as we did for $\mathbf{S R}(n)$ until the problem becomes unsatisfiable. We can now negate a literal in the final clause to get a satisfiable problem $p_{s}$, and then we can swap $u^{\prime}$ for $u$ in $p_{s}$ to get $p_{u}$, which is unsatisfiable since it contains the unsat core $u$. We created train and test datasets from $\mathbf{S R C}(40, u)$ with $u$ sampled at random for each problem from a collection of three unsat cores ranging from three clauses to nine clauses: the unsat core $R$ from Knuth (2015), and the two unsat cores resulting from encoding the pigeonhole principles $\mathbf{P P}(2,1)$ and $\mathbf{P P}(3,2) .{ }^{7}$ We trained our architecture on this dataset, and we refer to the trained model as NeuroUNSAT.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The sequence of literal votes $L_{*}^{(t)}$ as NeuroUNSAT runs on a pair of problems from $\mathbf{S R C}(30, \mathbf{P P}(3,2))$. In both cases, the literals in the first six rows are involved in the unsat core. In 7a, NeuroUNSAT inspects the modified core $u^{\prime}$ of the satisfiable problem but concludes that it does not match the pattern. In 7b, NeuroUNSAT finds the unsat core $u$ and votes unsat with high confidence (dark blue).</p>
<p>NeuroUNSAT is able to predict satisfiability on the test set with 100\% accuracy. Upon inspection, it seems to do so by learning to recognize the unsat cores. Figure 7 shows NeuroUNSAT running on a pair of problems from $\mathbf{S R C}(30, \mathbf{P P}(3,2))$. In both cases, the literals in the first six rows are involved in the unsat core. In Figure 7a, NeuroUNSAT inspects the modified core $u^{\prime}$ of the satisfiable problem but concludes that it does not match the pattern exactly. In Figure 7b, NeuroUNSAT finds the unsat core $u$ and votes unsat with high confidence (dark blue). As in $\S 6$, the literals involved in the unsat core can sometimes be decoded from the literal votes $L_{*}^{(T)}$, but it is more reliable to 2 cluster the higher-dimensional literal embeddings $L^{(T)}$. On the test set, the small number of literals involved in the unsat core end up in their own cluster $98 \%$ of the time.</p>
<p>Note that we do not expect NeuroUNSAT to generalize to arbitary unsat cores: as far as we know it is simply memorizing a collection of specific subgraphs, and there is no evidence it has learned a generic procedure to prove unsat.</p>
<h2>9 RELATED WORK</h2>
<p>There have been many attempts over the years to apply statistical learning to various aspects of the SAT problem: restart strategies (Haim \&amp; Walsh, 2009), branching heuristics (Liang et al., 2016;</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Grozea \&amp; Popescu, 2014; Flint \&amp; Blaschko, 2012), parameter tuning (Singh et al., 2009), and solver selection (Xu et al., 2008). None of these approaches use neural networks, and instead make use of both generic graph features and features extracted from the runs of SAT solvers. Moreover, these approaches are designed to assist existing solvers and do not aim to solve SAT problems on their own.</p>
<p>From the machine learning perspective, the closest work to ours is Palm et al. (2017), which showed that an MPNN can be trained to predict the unique solutions of Sudoku puzzles. We believe that their network's success is an instance of the phenomenon we study in this paper, namely that MPNNs can synthesize local search algorithms for constraint satisfaction problems. Evans et al. (2018) present a neural network architecture that can learn to predict whether one propositional formula entails another by randomly sampling and evaluating candidate assignments. Unlike NeuroSAT, their network does not perform heuristic search and can only work on simple problems for which random guessing is tractable. There have also been several recent papers showing that various neural network architectures can learn good heuristics for NP-hard combinatorial optimization problems (Vinyals et al., 2015; Bello et al., 2016; Dai et al., 2017); however, finding low-cost solutions to optimization problems requires less precise reasoning than finding satisfying assignments.</p>
<h1>10 DISCUSSION</h1>
<p>Our main motivation has been scientific: to better understand the extent to which neural networks are capable of precise, logical reasoning. Our work has definitively established that neural networks can learn to perform discrete search on their own without the help of hard-coded search procedures, even after only end-to-end training with minimal supervision. We found this result surprising and think it constitutes an important contribution to the community's evolving understanding of the capabilities and limitations of neural networks.</p>
<p>Although not our primary concern, we also hope that our findings eventually lead to improvements in practical SAT solving. As we stressed early on, as an end-to-end SAT solver the trained NeuroSAT system discussed in this paper is still vastly less reliable than the state-of-the-art. We concede that we see no obvious path to beating existing SAT solvers. One approach might be to continue to train NeuroSAT as an end-to-end solver on increasingly difficult problems. A second approach might be to use a system like NeuroSAT to help guide decisions within a more traditional SAT solver, though it is not clear that NeuroSAT provides any useful information before it finds a satisfying assignment. However, as we discussed in $\S 8$, when we trained our architecture on different data it learned an entirely different procedure. In a separate experiment omitted for space reasons, we also trained our architecture to predict whether there is a satisfying assignment involving each individual literal in the problem and found that it was able to predict these bits with high accuracy as well. Unlike NeuroSAT, it made both type I and type II errors, had no discernable phase transition, and could make reasonable predictions within only a few rounds. We believe that architectures descended from NeuroSAT will be able to learn very different mechanisms and heuristics depending on the data they are trained on and the details of their objective functions. We are cautiously optimistic that a descendant of NeuroSAT will one day lead to improvements to the state-of-the-art.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We thank Steve Mussmann, Alexander Ratner, Nathaniel Thomas, Vatsal Sharan and Cristina White for providing valuable feedback on early drafts. We also thank William Hamilton, Geoffrey Irving and Arun Chaganty for helpful discussions. This work was supported by Future of Life Institute grant 2017-158712.</p>
<h2>REFERENCES</h2>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.</p>
<p>Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.</p>
<p>Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh. Conflict-driven clause learning sat solvers. Handbook of Satisfiability, Frontiers in Artificial Intelligence and Applications, pp. $131-153,2009$.</p>
<p>Alfredo Braunstein, Marc Mézard, and Riccardo Zecchina. Survey propagation: An algorithm for satisfiability. Random Structures \&amp; Algorithms, 27(2):201-226, 2005.</p>
<p>Stephen A Cook. The complexity of theorem-proving procedures. In Proceedings of the third annual ACM symposium on Theory of computing, pp. 151-158. ACM, 1971.</p>
<p>Hanjun Dai, Elias B Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. arXiv preprint arXiv:1704.01665, 2017.</p>
<p>Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? arXiv preprint arXiv:1802.08535, 2018.</p>
<p>Alex Flint and Matthew Blaschko. Perceptron learning of sat. In Advances in Neural Information Processing Systems, pp. 2771-2779, 2012.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.</p>
<p>Carla P Gomes, Henry Kautz, Ashish Sabharwal, and Bart Selman. Satisfiability solvers. Foundations of Artificial Intelligence, 3:89-134, 2008.</p>
<p>Cristian Grozea and Marius Popescu. Can machine learning learn a decision oracle for np problems? a test on sat. Fundamenta Informaticae, 131(3-4):441-450, 2014.</p>
<p>Shai Haim and Toby Walsh. Restart strategy selection using machine learning techniques. CoRR, abs/0907.5032, 2009.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997.</p>
<p>Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Donald E Knuth. The art of computer programming, volume 4, fascicle 6: Satisfiability, 2015.
Harry R Lewis. Computers and intractability. a guide to the theory of np-completeness, 1983.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.</p>
<p>Jia Hui Liang, Vijay Ganesh, Pascal Poupart, and Krzysztof Czarnecki. Learning rate based branching heuristic for sat solvers. In International Conference on Theory and Applications of Satisfiability Testing, pp. 123-140. Springer, 2016.</p>
<p>Mark Newman. Networks: an introduction. Oxford university press, 2010.
Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks for complex relational reasoning. arXiv preprint arXiv:1711.08028, 2017.</p>
<p>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. CoRR, abs/1211.5063, 2012.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.</p>
<p>Rishabh Singh, Joseph P Near, Vijay Ganesh, and Martin Rinard. Avatarsat: An auto-tuning boolean sat solver. 2009.</p>
<p>Niklas Sorensson and Niklas Een. Minisat v1. 13-a sat solver with conflict-clause minimization. SAT, 2005(53):1-2, 2005.</p>
<p>G Tseitin. On the complexity ofderivation in propositional calculus. Studies in Constrained Mathematics and Mathematical Logic, 1968.
O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In Advances in Neural Information Processing Systems (NIPS), pp. 2674-2682, 2015.
L. Xu, F. Hutter, H. H. Hoos, and K. Leyton-Brown. SATzilla: portfolio-based algorithm selection for SAT. Journal of Artificial Intelligence Research (JAIR), 32:565-606, 2008.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ The pigeonhole principle and the standard SAT encoding are described in Knuth (2015).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
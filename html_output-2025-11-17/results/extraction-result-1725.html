<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1725 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1725</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1725</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-272146327</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.16498v2.pdf" target="_blank">A Survey on Evaluating Large Language Models in Code Generation Tasks</a></p>
                <p><strong>Paper Abstract:</strong> This paper provides a comprehensive review of the current methods and metrics used to evaluate the performance of Large Language Models (LLMs) in code generation tasks. With the rapid growth in demand for automated software development, LLMs have demonstrated significant potential in the field of code generation. The paper begins by reviewing the historical development of LLMs and their applications in code generation. Next, it details various methods and metrics for assessing the code generation capabilities of LLMs, including code correctness, efficiency, readability, and evaluation methods based on expert review and user experience. The paper also evaluates the widely used benchmark datasets, identifying their limitations and proposing directions for future improvements. Specifically, the paper analyzes the performance of code generation models across different tasks by combining multiple evaluation metrics, such as code compilation/interpretation success rates, unit test pass rates, and performance and efficiency metrics, to comprehensively assess the practical application of LLMs in code generation. Finally, the paper discusses the challenges faced in evaluating LLMs in code generation, particularly how to ensure the comprehensiveness and accuracy of evaluation methods and how to adapt to the evolving practices of software development. These analyses and discussions provide valuable insights for further optimizing and improving the application of LLMs in code generation tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1725.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1725.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated code similarity metric that combines weighted n-gram matching, AST-based syntactic matching, and data-flow semantic matching to better capture syntactic and semantic correspondence between generated and reference code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codebleu: a method for automatic evaluation of code synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metric (CodeBLEU)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>text-to-code / multiple programming languages</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>syntactic and semantic similarity, code correctness</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>programmer scoring / programmer judgments (as reported in the CodeBLEU study compared against automated metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>programmers (not numerically specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Pearson correlation reported higher for CodeBLEU vs. programmer scoring than for BLEU and accuracy metrics (numeric value not reported in this survey)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>metrics that incorporate syntactic (AST) and semantic (data-flow) information tend to align better with programmer scoring</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>CodeBLEU showed higher Pearson correlation with programmer scoring than BLEU and simple accuracy metrics, indicating better alignment with human judgments (no numeric comparison provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code-aware automated metrics that include AST and data-flow (CodeBLEU) correlate better with programmer judgments than surface-level metrics such as BLEU or raw exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Survey reports the qualitative higher correlation but does not provide the numeric correlation coefficient here; reliance on AST/data-flow extraction may limit applicability across all languages or code styles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluating Large Language Models in Code Generation Tasks', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1725.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1725.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RealHumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RealHumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-subject evaluation framework that measures how well LLMs support programmers on real programming tasks by collecting Likert-style user ratings and automated correctness metrics (Pass@k) under different model-support conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The realhumaneval: Evaluating large language models' abilities to support programmers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Likert-style user ratings combined with automated metrics (Pass@k)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code / code suggestions used during programming tasks</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>usability/helpfulness (user rating), correctness (Pass@k), task completion time, code quality/readability/maintainability</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>213 experienced programmers randomly assigned to one of seven conditions (control, three auto-completion models, three chat models); participants used model support during tasks and provided ratings (1-5).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>213</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>experienced programmers (professional/experienced; exact years not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Clear instructions and tasks where models produced functional code (e.g., data-processing tasks) yielded higher user ratings and better Pass@k outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Tasks requiring complex logical reasoning and subtle error correction showed weaker model helpfulness and lower effective support.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Model helpfulness and effectiveness decreased on more complex logical-reasoning tasks; simpler tasks saw stronger alignment between automated metrics and user usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clearer instructions improved model outputs and user-perceived helpfulness (improved alignment/effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>213 human participants; tasks drawn from HumanEval (dataset of 164 problems) though exact task-sampling per participant specified in original study.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>The study records both human Likert ratings and automated Pass@k metrics, but the survey does not report a numeric agreement statistic between these proxies and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM support (notably GPT-3–class models in the reported summary) produced higher user ratings and improved Pass@k in many tasks; usefulness varied by task type and clarity of instructions, with poorer performance on complex reasoning and subtle logical/boundary errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>The survey summarizes results but does not provide explicit agreement coefficients comparing Likert ratings to automated metrics; outcomes are task-dependent and sensitive to prompt clarity and error type.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluating Large Language Models in Code Generation Tasks', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1725.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1725.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge (surveyed mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge evaluation (e.g., MT-Bench / Chatbot Arena studies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The use of LLMs themselves as automated evaluators of model outputs (LLM-as-judge) has been increasingly adopted; the survey notes promising performance but reports limited quantitative alignment details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging llm-as-a-judge with mt-bench and chatbot arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (automated evaluation using LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>model outputs / generated text or code snippets</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-purpose (applied to code and conversational outputs in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>functionality, clarity, maintainability, overall quality (as used in MT-Bench–style comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Survey-level claim: LLM-evaluators perform well on clearer, more objective criteria and in multi-round comparative setups as studied by MT-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Subjective, nuanced, or highly domain-specific criteria are likely more challenging for LLM judges; the survey flags ongoing work to quantify limits.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Implicitly, agreement/accuracy of LLM judges likely degrades on more complex or subjective artifacts, though the survey does not provide quantitative evidence here.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear, well-specified evaluation criteria improve LLM judge behaviour (survey implication).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>The survey references dedicated studies (e.g., MT-Bench and the 'Judging llm-as-a-judge' paper) that investigate LLM-as-judge behavior but does not itself report numeric comparisons between LLM-judges and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs are increasingly used as evaluators and have shown promising results in prior work, but this survey emphasizes that standardized prompting, calibration, and quantitative alignment with humans remain active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lack of standardized prompts and calibration for LLM judges; few standardized numeric agreement statistics reported in this survey; potential for bias and lack of nuance on subjective aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluating Large Language Models in Code Generation Tasks', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1725.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1725.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlignJudge (survey description)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlignJudge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alignment-focused evaluation method that uses a subset of HumanEval and a set of intentionally subtly-buggy solutions to test whether models can generate correct code and fix errors; emphasizes pass-rate-based alignment assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated correctness metrics (unit-test pass rate) on crafted alignment tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python (HumanEval tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pass@k (unit-test pass rate), alignment (ability to correct buggy solutions), generation time, code length, code complexity</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Researchers created 30 subtle-error problems (in addition to HumanEval) to probe model alignment; evaluation primarily via unit tests and automated pass/fail criteria rather than human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>researchers crafted subtle errors (expert-design of test cases), but no panel of human raters described for AlignJudge in the survey summary</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Clear instructions and simpler error types (e.g., typos) led to better model alignment and higher pass rates.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Subtle logical errors and boundary-condition bugs reduced model alignment and success at correction.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Models struggled more with logical and boundary errors, indicating reduced alignment on more complex or semantically subtle artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clarity of instructions improved model performance and alignment behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Used all 164 HumanEval problems plus 30 additional crafted subtle-error problems (as described in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When probed with subtle incorrect solutions, models often failed to correct logical/boundary errors; they handled simple typos better and benefitted from clearer instructions — indicating alignment is sensitive to error type and prompt clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>This evaluation relies on unit tests and crafted buggy inputs; the survey summary does not report numeric agreement between these automated pass-rate proxies and independent human expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluating Large Language Models in Code Generation Tasks', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Codebleu: a method for automatic evaluation of code synthesis <em>(Rating: 2)</em></li>
                <li>The realhumaneval: Evaluating large language models' abilities to support programmers. <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Aligning offline metrics and human judgments of value for code generation models <em>(Rating: 2)</em></li>
                <li>MT-Bench <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1725",
    "paper_id": "paper-272146327",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "CodeBLEU",
            "name_full": "CodeBLEU",
            "brief_description": "An automated code similarity metric that combines weighted n-gram matching, AST-based syntactic matching, and data-flow semantic matching to better capture syntactic and semantic correspondence between generated and reference code.",
            "citation_title": "Codebleu: a method for automatic evaluation of code synthesis",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "automated metric (CodeBLEU)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code",
            "artifact_domain": "text-to-code / multiple programming languages",
            "evaluation_criteria": "syntactic and semantic similarity, code correctness",
            "human_evaluation_setup": "programmer scoring / programmer judgments (as reported in the CodeBLEU study compared against automated metrics)",
            "human_expert_count": null,
            "human_expert_expertise": "programmers (not numerically specified)",
            "agreement_metric": "Pearson correlation",
            "agreement_score": "Pearson correlation reported higher for CodeBLEU vs. programmer scoring than for BLEU and accuracy metrics (numeric value not reported in this survey)",
            "high_agreement_conditions": "metrics that incorporate syntactic (AST) and semantic (data-flow) information tend to align better with programmer scoring",
            "low_agreement_conditions": null,
            "artifact_complexity_effect": null,
            "criteria_clarity_effect": null,
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "CodeBLEU showed higher Pearson correlation with programmer scoring than BLEU and simple accuracy metrics, indicating better alignment with human judgments (no numeric comparison provided in text).",
            "calibration_or_training": null,
            "key_findings": "Code-aware automated metrics that include AST and data-flow (CodeBLEU) correlate better with programmer judgments than surface-level metrics such as BLEU or raw exact-match accuracy.",
            "limitations_noted": "Survey reports the qualitative higher correlation but does not provide the numeric correlation coefficient here; reliance on AST/data-flow extraction may limit applicability across all languages or code styles.",
            "uuid": "e1725.0",
            "source_info": {
                "paper_title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "RealHumanEval",
            "name_full": "RealHumanEval",
            "brief_description": "A human-subject evaluation framework that measures how well LLMs support programmers on real programming tasks by collecting Likert-style user ratings and automated correctness metrics (Pass@k) under different model-support conditions.",
            "citation_title": "The realhumaneval: Evaluating large language models' abilities to support programmers.",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "Likert-style user ratings combined with automated metrics (Pass@k)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code / code suggestions used during programming tasks",
            "artifact_domain": "Python",
            "evaluation_criteria": "usability/helpfulness (user rating), correctness (Pass@k), task completion time, code quality/readability/maintainability",
            "human_evaluation_setup": "213 experienced programmers randomly assigned to one of seven conditions (control, three auto-completion models, three chat models); participants used model support during tasks and provided ratings (1-5).",
            "human_expert_count": "213",
            "human_expert_expertise": "experienced programmers (professional/experienced; exact years not specified)",
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Clear instructions and tasks where models produced functional code (e.g., data-processing tasks) yielded higher user ratings and better Pass@k outcomes.",
            "low_agreement_conditions": "Tasks requiring complex logical reasoning and subtle error correction showed weaker model helpfulness and lower effective support.",
            "artifact_complexity_effect": "Model helpfulness and effectiveness decreased on more complex logical-reasoning tasks; simpler tasks saw stronger alignment between automated metrics and user usefulness.",
            "criteria_clarity_effect": "Clearer instructions improved model outputs and user-perceived helpfulness (improved alignment/effectiveness).",
            "sample_size": "213 human participants; tasks drawn from HumanEval (dataset of 164 problems) though exact task-sampling per participant specified in original study.",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "The study records both human Likert ratings and automated Pass@k metrics, but the survey does not report a numeric agreement statistic between these proxies and human judgments.",
            "calibration_or_training": null,
            "key_findings": "LLM support (notably GPT-3–class models in the reported summary) produced higher user ratings and improved Pass@k in many tasks; usefulness varied by task type and clarity of instructions, with poorer performance on complex reasoning and subtle logical/boundary errors.",
            "limitations_noted": "The survey summarizes results but does not provide explicit agreement coefficients comparing Likert ratings to automated metrics; outcomes are task-dependent and sensitive to prompt clarity and error type.",
            "uuid": "e1725.1",
            "source_info": {
                "paper_title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLM-as-a-judge (surveyed mentions)",
            "name_full": "LLM-as-a-judge evaluation (e.g., MT-Bench / Chatbot Arena studies)",
            "brief_description": "The use of LLMs themselves as automated evaluators of model outputs (LLM-as-judge) has been increasingly adopted; the survey notes promising performance but reports limited quantitative alignment details in this paper.",
            "citation_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "LLM-as-a-judge (automated evaluation using LLMs)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "model outputs / generated text or code snippets",
            "artifact_domain": "general-purpose (applied to code and conversational outputs in cited works)",
            "evaluation_criteria": "functionality, clarity, maintainability, overall quality (as used in MT-Bench–style comparisons)",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Survey-level claim: LLM-evaluators perform well on clearer, more objective criteria and in multi-round comparative setups as studied by MT-Bench.",
            "low_agreement_conditions": "Subjective, nuanced, or highly domain-specific criteria are likely more challenging for LLM judges; the survey flags ongoing work to quantify limits.",
            "artifact_complexity_effect": "Implicitly, agreement/accuracy of LLM judges likely degrades on more complex or subjective artifacts, though the survey does not provide quantitative evidence here.",
            "criteria_clarity_effect": "Clear, well-specified evaluation criteria improve LLM judge behaviour (survey implication).",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "The survey references dedicated studies (e.g., MT-Bench and the 'Judging llm-as-a-judge' paper) that investigate LLM-as-judge behavior but does not itself report numeric comparisons between LLM-judges and human experts.",
            "calibration_or_training": null,
            "key_findings": "LLMs are increasingly used as evaluators and have shown promising results in prior work, but this survey emphasizes that standardized prompting, calibration, and quantitative alignment with humans remain active research areas.",
            "limitations_noted": "Lack of standardized prompts and calibration for LLM judges; few standardized numeric agreement statistics reported in this survey; potential for bias and lack of nuance on subjective aspects.",
            "uuid": "e1725.2",
            "source_info": {
                "paper_title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AlignJudge (survey description)",
            "name_full": "AlignJudge",
            "brief_description": "An alignment-focused evaluation method that uses a subset of HumanEval and a set of intentionally subtly-buggy solutions to test whether models can generate correct code and fix errors; emphasizes pass-rate-based alignment assessment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "automated correctness metrics (unit-test pass rate) on crafted alignment tasks",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code",
            "artifact_domain": "Python (HumanEval tasks)",
            "evaluation_criteria": "Pass@k (unit-test pass rate), alignment (ability to correct buggy solutions), generation time, code length, code complexity",
            "human_evaluation_setup": "Researchers created 30 subtle-error problems (in addition to HumanEval) to probe model alignment; evaluation primarily via unit tests and automated pass/fail criteria rather than human scoring.",
            "human_expert_count": null,
            "human_expert_expertise": "researchers crafted subtle errors (expert-design of test cases), but no panel of human raters described for AlignJudge in the survey summary",
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Clear instructions and simpler error types (e.g., typos) led to better model alignment and higher pass rates.",
            "low_agreement_conditions": "Subtle logical errors and boundary-condition bugs reduced model alignment and success at correction.",
            "artifact_complexity_effect": "Models struggled more with logical and boundary errors, indicating reduced alignment on more complex or semantically subtle artifacts.",
            "criteria_clarity_effect": "Clarity of instructions improved model performance and alignment behavior.",
            "sample_size": "Used all 164 HumanEval problems plus 30 additional crafted subtle-error problems (as described in the survey).",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": null,
            "calibration_or_training": null,
            "key_findings": "When probed with subtle incorrect solutions, models often failed to correct logical/boundary errors; they handled simple typos better and benefitted from clearer instructions — indicating alignment is sensitive to error type and prompt clarity.",
            "limitations_noted": "This evaluation relies on unit tests and crafted buggy inputs; the survey summary does not report numeric agreement between these automated pass-rate proxies and independent human expert judgments.",
            "uuid": "e1725.3",
            "source_info": {
                "paper_title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Codebleu: a method for automatic evaluation of code synthesis",
            "rating": 2,
            "sanitized_title": "codebleu_a_method_for_automatic_evaluation_of_code_synthesis"
        },
        {
            "paper_title": "The realhumaneval: Evaluating large language models' abilities to support programmers.",
            "rating": 2,
            "sanitized_title": "the_realhumaneval_evaluating_large_language_models_abilities_to_support_programmers"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Aligning offline metrics and human judgments of value for code generation models",
            "rating": 2,
            "sanitized_title": "aligning_offline_metrics_and_human_judgments_of_value_for_code_generation_models"
        },
        {
            "paper_title": "MT-Bench",
            "rating": 1
        }
    ],
    "cost": 0.01895025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Evaluating Large Language Models in Code Generation Tasks
4 Mar 2025</p>
<p>Liguo Chen 
Peking University
BeijingChina</p>
<p>Qi Guo 
Hongrui Jia 
Peking University
BeijingChina</p>
<p>Zhengran Zeng 
Peking University
BeijingChina</p>
<p>Xin Wang 
Peking University
BeijingChina</p>
<p>Yijiang Xu 
Peking University
BeijingChina</p>
<p>Jian Wu 
Peking University
BeijingChina</p>
<p>Tokyo Institute of Technology
TokyoJapan</p>
<p>Yidong Wang 
Qing Gao 
Peking University
BeijingChina</p>
<p>Jindong Wang 
Peking University
BeijingChina</p>
<p>Microsoft Research Asia
BeijingChina</p>
<p>Wei Ye 
Peking University
BeijingChina</p>
<p>Shikun Zhang 
Peking University
BeijingChina</p>
<p>J Comput 
Technol Sci 
Metrics 
Other Metrics 
A Survey on Evaluating Large Language Models in Code Generation Tasks
4 Mar 2025AAFCE3CF927D726AB4F625F57E27AB4FarXiv:2408.16498v2[cs.SE]Large Language ModelsCode GenerationEvaluation MethodsEvaluation Metrics BLEU [33]Exact Match Accuracy [27]Text-Match Ratio [46] Code-Based Code-BLEU [36]Component Matching [50]Syntax Match [14]Dataflow Matchh [14]EDIT-SIM [11] Execution Based Metrics Testing-Based Pass@k [9]n@k [24]Count@n [51]synthesis success rate [25]Simulation Pass Rate [25]Error Catching [16]Execution Accuracy [50]Compilation Accuracy [14] Statistical Analysis Based Time to Solve [3]Code Complexity [3]Coverage@n [51]Memory Usage [17]CPU Utilization [17]Execution Time [12]Energy Consumption [12] Accuracy [51]F1 score [27]Mean Reciprocal Rank(MRR) [55]
This paper provides a comprehensive review of the current methods and metrics used to evaluate the performance of Large Language Models (LLMs) in code generation tasks.With the rapid growth in demand for automated software development, LLMs have demonstrated significant potential in the field of code generation.The paper begins by reviewing the historical development of LLMs and their applications in code generation.Next, it details various methods and metrics for assessing the code generation capabilities of LLMs, including code correctness, efficiency, readability, and evaluation methods based on expert review and user experience.The paper also evaluates the widely used benchmark datasets, identifying their limitations and proposing directions for future improvements.Specifically, the paper analyzes the performance of code generation models across different tasks by combining multiple evaluation metrics, such as code compilation/interpretation success rates, unit test pass rates, and performance and efficiency metrics, to comprehensively assess the practical application of LLMs in code generation.Finally, the paper discusses the challenges faced in evaluating LLMs in code generation, particularly how to ensure the comprehensiveness and accuracy of evaluation methods and how to adapt to the evolving practices of software development.These analyses and discussions provide valuable insights for further optimizing and improving the application of LLMs in code generation tasks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have seen rapid development in recent years, garnering widespread attention in the academic community.By learning vast amounts of natural language data, LLMs are capable of producing fluent human language and have demonstrated the ability to understand semantics and perform complex tasks.With the continuous evolution of LLMs, they have exhibited outstanding capabilities in generating code, bringing new possibilities to automated software development [4,8].</p>
<p>The complexity of software systems continues to increase, and the demand for shorter development cycles is becoming increasingly urgent, making automated code generation an urgent need in the software indus-try.Traditional automatic code generation methods based on rules and pattern matching are no longer sufficient, while LLMs offer a new approach to this problem.</p>
<p>LLMs can generate functional code based on natural language descriptions, greatly improving the efficiency of software development [13,28].</p>
<p>Code generation tasks involve the automatic production of source code based on a given specification, which is often expressed in natural language or another high-level description.These tasks can include a wide range of activities, such as:</p>
<p>• Translating natural language requirements into executable code: For example, converting a user story or a feature description into a functional programming script.</p>
<p>• Filling in code templates: Completing partially written code snippets by adding the missing parts based on the provided context.</p>
<p>• Refactoring and optimizing existing code: Modifying and improving code structure and efficiency without altering its functionality.</p>
<p>• Generating test cases: Creating test scripts and scenarios to validate the correctness and performance of a given piece of code.</p>
<p>By automating these tasks, LLMs can significantly streamline the software development process, reduce the likelihood of human error, and enable developers to focus on more complex and creative aspects of programming.As LLMs continue to be deeply integrated into the field of software engineering, it is necessary for us to fully understand their capabilities and limitations in code generation.This will not only help to improve the efficiency of software development but also promote further development of LLM technology in related fields [8,38].</p>
<p>However, comprehensively understanding the evaluation methods of LLMs in the field of code generation remains a challenge.This paper aims to provide a comprehensive and systematic review of the latest developments in the evaluation of LLMs for code generation, analyze the achievements and challenges in this area, and provide guidance for future research and applications.First, we will focus on discussing various methods and metrics for evaluating the performance of LLM code generation, including aspects such as code correctness, efficiency, and readability.In addition, we will summarize the widely used benchmark test suites in the industry and their limitations, and propose directions for future improvement.Finally, we will discuss the challenges faced by LLMs in the field of code gener-ation, such as reliability, security, and interpretability, and look forward to future research prospects [26,35].</p>
<p>The rest of this work is organized as follows: Section 2 focuses on the metrics for assessing the code generation capabilities of large language models.This section first discusses metrics such as code correctness, efficiency, and readability [26].It then introduces evaluation methods based on expert review and user experience [7].These evaluation metrics and methods can comprehensively reflect the performance of large language models in code generation tasks.Section 3 reviews the current benchmarks and datasets used for evaluating code generation.This section begins with a summary of existing code generation benchmark suites, such as HumanEval, MBPP, and CodeXGLUE [32,49].</p>
<p>It then analyzes in detail the various metrics covered by these benchmark suites, including code correctness, efficiency, and readability [15].The analysis indicates that while existing benchmarks and evaluation metrics have made certain progress, they still have some limitations, such as the difficulty in fully reflecting the performance of code in practical applications.Section 4 discusses the future challenges and opportunities in evaluating the code generation capabilities of large language models.This section first discusses the issue of scalability, that is, how to evaluate these models on a larger scale of data and more complex tasks [47].Secondly, it analyzes the importance of multilingual generalization capabilities and how to evaluate the performance of large language models across a broader range of programming languages [6].Furthermore, security and robustness are also important aspects that must be considered in the evaluation of large language models [40].Finally, the section also explores how to evaluate the efficiency and usability of these models in practical applications [17].</p>
<p>In summary, this paper provides a comprehensive reference for evaluating the performance of large lan-guage models in code generation tasks by systematically reviewing existing methods and metrics and looking forward to future development trends.Additionally, AutoSurvey [43] was utilized to retrieve relevant literature, ensuring a comprehensive overview.We hope that this review provides valuable insights for researchers and practitioners in the field, thereby fostering further advancements in this area.</p>
<p>2 Code Generation Evaluation Metrics</p>
<p>Evaluation Based on Similarity</p>
<p>Similarity-based evaluation methods in code generation primarily assess the quality of generated code by comparing its similarity to reference code.Figure 1 provides a classification of various evaluation metrics used in code generation benchmarks.These similarity evaluation methods are generally divided into following categories: similarity based metrics, execution based metrics and feedback based metrics.I will discuss several representative metrics in detail in the following sections.</p>
<p>Traditional Similarity Metrics</p>
<p>Traditional similarity metrics, initially used in the field of natural language processing, have also been applied to the evaluation of code generation capabilities.</p>
<p>These methods assess the quality of generated code by calculating its similarity to reference code.Common similarity metrics include BLEU, ROUGE, and METEOR, with the CodeXGLUE dataset utilizing the BLEU metric to calculate the similarity between generated and correct code.</p>
<p>BLEU (Bilingual Evaluation Understudy</p>
<p>) is a metric used for machine translation evaluation, first proposed by Papineni et al [33].It measures the overlap of n-grams between the generated text and reference text.Specifically, BLEU calculates the number of matching n-grams between the generated text and reference text, then averages these matches with weights to produce an overall similarity score.</p>
<p>In the context of code generation, BLEU is used to evaluate the similarity between generated and reference code.Although BLEU excels in natural language processing, it also has some issues when applied to code generation.Firstly, the syntax and semantic structure of code are more complex than natural language, so relying solely on n-gram matching may not accurately measure code similarity.Secondly, code often contains a large number of unique identifiers such as variable names and function names, which may be completely different across different code snippets but actually perform the same function.</p>
<p>Nevertheless, due to its simplicity and intuitiveness, BLEU is still widely used for preliminary evaluation in code generation.For example, the CodeXGLUE dataset [27] uses BLEU as one of its main evaluation metrics to measure the similarity between generated and correct code.This indicates that, despite its flaws, BLEU remains a useful tool, especially when more complex alternatives are not available.Exact Match (EM) [27] metric is a straightforward and stringent evaluation method used to measure the accuracy of generated code by directly comparing it to the reference code.This metric is particularly useful in assessing the overall correctness of the code completion process, taking into account elements such as identifiers, keywords, operators, delimiters, and literals.</p>
<p>Exact Match (EM) calculates the percentage of generated code snippets that exactly match the reference code snippets.This means that for a generated code snippet to be considered a match, it must be identical to the reference code in every aspect, including syntax, structure, and content.It provides a clear and unambiguous measure of code generation accuracy, making it a valuable tool for evaluating code completion models.</p>
<p>Edit Distance [1], also known as Levenshtein Distance, is a widely used metric for evaluating the similarity between two sequences by measuring the minimum number of single-character edits required to transform one sequence into the other.These edits include insertions, deletions, and substitutions.In the context of code generation, Edit Distance can be employed to assess how closely the generated code matches the reference code by quantifying the effort needed to convert one into the other.Edit Distance is calculated using the following recursive definition:
lev(a, b) =                  |a| if |b| = 0 |b| if |a| = 0 lev(tail(a), tail(b)) if head(a) = head(b) 1 + min      lev(tail(a), b) lev(a, tail(b)) lev(tail(a), tail(b)) otherwise(1)
The tail(x) denotes the string obtained by removing the first character of x (i.e., tail
(x 0 x 1 . . . x n ) = x 1 x 2 . . . x n ),</p>
<p>Code-Specific Similarity Metrics</p>
<p>To more accurately assess the quality of code generation, researchers have developed similarity metrics specifically tailored for code.These methods introduce programming-specific characteristics, such as Abstract Syntax Trees (ASTs), data flow, and token matching, to provide a more comprehensive evaluation of the syntactic and semantic similarity between generated and reference code.</p>
<p>CodeBLEU is an evaluation method specifically for code that extends the traditional BLEU metric [36].In summary, when calculating similarity, CodeBLEU analyzes the AST structures of both the generated and reference code, ensuring that not only the textual content is similar but also that the code structure and logic are as close as possible.This allows CodeBLEU to more fully consider the syntactic and semantic similarity of code when evaluating the quality of code generation, thereby providing a more accurate assessment.</p>
<p>CodeBLEU was used to evaluate the task of generating code from natural language descriptions, and the results showed that the Pearson correlation coefficient between CodeBLEU and programmer scoring was higher than that of BLEU and accuracy metrics [36].</p>
<p>The application of CodeBLEU is not limited to a single programming language; it performs excellently across multiple programming languages [36].</p>
<p>Other Similarity Methods include metrics based on data flow analysis and semantic similarity metrics.expected functionality [10].These metrics are also used to evaluate code optimization and repair, significantly enhancing code performance [48].</p>
<p>Semantic similarity metrics focus on the actual functionality and behavior of the code.For instance, in code summarization tasks, semantic similarity metrics evaluate the quality of the summary by measuring the semantic similarity between the generated code summary and the reference summary [15].Another example is DeepSemantic, which utilizes deep learning models to generate semantic representations of binary code for code similarity measurement, showing potential in cross-architecture vulnerability detection and patch analysis [21].</p>
<p>Execution-Based Evaluation</p>
<p>Compilation/Interpretation Success Rate</p>
<p>The compilation or interpretation success rate is a crucial metric for evaluating the quality of code generation, assessing whether the generated code can be successfully compiled or interpreted without syntactic errors [39,41].A high compilation or interpretation success rate indicates that the code adheres to the syntactic rules of the programming language, which is a fundamental requirement for any functional code.If the code cannot be successfully compiled or interpreted, it cannot be executed further and thus cannot achieve its intended functionality.</p>
<p>To evaluate the compilation or interpretation success rate, we typically use standard compilers and interpreters for various programming languages, such as GCC for C/C++, the Python interpreter, etc.These tools can verify the syntactic correctness of the generated code and prepare it for execution.Through these tools, we can directly assess the compilation or interpretation success rate of the generated code, thereby gaining a basic understanding of the performance of the code generation model.For instance, the FRANC framework significantly improves the proportion of generated code that passes compilation through the use of static filters, enhancing the quality of Java suggestions by 9% to 46% and Python suggestions by 10%</p>
<p>to 43% [39].COMPCODER proposes a three-stage pipeline that uses compiler feedback to generate compilable code [41].Its pipeline includes language model fine-tuning, compilability enhancement, and compilability discrimination.This method not only improves the successful compilation rate but also makes the generated code more reliable in practical applications.</p>
<p>Unit Test Pass Rate</p>
<p>Unit testing is an important metric for evaluating code quality, which verifies the correctness of the code by running the generated code with predefined test cases [31,37,53].This method is crucial for assessing the expected performance of the code under various conditions, as it ensures the practical utility and reliability of the code.For instance, Humaneval is a representative unit testing framework, and its Pass@k metric has become a classic evaluation metric for the code generation capabilities of large language models (LLMs) [9].The Pass@k metric measures the probability of the generated code passing the test within the first k attempts, effectively evaluating the performance and reliability of the code generation model.</p>
<p>By systematically integrating unit testing steps and using error feedback to iteratively correct the generated code, the unit test pass rate of the generated code can be significantly improved, ensuring the reliability and stability of the code in practical applications.For example, CodeGen-Test adds a program testing step during the code generation process, combining testing information to iteratively produce code that meets functional requirements [53].LEVER utilizes execution results to detect and correct erroneous programs, continuously improving the quality of the generated code [31].</p>
<p>Furthermore, the Multi-Stage Generation Process introduced by VCP transforms verification errors into specific hints, guiding the model to regenerate outputs that address the discovered errors.This process significantly reduces the error rate and improves generation quality [37].</p>
<p>Performance and Efficiency Evaluation</p>
<p>Performance and efficiency evaluation refers to the assessment of the actual runtime performance of generated code by measuring its time and space complexity.</p>
<p>Efficient code is crucial for practical applications, and performance evaluation helps identify potential bottlenecks and optimize the code.In software development, optimizing computational efficiency, in addition to ensuring functional correctness, is a universal and significant objective.Efficient code enhances system performance and plays a more substantial role in resourceconstrained environments.Therefore, focusing on improving the efficiency and performance of code during the development and evaluation of code generation models is essential.</p>
<p>By integrating performance and efficiency evaluation into the code generation and testing process, we can ensure that the generated code is not only functionally correct but also performs well in practical applications, providing foundational data support for optimization.EffiBench [17] and Mercury [12] are notable frameworks in this domain.EffiBench is a benchmarking framework for evaluating the efficiency of automatically generated code, encompassing a variety of programming tasks and languages.Mercury, on the other hand, is a specialized benchmarking framework designed to assess the efficiency of code generated by Large Language Models (LLMs).</p>
<p>Feedback-Based Evaluation</p>
<p>Feedback-based evaluation methods are essential for comprehensively assessing the quality of generated code, as they incorporate human judgment and expertise to evaluate various aspects of code quality.These methods often involve blind peer review, real-world application evaluation, readability evaluation, and maintainability evaluation.</p>
<p>Blind Peer Review</p>
<p>Blind peer review is a common and effective method for evaluating code quality comprehensively.In this method, reviewers assess code snippets generated by different models without knowing the identity of the models, selecting the superior code based on predetermined criteria.This approach eliminates potential biases, making the evaluation results more objective and fair.For instance, in the MT-Bench study [52], reviewers conducted multiple rounds of comparative evaluations based on criteria such as functionality, clarity, and maintainability.Multiple reviewers and rounds of comparison ensure fairness and consistency, providing detailed insights into the strengths and weaknesses of different code generation models.</p>
<p>Real-World Application Evaluation</p>
<p>Another important evaluation method is to deploy the generated code in actual application environments and assess its performance in real-world tasks.This method fully evaluates the practicality and reliability of the code, reflecting its real-world effectiveness.Generated code is applied to real programming tasks, with metrics such as error rate, debugging time, and maintenance cost recorded.This approach provides valuable feedback on the code's functionality, stability, and adaptability.For example, generated code might perform excellently in a controlled environment but face performance bottlenecks or compatibility issues in practical applications.Real-world application evaluation helps identify and address these issues, thereby improving the overall quality of the generated code [5].</p>
<p>Readability Evaluation</p>
<p>The readability of code is crucial for understanding and maintaining it.Human evaluation methods focus on assessing the functionality, clarity, and maintainability of the code.Reviewers consider naming conventions, comments, and code logic to determine clarity and conciseness.Clear and concise code improves development efficiency and long-term sustainability.For example, reviewers check if variable and function names are descriptive, if appropriate comments explain the code logic, and if the code structure is easy to understand [42].</p>
<p>Maintainability Evaluation</p>
<p>Maintainability refers to the ease with which code can be updated and modified in the future.Code with high maintainability should have good modular design [22], detailed documentation [45], and adherence to programming standards.Modular design makes code easier to modify and extend by dividing it into independent, reusable modules, each responsible for a specific function [22].Reviewers evaluate whether the code is reasonably divided into such modules.Additionally, they check for comprehensive documentation and comments, such as descriptions of functions and classes, parameters, and return values.For instance, each function should have detailed comments explaining its functionality, input parameters, and return values.Good documentation and comments help current and future developers understand and maintain the code [45].</p>
<p>Code Generation Evaluation Benchmarks</p>
<p>Evaluating the performance of code generation models is a multifaceted task that involves various benchmarks designed to test different aspects of code quality.Figure 2 presents a classification of these benchmarks, categorizing them based on the specific evaluation criteria they address.These benchmarks are essential for understanding how well a model performs in generating accurate, efficient, and practical code.I will delve into several representative benchmarks in the following sections.</p>
<p>Code Correctness</p>
<p>In the field of code generation, testing for code correctness is an essential task.It not only helps evaluate the quality of the code generated by models but also provides feedback for model optimization.In this chapter, we will introduce five datasets used for code correctness testing: CodeXGLUE [27], HumanEval [9],</p>
<p>MBPP [3], CoderUJB [51], and VerilogEval [25].These methods each have their own characteristics in terms of dataset composition, testing methods, performance metrics (such as pass@k), and analysis of test results.</p>
<p>CodeXGLUE</p>
<p>CodeXGLUE encompasses various code understanding and generation tasks using multiple large datasets.For example, code clone detection utilizes BigCloneBench and POJ-104 [29] datasets, defect detection uses the Devign [54] dataset, text-to-code generation employs the CONCODE [19] dataset, and code summary generation relies on the CodeSearchNet [18] dataset.The HumanEval dataset is used to assess the practical performance of code generation models.It comprises 164 Python programming tasks, each with a natural language description and corresponding test cases.</p>
<p>HumanEval</p>
<p>Multiple test cases are designed for each task to ensure comprehensive functionality coverage and correctness of the generated code.These tasks span various programming concepts, from basic control structures to complex algorithms and data structures, thoroughly testing the capabilities of code generation models.The hand-written nature of these tasks ensures quality and uniqueness, avoiding issues from programmatically copied tasks.</p>
<p>The main performance metric of HumanEval is the pass rate (Pass@k), the proportion of at least one of the top k generated code snippets passing all test cases.</p>
<p>Pass@1, Pass@5, and Pass@10 are commonly used metrics.By comparing pass rates, the relative advantages and disadvantages of different models can be assessed.</p>
<p>Pass@1 reflects the model's ability to generate highquality code on the first attempt, while Pass@5 and Pass@10 reflect performance in diversity.</p>
<p>Analysis of HumanEval test results shows that GPT based models perform well, with higher Pass@1 scores than traditional methods.This indicates effective performance of pretrained GPT in real programming tasks, particularly in code correctness and functionality.Detailed analysis of different models' performances on various tasks helps identify strengths and weaknesses, guiding further optimization.For instance, CodeX excels in string processing tasks but may require optimization for complex algorithms.In-depth analysis also reveals differences in handling various natural language descriptions, offering insights for improving description expressions.</p>
<p>Figure 3 illustrates the Pass@1 performance of representative LLMs on the HumanEval dataset over time, highlighting the improvements in code generation capabilities.</p>
<p>In summary, the HumanEval dataset and testing method provide important references for evaluating and improving code generation models, guiding future research.Comparing Pass@k scores identifies strengths and weaknesses, facilitating targeted improvements.This research demonstrates the potential of current LLMs while identifying key challenges for practical applications, guiding future improvements in model training and assessment methods.</p>
<p>MBPP (Mostly Basic Python Problems)</p>
<p>VerilogEval</p>
<p>VerilogEval is a dataset dedicated to the code generation and verification of Verilog, a hardware description language.Unlike HumanEval and MBPP, which focus on software, VerilogEval targets hardware design tasks, ensuring the generated code's effectiveness in synthesis and simulation.The dataset includes tasks covering combinational logic circuits, sequential logic circuits, and state machine design, with each task providing detailed natural language descriptions and design constraints like timing, power, and area.</p>
<p>The testing method involves synthesis and simula- • Fixeval: A benchmark comprising of buggy code submissions to competitive programming problems and their corresponding fixes.</p>
<p>These benchmarks cover a wide range of programming languages and problem types, providing a comprehensive assessment framework for evaluating code generation models in various real-world scenarios.</p>
<p>Code Efficiency Evaluation</p>
<p>In the realm of code generation, the efficiency assessment of code is a critical aspect, directly influencing the feasibility and value of the generated code in practical applications.Efficient code not only conserves computational resources but also enhances user experience and system response speed.Therefore, evaluating the efficiency of generated code is an essential step in ensuring code quality.This chapter will introduce three methods for assessing code efficiency: EffiBench [17],</p>
<p>Mercury [12].Through these methods, we can gain a more comprehensive understanding of the efficiency of generated code, promoting the development of more efficient code generation models.Figure 5 illustrates the execution time performance of various language models evaluated using EffiBench, plotted against their release periods.This visualization helps in understanding how the efficiency of these models, in terms of execution time, has evolved over time.</p>
<p>EffiBench</p>
<p>Similarly, Figure 6 depicts the memory usage performance of LLMs, again plotted over their release periods.</p>
<p>This figure provides insights into the trends and improvements in memory efficiency across different LLMs.</p>
<p>EffiBench's results show significant differences in efficiency among code generated by different models.</p>
<p>Some models have longer execution times or higher memory usage than benchmark code.For instance, GPT-4-turbo performs better than other models but still lags behind human-written solutions, with an average execution time 1.69 times that of the standard solution and worst-case memory usage 142.89 times higher.</p>
<p>These results highlight areas for optimization and provide a foundation for future improvements in code generation models.</p>
<p>Mercury</p>
<p>Mercury is a benchmarking framework designed to</p>
<p>Practicality Assessment of Code Generation Models</p>
<p>In code generation, usability and user experience are key metrics for evaluating model capabilities, alongside code correctness.Code must be logically cor-rect and robust enough to handle minor coder errors, while also being user-friendly in terms of readability and maintainability.This paper introduces three methods for evaluating practicality: AlignJudge [11], Real-HumanEval [30], and Copilot Evaluation Harness [2].</p>
<p>Each method is discussed in terms of dataset composition, testing methods, performance metrics, and test result analysis.This detailed exploration aims to provide a comprehensive framework for assessing the readability and maintainability of code generation models.</p>
<p>AlignJudge</p>
<p>The AlignJudge method uses a subset of the Hu- Results show that GPT-3-based models perform exceptionally well in supporting programmers, especially in complex tasks, with user ratings significantly higher than traditional methods.RealHumanEval effectively evaluates model support in real programming environments, particularly in user experience and task completion efficiency.Detailed user feedback reveals the model's strengths and weaknesses, providing insights for further optimization.For example, the model is especially helpful in data processing tasks, significantly reducing completion time, but needs improvement in tasks requiring complex logical reasoning.These findings highlight the importance of improving model alignment and handling complex tasks.</p>
<p>Other Benchmarks</p>
<p>In addition to the methods detailed above, numerous other innovative approaches and metrics exist for evaluating the practicality of code generation in large language models.</p>
<p>• Plot2Code: Plot2Code is a benchmark designed to evaluate the ability of Multi-modal Large Lan-</p>
<p>Multimodal Evaluation Approaches</p>
<p>The future of code evaluation will likely incorporate multimodal approaches, combining textual, visual, and even auditory modalities to assess the generated code more holistically.For instance, code often interacts with various user interfaces, databases, and hardware systems, where evaluating the integration of generated code with these components is crucial.A multimodal evaluation approach could include not only traditional metrics like code correctness and efficiency but also the code's ability to generate and interact with graphical user interfaces (GUIs), manage databases, or control hardware components.</p>
<p>Such an approach could be particularly valuable in evaluating code used in embedded systems, robotics, or web development, where the interaction between software and hardware or user interfaces is critical.For example, in web development, the generated code could be evaluated on its ability to render correctly across different devices and browsers, which requires a blend of code analysis and visual inspection.Similarly, in robotics, code generation could be evaluated on its ability to control physical devices, which might involve integrating sensory feedback into the evaluation process.</p>
<p>This kind of multimodal evaluation would ensure that LLMs are not only generating syntactically correct code but also creating functional, reliable, and user-friendly systems.</p>
<p>Context-Aware Evaluation</p>
<p>Future evaluations will increasingly need to consider the context in which the code is generated and used.</p>
<p>Context-aware evaluation would involve assessing how well the generated code fits into a specific environment, such as a particular software architecture, project history, or even the coding style of a development team.</p>
<p>This direction recognizes that code is rarely written in isolation but is instead part of a larger ecosystem with specific requirements and constraints.</p>
<p>Context-aware evaluation could leverage tools that analyze the broader codebase or project documentation to ensure that the generated code is compatible with existing systems.This might include checking for adherence to project-specific coding standards, compatibility with existing modules or libraries, and alignment with the overall project architecture.For instance, in large-scale software projects, it is crucial that new code integrates seamlessly with the existing codebase, following the same design patterns and conventions.Future evaluation methods could automatically assess this alignment, ensuring that the generated code does not introduce inconsistencies or technical debt.</p>
<p>Moreover, context-aware evaluation could extend to assessing the generated code's compliance with domainspecific regulations or standards, which is particularly important in fields like healthcare, finance, or aerospace, where regulatory compliance is critical.</p>
<p>Ethical and Responsible Code Evaluation</p>
<p>As LLMs become more powerful and their code generation capabilities more sophisticated, the ethical implications of their use in software development must be considered.Future evaluation methods will likely include criteria for assessing the ethical and responsible use of generated code, particularly in areas such as privacy, security, and bias.</p>
<p>Ethical code evaluation could involve checking for security vulnerabilities that could be exploited, ensuring that the generated code adheres to privacy standards, and evaluating whether the code might perpetuate or introduce biases.For instance, code that interacts with user data must be evaluated for how it handles sensitive information, ensuring compliance with privacy laws such as GDPR or HIPAA.Similarly, algorithms generated for decision-making processes need to be checked for biases that could lead to unfair or discriminatory outcomes.</p>
<p>This direction also involves assessing the transparency and explainability of the generated code.As LLMs become involved in more critical and high-stakes applications, there is a growing need for code that is not only functional but also interpretable and explainable by humans.This might involve developing metrics that evaluate how well the generated code can be understood, audited, and justified by developers and stakeholders.</p>
<p>Continuous and Automated Evaluation Pipelines</p>
<p>In the future, code evaluation is likely to become more integrated into the continuous integration and continuous deployment (CI/CD) pipelines that are common in modern software development.Automated evaluation pipelines could continuously assess the quality of generated code as part of the software development lifecycle, providing real-time feedback and enabling rapid iteration and improvement.</p>
<p>Automated evaluation pipelines could integrate with existing CI/CD tools to automatically run tests, check for code quality, and even provide suggestions for improving the generated code.This would enable developers to incorporate LLMs into their workflows more seamlessly, with the confidence that the generated code is being continuously evaluated and validated.Such pipelines could also include feedback loops where the results of evaluations are fed back into the LLM to improve future generations, creating a dynamic and evolving evaluation process.This approach would be particularly valuable in agile development environments where speed and iteration are key.By integrating automated evaluations into the development pipeline, teams could quickly identify and address issues with generated code, ensuring that it meets the required standards before it is deployed.</p>
<p>Human-AI Collaboration in Evaluation</p>
<p>As LLMs continue to advance, they have demonstrated strong performance as evaluators in various tasks [44,52].This progress opens up more opportunities for human-AI collaboration in code evaluation.</p>
<p>Future evaluation methods may involve a hybrid approach where AI assists human reviewers by identifying potential issues, suggesting improvements, and automating routine checks, while humans provide the nuanced judgment and contextual understanding that AI still lacks.This collaborative approach could leverage AI to handle the more routine and time-consuming aspects of code evaluation, such as checking for syntax errors, running test cases, and ensuring compliance with coding standards.Meanwhile, human reviewers could focus on the higher-level aspects of code quality, such as design, architecture, and the alignment of the code with project goals and user needs.This collaboration could be facilitated through tools that provide human reviewers with AI-generated insights and recommendations, enabling them to make more informed decisions.For instance, an AI might highlight areas of the code that are particularly complex or prone to errors, allowing human reviewers to focus their attention where it is most needed.</p>
<p>Conclusion</p>
<p>The future of code evaluation for large language models is poised to address the current limitations and explore new avenues that align with the evolving capabilities of LLMs and the demands of modern software development.By incorporating multimodal, context-aware, ethical, automated, and collaborative approaches, the field can ensure that LLMs continue to generate high-quality, reliable, and responsible code that meets the diverse needs of the software industry.These advancements will not only improve the accuracy and robustness of evaluations but also enhance the practical utility and trustworthiness of LLM-generated code in real-world applications.</p>
<p>Reddy.</p>
<p>Xlcost: A benchmark dataset for cross-lingual code intelligence.arXiv preprint arXiv:2206.08474,2022.</p>
<p>Besides BLEU, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and METEOR (Metric for Evaluation of Translation with Explicit ORdering) are two traditional similarity metrics initially used in natural language processing that have been adapted for code generation evaluation.ROUGE focuses on recall, measuring the frequency of n-grams from the reference text appearing in the generated text, which can capture subtle differences when the generated code covers the logical steps of the reference code despite different variable and function names.METEOR, on the other hand, combines precision and recall while considering lexical matching, multi-word matching, and semantic relationships.Its nuanced ap-</p>
<p>Fig. 1 .
1
Fig.1.Classification of Evaluation Metrics for Code Generation Benchmarks.</p>
<p>and head(x) represents the first character of x (i.e., head(x 0 x 1 . . .x n ) = x 0 ).In the minimum operation, the first term corresponds to deletion (from a to b), the second to insertion, and the third to replacement.The Edit Distance metric is particularly useful for code generation evaluation as it provides a more flexible measure of similarity compared to Exact Match.It accounts for minor differences and variations in the code, such as different variable names or slight changes in syntax, while still reflecting the overall effort needed to achieve an exact match.This makes it a valuable complement to other metrics like BLEU, ROUGE, METEOR, and Exact Match, offering a more nuanced understanding of the generated code's quality.</p>
<p>CodeBLEU combines n-gram matching from BLEU while introducing syntactic and semantic information of code.Specifically, it includes weighted n-gram matching, syntactic AST matching, and semantic data flow matching.Each component calculates a score, which is then combined in a weighted manner to produce the total score.The weighted n-gram matching is an extension of the traditional BLEU algorithm, assigning different weights to different n-grams to better reflect the keywords and structures in code.Syntactic AST matching uses Abstract Syntax Trees (ASTs) to compare the syntactic structure of candidate and reference code, calculating scores by matching subtrees.Semantic data flow matching evaluates the semantic similarity of code by analyzing the data flow diagrams within the code.Such a design enables CodeBLEU to capture not only the surface similarity of code but also to deeply understand the internal logic and functionality, improving the accuracy and comprehensiveness of the evaluation.</p>
<p>Data flow analysis assesses code quality by comparing the similarity of data flows between generated and reference code, providing a deeper understanding of the code's semantics.Data-aware techniques analyze variables and data flows in generated code to verify functional correctness, ensuring the generated code achieves</p>
<p>Fig. 2 .
2
Fig.2.Classification of Code Generation Benchmarks.</p>
<p>A u g 2 0 2 2 FFig. 3 .
23
Fig.3.Pass@1 Performance of LLMs on HumanEval Over Time.</p>
<p>5 GL l a m a 2 L l a m a 3 -Fig. 4 .
5234
Fig.4.Pass@1 Performance of LLMs on MBPP Over Time.</p>
<p>Figure 4
4
Figure 4 illustrates the Pass@1 performance of representative LLMs on the MBPP dataset over time, showcasing the remarkable improvements in the capabilities of code generation models.In summary, MBPP's comprehensive testing methods and detailed result analysis enhance understanding</p>
<p>N o v 2 CFig. 5 . 2 s t a r c o d e r s t a r c o d e r 2 Fig. 6 .
25226
Fig.5.Execution Time (ET) Evaluation of LLMs on EffiBench Over Release Time.</p>
<p>assess the efficiency of code generated by Large Language Models (LLMs).The dataset includes code snippets in multiple languages, covering tasks from basic data structures to complex algorithms.Sourced from open-source projects and programming competitions, the dataset ensures a broad and authoritative evaluation scope.Tasks are categorized by difficulty, from basic operations to complex implementations.Mercury's testing methodology is based on actual code execution performance data.Preset test cases run the generated code in a fixed hardware environment, recording execution time, memory usage, and energy consumption.Repeated testing ensures accuracy, with average values taken as final results.Tasks are executed in a sandbox environment with strict time and memory limits, preventing resource monopolization and ensuring security.Performance metrics include execution time, memory usage, energy consumption, and code efficiency scores.Execution time measures task completion time, memory usage records occupied memory, and energy consumption assesses power usage.Code efficiency scores integrate these metrics for comprehensive evaluation.Mercury's results reveal significant performance differences among LLM-generated code.Some models excel in execution time but lag in memory usage and energy consumption, while others show the opposite.These findings highlight the need for optimization in code efficiency.Mercury provides a tool for identifying and improving the efficiency of generated code, guiding future research and development.Both EffiBench and Mercury offer valuable insights into the efficiency and performance of code generation models, supporting the development of more efficient and reliable LLM-generated code.</p>
<p>manEval dataset, which contains 164 programming problems with task descriptions, reference solutions, and test cases.Researchers selected 30 problems and created solutions with subtle errors, covering common programming tasks such as algorithms, data structures, and mathematics, to evaluate model alignment.Errors include variable name typos, logical errors, and boundary condition handling errors.This design tests the model's ability to generate correct code and correct erroneous code, providing a comprehensive understanding of the model's practical capabilities.The method involves assessing whether the generated code can pass given test cases.Researchers used all 164 HumanEval problems and added 30 problems with subtle errors to further test model alignment.The model must generate code that passes unit tests and corrects errors in the provided solutions.Metrics such as generation time, code length, and complexity help evaluate performance, efficiency, readability, and maintainability.Key performance metrics include pass rate (Pass@k) and alignment.Pass@k measures the proportion of the top k generated code snippets that pass all test cases.Alignment evaluates the model's performance in differ-ent contexts, especially with erroneous prompts.Additional metrics like edit distance and syntactic similarity provide deeper insights into code differences and structural similarities.Experimental results show that models struggle with subtle errors in prompts, leading to lower quality code.Clear instructions improve results, highlighting the need for better model alignment.Different error types affect performance variably; simple typos are corrected well, while logical and boundary errors pose challenges.The diversity and coverage of training data are crucial for improving performance.Researchers suggest optimizing training data, prompt design, and error handling to enhance model capabilities.3.3.2RealHumanEvalRealHumanEval uses a new evaluation framework to assess the capability of large language models (LLMs) in supporting programmers.The dataset includes complex programming tasks with detailed natural language descriptions and multiple test cases, primarily in Python.These tasks are designed to cover diverse programming challenges, ensuring comprehensive evaluation.Researchers verified each task multiple times to validate their applicability in real-world scenarios.The method is based on real user feedback.Researchers invited 213 experienced programmers to complete actual programming tasks using model-generated code.Participants rated the usability and helpfulness of the code and verified its correctness through preset test cases.Participants were randomly assigned to one of seven conditions: a control with no LLM support, three with auto-completion (using CodeLlama-7b, CodeLlama-34b, and GPT-3.5-turbo-instruct), and three with chat support (using chat versions of the aforementioned models).Each participant was assigned to one condition for the entire test to minimize con-text switching.The RealHumanEval platform supports auto-completion and chat.In auto-completion mode, LLM provides code suggestions based on cursor position.In chat mode, programmers can ask questions and receive answers, copying code from chat responses into the editor.All interactions, such as suggestion acceptance rates and copied code frequencies, were recorded.Main performance metrics include user ratings and pass rate (Pass@k).User ratings measure the helpfulness of the model in real programming tasks on a scale from 1 to 5. Pass rate measures the proportion of the top k generated code snippets that pass all test cases.Additional metrics like task completion time and code quality were also considered.User ratings provide subjective usability measures, while pass rate directly evaluates code accuracy.Researchers also analyzed code readability and maintainability for a comprehensive assessment.</p>
<p>guage•</p>
<p>Models (MLLMs) to generate code from scientific plots.It includes 132 high-quality matplotlib plots across six types, each accompanied by source code and a descriptive instruction summarized by GPT-4.The benchmark uses three evaluation metrics: code pass rate, text-match ratio, and GPT-4V overall rating, to assess the models' performance.•DevBench: DevBench is a comprehensive benchmark for evaluating large language models (LLMs) across various stages of the software development lifecycle, including design, setup, implementation, and testing.Unlike other benchmarks that focus on isolated tasks, DevBench covers a wide range of programming languages and real-world challenges.4 Challenges and Future Directions of Code Evaluation 4.1 The limitations of current code evaluation Evaluating the performance of large language models in code generation has made significant strides, yet several challenges remain.Addressing these challenges is crucial for advancing the field and maximizing the potential of code generation technologies in practical applications.The limitations of current code evaluation methods are multifaceted: Underassessment of Less Common Languages: Existing evaluation benchmarks predominantly focus on mainstream programming languages such as Python, C, and Java.This results in relatively less attention and fewer evaluation resources for less common languages.This focus on mainstream languages is largely due to the availability of large datasets and active communities that support the development and testing of models.However, this bias limits the applicability of LLMs in industries and applications where niche languages are more prevalent.For example, languages like R are crucial in data science, while Erlang is significant in telecommunications, yet these languages receive relatively little attention in benchmark evaluations.The lack of resources and evaluation tools for these less common languages could lead to suboptimal performance when LLMs are applied to tasks in these domains.Moreover, the syntactic and semantic peculiarities of these languages may not be adequately captured by models trained predominantly on data from more common languages, leading to errors in code generation.To address this, future research should prioritize the creation of diverse, high-quality datasets and evaluation tools for a broader range of programming languages.This could involve community-driven efforts to collect and curate data or the development of transfer learning techniques that allow models to adapt to new languages with minimal additional training.• Limited Evaluation Metrics: The metrics used for code evaluation are somewhat constrained, primarily relying on indicators such as code BLEU and Pass@k.These metrics may not fully capture the various dimensions of code quality and performance.While BLEU and Pass@k are useful for assessing surface-level accuracy and syntax, they fall short in evaluating deeper aspects of code quality, such as maintainability, readability, and performance efficiency.Maintainability is crucial in long-term projects where code is frequently updated or refactored.Metrics that evaluate the modularity of the code, adherence to coding standards, and the clarity of comments and documentation could provide a more holistic assessment.Similarly, performance efficiency metrics, such as time and space complexity, are critical in assessing the practicality of generated code in resource-constrained environments.For example, generated code that passes all test cases (high Pass@k) might still be inefficient in terms of execution time or memory usage.Moreover, the inclusion of human-in-the-loop evaluations, where developers assess the usability and readability of code, could also add valuable qualitative insights that automated metrics might miss.</p>
<p>J. Comput. Sci. &amp; Technol.
CategoryCode CorrectnessCodeXGLUE[32,49], HumanEval[9], MBPP[3], CoderUJB[51],Co-deNet[34],APPS[16],Spider[50],AtCoder,CodeContest[24], Defects4J[20], Fixeval[14]Code Efficiency Evaluation EffiBench[17], Mercury[12]
. Levenshtein Distance, Website, 2023</p>
<p>Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, Michele Tufano, arXiv:2402.14261Copilot evaluation harness: Evaluating llm-guided software programming. 2024arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Exploring large language models for code explanation. Paheli Bhattacharya, Manojit Chakraborty, Nsn Kartheek, Vikas Palepu, Ishan Pandey, Rakesh Dindorkar, Rishabh Rajpurohit, Gupta, arXiv:2310.166732023arXiv preprint</p>
<p>Human or machine: Automating human likeliness evaluation of nlg texts. C Erion, Ondřej Bojar, arXiv:2006.031892020arXiv preprint</p>
<p>Multipl-e: A scalable and extensible approach to benchmarking neural code generation. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, 20222208arXiv eprints</p>
<p>Can it edit? evaluating the ability of large language models to follow code editing instructions. Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Anton Lozhkov, Carolyn Anderson, Arjun Guha, arXiv-23122023arXiv e-prints</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Automating the correctness assessment of aigenerated code for security contexts. Domenico Cotroneo, Alessio Foggia, Cristina Improta, Pietro Liguori, Roberto Natella, Journal of Systems and Software. 1121132024</p>
<p>Aligning offline metrics and human judgments of value for code generation models. Victor Dibia, Adam Fourney, Gagan Bansal, Forough Poursabzi-Sangdeh, Han Liu, Saleema Amershi, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Mercury: An efficiency benchmark for llm code synthesis. Mingzhe Du, Anh Tuan Luu, Bin Ji, See-Kiong Ng, arXiv:2402.078442024arXiv preprint</p>
<p>Large language models for software engineering: Survey and open problems. Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, Jie M Zhang, arXiv:2310.035332023arXiv preprint</p>
<p>Fixeval: Execution-based evaluation of program fixes for programming problems. Md Mahim, Anjum Haque, Uddin Wasi, Ismini Ahmad, Chris Lourentzou, Brown, 2023</p>
<p>Semantic similarity metrics for evaluating source code summarization. Sakib Haque, Zachary Eberhart, Aakash Bansal, Collin Mcmillan, Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. the 30th IEEE/ACM International Conference on Program Comprehension2022</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Dong Huang, Jie M Zhang, Yuhao Qing, Heming Cui, arXiv-2402Effibench: Benchmarking the efficiency of automatically generated code. arXiv e-prints. 2024</p>
<p>Codesearchnet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, CoRR, abs/1909.094362019</p>
<p>Mapping language to code in programmatic context. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober 31 -November 4. 2018. 2018</p>
<p>Defects4j: a database of existing faults to enable controlled testing studies for java programs. René Just, Darioush Jalali, Michael D Ernst, Proceedings of the 2014 International Symposium on Software Testing and Analysis. the 2014 International Symposium on Software Testing and AnalysisAssociation for Computing Machinery2014</p>
<p>Semanticaware binary code representation with bert. Koo, Park, Choi, Kim, arXiv:2106.054782021arXiv preprint</p>
<p>Interactive code generation via test-driven user-intent formalization. Aaditya Shuvendu K Lahiri, Georgios Naik, Piali Sakkas, Choudhury, Madanlal Curtis Von Veh, Jeevana Musuvathi, Chenglong Priya Inala, Jianfeng Wang, Gao, arXiv:2208.059502022arXiv preprint</p>
<p>Devbench: A comprehensive benchmark for software development. Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen, 2024</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Schrittwieser, Science. 2022</p>
<p>Verilogeval: Evaluating large language models for verilog code generation. Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, Haoxing Ren, 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD). </p>
<p>No need to lift a finger anymore? assessing the quality of code generation by chatgpt. Zhijie Liu, Yutian Tang, Xiapu Luo, Yuming Zhou, Liang Feng Zhang, IEEE Transactions on Software Engineering. 2024</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran, Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, Dan Roth, ACM Computing Surveys. 5622023</p>
<p>Convolutional neural networks over tree structures for programming language processing. Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. Dale Schuurmans, Michael P Wellman, the Thirtieth AAAI Conference on Artificial IntelligencePhoenix, Arizona, USAAAAI PressFebruary 12-17, 2016. 2016</p>
<p>The realhumaneval: Evaluating large language models' abilities to support programmers. Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, David Sontag, arXiv:2404.028062024arXiv preprint</p>
<p>Lever: Learning to verify language-to-code generation with execution. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-Tau Yih, Sida Wang, Xi Victoria, Lin , International Conference on Machine Learning. PMLR2023</p>
<p>L2ceval: Evaluating language-to-code generation capabilities of large language models. Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, 20232309arXiv e-prints</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, Frederick Reiss, 2021</p>
<p>Ridwan Shariffdeen, and Ganesh Neelakanta Iyer. An empirical study on usage and perceptions of llms in a software engineering project. Sanka Rasnayaka, Guanlin Wang, 2023</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.102972020arXiv preprint</p>
<p>You can generate it again: Data-to-text generation with verification and correction prompting. Xuan Ren, Lingqiao Liu, arXiv:2306.159332023arXiv preprint</p>
<p>The programmer's assistant: Conversational interaction with a large language model for software development. Fernando Steven I Ross, Stephanie Martinez, Michael Houde, Justin D Muller, Weisz, Proceedings of the 28th International Conference on Intelligent User Interfaces. the 28th International Conference on Intelligent User Interfaces2023</p>
<p>A lightweight framework for high-quality code generation. Mohammed Latif Siddiq, Beatrice Casey, Joanna Santos, arXiv:2307.082202023arXiv preprint</p>
<p>Enhancing large language models for secure code generation: A dataset-driven study on vulnerability mitigation. Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai, arXiv-23102023arXiv e-prints</p>
<p>Compilable neural code generation with compiler feedback. Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu, arXiv:2203.051322022arXiv preprint</p>
<p>Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, Heng Ji, Leti, arXiv:2305.10314Learning to generate from textual interactions. 2023arXiv preprint</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, arXiv:2406.102522024arXiv preprint</p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, arXiv:2306.050872023arXiv preprint</p>
<p>Daan Wout, Jan Scholten, Carlos Celemin, Jens Kober, arXiv:1903.05216Learning gaussian policies from corrective human feedback. 2019arXiv preprint</p>
<p>Plot2code: A comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots. Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, Ping Luo, 2024</p>
<p>Codescope: An execution-based multilingual multitask multidimensional benchmark for evaluating llms on code understanding and generation. Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Shuiguang Deng, 20232311arXiv eprints</p>
<p>Asteria-pro: Enhancing deep learning-based binary code similarity detection by incorporating domain knowledge. Shouguo Yang, Chaopeng Dong, Yang Xiao, Yiran Cheng, Zhiqiang Shi, Zhi Li, Limin Sun, ACM Transactions on Software Engineering and Methodology. 3312023</p>
<p>Codereval: A benchmark of pragmatic code generation with generative pre-trained models. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, Tao Xie, 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE). IEEE Computer Society2023</p>
<p>Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev, 2019</p>
<p>Coderujb: An executable and unified java benchmark for practical programming scenarios. Zhengran Zeng, Yidong Wang, Rui Xie, Wei Ye, Shikun Zhang, arXiv:2403.192872024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Codegen-test: An automatic code generation model integrating program test information. Maosheng Zhong, Zhixiang Wang, Gen Liu, Youde Chen, Huizhu Liu, Ruping Wu, 2023 2nd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE). IEEE2023</p>
<p>Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. Yaqin Zhou, Shangqing Liu, Jing , Kai Siow, Xiaoning Du, Yang Liu, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. M Hanna, Hugo Wallach, Alina Larochelle, Beygelzimer, Emily B Florence D'alché-Buc, Roman Fox, Garnett, NeurIPS; BC, CanadaVancouver2019. 2019. December 8-14, 2019. 2019</p>
<p>. Ming Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni, K Chandan, </p>            </div>
        </div>

    </div>
</body>
</html>
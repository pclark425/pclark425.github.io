<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-365 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-365</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-365</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-251903775</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2208.13266v4.pdf" target="_blank">JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\% to 15.8\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e365.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e365.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language Planning LLM (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART-based Language Planning Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned BART language model that converts free-form dialog into a sequence of actionable symbolic sub-goals (tokenized sub-goals) for downstream embodied planning; used both when visual input is available and when it is not (e.g., TfD).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-LARGE (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained sequence-to-sequence transformer (BART) fine-tuned on TEACh-derived pairs of (dialogue + history sub-goals) -> (future sub-goals). Uses tokenization with special tokens for speaker and history and is trained with cross-entropy reconstruction loss and beam search decoding at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Trajectory from Dialog (TfD) / Execution from Dialogue History (EDH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dialog-based embodied tasks from TEACh where an agent must convert multi-turn dialog (and optionally partial trajectory/observations) into sub-goals and then actions to complete long-horizon household tasks in an egocentric simulated environment (AI2-THOR); TfD specifically gives only dialog and initial state, EDH gives dialog + partial trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / instruction following / household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (procedural sub-goal sequences; object-action associations); when combined with maps also spatial+procedural</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (BART) + fine-tuning on TEACh task data (dialog -> sub-goal sequences); in-context history sub-goals included during fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning (supervised) on paired dialogue->sub-goal sequences; autoregressive generation (beam search) at inference</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>natural-language tokenized sub-goals produced as symbolic action-object pairs (e.g., 'Navigate Knife', 'PickUp Knife'), which are then used as explicit symbolic inputs to the downstream symbolic reasoning module</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Goal-Condition (GC) Success, Trajectory-Weighted metrics (TLW-SR / TLW-GC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When integrated in JARVIS the full system (where BART is the language planner) achieved EDH SR (seen) 15.1% [TLW 3.3] and EDH SR (unseen) 15.8% [TLW 2.6], outperforming the E.T. baseline (EDH seen 8.4% [0.8], unseen 6.1% [0.9]). In TfD (dialog-only) the full system achieved SR (seen) 1.7% [0.2] and (unseen) 1.8% [0.3] (both higher than Episodic Transformer baseline). These numbers reflect the integrated system; they show the benefit of an explicit language->symbolic-subgoal mapping when sensory input is limited or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively extracts procedural structure from free-form dialog (breaking high-level goals into ordered sub-goals), produces explicit action-object sub-goal tokens that enable symbolic reasoning and insertion of missing prerequisite steps (e.g., inserting 'PickUp knife' before 'Slice bread'). Performs robustly in few-shot/fewer-data regimes compared to end-to-end models because it outputs compact symbolic plans.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Errors in sub-goal generation (wrong objects, missing prerequisites, wrong quantities) lead to downstream task failure; when dialog is ambiguous or lacks specifics the generated sub-goals can be misordered or infeasible. TfD (dialog-only) remains challenging and yields low absolute SRs even when sub-goals are generated, showing limits of purely language-based procedural grounding without perception.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to Episodic Transformer (end-to-end action predictor), JARVIS with the BART planner improved EDH SR from 8.4% to 15.1% (seen) and from 6.1% to 15.8% (unseen).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablation study shows replacing the learned language planner with ground-truth sub-goals yields a larger improvement than replacing the executor, indicating the language planner's central role; exact ablation numbers are reported qualitatively in the paper (language-planner replacement produced the greatest gains).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A fine-tuned LLM can reliably extract procedural knowledge (ordered sub-goals) from free-form dialog and present that knowledge as explicit symbolic tokens that downstream symbolic planners can use; crucially, this enables operation when direct sensory input is unavailable (TfD) by supplying procedural structure even without perception, improving generalization over end-to-end models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e365.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e365.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goal Transformer (GT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal Transformer (adapted Episodic Transformer for action prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based action predictor adapted from the Episodic Transformer that predicts next low-level motion actions autoregressively from history images, actions, and sub-goals when semantic map information is missing or uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic transformer for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goal Transformer (Episodic Transformer adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modified Episodic Transformer with: a language encoder for future sub-goals, an image encoder (ResNet-50) for current/history images, and an action embedding for past actions; trained autoregressively on TEACh/EDH samples to predict next actions when the semantic map does not provide a visible target.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Execution from Dialogue History (EDH) / Goal-driven exploration</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given history images, actions, and sub-goals, predict the next motion action (forward, turn, pan, etc.) to find/approach objects and complete sub-goals; used as fallback when the semantic map lacks the target object.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation / exploration / action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + spatial (short-horizon motion policies conditioned on sub-goals and past sensory history)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised training on TEACh/EDH episodes (history images, actions, future sub-goals -> future actions)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning (supervised) on recorded episodes; autoregressive decoding during deployment</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit procedural policies encoded in Transformer weights, conditioned on encoded sub-goals and image embeddings; outputs discrete motion actions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Trajectory-weighted Success Rate (TLW-SR), SR, GC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Paper reports that inclusion of the Goal Transformer improves trajectory-weighted success and efficient action planning; exact system numbers with GT included are the reported JARVIS results (e.g., EDH seen SR 15.1% [TLW 3.3]); ablation removing GT (replacing with random exploration when object not found) reduces performance (qualitatively reported).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improves exploration and short-horizon action selection when the semantic map is incomplete; helps avoid repetitive stuck behaviours (a failure mode of Episodic Transformer) and increases trajectory efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still limited by noisy or incomplete egocentric history; if the GT outputs repetitive motion that led to collisions previously, the system falls back to random actions; GT needs sufficient training data and fails in very long-horizon tasks where single subtask failures cascade.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to using random exploration when the object is not in map (No GT), GT improves TLW metrics and overall task efficiency; Episodic Transformer end-to-end baseline performs worse (gets stuck or repeats actions).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing GT (No Goal Transformer) causes the agent to explore randomly when the target is not found, which lowers trajectory-weighted success; the Goal Transformer specifically increased TLW-SR in ablations (exact numeric deltas not tabulated in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A transformer conditioned on sub-goal tokens plus egocentric visual history can internalize short-horizon procedural/spatial policies useful for exploration when symbolic maps are incomplete; this is complementary to explicit symbolic planning and reduces failure modes of end-to-end navigation models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e365.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e365.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Commonsense Reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-level and Action-level Symbolic Commonsense Reasoning Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic reasoning module that validates and refines LLM-generated sub-goals using explicit predicates (properties and causality), object affordance collections, and semantic map / action-state checks to produce executable actions and fallback strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neuro-Symbolic Reasoning Module (task-level + action-level)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rule-based symbolic system that: (1) enforces property constraints (movable, sliceable, openable, etc.), (2) enforces causal preconditions/prerequisite relations (e.g., require 'PickUp knife' before 'Slice bread'), (3) checks action validity against the semantic map (path exists) and action-state, and (4) invokes fallback policies (Goal Transformer, random exploration) when constraints fail.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>EDH / TfD / TATC (symbolic action execution)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given symbolic sub-goals (from LLM) and an evolving semantic world representation, produce ordered, feasible action sequences that respect object affordances, causal preconditions, and environmental constraints, for household task completion in TEACh.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation + navigation + multi-step planning (household tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + procedural + spatial (affordances/properties, causal ordering of steps, spatial path feasibility checks)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>hand-crafted symbolic predicates, object property collections derived from commonsense rules, and the semantic map produced by perception modules; uses LLM outputs as input plans</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>symbolic rule checking and repair applied at runtime to LLM-generated sub-goals; uses map updates from perception for action-level checks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic predicates and rules (e.g., Pick(agent,x), Move(x,y)), explicit object property sets (movable, sliceable, openable, toggleable, supportable), and a 2D/3D semantic map (voxels projected to 2D) used to check feasibility</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, GC, TLW metrics; also internal metric: success of symbolic module given ground-truth inputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>With ground-truth perception and ground-truth sub-goals, the Symbolic Commonsense Reasoning module alone achieved over 60% success on EDH (reported in unit tests), demonstrating its effectiveness when inputs are accurate; integrated system improves EDH SR from 8.4% (E.T. baseline) to 15.1% (JARVIS).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully enforces object affordances and causal preconditions (e.g., inserts 'PickUp knife' before 'Slice bread', removes impossible 'Place' when hand empty), corrects infeasible sub-goals, and reduces navigation inefficiencies by using map-based path planning when targets are visible.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Sensitive to perception errors and incorrect sub-goal generation; wrong or missing map entries (depth/segmentation errors) produce invalid feasibility checks and lead symbolic module to incorrect repairs or fallbacks; inability to detect failed prior interactions (false positives in pick/place) causes cascading failures.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>End-to-end Episodic Transformer lacks explicit commonsense constraints and suffers from getting stuck or repeating actions; symbolic module with accurate inputs substantially outperforms end-to-end executor in unit tests (>60% vs much lower).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations indicate symbolic reasoning is critical: with ground-truth perception and sub-goals symbolic reasoning yields strong performance; replacing language planner with ground-truth sub-goals yields greater improvement than replacing executor, emphasizing symbolic reasoning's role in short-horizon correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit symbolic encoding of object affordances, preconditions, and causal ordering allows an LLM-produced procedural plan to be validated and repaired, enabling better generalization and interpretability; however, the approach hinges on accurate perception (semantic maps) and correct sub-goal outputs from the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e365.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e365.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic World Representation (semantic maps)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Map / 3D voxel -> 2D projected Semantic World Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A perception pipeline that converts egocentric RGB observations into object-level semantic 3D voxels (Mask R-CNN segmentation + Unet depth prediction + SLAM/pose) and projects them into compact 2D semantic maps (per-class 240x240 maps) used by symbolic reasoning and path planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mask R-CNN (segmentation) + U-Net (depth) -> semantic voxel map</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mask R-CNN (fine-tuned on TEACh/AI2-THOR data) provides per-frame semantic masks and object types; a U-Net depth prediction discretized into 50 depth classes (0-5m) provides per-pixel depth; combining camera pose yields 3D point cloud voxels which are vertically projected to per-class 2D semantic maps (240x240, 5cm per pixel).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Perception for EDH / TfD / TATC embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transform egocentric RGB frames into persistent spatial maps that record object locations and obstacles, to enable symbolic reasoning, path planning (FMM), and interaction target localization in household task execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial mapping / perception for navigation & manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (object locations, occupancy, per-object class maps and implied relations like 'object inside receptacle')</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learned visual models (Mask R-CNN fine-tuned on in-domain data, depth U-Net trained on AI2-THOR depth frames) and pose information from simulator (SLAM-like integration)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>per-frame segmentation and depth prediction, voxel projection, vertical projection to 2D semantic maps; used deterministically by symbolic module</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit spatial maps (3D voxels projected to 2D per-class semantic occupancy maps) plus obstacle maps; map pixels correspond to 5cm x 5cm patches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used indirectly via downstream SR/GC/TLW metrics and by error analysis categories (action failure rates due to perception errors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Semantic mapping enables map-based FMM navigation and contributes to improved trajectory efficiency (EDH TLW-SR increased in JARVIS vs baseline: e.g., EDH TLW-SR seen 3.3 for JARVIS vs 0.8 for E.T.). However, perception errors (depth or segmentation) are the dominant cause of many action failures: 'Blocked when moving' due to depth/obstacle errors: EDH ~40.8% of failure cases, and 'Object not found at location / cannot be picked up' ~21.5% (see Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When segmentation and depth are correct, the semantic map enables accurate localization of targets, feasible path planning, and correct interaction coordinates, allowing symbolic reasoning to succeed and reducing wandering/stuck behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Depth-prediction and segmentation errors cause obstacle map mistakes leading to blocked movement, collisions, 'too far to interact' errors, and failed pick/place interactions; these perception failures are the most frequent causes of action-level failures.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>End-to-end E.T. uses ResNet visual encodings without an explicit persistent semantic map and tends to get stuck/repeat actions; JARVIS's explicit map reduces those failure modes and improves TLW metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Unit tests with ground-truth perception (i.e., perfect maps) dramatically improve symbolic module performance (>60% EDH success), indicating semantic map accuracy is a critical component.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit, persistent semantic maps are an effective representation bridging perception and symbolic reasoning: they encode spatial and object-relational information usable by symbolic planners, but the overall system performance is highly sensitive to depth/segmentation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e365.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e365.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fast Marching Method (FMM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast Marching Method for map-based path planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical continuous path-planning algorithm (FMM) used on the discrete semantic maps to compute trajectories to the nearest feasible space adjacent to a visible target object whenever the target appears in the semantic map.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fast marching methods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fast Marching Method (Sethian)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic geometric path planner applied to the 2D projected semantic map/obstacle map to compute a path from current agent pose to the closest empty cell near the target; invoked when the target object is observed in the map.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Navigation subtask within EDH/TfD/TATC</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a 2D occupancy/semantic map with a visible target object, compute a collision-free path to a feasible interaction location for subsequent manipulation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial / navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (layout, traversability, distances, reachable locations)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit semantic map constructed from visual perception (Mask R-CNN + depth + pose)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>classical numerical algorithm applied to explicit map representations at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit spatial cost fields and occupancy grids derived from semantic maps; FMM computes geodesic distances on this field to derive paths</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Trajectory length efficiency (reflected in TLW metrics) and reduction of stuck/repetitive navigation behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Use of FMM on semantic maps contributed to improved trajectory-weighted metrics for JARVIS: EDH TLW-SR (seen) increased to 3.3 for JARVIS vs 0.8 for baseline E.T., indicating more efficient navigation; qualitative reductions in 'getting stuck' failure modes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When targets are visible in the semantic map, FMM reliably generates feasible, efficient paths and reduces repetitive circling or stuck behaviors seen in end-to-end baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If the semantic/obstacle map is incorrect (due to depth/segmentation error), the FMM may plan paths that collide with unseen obstacles or mark reachable space as blocked, contributing to 'Blocked when moving' and collision failures.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Episodic Transformer baseline lacks explicit map-based geometric planning and shows more stuck/repetitive navigation failures; FMM+semantic-map planning is a key factor in JARVIS outperforming the baseline on trajectory-weighted metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>When the semantic map contains the object, FMM is used; when it does not, system falls back to GT â€” ablation replacing FMM with teleportation (language only) or random exploration degrades navigation efficiency (qualitatively reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Classical geometric planners applied to explicit semantic maps complement learned components and substantially improve navigation behavior when perception is accurate; the approach highlights the benefit of mixing symbolic/geometric planning with learned language/procedural modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>A persistent spatial semantic representation for high-level natural language instruction execution <em>(Rating: 2)</em></li>
                <li>Episodic transformer for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>TEACh: Task-driven embodied agents that chat <em>(Rating: 2)</em></li>
                <li>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision <em>(Rating: 1)</em></li>
                <li>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-365",
    "paper_id": "paper-251903775",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Language Planning LLM (BART)",
            "name_full": "BART-based Language Planning Module",
            "brief_description": "A fine-tuned BART language model that converts free-form dialog into a sequence of actionable symbolic sub-goals (tokenized sub-goals) for downstream embodied planning; used both when visual input is available and when it is not (e.g., TfD).",
            "citation_title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "mention_or_use": "use",
            "model_name": "BART-LARGE (fine-tuned)",
            "model_size": null,
            "model_description": "Pre-trained sequence-to-sequence transformer (BART) fine-tuned on TEACh-derived pairs of (dialogue + history sub-goals) -&gt; (future sub-goals). Uses tokenization with special tokens for speaker and history and is trained with cross-entropy reconstruction loss and beam search decoding at inference.",
            "task_name": "Trajectory from Dialog (TfD) / Execution from Dialogue History (EDH)",
            "task_description": "Dialog-based embodied tasks from TEACh where an agent must convert multi-turn dialog (and optionally partial trajectory/observations) into sub-goals and then actions to complete long-horizon household tasks in an egocentric simulated environment (AI2-THOR); TfD specifically gives only dialog and initial state, EDH gives dialog + partial trajectory.",
            "task_type": "multi-step planning / instruction following / household tasks",
            "knowledge_type": "procedural + object-relational (procedural sub-goal sequences; object-action associations); when combined with maps also spatial+procedural",
            "knowledge_source": "pre-training on large text corpora (BART) + fine-tuning on TEACh task data (dialog -&gt; sub-goal sequences); in-context history sub-goals included during fine-tuning",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning (supervised) on paired dialogue-&gt;sub-goal sequences; autoregressive generation (beam search) at inference",
            "knowledge_representation": "natural-language tokenized sub-goals produced as symbolic action-object pairs (e.g., 'Navigate Knife', 'PickUp Knife'), which are then used as explicit symbolic inputs to the downstream symbolic reasoning module",
            "performance_metric": "Success Rate (SR), Goal-Condition (GC) Success, Trajectory-Weighted metrics (TLW-SR / TLW-GC)",
            "performance_result": "When integrated in JARVIS the full system (where BART is the language planner) achieved EDH SR (seen) 15.1% [TLW 3.3] and EDH SR (unseen) 15.8% [TLW 2.6], outperforming the E.T. baseline (EDH seen 8.4% [0.8], unseen 6.1% [0.9]). In TfD (dialog-only) the full system achieved SR (seen) 1.7% [0.2] and (unseen) 1.8% [0.3] (both higher than Episodic Transformer baseline). These numbers reflect the integrated system; they show the benefit of an explicit language-&gt;symbolic-subgoal mapping when sensory input is limited or absent.",
            "success_patterns": "Effectively extracts procedural structure from free-form dialog (breaking high-level goals into ordered sub-goals), produces explicit action-object sub-goal tokens that enable symbolic reasoning and insertion of missing prerequisite steps (e.g., inserting 'PickUp knife' before 'Slice bread'). Performs robustly in few-shot/fewer-data regimes compared to end-to-end models because it outputs compact symbolic plans.",
            "failure_patterns": "Errors in sub-goal generation (wrong objects, missing prerequisites, wrong quantities) lead to downstream task failure; when dialog is ambiguous or lacks specifics the generated sub-goals can be misordered or infeasible. TfD (dialog-only) remains challenging and yields low absolute SRs even when sub-goals are generated, showing limits of purely language-based procedural grounding without perception.",
            "baseline_comparison": "Compared to Episodic Transformer (end-to-end action predictor), JARVIS with the BART planner improved EDH SR from 8.4% to 15.1% (seen) and from 6.1% to 15.8% (unseen).",
            "ablation_results": "Ablation study shows replacing the learned language planner with ground-truth sub-goals yields a larger improvement than replacing the executor, indicating the language planner's central role; exact ablation numbers are reported qualitatively in the paper (language-planner replacement produced the greatest gains).",
            "key_findings": "A fine-tuned LLM can reliably extract procedural knowledge (ordered sub-goals) from free-form dialog and present that knowledge as explicit symbolic tokens that downstream symbolic planners can use; crucially, this enables operation when direct sensory input is unavailable (TfD) by supplying procedural structure even without perception, improving generalization over end-to-end models.",
            "uuid": "e365.0",
            "source_info": {
                "paper_title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Goal Transformer (GT)",
            "name_full": "Goal Transformer (adapted Episodic Transformer for action prediction)",
            "brief_description": "A transformer-based action predictor adapted from the Episodic Transformer that predicts next low-level motion actions autoregressively from history images, actions, and sub-goals when semantic map information is missing or uncertain.",
            "citation_title": "Episodic transformer for vision-and-language navigation",
            "mention_or_use": "use",
            "model_name": "Goal Transformer (Episodic Transformer adaptation)",
            "model_size": null,
            "model_description": "Modified Episodic Transformer with: a language encoder for future sub-goals, an image encoder (ResNet-50) for current/history images, and an action embedding for past actions; trained autoregressively on TEACh/EDH samples to predict next actions when the semantic map does not provide a visible target.",
            "task_name": "Execution from Dialogue History (EDH) / Goal-driven exploration",
            "task_description": "Given history images, actions, and sub-goals, predict the next motion action (forward, turn, pan, etc.) to find/approach objects and complete sub-goals; used as fallback when the semantic map lacks the target object.",
            "task_type": "navigation / exploration / action prediction",
            "knowledge_type": "procedural + spatial (short-horizon motion policies conditioned on sub-goals and past sensory history)",
            "knowledge_source": "supervised training on TEACh/EDH episodes (history images, actions, future sub-goals -&gt; future actions)",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuning (supervised) on recorded episodes; autoregressive decoding during deployment",
            "knowledge_representation": "implicit procedural policies encoded in Transformer weights, conditioned on encoded sub-goals and image embeddings; outputs discrete motion actions",
            "performance_metric": "Trajectory-weighted Success Rate (TLW-SR), SR, GC",
            "performance_result": "Paper reports that inclusion of the Goal Transformer improves trajectory-weighted success and efficient action planning; exact system numbers with GT included are the reported JARVIS results (e.g., EDH seen SR 15.1% [TLW 3.3]); ablation removing GT (replacing with random exploration when object not found) reduces performance (qualitatively reported).",
            "success_patterns": "Improves exploration and short-horizon action selection when the semantic map is incomplete; helps avoid repetitive stuck behaviours (a failure mode of Episodic Transformer) and increases trajectory efficiency.",
            "failure_patterns": "Still limited by noisy or incomplete egocentric history; if the GT outputs repetitive motion that led to collisions previously, the system falls back to random actions; GT needs sufficient training data and fails in very long-horizon tasks where single subtask failures cascade.",
            "baseline_comparison": "Compared to using random exploration when the object is not in map (No GT), GT improves TLW metrics and overall task efficiency; Episodic Transformer end-to-end baseline performs worse (gets stuck or repeats actions).",
            "ablation_results": "Removing GT (No Goal Transformer) causes the agent to explore randomly when the target is not found, which lowers trajectory-weighted success; the Goal Transformer specifically increased TLW-SR in ablations (exact numeric deltas not tabulated in the main text).",
            "key_findings": "A transformer conditioned on sub-goal tokens plus egocentric visual history can internalize short-horizon procedural/spatial policies useful for exploration when symbolic maps are incomplete; this is complementary to explicit symbolic planning and reduces failure modes of end-to-end navigation models.",
            "uuid": "e365.1",
            "source_info": {
                "paper_title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Symbolic Commonsense Reasoner",
            "name_full": "Task-level and Action-level Symbolic Commonsense Reasoning Module",
            "brief_description": "A neuro-symbolic reasoning module that validates and refines LLM-generated sub-goals using explicit predicates (properties and causality), object affordance collections, and semantic map / action-state checks to produce executable actions and fallback strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Neuro-Symbolic Reasoning Module (task-level + action-level)",
            "model_size": null,
            "model_description": "Rule-based symbolic system that: (1) enforces property constraints (movable, sliceable, openable, etc.), (2) enforces causal preconditions/prerequisite relations (e.g., require 'PickUp knife' before 'Slice bread'), (3) checks action validity against the semantic map (path exists) and action-state, and (4) invokes fallback policies (Goal Transformer, random exploration) when constraints fail.",
            "task_name": "EDH / TfD / TATC (symbolic action execution)",
            "task_description": "Given symbolic sub-goals (from LLM) and an evolving semantic world representation, produce ordered, feasible action sequences that respect object affordances, causal preconditions, and environmental constraints, for household task completion in TEACh.",
            "task_type": "object manipulation + navigation + multi-step planning (household tasks)",
            "knowledge_type": "object-relational + procedural + spatial (affordances/properties, causal ordering of steps, spatial path feasibility checks)",
            "knowledge_source": "hand-crafted symbolic predicates, object property collections derived from commonsense rules, and the semantic map produced by perception modules; uses LLM outputs as input plans",
            "has_direct_sensory_input": true,
            "elicitation_method": "symbolic rule checking and repair applied at runtime to LLM-generated sub-goals; uses map updates from perception for action-level checks",
            "knowledge_representation": "explicit symbolic predicates and rules (e.g., Pick(agent,x), Move(x,y)), explicit object property sets (movable, sliceable, openable, toggleable, supportable), and a 2D/3D semantic map (voxels projected to 2D) used to check feasibility",
            "performance_metric": "SR, GC, TLW metrics; also internal metric: success of symbolic module given ground-truth inputs",
            "performance_result": "With ground-truth perception and ground-truth sub-goals, the Symbolic Commonsense Reasoning module alone achieved over 60% success on EDH (reported in unit tests), demonstrating its effectiveness when inputs are accurate; integrated system improves EDH SR from 8.4% (E.T. baseline) to 15.1% (JARVIS).",
            "success_patterns": "Successfully enforces object affordances and causal preconditions (e.g., inserts 'PickUp knife' before 'Slice bread', removes impossible 'Place' when hand empty), corrects infeasible sub-goals, and reduces navigation inefficiencies by using map-based path planning when targets are visible.",
            "failure_patterns": "Sensitive to perception errors and incorrect sub-goal generation; wrong or missing map entries (depth/segmentation errors) produce invalid feasibility checks and lead symbolic module to incorrect repairs or fallbacks; inability to detect failed prior interactions (false positives in pick/place) causes cascading failures.",
            "baseline_comparison": "End-to-end Episodic Transformer lacks explicit commonsense constraints and suffers from getting stuck or repeating actions; symbolic module with accurate inputs substantially outperforms end-to-end executor in unit tests (&gt;60% vs much lower).",
            "ablation_results": "Ablations indicate symbolic reasoning is critical: with ground-truth perception and sub-goals symbolic reasoning yields strong performance; replacing language planner with ground-truth sub-goals yields greater improvement than replacing executor, emphasizing symbolic reasoning's role in short-horizon correctness.",
            "key_findings": "Explicit symbolic encoding of object affordances, preconditions, and causal ordering allows an LLM-produced procedural plan to be validated and repaired, enabling better generalization and interpretability; however, the approach hinges on accurate perception (semantic maps) and correct sub-goal outputs from the LLM.",
            "uuid": "e365.2",
            "source_info": {
                "paper_title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Semantic World Representation (semantic maps)",
            "name_full": "Semantic Map / 3D voxel -&gt; 2D projected Semantic World Representation",
            "brief_description": "A perception pipeline that converts egocentric RGB observations into object-level semantic 3D voxels (Mask R-CNN segmentation + Unet depth prediction + SLAM/pose) and projects them into compact 2D semantic maps (per-class 240x240 maps) used by symbolic reasoning and path planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mask R-CNN (segmentation) + U-Net (depth) -&gt; semantic voxel map",
            "model_size": null,
            "model_description": "Mask R-CNN (fine-tuned on TEACh/AI2-THOR data) provides per-frame semantic masks and object types; a U-Net depth prediction discretized into 50 depth classes (0-5m) provides per-pixel depth; combining camera pose yields 3D point cloud voxels which are vertically projected to per-class 2D semantic maps (240x240, 5cm per pixel).",
            "task_name": "Perception for EDH / TfD / TATC embodied tasks",
            "task_description": "Transform egocentric RGB frames into persistent spatial maps that record object locations and obstacles, to enable symbolic reasoning, path planning (FMM), and interaction target localization in household task execution.",
            "task_type": "spatial mapping / perception for navigation & manipulation",
            "knowledge_type": "spatial + object-relational (object locations, occupancy, per-object class maps and implied relations like 'object inside receptacle')",
            "knowledge_source": "learned visual models (Mask R-CNN fine-tuned on in-domain data, depth U-Net trained on AI2-THOR depth frames) and pose information from simulator (SLAM-like integration)",
            "has_direct_sensory_input": true,
            "elicitation_method": "per-frame segmentation and depth prediction, voxel projection, vertical projection to 2D semantic maps; used deterministically by symbolic module",
            "knowledge_representation": "explicit spatial maps (3D voxels projected to 2D per-class semantic occupancy maps) plus obstacle maps; map pixels correspond to 5cm x 5cm patches",
            "performance_metric": "Used indirectly via downstream SR/GC/TLW metrics and by error analysis categories (action failure rates due to perception errors)",
            "performance_result": "Semantic mapping enables map-based FMM navigation and contributes to improved trajectory efficiency (EDH TLW-SR increased in JARVIS vs baseline: e.g., EDH TLW-SR seen 3.3 for JARVIS vs 0.8 for E.T.). However, perception errors (depth or segmentation) are the dominant cause of many action failures: 'Blocked when moving' due to depth/obstacle errors: EDH ~40.8% of failure cases, and 'Object not found at location / cannot be picked up' ~21.5% (see Table 8).",
            "success_patterns": "When segmentation and depth are correct, the semantic map enables accurate localization of targets, feasible path planning, and correct interaction coordinates, allowing symbolic reasoning to succeed and reducing wandering/stuck behaviors.",
            "failure_patterns": "Depth-prediction and segmentation errors cause obstacle map mistakes leading to blocked movement, collisions, 'too far to interact' errors, and failed pick/place interactions; these perception failures are the most frequent causes of action-level failures.",
            "baseline_comparison": "End-to-end E.T. uses ResNet visual encodings without an explicit persistent semantic map and tends to get stuck/repeat actions; JARVIS's explicit map reduces those failure modes and improves TLW metrics.",
            "ablation_results": "Unit tests with ground-truth perception (i.e., perfect maps) dramatically improve symbolic module performance (&gt;60% EDH success), indicating semantic map accuracy is a critical component.",
            "key_findings": "Explicit, persistent semantic maps are an effective representation bridging perception and symbolic reasoning: they encode spatial and object-relational information usable by symbolic planners, but the overall system performance is highly sensitive to depth/segmentation quality.",
            "uuid": "e365.3",
            "source_info": {
                "paper_title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Fast Marching Method (FMM)",
            "name_full": "Fast Marching Method for map-based path planning",
            "brief_description": "A classical continuous path-planning algorithm (FMM) used on the discrete semantic maps to compute trajectories to the nearest feasible space adjacent to a visible target object whenever the target appears in the semantic map.",
            "citation_title": "Fast marching methods",
            "mention_or_use": "use",
            "model_name": "Fast Marching Method (Sethian)",
            "model_size": null,
            "model_description": "Deterministic geometric path planner applied to the 2D projected semantic map/obstacle map to compute a path from current agent pose to the closest empty cell near the target; invoked when the target object is observed in the map.",
            "task_name": "Navigation subtask within EDH/TfD/TATC",
            "task_description": "Given a 2D occupancy/semantic map with a visible target object, compute a collision-free path to a feasible interaction location for subsequent manipulation actions.",
            "task_type": "spatial / navigation",
            "knowledge_type": "spatial (layout, traversability, distances, reachable locations)",
            "knowledge_source": "explicit semantic map constructed from visual perception (Mask R-CNN + depth + pose)",
            "has_direct_sensory_input": true,
            "elicitation_method": "classical numerical algorithm applied to explicit map representations at runtime",
            "knowledge_representation": "explicit spatial cost fields and occupancy grids derived from semantic maps; FMM computes geodesic distances on this field to derive paths",
            "performance_metric": "Trajectory length efficiency (reflected in TLW metrics) and reduction of stuck/repetitive navigation behaviors",
            "performance_result": "Use of FMM on semantic maps contributed to improved trajectory-weighted metrics for JARVIS: EDH TLW-SR (seen) increased to 3.3 for JARVIS vs 0.8 for baseline E.T., indicating more efficient navigation; qualitative reductions in 'getting stuck' failure modes reported.",
            "success_patterns": "When targets are visible in the semantic map, FMM reliably generates feasible, efficient paths and reduces repetitive circling or stuck behaviors seen in end-to-end baselines.",
            "failure_patterns": "If the semantic/obstacle map is incorrect (due to depth/segmentation error), the FMM may plan paths that collide with unseen obstacles or mark reachable space as blocked, contributing to 'Blocked when moving' and collision failures.",
            "baseline_comparison": "Episodic Transformer baseline lacks explicit map-based geometric planning and shows more stuck/repetitive navigation failures; FMM+semantic-map planning is a key factor in JARVIS outperforming the baseline on trajectory-weighted metrics.",
            "ablation_results": "When the semantic map contains the object, FMM is used; when it does not, system falls back to GT â€” ablation replacing FMM with teleportation (language only) or random exploration degrades navigation efficiency (qualitatively reported).",
            "key_findings": "Classical geometric planners applied to explicit semantic maps complement learned components and substantially improve navigation behavior when perception is accurate; the approach highlights the benefit of mixing symbolic/geometric planning with learned language/procedural modules.",
            "uuid": "e365.4",
            "source_info": {
                "paper_title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "A persistent spatial semantic representation for high-level natural language instruction execution",
            "rating": 2,
            "sanitized_title": "a_persistent_spatial_semantic_representation_for_highlevel_natural_language_instruction_execution"
        },
        {
            "paper_title": "Episodic transformer for vision-and-language navigation",
            "rating": 2,
            "sanitized_title": "episodic_transformer_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "TEACh: Task-driven embodied agents that chat",
            "rating": 2,
            "sanitized_title": "teach_taskdriven_embodied_agents_that_chat"
        },
        {
            "paper_title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "rating": 1,
            "sanitized_title": "the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision"
        },
        {
            "paper_title": "Habitat-web: Learning embodied object-search strategies from human demonstrations at scale",
            "rating": 1,
            "sanitized_title": "habitatweb_learning_embodied_objectsearch_strategies_from_human_demonstrations_at_scale"
        }
    ],
    "cost": 0.01841125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents
2 Sep 2025</p>
<p>Kaizhi Zheng kzheng31@ucsc.edu 
Equal contribution 1</p>
<p>Kaiwen Zhou 
Equal contribution 1</p>
<p>Jing Gu 
Equal contribution 1</p>
<p>Yue Fan 
Equal contribution 1</p>
<p>Jialu Wang 
Equal contribution 1</p>
<p>Zonglin Di 
Eric Xin 
Wang 
Leilani H Gilpin 
Eleonora Giunchiglia 
Pascal Hitzler 
Emile Van Krieken </p>
<p>University of California
Santa Cruz</p>
<p>JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents
2 Sep 2025B1452FDA7304632CECB59CCB15C382D6arXiv:2208.13266v4[cs.AI]
Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain.To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents.First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations.Then the symbolic module reasons for sub-goal planning and action generation based on task-and action-level common sense.Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1% to 15.8%).Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings.Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge 1 .</p>
<p>Introduction</p>
<p>A long-term goal of embodied AI research is to build an intelligent agent capable of communicating with humans in natural language, perceiving the environment, and completing real-life tasks.Such an agent can autonomously execute tasks such as household chores, or follow a human commander to work in dangerous environments.Figure 1 demonstrates an example of dialog-based embodied tasks: the agent communicates with the human commander and completes a complicated task "making a sandwich", which requires reasoning about dialog and visual environment, and procedural planning of a series of sub-goals.</p>
<p>Although end-to-end deep learning models have extensively shown their effectiveness in various tasks such as image recognition (Dosovitskiy et al., 2021;He et al., 2017) and natural language understanding and generation (Lewis et al., 2020;Dathathri et al., 2020), they achieved little success on dialog-based embodied navigation and task completion with high task complexity and scarce training data due to an enormous action space (Padmakumar et al., 2021).In particular, they often fail to reason about the entailed logistics when connecting natural language guidance with visual observations, and plan efficiently in the huge action space, leading to ill-advised behaviors under unseen environments.Conventionally, symbolic systems equipped with commonsense knowledge are more conducive to emulating humanlike decision-makings that are more credible and interpretable.Both connectionism and symbolism have their advantages, and connecting both worlds would cultivate the development of conversational embodied agents.</p>
<p>To this end, we propose JARVIS, a neuro-symbolic commonsense reasoning framework towards modular, generalizable, and interpretable embodied agents that can execute dialogbased embodied tasks such as household chores.First, to understand free-form dialogs, a large language model (LLM) is applied to extract task-relevant information from human guidance and produce actionable sub-goals for completing the task (e.g., the sub-goals 1.1, 1.2,...,3.6 in Figure 1).During the process, a semantic world representation of the house environment and object states is actively built from raw visual observations as the agent walks in the house.Given the initial sub-goal sequence and the semantic world representation being built, we design a symbolic reasoning module to generate executable actions based on task-level and action-level common sense.</p>
<p>We evaluate our JARVIS framework on three different levels of dialog-based embodied task execution on the TEACh dataset (Padmakumar et al., 2021), including Execution from Dialogue History (EDH), Trajectory from Dialogue (TfD), and Two-Agent Task Completion (TATC).Our framework achieves state-of-the-art (SOTA) results across all three settings.In a more realistic few-shot setting where available expert demonstrations are limited for training, we show our framework can learn and adapt well to unseen environments.Meanwhile, we also systematically analyze the modular structure of JARVIS in a variety of comparative studies.Our contributions are as follows:</p>
<p>â€¢ We propose a neuro-symbolic commonsense reasoning framework blending the two worlds of connectionism and symbolism for conversational embodied agents.The neural modules convert dialog and visual observations into symbolic information, and the symbolic modules integrate sub-goals and semantic maps into actions based on task-level and action-level common sense.</p>
<p>â€¢ Our framework is modular and can be adapted to different levels of conversational embodied tasks, including EDH, TfD, and TATC.It consistently achieves the SOTA performance across all three tasks, with great generalization ability in new environments.</p>
<p>â€¢ We systematically study the essential factors that affect the task performance, and demonstrate that our framework can be generalized to few-shot learning scenarios when available training instances are limited.</p>
<p>Related Work</p>
<p>Embodied AI Tasks Embodied agents capable of navigation and interaction have been long studied in AI, with early work focusing on goal-directed navigation in indoor and outdoor environments (Zhu et al., 2017;Yang et al., 2019;Kim et al., 2006).More recently, research has shifted toward language-grounded agents.Vision-and-Language Navigation (Anderson et al., 2018;Ku et al., 2020;Zhu et al., 2020a;Chen et al., 2019;Qi et al., 2020;Vasudevan et al., 2021;He et al., 2021;Gu et al., 2022) explores how agents follow natural language instructions to reach target locations, while Vision-and-Dialog Navigation (Thomason et al., 2019;Banerjee et al., 2020;Nguyen et al., 2019;Nguyen and DaumÃ© III, 2019) incorporates real-time dialogue for guidance.Beyond navigation, recent efforts (Shridhar et al., 2020;Misra et al., 2018;Gordon et al., 2018) introduce interactive task completion involving both navigation and object manipulation.Dialog-based embodied tasks (Padmakumar et al., 2021;Narayan-Chen et al., 2019) further align with real-world settings, enabling agents to collaborate with humans through conversation.Our work builds on this line, aiming to develop a conversational agent for complex household tasks.</p>
<p>Vision-and-Language Navigation and Task Completion Prior work in embodied AI has explored vision-and-language navigation (Wang et al., 2019;Hong et al., 2021;Tan et al., 2019;Fried et al., 2018;Guhur et al., 2021;Chen et al., 2021a;Gu et al., 2022), vision-anddialog navigation (Wang et al., 2020;Zhu et al., 2020b;Kim et al., 2021), and task-oriented</p>
<p>Semantic Mapping</p>
<p>Visual Semantic Module</p>
<p>Task Common Sense</p>
<p>Step 0.1: Fine-tune LLM Figure 2: An overview of our JARVIS framework.The fine-tuned language planning model takes dialogue and previous sub-goals G 0:tâˆ’1 as input and produces the future subgoals G t:T (Section 3).G t:T will be further examined by our Task-Level Common Sense model and converted to more reasonable and detailed future sub-goals G â€² t:T .Meanwhile, the Visual Semantic module actively updates the semantic world representations (Section 3).If the object is found in the world representation, the next action is determined by the Fast Marching method.If not, the Goal Transformer will generate the next action.The next action a t+1 âˆˆ A will be post-processed by the Action-Level Common Sense model.object navigation (Ramrakhya et al., 2022).Vision-and-language task completion has also gained attention (Pashevich et al., 2021;Min et al., 2022;Blukis et al., 2021;Song et al., 2022), with early methods like Shridhar et al. (2020) using LSTMs for multi-modal fusion, and Pashevich et al. (2021) introducing transformers for improved temporal modeling.Modular approaches further decompose tasks: Blukis et al. (2021) use semantic voxel grids with hierarchical controllers, and Min et al. (2022) integrate semantic policies for fine-grained object search.However, these systems are designed for structured, instruction-following tasks and often struggle with the ambiguity and complexity of dialog-based scenarios.In contrast, our approach extracts task-relevant information directly from free-form dialogue and performs neuro-symbolic reasoning to generalize across diverse dialog-based embodied tasks.</p>
<p>Neuro-Symbolic Reasoning While neural models have achieved great success across vision and language tasks (He et al., 2017;Devlin et al., 2019;He et al., 2016;Dosovitskiy et al., 2021), they often lack interpretability and reasoning ability in complex, multi-modal settings.Neuro-symbolic methods address this by combining neural perception with symbolic reasoning (Mao et al., 2019;Chen et al., 2021b;Gupta et al., 2020;Hudson and Manning, 2019;Sakaguchi et al., 2021).For example, Mao et al. (2019) introduce a symbolic program generator for visual question answering, and Chen et al. (2021b) extend it to video reasoning.In dialog-based household tasks, neural models struggle to integrate diverse inputs and infer correct actions (Padmakumar et al., 2021).To address this, we propose a modular neuro-symbolic framework that converts multi-modal observations into symbolic representations and applies commonsense reasoning for robust task execution.</p>
<p>Neuro-Symbolic Conversational Embodied Agents</p>
<p>Problem Formulation</p>
<p>We study dialogue-based embodied agents, where a Follower agent must interpret natural language instructions from a Commander (a human or another agent) to complete longhorizon tasks in a visual environment.Each task begins from an initial state s i âˆˆ S and proceeds through multi-turn dialogue D = {(p i , u i )}, where p i âˆˆ {Commander, Follower} indicates the speaker and u i is the utterance.At each timestep t, the Follower receives a visual observation v t and executes an action a t âˆˆ A : S t â†’ S t+1 , aiming to reach a goal state s f âˆˆ S.</p>
<p>Tasks often involve intermediate sub-goals (e.g., "Find bread", "Cook egg") under a high-level goal (e.g., "Make breakfast").The Commander has oracle access to task and environment details, while the Follower has only egocentric perception and can request clarification during the dialogue.</p>
<p>Following Padmakumar et al. (2021), we consider three settings: in Execution from Dialog History (EDH), the agent completes an unfinished task given past dialogue and partial trajectory; in Trajectory from Dialog (TfD), the agent reconstructs a full action sequence from complete dialogue; and in Two-Agent Task Completion (TATC), the agent collaborates interactively with a Commander to complete the task.Our proposed JARVIS framework is designed to handle all three scenarios.</p>
<p>Proposed Methods</p>
<p>Our JARVIS framework (as shown in Figure 2) consists of a Language Planning module, a Visual Semantic module, and a Symbolic Reasoning module.Specifically, the Language Planning module utilizes a pre-trained large language model to process free-form language input and produces procedural sub-goals of the task.In the Visual Semantic module, we use a semantic segmentation model, a depth prediction model, and the SLAM algorithm to transform raw visual observations into a more logistic form -semantic maps containing spatial relationships and states of the objects.Finally, in the Symbolic Reasoning module, we utilize task-level and action-level commonsense knowledge to reason about transferred symbolic vision-and-language information and generate actions, and a Goal Transformer is trained to deal with uncertainty by directly producing actions when no relevant information can be retrieved from the visual symbolic representations.Below we introduce our methods in detail; please refer to Appendix A for more implementation details and Appendix B for our notation table.</p>
<p>Language Understanding and Planning The language commands in dialog-based embodied tasks are free-form and high-level, and do not contain low-level procedural instructions.Therefore, it is essential to break a command like "can you make breakfast for me?" into sub-goals such as "Find bread", "Find knife", "Slice bread", "Cook egg", and then generate actions a i to complete the sub-goals sequentially.Large language models (LLMs) are shown to be capable of extracting actionable knowledge from learned world knowledge (Huang et al., 2022).In order to understand free-form instructions and generate the sub-goal sequence for action planning, we leverage an LLM, the pre-trained BART (Lewis et al., 2020) model, to process dialog and action, and predict future sub-goal sequence G t:T for completing the whole task:
G t:T = LLM(D, G 0:tâˆ’1 ) (1)
where D = {(p i , u i )} is the collected set of user-utterance pairs and the previous sub-goal sequence G 0:tâˆ’1 = {g 0 , g 1 , g 2 , ..., g tâˆ’1 }.For sequential inputs G 0:tâˆ’1 , we encode the sub-goals into tokens and concatenate them as the input of the LLM model.For the ground-truth subgoal sequence G 0:T , we acquire it by a rule-based transformation from the action sequence A 0:T = {a 0 , a 1 , a 2 , ..., a t , ..., a T }.Concretely, we note that the actions can be categorized as navigations and interactions.For interactions, we coalesce the action and targeted object as the sub-goal.For example, if the embodied agent executes the action of picking up a cup, we will record the sub-goal "PickUp Cup".For navigation, we coalesce "Navigate" with the target object of the next interaction.Note that in cases like the Trajectory from History (TfD) task, where only dialog information D is given and other history information is missing, Equation 1 reduces to
G t:T = LLM(D) (2)
The BART model is trained with a reconstruction loss by computing the cross entropy between the generated future sub-goal sequence G t:T and the ground truth future sub-goal sequence G t:T .</p>
<p>Semantic World Representation</p>
<p>The visual input into a embodied agent is usually a series of RGB images V = {v 0 , v 1 , v 2 , ..., v T }.One conventional way in deep learning is to fuse the visual information with other modalities into a neural network (e.g., Transformers) for decision making, which, however, is uninterpretable and often suffers from poor generalization under data scarcity or high task complexity.Thus, we choose to transform visual input into a semantic representation similar to Blukis et al. (2021) and Min et al. ( 2022), which can be used for generalized symbolic reasoning later.At each step t, we first use a pre-trained Mask-RCNN model (He et al., 2017) for semantic segmentation, which we fine-tune on in-domain training data of the TEACh dataset, to get object types O t = {o 0 , o 1 , o 2 , ..., o k } and semantic mask M t = {m 0 , m 1 , m 2 , ..., m k } from egocentric RGB input v t at time stamp t.Then we use a Unet (Ronneberger et al., 2015) based depth prediction model as Blukis et al. (2021) to predict the depth of each pixel of the current egocentric image frame.Then, by combining the agent location and camera parameters, we transform the visual information into symbolic information: if a certain object exists in a 3D space, which we store in a 3D voxel.Then we further project the 3D voxel along the height dimension into a 2D map and obtain more concise 2D symbolic information.So far, we have transformed the egocentric RGB image into a series of 2D semantic maps, among which each map records a certain object's spatial location in the projected 2D plane.This symbolic environment information will be maintained and updated during the task completion process as in Figure .2.</p>
<p>Action Execution via Symbolic Commonsense Reasoning</p>
<p>Once the sub-goal sequence and semantic world representation are obtained, the agent must generate executable actions.However, these inputs may be noisy-sub-goals can be misordered or infeasible, and the semantic map may be incomplete.To address this, we introduce a Symbolic Reasoning module that leverages task-level and action-level commonsense logic to validate and refine execution plans.Sample logic predicates are listed in Table 5 (Appendix A).</p>
<p>Task-level reasoning enforces logical preconditions between sub-goals.For instance, the predicate P ick(agent, x) holds only if x is movable and the agent is free-handed.Sub-goals violating such constraints are revised-e.g., inserting "PickUp knife" before "Slice bread."This process assumes ideal execution and updates the agent's internal state accordingly.</p>
<p>Given validated sub-goals and the semantic map, we employ two action generation methods.If the target is visible, the Fast Marching Method (FMM) (Sethian, 1999) plans a path to the nearest empty space near the object.Otherwise, we use a Goal Transformer (GT), adapted from the Episodic Transformer (Pashevich et al., 2021), trained on TEACh data.GT predicts the next action from past observations V 0:tâˆ’1 , actions A 0:tâˆ’1 , and subgoals G 0:tâˆ’1 : Ã¢t = GT([V 0:tâˆ’1 , A 0:tâˆ’1 , G 0:tâˆ’1 ]).</p>
<p>(3)</p>
<p>Action-level reasoning ensures that planned actions respect environmental constraints.For example, M ove(x, y) is valid only if a path exists from y to x.If constraints are violated, the agent adapts-e.g., executing a fallback policy like random exploration or navigating to the nearest feasible location.</p>
<p>Experiments</p>
<p>Dataset and Tasks</p>
<p>We evaluate JARVIS on the TEACh dataset (Padmakumar et al., 2021), which contains over 3,000 human-human interaction sessions involving household tasks.Each session begins from an initial state s i , proceeds through a multi-turn dialogue D between a Commander and a Follower, and ends in a final state s f after executing a reference action sequence A = {a 0 , a 1 , . . .}.These sessions form the basis for three evaluation settings: Execution from Dialog History (EDH) requires the agent to complete part of a task by predicting future actions A t:T , given the current state s i , dialogue history D, and previous actions A 0:tâˆ’1 .Success is determined by whether the final simulated state Å matches the reference s f .Given an initial instruction, the Follower communicates with the Commander to complete the task.The Commander has oracle access to environment metadata via three APIs: Pro-gressCheck, which lists task-relevant state differences; SelectOid, for querying object identifiers; and SearchObject, for locating objects.Following TEACh conventions, we implement JARVIS as two interacting agents: the Commander uses a Task-Level Commonsense Module to issue instructions, while the Follower executes actions and queries for help via an Action Execution Module.</p>
<p>Experimental Setup</p>
<p>Evaluation Metrics We adopt Success Rate (SR), Goal-Condition Success (GC), and Trajectory Weighted Metrics (TLW) as evaluation metrics.Task success is a binary value, defined as 1 when all the expected state changes s f are presented in Å otherwise 0. SR is the ratio of success cases among all the instances.GC Success is a fraction of expected state changes in s f present in Å which is in (0, 1) and the GC success of the dataset is the average of all the trajectories.Trajectory weighted SR (TLW-SR) and GC (TLW-GC) are calculated based on a reference trajectory A R and a predicted action sequence A. The Table 4: Ablation studies on the EDH and TfD validation sets.Language: the agent directly teleports to the target goals generated by the language language planning module, thus trajectory length weighted metrics do not make sense here.Executor: the agent will use the ground truth sub-goals.Reasoning: the agent use both ground truth sub-goals and visual information.No Goal Transformer: the agent explore random place when it did not find the target object.</p>
<p>EDH TfD</p>
<p>Seen Unseen Seen Unseen trajectory length weighted metrics for metric value m can be calculated as</p>
<p>Model SR [TLW] GC [TLW] SR [TLW] GC [TLW] SR [TLW] GC [TLW] SR [TLW] GC
TLW-m = m * |A R | max |A R | , A(4)
In our evaluation, we use m = 1, and calculate the weighted average for each split by using
|A R | N i=1 |A i R |
as the weight of each instance.Following evaluation rules in TEACh (Padmakumar et al., 2021), the maximum number of action steps is 1,000 and the failure limit is 30.</p>
<p>Baseline We select Episodic Transformer (E.T.) (Padmakumar et al., 2021) as the baseline method.E.T. achieved SOTA performance on the TEACh dataset.It learns the action prediction based on the TEACh dataset using a ResNet-50 (He et al., 2016) backbone to encode visual observations, two multi-modal transformer layers to fuse the embedded language, visual and action information, and output the executable action directly.</p>
<p>Main Results and Analysis</p>
<p>EDH We establish the state-of-the-art performance with a large margin over the baseline E.T.model.Our model achieves a higher relative performance than baselines on trajectoryweighted metrics, as shown in Table 1, suggesting that adding symbolism can also reduce navigation trajectory length.Besides, our framework has less performance gap between seen and unseen environments, showing better generalizability in new environments.We find that E.T. usually gets stuck in a corner or keeps repeatedly circling, while our model barely suffers from those issues, benefiting from neuro-symbolic commonsense reasoning.</p>
<p>TfD Our model achieves state-of-the-art performance across all metrics in TfD tasks, as shown in Table 1, which shows that JARVIS has better capability for task execution based on offline human-human conversation.Compared with EDH tasks, TfD tasks provide only the entire dialog history for the agent, which increases the difficulty and causes performance decreases in both models, while our JARVIS still outperforms the E.T. by a large margin, showing that the end-to-end model can not learn an effective and generalized strategy for long tasks completion providing only dialog information.</p>
<p>TATC We evaluate our framework on the TATC task with a Commander under three distinct constraint levels, simulating varied human assistance.The Commander is provided with: 1) only the current sub-goal; 2) the sub-goal and target object location; or 3) the sub-goal, location, and segmentation mask (the original TATC setting Padmakumar et al. (2021)).As shown in Table 2, JARVIS greatly outperforms the only open-source baseline, which fails on all TATC instances, highlighting the task's complexity for end-to-end methods.Furthermore, the Success Rate (SR) improves as the Commander is given more information, demonstrating that JARVIS effectively adapts its instructions to leverage all available knowledge.</p>
<p>Few-Shot Learning</p>
<p>Data scarcity has been known as a severe issue for deep neural methods.Especially, it is even more severe in language-involved embodied agent tasks since collecting training data is more expensive and time-consuming.Here we also conduct experiments in the few-shot setting, shown in Table 1.We randomly sample ten instances from each of the 12 types of household tasks in the TEACh dataset and train the language understanding and planning module and Goal Transfomer in the same way as the whole dataset setting.For E.T., we notice a significant performance drop on both EDH and TfD (e.g., 0 success rate in TfD tasks), since it overfits and can not learn effective and robust strategy.Since our framework breaks down the whole problem into smaller sub-problems and incorporates a solid symbolic commonsense reasoning module, it still has the ability to complete some complex tasks.This also indicates the importance of connecting connectionism and symbolism.</p>
<p>Unit Test of Individual Module</p>
<p>To analyze the performance gain of JARVIS, we conduct ablation studies on EDH and TfD tasks (Table 4).With ground truth perception and sub-goals, the Symbolic Commonsense Reasoning module achieves over 60% success in EDH, showing its effectiveness in action inference when provided accurate inputs.Replacing our language planner with ground truth yields greater improvement than replacing the executor, confirming the importance of symbolic reasoning in short-horizon tasks.</p>
<p>In longer TfD tasks, where a single sub-task failure leads to overall task failure, executor quality becomes the bottleneck.Here, replacing our executor with a teleport agent boosts performance more than using ground truth sub-goals.We also observe that our Goal Transformer improves trajectory-weighted success, indicating more efficient action planning.</p>
<p>Conclusion</p>
<p>This work studies how to teach embodied agents to execute dialog-based household tasks.We propose JARVIS, a neuro-symbolic framework that can incorporate the perceived neural information from multi-modalities and make decisions by symbolic commonsense reasoning.Our framework outperforms baselines by a large margin on three benchmarks of the TEACh (Padmakumar et al., 2021) dataset.We hope the methods and findings in this work shed light on the future development of neuro-symbolic embodied agents.</p>
<p>For the depth prediction model, we use an Unet-based model architecture same as Blukis et al. (2021).We get the images and ground truth depth frames from the training environments of TEACh dataset by AI2THOR Kolve et al. (2017) too.The range of depth prediction is from 0-5 meters, with an interval of 5 centimeters.Thus, the depth prediction problem is formulated as a classification problem over 50 classes on each pixel.During training, the loss function is a pixel-wise cross entropy loss:
L = CE(D predict , D gt )(6)
D gt stands for ground truth depth and D predict stands for predicted depth.During training, we follow the training details in Blukis et al. (2021) and using the batch size of 4, learning rate of 1Ã—10 âˆ’4 .We train for 4 epochs and select by the best performance on data in unseen validation environments.</p>
<p>For semantic map construction, we first project the depth and semantic information predicted by learned models into a 3-D point cloud voxel, based on which we do vertical projection and build a semantic map of 240 Ã— 240 for each object class and obstacle map.Each pixel represents a 5cm Ã— 5cm patch in the simulator.</p>
<p>Goal Transformer</p>
<p>We collect one data sample from each EDH instance to train the Goal Transformer.The Goal Transformer takes the ground truth future sub-goals, history images, and history actions as inputs.We modify the prediction head of the Episodic Transformer to generate the actions in TEACh benchmarks Padmakumar et al. (2021).The Goal Transformer uses a language encoder to encode the future sub-goals, an image encoder (a pre-trained ResNet-50) to encode the current and history images, and an action embedding layer to encode the history actions in order.The model is trained to predict future actions autoregressively.</p>
<p>During training, we use the cross entropy loss between the ground truth future action A gt and predicted future action, as is A predict :
L = CE(A predict , A gt ) (7)
We follow the episodic transformer (E.T.) Pashevich et al. (2021) training details for the goal transformer model.The batch size is 8 and the training epoch is 20.We use the Adamw optimizer Loshchilov and Hutter (2017) with a learning rate of 1 Ã— 10 âˆ’4 .</p>
<p>Symbolic Reasoning</p>
<p>Here, we provide some details of the implementations of the symbolic reasoning part.In task-level commonsense reasoning, we mainly check sub-goals from two perspectives: properties and causality.For properties, we need to check whether the action is affordable for the object.Therefore, we define some object collections depending on the properties, including movable, sliceable, openable, toggleable, and supportable.We can determine the unreasonable sub-goals by checking whether the planned object can afford the corresponding action.Then, causality means whether the sub-goal sequence obey the causal relations, like "a knife in hand" should always be the prerequisite of "slice a bread".To achieve this purpose, we define some rules of prerequisites and solutions according to commonsense, including placing the in-hand object before picking something, removing the placing action if nothing in hand, etc.To check the prerequisites, we assume all the previous sub-goals are completed and check if the agent's states matches the desired states.If the current sub-goal and the agent's state have causal conflict, we will use predefined solutions to deal with it.</p>
<p>For example, we will remove Place(agent, x) if there is nothing in the hand.We will add Pick(agent, "Knife") before Slice(agent, "Bread") if the agent do not grasp a knife in hand.</p>
<p>The whole process can be found in Algorithm 2. We consider both the semantic map and action states for action-level commonsense reasoning.In general, the agent updates the semantic map according to the observation of every step.We use the pixels change to detect whether the previous action success.To determine the next step, the agent must check whether the target object has been observed.If the target object has been observed, an FMM algorithm will plan a path to the closest feasible space.Otherwise, we will use the Goal Transformer to determine the next action for exploration.The action space of the Goal Transformer is all motion actions, including forwarding, backward, panning left, panning right, turning right, and turning left.During the movement to the target position, if the agent counterfaces unexpected collisions due to the error of the semantic map, the agent will save the current pose and the action from causing the collision.Then, the agent will first consider using the estimated motion action from Goal Transformer.But if the output action of GT is the same as the previous one leading to the collision, it will take a random motion action.After the agent reaches the target position, the agent will rotate and move around the place if the target object cannot be found in the current observation.If all attempts have been made and the object is still unobservable, the agent will consider it as a false detection situation and add the corresponding signal in the semantic map.The whole process can be found in Algorithm 3.</p>
<p>Task Completion Process of the JARVIS framework</p>
<p>We elaborate logic predicates for symbolic commonsense reasoning in Table 5.In Algorithm 1, we show the overall process when our JARVIS framework tries to finish an EDH or TfD tasks, which includes two algorithms to implement symbolic commonsense reasoning predicates: Algorithm.2 describes semantic map building process and language planning process, which including the task-level commonsense reasoning.Algorithm.3 describes the detailed action generation process with action-level commonsense reasoning.</p>
<p>Experiment Details for Two Agent Task Completion (TATC)</p>
<p>We experiment with our JARVIS framework on TATC task in three different settings, where there are different constraints about how much information will be available to the Commander for generating instructions.As in Table .2, in the first setting named full info.setting, the Commander has all information about the current subgoal, eg.pickup knife, target object location and the ground truth segmentation of the target object in the view.In this setting, same as the setting in Padmakumar et al. (2021), the Commander can specifically instructs where the Follower should interact to finish the current subgoal.In the second setting, we eliminate the ground truth segmentation of the target object from the information provided to the Commander.As a result, the Commander could still instruct the Follower to arrive near the target object but will not be able to explicitly tell the The lettuce is in the fridge.</p>
<p>Please now cook a slice of potato.
1 2 3
Where is the potato?The potato is in the fridge.Other frequent reasons for failure are 'Cannot place without holding any object,' 'Hand occupied when picking up,' and 'Knife not in hand when cutting.'These are because of the wrong judgment of whether the former 'Place' and 'Pickup' actions have succeeded.For example, the agent might think it has picked up the knife, while it failed to pick up in fact,</p>
<p>Semantic map in progress:</p>
<p>2 3Figure 1 :
21
Figure1: Dialogue-based embodied navigation and task completion.The Commander (often a human) issues a task such as making a sandwich, and the Follower agent completes the task while communicating with the Commander.Unlike the agent in finegrained instruction following tasks, the Follower agent needs to extract sub-goals from the free-form dialogue and execute actions in the visual environment.Note that the Follower agent can only navigate and interact with objects in an egocentric view and has no access to the map or other oracle information.</p>
<p>Fine</p>
<p>Trajectory from Dialog (TfD) tasks the agent with reconstructing the entire action sequence A 0:T from the complete dialogue D and initial state s i , aiming to reach the target final state s f .Two-Agent Task Completion (TATC) models both Commander and Follower roles.</p>
<p>Figure 3: EDH example.(a) shows an example of our JARVIS in EDH task, where the inputs are dialog history and sub-goal history (converted from action history input).The inputs are first interpreted by the Language Parsing Module to become sub-goals.Then, our Symbolic Reasoning Module will generate action predictions.The predicted actions will change the follower's egocentric views and the semantic map will be built up and completed gradually.} m 1 shows the agent is opening the fridge } m 2 shows the agent has placed the knife and navigate back to the fridge.} m 3 shows the agent is picking up the potato.(b) is an example demonstrating a typical way of how Episodic Transformer fails on EDH task.In this case, the E.T. model predicts "Forward" repetitively even facing the wall, therefore stuck at the current position.</p>
<p>Figure 6 :
6
Figure 6: Action failure examples</p>
<p>The object is in the map The object is not in the map Symbolic Reasoning Module
Step 0.2: Train Goal TransformerGoal TransformerifGoal TransformerFast Marching MethodAction Common Sense</p>
<dl>
<dt>Semantic World Representation The location of the agent The location of the target object Pretrained LLM Language Planning Module Future Sub-goals <action> <object> Sub-goals</dt>
<dd>
<p>: &lt; .&gt; &lt; .&gt;</p>
</dd>
</dl>
<ol>
<li>Slice bread 2. Slice Potato 3. â€¦ Sub-goals : â€² 1.1 Navigate Knife 1.2 Pick up Knife 1.3 Navigate Bread 1.4 Slice Bread 2.1 Navigate Potato 2.2 Slice Potato 2. â€¦ Dialogue History Sub-goals :âˆ’ State S</li>
</ol>
<p>Table 1 :
1
Results in percentages on the EDH and TfD validation sets, where trajectory length weighted (TLW) metrics are included in[ brackets ].For all metrics, higher is better.
EDHTfDSeenUnseenSeenUnseenModelSR [TLW] GC [TLW] SR [TLW] GC [TLW] SR [TLW] GC [TLW] SR [TLW] GC [TLW]E.T. (Padmakumar et al., 2021)8.4 [0.8]14.9 [3.0]6.1 [0.9]6.4 [1.1]1.0 [0.2]1.4 [4.8]0.5 [0.1]0.4 [0.6]Ours15.1 [3.3] 22.6 [8.7] 15.8 [2.6] 16.6 [8.2]1.7 [0.2]5.4 [4.5]1.8 [0.3]3.1 [1.6]E.T. (few-shot) 26.1 [1.0]4.7 [2.8]6.0 [0.9]4.8 [3.6]0.0 [0.0]0.0 [0.0]0.0 [0.0]0.0 [0.0]Ours (few-shot)10.7 [1.5] 15.3 [5.3] 13.7 [1.7] 12.7 [5.4]0.6 [0.0]3.6 [3.0]0.3 [0.0]0.6 [0.1]</p>
<p>Table 2 :
2
Success Rate results on the TATC task with different assumptions of the Commander agent.Our JARVIS establishes the best performance among the current implemented methods.
w/ full info.w/o GT seg.w/o GT &amp; goal loc.ModelSeenUnseenSeenUnseenSeenUnseen</p>
<p>Table 3 :
3
Success Rates of 12 different task categories in the validation set.For EDH, the results are based on relatively shorter EDH instances, while the results for TdD and TATC tasks are from instances with full trajectories.
Plant Coffee Clean All X Y Boil Toast N Slices X One Y Cooked Sndwch Salad BfastEDH 21.0 21.3 14.515.2 15.5 12.822.119.615.815.5 10.3 14.0TfD13.56.7 6.</p>
<p>What should I do today?We will make a salad.Please begin with 2 slices of lettuce.I have sliced the lettuce.Navigate Knife, Pickup Knife, Navigate Fridge, Open Fridge, Navigate CounterTop, Place CounterTop, Navigate Lettuce, Pickup Lettuce, Navigate CounterTop, Place.CounterTop, Pickup Knife, Navigate Lettuce, Slice Lettuce, Close Fridge
History dialogue:Sub-goals:CommanderFollowerNavigate FridgeOpen FridgeNavigate CounterTopPlace CounterTopNavigate PotatoHistory sub-goals:Pickup PotatoNavigate FridgeClose FridgeInitial egocentric views:
â€¦HelloWhere is the lettuce?</p>
<p>Table 8 :
8
Action failure categorization result on a sub-set of validation set for all three tasks.The numbers show the rate of a certain failure action occurs among the total action failure time.
Action failure reasonEDH(%) TfD(%) TATC(%)Hand occupied when picking up3.201.461.04Knife not in hand when cutting3.121.821.36Target object not support open/close0.210.360.83Cannot open when running0.070.800.31Blocked when moving40.8140.7447.34Collide when rotating5.683.570.63Collide when picking up0.140.000.10Invalid position for placing the held object9.377.944.28Too far to interact8.5210.134.28Pouring action not available for the held object1.131.820.10Object not found at location or cannot be picked up21.5019.3928.26Cannot place without holding any object6.1711.308.03Collide when placing0.000.000.21Target receptacle full when placing0.070.663.23
. Our reimplementation version ofPadmakumar et al. (2021).
. From the official TATC Challenge: https://github.com/GLAMOR-USC/teach_tatc. At the time of submission, the E.T. baseline fails on all the TATC instances.
Appendix A. Implementation DetailsJARVIS takes advantage of both connectionism and symbolism.Here we first introduce the learning details of the deep learning modules in JARVIS, followed by the symbolic commonsense reasoning.Then we introduce the task completion process of the JARVIS framework, shown in the Algorithm 1.Learning ModulesHere we elaborate on the implementation details of the learning and symbolic reasoning modules in our framework, including language understanding and planning, semantic world representation, and goal transformer.Language Understanding and PlanningFor data collection of the EDH task, we collect one data sample from each EDH instance.In each data sample, the input contains a history dialogue between the commander and the follower and history sub-goals that have been executed, and the output is the future subgoals.We create special tokens &lt; COM &gt;, &lt; FOL &gt; to concatenate different utterances of the dialogue, and &lt; HIS &gt; to concatenate the history sub-goals and dialogue.Note that the history sub-goals are not available in the training set, so we translate the provided history actions into history sub-goals by excluding all the navigation.For the TfD task, we collect the whole dialog sequence and the whole ground truth future sub-goal sequence from each TfD instance as input and output for training.We collect the same data scheme from seen validation EDH/TfD instances for validation.We adapt the pre-trained BART-LARGELewis et al. (2020)model and fine-tune the BART model separately on the collected training data for each task.We follow the HuggingfaceWolf et al. (2019)implementation for the BART model as well as the tokenizer.The BART model is finetuned for 50 epochs and selected by the best validation performance.We use the Adam optimizer with a learning rate of 5 Ã— 10 âˆ’5 .The maximum length of generated subgoal is set to 300, and the beam size in beam search is set to 4.Semantic World RepresentationFor Mask-RCNN He et al. (2017), we use a model pre-trained onMSCOCO Lin et al. (2014)data and fine-tune it on collected data from training environments of TEACh dataset.We get the ground truth data samples from the provided interface of AI2THOR.During training, the loss function is as follows:Where L cls is the cross entropy loss of object class prediction.L box is a robust L 1 loss of bounding box regression.And L mask is the average binary cross entropy loss of mask prediction, where the neural network predict a binary mask for each object class.FollowingShridhar et al. (2021), we use the batch size of 4 and learning rate of 5 Ã— 10 âˆ’3 for Mask-RCNN training.We train for 10 epochs and select by the best performance on data in unseen validation environments.follower the ground truth location of the target object in the view.In the third setting, we further restrict the target object location (goal location) from the information to the Commander and thus the instruction generated from the Commander will not include any ground truth information about where the target object is.Appendix B. Notation TableWe describe the notation used in this paper in Table.6.Appendix C. Case StudyIn the EDH task, the agent can process history dialog and execute sub-goals, and plan future actions to complete the task.With well-designed Symbolic Commonsense Reasoning module, the agent can efficiently navigate to the target location and execute planned actions, as shown in Figure.3. In the TfD task, the agent is able to understand the dialog and break down the whole task into future sub-goals, and execute it correctly, as in Figure.4. We also show an example of one classic error of episodic transformer in 3(b).The E.T. model will repetitively predict "Forward" when facing the wall, or "Pickup Place" in some other cases.This is because the agent can not correctly and robustly infer correct actions from all the input information.Figure.5 illustrates how our JARVIS framework can be adapted to the TATC task under the setting that the commander can acquire state changes needed to be complete,   Where is the cloth Commander FollowerSemantic map in progress:Figure4: Successful TfD example from our JARVIS framework.According to the dialog, the language planner estimate four future sub-goals: ("Navigate cloth", "PickUp Cloth", "Navigate Bathtub", "Place Bathtub").Then with the symbolic reasoning module, interaction and navigation actions are predicted.} m 1 shows the agent is finding the cloth.} m 2 shows the agent has picked up the cloth and then found the bathtub.} m 3 shows the agent can correctly put the cloth on the bathtub.Table7: Average action failure rate on validation set for EDH, TfD and TATC tasks.When the time of action failure in a session reaches 30, the session will be forced to end, causing a task failure.The action failure exist widely in sessions.Failure Mode EDH(%) TfD(%) TATC(%)No action failure 4.0 17.0 5.8 Action failures exist but are less than 30 times 78.8 79.9 69.0 Action failures reaches 30 times 17.2 3.1 25.2Appendix D. Error AnalysisAccording to Table.7, we find that all failed tasks include at least one action failure.For the shorter tasks, like EDH, the dominant situation is "Action failures are less than 30 times", while the longer tasks, like TfD and TATC, include even more action failures due to more sub-goals.To this aspect, we further analyze the action failure reason as in Table.8, which shows a quantitative analysis of the failed actions categories in the validation set.We randomly sampled 50 failure episodes in both seen and unseen splits and computed the average ratio of each action failure category in all the failed actions.From Table.8, we notice that 'Blocked when moving' is the most frequent cause of failure action in all the three tasks, EDH, TfD, and TATC.The main reason is that errors in depth prediction cause semantic obstacle map errors.Besides, 'Too far to interact' is also mainly caused by wrongly predicting the 3D location of an object.To solve this, we need to improve the precision of the depth prediction model or design a more delicate and robust algorithm to update the semantic map.The second frequent failure action is 'Object not found at the location or cannot be picked up,' which could be due to the wrong predicted location of an object by Mask R-CNN.Moreover, it is also likely caused by the situation where the 1 : Navigate to Faucet @ (3, 5.25, 270Â°)), then uses ground-truth segmentation to compute an interaction target (e.g.} m 3 : Pickup Mug @ (0.7, 0.37)).Steps 1-6 repeat for Faucet, Sink, Mug, Sink, Faucet, and Mug; step 7 pours water on the Plant @ (0.55, 0.6).After each sub-goal the follower executes in its egocentric view and reports completion.target object is inside a receptacle that needs to be open first, as inFigure. 6(a).'Invalid position for placing the held object' is also mainly caused by the segmentation error of Mask  which causes the latter 'Knife not in hand when cutting.' We need to further refine our Symbolic Commonsense Reasoning Action module for these cases.Instead of the failed actions, a miss-recognized object will also lead to an unsuccessful task.As in Figure6(b), the tomato is falsely recognized as an apple, which leads to a failure when the target object is about "Tomato".Additionally, the false sub-goals estimations also contribute to the failures in task completion.In Section 4, we quantitatively analyze the performance of the Language Planning Module in the JARVIS framework.Here, we show a typical qualitative error result in Fig.7.According to the dialog, the ground truth future sub-goals need the follower to put all potatoes on the plate.However, the estimated sub-goals indicate one slice of potatoes and tomatoes will be put on the plate, which will cause the unsatisfied state changes.
Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko SÃ¼nderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)June 2018</p>
<p>The robotslang benchmark: Dialogguided robot localization and navigation. Shurjo Banerjee, Jesse Thomason, Jason J Corso, of Proceedings of Machine Learning Research. Jens Kober, Fabio Ramos, Claire J Tomlin, Cambridge, MA, USAPMLRCoRL 2020, 16-18 November 2020. 20201554th Conference on Robot Learning</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi, 5th Annual Conference on Robot Learning. 2021</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2019</p>
<p>History aware multimodal transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev, NeurIPS2021a</p>
<p>Grounding physical concepts of objects and events through dynamic visual reasoning. Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee K Wong, Joshua B Tenenbaum, Chuang Gan, International Conference on Learning Representations. 2021b</p>
<p>Plug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, International Conference on Learning Representations. 2020</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaJune 20191Association for Computational Linguistics</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>Speakerfollower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, NeurIPS2018</p>
<p>Iqa: Visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)June 2018</p>
<p>Vision-and-language navigation: A survey of tasks, methods, and future directions. Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, Xin Wang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Airbert: In-domain pretraining for vision-and-language navigation. Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)October 2021</p>
<p>Neural module networks for reasoning over text. Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner, International Conference on Learning Representations. 2020</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 10.1109/CVPR.2016.902016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016</p>
<p>Mask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Oct 2017</p>
<p>Landmark-rxr: Solving vision-and-language navigation with fine-grained alignment supervision. Keji He, Yan Huang, Qi Wu, Jianhua Yang, Dong An, Shuanglin Sima, Liang Wang, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Vln bert: A recurrent vision-and-language bert for navigation. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2021</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, arXiv:2201.072072022arXiv preprint</p>
<p>Learning by abstraction: The neural state machine. Drew Hudson, Christopher D Manning, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F AlchÃ©-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Traversability classification using unsupervised on-line visual learning for outdoor robot navigation. Dongshin Kim, Jie Sun, Sang Min Oh, J M Rehg, A F Bobick, 10.1109/ROBOT.2006.1641763Proceedings. null2006. 2006. ICRA 2006. 2006</p>
<p>NDH-full: Learning and evaluating navigational agents on full-length dialogue. Hyounghun Kim, Jialu Li, Mohit Bansal, 10.18653/v1/2021.emnlp-main.518Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicNovember 2021Online and Punta CanaAssociation for Computational Linguistics</p>
<p>AI2-THOR: an interactive 3d environment for visual AI. Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, CoRR, abs/1712.054742017</p>
<p>Room-acrossroom: Multilingual vision-and-language navigation with dense spatiotemporal grounding. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)November 2020</p>
<p>BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, C Lawrence Zitnick, Computer Vision -ECCV 2014. David Fleet, Tomas Pajdla, Bernt Schiele, Tinne Tuytelaars, ChamSpringer International Publishing2014</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, Jiajun Wu, International Conference on Learning Representations. 2019</p>
<p>FILM: Following instructions in language with modular methods. So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov, International Conference on Learning Representations. 2022</p>
<p>Mapping instructions to actions in 3d environments with visual goal prediction. Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, Yoav Artzi, 10.18653/v1/D18-1287Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumOctober-November 2018</p>
<p>Collaborative dialogue in Minecraft. Anjali Narayan-Chen, Prashant Jayannavar, Julia Hockenmaier, 10.18653/v1/P19-1537Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. Khanh Nguyen, Hal DaumÃ©, Iii , 10.18653/v1/D19-1063Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Vision-based navigation with language-based assistance via imitation learning with indirect intervention. Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). June 2019</p>
<p>Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur, arXiv:2110.00534Teach: Task-driven embodied agents that chat. 2021arXiv preprint</p>
<p>Episodic transformer for visionand-language navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)October 2021</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. Ram Ramrakhya, Eric Undersander, Dhruv Batra, Abhishek Das, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>U-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, MICCAI. Oct 2015</p>
<p>proscript: Partially ordered scripts generation via pre-trained language models. Keisuke Sakaguchi, Chandra Bhagavatula, Le Ronan, Niket Bras, Peter Tandon, Yejin Clark, Choi, arXiv:2104.082512021arXiv preprint</p>
<p>James A Sethian, Fast marching methods. 199941</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2020</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>One step at a time: Long-horizon vision-and-language navigation with milestones. Hee Chan, Jihyung Song, Tai-Yu Kil, Brian M Pan, Wei-Lun Sadler, Yu Chao, Su, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2022</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. Licheng Hao Tan, Mohit Yu, Bansal, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language TechnologiesJune 20191</p>
<p>Vision-and-dialog navigation. Jesse Thomason, Michael Murray, Maya Cakmak, Luke Zettlemoyer, Conference on Robot Learning (CoRL). 2019</p>
<p>Talk2nav: Long-range visionand-language navigation with dual attention and spatial memory. Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool, 10.1007/s11263-020-01374-3Int. J. Comput. Vision. 0920-56911291jan 2021</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang, Wang , Lei Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2019</p>
<p>Environment-agnostic multitask learning for natural language grounded navigation. Eric Xin, Vihan Wang, Eugene Jain, William Yang Ie, Zornitsa Wang, Sujith Kozareva, Ravi, 10.1007/978-3-030-58586-0_25Computer Vision -ECCV 2020: 16th European Conference. Glasgow, UK; Berlin, HeidelbergSpringer-VerlagAugust 23-28, 2020. 2020Proceedings, Part XXIV</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, 2019</p>
<p>Visual semantic navigation using scene priors. Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi, 2019</p>
<p>Babywalk: Going farther in vision-and-language navigation by taking baby steps. Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, Fei Sha, 10.18653/v1/2020.acl-main.229Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020a</p>
<p>Vision-dialog navigation by exploring cross-modal memory. Yi Zhu, Fengda Zhu, Zhaohuan Zhan, Bingqian Lin, Jianbin Jiao, Xiaojun Chang, Xiaodan Liang, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2020b</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi, 10.1109/ICRA.2017.79893812017 IEEE International Conference on Robotics and Automation (ICRA). 2017</p>            </div>
        </div>

    </div>
</body>
</html>
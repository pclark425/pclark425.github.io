<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4430 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4430</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4430</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-276235403</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.05151v2.pdf" target="_blank">Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of"AI4Science".</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4430.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4430.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bilingual Evaluation Understudy (BLEU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram overlap automated metric originally for machine translation that measures similarity between generated and reference texts; used in the paper to assess similarity of LLM-generated hypotheses/ideas to gold hypotheses when available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU similarity to gold hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute BLEU scores between an LLM-generated hypothesis/idea and a gold (published) hypothesis or reference text, interpreting higher n-gram overlap as greater similarity/quality. Applied when ground-truth hypotheses or known discoveries exist to quantify how closely generation matches prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Textual similarity to reference (overlap of n-grams), which proxies fidelity to known hypotheses and (indirectly) reproducibility/consistency with prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-domain (used in AI and general scientific hypothesis evaluation where gold hypotheses exist)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypotheses/ideas (textual descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports BLEU has been used in works that compare generated hypotheses to known discoveries; no single numeric benchmark is given in the survey for BLEU on hypotheses, only that it is an adopted automated similarity metric when gold exists.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric (BLEU); typically complemented by human assessment when BLEU is insufficient to judge novelty, feasibility, or scientific value.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Implicit: comparison to gold references; validation depends on correlation studies with human judgments reported in the literature (not detailed in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>BLEU measures surface n-gram overlap and can miss semantic equivalence, paraphrases, novelty, and explanatory adequacy; it does not assess falsifiability, empirical adequacy, or mechanistic plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied where gold hypotheses/ground-truth discoveries are available (e.g., comparisons to published hypotheses in Nature/Science in cited works); no single standard benchmark for hypothesis BLEU is given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4430.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4430.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs-as-a-metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs-as-a-metric evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology that uses LLMs themselves to rate generated hypotheses across scientific aspects (novelty, relevance, significance, verifiability) or to aggregate automated judgments about hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLMs-as-a-metric (four-aspect rubric application)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an LLM (or an ensemble of LLMs) to score or assess generated hypotheses on predefined scientific aspects — novelty, relevance to background, significance to the community, and verifiability/testability — producing scalar or categorical judgments. Can be used to rank or filter candidate hypotheses automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty (degree of newness relative to existing literature), relevance (fits the provided background/problem), significance (expected impact within the field), verifiability/testability (whether the hypothesis is empirically testable).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain (applied in AI and broader scientific hypothesis assessment in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypotheses/ideas/evidence-grounded explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey cites Qi et al. leveraging LLMs-as-a-metric to evaluate hypotheses on the four aspects; results note feasibility of automated scoring but do not report a unified numeric performance; the approach is presented as promising for large-scale pre-screening.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-based) evaluation and thus scalable; typically recommended to be complemented by human expert review for final decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Discussed validation approaches include comparing LLM scores to human expert judgments or to gold labels where available; however, the survey reports that validation is an open challenge and that correlation with expert ratings is an important validation step (not always performed).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM evaluators can inherit hallucinations and biases, may conflate novelty with irrelevance, and can be overly optimistic or inconsistent; lack of standardized prompts and calibration leads to variability; correlation with human experts is not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>No single benchmark; applied in contexts where gold evaluations exist or via human-labeled datasets for the four aspects when available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4430.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4430.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ground-truth comparison (Nature/Science)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ground-truth published-hypothesis comparison pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline used in cited work that retrieves related literature then compares LLM-generated hypotheses to published ground-truth hypotheses (e.g., from Nature or Science) to measure similarity/overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Ground-truth hypothesis comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>First search for and retrieve related works, feed those and the research question into an LLM to generate hypotheses, then quantitatively and/or qualitatively compare generated hypotheses to gold hypotheses published in top journals (e.g., using similarity metrics and human judgment) to assess how many generated hypotheses recover or mirror known scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Degree of textual/semantic overlap with published hypotheses, recovery rate of known discoveries, and qualitative match judged by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>varied — applied in experiments comparing to published hypotheses in general science (Nature/Science examples in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>proposed scientific hypotheses (claims about phenomena/mechanisms)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports that Yang et al. used such a pipeline and found that many LLM-generated hypotheses had a very high degree of similarity to ground-truth hypotheses published in Nature and Science (no single numeric aggregate reported in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated retrieval + automated similarity metrics for large-scale screening, plus human expert validation for final assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by direct comparison to published 'gold' hypotheses and by human expert judgments on similarity/quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Recovers known work more readily than producing genuinely novel, previously unknown valid hypotheses; success can reflect training-set contamination (models memorizing published results); similarity does not imply scientific correctness or testability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Gold hypotheses taken from top-tier journals (Nature, Science) in the cited experimental setup; no public standardized dataset for this specific pipeline is provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4430.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4430.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert (domain-expert) judgment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual evaluation by domain experts who judge generated hypotheses/ideas on testability, novelty, feasibility, and scientific value when no gold reference exists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert evaluation (qualitative and scored assessments)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Experts read LLM-generated hypotheses or ideas and rate them along multiple dimensions (e.g., testability, novelty, feasibility, significance), often using Likert scales, binary decisions, or ranking tasks; experts may also provide qualitative feedback and possible experimental designs to test hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Testability/verifiability, novelty, feasibility/practicality, significance/impact, alignment with background literature, and potential for empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>applied across domains (NLP, engineering, physics, chemistry, social science, medicine) as discussed in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypotheses, research ideas, mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey summarizes many works using human expert ratings; examples include Qi et al. showing few-shot LLM outputs judged more testable than zero-shot; other studies report human experts finding LLM ideas more novel but slightly less feasible than human ideas (cited large study).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based; often used as the gold standard; sometimes combined with automated pre-screening.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-rater agreement statistics (when reported) and comparison of aggregate expert ratings with automated metrics; validation chiefly via consistency across experts and correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Expensive and slow; subjectivity and inter-expert variability can be high; scaling to large numbers of candidates is impractical; experts can be biased and influenced by presentation and framing.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Typically ad hoc datasets assembled per study (expert-annotated hypothesis collections); no single universal expert-judged benchmark is reported in the survey for hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4430.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4430.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Novelty/Feasibility human study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale human vs LLM idea comparison study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study (cited as [206] in the survey) comparing ideas produced by humans and LLMs, reporting that LLMs generate ideas judged more novel but slightly less feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human comparative judgment of novelty and feasibility</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect idea/hypothesis samples from both human researchers and LLMs; have human raters (often domain experts or structured survey participants) rate each item for novelty and feasibility (and possibly other dimensions); compare distributions and statistically test differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty (degree of novelty relative to existing knowledge) and feasibility (practicality and likelihood of successful empirical validation); sometimes additional criteria like significance or clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>broad / multi-domain (the cited study was large-scale across research ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>research ideas and hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports the large-scale study found LLMs produce ideas judged to be more novel but slightly less feasible than human-generated ideas; no precise numeric effect sizes are given in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based comparative evaluation (statistical analysis of human ratings across source types).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Statistical comparison (e.g., significance tests) of rater scores across human vs LLM items; validation via sample sizes and rater pools (detailed validation procedures reported in the original study, not reproduced in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Rater bias, evaluation framing effects, ambiguous definitions of novelty and feasibility, and possible contamination of LLM outputs with memorized recent discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Study-specific collection of human and LLM-generated ideas; not a publicly standardized benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4430.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4430.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Truthfulness & hallucination benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Truthfulness and hallucination benchmark suite (TruthfulQA, HaluEval, FELM, SelfAware, FreshQA, Pinocchio, TrustLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of benchmarks and datasets cited in the survey designed to measure hallucination, truthfulness, knowledge-limits awareness, and adaptability of LLMs — dimensions relevant when evaluating the factual and explanatory reliability of generated scientific hypotheses/explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Truthfulness / hallucination benchmarking (TruthfulQA, HaluEval, FELM, SelfAware, FreshQA, Pinocchio, TrustLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply specialized benchmark datasets that probe model tendencies to produce falsehoods, fabricated references, or overconfident claims, as well as tests of being aware of knowledge limits and freshness of information; TrustLLM aggregates multiple such datasets to evaluate truthfulness, safety, fairness, robustness, privacy, and machine ethics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Factual accuracy, rate of hallucinations/fabrications, calibration/uncertainty awareness, freshness/adaptability to new information, safety and fairness-related dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general but directly applicable to scientific hypothesis/explanation generation across domains because truthfulness and non-hallucination are domain-agnostic requirements</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanations, factual claims, and hypotheses (where factual grounding matters)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey notes that proprietary LLMs generally outperform most open-source models on TrustLLM, but both types struggle when relying solely on internal knowledge; performance improves substantially with external grounding. No single numeric summary is provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated benchmarking (dataset-based testing) often supplemented by human evaluation for nuanced cases.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarks validated by curated questions, human annotations of truth/falsity, and in the case of TrustLLM, aggregation across multiple datasets; correlation with human judgments is used to assess benchmark usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Benchmarks often focus on general factuality and may not capture scientific testability, mechanistic plausibility, or falsifiability; domain-specific ground-truth evidence can be required for rigorous scientific evaluation; benchmark scope and coverage vary.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Includes TruthfulQA, HaluEval, FELM, SelfAware, FreshQA, Pinocchio, and the aggregated TrustLLM suite as described in the survey; each targets different facets of truthfulness and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis Generation with Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models <em>(Rating: 2)</em></li>
                <li>Improving the Peer Review of Narrative Literature Reviews <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4430",
    "paper_id": "paper-276235403",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "BLEU",
            "name_full": "Bilingual Evaluation Understudy (BLEU)",
            "brief_description": "An n-gram overlap automated metric originally for machine translation that measures similarity between generated and reference texts; used in the paper to assess similarity of LLM-generated hypotheses/ideas to gold hypotheses when available.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "BLEU similarity to gold hypotheses",
            "evaluation_method_description": "Compute BLEU scores between an LLM-generated hypothesis/idea and a gold (published) hypothesis or reference text, interpreting higher n-gram overlap as greater similarity/quality. Applied when ground-truth hypotheses or known discoveries exist to quantify how closely generation matches prior work.",
            "evaluation_criteria": "Textual similarity to reference (overlap of n-grams), which proxies fidelity to known hypotheses and (indirectly) reproducibility/consistency with prior work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "cross-domain (used in AI and general scientific hypothesis evaluation where gold hypotheses exist)",
            "theory_type": "hypotheses/ideas (textual descriptions)",
            "human_comparison": null,
            "evaluation_results": "Survey reports BLEU has been used in works that compare generated hypotheses to known discoveries; no single numeric benchmark is given in the survey for BLEU on hypotheses, only that it is an adopted automated similarity metric when gold exists.",
            "automated_vs_human_evaluation": "Automated metric (BLEU); typically complemented by human assessment when BLEU is insufficient to judge novelty, feasibility, or scientific value.",
            "validation_method": "Implicit: comparison to gold references; validation depends on correlation studies with human judgments reported in the literature (not detailed in survey).",
            "limitations_challenges": "BLEU measures surface n-gram overlap and can miss semantic equivalence, paraphrases, novelty, and explanatory adequacy; it does not assess falsifiability, empirical adequacy, or mechanistic plausibility.",
            "benchmark_dataset": "Applied where gold hypotheses/ground-truth discoveries are available (e.g., comparisons to published hypotheses in Nature/Science in cited works); no single standard benchmark for hypothesis BLEU is given in the survey.",
            "uuid": "e4430.0",
            "source_info": {
                "paper_title": "Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLMs-as-a-metric",
            "name_full": "LLMs-as-a-metric evaluation framework",
            "brief_description": "A methodology that uses LLMs themselves to rate generated hypotheses across scientific aspects (novelty, relevance, significance, verifiability) or to aggregate automated judgments about hypothesis quality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "LLMs-as-a-metric (four-aspect rubric application)",
            "evaluation_method_description": "Prompt an LLM (or an ensemble of LLMs) to score or assess generated hypotheses on predefined scientific aspects — novelty, relevance to background, significance to the community, and verifiability/testability — producing scalar or categorical judgments. Can be used to rank or filter candidate hypotheses automatically.",
            "evaluation_criteria": "Novelty (degree of newness relative to existing literature), relevance (fits the provided background/problem), significance (expected impact within the field), verifiability/testability (whether the hypothesis is empirically testable).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / cross-domain (applied in AI and broader scientific hypothesis assessment in cited works)",
            "theory_type": "hypotheses/ideas/evidence-grounded explanations",
            "human_comparison": null,
            "evaluation_results": "Survey cites Qi et al. leveraging LLMs-as-a-metric to evaluate hypotheses on the four aspects; results note feasibility of automated scoring but do not report a unified numeric performance; the approach is presented as promising for large-scale pre-screening.",
            "automated_vs_human_evaluation": "Automated (LLM-based) evaluation and thus scalable; typically recommended to be complemented by human expert review for final decisions.",
            "validation_method": "Discussed validation approaches include comparing LLM scores to human expert judgments or to gold labels where available; however, the survey reports that validation is an open challenge and that correlation with expert ratings is an important validation step (not always performed).",
            "limitations_challenges": "LLM evaluators can inherit hallucinations and biases, may conflate novelty with irrelevance, and can be overly optimistic or inconsistent; lack of standardized prompts and calibration leads to variability; correlation with human experts is not guaranteed.",
            "benchmark_dataset": "No single benchmark; applied in contexts where gold evaluations exist or via human-labeled datasets for the four aspects when available.",
            "uuid": "e4430.1",
            "source_info": {
                "paper_title": "Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Ground-truth comparison (Nature/Science)",
            "name_full": "Ground-truth published-hypothesis comparison pipeline",
            "brief_description": "A pipeline used in cited work that retrieves related literature then compares LLM-generated hypotheses to published ground-truth hypotheses (e.g., from Nature or Science) to measure similarity/overlap.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Ground-truth hypothesis comparison",
            "evaluation_method_description": "First search for and retrieve related works, feed those and the research question into an LLM to generate hypotheses, then quantitatively and/or qualitatively compare generated hypotheses to gold hypotheses published in top journals (e.g., using similarity metrics and human judgment) to assess how many generated hypotheses recover or mirror known scientific discoveries.",
            "evaluation_criteria": "Degree of textual/semantic overlap with published hypotheses, recovery rate of known discoveries, and qualitative match judged by experts.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "varied — applied in experiments comparing to published hypotheses in general science (Nature/Science examples in cited works)",
            "theory_type": "proposed scientific hypotheses (claims about phenomena/mechanisms)",
            "human_comparison": true,
            "evaluation_results": "Survey reports that Yang et al. used such a pipeline and found that many LLM-generated hypotheses had a very high degree of similarity to ground-truth hypotheses published in Nature and Science (no single numeric aggregate reported in survey).",
            "automated_vs_human_evaluation": "Hybrid: automated retrieval + automated similarity metrics for large-scale screening, plus human expert validation for final assessment.",
            "validation_method": "Validated by direct comparison to published 'gold' hypotheses and by human expert judgments on similarity/quality.",
            "limitations_challenges": "Recovers known work more readily than producing genuinely novel, previously unknown valid hypotheses; success can reflect training-set contamination (models memorizing published results); similarity does not imply scientific correctness or testability.",
            "benchmark_dataset": "Gold hypotheses taken from top-tier journals (Nature, Science) in the cited experimental setup; no public standardized dataset for this specific pipeline is provided in the survey.",
            "uuid": "e4430.2",
            "source_info": {
                "paper_title": "Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Human expert evaluation",
            "name_full": "Human expert (domain-expert) judgment",
            "brief_description": "Manual evaluation by domain experts who judge generated hypotheses/ideas on testability, novelty, feasibility, and scientific value when no gold reference exists.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Human expert evaluation (qualitative and scored assessments)",
            "evaluation_method_description": "Experts read LLM-generated hypotheses or ideas and rate them along multiple dimensions (e.g., testability, novelty, feasibility, significance), often using Likert scales, binary decisions, or ranking tasks; experts may also provide qualitative feedback and possible experimental designs to test hypotheses.",
            "evaluation_criteria": "Testability/verifiability, novelty, feasibility/practicality, significance/impact, alignment with background literature, and potential for empirical validation.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "applied across domains (NLP, engineering, physics, chemistry, social science, medicine) as discussed in the survey",
            "theory_type": "hypotheses, research ideas, mechanistic explanations",
            "human_comparison": null,
            "evaluation_results": "Survey summarizes many works using human expert ratings; examples include Qi et al. showing few-shot LLM outputs judged more testable than zero-shot; other studies report human experts finding LLM ideas more novel but slightly less feasible than human ideas (cited large study).",
            "automated_vs_human_evaluation": "Human-based; often used as the gold standard; sometimes combined with automated pre-screening.",
            "validation_method": "Inter-rater agreement statistics (when reported) and comparison of aggregate expert ratings with automated metrics; validation chiefly via consistency across experts and correlation analyses.",
            "limitations_challenges": "Expensive and slow; subjectivity and inter-expert variability can be high; scaling to large numbers of candidates is impractical; experts can be biased and influenced by presentation and framing.",
            "benchmark_dataset": "Typically ad hoc datasets assembled per study (expert-annotated hypothesis collections); no single universal expert-judged benchmark is reported in the survey for hypotheses.",
            "uuid": "e4430.3",
            "source_info": {
                "paper_title": "Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Novelty/Feasibility human study",
            "name_full": "Large-scale human vs LLM idea comparison study",
            "brief_description": "A study (cited as [206] in the survey) comparing ideas produced by humans and LLMs, reporting that LLMs generate ideas judged more novel but slightly less feasible.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Human comparative judgment of novelty and feasibility",
            "evaluation_method_description": "Collect idea/hypothesis samples from both human researchers and LLMs; have human raters (often domain experts or structured survey participants) rate each item for novelty and feasibility (and possibly other dimensions); compare distributions and statistically test differences.",
            "evaluation_criteria": "Novelty (degree of novelty relative to existing knowledge) and feasibility (practicality and likelihood of successful empirical validation); sometimes additional criteria like significance or clarity.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "broad / multi-domain (the cited study was large-scale across research ideas)",
            "theory_type": "research ideas and hypotheses",
            "human_comparison": true,
            "evaluation_results": "Survey reports the large-scale study found LLMs produce ideas judged to be more novel but slightly less feasible than human-generated ideas; no precise numeric effect sizes are given in the survey summary.",
            "automated_vs_human_evaluation": "Human-based comparative evaluation (statistical analysis of human ratings across source types).",
            "validation_method": "Statistical comparison (e.g., significance tests) of rater scores across human vs LLM items; validation via sample sizes and rater pools (detailed validation procedures reported in the original study, not reproduced in the survey).",
            "limitations_challenges": "Rater bias, evaluation framing effects, ambiguous definitions of novelty and feasibility, and possible contamination of LLM outputs with memorized recent discoveries.",
            "benchmark_dataset": "Study-specific collection of human and LLM-generated ideas; not a publicly standardized benchmark.",
            "uuid": "e4430.4",
            "source_info": {
                "paper_title": "Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Truthfulness & hallucination benchmarks",
            "name_full": "Truthfulness and hallucination benchmark suite (TruthfulQA, HaluEval, FELM, SelfAware, FreshQA, Pinocchio, TrustLLM)",
            "brief_description": "A set of benchmarks and datasets cited in the survey designed to measure hallucination, truthfulness, knowledge-limits awareness, and adaptability of LLMs — dimensions relevant when evaluating the factual and explanatory reliability of generated scientific hypotheses/explanations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Truthfulness / hallucination benchmarking (TruthfulQA, HaluEval, FELM, SelfAware, FreshQA, Pinocchio, TrustLLM)",
            "evaluation_method_description": "Apply specialized benchmark datasets that probe model tendencies to produce falsehoods, fabricated references, or overconfident claims, as well as tests of being aware of knowledge limits and freshness of information; TrustLLM aggregates multiple such datasets to evaluate truthfulness, safety, fairness, robustness, privacy, and machine ethics.",
            "evaluation_criteria": "Factual accuracy, rate of hallucinations/fabrications, calibration/uncertainty awareness, freshness/adaptability to new information, safety and fairness-related dimensions.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general but directly applicable to scientific hypothesis/explanation generation across domains because truthfulness and non-hallucination are domain-agnostic requirements",
            "theory_type": "explanations, factual claims, and hypotheses (where factual grounding matters)",
            "human_comparison": null,
            "evaluation_results": "Survey notes that proprietary LLMs generally outperform most open-source models on TrustLLM, but both types struggle when relying solely on internal knowledge; performance improves substantially with external grounding. No single numeric summary is provided in the survey.",
            "automated_vs_human_evaluation": "Automated benchmarking (dataset-based testing) often supplemented by human evaluation for nuanced cases.",
            "validation_method": "Benchmarks validated by curated questions, human annotations of truth/falsity, and in the case of TrustLLM, aggregation across multiple datasets; correlation with human judgments is used to assess benchmark usefulness.",
            "limitations_challenges": "Benchmarks often focus on general factuality and may not capture scientific testability, mechanistic plausibility, or falsifiability; domain-specific ground-truth evidence can be required for rigorous scientific evaluation; benchmark scope and coverage vary.",
            "benchmark_dataset": "Includes TruthfulQA, HaluEval, FELM, SelfAware, FreshQA, Pinocchio, and the aggregated TrustLLM suite as described in the survey; each targets different facets of truthfulness and hallucination.",
            "uuid": "e4430.5",
            "source_info": {
                "paper_title": "Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis Generation with Large Language Models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation",
            "rating": 2,
            "sanitized_title": "large_language_models_as_biomedical_hypothesis_generators_a_comprehensive_evaluation"
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
            "rating": 2,
            "sanitized_title": "improving_scientific_hypothesis_generation_with_knowledge_grounded_large_language_models"
        },
        {
            "paper_title": "Improving the Peer Review of Narrative Literature Reviews",
            "rating": 1,
            "sanitized_title": "improving_the_peer_review_of_narrative_literature_reviews"
        }
    ],
    "cost": 0.02035275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation
16 Apr 2025</p>
<p>Steffen Eger steffen.eger@utn.de 
Yong Cao yong.cao@uni-tuebingen.de 
Jennifer D ' Souza jennifer.dsouza@tib.eu 
Andreas Geiger a.geiger@uni-tuebingen.de 
Brigitte Krenn brigitte.krenn@ofai.at </p>
<p>University of Technology Nuremberg (UTN)
Germany</p>
<p>University of Tübingen
Tübingen AI CenterGermany</p>
<p>TIB Leibniz Information Centre for Science and Technology
Germany</p>
<p>University of Tübingen
Tübingen AI CenterGermany</p>
<p>CHRISTIAN GREISINGER
University of Technology Nuremberg (UTN)
Germany</p>
<p>Austrian Research Institute for Artificial Intelligence
STEPHANIE GROSS
Austria</p>
<p>IT:U Interdisciplinary
YUFANG HOU
Transformation University Austria
Austria</p>
<p>Austrian Research Institute for Artificial Intelligence
Austria</p>
<p>ANNE LAUSCHER
University of Hamburg
Germany</p>
<p>YIZHI LI
University of Manchester
United Kingdom</p>
<p>CHENGHUA LIN
University of Manchester
United Kingdom</p>
<p>NAFISE SADAT MOOSAVI
University of Sheffield
United Kingdom</p>
<p>WEI ZHAO
University of Aberdeen
United Kingdom</p>
<p>TRISTAN MILLER
University of Manitoba
Canada</p>
<p>University of Technology Nuremberg (UTN)
NurembergGermany</p>
<p>University of Tübingen
Jennifer D'SouzaTübingen AI Center, TübingenGermany</p>
<p>TIB Leibniz Information Centre for Science and Technology
Hannover, Andreas GeigerGermany</p>
<p>University of Tübingen
Tübingen AI Center
TübingenGermany</p>
<p>University of Technology Nuremberg (UTN)
NurembergGermany</p>
<p>Austrian Research Institute for Artificial Intelligence
Yufang HouViennaAustria</p>
<p>U Interdisciplinary Transformation University Austria
Linz, Brigitte KrennAustria</p>
<p>Austrian Research Institute for Artificial Intelligence
ViennaAustria</p>
<p>University of Hamburg
Germany; Yizhi Li, yizhi.liHamburg</p>
<p>University of Manchester
Manchester, Chenghua LinUnited Kingdom</p>
<p>University of Manchester
ManchesterUnited Kingdom</p>
<p>Nafise Sadat Moosavi</p>
<p>University of Sheffield
Sheffield, Wei Zhao, weiUnited Kingdom</p>
<p>University of Aberdeen
AberdeenUnited Kingdom</p>
<p>Tristan Miller</p>
<p>University of Manitoba
WinnipegManitobaCanada</p>
<p>Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation
16 Apr 20252C1F638746E42C3D3177D837A7B285DBarXiv:2502.05151v2[cs.CL]Language Language ModelsScienceAI4ScienceSearchExperimentationIdea GenerationMultimodal Content GenerationEvaluationPeer Review
With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation.Recently, a plethora of new AI models and tools have been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently.This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review.In this survey, we provide an in-depth overview over these recent advances, which promise to fundamentally alter the scientific research process for good.Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research.Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion.We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of "AI4Science".CCS Concepts: • Social and professional topics → Assistive technologies; • Applied computing → Physical sciences and engineering; Life and medical sciences; Law, social and behavioral sciences; • Computing methodologies → Natural language processing; Artificial intelligence; • General and reference → Surveys and overviews.</p>
<p>Introduction</p>
<p>Throughout history, science has undergone a number of paradigm shifts, culminating in today's era of data-intensive exploration [88].Although new tools and frameworks have accelerated the pace of scientific discovery, its basic steps have remained unchanged for centuries.These include (1) conception of a research question or problem, typically arising from a gap in disseminated knowledge; (2) collection and study of existing literature or data relevant to the problem; (3) formulation of a falsifiable hypothesis; (4) design and execution of experiments to test this hypothesis;</p>
<p>(5) analysis and interpretation of the resulting data; and (6) reporting on the findings, allowing for their exploitation in real-world applications or as a source of knowledge for a further iteration of the scientific cycle.A more detailed discussion of these steps is provided in Appendix A. 1.With the advent of large multimodal foundation models such as ChatGPT, Gemini, Qwen, or DeepSeek, many research fields and sectors of everyday life now stand at the threshold of an AI-based technological transformation.</p>
<p>Science is no exception.A recent study analyzed approximately 148,000 papers from 22 non-CS fields that cited large language models (LLMs) between 2018 and 2024, reporting a growing prevalence of LLMs in these disciplines [176].</p>
<p>Additionally, a very recent survey among almost 5000 researchers in more than 70 countries, by the American Publishing Company Wiley, suggests that AI adoption is embraced by a majority of researchers who think that AI will become mainstream in science within the next two years, despite current usages often limited to forms of writing assistance. 1hile science has traditionally relied on human ingenuity and labor for generating research ideas, formulating hypotheses, searching for relevant literature, conducting experiments, and reporting results, recent advancements in AI have introduced a surge of models and tools promising to assist researchers at every stage of this cycle.These include models like Elicit or ORKG ASK for search; models like The AI Scientist [151] for experimentation; and models like AutomaTikZ [14] and DeTikZify [15] for multimodal scientific content generation.Moreover, there is even research investigating the extent to which these AI models can evaluate scientific outcomes through automated peer review [259].These AI-driven advancements promise to accelerate the scientific process, leading to unexpected discoveries, improved documentation, and more accessible research communication. 2espite this rapid progress, to our knowledge, there is no comprehensive survey covering the full breadth of AIassisted tools, models, and functionalities available for supporting and improving the research cycle.Existing reviews are typically domain-specific, such as in the social sciences [249] or branches of physics [265]. 3To address this gap, our survey provides an extensive overview of five central aspects of the research cycle where AI is making transformative contributions: (1) search and content summarization (Section 3.1); (2) scientific experimentation and research idea generation (Section 3.2); (3) unimodal content generation, such as drafting titles, abstracts, suggesting citations, and assisting in text refinement (Section 3.3); (4) multimodal content generation, including the creation and interpretation of figures, tables, slides, and posters (Section 3.4); and (5) AI-assisted peer review processes (Section 3.5).The recent Wiley survey underscores the significance of this endeavor, highlighting that "63% [of respondents indicated] a lack of clear guidelines and consensus on what uses of AI are accepted in their field and/or the need for more training and skills."</p>
<p>This article offers a detailed, disciplinarily contextualized survey of state-of-the-art AI applications in scientific research, spanning every stage from the initial conception of ideas to the dissemination of results.It is intended primarily to help researchers in fields within AI (natural language processing (NLP), computer vision (CV), etc.) quickly familiarize themselves with the transdisciplinary foundations of and latest developments in this broad-ranging and rapidly evolving research area.Some of the material will also be useful to policymakers, practitioners, and research collaborators in adjacent fields, including human-computer interaction, library and information science, communication studies, metascience, science journalism, and research ethics.</p>
<p>We believe our contribution to be timely because, despite the growing interest in the topic, its researchers are only just now coalescing into a community with dedicated dissemination venues.Recent examples include the workshops Natural Scientific Language Processing and Research Knowledge Graphs (NSLP) [189], Foundation Models for Science (FM4Science), AI &amp; Scientific Discovery (AISD), and Towards a Knowledge-grounded Scientific Research Lifecycle (AI4Research), all of which held their first editions in 2024 or 2025.The few existing reviews of AI-for-science literature have addressed only isolated topics or application domains.The earliest examples (e.g., [58,122]), now long out of date, tend to be organized into case studies of AI for specialized tasks such as equation or drug discovery.More recent surveys, such as [85], cover a wider variety of application domains but focus on a narrower sector of the scientific life cycle, and are pitched more towards the potential users of the AI tools than towards AI researchers aiming to understand and advance the underlying data sets, methodologies, and evaluation metrics.</p>
<p>Given our topic's wide scope, rapid progress, and dependence on knowledge and methods from different domains, we have opted to take a narrative approach to our survey.This methodology allows for greater freedom in the selection and framing of relevant papers [111], which promotes "breadth of literature coverage and flexibility to deal with evolving knowledge and concepts" [21] as well as the ability to "bridge related areas of work, provoke thoughts, inspire new theoretical models, and direct future efforts in a research domain" [171].Systematic reviews, while regarded as more objective, are better suited to relatively narrow topics with well-defined, empirical research questions [171].Accordingly, we have adopted no fixed inclusion or exclusion criteria for the studies referenced in this survey, but have rather selected them on the basis of our own relevance judgements.In assembling the co-authors for this survey, we have therefore endeavoured to include researchers actively publishing in each of the various subtopics we cover.</p>
<p>3 AI Support for Individual Topics and Tasks</p>
<p>Literature Search, Summarization, and Comparison</p>
<p>The rapid growth of scientific literature presents a significant challenge for researchers who need to search, analyze, and summarize vast amounts of information efficiently.AI-powered tools are transforming these tasks by leveraging NLP, machine learning (ML), LLMs, citation and knowledge graphs (KGs) to automate the retrieval, extraction, and summarization of scientific information.Unlike traditional search engines that rely on basic keyword matching, AI-equipped systems provide context-aware, semantic search with additional features that enhance the overall search experience.These systems go beyond finding relevant papers; they generate answers to research questions from the search results, provide structured summaries, and offer comparative insights, helping researchers identify gaps, trends, and contradictions across multiple studies.</p>
<p>3.1.1Data.Scientific search engines rely on vast publisher databases to provide access to scientific literature.Understanding the classification of these repositories is essential for assessing search engines' coverage, reliability, and effectiveness in evidence-based research.Repositories vary by access model, subject focus, and content type, each serving a distinct role in academic discovery and knowledge dissemination.By access model, repositories fall into open access repositories, which provide unrestricted access to research articles (e.g., PubMed Central, arXiv); subscription-based repositories, requiring institutional or individual subscriptions (e.g., ScienceDirect, SpringerLink); and hybrid repositories, offering both free and paywalled content (e.g., Taylor &amp; Francis Online, Oxford Academic).By subject focus, repositories are either multidisciplinary, covering broad disciplines (e.g., Web of Science, Scopus), or subject-specific, specializing in fields such as medicine (PubMed), physics (INSPIRE-HEP), and social sciences (SSRN).By content type, institutional repositories archive research outputs from specific organizations (e.g., MIT DSpace, Harvard DASH); preprint repositories enable early dissemination of research before peer review (e.g., bioRxiv, chemRxiv); and government and public sector repositories provide access to publicly funded research (e.g., NASA ADS, OpenAIRE).Data repositories (e.g., Dryad, Zenodo) store research datasets, supporting transparency and reproducibility, while aggregator repositories (e.g., BASE, CORE [113]) index content from multiple sources for broader searches.Lastly, grey literature repositories (e.g., OpenGrey, EThOS) provide access to non-traditional research outputs such as theses, reports, and white papers, which may not be available through conventional publisher platforms.</p>
<p>The structure of scientific repositories shapes AI-enhanced search.While broad AI-based search engines like Elicit and ORKG ASK query multiple publisher repositories, similar to Google Scholar, tools like NotebookLM focus on user-selected documents, and recommender systems such as Scholar Inbox rank new literature by relevance.AI-driven search enables customizable knowledge bases while optimizing discovery, retrieval, and personalization in research.</p>
<p>Methods and Results</p>
<p>. This section surveys state-of-the-art AI-enhanced scientific discovery tools, identified as four main types based on their core functionality: (1) AI-enhanced search, which retrieves relevant literature from vast repositories; (2) graph-based systems, which map relationships between research concepts and publications;</p>
<p>(3) paper chat and QA, which enable interactive exploration of scientific content; and (4) recommender systems, which suggest relevant papers based on user preferences.A detailed overview of these tools is provided in Table 1.</p>
<p>Additionally, two traditional scientific discovery tools, namely search engines and benchmarks with leaderboards, are discussed in Appendix A.2.1.AI-enhanced Search.Platforms such as Elicit, Consensus, OpenScholar, and SciSpace leverage AI, including LLMs, to extend beyond traditional search by enabling semantic search, paper summarization, evidence synthesis, and trend analysis.Unlike conventional search engines that rely on keyword matching, these tools use NLP and machine learning to extract key insights, synthesize information to answer research queries [76], and generate structured summaries.Their ability to quickly summarize and categorize findings-such as study outcomes, methodologies, and limitations-helps researchers efficiently compare and interpret literature.</p>
<p>Graph-based Systems.Graph-based systems such as ORKG ASK are designed to facilitate structured access to scientific knowledge.Unlike conventional paper search engines, they leverage a KG that organizes research contributions as structured data rather than unstructured text.Such contributions are typically extracted from the abstract, introduction, and result sections [56,175].Those systems enable users to ask complex, domain-specific questions and receive answers synthesized from semantically structured scientific data.They typically use techniques such as KG-based reasoning and retrieval-augmented generation (RAG) to extract relevant information from the KG, providing more interpretable and  1. Overview of popular literature search, summarization and comparison tools and their key features.
✓ ✓ ✓ ✓ ✓ ✓ Freemium Consensus ✓ ✓ ✓ ✓ ✓ ✓ Freemium over 200 million SciSpace ✓ ✓ ✓ ✓ ✓ ✓ ✓ Freemium scienceQA ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Freemium 220 million PaperQA2 ✓ ✓ ✓ Free Paperguide ✓ ✓ ✓ ✓ ✓ ✓ ✓ Freemium HyperWrite ✓ ✓ ✓ ✓ ✓ ✓ ✓ Premium ResearchKick ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Premium Graph-Based Connected Papers ✓ ✓ ✓ Freemium 214 million ScholarGPS ✓ ✓ ✓ ✓ ✓ Free over 200 million CiteSpace ✓ ✓ Freemium Sci2 ✓ Free NLP KG ✓ ✓ ✓ ✓ ✓ Free ORKG ASK ✓ ✓ ✓ ✓ Free 76 million Paper Chat ChatGPT ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Freemium 10 pdf files Claude ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Freemium 5 pdf files Deepseek ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Free Research ✓ ✓ ✓ ✓ ✓ ✓ Freemium 1 pdf file NotebookLM ✓ ✓ ✓ ✓ ✓ ✓ Freemium 50 pdf files Enago Read ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Freemium 1 pdf file DocAnalyzer.AI ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Premium few pdf files CoralAI ✓ ✓ ✓ ✓ ✓ ✓ Freemium 1 pdf file ExplainPaper ✓ ✓ ✓ ✓ ✓ Freemium 1 pdf file ChatPDF ✓ ✓ ✓ ✓ ✓ ✓ ✓ Premium 1 pdf file Recommender Arxiv Sanity ✓ ✓ ✓ ✓ Free Scholar Inbox ✓ ✓ ✓ ✓ ✓ ✓ ✓ Free ResearchTrend.ai ✓ ✓ Freemium TrendingPapers ✓ ✓ ✓ ✓ ✓ ✓ Free Bytez ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Freemium Notesum.ai ✓ ✓ ✓ ✓ ✓ ✓ Freemium Research Rabbit ✓ ✓ ✓ Free Table
verifiable answers compared to traditional LLM-based QA systems.CiteSpace and Sci2 are specialized bibliometric analysis and network analysis tools to study the structure and evolution of scientific research.CiteSpace focuses on identifying research trends, keyword co-occurrence networks, and citation bursts, using visual analytics to highlight emerging topics and influential papers using graphs.Sci2 is a more general-purpose tool designed for analyzing scholarly datasets, enabling users to perform network analysis, geospatial mapping, and temporal modeling of scientific literature and collaboration patterns.Connected Papers is a scientific literature exploration tool designed to help researchers discover related papers based on a given seed paper.Unlike traditional citation-based systems, it builds a graph of papers using a similarity metric derived from co-citation and bibliographic coupling analysis.The platform constructs a network where each node represents a paper, and edges indicate similarity based on shared references and citations rather than direct citation links.This approach allows users to find relevant papers that may not be directly cited but are conceptually related.Graph-based visualizations provide an intuitive way to explore clusters of research, identify foundational and emerging works, and track the evolution of scientific ideas.</p>
<p>Paper Chat and QA.Paper chat and question-answering (QA) systems such as ChatGPT, Deepseek Chat, NotebookLM, ExplainPaper, ChatPDF, and DocAnalyzer.AI allow users to interact with scientific papers by asking questions and receiving responses based on the document's content.They typically process a limited number of user-provided PDFs or text from specific websites.The core technology behind them is RAG [8,105,126], a technique that combines information retrieval with LLMs to improve accuracy and grounding.A typical RAG system first partitions the document into smaller sections and converts them into vector representations using embedding models.Upon a user query, the system retrieves the most relevant sections based on semantic similarity and passes them as context to an LLM, which then generates a response.This mechanism ensures that answers are directly grounded in the provided documents rather than relying solely on the model's pre-trained knowledge, enhancing factual reliability and interpretability.Some systems incorporate LLM agents [22,138,223] that can reason over retrieved information, summarize findings, or extract key insights.These agents can follow multi-step reasoning strategies to provide more nuanced responses, such as synthesizing information from multiple sections or explaining technical terms in simpler language.By anchoring responses to document content, RAG-based systems mitigate hallucinations and make it easier for users to verify claims by checking the referenced passages.The effectiveness of these systems depends on the quality of document chunking, the efficiency of retrieval, and the model's ability to integrate information into coherent, context-aware answers.</p>
<p>Recommender Systems.Scientific paper recommender systems such as Arxiv Sanity, Scholar Inbox, ResearchTrend.ai, and Research Rabbit leverage machine learning and information retrieval techniques to help researchers discover relevant literature.These systems generally fall into two main categories: content-based filtering, collaborative filtering and hybrid approaches.Content-based methods [4,16] analyze the text of papers to build representations that capture their meaning.Traditional approaches rely on sparse abstract or document representations such as TF-IDF [210], which assigns importance to words based on their frequency and distinctiveness in a corpus.More advanced models use dense abstract or document embeddings derived from neural networks, such as SPECTER [43] or GTE [139], which map papers into a high-dimensional vector space where similar documents are close to each other.The Massive Text Embedding Benchmark (MTEB) [164] ranks many state-of-the-art embedding models on a comprehensive benchmark comprising various different datasets and tasks.These embeddings enable fast similarity searches and improve over simple keyword matching.In contrast, collaborative filtering [12,236] relies on user interactions, such as downloads, bookmarks, and citations, to recommend papers based on the behavior of similar users.One challenge of pure collaborative filtering is the cold start problem, where new papers or users lack sufficient data for recommendations.To mitigate this, many modern systems employ hybrid approaches, such as two-tower architectures [46,255,258].These models learn separate representations for papers and users, combining textual embeddings with user interaction data to generate more personalized recommendations.State-of-the-art systems often use a mix of these techniques to balance relevance, novelty, and diversity.The effectiveness of these systems depends on the quality of embeddings, the availability of interaction data, and the efficiency of ranking algorithms that surface the most useful papers.</p>
<p>Ethical Concerns.</p>
<p>The use of AI in scientific search, summarization and comparison raises ethical considerations, particularly in ensuring transparency, accountability, and equity.AI can significantly accelerate the pace of discovery, automate search tasks, and uncover patterns that may elude human researchers, but it also introduces risks and biases.</p>
<p>Existing dynamics such as the Matthew effect, where well-known researchers receive disproportionate attention, might Eger et al. be reinforced by the AI algorithms, intensifying inequalities.We believe that research should follow a human-centric approach, in which the human researcher is provided with advanced tools but remains fully responsible for executing the research and summarizing the results in research papers.It is also important to develop algorithms to reduce biases by recommending relevant work to researchers based on the content of the research, independent of the popularity of the authors.Tools that are able to uncover gaps in the existing literature might even lead to a more uniform allocation of researchers to topics, reducing the bias towards overpopulated areas.</p>
<p>3.1.4Domains of Application.The search, summarization and comparison tools discussed in this section are general and apply to all fields of science.The presented benchmarks, however, are specific to the field of computer science in general and artificial intelligence in particular.</p>
<p>3.1.5Limitations and Future Directions.Despite the significant advancements in AI-powered scholarly search systems, several limitations persist that hinder their full potential.One of the primary challenges is data quality and coverage gaps, as these systems often struggle with handling incomplete, non-standardized, or outdated data sources, which can lead to inaccuracies and inconsistencies in retrieved information.Additionally, bias in AI models remains a critical concern, where search and ranking algorithms may introduce biases based on training data, potentially influencing the visibility of certain research areas and limiting the diversity of perspectives presented to users.Another major limitation lies in scalability and real-time processing, as efficiently handling large-scale datasets while maintaining low latency and high retrieval accuracy remains a technical challenge.Addressing these limitations opens several promising future directions.One potential avenue is enhanced personalization which can be achieved by adapting search engines to user preferences, providing more tailored recommendations based on research interests and behavioral patterns.</p>
<p>Lastly, fostering interdisciplinary collaboration through the integration of AI-powered search systems with other digital tools, such as data visualization platforms and research management software, could facilitate more comprehensive and insightful research outcomes.Addressing these challenges and exploring future directions will be crucial for realizing the full potential of AI-driven scholarly search and synthesis.</p>
<p>AI-Driven Scientific Discovery: Ideation, Hypothesis Generation, and Experimentation</p>
<p>Idea formation, hypothesis generation, and experimentation are fundamental to scientific discoveries.Idea formation, particularly in AI, focuses on proposing new tools or benchmarking existing ones.Hypothesis formation involves formulating specific, testable questions that guide empirical or theoretical justifications.</p>
<p>Experimentation then tests hypotheses and evaluates ideas through systematic observation, data collection, and analysis.In AI research, this often includes benchmarking models, running simulations, or conducting ablation studies.Traditionally, these processes have been carried out by human researchers.However, in an age of rapidly growing scientific literature, efforts of moving from literature review to hypothesis and idea formation have become increasingly time-consuming.Experimentation adds further complexity, requiring careful methodological design, large-scale simulations, and in-depth result analysis.As AI-driven approaches accelerate hypothesis generation, idea formation, and evaluation, it is essential for the research community to assess a broad range of candidates, selecting those that are most meaningful, relevant, and potentially novel for further validation.[26], prototype ideas stemming from a design hackathon [59], innovative ideas for space-related technologies and missions [71].For hypothesis generation, datasets contain scientific papers, and each paper is annotated by domain experts with background, research questions, related works and hypotheses in various domains [182,252,253].For automated experimentation, datasets consist of structured problem-solving tasks that require generating or modifying Python code.These include AI development tasks with hierarchical user requirements [271], scientific discovery automation based on data and expert knowledge [35], software engineering tasks requiring code edits to resolve issues [102], and the MLGym framework [166], which evaluates agents on open-ended AI research tasks across multiple domains.Details of these datasets are presented in Table 2.</p>
<p>Methods and Results</p>
<p>. Here, we discuss state-of-the-art methods and results in hypotheses generation, idea formation, and automated experimentation.Fig. 2 provides some examples for each approach.</p>
<p>Idea Generation.Research ideation has become a critical area where LLMs are increasingly applied to enhance novelty and accelerate discovery.Several methods have been proposed to improve the creative abilities of LLMs, focusing on iterative refinement, multi-agent systems, human alignment and evaluation [92,131,151,184,215].</p>
<p>For iterative refinement, Hu et al. [92] introduce an iterative planning and search framework aimed at enhancing the novelty and diversity of ideas generated by LLMs.By systematically retrieving external knowledge, the approach addresses the limitations of existing models in producing simplistic or repetitive suggestions.Similarly, Pu et al. [178] focus on iterative refinement by providing literature-grounded feedback.Representing research ideas as nodes on a canvas, their approach, IdeaSynth, facilitates the iterative exploration and composition of idea facets, enabling to develop more detailed and diverse ideas, particularly at various stages of ideation.For human alignment, other works seek to organize information in ways that mirror human research processes.Chain of Ideas (CoI) [131] proposes structuring literature into a chain to emulate the progressive development of research domains.This facilitates the identification of meaningful directions and has been shown to outperform existing methods in generating ideas comparable in quality to those produced by human researchers.Scideator [184], in contrast, focuses on recombining facets (e.g., purposes, mechanisms, and evaluations) from existing research papers to synthesize novel ideas.By incorporating automated novelty assessments, Scideator enables users to identify overlaps and refine their ideas.Moreover, research has explored fully autonomous and multi-agent systems for scientific discovery.For instance, the AI Scientist [151] presents a framework for automating the entire research pipeline, including idea generation, experiment execution, and paper User-generated content (like unboxing videos and reviews) greatly influences blind box purchases among young consumers.</p>
<p>A method that leverages memoryaugmented neural networks for knowledge acquisition in a lifelong scenario.SciMON [26] HypoGeniC [253] Nova [81] IdeaSynth [165] Chain of Ideas [118] Scideator [171] GVIM [141] DrugAgent [137] AIDE [186] OpenHands [226] Methods</p>
<p>Sample Output
Task</p>
<p>Hypothesis Generation</p>
<p>Generate a hypothesis related to online consumer behavior.</p>
<p>Idea Generation</p>
<p>Generate a novel AI research idea.</p>
<p>Automatic Experimentation</p>
<p>Train a multitask model on Clintox to predict toxicity and FDA approval.Save test predictions ... writing.VirSci [215] employs a multi-agent system of virtual agents to collectively generate, evaluate, and refine ideas, which outperforms individual LLMs, underscoring the potential of teamwork in enhancing scientific innovation.</p>
<p>Hypothesis Generation.Recently, there have been many works that leverage LLMs to generate hypotheses and ideas [26,147,182,235,247,253].These works differ in their use of LLMs for addressing various technique challenges, including (a) handling long context input due to the need for LLMs to analyze related works, (b) strategies for refining LLMs to generate meaningful hypotheses, (c) lowering the possibility of generating hallucinated hypotheses and ideas.</p>
<p>For hallucinated hypotheses, Yang et al. [253] address the hallucination issue through a pipeline that starts with a search for related works.The identified related works and a given research question are provided as input for LLMs to generate hypotheses.The generated hypotheses are evaluated against ground-truth hypotheses published in Nature and Science.Their results show that many generated hypotheses exhibit a very high degree of similarity to the ground-truth ones.Regarding long context input, Chai et al. [26] focus on the efficient use of limited context size of LLMs.They introduce a selection mechanism that extracts important and relevant information from the literature and takes them as input for LLMs to generate hypotheses.Their results show that filtering out unnecessary information helps improve the quality of generated hypotheses.For refinement strategies, many works have explored strategies for refining LLMs to generate hypotheses.Major strategies include (a) few-shot learning, (b) fine-tuning on training data and (c) iterative refinement.For instance, Qi et al. [181] show that hypotheses generated by few-shot learning are judged by humans more testable than those generated in the zero-shot setup; while fine-tuning improves the overall quality of hypotheses, the improvement is limited to the domain of training data; in unseen domains, fine-tuning harms hypothesis quality, particularly the novelty aspect.Zhou et al. [270] iteratively refine hypotheses through reinforcement learning, with the aim of increasing the similarity between a given research problem and a generated hypothesis.A recent advancement in this space is the AI co-scientist 7 , a multi-agent system that employs a generate-debate-evolve framework.It iteratively enhances its hypotheses through collaboration, grounding in prior evidence and tournament-based selection.</p>
<p>Automated Experimentation.Experimentation is central to AI-driven research, encompassing task formulation, implementation, evaluation, and iteration.Automated experimentation aims to streamline this workflow, with approaches like Neural Architecture Search [62] and AutoML [86].LLMs further enhance this by enabling automation through natural language prompts.AutoML-GPT [227] and MLcopilot [264] use LLMs for hyperparameter tuning, while MLAgentBench [98] benchmarks fundamental automation tasks.Recent work explores advanced frameworks incorporating multi-agent collaboration, tree search, and iterative refinement for scientific experimentation.</p>
<p>For multi-agent workflow, GVIM [154] enhances chemical research with domain-specific functions, while DrugAgent [150] employs LLMs for task planning in drug discovery.AutoML-Agent [226] integrates retrieval-augmented planning for AutoML tasks, and MLAgentBench [98] benchmarks LLM-driven agents in machine learning experimentation.The Agent-as-a-Judge framework [271] introduces structured agent evaluation.For tree search, AIDE [201] applies Solution Space Tree Search to refine solutions in Kaggle challenges.The "Tree Search for Language Model Agents" framework [115] enables LLM agents to plan multi-step interactions using best-first tree search, pruning less promising options.SELA [39] combines LLM-generated insights with Monte Carlo Tree Search, iteratively refining machine learning experiments by selecting promising configurations and executing them.For Iterative refinement, APEx [44] automates LLM-based experimentation with an orchestrator, execution engine, benchmark generator, and model library.OpenHands [241] enables AI agents to interact with software, execute actions in a sandboxed runtime, and collaborate across tasks using predefined benchmarks.</p>
<p>Evaluation.While LLMs accelerate ideation and hypothesis generation, it is crucial for the research community to assess their usefulness and identify those worth further validation.Many evaluation approaches have been proposed, differing primarily in whether gold hypotheses and ideas are available.When gold hypotheses and ideas are available, metrics such as BLEU [168] have been used to assess hypothesis and idea quality by measuring similarity to known scientific discoveries [26,253].Qi et al. [182] leverage LLMs-as-a-metric to evaluate hypotheses based on four scientific aspects: (a) novelty, (b) relevance to the given background, (c) significance within the research community, and (d) verifiability, i.e., testability.When gold hypotheses and ideas are unavailable, a generated hypothesis or idea is typically assessed by human experts, who compare it with the given research question and provide feedback [252].</p>
<p>Ethical Concerns.</p>
<p>In the area of idea generation, there is a risk of reinforcing established research paradigms.AI systems trained on the basis of existing literature may favor popular paths and neglect underrepresented research directions.As a result, unconventional ideas may be unintentionally marginalized.AI-generated hypotheses may lack transparency, making it difficult to assess their validity or underlying assumptions, which could lead to flawed experiments.For example, an AI might identify a statistical correlation in its training data and propose hypotheses without clearly revealing the underlying assumptions or data sources, making it difficult for researchers to verify its scientific soundness or hold anyone accountable if the hypotheses proves misleading.Automated experimentation presents its own ethical challenges.The speed and volume in which AI can design and execute experiments can lead to insufficient ethical oversight and inadequate safety controls.Consider an AI system that suggests experimental protocols in biomedical research (e.g., chemical components with unknown toxicity) without the rigorous human review Eger et al. needed to identify potential risks.This could lead to experiments that pose unforeseen dangers or violate established ethical standards.</p>
<p>Domains of Application.</p>
<p>Regarding domains of interest, previous works have addressed idea and hypothesis in NLP, Engineering, Physics, Chemistry, Social Science and Medicine.Similarly, automated experimentation also relies on domain-specific datasets to guide the process of designing and testing experiments.In contrast, methods are typically domain-agnostic, though they are often developed and evaluated in specific domains.These methods address fundamental issues in LLMs, such as long-context inputs, post-tuning strategies, and model hallucination, making them potentially applicable across multiple domains.</p>
<p>Limitations and Future Directions.</p>
<p>A large-scale study [206] comparing human researchers and LLMs finds that LLMs generate ideas judged to be more novel but slightly less feasible, highlighting challenges like limited diversity and self-evaluation failures.Additionally, given that ideas and hypotheses are theoretical and costly to validate, it is unclear whether they could lead to scientific discovery.Furthermore, previous methods lack due diligence through data, and therefore generated ideas and hypotheses are often too general [253].Moreover, LLMs may generate recently discovered ideas and hypotheses, as they lack access to recent scientific papers [148].Their outputs are very sensitive to the framing of input prompts [170].Future work should focus on improving feasibility and diversity of ideas and hypotheses, incorporating real-time scientific papers, refining ideation and hypothesis generation through data inspection.</p>
<p>Automated experimentation with LLMs faces several additional challenges.First, LLMs often make critical errors, such as hallucinating results or outputting invalid references, which disrupt the precise steps required for experimental workflows.Another significant limitation is that LLMs struggle to integrate and align different modalities, such as video, audio, or sensory data, which are increasingly essential in modern scientific experimentation.Moreover, LLMs lack the critical analysis capabilities necessary to identify flaws or refine hypotheses during experimentation.In highly specialized scientific domains such as biology and chemistry, they may also struggle with precise reasoning and tool usage, which are vital for ensuring experimental success [187].</p>
<p>Text-based Content Generation</p>
<p>Under text-based content generation for science, we subsume different tasks generating specific text-based subparts of a scientific paper, such as automatically generating (i) the title, (ii) the abstract, (iii) the related work section, as well as (iv) citation generation.Also, frameworks aiming to automate the full paper writing process will be discussed, as well as using AI systems for subtasks such as proof-reading, paraphrasing, and press release generation.</p>
<p>Data.</p>
<p>Open access research articles are a valuable data source for text-based content generation.These include scientific publisher repositories offering at least some open access content (e.g., Nature portfolio, Taylor &amp; Francis) as well as preprint repositories (e.g., arXiv, bioRxiv).These open access repositories can be leveraged to develop datasets with pairs of titles and abstracts or abstract and conclusion/future work pairs.Wang et al. [238] for example extract (i) title to abstract pairs, (ii) abstract to conclusion and future work pairs, and (iii) conclusion and future work to title pairs from PubMed.Annotated, task-specific datasets for scientific text generation are presented in Table 3.</p>
<p>Methods and Results.</p>
<p>In the following, we survey approaches to generating textual content for science, such as title, abstract, related work and bibliography.An overview of these processes is given in Appendix A.2.3.</p>
<p>Dataset Size Sources Application</p>
<p>Abstract-title humor annotated dataset [ [254] 1,000 papers + 20 citation sentences each ACL Anthology Network Related work generation CORWA [136] 927 related work sections NLP domain Related work generation CiteBench [69] 358,765 documents + citations multiple, e.g., arXiv.orgRelated work generation SciTechNews [24] 2,431 papers + press releases ACM TechNews Press release generation Table 3. Annotated or task-specific datasets for scientific text generation.</p>
<p>Title Generation.Generating adequate titles for scientific papers is an important task because titles are the first access point of a paper and can attract substantial reader interest; titles can also influence the reception of a paper [125].Consequently, several works have targeted generating titles automatically, often using paper abstracts as input.</p>
<p>For example, Mishra et al. [160] use a pipeline of three modules, viz.generation by transformer based (GPT2) models, selection (from multiple candidates) and refinement.Chen and Eger [34] also leverage transformers for title generation from abstracts but they in addition allow for generation of humorous titles (which may be even more impactful) when an input flag is set appropriately.To achieve this, they annotate a training dataset of humorous titles from the fields of machine learning and NLP.They explore different models including BART, GPT2, and T5 besides the more recent ChatGPT-3.5 LLM, finding that none of them can adequately generate humorous titles.They also explore generating titles from full texts instead of abstracts, with mixed results.Wang et al. [238] address the problem differently by drafting title names based on future work sections of previous related papers.</p>
<p>Abstract Generation.There are several approaches trying to assess the capabilities of proprietary LLMs to generate abstracts based on context information such as paper titles, journal names, keywords or the full text of the paper.</p>
<p>Hwang et al. [99] assess the ability of GPT 3.5 and GPT 4 to generate abstracts based on a full text.The results are manually evaluated using the Consolidated Standards of Reporting Trials for abstracts, a standard published with an aim to enhance the overall quality of scientific abstracts [90].While the readability of abstracts generated by GPT is rated higher, their overall quality is inferior to the original abstracts.Also, minimal errors are reported in the AI generated abstracts.Wang et al. [238] generate abstracts from titles, leveraging transformers and knowledge bases.Also generating abstracts from titles, Gao et al. [70] collect 50 research abstracts from five medical journals and apply ChatGPT to generate research abstracts based on their titles and the name of one of the five journals.The original and the generated abstracts are then evaluated with AI output detectors and with blinded human reviewers to identify which of the abstracts are automatically generated.Human reviewers are able to identify 68% of the generated abstracts as being automatically generated, but also incorrectly identify 14% of original abstracts as being LLM generated.Applying AI output detectors, most generated abstracts can be identified by the GPT-2 Output Detector assigning a median of 99.98% generated scores to generated abstracts and a median 0.02% to original abstracts.However, Anderson et al. [6] have shown that after automatically paraphrasing AI generated text, the performance of AI detectors such as GPT-2</p>
<p>Output Detector decrease drastically.Farhat et al. [63] evaluate the performance of ChatGPT generating abstracts based on 3 keywords, the name of a database (Scopus or web of science) and the task to analyze bibliographic data in the domain indicated by the keywords.The authors then compare the generated abstract to an actual abstract on the same topic.After a manual comparison of the results, the authors come to the conclusion that at the time the study was conducted, ChatGPT is not a trustworthy tool for retrieving and assessing bibliographic data.</p>
<p>Long Text Generation.Some approaches aim at automating the full paper writing process.The AI Scientist [151] presents a comprehensive framework designed to support the entire scientific research cycle, encompassing tasks such as idea generation, hypothesis formulation, experimental planning, and execution.While its primary focus is not on long-form text generation, AI Scientist is able to generate entire scientific papers.By incorporating structured scientific knowledge (e.g.experimental results), the framework can draft papers that adhere to domain-specific requirements, involving the integration of relevant citations and conforming to disciplinary norms.Despite its ability to produce comprehensive paper drafts, the framework does not explicitly address the challenge of maintaining coherence across extended narratives, and their dependencies.LongWriter [9] and LongEval [245] directly address the challenge of generating extended text by introducing architectural modifications aimed at enhancing coherence and structural consistency in long-form outputs.The framework employs hierarchical attention mechanisms to ensure thematic consistency across long text and applies fine-tuning strategies to align outputs with user prompts.LongWriter conducts experiments on several domains, including academic and monograph texts.For academic content, the model can generate structured arguments and effectively incorporate domain-specific terminologies.However, noticeable issues remain around factual consistency, the integration of citations, and redundancy in the generated text.However, by conducting experiments on various models in academic, wikipedia and blog domains, LongEval shows that the larger models trained with general instruction data performs similar to those specifically trained (e.g., LongWriter).LongReward Related Work Generation.Already in the past, there has been a substantial body of work on related work generation through text summarization, most of which differ in their approach (extractive or abstractive) and the length of citation text (sentence-level or paragraph-level).Extractive approaches focus on selecting sentences from cited papers and reordering the extracted sentences to form a paragraph of related work.For instance, Hoang and Kan [89] propose an extractive summarization approach that selects sentences describing the cited papers to generate the related work section of a target paper.This approach relies on the full text of the target paper.Subsequent extractive approaches differ from this approach in how they order the extracted sentences: While Wang et al. [242], Chen and Zhuge [33], and Wang et al. [237] assume that the sentence order is given, Hu and Wan [94] and Deng et al. [50] take advantage of an automatic approach to reorder sentences based on topic coherence.However, extractive approaches often struggle to produce coherent text, as they simply concatenate sentences without ensuring a cohesive narrative flow.In contrast, abstractive related work generation leverages devices of rewriting and restructuring to generate a summary of a cited paper.Most of the abstractive approaches are based on language models and focus on either generating (a) a single sentence from a single reference or (b) a paragraph from multiple references.Typically, the abstractive process is repeated multiple times until a related work section is complete.AbuRa'ed et al. [1] introduce an abstractive summarization approach to generate citation sentences in a single-reference setup.Their approach has been trained on the ScisummNet corpus with paper abstracts as inputs and citation sentences as outputs.Li et al. [136] further extend this idea to a multiple-reference setup, namely generating a paragraph of citation sentences from various cited papers.Their approach has been trained on the CORWA corpus to generate both citation and transition sentences.Additionally, instead of using paper abstracts as inputs, Li et al. [135] propose to retrieve relevant sentences from cited papers to generate citation sentences.More recently, works such as Şahinuç et al. [193] explore instruction promoting with LLMs, which is alternative to extractive and abstractive approaches, to generate citation sentences.Overall, extractive approaches, while factual, often lack fluency and coherence.In contrast, abstractive approaches and instruction prompting, which are based on (large) language models, do not struggle with these issues, however, they suffer from factual errors, known as hallucination.</p>
<p>Citation Generation.Bibliographic references in scientific papers are important components for ensuring the scientific integrity of the authors.However, in many cases, cited articles of bibliographic references generated by LLMs such as ChatGPT are reported not to exist, that is, are hallucinated or incorrect [63,96,134,137].Most of the studies reporting hallucinated or erroneous bibliographic references are case studies presenting one or more examples.Walters and Wilder [233], however, present a study in which they use ChatGPT-3.5 and ChatGPT-4 to produce 84 documents (short reviews of the literature) on 42 multidisciplinary topics.The resulting documents contain 636 bibliographic citations, which are further analyzed for errors and hallucinations.Their results show that 55% of the GPT-3.5 citations but only 18% of the GPT-4 citations are fabricated.Of the actual existing (non-fabricated) GPT-3.5 citations, 43% include substantive citation errors, and of the non-fabricated GPT-4 citations it is 24%.Even though this is a major improvement from GPT-3.5 to GPT4, problems with fabrication and errors in bibliographic citations remain.Therefore, for generated citations and references, it is of particular importance to ensure their accuracy and completeness.</p>
<p>Proof-reading and Paraphrasing.LLMs such as ChatGPT have been reported to provide useful assistance for scientific writing with regards to proof-reading and language review in order to enhance the readability of the paper.Subtasks these models can provide support for during the writing process include providing suggestions for improving the writing style, or proof-reading [195].Additionally, some authors emphasize that LLMs can be helpful especially for non-native English speakers with regards to grammar, sentence structure, vocabulary and even translation, i.e., providing an English editing service [25,97,110].Most papers on this topic are case studies, illustrating their research questions with one or more examples and their results are qualitatively evaluated by a human expert (typically the author of the paper).Hassanipour et al. [84] evaluate the effectiveness of ChatGPT in rephrasing not for improving the writing style, but for reducing plagiarism in the process of scientific paper writing.The results showed that even with explicit instructions to paraphrase or reduce plagiarism, the plagiarism rate remained relatively high.Press Release Generation.Several studies attempt to generate press release articles for the general public based on scientific papers.Cao et al. [23] construct a manually annotated dataset for expertise-style transfer in the medical domain and apply various style transfer and sentence simplification models to convert expert-level language into layman's terms.Goldsack et al. [79] develop standard seq-to-seq models to generate news summaries for scientific articles.Lastly, Cardenas et al. [24] propose a framework that integrates metadata from scientific papers and scientific discourse structures to model journalists' writing strategies.</p>
<p>Ethical Concerns.</p>
<p>In scientific work, authorship and plagiarism in AI generated texts are major concerns.In general, it is a challenge to distinguish between AI generated and human generated texts.Although there is a number of tools to detect AI-generated text (e.g., GPTZero or Hive), Anderson et al. [6] show that after applying automatic paraphrasing to AI generated text, the probability of a text to be human generated, increases.Therefore it is not possible to reconstruct if a text is an original work from a scientist or has been generated by an AI.In addition, it is also found that ChatGPT generated texts easily pass plagiarism detectors [3,61].Moveover, Macdonald et al. [155] raise the concern that the frequent use of LLMs for drafting research articles might lead to similar paragraphs and structure of many Eger et al. papers in the same field.This again raises the question whether there should be a threshold for the acceptable amount of AI-generated content in scientific work [155].Therefore it is crucial that automatically generated text is always assessed by a human expert.Factual consistency and truthfulness are issues which need to be reviewed by a human in the loop for all types of text-based generated content.Current proprietary LLMs for example struggle in particular with generating existing and correct bibliographic citations.However, LLMs are advancing rapidly and studies evaluating LLMs are quickly outdated.Still, several ethical issues arise when text-generating systems are included in the scientific writing process, such as authorship, plagiarism, bias, and truthfulness.Therefore, in future research a focus on trustworthy, ethical AI systems is required.</p>
<p>Multimodal Content Generation and Understanding</p>
<p>Multimodal content generation in the scientific domain refers to generating multimodal scientific content such as figures and tables in scientific papers or, e.g., slides and posters in a post-publication process.</p>
<p>Automatizing such tasks via AI is important for multiple reasons: (i) generating high-quality figures, tables, slides and posters is difficult and time-consuming (dimension: cost); (ii) high-quality multimodal content in a paper can have a large effect (dimension: benefit for authors) on citation or acceptance decisions [123]; (iii) tables, figures, posters and slides make scientific content easier accessible for a scientific audience and often represent compact representations of research results (dimension: benefit for readers).Multimodal scientific content understanding refers to understanding scientific images and tables, e.g., answering questions about the multimodal scientific content, providing captions or summaries for scientific figures and tables.Automatizing this understanding process promises to allow to automatically describe such multimodal objects, which can likewise be time-consuming and costly for human authors, and helps readers digest the content more easily.6.4m images and 3.9m captions from 572k papers Arxiv FigureQA [103] &gt; 100k scientific-style figures Synthetic ChartQA [159] 4.8k charts, 9.6k QA pairs statista.com,pewresearch.com,etc. CharXiv [243] 2.3k charts with descriptive and reasoning questions Arxiv ArxivQA [130] 35k figures with 100k QA pairs Arxiv SPIQA [177] 152k figures with 270k QAs 19 top-tier academic conferences ChartSumm [248] 84k charts Knoema SciMMIR [246] 530K figures and tables image-text pairs Arxiv Scientific Figure Generation DaTikZ [14,15] 180k-360k pairs of text captions and Tikz code Mostly Arxiv and TeX.StackExchange ScImage [263] 404 instructions and 3k generated scientific images Manual (template) construction SciDoc2DiagramBench [161] 1,080 Extrapolated-Diagrams with the format of "<paper(s), intent of diagram, gold diagram>" ACL Anthology ChartMimic [204] 1000 triplets of (figure, instruction, code) instances Physics, Computer Science, Economics, etc.</p>
<p>Scientific Table Understanding</p>
<p>SciGen [163] 1.3k pairs of scientific tables and their descriptions Arxiv (especially cs.CL and cs.LG) NumericNLG [216] 1.3k pairs of scientific tables and their descriptions ACL Anthology SciXGen [32] 484k tables from 205k papers Arxiv Scientific are more than 1m questions generated using 15 different templates.Later research focuses on harder and more realistic QA pairs.Masry et al. [159] present ChartQA, which provides complex reasoning questions over charts sourced from various sources related to science.Wang et al. [243] introduce CharXiv, a manually curated dataset of 2.3k "natural, challenging, and diverse" charts from Arxiv papers and both descriptive and reasoning questions for them.Li et al. [130] introduce ArxivQA, a dataset of 35k scientific figures sourced from Arxiv for which GPT4o generates 100k QA pairs after manual filtering.Pramanick et al. [177] present SPIQA, a dataset of 270k manually and automatically created QA pairs to interpret complex scientific figures and tables.In contrast to focusing on question-answering for scientific figures, Xu et al. [248] consider the chart summarization problem and a dataset comprising more than 190k instances building on top of existing datasets such as ChartSumm [185], which contains more than 84k charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries.</p>
<p>Scientific Figure Generation.</p>
<p>Recently, several datasets for scientific figure generation have been proposed.Belouadi et al. [14,15] propose DaTikZ and DaTikZ-v2 which contain (augmented) captions of scientific figures as instructions along with corresponding TikZ code, sourced from Arxiv submissions.Belouadi et al. [15] also provide SketchFig, a benchmark of 549 figure-sketch pairs to convert sketches into scientific figures; SketchFig is sourced from TEX stackexchange.Shi et al. [204] provide ChartMimic, a manually curated benchmark of 1000 triplets of (instruction, code, figure) instances for Chart generation across various domains (Physics, Computer Science, Economics, etc.).</p>
<p>Eger et al.</p>
<p>ChartMimic is obtained by having human annotators write Python code for prototype charts.Zhang et al. [263] provide ScImage, which contains targeted template instructions focusing on different understanding dimensions (spatial, numeric, attribute).For a subset of the data, the authors also provide reference figures, evaluated as being of high-quality by human annotators.In contrast to the other benchmarks, ScImage also contains instructions in non-English languages.</p>
<p>Mondal et al. [161] provide SciDoc2DiagramBench, a benchmark comprising 1,000 extrapolated diagrams paired with 89 ACL papers, along with human-written intents.All diagrams are extracted from the corresponding presentation slides of these papers.The intents describe how the content from each paper can be translated into the extrapolated diagrams for presentation purposes.Luo et al. [152] provide nvBench, a benchmark of 25k tuples of natural language queries and corresponding visualizations.nvBench is based on 153 databases and contains more than 7k visualizations on seven chart types.nvBench is synthesized from natural language to SQL benchmarks.</p>
<p>Scientific Table Understanding. Table understanding often comes as table-</p>
<p>to-text generation, which focuses on producing accurate textual descriptions that reflect table content.SciGen [163] and numericNLG [216] are benchmarks specifically focused on scientific table reasoning, both emphasizing arithmetic reasoning over numerical tables, containing 1.3k expert-annotated tables.The annotations include the tables and parts of the scientific papers that describe the corresponding findings of the annotated tables.A specific subtask of these benchmarks is explored in Ampomah et al. [5], which focuses on generating textual explanations for tables reporting ML model performance metrics.This dataset pairs numerical tables of classification performance (e.g., precision, recall, and accuracy) with expert-written textual explanations that analyze and interpret the metrics.Datasets like HiTab [38] tackle the complexity of hierarchical tables commonly found in statistical reports, introducing numerical reasoning tasks that require models to account for implicit relationships and hierarchical indexing within tables.SciXGen [32] broadens the scope of table-to-text generation with context-aware scientific text generation.By drawing from over 200k scientific papers, SciXGen requires models to generate descriptions for tables, figures, and algorithms, grounded in the surrounding body text.[49,101,205], the process of converting unstructured textual information into structured tabular formats.This process is particularly valuable for scientific domains where textual data often contains detailed experimental results, observations, or findings that need transformation into structured tables.In the scientific domain, ArxivDIGESTables [167] addresses the specific challenge of automating the creation of literature review tables.Rows in these tables represent individual papers, while columns capture comparative aspects such as methods, datasets, and results.ArxivDIGESTables supports the generation of literature review tables by leveraging additional grounding context, such as captions and in-text references.</p>
<p>Scientific Table Generation. Table generation often comes in the form of text-to-table generation</p>
<p>Scientific Slide and Poster Generation.Most early efforts to automatically generate presentation slides from scientific papers relied on relatively small datasets for system development and evaluation.For example, Sravanthi et al. [211] collect source code (.tex files and figures) from eight papers and generate presentations based on them.Similarly, Hu and Wan [93] and Wang et al. [240] utilize 1,200 paper-slide pairs and 175 paper-slide pairs, resp., within the CS domain.For scientific poster generation, Qiang et al. [183] construct a dataset of 25 pairs of scientific papers and their corresponding posters.However, these datasets are often inaccessible to the public due to various constraints.Two open-source datasets have emerged as widely used resources for follow-up research for scientific slide generation:</p>
<p>DOC2PPT [68] and SciDuet [217].DOC2PPT [68] comprises 5,873 paired scientific documents and their associated presentation slide decks with around 100,000 slides, drawn from three research communities: computer vision (CVPR, ECCV, BMVC), natural language processing (ACL, NAACL, EMNLP), and machine learning (ICML, NeurIPS, ICLR).</p>
<p>SciDuet [217] comprises 1,088 papers and 10,034 slides from conferences such as ICML, NeurIPS, and the ACL Anthology.</p>
<p>3.4.2Methods and Results.In the following, we survey approaches to multimodal content generation and understanding.A summary table, along with additional related works, is provided in Appendix A.2.4.</p>
<p>Scientific figure understanding.Scientific figure understanding is typically framed in terms of (visual) QA, e.g., whether models are able to adequately answer questions on a given input figure [108].Several recent works train baseline models such as transformers [159] or alternatives [103] such as [196] as well as explore recent LLMs on benchmarks [130,243].</p>
<p>They generally show a big gap between proprietary models like GPT4o and the strongest open-source models and a big gap of all models to human performance.For chart summarization, Rahman et al. [185] find that older PLMs such as BART and T5 suffer from hallucination and missing out of important data points.Xu et al. [248] propose ChartAdapter, a lightweight transformer module that can be combined with LLMs for improved modeling of chart summarization.</p>
<p>Evaluation of scientific figure understanding benchmarks mostly leverages automatic metrics.For example, Xu et al. [248] report out-dated and unreliable metrics such as BLEU and ROUGE for evaluating chart summaries; Pramanick et al. [177] report both human and automatic evaluation, using traditional QA metrics such as Meteor, Rouge, and BERTScore and novel LLM based metrics.</p>
<p>Scientific Figure Generation.</p>
<p>Early work in the context of visualization for science (and beyond) dates back to the 1980s and 1990s at least [156,191,192].Subsequent research [165,209,220] further developed rule-based or hybrid approaches, involving parsers and grammars, while custom neural architectures were later also explored [51,145,224].</p>
<p>Most recent work leverages multimodal LLMs.While Maddigan and Susnjak [157] explore diverse pre-trained LLMs, such as ChatGPT and GPT3, Voigt et al. [229] investigate smaller LLMs for real-time graphics generation on a CPU.Belouadi et al. [14,15] treat the problem as a TikZ code generation problem where the input is (i) a scientific caption [14] or (ii) a sketch or image [15].Both works fine-tune custom LLMs on datasets leveraged from Arxiv.Shi et al. [204] aim to generate Python code from instructions and/or images, specifically focusing on charts.They evaluate 3 proprietary and 11 open-weight LLMs on their ChartMimic benchmark, finding that even the best models (GPT-4 and Claude-3-opus) have substantial room for improvement.Zhang et al. [263] provide a template based approach to evaluate various multimodal LLMs in generating scientific images.They explore LLMs that can generate code (TikZ and Python) and ones that directly generate images, without intermediate code synthesis and in addition consider different input languages (English, German, Chinese, Farsi).They find that, except for GPT4o, most models struggle substantially in generating adequate scientific images.Zala et al. [260] explore the diagram generation task where LLMs first generate diagram plans and then the diagrams themselves.Mondal et al. [161] explore the same task with an additional refinement (feedback from multiple critic models) to enhance factual correctness.Evaluation of the models comprises automatic metrics including DreamSim [67], for image similarity, crystal Bleu [60] for code similarity and ClipScore [87] for text-image similarity, and human evaluation by 'domain experts'.The former are typically reported to have low or medium correlation with the latter, establishing the need for domain specific evaluation in future work.-to-text generation encompasses a range of methodologies designed to transform structured tabular data into coherent and accurate textual descriptions.These techniques process, reason over, and utilize tabular structures to address challenges such as logical reasoning, content fidelity, and domain-specific adaptation.</p>
<p>Scientific Table Understanding. Table</p>
<p>Serialization is a foundational approach where tables are linearized into sequences compatible with transformer-based language models.In this method, tables are converted into linear text sequences using special characters to delineate structure [7,163,169].Structure-aware methods explicitly model the inherent relationships and hierarchies within tables to enhance reasoning and generation fidelity.These include (a) Intermediate Representations [133,267,268];</p>
<p>(b) Structure-Aware Pretraining [116,172,251]; (c) Structure-Aware Self-Attention Mechanisms [146,234].Evaluation:</p>
<p>Common metrics like BLEU and BARTScore are widely used to evaluate the fluency and relevance of generated text against reference outputs.However, ensuring faithfulness to the source table remains a significant challenge, often requiring human evaluation for accurate assessment [163,172].Generation.While none of the existing approaches have yet been applied specifically to scientific table generation, several methodologies present promising directions.The gTBLS (Generative Tables) approach [221] proposes a two-stage table generation process.The first stage infers the table structure from input text, while the second stage generates table content by formulating table-guided questions; this enhances syntactic validity and logical coherence of generated tables.In the context of open-structure table extraction, OpenTE [53] tackles the task of extracting tables with intrinsic semantic, calculational, and hierarchical structure from unstructured text.OpenTE introduces a three-step pipeline that identifies semantic and relational connections among table columns, extracts structured data, and grounds the output by aligning extracted data with the source text and table structure.The evaluation of text-to-table generation should focus on structural accuracy, value fidelity, and semantic coherence.TabEval [186] provides a promising direction by introducing a decomposition-based framework that breaks tables into atomic statements and evaluates them using entailment-based measures, though comprehensive evaluation still requires further advancements.</p>
<p>Scientific Table</p>
<p>Scientific Slide and Poster Generation.For scientific slide generation, early works typically relied on heuristic rulebased approaches [211].Later, researchers began to leverage machine learning approaches to extract key phrases and their corresponding important sentences [93,127,240].All the aforementioned works focus on extracting sentences or phrases from the given paper to serve as the slide text content ("extractive approach").In contrast, Fu et al. [68] and Sun et al. [217] take a different approach by training sequence-to-sequence models to generate sentences for the slide text content ("abstractive approach").With recent advancements, researchers have started utilizing (multimodal) LLMs for generating scientific presentation slides [11,158,162].Notably, all existing approaches take an extractive approach, where they extract images or tables directly from the original papers rather than generating new ones [11,68,162,211,217].Generating posters from scientific papers has received less attention.Previous work has mainly explored different machine learning-based methods for generating key content and panel layouts from data [183,250].</p>
<p>Evaluation: For scientific slide generation, most works evaluate the effectiveness of proposed approaches using automatic evaluation metrics and conduct human evaluation.The most common automatic evaluation metric is ROUGE.studies have also begun utilizing LLMs to assess the quality of the generated slides [11,158].For scientific poster generation, in addition to conducting user studies, Qiang et al. [183] also measure the mean-square error (MSE) of the panel parameters (e.g., panel size, aspect ratio).</p>
<p>Ethical Concerns.</p>
<p>Ethical concerns relating to models for figure, table, slide and poster generation especially include that these tools are technically limited (e.g., they may hallucinate content, be factually incorrect, and not correspond to the authors' intentions), which could be overlooked, ignored or maliciously abused by human authors.</p>
<p>Such tools could also be misused to attack the scientific process, by purposefully producing incorrect results (e.g., as a testcase for adequate reviewing).Such risks may also be relevant in an educational context, e.g., when students use such tools for preparing term papers or theses.</p>
<p>3.4.4Domains of Application.Many recent datasets for multimodal content generation and understanding are from ArXiv and more generally the STEM domain (science, technology, engineering and mathematics).Models such as DeTikZify and AutomaTikZ have also been fine-tuned on such data.This indicates a limitation both in terms of application scenarios and model assessments, as these may perform worse when applied in cross-domain scenarios.</p>
<p>3.4.5Limitations and Future Directions.Common limitations among all multimodal generating and understanding approaches discussed include: (1) comparatively small size of datasets for fine-tuning models; (2) models often perform considerably below human performance on recently proposed benchmarks; (3) this concerns especially open-source non-proprietary models; (4) models and benchmarks are often limited to STEM domains and particularly Arxiv; (5) models lack reasoning abilities; (6) evaluation of models is difficult, especially for generation models, and current automatic metrics are often unsatisfactory.Specific problems occur in subfields: for example, for table generation, input text may be very long, which constitutes a problem for many current LLMs; for slide generation, there are no approaches that can generate slides from multiple documents (e.g., for tutorials) or that generate content beyond a paper (which may be necessary to include relevant background); for figure generation, models like AutomaTikZ are trained on captions, which are often not appropriate for generating the corresponding figure (e.g., a caption may simply be 'Proof of Theorem X'), leading to mismatch between input descriptions and output figures.Fig. 3. Process of AI-enhanced peer review.In the analysis step, the LLM reviewer examines research manuscripts and evaluates peer reviews to assess scientific rigor.The review step involves providing feedback on the paper and verifying scientific claims.Finally, the gathered information is synthesized to generate a final meta-review.</p>
<p>The highest standard in scientific quality control is peer reviewing.In this process, the authors present their scientific argument (e.g., the findings of a study, a grant proposal, etc.), in form of a manuscript to their peers, who then assess its scientific validity and excellence.Often, this process has multiple stages, as shown in Fig. 3.For instance, in the ACL Rolling Review system, 8 reviewers write detailed assessments.Afterwards, the authors may rebut the reviewers' arguments and clarify questions to convince them to raise their scores.Finally, a meta-reviewer re-evaluates the whole scientific discussion and gives a final acceptance recommendation (which the overall program chairs may or may not adhere to).During this process, multiple (potentially multi-modal) artifacts are involved and created, mainly the manuscript under review, the written reviews, the author-reviewer discussion texts, and the meta-review.In general, peer review is considered a challenging, and subjective process, where reviewers are prone to unfair biases like sexism and racism, often relying on quick, simple heuristics [e.g., 188,214].At the same time, we are faced with an exploding number of submissions in some fields like AI [119], pushing peer reviewing systems to the limits of their capacities.</p>
<p>To counteract this problematic situation, researchers have worked on several problems under the umbrella of AIsupported peer review.Related overviews on the topic (or on some of its aspects) are given by [31,54,117,120,142,213].</p>
<p>pointing to the high relevancy of this problem.Here, we focus on existing works targeting the most established tasks, following the same structure as before, and provide an update on the recently published literature.</p>
<p>3.5.1 Data.Peer reviewing data is generally scarce, given that the scientific communities do not always make all reviewing artifacts publicly available under openly accessible licenses, with some exceptions like ICLR.As some exceptions, the PeerRead [104] collection of data from various sources (e.g., ACL, ICRL) and CiteTracked [174] are published along with citation information.As a prime example of how larger-scale open publishing of raw peer reviewing data may work, Dycke et al. [57] recently published the NLPeer corpus based on ARR reviews, for which they explicitly obtained the consent of the respective actors involved.For several tasks around peer review analyses,</p>
<p>Dataset Size Sources Application</p>
<p>HedgePeer [74] 2,966 documents ICLR 2018 reviews Uncertainty detection PolitePeer [17] 2,500 sentences Various, e.g., ICLR Politeness Analysis COMPARE [207] 1,800 sentences ICLR Comparison Analysis ReAct [40] 1,250 comments ICLR Actionability Analysis MReD [203] 7,089 meta-reviews ICLR Meta-review analysis and generation CiteTracked [174] 3 researchers have created annotated datasets.An overview of annotated and/ or task-specific datasets focusing on diverse aspects of peer review is provided in Table 5.Most recently, researchers focused on curating resources for supporting more complex tasks, like understanding the effect of peer review feedback on revisions of the manuscript [48] or on identifying the underlying attitudes that cause a specific criticism in peer review [180].</p>
<p>3.5.2Methods and Results.Initial works were mostly based on more traditional machine learning methods and targeted simpler analyses involving sentence classification tasks.Later, deep learning approaches (also based on pre-trained language models) and more complex analyses, e.g., argumentation analyses, were defining the state-of-the-art in computational peer review processing.Nowadays, researchers started exploring LLMs in prompting-based frameworks for complex tasks like peer review generation and meta-review generation.</p>
<p>Analysis of Peer Reviews.Prior works have analyzed peer reviews for a multitude of aspects, like uncertainty [74], politeness [17], and sentiment [27].However, given that science as a whole and especially peer review relies to a large extent on convincing peers, large efforts have been spent on understanding arguments or argument-related aspects (e.g., substantiation of arguments) in peer review artifacts [e.g., 66,95].Here, most approaches leveraged pre-trained language models.For instance, Hua et al. [95] work on mining the arguments in peer reviews using conditional random fields, LSTMs, and BERT.In contrast, Guo et al. [81] and Fromm et al. [66] fully rely on (domain adjusted) pre-trained language models for argument mining like SciBERT, ArgBERT, and PeerBERT.Cheng et al. [37] leverage multi-task learning approaches based on LSTMs and BERT.In a similar vein, Purkayastha et al. [180] study the generation of rebuttals for author-reviewer discussions based on Jiu-Jitsu argumentation, a specific theory in argumentation theory.</p>
<p>Paper Feedback and Automatic Reviewing.Several works have explored methods to provide general feedback on scientific publications to fully or partially automate peer reviews.For instance, Li et al. [129] propose a multi-task learning approach for peer review score prediction, where different aspect score prediction tasks (e.g., novelty) can inform each other.Ghosal et al. [75] leverage the concept of sentiment to predict scores based on review texts.In a similar vein, Bharti et al. [18] leverage paper-review interactions to predict final decisions of a review process.Wang et al. [239] focus on explainability during review score prediction for several review categories by constructing knowledge graphs (e.g., one which represents the background of a paper).More recent works have included the generation of feedback texts into the problem setup.Bartoli and Medvet [13] frame the problem as exploring the potential of GPT-2 for conducting academic fraud by generating fake reviews.In contrast, Yuan et al. [259] ask whether it would be possible to automate reviewing leveraging targeted summarization models, a recently trending topic.For instance, Liu and Shah [149] explore prompting-based review generation with several LLMs like GPT-4, Vicuna, Llama.They find that GPT-4 performs best among the models tested and that task granularity matters.Similarly, Robertson [190] find GPT-4 to be "slightly" helpful for peer reviewing, and Liang et al. [140] demonstrate in a comparative study that users of a GPT-4-based peer review system found the feedback to be (very) helpful in more than half of the cases.D'Arcy et al. [47] show a multi-agent approach with LLMs that engage in a discussion to produce better results than a single model.Scientific Rigor.Several attempts have been made to computationally analyze the rigor of scientific papers.For example, Soliman and Siponen [208] investigate how researchers use the word "rigor" in information system literature but discovered that the exact meaning was ambiguous in current research.Additionally, various automated tools have been proposed to assess the rigor of academic papers.Phillips [173] develop an online software that spots genetic errors in cancer papers, while Sun et al. [218] use knowledge graphs to assess the credibility of papers based on meta-data such as publication venue, affiliation, and citations.However, these methods are neither domain-specific nor do they provide sufficient guidance for authors to improve their narrative and writing.In contrast, SciScore [202] is an online system that uses language models to produce rigor reports for paper drafts, helping authors identify weaknesses in their presentation.More recently, James et al. [100] propose a bottom-up, data-driven framework that automates the identification and definition of rigor criteria while assessing their relevance in scientific texts.Their framework integrates three key components: rigor keyword extraction, detailed definition generation, and the identification of salient criteria.Additionally, its domain-agnostic design allows for flexible adaptation across different fields.</p>
<p>Scientific Claim Verification.The increasing number of publications requires the development of automated methods for verifying the validity and reliability of research claims.Scientific fact verification, which aims to assess the accuracy of scientific statements, often relies on external knowledge to support or refute claims [52,228].Several datasets have been developed to address this including SciFact-Open [232], which provides scientific claims and supporting evidence from abstracts.However, they are limited to the use of abstracts as the primary source of evidence.As the statements in abstract can also be inaccurate (e.g.overstated claims), it is important to evaluate the evidence in the main body of the paper to determine if the statements made in the abstract are well supported.On the other side, Glockner et al. [77,78] propose a theoretical argumentation model to reconstruct fallacious reasoning of false claims that misrepresent scientific publications.The need to contextualize claims with supporting evidence is highlighted by Chan et al. [30], who introduce a dataset of claims extracted from lab notes.Unlike other datasets, this resource contains claims "actually in use", providing a more realistic understanding of how researchers interact with scientific findings.The authors annotate these claims with links to figures, tables, and methodological details, and develop associated tasks to improve retrieval.</p>
<p>While this provides valuable resources for context-based verification, it primarily focuses on factual verification and does not evaluate the potential for overstated claims.Beyond factual correctness, there is a growing recognition for the need to analyze how researchers present their findings.This includes the detection of overstatements, where authors exaggerate their achievements, and understatements, where the true impact of the research is downplayed [106].Such analysis goes beyond the simple fact of a claim and is necessary to understand the presentation of a claim.Schlichtkrull et al. [200] present a qualitative analysis of how intended uses of fact verification are described in highly-cited NLP papers, particularly focusing on the introductions of the papers, to understand how these elements are framed.The work suggests that claims should be supported by relevant prior work and empirical results.Meta Review Generation.Kumar et al. [118] tackle meta-review generation using a multi-encoder transformer network, and Li et al. [132] use a multi-task learning approach for refining pre-trained language models for the task.Stappen et al. [212] explore the aggregation of reviews for providing additional computational decision support to editors based on uncertainty-aware methods like soft labeling.Both Zeng et al. [261] and Santu et al. [197] rely on LLMs which they specifically prompt for the task.</p>
<p>Ethical Concerns.</p>
<p>Given the critical role of scientific peer review for science, and, accordingly, for society as a whole, ethical considerations around AI-supported peer review are of utmost importance.As the general concerns around unfair biases in AI and the resulting harms apply [120], research on safe peer-reviewing support needs to be prioritized.For instance, von Wedel et al. [230] recently showed that LLMs exhibit affiliation biases when reviewing abstracts.In this context, any AI-support for peer reviewing needs to be critically evaluated [198], and solutions that target only a particular aspect in a collaborative environment that leaves the scientific autonomy to the human expert, may need to be preferred over end-to-end reviewing systems.</p>
<p>3.5.4Domains of Application.Generally, peer review comes in many variations.For instance, the specific aspects to review for, how much textual content to produce, the specific scoring schemes, and the envisioned stages and dynamics of the reviewer and reviewer-author discussions may change.Thus, while none of the studies presented above targets a problem that is truly unique to any scientific domain, the particularities will likely be very different for each specific community and existing systems will need to be carefully evaluated before deployment.</p>
<p>3.5.5Limitations and Future Directions.For existing studies on peer review, in particular, the variety of scientific domains that have been studied is still limited.As most of the works rely on data from OpenReview, most studies focus on peer review within the ICLR and ACL communities [e.g., 40,109].To the best of our knowledge, for some domains, no single data set (yet, a data set further enriched with annotations or other additional information) exists (e.g., legal studies).Furthermore, scientific rigor, a critical aspect of peer review, remains underexplored.Most existing studies rely on predefined rigor checklists, such as those suggested by the NIH and MDAR [29], which are not easily scalable or transferable across different domains.Given these gaps, future research could benefit from exploring new domains of peer review, developing domain adaptation approaches, and advancing models for assessing scientific rigor.</p>
<p>Additionally, in light of the ethical concerns discussed earlier, it is crucial to prioritize research on trustworthy AI support for peer review -ensuring that human experts retain autonomy in the review process.</p>
<p>Ethical Concerns</p>
<p>By now, there is a body of work addressing major ethical concerns related to generative AI.Baldassarre et al. [10], for instance, present a systematic literature review regarding the social impact of generative AI, especially taking into account 71 papers on ChatGPT.They identify the following areas of concern: privacy, inequality, bias, discrimination, and stereotypes.Another literature review on ethics and generative AI conducted by Hagendorff [82] identifies the following topics as becoming increasingly of interest: jailbreaking, hallucination, alignment, harmful content, copyright, models leaking private data, impacts on human creativity.The work also identifies 19 distinct clusters of ethics topics with fairness/bias being the most frequently mentioned, followed by safety, harmful content/toxicity, hallucinations, privacy, interaction risks, security/robustness on ranks two to six, and writing/research on rank 18. Ali and Aysan [2] review 364 recent papers on generative AI and ethics published from 2022 to 2024 in different domains including the use of generative AI in scientific research.Regarding academia, the prevalent topics identified as critical are the authenticity of the work, intellectual property and academic integrity.Sun et al. [219] argue that in application areas such as scientific research, ensuring the trustworthiness of LLMs is crucial.In particular truthfulness, i.e., the accurate representation of information, facts and results by an AI systems, is an essential challenge for LLMs.Some benchmarks and datasets are designed to evaluate different aspects of truthfulness, for instance: TruthfulQA [144], HaluEval [128], and the FELM dataset [269] identify hallucinations.SelfAware [256] assesses the awareness of knowledge limitations.FreshQA [231] and Pinocchio [256] explore adaptability to rapidly evolving information.</p>
<p>TrustLLM [219] is an extensive benchmark incorporating existing and new datasets on the six aspects truthfulness, safety, fairness, robustness, privacy, and machine ethics to assess the trustworthiness of LLMs.Their results show that, in general, proprietary LLMs (e.g., ChatGPT, GPT-4) outperform most open-source LLMs in trustworthinesswith Llama2 [225] as an exception.However, both proprietary LLMs, as well as Llama2 often struggled to provide truthful responses when relying solely on internal knowledge.However, their performance improved significantly with additional external knowledge.Moreover, the authors observed a positive correlation between trustworthiness and the functional effectiveness of the model in downstream tasks.</p>
<p>Editors of scientific publications are particularly challenged as the proportion of AI generated text in academic manuscripts is steadily increasing [36,80,114,141] and AI models are also on the brink of being used in peer reviewing, cf.Section 3.5.The editors-in-chief of the Journal of Information Technology, for instance, elaborate in [199] on the limitations and risks of using generative AI in the production of scientific publications.They refer to the eight-point 'Artificial Imperfections' test to illustrate current limitations of generative AI: AI is (1) brittle, (ii) opaque, (iii) greedy, (iv) shallow and tone-deaf, (v) manipulative and hackable, (vi) biased, (vii) invasive, (viii) 'faking it'.Nevertheless, the editors conclude that the use of AI should not be forbidden, however, if it is used, the authors must take full responsibility of the outcome, adhere to the "scientific principle of transparency" and give full and transparent disclosure of the usage of AI in the respective publication, and moreover that "it is then up to the reviewers and editors to assess and make decisions on the specific use of that generative AI in a specific piece of research."Similarly, Pu et al. [179] in their editorial to iMeta 3(2), 2024 state that AI-assisted technologies cannot be recognized as authors, the use of generative AI in a scientific manuscript/publication must be transparently disclosed, including the prompts, specific versions of the tools used.The authors are fully responsible for the integrity of their manuscripts.They must address ethical concerns and ensure the accuracy and fairness of AI-generated content, comply with data protection and privacy laws, and also consider copyright and intellectual property issues surrounding AI-generated content.The use of AI-generated images and multi-media must be specifically allowed.They explicitly prohibit the use of AI in the reviewing process.</p>
<p>Conclusion</p>
<p>In this paper, we surveyed approaches in the area of AI4Science, with a particular focus on recent large language model-based methods.We examined five key aspects of the research cycle: (1) search, (2) experimentation and research idea generation, (3) text-based content production, (4) multimodal content production, and (5) peer review.For each topic, we discussed relevant datasets, methods, and results, including evaluation strategies, while highlighting limitations and avenues for future research.Ethical concerns featured prominently in our survey, given the potential for misuse and challenges in maintaining scientific integrity in the face of AI-assisted content generation.</p>
<p>We hope that this survey inspires new initiatives in AI4Science, driving faster, more efficient, and more inclusive scientific discovery, experimentation, reporting and content synthesis-while upholding the highest ethical standards.</p>
<p>As the ultimate goal of science is to serve humanity, we hope these advancements will accelerate knowledge creation and enhance the accessibility and reliability of research, leading to improved healthcare, medical treatments, economic processes, among a myriad of other societal benefits.</p>
<p>Eger et al.</p>
<p>Fisher, whose seminal works on statistical methods [64] and experimental design [65] popularized the principles of randomization (assigning experimental subjects by chance), replication (observing different experimental subjects under the same conditions), and blocking (eliminating undesired sources of variation).Besides these considerations, experimental design involves the determination of the (statistical) analysis that will be performed, and is often constrained by the availability of resources such as the time, effort, or cost to gather and analyze observations or data [112].</p>
<p>The final step in the scientific cycle, reporting, encompasses the dissemination of research findings, typically but not exclusively to the wider scientific community through articles, books, and presentations.The practice of scientific communication has itself attracted scientific study, leading to descriptive and pedagogical treatments of its various processes and strategies (e.g., [83,257]).The essential role of peer review [244] has attracted special attention, albeit more on its high-level processes, its efficacy and reliability, and its objectivity and bias rather than on how reviewers go about evaluating manuscripts and communicating this evaluation.Accordingly, technological developments in the peer review workflow have until very recently tended to focus on managing or streamlining the review process for the benefit of the editor and publisher, or on supporting open or collaborative reviewing [55,244].Search engines.Traditional academic search engines such as Google Scholar, Semantic Scholar, Baidu Scholar, Science.gov, and BASE, as shown in Table 6, are characterized by their broad literature coverage, citation tracking capabilities, and keyword-based search functionality.Their primary advantages include extensive indexing of scholarly content, which involves aggregating and organizing vast amounts of academic documents from various sources such as publisher websites, institutional repositories, and open-access archives.This comprehensive indexing spans multiple disciplines and document types, ensuring that users can access a diverse set of resources.Additionally, these platforms</p>
<p>Human &amp; various metrics</p>
<p>Scientific Table Understanding</p>
<p>Table description [163] Tables from scientific articles</p>
<p>Table description SciGen</p>
<p>Fine-tuning Automatic &amp; human evaluation Numerical reasoning [216] Tables from scientific papers Numerical descriptions</p>
<p>NumericNLG</p>
<p>Fine-tuning Automatic &amp; human evaluation</p>
<p>Scientific Table Generation</p>
<p>Literature review table generation [167] A list of papers relevant sentences from papers.These sentences are used to generate draft slides for four topics: Contribution, Dataset, Baseline, and Future Work.</p>
<p>It is important to note that all the aforementioned works focus on extracting sentences or phrases from the given paper to serve as the slide text content.In contrast, Fu et al. [68] and Sun et al. [217] take a different approach by training sequence-to-sequence models to generate sentences for the slide text content.This distinction is analogous to the difference between "extractive" and "abstractive" summaries in text summarization.More specifically, Fu et al. [68] design a hierarchical recurrent sequence-to-sequence architecture to encode the input document, including sentences Various input types including sketches, screenshots, and text, can be used to generate TikZ code with tools such as AutomaTikZ [14] and DeTikZify [15].The generated code is then rendered into high-quality vector graphics images.</p>
<p>and images, and generate a slide deck.In contrast, Sun et al. [217] assume that slide titles would be provided by end users.The authors use these titles to retrieve relevant and engaging text, figures, and tables from the given paper using a dense retrieval model.They then summarize the retrieved content into points with a fine-tuned long-form question answering system based on BART.</p>
<p>With recent advancements in LLMs and vision-language models (VLMs), researchers have started utilizing these technologies for generating scientific presentation slides.Mondal et al. [162] propose a system to generate personaaware presentation slides by fine-tuning LLMs such as text-davinci-003 and gpt-3.5-turbowith a small training dataset containing personalized slide decks for each paper.Maheshwari et al. [158] focus solely on generating text content and develop an approach that combines graph neural networks (GNNs) with LLMs to capture non-linearity in presentation generation, while attributing source paragraphs to each generated slide within the presentation.Bandyopadhyay et al.</p>
<p>[11] design a bird's-eye view document representation to generate an outline, map slides to sections, and then create text content for each slide individually using LLMs.The approach then extracts images from the original papers by identifying text-image similarity in a shared subspace through a VLM.</p>
<p>Generating posters from scientific papers has received less attention compared to scientific slide generation.Qiang et al. [183] introduce a graphical model to infer key content, panel layouts, and the attributes of each panel from data.Xu and Wan [250] develop a demo system for automated poster generation.The system first identifies important sections using a trained classifier.It then employs a summarization model to extract key sentences and related graphs from each section to construct corresponding panels.Finally, the system generates a LaTeX document for the poster based on the template selected by the user.</p>
<p>A.3 AI use case and abbreviations</p>
<p>Throughout this paper, we have integrated AI tools to support specific aspects of the research workflow.For example, in the subsection of Literature Search, Summarization, and Comparison, we used Google Search, ChatGPT, NotebookLM, and Scholar Inbox to retrieve relevant tools and related work.Additionally, LLMs assisted with grammar and spell checking, as well as generating code for formatting tables.</p>
<p>Scientific</p>
<p>Literature (Papers, Metaanalyses, Systematic Reviews) Web Data (Scientific Discussions, Social Media Forums) Datasets (Open Databases, Government Statistics, Lab Datasets) Knowledge Graphs (Semantic Scholar, Microsoft Academic) Feasibility Constraints (Resources, Ethics, Funding)</p>
<p>Fig. 2 .
2
Fig. 2. Examples of idea generation, hypothesis generation, and automated experimentation follow a four-component structure: task, sample input, methods, and sample output.The task defines the goal of each process.Sample input consist of benchmark datasets for each task.Methods encompass relevant scientific approaches.Sample output differs by process: idea generation and hypothesis yield textual outputs (descriptions or explanations), whereas automated experimentation produces executable code.</p>
<p>[ 262 ]
262
leverages reinforcement learning to improve long-text generation.The model employs custom reward signals that prioritize coherence, factual accuracy, and linguistic quality.These reward mechanisms are particularly relevant for scientific text generation, where accuracy and adherence to domain-specific conventions are crucial.</p>
<ol>
<li>
<p>3 . 4
34
Domains of Application.Text-based content generation is relevant for all scientific domains.Liang et al.[141] conduct a large-scale analysis across 950,965 paper published between January 2020 and February 2024 to measure the prevalence of LLM modified content over time.The papers they investigated were published on (i) arXiv including the five areas Computer Science, Electrical Engineering and Systems Science, Statistics, Physics, and Mathematics, (ii) bioRxiv, and (iii) Nature portfolio.Their results show the largest and fastest growth in Computer Science with up to 17.5% of the papers containing LLM modified content and the least LLM modifications in Mathematics papers (up to 6.3%).However, according to the Natural Language Learning &amp; Generation arXiv report from September 2024, top-cited papers show notably fewer markers of AI-generated content compared to random samples[124].</p>
</li>
<li>
<p>3 . 5
35
Limitations and Future Directions.Numerous studies have investigated text-based content generation for the scientific domain and have shown their potential to assist scientists in different phases of writing a paper.While for some tasks such as proof-reading and paraphrasing, its capabilities are well established, others pose limitations.</p>
</li>
<li>
<p>4 . 1
41
figures from five classes: (dot-)line plots, vertical and horizontal bar graphs, and pie charts.Associated with the images</p>
</li>
</ol>
<p>Fu</p>
<p>et al. [68] introduce the Longest Common Figure Subsequence, which measures the quality of figures in the generated slides; Text-Figure Relevance (TFR), which assesses the similarity between the text of the ground truth slide and the generated slide containing the same figure; and Mean Intersection over Union, which evaluates layout quality.Recent</p>
<p>A. 2
2
Supplement on AI Support for Specific Topics and Tasks A.2.1 Additional Literature Search, Summarization, and Comparison s o n a l i z a</p>
<p>Fig. 7 .
7
Fig.7.Overview of the scientific figure generation process.Various input types including sketches, screenshots, and text, can be used to generate TikZ code with tools such as AutomaTikZ[14] and DeTikZify[15].The generated code is then rendered into high-quality vector graphics images.</p>
<p>Table Generation
GenerationArXivDigestTables [167]2,228 literature review tables extracted from arXivliterature review tables from ArXiv paperspapers that synthesize a total of 7,542 research paperfrom April 2007 until November 2023Scientific Slides and Poster GenerationSciDuet [217]1,088 papers and 10,034 slides by their authorsNeurIPS/ICML/ACL AnthologyDOC2PPT [68]5,873 papers and 98,856 slides by their authorsCV (CVPR, ECCV, BMVC), NLP (ACL,NAACL, EMNLP), ML (ICML, NeurIPS, ICLR)Persona-Aware-D2S [162] 75 papers from SciDuet, and 300 slidesACL Anthology</p>
<p>Table 4 .
4
Multimodal content generation and understanding datasets.</p>
<p>Table 5 .
5
Annotated or task-specific datasets for analyzing peer reviewing.
,427 papers and 12kNeurIPScitation predictionreviewssMOPRD [143]6,578 papersPeerJReview Comment GenerationRevise and Resubmit5.4k papersF1000ResearchTagging, Linking, Version Alignment[121]ORB [222]92,879 reviewsOpenReview, Sci-Acceptance PredictionPostARIES [48]3.9k commentsOpenReviewFeedback-Edits Alignment, Revision GenerationDISAPERE [109]506 review-rebuttalICLRreview action analysis, polarity prediction, review as-pairspectPeerReviewAnalyze1,199 reviewsICLRReview Paper Section Correspondence, Paper Aspect[73]Category Detection, Review Statement Role Prediction,Review Statement Significance Detection, and Meta-Review GenerationJitsuPeer [180]9,946 review andICLRArgumentation Analysis, Canonical Rebuttal Scoring,11,103 rebuttal sen-Review Description Generation, End2End CanonicaltencesRebuttal Generation</p>
<p>Table 6 .
6
Overview of popular literature search, summarization and comparison tools and their key features.
t i o nCostData Source</p>
<p>Table 7 .
7
Multimodal Content Generation and Understanding Approaches.
Table schema +ArXivDigestPromptingAutomatic &amp; hu-valuesTablesman evaluationScientific Slide and Poster GenerationSingle slide genera-Paper + slide titleSlide contentSciDuetTwo-stepROUGE and hu-tion [217]methodman evaluationSlide deck generationPaperA deck of slidesDOC2PPTHierarchicalAutomatic &amp; hu-[68]generativeman evaluationmodelPersonalizedslidePaper + target audi-A deck of slidesPersona-Fine-tuningAutomatic &amp; hu-deck generation [162]ence (technical or non-Aware-D2Sman evaluationtechnical)
https://www.wiley.com/en-us/ai-study, https://www.nature.com/articles/d41586-025-00343-5
The benefits are expected to be particularly significant for non-native English speakers and those with lower technical skills, potentially increasing diversity and inclusivity in research.
We note two contemporaneous works developed completely independently from us[153,266]. Both are substantially narrower in scope than this survey; for example, Luo et al.[153] neither cover multimodal approaches to scientific content synthesis nor search and also do not address ethical concerns in nearly the same depth as we do.
https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/
https://aclrollingreview.org
AcknowledgmentsYong Cao was supported by a VolkswagenStiftung Momentum grant.Jennifer D'Souza was supported by the SCINEXT project (BMBF, German Federal Ministry of Education and Research, Grant ID: 01lS22070).The NLLG Lab at UTN gratefully acknowledges support from the Federal Ministry of Education and Research (BMBF) via the research grant "Metrics4NLG" and the German Research Foundation (DFG) via the Heisenberg Grant EG 375/5-1.The work of Anne Lauscher is supported by the Excellence Strategy of the German Federal Government and the Federal States.Our AI use cases are document in the supplemental material.AppendixThis appendix provides supplementary materials intended to support and extend the main text.It includes a background overview, further elaboration on AI support for specific topics and tasks, and a section on AI use cases that illustrates how AI tools were integrated into the workflow and phrasing of this paper.A.1 BackgroundQuestion Study Hypothesize Experiment Analyze Report Fig.4. Scientific discovery cycle, after[45]Over time, science has progressed through numerous paradigm shifts, leading to the modern era of data-intensive exploration[88].Despite advancements in tools and methodologies that have accelerated discovery, the fundamental steps of the scientific process have remained consistent.As illustrated in Fig.4, this process typically begins with identifying a research question, followed by reviewing relevant literature, formulating a hypothesis, designing and conducting experiments, analyzing data, and ultimately reporting findings.This iterative cycle ensures the continuous refinement and expansion of scientific knowledge.With respect to the first two of these steps, a major challenge for any scholar is achieving, and then maintaining, sufficient familiarity with existing research on a given topic to be able to identify new research questions or to discover the knowledge required to answer them.Before the 20th century, it was often feasible to keep abreast of developments in a specialty simply by reading all the relevant books and journals as they were published.In modern times, however, the number of scientific publications has been doubling every 17 years[20], making this exhaustive approach unworkable.The need to sift through large quantities of scholarly knowledge spurred the specialization of simple library catalogs (in use since ancient times) into abstracting journals, bibliographic indexes, and citation indexes.By the 1960s and 1970s, many of these resources were being produced with standardized control principles and technologies, and could be queried interactively using automated information retrieval systems[19, pp. 88-91].These technical developments have enabled the widespread adoption of more principled approaches to the exploration of scientific knowledge, such systematic reviews[28]and citation analysis[72].How experts propose hypotheses to explain observed phenomena has been extensively discussed in the philosophy and psychology of science, albeit with little empirical work until relatively recently[41,42].Contrary to the idealized notion of scientific reasoning, hypotheses rarely come about solely through induction (i.e., the abstraction of a general principle from a set of empirical observations).Rather, case studies employing think-aloud protocols suggest that hypotheses are generated through a process of successive refinement.These processes may involve non-inductive heuristics (analogies, simplifications, imagistic reasoning, etc.) that often fail individually, but may lead to valid explanatory models after "repeated cycles of generation, evaluation, and modification or rejection"[41,42].Experimentation and analysis aim to establish a causal relationship between the independent and dependent variables germane to a given scientific hypothesis.The metascientific literature abounds with practical advice on the design and execution of experiments, much of it discipline-specific.However, the general ideas at play can be traced to Ronald offer robust citation analysis features allow researchers to track citation counts, measure the impact of publications, and explore citation networks to identify influential works and emerging trends within a given field.Another significant advantage is their free access to a wide range of academic resources, such as peer-reviewed journal articles, conference papers, preprints, theses and dissertations, technical reports, books and book chapters, as well as grey literature like white papers, government reports, and institutional research outputs.However, these search engines have certain limitations, such as limited AI-driven filtering options and relatively basic relevance ranking mechanisms compared to more advanced AI-enhanced search tools.Benchmarks and leaderboards.Code and Dataset-Focused Search Engines include platforms such as Papers with Code, ScienceAgentBench, and Huggingface, which are specifically designed to bridge the gap between academic publications and practical implementation by linking research papers with associated code and datasets.These platforms facilitate reproducibility and practical application of research findings by aggregating code repositories, enabling researchers and practitioners to easily explore implementations, compare results, and benchmark their models.A key feature of such platforms is their ability to provide dataset discovery tools, which allow users to identify relevant datasets for specific research problems, fostering collaboration and accelerating experimentation cycles.These search engines are particularly valuable for machine learning practitioners, as they facilitate quick access to ready-to-use codebases, helping them implement cutting-edge research more efficiently.Based on these community-curated leaderboards, some studies have proposed models for constructing leaderboards directly from scientific papers[91,107,194].A.2.2 Extended AI-Driven Discovery: Ideation, Hypothesis Development, and ExperimentationFig.5provides a broad methodological overview, grouping methods applied in hypothesis generation, idea generation, and automated experimentation.In contrast, Fig.2presents concrete examples and references to exemplary papers.Hypothesis GenerationHallucination:Long context input:Refinement strategies:Idea GenerationIterative refinement:Human alignment:Multi-agent systems:Automated ExperimentationMulti-agent workflow: Fig.5. Visualization of the hypothesis generation, idea generation, and automated experimentation process.Most works in hypothesis generation focus on reducing hallucinations, handling long contexts, and iteratively refining outputs.To reduce hallucinations, an initial hypothesis is validated against a knowledge base for refinement.For long-context inputs, different contexts are summarized and integrated, while refinement strategies iteratively improve the hypothesis until it meets a satisfactory level.A similar iterative refinement strategy is also applied to idea generation.Additionally, alignment strategies are employed to make generated ideas more thoughtful and feasible.In multi-agent systems, multiple agents collaborate to enhance the idea generation process.In contrast, automated experimentation often relies on tree search for selecting optimal examples, multi-agent workflows where LLMs collaborate on distinct tasks, and iterative refinement to improve task performance.While hypothesis and idea generation leverage diverse sources such as scientific literature, web data, and datasets, automated experimentation operates on predefined ideas and requires access to computational models, simulations, and raw data.A.2.3 Text-based Content GenerationIn this section, we provide additional figure (Fig.6) to illustrate the content generation process for academic papers, covering title, abstract, related work, and bibliography generation with their respective methods.A.2.4 Multimodal content generation and understanding: additional information A.2.4.1 Methods and results.In the following, we provide a summary of representative approaches for multimodal content generation and understanding in Table7, illustrate the process of scientific figure generation in Fig.7, and present an extended description of scientific slide and poster generation.Scientific slide and poster generation.For scientific slide generation, early works typically relied on heuristic rule-based approaches.For instance, Sravanthi et al.[211]develop a rule-based system to generate slides for each section and subsection of a paper.The slide text content is generated using a query-based extractive summarization system.Later, researchers began to leverage machine learning approaches to extract key phrases and their corresponding important sentences.Hu and Wan[93]use a Support Vector Regression (SVR) model to learn the importance of each sentence in a paper.The slides are then generated using an integer linear programming (ILP) model to select and align key phrases and sentences.Wang et al.[240]propose a system to generate slides for each section of a given paper, focusing on creating two-layer bullet points.The authors first extract key phrases from the paper using a parser and then use a random forest classifier to predict the hierarchical relationships between pairs of phrases.Li et al.[127]
Automatic related work section generation: experiments in scientific document abstracting. Ahmed Abura'ed, Horacio Saggion, Alexander Shvets, Àlex Bravo, Scientometrics. 1252020. 2020</p>
<p>Ethical dimensions of generative AI: a cross-domain analysis using machine learning structural topic modeling. Hassnian Ali, Ahmet Faruk Aysan, Int. J. Ethics Syst. 2024. 2024</p>
<p>Artificial intelligence in scientific writing: a friend or a foe? Reproductive. Signe Altmäe, Alberto Sola-Leyva, Andres Salumets, BioMedicine Online. 472023. 2023</p>
<p>An lda-based approach to scientific paper recommendation. Maha Amami, Gabriella Pasi, Fabio Stella, Rim Faiz, Proc. 21. 21Salford, UKSpringer2016. 2016. June 22-24, 2016</p>
<p>Generating Textual Explanations for Machine Learning Models Performance: A Table-to-Text Task. Isaac Ampomah, James Burton, Amir Enshaei, Noura Al, Moubayed , Proc. Thirteen. Lang. Resour. Eval. Conf.. European Language Resources Association. Thirteen. Lang. Resour. Eval. Conf.. European Language Resources Association2022</p>
<p>AI did not write this manuscript, or did it? Can we trick the AI text detector into generated texts? The potential future of ChatGPT and AI in Sports &amp; Exercise Medicine manuscript generation. Nash Anderson, Daniel L Belavy, Sharief Stephen M Perle, Luiz Hendricks, Evert Hespanhol, Verhagen, Aamir R Memon, BMJ open sport &amp; exercise medicine. 9e0015682023. 2023</p>
<p>Table-To-Text generation and pre-training with TabT5. Ewa Andrejczuk, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, Yasemin Altun, In Find. Assoc. for Comput. Linguist. EMNLP. 2022. 2022</p>
<p>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, The Twelfth Int. Conf. on Learn. 2024Represent.</p>
<p>Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li, arXiv:2408.07055Longwriter: Unleashing 10,000+ word generation from long context llms. 2024. 2024arXiv preprint</p>
<p>The social impact of generative ai: An analysis on chatgpt. Maria Teresa Baldassarre, Danilo Caivano, Berenice Fernandez Nieto, Domenico Gigante, Azzurra Ragone, Proc. 2023 ACM Conf. on Inf. 2023 ACM Conf. on InfTechnol. for Soc. Good2023</p>
<p>Enhancing Presentation Slide Generation by LLMs with a Multi-Staged End-to-End Approach. Sambaran Bandyopadhyay, Himanshu Maheshwari, Anandhavelu Natarajan, Apoorv Saxena, Proc. 17th Int. Nat. Lang. Gener. Conf.. ACL. 17th Int. Nat. Lang. Gener. Conf.. ACL2024</p>
<p>Ask the gru: Multi-task learning for deep text recommendations. Trapit Bansal, David Belanger, Andrew Mccallum, proceedings 10th ACM Conf. on Recomm. Syst. 10th ACM Conf. on Recomm. Syst2016</p>
<p>Exploring the Potential of GPT-2 for Generating Fake Reviews of Research Papers. Alberto Bartoli, Eric Medvet, 2020</p>
<p>AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ. Jonas Belouadi, Anne Lauscher, Steffen Eger, Proc. ICLR. ICLR2024</p>
<p>DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ. Jonas Belouadi, Simone Paolo Ponzetto, Steffen Eger, The Thirty-eighth Annu. Conf. on Neural Inf. 2024</p>
<p>Content-Based Citation Recommendation. Chandra Bhagavatula, Sergey Feldman, Russell Power, Waleed Ammar, Proc. Long Pap, Marilyn Walker, Ji Heng, Amanda Stent, nullNew Orleans, LouisianaACL2018. 20181</p>
<p>PolitePEER: does peer review hurt? A dataset to gauge politeness intensity in the peer reviews. Prabhat Bharti, Meith Navlakha, Mayank Agarwal, Asif Ekbal, Lang. Resour. Eval. 2023. 05 2023</p>
<p>PEERAssist: Leveraging on Paper-Review Interactions to Predict Peer Review Decisions. Prabhat Kumar Bharti, Shashi Ranjan, Towards Open Trust. Springer International Publishing2021Tirthankar Ghosal, Mayank Agrawal, and Asif Ekbal</p>
<p>Christine L Borgman, Scholarship in the Digital Age: Information, Infrastructure, and the Internet. MIT Press2007</p>
<p>Growth Rates of Modern Science: a Latent Piecewise Growth Curve Approach to Model Publication Numbers from Established and New Literature Databases. Robin Lutz Bornmann, Rüdiger Haunschild, Mutz, Humanit. Soc. Sci. Commun. 82242021. 2021</p>
<p>Improving the Peer Review of Narrative Literature Reviews. Jennifer A Byrne, Res. Integr. Peer Rev. 1122016. 2016</p>
<p>MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity. Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen. Yixin Cao, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan Liu, Tat-Seng Chua, Proc. 58th Annu. 58th Annu2020</p>
<p>Don't Get Too Technical with Me': A Discourse Structure-Based Framework for Automatic Science Journalism. Ronald Cardenas, Bingsheng Yao, Dakuo Wang, Yufang Hou, Proc. 2023 Conf. on Empir. 2023 Conf. on Empir2023</p>
<p>Good practices for scientific article writing with ChatGPT and other artificial intelligence language models. Andres Castellanos-Gomez, Nanomanufacturing. 32023. 2023</p>
<p>Exploring Scientific Hypothesis Generation with Mamba. Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal, Proc. 1st Workshop on NLP for Sci. 1st Workshop on NLP for Sci2024. NLP4Science</p>
<p>Aspect-based Sentiment Analysis of Scientific Reviews. Souvic Chakraborty, Pawan Goyal, Animesh Mukherjee, Proc. ACM/IEEE Jt. Conf. on Digit. Libr. 2020 (Virtual Event, China) (JCDL '20). ACM/IEEE Jt. Conf. on Digit. Libr. 2020 (Virtual Event, China) (JCDL '20)New York, NY, USAAssociation for Computing Machinery2020</p>
<p>A Brief History of Research Synthesis. Iain Chalmers, Larry V Hedges, Harris Cooper, Eval. &amp; Health Prof. 252002. 2002</p>
<p>Towards minimum reporting standards for life scientists. Karen Chambers, Andy Collings, Chris Graf, Veronique Kiermer, David Thomas Mellor, Malcolm Robert Macleod, Sowmya Swaminathan, Deborah Sweet, 2019. 2019</p>
<p>Overview of the Context24 Shared Task on Contextualizing Scientific Claims. Chu Sern, Joel Chan, Aakanksha Naik, Matthew Akamatsu, Hanna Bekele, Erin Bransom, Ian Campbell, Jenna Sparks, Proc. Fourth Workshop on Sch. Fourth Workshop on Sch2024Document Process. (SDP 2024</p>
<p>AI-assisted peer review. A Checco, L Bracciale, P Loreti, S Pinfield, G Bianchi, Humanit. Soc. Sci. Commun. 82021. January 2021</p>
<p>SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation. Hong Chen, Hiroya Takamura, Hideki Nakayama, Find. Assoc. for Comput. Linguist. EMNLP 2021. ACL. 2021</p>
<p>Automatic generation of related work through summarizing citations. Jingqiang Chen, Hai Zhuge, Concurr. Comput. Pract. Exp. 31e42612019. 2019</p>
<p>Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End. Yanran Chen, Steffen Eger, Proc. 4th Workshop on Eval. 4th Workshop on Eval2023</p>
<p>Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, arXiv:2410.05080[cs.CL]ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery. 2024</p>
<p>Huzi Cheng, Bin Sheng, Aaron Lee, Varun Chaudhary, Nan Atanas G Atanasov, Yue Liu, Tien Yin Qiu, Yih-Chung Wong, Ying-Feng Tham, Zheng, Have AI-Generated Texts from LLM Infiltrated the Realm of Scientific Writing? A Large-Scale Analysis of Preprint Platforms. 2024. 2024</p>
<p>APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning. Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, Luo Si, Proc. 2020 Conf. on Empir. Methods Nat. Lang. Process. (EMNLP). ACL. 2020 Conf. on Empir. Methods Nat. Lang. ess. (EMNLP). ACL2020</p>
<p>HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, Dongmei Zhang, Proc. 60th Annu. 60th Annu20221</p>
<p>Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, arXiv:2410.17238SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning. 2024. 2024arXiv preprint</p>
<p>ReAct: A Review Comment Dataset for Actionability (and more). Gautam Choudhary, Natwar Modani, Nitish Maurya, 2021Springer International Publishing</p>
<p>Learning via Model Construction and Criticism: Protocol Evidence on Sources of Creativity in Science. John Clement, Handbook of Creativity: Assessment, Theory and Research. John A Glover, Royce R Ronning, Cecil R Reynolds, 1989</p>
<p>Multiple Levels of Heuristic Reasoning Processes in Scientific Model Construction. John J Clement, Front. Psychol. 132022. 2022</p>
<p>SPECTER: Document-level Representation Learning using Citation-informed Transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel Weld, Proc. 58th Annu. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, 58th Annu2020ACL, Online</p>
<p>Automatic benchmarking of large multimodal models via iterative experiment programming. Alessandro Conti, Enrico Fini, Paolo Rota, Yiming Wang, Massimiliano Mancini, Elisa Ricci, 2024. jun 2024arXiv preprint</p>
<p>Combining Data and Theory for Derivable Scientific Discovery with AI-Descartes. Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R Josephson, Joao Goncalves, Kenneth L Clarkson, Nimrod Megiddo, Bachir El Khadir, Lior Horesh, Nat. Commun. 1417772023. 2023</p>
<p>Deep neural networks for youtube recommendations. Paul Covington, Jay Adams, Emre Sargin, Proc. 10th ACM conference on recommender systems. 10th ACM conference on recommender systems2016</p>
<p>D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.04259MARG: Multi-Agent Review Generation for Scientific Papers. 2024. 2024arXiv preprint</p>
<p>D' Mike, Alexis Arcy, Erin Ross, Bailey Bransom, Jonathan Kuehl, Tom Bragg, Doug Hope, Downey, arXiv:2306.12587ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. 2023. 2023arXiv preprint</p>
<p>Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. Zheye Deng, Chunkit Chan, Weiqi Wang, Yuxi Sun, Wei Fan, Tianshi Zheng, Yauwai Yim, Yangqiu Song, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>Automatic Related Work Section Generation by Sentence Extraction and Reordering. Zekun Deng, Zixin Zeng, Weiye Gu, Jiawen Ji, Bolin Hua, 2021. 2021</p>
<p>Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks. Victor Dibia, Çağatay Demiralp, IEEE Comput. Graph. Appl. 392019. 2019</p>
<p>Alphaeus Dmonte, Roland Oruche, Marcos Zampieri, Prasad Calyam, Isabelle Augenstein, arXiv:2408.14317Claim Verification in the Age of Large Language Models: A Survey. 2024. 2024arXiv preprint</p>
<p>OpenTE: Open-Structure Table Extraction From Text. Haoyu Dong, Mengkang Hu, Qinyu Xu, Haochen Wang, Yue Hu, ICASSP 2024 -2024 IEEE Int. Conf. on Acoust. Speech Signal Process. (ICASSP). 2024</p>
<p>Human-in-the-Loop AI Reviewing: Feasibility, Opportunities, and Risks. Iddo Drori, Dov Te, ' Eni, J. Assoc. for Inf. Syst. 252024. 2024</p>
<p>The Peer Review Process: Past, Present, and Future. John A Drozdz, Michael R Ladomery, Br. J. Biomed. Sci. 812024. 2024</p>
<p>SemEval-2021 Task 11: NLPContributionGraph -Structuring Scholarly NLP Contributions for a Research Knowledge Graph. D' Jennifer, Sören Souza, Ted Auer, Pedersen, Proc. 15th Int. Workshop on Semantic Eval. (SemEval-2021). Natalie Schluter, Guy Emerson, Aurelie Herbelot, Xiaodan Zhu, 15th Int. Workshop on Semantic Eval. (SemEval-2021)Alexis Palmer, Nathan Schneider2021ACL, Online</p>
<p>NLPeer: A Unified Resource for the Computational Study of Peer Review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, Proc. 61st Annu. Jordan Boyd-Graber, Naoaki Okazaki, 61st AnnuAnna Rogers; Toronto, Canada20231</p>
<p>Sašo Džeroski, Ljupčo Todorovski, Computational Discovery of Scientific Knowledge: Introduction, Techniques, and Applications in Environmental and Life Sciences. Number 4660. Springer2007</p>
<p>D N Ege, M Goudswaard, J Gopsill, B Hicks, M Steinert, The IDEA Challenge 2022 dataset. 2023. 2023</p>
<p>CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code. Aryaz Eghbali, Michael Pradel, Proc. 37th IEEE/ACM Int. Conf. on Autom. Softw. Eng. 37th IEEE/ACM Int. Conf. on Autom. Softw. EngAssociation for Computing Machinery202328</p>
<p>Abstracts written by ChatGPT fool scientists. Holly Else, Nature. 6134232023. 2023</p>
<p>Neural architecture search: A survey. Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, J. Mach. Learn. Res. 202019. 2019</p>
<p>How trustworthy is ChatGPT? The case of bibliometric analyses. Faiza Farhat, Shahab Saquib Sohail, Dag Øivind Madsen, Cogent Eng. 1022229882023. 2023</p>
<p>Statistical Methods for Research Workers. Ronald A Fisher, 1925Oliver and Boyd</p>
<p>The Design of Experiments. Ronald A Fisher, 1935Oliver and Boyd</p>
<p>Argument Mining Driven Analysis of Peer-Reviews. Michael Fromm, Evgeniy Faerman, Max Berrendorf, Siddharth Bhargava, Ruoxia Qi, Yao Zhang, Lukas Dennert, Sophia Selle, Yang Mao, Thomas Seidl, Proc. AAAI Conf. AAAI Conf2021. May 202135</p>
<p>DreamSim: learning new dimensions of human visual similarity using synthetic data. Stephanie Fu, Y Netanel, Shobhita Tamir, Lucy Sundaram, Richard Chai, Tali Zhang, Phillip Dekel, Isola, Proc. 37th Int. Conf. on Neural Inf. Process. Syst. 37th Int. Conf. on Neural Inf. ess. SystCurran Associates Inc2023Article 2208, 27 pages</p>
<p>DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents. Tsu-Jui Fu, William Yang Wang, Daniel J Mcduff, Yale Song, AAAI Conf. on Artif. Intell. 2021</p>
<p>CiteBench: A Benchmark for Scientific Citation Text Generation. Martin Funkquist, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych, Proc. 2023 Conf. on Empir. 2023 Conf. on Empir2023</p>
<p>Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers. Catherine A Gao, Frederick M Howard, Nikolay S Markov, Emma C Dyer, Siddhi Ramesh, Yuan Luo, Alexander T Pearson, NPJ Digit. Med. 6752023. 2023</p>
<p>SPACE-IDEAS: A Dataset for Salient Information Detection in Space Innovation. Andres Garcia-Silva, Cristian Berrio, Jose Manuel Gomez-Perez, LREC-COLING 2024. ELRA and ICCL2024. 2024Proc.</p>
<p>Citation Indexes for Science. Eugene Garfield, Science. 1221955. 1955</p>
<p>Peer review analyze: A novel benchmark resource for computational analysis of peer reviews. Tirthankar Ghosal, Sandeep Kumar, Prabhat Kumar Bharti, Asif Ekbal, PLOS One. 1712022. 01 2022</p>
<p>HedgePeer: A dataset for uncertainty detection in peer reviews. Tirthankar Ghosal, Kaushik Kamal, Valia Varanasi, Kordoni, Proc. 22nd ACM/IEEE Jt. Conf. on Digit. Libr. 22nd ACM/IEEE Jt. Conf. on Digit. Libr2022</p>
<p>DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions. Tirthankar Ghosal, Rajeev Verma, Asif Ekbal, Pushpak Bhattacharyya, Proc. 57th Annu. 57th Annu2019</p>
<p>LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis. Hamed Babaei Giglou, D' Jennifer, Sören Souza, Auer, ACM/IEEE Jt. Conf. on Digit. Libr. (JCDL). 2024. 2024</p>
<p>Max Glockner, Yufang Hou, Preslav Nakov, Iryna Gurevych, arXiv:2408.12812Grounding Fallacies Misrepresenting Scientific Publications in Evidence. 2024</p>
<p>Missci: Reconstructing Fallacies in Misrepresented Science. Max Glockner, Yufang Hou, Preslav Nakov, Iryna Gurevych, Proc. ACL. ACL. ACL. ACL2024</p>
<p>Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature. Tomas Goldsack, Zhihao Zhang, Chenghua Lin, Carolina Scarton, Proc. 2022 Conf. on Empir. 2022 Conf. on Empir2022</p>
<p>ChatGPT "contamination": estimating the prevalence of LLMs in the scholarly literature. Andrew Gray, arXiv:2403.16887[cs.DL]2024</p>
<p>Automatic Analysis of Substantiation in Scientific Peer Reviews. Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, and Chloé Clavel. 2023. 2023</p>
<p>Mapping the ethics of generative ai: A comprehensive scoping review. Thilo Hagendorff, arXiv:2402.083232024. 2024arXiv preprint</p>
<p>Academic Writing and Publishing: A Practical Handbook. James Hartley, 2008Routledge</p>
<p>The ability of ChatGPT in paraphrasing texts and reducing plagiarism: a descriptive analysis. Soheil Hassanipour, Sandeep Nayak, Ali Bozorgi, Mohammad-Hossein Keivanlou, Tirth Dave, Abdulhadi Alotaibi, Farahnaz Joukar, Parinaz Mellatdoust, Arash Bakhshi, Dona Kuriyakose, JMIR Med. Educ. 10e533082024. 2024</p>
<p>AI for Scientific Discovery. Janna Hastings, 2023CRC PRess</p>
<p>AutoML: A survey of the state-of-the-art. Xin He, Kaiyong Zhao, Xiaowen Chu, Knowledge-based systems. 2121066222021. 2021</p>
<p>CLIPScore: A Reference-free Evaluation Metric for Image Captioning. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi, Proc. 2021 Conf. on Empir. 2021 Conf. on Empir2021</p>
<p>Jim Gray on eScience: a Transformed Scientific Method. Tony Hey, Stewart Tansley, Kristin Tolle, The Fourth Paradigm: Data-Intensive Scientific Discovery. Tony Hey, Stewart Tansley, Kristin Tolle, Microsoft Research2009</p>
<p>Towards Automated Related Work Summarization. Cong Duy, Vu Hoang, Min-Yen Kan, Coling 2010 Organizing Committee. Chu-Ren Posters, Dan Huang, Jurafsky, Beijing, China2010. 2010</p>
<p>CONSORT for reporting randomized controlled trials in journal and conference abstracts: explanation and elaboration. Sally Hopewell, Mike Clarke, David Moher, Elizabeth Wager, Philippa Middleton, Kenneth F Douglas G Altman, Consort Schulz, Group, PLoS medicine. 5e202008. 2008</p>
<p>Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, Anna Korhonen, David Traum, and Lluís Màrquez2019Florence, Italy</p>
<p>Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.14255Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. 2024. 2024arXiv preprint</p>
<p>PPSGen: Learning to Generate Presentation Slides for Academic Papers. Yue Hu, Xiaojun Wan, In Int. Jt. Conf. on Artif. Intell. 2013</p>
<p>Automatic generation of related work sections in scientific papers: an optimization approach. Yue Hu, Xiaojun Wan, Proc. 2014 Conf. on Empir. Methods Nat. Lang. Process. (EMNLP). 2014 Conf. on Empir. Methods Nat. Lang. ess. (EMNLP)2014</p>
<p>Argument Mining for Understanding Peer Reviews. Xinyu Hua, Mitko Nikolov, Nikhil Badugu, Lu Wang, Proc. 2019 Conf. North Am. Long Short Pap. 2019 Conf. North Am20191</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2307.02185Citation: A key to building responsible and accountable large language models. 2023. 2023arXiv preprint</p>
<p>The role of ChatGPT in scientific communication: writing better scientific review articles. Jingshan Huang, Ming Tan, Am. journal cancer research. 1311482023. 2023</p>
<p>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first Int. Conf. on Mach. Learn. 2024</p>
<p>Can ChatGPT assist authors with abstract writing in medical journals? Evaluating the quality of scientific abstracts generated by ChatGPT and original abstracts. Taesoon Hwang, Nishant Aggarwal, Pir Zarak Khan, Thomas Roberts, Amir Mahmood, Madlen M Griffiths, Nick Parsons, Saboor Khan, Plos one. 19e02977012024. 2024</p>
<p>Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin, On the Rigour of Scientific Writing: Criteria, Analysis, and Insights. In Find. Assoc. Comput. Linguist. EMNLP 2024. ACL. 2024</p>
<p>TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs. Peiwen Jiang, Xinbo Lin, Zibo Zhao, Ruhui Ma, Yvonne Jie Chen, Jinhua Cheng, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>SWE-bench: Can Language Models Resolve Real-world Github Issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, The Twelfth Int. Conf. on Learn. 2024Represent.</p>
<p>FigureQA: An Annotated Figure Dataset for Visual Reasoning. Samira Ebrahimi Kahou, Adam Atkinson, Vincent Michalski, Ákos Kádár, Adam Trischler, Yoshua Bengio, 2018</p>
<p>A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz, Proc. 2018 Conf. North Am. Long Pap. 2018 Conf. North Am20181</p>
<p>Taxonomy-guided Semantic Indexing for Academic Paper Search. Seongku Kang, Yunyi Zhang, Pengcheng Jiang, Dongha Lee, Jiawei Han, Hwanjo Yu, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation. Wei-Yu Kao, An-Zi Yen, Companion Proc. ACM on Web Conf. 2024. 2024</p>
<p>AxCell: Automatic Extraction of Results from Machine Learning Papers. Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, Robert Stojnic, ACL, Online. Trevor Cohn, Yulan He, Yang Liu, Bonnie Webber2020</p>
<p>A Diagram is Worth a Dozen Images. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Comput. Vis2016. 2016Springer International Publishing</p>
<p>DISAPERE: A Dataset for Discourse Structure in Peer Review Discussions. Nayak Neha, Tim O Kennard, Rajarshi 'gorman, Akshay Das, Chhandak Sharma, Matthew Bagchi, Pranay Clinton, Hamed Kumar Yelugam, Andrew Zamani, Mccallum, Proc. NAACL. ACL. NAACL. ACL2022</p>
<p>Using ChatGPT for language editing in scientific articles. Maxillofac. plastic reconstructive surgery. Seong-Gon Kim, 2023. 20234513</p>
<p>Understanding the Role and Methods of Meta-Analysis in IS Research. William R King, Commun. Assoc. for Inf. Syst. 16Jun He. 2005. 2005</p>
<p>Experimental Design. Roger E Kirk, The SAGE Handbook of Quantitative Methods in Psychology. Roger E Millsap, Alberto Maydeu-Olivares, 2009</p>
<p>CORE: a Global aggregation Service for Open access. Petr Knoth, Drahomira Herrmannova, Matteo Cancellieri, Lucas Anastasiou, Nancy Pontika, Samuel Pearce, Bikash Gyawali, David Pride, Papers. Sci. Data. 103662023. 2023</p>
<p>Delving into ChatGPT usage in academic writing through excess vocabulary. Dmitry Kobak, Rita González-Márquez, Emőke-Ágnes Horvát, arXiv:2406.07016Jan Lause. 2024. 2024arXiv preprint</p>
<p>Jing Yu Koh, Stephen Mcaleer, Daniel Fried, Ruslan Salakhutdinov, arXiv:2407.01476Tree Search for Language Model Agents. 2024. 2024arXiv preprint</p>
<p>Integrating Table Representations into Large Language Models for Improved Scholarly Document Comprehension. Sibel Buse, Antonio Korkmaz, Rio Del, Chanona, Proc. Fourth Workshop on Sch. Document Process. Fourth Workshop on Sch. Document ess2024. 2024</p>
<p>Artificial intelligence to support publishing and peer review: A summary and review. Kayvan Kousha, Mike Thelwall, Learn. Publ. 372024. 2024</p>
<p>A Deep Neural Architecture for Decision-Aware Meta-Review Generation. Asheesh Kumar, Tirthankar Ghosal, Asif Ekbal, ACM/IEEE Jt. Conf. on Digit. Libr. (JCDL. 2021. 2021</p>
<p>Andrea Madarasova Geckova, Sarah Mantwill, and Olaf von Dem Knesebeck. 2022. «I Do Not Have Time» -Is This the End of Peer Review in Public Health Sciences?. Nino Künzli, Anke Berger, Katarzyna Czabanowska, Raquel Lucas, Public Health Rev. 432022</p>
<p>Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, arXiv:2405.06563What Can Natural Language Processing Do for Peer Review?. 2024. 2024arXiv preprint</p>
<p>Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review. Ilia Kuznetsov, Jan Buchmann, Max Eichler, Iryna Gurevych, Comput. Linguist. 4842022. 12 2022</p>
<p>The Computational Support of Scientific Discovery. Pat Langley, Int. J. Human-Computer Stud. 532000. 2000</p>
<p>Viziometrics: Analyzing Visual Information in the Scientific Literature. Po-Shen Lee, Jevin D West, Bill Howe, IEEE Trans. on Big Data. 42016. 2016</p>
<p>What are the most influential current AI Papers?. Christoph Leiter, Jonas Belouadi, Yanran Chen, Ran Zhang, Daniil Larionov, Aida Kostikova, Steffen Eger, ArXiv abs/2412.12121NLLG Quarterly. 2024. 2024</p>
<p>The Advantage of Short Paper Titles. Adrian Letchford, Helen Susannah Moat, Tobias Preis, Royal Soc. Open Sci. 22015. 2015</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Proc. NeurIPS. NeurIPS202033</p>
<p>Towards Topic-Aware Slide Generation For Academic Papers With Unsupervised Mutual Learning. Da-Wei Li, Danqing Huang, Tingting Ma, Chin-Yew Lin, AAAI Conf. on Artif. Intell. 2021</p>
<p>Halueval: A large-scale hallucination evaluation benchmark for large language models. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, arXiv:2305.117472023. 2023arXiv preprint</p>
<p>Multi-task Peer-Review Score Prediction. Jiyi Li, Ayaka Sato, Kazuya Shimura, Fumiyo Fukumoto, Proc. First Workshop on Sch. Document Process.. ACL. First Workshop on Sch. Document ess.. ACL2020</p>
<p>Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, Qi Liu, Proc. 62nd Annu. 62nd Annu20241</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.13185Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents. 2024. 2024arXiv preprint</p>
<p>Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation. Miao Li, Eduard Hovy, Jey Lau, EMNLP 2023. ACL. 2023</p>
<p>Unifying Structured Data as Graph for Data-to-Text Pre-Training. Shujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu Yuan, Wanwei He, Can Shao Yuan, Fei Ma, Yongbin Huang, Li, 10.1162/tacl_a_00641/2346090/tacl_a_00641.pdfTrans. Assoc. for Comput. Linguist. 122024. 03 2024</p>
<p>Citation-Enhanced Generation for LLM-based Chatbot. Weitao Li, Junkai Li, Weizhi Ma, Yang Liu, arXiv:2402.160632024. 2024arXiv preprint</p>
<p>Cited Text Spans for Scientific Citation Text Generation. Xiangci Li, Yi-Hui Lee, Jessica Ouyang, Proc. Fourth Workshop on Sch. Document Process. Fourth Workshop on Sch. Document ess2024. 2024</p>
<p>CORWA: A Citation-Oriented Related Work Annotation Dataset. Xiangci Li, Biswadip Mandal, Jessica Ouyang, Proc. 2022 Conf. North Am. Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, 2022 Conf. North AmMarine Carpuat; Seattle, United States2022ACL</p>
<p>Related Work and Citation Text Generation: A Survey. Xiangci Li, Jessica Ouyang, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen, Proc. 31st Int. Conf. on Comput. Linguist.. ACL. 31st Int. Conf. on Comput. Linguist.. ACL2025</p>
<p>Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, arXiv:2308.03281Towards general text embeddings with multi-stage contrastive learning. 2023. 2023arXiv preprint</p>
<p>Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, arXiv:2310.01783Can large language models provide useful feedback on research papers? A large-scale empirical analysis. 2023. 2023arXiv preprint</p>
<p>Mapping the Increasing Use of LLMs in Scientific Papers. Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y Zou, Proc. COLM. COLM2024</p>
<p>Automated scholarly paper review: Concepts, technologies, and challenges. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, Inf. Fusion. 981018302023. 2023</p>
<p>MOPRD: A multidisciplinary open peer review dataset. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, Neural Comput. Appl. 352023. Sept. 2023</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2109.07958Truthfulqa: Measuring how models mimic human falsehoods. 2021. 2021arXiv preprint</p>
<p>Advisor: Automatic visualization answer for natural-language question on tabular data. Can Liu, Yun Han, Ruike Jiang, Xiaoru Yuan, 2021 IEEE 14th Pac. Vis. Symp. (PacificVis). IEEE2021</p>
<p>Structure-aware Table-to-Text Generation with Prefix-tuning. Cencen Liu, Yi Xu, Wen Yin, Dezhang Zheng, Proc. 2023 4th Int. Conf. on Control. 2023 4th Int. Conf. on ControlAssociation for Computing Machinery2023</p>
<p>Literature meets data: A synergistic approach to hypothesis generation. Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan, arXiv:2410.173092024. 2024arXiv preprint</p>
<p>Beyond designer's knowledge: Generating materials design hypotheses via large language models. Quanliang Liu, P Maciej, So Yeon Polak, Kim, Hrishikesh Shuvo, Jeongsoo Shridhar Deodhar, Dane Han, Hyunseok Morgan, Oh, arXiv:2409.067562024. 2024arXiv preprint</p>
<p>Ryan Liu, Nihar B Shah, arXiv:2306.00622ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. 2023. 2023</p>
<p>DrugAgent: Automating AI-aided Drug Discovery Programming through LLM Multi-Agent Collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Tianfan Fu, Yue Zhao, arXiv:2411.156922024. 2024arXiv preprint</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024. 2024arXiv preprint</p>
<p>nvBench: A Large-Scale Synthesized Dataset for Cross-Domain Natural Language to Visualization Task. Yuyu Luo, Jiawei Tang, Guoliang Li, ArXiv abs/2112.129262021. 2021</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306[cs.CL]LLM4SR: A Survey on Large Language Models for Scientific Research. 2025</p>
<p>AI agents in chemical research: GVIM -an intelligent research assistant system. Kangyong Ma, Digit. Discov. 2025. jan 2025</p>
<p>Can ChatGPT draft a research article? An example of population-level vaccine effectiveness analysis. Calum Macdonald, Davies Adeloye, Aziz Sheikh, Igor Rudan, J. global health. 132023. 2023</p>
<p>Automating the design of graphical presentations of relational information. Jock Mackinlay, Acm Trans. On Graph. (Tog). 21986. 1986</p>
<p>Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models. Paula Maddigan, Teo Susnjak, IEEE Access. 112023. 2023</p>
<p>Presentations are not always linear! GNN meets LLM for Text Document-to-Presentation Transformation with Attribution. Himanshu Maheshwari, Sambaran Bandyopadhyay, Aparna Garimella, Anandhavelu Natarajan, In Find. Assoc. for Comput. Linguist. EMNLP. 2024. 2024</p>
<p>ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, Enamul Hoque, In Find. Assoc. for Comput. Linguist. ACL. 2022. 2022</p>
<p>Automatic title generation for text with pre-trained transformer language model. Prakhar Mishra, Chaitali Diwan, Srinath Srinivasa, Gopalakrishnan Srinivasaraghavan, IEEE 15th Int. Conf. on Semantic Comput. (ICSC). 2021. 2021IEEE</p>
<p>SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement. Ishani Mondal, Zongxia Li, Yufang Hou, Anandhavelu Natarajan, Aparna Garimella, Jordan Lee Boyd-Graber, In Find. Assoc. for Comput. Linguist. EMNLP. 2024. 2024</p>
<p>Presentations by the Humans and For the Humans: Harnessing LLMs for Generating Persona-Aware Slides from Documents. Ishani Mondal, S Shwetha, Anandhavelu Natarajan, Aparna Garimella, Sambaran Bandyopadhyay, Jordan Boyd-Graber, Proc. EACL. ACL. EACL. ACL2024</p>
<p>SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables. Nafise Moosavi, Andreas Rücklé, Dan Roth, Iryna Gurevych, Proc. Neural Inf. Process. Syst. Track on Datasets Benchmarks. Neural Inf. ess. Syst. Track on Datasets Benchmarks20211</p>
<p>MTEB: Massive Text Embedding Benchmark. Niklas Muennighoff, Nouamane Tazi, Proc. 17th Conf. 17th Conf2023Loic Magne, and Nils Reimers</p>
<p>NL4DV: A toolkit for generating analytic specifications for data visualization from natural language queries. Arpit Narechania, Arjun Srinivasan, John Stasko, IEEE Trans. on Vis. Comput. Graph. 272020. 2020</p>
<p>Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang, Wang , Roberta Raileanu, arXiv:2502.14499[cs.CL]MLGym: A New Framework and Benchmark for Advancing AI Research Agents. 2025</p>
<p>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Joseph Chee Daniel S Weld, Kyle Chang, Lo, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proc. 40th annual meeting Assoc. for Comput. Linguist. 40th annual meeting Assoc. for Comput. Linguist2002</p>
<p>ToTTo: A Controlled Table-To-Text Generation Dataset. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, Proc. 2020 Conf. on Empir. Methods Nat. Lang. Process. (EMNLP). ACL. 2020 Conf. on Empir. Methods Nat. Lang. ess. (EMNLP). ACL2020</p>
<p>Can ChatGPT be used to generate scientific hypotheses?. Yang Jeong, Park , Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, Ju Li, J. Materiomics. 102024. 2024</p>
<p>Synthesizing Information Systems Knowledge: a Typology of Literature Reviews. Guy Paré, Marie-Claude Trudel, Mirou Jaana, Spyros Kitsiou, Inf. &amp; Manag. 522015. 2015</p>
<p>Arithmetic-Based Pretraining Improving Numeracy of Pretrained Language Models. Dominic Petrak, Nafise Sadat Moosavi, Iryna Gurevych, Proc. 12th Jt. Conf. on Lexical Comput. Semant. 12th Jt. Conf. on Lexical Comput. Semant2023SEM 2023</p>
<p>Online software spots genetic errors in cancer papers. Nicky Phillips, Nature. 55176812017. 2017</p>
<p>CiteTracked: A longitudinal dataset of peer reviews and citations. Barbara Plank, Reinard Van Dalen, CEUR Workshop Proceedings. 2019. 2019Proc. 4th Jt</p>
<p>Aniket Pramanick, Yufang Hou, M Saif, Iryna Mohammad, Gurevych, arXiv:2409.19505[cs.CL]The Nature of NLP: Analyzing Contributions in NLP Papers. 2024</p>
<p>Aniket Pramanick, Yufang Hou, M Saif, Iryna Mohammad, Gurevych, arXiv:2409.19508[cs.CL]Transforming Scholarly Landscapes: Influence of Large Language Models on Academic Fields beyond Computer Science. 2024</p>
<p>SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers. Shraman Pramanick, Rama Chellappa, Subhashini Venugopalan, The Thirty-eight Conf. on Neural Inf. Process. Syst. Datasets Benchmarks Track. 2024</p>
<p>Kevin Pu, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, arXiv:2410.04025IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback. 2024. 2024arXiv preprint</p>
<p>ChatGPT and generative AI are revolutionizing the scientific community: A Janus-faced conundrum. Zhongji Pu, Chun-Lin Shi, Che Ok Jeon, Jingyuan Fu, Shuang-Jiang Liu, Canhui Lan, Yanlai Yao, Yong-Xin Liu, Baolei Jia, iMeta. 3e1782024. 2024</p>
<p>Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals. Sukannya Purkayastha, Anne Lauscher, Iryna Gurevych, Proc. 2023 Conf. on Empir. 2023 Conf. on Empir2023</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023. 2023arXiv preprint</p>
<p>Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, arXiv:2407.08940Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation. 2024. 2024arXiv preprint</p>
<p>Learning to Generate Posters of Scientific Papers. Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou, Leonid Sigal, AAAI Conf. on Artif. Intell. 2016</p>
<p>Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.14634Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination. 2024. 2024arXiv preprint</p>
<p>ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. Raian Rahman, Rizvi Hasan, Abdullah Al Farhad, Md Tahmid Rahman Laskar, Md Hamjajul Ashmafee, Abu Raihan, Mostofa Kamal, ArXiv abs/2304.136202023. 2023</p>
<p>Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text. Pritika Ramu, Aparna Garimella, Sambaran Bandyopadhyay, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>K Chandan, Parshin Reddy, Shojaee, arXiv:2412.11427[cs.LG]Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges. 2024</p>
<p>Committees with implicit biases promote fewer women when they do not believe gender bias exists. Isabelle Régner, Catherine Thinus-Blanc, Agnès Netter, Toni Schmader, Pascal Huguet, Nat. Hum. Behav. 32019. 2019</p>
<p>Georg Rehm, Stefan Dietze, Sonja Schimmler, Frank Krüger, Natural Scientific Language Processing and Research Knowledge Graphs: First International Workshop. Hersonissos, Crete, GreeceSpringer2024. May 27, 2024. 147702024</p>
<p>Zachary Robertson, arXiv:2307.05492GPT-4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. 2023. 2023arXiv preprint</p>
<p>Interactive graphic design using automatic presentation knowledge. John Steven F Roth, Joe Kolojejchick, Jade Mattis, Goldstein, Proc. SIGCHI conference on Hum. factors computing systems. SIGCHI conference on Hum. factors computing systems1994</p>
<p>Automating the presentation of information. F Steven, Joe Roth, Mattis, Proc. IEEE Conf. IEEE Conf1991</p>
<p>Systematic Task Exploration with LLMs: A Study in Citation Text Generation. Furkan Şahinuç, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych, Proc. 62nd Annu. 62nd Annu20241</p>
<p>Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards. Furkan Şahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>Can artificial intelligence help for scientific writing?. Michele Salvagno, Fabio Silvio Taccone, Alberto Giovanni Gerli, Crit. care. 27752023. 2023</p>
<p>A simple neural network module for relational reasoning. Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap, Adv. Neural Inf. Process. Syst. 302017Curran Associates, Inc</p>
<p>Shubhra Kanti, Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Matthew Freestone, Matthew C WilliamsJr, arXiv:2402.15589arXiv:2402.15589 [cs.CL]Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts. 2024. 2024arXiv preprint</p>
<p>Laurie A Schintler, Connie L Mcneely, James Witte, arXiv:2309.12356A Critical Examination of the Ethics of AI-Mediated Peer Review. 2023. 2023arXiv preprint</p>
<p>ChatGPT et al. ': The ethics of using (generative) artificial intelligence in research and science. Daniel Schlagwein, Leslie Willcocks, J. Inf. Technol. 382023. 2023</p>
<p>The intended uses of automated fact-checking artefacts: Why, how and who. Michael Schlichtkrull, Nedjma Ousidhoum, Andreas Vlachos, arXiv:2304.142382023. 2023arXiv preprint</p>
<p>. Dominik Schmidt, Zhengyao Jiang, Yuxiang Wu, 2024</p>
<p>Sciscore, The best methods review tool for scientific research. 2024. 12 June 2024</p>
<p>MReD: A Meta-Review Dataset for Structure-Controllable Text Generation. Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, Luo Si, Find. Assoc. for Comput. Linguist. ACL 2022. ACL. 2022</p>
<p>Chufan Shi, Cheng Yang, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, Yujiu Yang, arXiv:2406.09961[cs.SE]ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation. 2024</p>
<p>Haoxiang Shi, Jiaan Wang, Jiarong Xu, Cen Wang, Tetsuya Sakai, arXiv:2405.12174CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models. 2024. 2024arXiv preprint</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. 2024</p>
<p>Shruti Singh, Mayank Singh, Pawan Goyal, arXiv:2108.04366[cs.CL]COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer Reviews. 2021</p>
<p>What Do We Really Mean by Rigor in Information Systems Research?. Wael Soliman, Mikko Siponen, 2022</p>
<p>RGVisNet: A Hybrid Retrieval-Generation Neural Framework Towards Automatic Data Visualization Generation. Yuanfeng Song, Xuefang Zhao, Raymond , Chi-Wing Wong, Di Jiang, Proc. 28th ACM SIGKDD Conf. on Knowl. Discov. Data Min. 28th ACM SIGKDD Conf. on Knowl. Discov. Data MinAssociation for Computing Machinery2022</p>
<p>A statistical interpretation of term specificity and its application in retrieval. Karen Sparck, Jones , J. documentation. 281972. 1972</p>
<p>SlidesGen: Automatic Generation of Presentation Slides for a Technical Paper Using Summarization. M Sravanthi, C Ravindranath Chowdary, P Sreenivasa Kumar, The Fla. AI Res. Soc2009</p>
<p>Uncertainty-aware machine support for paper reviewing on the interspeech 2019 submission corpus. Lukas Stappen, Georgios Rizos, Madina Hasan, Thomas Hain, Björn W Schuller, 2020. 2020</p>
<p>An Analysis of Tasks and Datasets in Peer Reviewing. Moritz Staudinger, Wojciech Kusa, Florina Piroi, Allan Hanbury, Proc. Fourth Workshop on Sch. Document Process. Fourth Workshop on Sch. Document ess2024. 2024</p>
<p>Racism and censorship in the editorial and peer review process. Dana Strauss, Sophia Gran-Ruaz, Muna Osman, Monnica T Williams, Sonya C Faber, Front. Psychol. 142023. 2023</p>
<p>Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.09403Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation. 2024. 2024arXiv preprint</p>
<p>Towards Table-to-Text Generation with Numerical Reasoning. Lya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura, Hiroya Takamura, Proc. 59th Annu. 59th Annu20211</p>
<p>D2S: Document-to-Slide Generation Via Query-Based Text Summarization. Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, Nancy X R Wang, Proc. NAACL. ACL. NAACL. ACL2021</p>
<p>Assessing scientific research papers with knowledge graphs. Kexuan Sun, Zhiqiang Qiu, Abel Salinas, Yuzhong Huang, Dong-Ho Lee, Daniel Benjamin, Fred Morstatter, Xiang Ren, Kristina Lerman, Jay Pujara, Proc. 45th Int. 45th Int2022</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, arXiv:2401.05561Trustllm: Trustworthiness in large language models. 2024. 2024</p>
<p>Articulate: Creating meaningful visualizations from natural language. Yiwen Sun, Jason Leigh, Andrew Johnson, Barbara Di, Eugenio , Innovative Approaches of Data Visualization and Visual Analytics. Lin Mao, Weidong Huang, Huang, IGI Global2014</p>
<p>Anirudh Sundar, Christopher Richardson, Larry Heck, arXiv:2403.14457Generating Tables from Text by Conditional Question Answering. 2024. 2024arXiv preprint</p>
<p>Jaroslaw Szumega, Lamine Bougueroua, Blerina Gkotse, Pierre Jouvelot, Federico Ravotti, arXiv:2312.04576[cs.DL]The Open Review-Based (ORB) dataset: Towards Automatic Assessment of Scientific Papers and Experiment Proposals in High-Energy Physics. 2023</p>
<p>Multi2Claim: Generating Scientific Claims from Multi-Choice Questions for Scientific Fact-Checking. Neset Tan, Trung Nguyen, Josh Bensemann, Alex Peng, Qiming Bao, Yang Chen, Mark Gahegan, Michael Witbrock, Proc. 17th Conf. 17th Conf2023</p>
<p>Sevi: Speech-to-Visualization through Neural Machine Translation. Jiawei Tang, Yuyu Luo, Mourad Ouzzani, Guoliang Li, Hongyang Chen, Proc. 2022 Int. Conf. on Manag. Data. 2022 Int. Conf. on Manag. DataAssociation for Computing Machinery2022</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023. 2023arXiv preprint</p>
<p>Patara Trirat, Wonyong Jeong, Sung Ju Hwang, arXiv:2410.02958AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML. 2024. 2024arXiv preprint</p>
<p>Yun-Da Tsai, Yu-Che Tsai, Bo-Wei Huang, Chun-Pai Yang, Shou-De Lin, arXiv:2309.01125Automl-gpt: Large language model for automl. 2023. 2023arXiv preprint</p>
<p>Scientific fact-checking: A survey of resources and approaches. Juraj Vladika, Florian Matthes, arXiv:2305.168592023. 2023arXiv preprint</p>
<p>Plots Made Quickly: An Efficient Approach for Generating Visualizations from Natural Language Queries. Henrik Voigt, Kai Lawonn, Sina Zarrieß, Proc. nullLREC-COLING2024. 2024. 2024</p>
<p>Affiliation Bias in Peer Review of Abstracts by a Large Language Model. Rico A Dario Von Wedel, Moritz Schmitt, Raphael Thiele, Denys Leuner, Simone Shay, Maximilian S Redaelli, Schaefer, JAMA. 33132024. 01 2024</p>
<p>Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, arXiv:2310.03214Freshllms: Refreshing large language models with search engine augmentation. 2023. 2023arXiv preprint</p>
<p>David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi, arXiv:2210.13777SciFact-open: Towards open-domain scientific claim verification. 2022. 2022arXiv preprint</p>
<p>Fabrication and errors in the bibliographic citations generated by ChatGPT. H William, Esther Isabelle Walters, Wilder, Sci. Reports. 13140452023. 2023</p>
<p>Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning. Fei Wang, Zhewei Xu, Pedro Szekely, Muhao Chen, Proc. 2022 Conf. North Am. 2022 Conf. North Am2022</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 6202023. 2023</p>
<p>Relational collaborative topic regression for recommender systems. Hao Wang, Wu-Jun Li, IEEE Trans. on Knowl. Data Eng. 272014. 2014</p>
<p>ToC-RWG: Explore the Combination of Topic Model and Citation Information for Automatic Related Work Generation. Pancheng Wang, Shasha Li, Haifang Zhou, Jintao Tang, Ting Wang, IEEE Access. 82019. 2019</p>
<p>PaperRobot: Incremental Draft Generation of Scientific Ideas. Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan, Proc. 57th Annu. 57th Annu2019</p>
<p>ReviewRobot: Explainable Paper Review Generation based on Knowledge Synthesis. Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Ji Heng, Nazneen Fatema, Rajani , Proc. 13th Int. Conf. on Nat. 13th Int. Conf. on Nat2020</p>
<p>Phrase-Based Presentation Slides Generation for Academic Papers. Sida Wang, Xiaojun Wan, Shikang Du, AAAI Conf. on Artif. Intell. 2017</p>
<p>Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, arXiv:2407.16741OpenHands: An Open Platform for AI Software Developers as Generalist Agents. 2024. 2024arXiv preprint</p>
<p>Neural Related Work Summarization with a Joint Context-driven Attention Mechanism. Yongzhen Wang, Xiaozhong Liu, Zheng Gao, Proc. 2018 Conf. on Empir. David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, 2018 Conf. on EmpirEllen Riloff; Brussels, Belgium2018ACL</p>
<p>CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen, The Thirty-eight Conf. on Neural Inf. Process. Syst. Datasets Benchmarks Track. 2024</p>
<p>Editorial Peer Review: Its Strengths and Weaknesses. C Ann, Weller, 2001American Society for Information Science and Technology</p>
<p>Siwei Wu, Yizhi Li, Xingwei Qu, Rishi Ravikumar, Yucheng Li, Tyler Loakman, Shanghaoran Quan, Xiaoyong Wei, Riza Batista-Navarro, Chenghua Lin, arXiv:2502.19103[cs.CL]LongEval: A Comprehensive Analysis of Long-Text Generation Through a Plan-based Paradigm. 2025</p>
<p>SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, Chenghua Lin, Find. Assoc. for Comput. Linguist. ACL 2024. ACL. 2024</p>
<p>Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024. 2024arXiv preprint</p>
<p>ChartAdapter: Large Vision-Language Model for Chart Summarization. Peixin Xu, Yujuan Ding, Wenqi Fan, arXiv:2412.20715[cs.MM]2024</p>
<p>AI for social science and social science of AI: A survey. Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, Xianpei Han, Inf. Process. &amp; Manag. 611036652024. 2024</p>
<p>PosterBot: A System for Generating Posters of Scientific Papers with Neural Models. Sheng Xu, Xiaojun Wan, AAAI Conf. on Artif. Intell. 2022</p>
<p>Table-to-Text Using Pre-trained Large Language Model and LoRA. Hidekazu Yanagimoto, Iroha Kisaku, Kiyota Hashimoto, 16th IIAI Int. Congr. on Adv. Appl. Informatics (IIAI-AAI. 2024. 2024</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023. 2023arXiv preprint</p>
<p>Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.07076MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses. 2024. 2024arXiv preprint</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, Dragomir R Radev, Proc. AAAI conference on artificial intelligence. AAAI conference on artificial intelligence201933</p>
<p>Samplingbias-corrected neural modeling for large corpus item recommendations. Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, Ed Chi, Proc. 13th ACM Conf. on Recomm. Syst. 13th ACM Conf. on Recomm. SystAssociation for Computing Machinery2019</p>
<p>Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing Huang, arXiv:2305.18153Do Large Language Models Know What They Don't Know?. 2023. 2023arXiv preprint</p>
<p>Scientists' Views of Science, Models of Writing, and Science Writing Practices. Larry D Yore, Brian M Hand, Marilyn K Florence, J. Res. Sci. Teach. 412004. 2004</p>
<p>A dual augmented two-tower model for online large-scale recommendation. Yantao Yu, Weipeng Wang, Zhoutian Feng, Daiyue Xue, DLP-KDD. 2021. 2021</p>
<p>Can We Automate Scientific Reviewing?. Weizhe Yuan, Pengfei Liu, Graham Neubig, J. Artif. Int. Res. 75pages2022. dec 2022</p>
<p>DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal, COLM. 2024</p>
<p>Qi Zeng, Mankeerat Sidhu, Pong Hou, Lu Chan, Heng Wang, Ji, arXiv:2305.14647Meta-review Generation with Checklist-guided Iterative Introspection. 2023. 2023arXiv preprint</p>
<p>Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin Niu, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li, arXiv:2410.21252[cs.CL]LongReward: Improving Long-context Large Language Models with AI Feedback. 2024</p>
<p>ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?. Leixin Zhang, Steffen Eger, Yinjie Cheng, Weihe Zhai, Jonas Belouadi, Christoph Leiter, Paolo Simone, Fahimeh Ponzetto, Zhixue Moafian, Zhao, arXiv:2412.02368[cs.AI]2024</p>
<p>Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, Yuqing Yang, arXiv:2304.149792023. 2023arXiv preprint</p>
<p>Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yu-Ching Lin, Zhao Xu, Keqiang Yan, ArXiv abs/2307.08423Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems. Keir Adams, Maurice Weiler, Xiner Li2023. 2023</p>
<p>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, Proc. 2024 Conf. on Empir. 2024 Conf. on Empir2024</p>
<p>SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation. Xueliang Zhao, Tingchen Fu, Lemao Liu, Lingpeng Kong, Shuming Shi, Rui Yan, Find. Assoc. for Comput. Linguist. ACL 2023. ACL. 2023</p>
<p>LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control. Yilun Zhao, Zhenting Qi, Linyong Nan, Lorenzo Jaime Flores, Dragomir Radev, Proc. 17th Conf. 17th Conf2023</p>
<p>Felm: Benchmarking factuality evaluation of large language models. Yiran Zhao, Jinghan Zhang, Siyang Chern, Pengfei Gao, Junxian Liu, He, Adv. Neural Inf. Process. Syst. 362024. 2024</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis Generation with Large Language Models. 2024. 2024arXiv preprint</p>
<p>Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, arXiv:2410.10934Vikas Chandra, and Jürgen Schmidhuber. 2024. Agent-as-a-Judge: Evaluate Agents with Agents. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5034 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5034</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5034</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-b784b2023593af98692b1d2063b53eca897512cd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b784b2023593af98692b1d2063b53eca897512cd" target="_blank">SAT: 2D Semantics Assisted Training for 3D Visual Grounding</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> 2D Semantics Ass Training (SAT) is proposed that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding.</p>
                <p><strong>Paper Abstract:</strong> 3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5034",
    "paper_id": "paper-b784b2023593af98692b1d2063b53eca897512cd",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0061505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SAT: 2D Semantics Assisted Training for 3D Visual Grounding</h1>
<p>Zhengyuan Yang ${ }^{1}$ Songyang Zhang ${ }^{1}$ Liwei Wang ${ }^{2}$ Jiebo Luo ${ }^{1}$<br>${ }^{1}$ University of Rochester ${ }^{2}$ The Chinese University of Hong Kong<br>{zyang39, szhang83, jluo}@cs.rochester.edu lwwang@cse.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from $37.7 \%$ to $49.2 \%$, which significantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., $+10.4 \%$ absolute accuracy on Nr3D, $+9.9 \%$ on Sr3D, and $+5.6 \%$ on ScanRef. Code is available at https://github.com/zyang-ur/SAT.</p>
<h2>1. Introduction</h2>
<p>Visual grounding provides machines the ability to ground a language description to the targeted visual region. The task has received wide attention in both datasets [55, 32, 20] and methods [16, 47, 54, 51]. However, most previous visual grounding studies remain on images [55, 32, 20] and videos [58, 39, 52], which contain 2D projections of inherently 3D visual scenes. The recently proposed 3D visual grounding task [1, 4] aims to ground a natural language description about a 3D scene to the region referred to by a language query (in the form of a 3D bounding box). The 3D visual grounding task has various applications, including autonomous agents [41, 48], human-machine interac-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. 3D visual grounding aims to ground a language query to a targeted 3D object region, as shown by the green 3D bounding box. (a) Previous 3D visual grounding studies are trained with a sole 3D grounding loss that maximizes the similarity between positive object-query pairs. However, the sole objective is less effective as point clouds are sparse and noisy. (b) 2D semantics contain rich and clean object representations and can be used as extra visual inputs to assist 3D grounding. However, requiring extra 2D inputs in inference limits potential application scenarios. (c) Our proposed 2D Semantics Assisted Training (SAT) uses 2D semantics only in training and does not require extra inputs in inference. The green and red boxes are the targeted and distracting objects.
tion in augmented/mixed reality [21, 23], intelligent vehicles [30, 12], and so on.</p>
<p>Visual grounding tries to learn a good joint representation between visual and text modalities, i.e., the 3D point cloud and language query in 3D visual grounding. As shown in Figure 1 (a), previous 3D grounding studies $[1,4,17,56]$ directly learn the joint representation with a sole 3D grounding objective of maximizing the positive object-query pairs' similarity scores. Specifically, the model first generates a set of 3D object proposals and then fuses each proposal with the language query to predict a similarity score. The framework is trained with a sole objective that maximizes the paired object-query scores and minimizes the unpaired ones' scores. However, direct joint representation learning is challenging and less effective since 3D point clouds are inherently sparse, noisy, and contain limited semantic information. Given that the 2D object representation provides rich and clean semantics, we explore using 2D image semantics to help 3D visual grounding.</p>
<p>How to assist 3D tasks with 2D image semantics remains an open problem. Previous studies on 3D object detection and segmentation have proposed a series of methods that take 2D semantics as extra visual inputs to assist 3D tasks. Representative approaches include aligning the 2D object detection results with 3D bounding boxes [35, 49, 26] and concatenating the image visual feature with 3D points [25, 14, 45, 42, 33]. However, these methods require extra 2D inputs in both training and inference. The necessity of extra input 2D data during inference limits potential application scenarios since 2D inputs might not exist in inference or require extra pre-processing, such as 2D-3D matching and 2D detection. Instead of as extra visual inputs in both training and inference (as shown in Figure 1 (b)), we explore using 2D semantics only in training to assist 3D visual grounding.</p>
<p>In this study, we propose 2D Semantics Assisted Training (SAT), which utilizes 2D image semantics (in the form of object label, image feature, and 2D geometric feature) to ease joint representation learning between the 3D scene and language query. As shown in Figure 1 (c), in addition to the main 3D visual grounding loss [1, 4] that maximizes the score between the paired 3D object and language query, SAT introduces auxiliary loss functions that align objects in 2D images with the corresponding ones in 3D point clouds or language queries. The learned auxiliary alignments effectively distill the rich and clean 2D object representation to assist 3D visual grounding. Specifically, in SAT, we study the training loss design for auxiliary alignments and the encoding method for 2D semantics features. For the former, we propose an object correspondence loss based on the triplet loss [19, 10, 46, 27] for 3D and 2D object alignment. For the latter, we propose a transformer attention mask that generates good 2D semantics features and prevents leaking 2D inputs to the output module.</p>
<p>We experiment with the SAT approach on a transformer-based model [43] we propose and name as 3D grounding transformer. We benchmark SAT on the Nr3D [1], Sr3D [1], and ScanRef [4] datasets. The extra 2D semantics, together with SAT's specially designed way of using them, effectively help the model learn a better 3D object point cloud representation and ease joint representation learning. With the same network architecture and inference inputs, SAT improves the grounding accuracy on Nr3D from the nonSAT baseline’s $37.7 \%$ to $49.2 \%$.</p>
<p>In summary, our main contributions are:</p>
<ul>
<li>We propose 2D Semantics Assisted Training (SAT) that assists 3D visual grounding with 2D semantics. To the best of our knowledge, SAT is the first method that helps 3D tasks with 2D semantics in training but does not require 2D inputs during inference.</li>
<li>With the proposed object correspondence loss and the 2D semantics encoding method, SAT effectively utilizes 2D semantics to learn a better 3D object repre- sentation, which leads to significant accuracy improvements on the Nr3D [1] ( $+10.4 \%$ in absolute accuracy), Sr3D [1] ( $+9.9 \%$ ), and ScanRef [4] ( $+5.6 \%$ ) datasets.</li>
</ul>
<h2>2. Related Work</h2>
<p>3D visual grounding. 3D visual grounding aims to ground the language referred object in a 3D scene (in the form of RGB-XYZ point clouds) to a 3D bounding box. Two recent works Referit3D [1] and ScanRef [4], independently proposed datasets and baseline methods for the 3D visual grounding task. Both works [1, 4] augment the 3D scans in the ScanNet [7] dataset with the manually annotated language queries to construct the 3D visual grounding datasets. Previous 3D grounding studies [1, 4, 17, 56, 11, 37, 59] follow a two-stage framework. In the first stage, multiple 3D object proposals are generated either with ground truth objects [1] or a 3D object detector [4, 34]. In the second stage, 3D object proposal features are fused with the language query to predict each proposal's matching scores. A softmax grounding loss is applied to maximize the score between the paired object proposal and language query.</p>
<p>We find that the sole objective of similarity score maximization is less effective because the point clouds for object proposals are sparse and noisy. In this study, we explore using 2D image semantics to assist 3D visual grounding.
2D semantics in 3D tasks. Studies on 3D object detection and segmentation have explored using 2D image semantics to assist 3D tasks. There exist two representative approaches, i.e., 1) projecting image object detection results into 3D space to assist 3D box prediction [35, 49, 26] and 2) concatenating the image feature with each point in the 3D scene as the extra information for the 3D tasks [25, 14, 45, 42, 33]. ImVoteNet [33] fuses the image object detection results with 3D points.</p>
<p>Previous studies use 2D image semantics as the extra inputs to 3D tasks and thus require the extra 2D information in both training and inference. Despite the performance improvement, the extra 2D inputs potentially limit the application scenarios since the 2D information either does not exist in inference or requires tedious pre-processing, such as 2D-3D matching and 2D object detection. In this study, we explore using 2D semantics only in training to assist 3D visual grounding.
Image visual grounding. 3D visual grounding is related to the image visual grounding task [20, 31, 55, 29]. There are mainly two approaches in image visual grounding, namely the one- and two-stage frameworks. The one-stage methods [51, 38, 50, 8] fuse the language query with each pixel/patch in image and predict grounding boxes densely at all spatial locations. The two-stage methods [55, 46, 54] first generate object proposals based on the visual objectiveness. The methods then compare each proposal with the language query to select the grounding prediction.</p>
<p>We follow previous 3D grounding studies [1, 4, 17] and experiment with our proposed SAT on a two-stage framework introduced in Section 3. We focus on using 2D semantics to assist 3D grounding in this study and leave the exploration of alternative frameworks to future studies.</p>
<h2>3 3D Grounding Transformer</h2>
<p>Before introducing our proposed 2D semantics assisted training (SAT), we first overview the problem modeling of the 3D visual grounding task, and a transformer-based network architecture that we experiment on, named the 3D grounding transformer.</p>
<h3>3.1 3D visual grounding inputs</h3>
<p>The input to the 3D visual grounding task is a 3D scene $S \in \mathbb{R}^{N \times 6}$ in the form of RGB-XYZ point clouds with $N$ points and a natural language query with $K$ words. In the training stage, SAT takes the extra input of 2D semantics extracted from the original ScanNet videos [7] to ease joint representation learning. We detail how SAT represents and utilizes 2D semantics in following sections.</p>
<h3>3.2 Embedding for all modalities</h3>
<p>3D scene embedding. Following previous studies [1, 4, 17], we assume the access to $M$ 3D object proposals (in the form of point cloud object segments) in scene $S$. The proposals are either generated with ground truth objects as in Referit3D [1] or by a detection network [34] as in ScanRef [4]. After getting the proposals, we normalize each object's center and size [44], and encode the point cloud segment of each proposal into a feature vector $x_{m}^{p c}$ with PointNet++ [36, 1, 4, 44]. We obtain the $d$-dimensional 3D proposal embedding $\left{O_{1}, \cdots, O_{M}\right}$ with two learned linear transforms, where</p>
<p>$$
O_{m}=\operatorname{LN}\left(W_{1} x_{m}^{p c}\right)+\operatorname{LN}\left(W_{2} x_{m}^{\text {offset }}\right)
$$</p>
<p>$x_{m}^{p c}$ is the PointNet++'s output feature. $x_{m}^{\text {offset }}$ is a 4D vector with the normalization offset, i.e., the center offsets $(x, y, z)$ and the original size $r$ for proposal $m . W_{1}, W_{2}$ are learned projection matrices. $\operatorname{LN}(\cdot)$ is layer normalization [3].
2D semantics embedding. For each 3D proposal $m$, we project its point clouds onto $L$ sampled frames in the original ScanNet videos [7] and get the corresponded 2D image semantics (image region, 2D bounding box, object class). In sampled frame $l \in{1, \cdots, L}$, we represent the 2D semantics for proposal $m$ by its visual feature $x_{m, l}^{R O I}$ (Region of interest feature from a visual genome [24] pre-trained Faster-RCNN detector [40]), semantic feature $x_{m, l}^{e l s}$ (onehot class vector), and geometric feature $x_{m, l}^{g e o}$ (2D bounding box coordinates and frame's camera pose). We obtain the $d$-dimensional 2D semantics $I_{m, l}$ with linear transforms:</p>
<p>$$
I_{m, l}=\operatorname{LN}\left(W_{3} x_{m, l}^{R O I}+W_{4} x_{m, l}^{e l s}\right)+\operatorname{LN}\left(W_{5} x_{m, l}^{g e o}\right)
$$</p>
<p>where $W_{3}, W_{4}, W_{5}$ are learned projection matrices and $\mathrm{LN}(\cdot)$ is layer normalization. We note that a 3D proposal $O_{m}$ corresponds to multiple 2D semantic feature vector $I_{m, l}$ obtained from different frames $l$. We randomly choose one of $I_{m, l}, l \in{1, \cdots, L}$ as the corresponding 2D semantics in each epoch of training in SAT. We refer to the sampled $d$-dimensional 2D semantic vector as $I_{m}$, which corresponds to the 3D proposal $O_{m}$.
Text embedding. Given a query with $K$ words, we embed the text input with a pre-trained BERT model [9] into a set of $d$-dimensional word feature vectors $\left{Q_{1}, \cdots, Q_{K}\right}$. We fine-tune the BERT text encoder during training.</p>
<h3>3.3 Fusion and grounding module</h3>
<p>After respectively embedding each modality into multiple $d$-dimension feature vectors, we apply a stack of transformer layers [43] to fuse the input modalities (query words, 3D objects proposals, and if training 2D semantics). We denote the transformer's output features at the language, 3D proposal, and 2D semantics positions as $F^{Q}, F^{O}$, and $F^{I}$.</p>
<p>An output grounding module that consists of two fully connected layers projects fused features $\left{F_{1}^{O}, \cdots, F_{M}^{O}\right}$ into a set of $M$ grounding scores $\left{S_{1}^{O}, \cdots, S_{M}^{O}\right}$, respectively. The object proposal $m$ with the highest grounding score is selected as the final grounding prediction.</p>
<h2>4 2D Semantics Assisted Training (SAT)</h2>
<p>SAT learns auxiliary alignments between the 2D object semantics and the objects in 3D scenes/language queries to assist 3D visual grounding. Figure 2 overviews SAT in training and inference with the 3D grounding transformer.</p>
<p>We study two technical problems in SAT. First, in Section 4.1, we propose the auxiliary training objectives that align 2D semantics with the 3D scene and language query. Second, in Section 4.2, we introduce the 2D semantics encoding method that generates the fused feature $F^{I}$ from 2D inputs $I$. We use $F^{I}$ in computing the auxiliary losses.</p>
<h3>4.1 Training objectives</h3>
<p>In addition to the main training objective between the 3D scene and language query, SAT introduces auxiliary training objectives to align 2D semantics with the 3D scene and language query. We apply the "visual grounding loss" between the query and 3D/2D visual inputs. We propose an "object correspondence loss" between the 3D and 2D objects.
3D visual grounding loss. We first introduce the main visual grounding loss $\mathcal{L}<em G="G" V="V">{V G}^{O}$ between the 3D scene and language query [28, 51, 1, 4]. Visual grounding loss $\mathcal{L}</em>$ for proposals $m \in{1, \cdots, M}$. The proposal with the highest Intersection over Union (IoU) with the ground truth region is labeled 1 and all remaining ones have label 0 (the highest IoU}$ is a softmax loss over grounding scores $S_{m}^{O</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: The proposed 2D semantics assisted training (SAT) for 3D visual grounding. (a) In training, SAT takes 2D semantics as extra input and helps 3D visual grounding with the auxiliary objectives of 2D visual grounding $\mathcal{L}<em cor="cor">{VG}^{I}$ and object correspondence prediction $\mathcal{L}</em>$. (b) In inference, SAT does not require 2D inputs and is easy to use. SAT’s attention mask prevents query words and 3D proposals from attending on 2D image semantics $I$ in training (top five rows of the mask), avoiding performance drop in inference when $I$ is not available.</p>
<p>equals 1.0 when experimented with ground truth object proposals). $\mathcal{L}_{VG}^{O}$ encourages the model to generate high scores for positive proposals. In inference, the proposal with the highest score $S^{O}$ is selected as the final prediction.</p>
<p>2D visual grounding loss. We apply the 2D grounding loss $\mathcal{L}<em VG="VG">{VG}^{I}$ with the same form as $\mathcal{L}</em>$. Object correspondence loss. The proposed object correspondence loss learns the correspondence between the objects in 3D scenes and the ones in 2D images. We design the object correspondence loss as a triplet loss }^{O}$ between the 2D semantics and language query. A separate grounding head with two fully connected layers projects the fused features $\left{F_{1}^{I},\cdots,F_{M}^{I}\right}$ into the 2D grounding scores $\left{S_{1}^{I},\cdots,S_{M}^{I}\right}$. $\mathcal{L}_{VG}^{I}$ is the softmax loss computed over $S^{I<em>[19, 10, 46, 27]</em>:</p>
<p>$\mathcal{L}<em m="1">{cor}=\sum</em>\right)\right]}^{M}$ $\left{\left[\alpha-s\left(F_{m}^{O},F_{m}^{I}\right)+s\left(F_{m}^{O},F_{i}^{I<em m="m">{+}\right.$
$\left.+\left[\alpha-s\left(F</em>\right},$}^{O},F_{m}^{I}\right)+s\left(F_{j}^{O},F_{m}^{I}\right)\right]_{+</p>
<p>where $\mathrm{s}(\cdot)$ is the similarity function. We use the inner product over the L2 normalized feature $F^{O}$ and $F^{I}$ as $\mathrm{s}(\cdot)$ in our experiments. $\alpha$ is the margin with a default value of 0.1. $i,j$ are the index for the hard negatives where $i=$ $\operatorname{argmax}<em m="m">{i \neq m} s\left(F</em>}^{O},F_{i}^{I}\right)$ and $j=$ $\operatorname{argmax<em j="j">{j \neq m} s\left(F</em>\right)$. We compute the object correspondence among the 3D and 2D object proposals $m$ within each sample (3D scene). We do not construct negatives across different 3D scenes.}^{O},F_{m}^{I</p>
<p>We optimize the model with the following loss function:</p>
<p>$\mathcal{L}=\mathcal{L}<em VG="VG">{VG}^{O}+\mathcal{L}</em>}^{I}+\mathcal{L<em c="c" l="l" s="s">{c o r}<em>w_{c o r}+\left(\mathcal{L}<em c="c" l="l" s="s">{c l s}^{O}+\mathcal{L}</em>\right)}^{Q</em>w</em>,$ (2)</p>
<p>where $w_{c o r}$ is the weight for the object correspondence loss with a default value of 10. In addition to 3D/2D grounding loss $\mathcal{L}<em c="c" o="o" r="r">{VG}$ and object correspondence loss $\mathcal{L}</em>}$, we add query and object classification losses $\mathcal{L<em c="c" l="l" s="s">{c l s}^{Q}$ and $\mathcal{L}</em>$ as in Referit3D }^{O<em>[1]</em>. The query feature $Q_{0}$ and proposal feature $O$ are projected with fully connected layers to predict the object classes for the language query and 3D proposals. We follow the classification loss weight $w_{c l s}$ of 0.5 <em>[1]</em>. Ablation studies on the losses are in the supplementary material.</p>
<h3>4.2 2D semantics encoding</h3>
<p>SAT uses the fused 2D semantic feature $F^{I}$ to compute 2D visual grounding loss $\mathcal{L}<em c="c" o="o" r="r">{VG}^{I}$ and object correspondence loss $\mathcal{L}</em>$ (the bottom three rows of the mask).}$. In this subsection, we introduce how to encode $F^{I}$ from 2D semantics $I$. We show that a simple yet effective approach to encode $F^{I}$ is by introducing proper attention masks in the multi-modal transformer. Specifically, we adopt the same stack of transformer layers to jointly encode the three input modalities $Q$, $O$, and $I$. We design the attention mask in Figure 2 (a) such that $F^{Q}$ and $F^{O}$ do not directly attend to 2D inputs $I$ (the top five rows of the mask). In this way, the proposed mask prevents the model from directly using 2D inputs $I$ for grounding prediction and thus avoids the performance drop in inference when $I$ is not available. Meanwhile, the proposed attention mask allows the model to reference both 2D semantics $I$ and other input features $Q$ and $O$ when generating $F^{I</p>
<p>We find that both properties of the proposed mask, i.e., masking 2D inputs $I$ from $F^{Q}$ and $F^{O}$, and referencing $Q$</p>
<p>and $O$ when generating $F^{I}$, are critical to SAT's success. We discuss alternative methods as follow. 1) Methods that do not mask $I$ from $F^{Q}$ and $F^{O}$ will leak 2D inputs to $F^{O}$ and partially rely on $I$ to generate the grounding prediction in training. Therefore, the grounding accuracy drops catastrophically in inference when no 2D inputs $I$ are available. 2) Encoding $F^{Q} / F^{O}$ and $F^{I}$ independently with $Q / O$ and $I$ avoid the 2D input leakage. However, without referencing the scene context $Q$ and $O$, the 2D feature $F^{I}$ fails to generate relevant object representations that effectively help 3D visual grounding. We show related ablations in Section 5.4.</p>
<h2>5 Experiments</h2>
<h3>5.1 Datasets</h3>
<p>Nr3D. The Natural Reference in 3D (Nr3D) dataset [1] augments the indoor 3D scene dataset ScanNet [7] with 41, 503 natural language queries annotated by Amazon Mechanical Turk (AMT) workers. There exist 707 unique indoor scenes with targets belong to one of the 76 object classes. There are multiple but no more than six distractors (objects in the same class as the target) in the scene for each target. The dataset splits follow the official ScanNet [7] splits.
Sr3D/Sr3D+. The Spatial Reference in 3D (Sr3D) dataset [1] contains 83, 572 queries automatically generated based on a "target"-"spatial relationship"-"anchor object" template. The Sr3D+ dataset further enlarges Sr3D with the samples that do not have multiple distractors in the scene and ends up with 114, 532 queries.
ScanRef. The ScanRef dataset [4] augments the 800 3D indoor scenes in the ScanNet [7] dataset with 51, 583 language queries. ScanRef follows the official ScanNet [7] splits and contains $36,665,9,508$, and 5, 410 samples in train/val/test sets, respectively.</p>
<h3>5.2 Experiment settings</h3>
<p>Evaluation metric. We follow the experiment settings in Referit3D [1] and ScanRef [4] for experiments with ground truth and detector-generated proposals, respectively. Specifically, Referit3D [1] assumes the access to ground truth objects as the 3D proposals and converts the grounding task into a classification problem. The models are evaluated by the accuracy, i.e., whether the model correctly selects the referred object among $M$ proposals. We choose this "using ground truth proposal" setting as the default setting and present the results on all experimented datasets (Nr3d, Sr3d, and ScanRef).</p>
<p>Alternatively, ScanRef [4] adopts a 3D object detector [34] to generate object proposals. On the ScanRef dataset, we also evaluate models using Acc@ $k$ IoU, i.e., the fraction of language queries whose predicted box overlaps the ground truth with IoU $&gt;k$ IoU. We experiment with the IoU threshold $k$ IoU of 0.25 and 0.5 . For clarity, we present the experiments with ground truth proposals in the main paper and postpone the experiments of "SAT with detectorgenerated proposals" to the supplementary material.
Implementation details. We set the dimension $d$ in all transformer layers as 768 . We experiment with a text transformer with 3 layers and a fusion transformer with 4 layers [15, 53]. The text transformer is initialized from the first three layers of BERT ${ }<em G="G" V="V">{\text {BASE }}$ [9], and the fusion transformer is trained from scratch. We sample 1024 points for each 3D proposal from its point cloud segment and encode the proposal with PointNet++ [36]. We follow the max sentence length and proposal numbers in Referit3D [1] and ScanRef [4] when experimented on Nr3D/Sr3D and ScanRef, respectively. The model is trained with the Adam [22] optimizer with a batch size of 16 . We set an initial learning rate of $10^{-4}$ and reduce the learning rate by a multiplicative factor of 0.65 every 10 epochs for a total of 100 epochs.
Compared methods. We compare SAT with the state-of-the-art methods [1, 4, 17, 56] and the non-SAT baseline. "Non-SAT" adopts the same "3D grounding transformer" architecture used in "SAT." The only difference is that "nonSAT" does not include 2D semantics in training and thus does not use the auxiliary losses $\mathcal{L}</em>$. With the same network architecture and experiment settings, "nonSAT" is a directly comparable baseline to "SAT." The performance difference shows how much SAT could help the 3D visual grounding task.}^{I}$ and $\mathcal{L}_{\text {cor }</p>
<h3>5.3 3D visual grounding results</h3>
<p>Nr3D. Table 1 reports the grounding accuracy on the Nr3D [1] dataset. Both "non-SAT" and "SAT" use the 3D grounding transformer introduced in Section 3. For SAT's reported accuracy, we encode 2D semantics $I$ from the visual feature $x^{R O I}$, object semantic feature $x^{c l s}$, and geometric feature $x^{g e o}$ following Eq. 1. We postpone the ablation studies on the types of 2D semantics to Section 5.4. Different columns show the results with different training data, i.e., using Nr3D's training set only, or jointly training with Sr3D/Sr3D+'s training set. We take "SAT-Nr3D" as the default setting and refer to it as "SAT." We refer to the experiments with extra data as "SAT w/ Sr3D/Sr3D+."</p>
<p>The top five rows of Table 1 show that our baseline "nonSAT" already achieves comparable performance to the state of the art (non-SAT: $37.7 \%$, InstanceRefer [56]: $38.8 \%$ ). By effectively utilizing 2D semantics in training, our proposed SAT improves the non-SAT baseline accuracy from $37.7 \%$ to $49.2 \%$, with the identical model architecture and inference inputs. SAT also outperforms the state-of-the-art accuracy [56] of $38.8 \%$ a large margin of $+10.4 \%$. Jointly using the Sr3D/Sr3D+ training data further improves the grounding accuracy. As shown in the last row, "SAT w/ Sr3D+" improves "SAT-Nr3D" from $49.2 \%$ to $56.5 \%$.</p>
<p>Analyses reveal that SAT learns a better 3D object repre-</p>
<p>Table 1: The 3D grounding accuracy on Nr3D <em>[1]</em> with different training data (Nr3D training set only or with extra data from Sr3D/Sr3D+).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Nr3D</th>
<th>w/ Sr3D</th>
<th>w/ Sr3D+</th>
</tr>
</thead>
<tbody>
<tr>
<td>V + L<em>[1]</em></td>
<td>26.6$\pm$0.5%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Ref3DNet<em>[1]</em></td>
<td>35.6$\pm$0.7%</td>
<td>37.2$\pm$0.3%</td>
<td>37.6$\pm$0.4%</td>
</tr>
<tr>
<td>TGNN<em>[17]</em></td>
<td>37.3$\pm$0.3%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>IntanceRefer<em>[56]</em></td>
<td>38.8$\pm$0.4%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>non-SAT</td>
<td>37.7$\pm$0.3%</td>
<td>43.9$\pm$0.3%</td>
<td>45.9$\pm$0.2%</td>
</tr>
<tr>
<td>SAT (Ours)</td>
<td>49.2$\pm$0.3%</td>
<td>53.9$\pm$0.2%</td>
<td>56.5$\pm$0.1%</td>
</tr>
</tbody>
</table>
<p>Table 3: The accuracy on ScanRef<em>[4]</em> with different training data (ScanRef training set only or with extra data from Nr3D/Sr3D+).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>ScanRef</th>
<th>w/ Nr3D</th>
<th>w/ Sr3D+</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ref3DNet<em>[1]</em></td>
<td>46.9$\pm$0.2%</td>
<td>47.5$\pm$0.4%</td>
<td>47.0$\pm$0.3%</td>
</tr>
<tr>
<td>non-SAT</td>
<td>48.2$\pm$0.2%</td>
<td>50.2$\pm$0.1%</td>
<td>51.7$\pm$0.1%</td>
</tr>
<tr>
<td>SAT (Ours)</td>
<td>53.8$\pm$0.1%</td>
<td>57.0$\pm$0.3%</td>
<td>56.5$\pm$0.2%</td>
</tr>
</tbody>
</table>
<p>sentation with the assist of 2D semantics, which leads to the $11.5\%$ improvement over the non-SAT baseline. The improvement brought by Sr3D/Sr3D+ mainly comes from better modeling the spatial relationships in queries. We present these analyses in Section 6.
Sr3D. Table 2 shows the grounding accuracy on Sr3D<em>[1]</em>. We draw similar conclusions from Table 2 as in Table 1 that 1) SAT significantly improves the grounding accuracy from $47.4\%$ to $57.9\%$, 2) SAT outperforms the previous state of the art<em>[1, 17, 56]</em> by large margins, and 3) extra training data (Nr3D) further boosts the accuracy from $57.9\%$ to $60.7\%$.
ScanRef. Table 3 reports the grounding accuracy on the ScanRef dataset<em>[4]</em> with ground truth object proposals. We observe a significant improvement of "SAT" over the nonSAT baseline (SAT: $53.8\%$, non-SAT: $48.2\%$ ). Extra training data from Nr3D and Sr3D+ further improves the accuracy from $53.8\%$ to $57.0\%$ and $56.5\%$, respectively.</p>
<p>In addition to the ground-truth object proposals<em>[1]</em>, we experiment with the proposal setting in ScanRef<em>[4]</em> that generates proposals with a 3D detector<em>[34]</em>. To apply SAT, we first compute the ground truth 2D semantics offline. In training, we match each predicted 3D proposal with a 2D semantics object that has the largest IoU with the 3D proposal. We then evaluate the models with the Acc@0.25 and Acc@0.50 metrics. SAT achieves the Acc@0.25 and Acc@0.50 of $44.54\%$ and $30.14\%$, outperforming the nonSAT baseline of $38.92\%$ and $26.40\%$ by a large margin. We introduce the details of "SAT with detector-generated proposals" in the supplementary material.</p>
<h3>5.4 Ablation studies</h3>
<p>Multi-modal transformer masks. SAT’s attention mask in the multi-modal transformer has two properties, i.e., 1) masking 2D semantics $I$ from $F^{Q}$ and $F^{O}$, and 2) referencing context $Q$ and $O$ when generating $F^{I}$. We verify the importance of both properties with the ablation studies in Table 4. In training, we replace our proposed transformer’s</p>
<p>Table 2: The 3D grounding accuracy on Sr3D<em>[1]</em> with different training data (Sr3D training set only or with extra data from Nr3D).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Sr3D</th>
<th>w/ Nr3D</th>
</tr>
</thead>
<tbody>
<tr>
<td>V + L<em>[1]</em></td>
<td>33.0$\pm$0.4%</td>
<td>-</td>
</tr>
<tr>
<td>Ref3DNet<em>[1]</em></td>
<td>40.8$\pm$0.2%</td>
<td>41.5$\pm$0.2%</td>
</tr>
<tr>
<td>TGNN<em>[17]</em></td>
<td>45.0$\pm$0.2%</td>
<td>-</td>
</tr>
<tr>
<td>IntanceRefer<em>[56]</em></td>
<td>48.0$\pm$0.3%</td>
<td>-</td>
</tr>
<tr>
<td>non-SAT</td>
<td>47.4$\pm$0.2%</td>
<td>50.1$\pm$0.1%</td>
</tr>
<tr>
<td>SAT (Ours)</td>
<td>57.9$\pm$0.1%</td>
<td>60.7$\pm$0.2%</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation studies on 2D semantics embedding with different transformer attention masks. The gray mask color indicates prevent from attending. SAT’s attention mask is in Figure 2 (a).</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Method</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training mask A</td>
<td>Training mask B</td>
<td>non-SAT</td>
<td>37.7$\pm$0.3%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SAT-mask A</td>
<td>33.9$\pm$0.2%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SAT-mask B</td>
<td>43.9$\pm$0.2%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SAT (Ours)</td>
<td>49.2$\pm$0.3%</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation studies on different types of 2D semantics inputs as in Eq 1. We highlight the default "SAT" setting by underline.</p>
<table>
<thead>
<tr>
<th></th>
<th>$+x^{geo}$</th>
<th>$+x^{cls}$</th>
<th>$+x^{ROI}$</th>
<th>Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>(a)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>37.7$\pm$0.3%</td>
</tr>
<tr>
<td>(b)</td>
<td>$\checkmark$</td>
<td>-</td>
<td>-</td>
<td>39.4$\pm$0.3%</td>
</tr>
<tr>
<td>(c)</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>-</td>
<td>48.1$\pm$0.2%</td>
</tr>
<tr>
<td>(d)</td>
<td>$\checkmark$</td>
<td>-</td>
<td>$\checkmark$</td>
<td>46.5$\pm$0.1%</td>
</tr>
<tr>
<td>(e)</td>
<td>-</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>43.2$\pm$0.2%</td>
</tr>
<tr>
<td>(f)</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>49.2$\pm$0.3%</td>
</tr>
</tbody>
</table>
<p>attention mask in Figure 2 (a) with the alternative masks A/B in Table 4. In inference, the model removes the extra 2D semantics input and follows the standard inference setting as in Figure 2 (b).</p>
<p>Mask A does not mask 2D semantics $I$ from $F^{Q}$ and $F^{O}$. We observe that the model directly relies on the extra 2D inputs $I$ for grounding prediction. Consequently, the grounding accuracy drops catastrophically to $33.9\%$ when no 2D inputs are available in inference. Mask B encodes $F^{I}$ with 2D semantics $I$ alone. Without referencing the scene context in $Q$ and $O$, the 2D feature $F^{I}$ fails to provide a relevant object representation and is thus less effective in helping 3D visual grounding. Although outperforming the non-SAT baseline accuracy of $37.7\%$, "SAT-mask B" performs much worse than the SAT with our proposed attention mask (SAT-mask B: $43.9\%$, SAT: $49.2\%$).
Types of 2D context inputs. Table 5 shows the ablation studies on the types of 2D semantics. The combination of $x^{ROI}$, $x^{cls}$, and $x^{geo}$ are projected into a $d$-dimension 2D semantics feature $I$ following Eq. 1.</p>
<p>Compared with the non-SAT baseline accuracy of $37.7\%$ in row (a), SAT with any 2D semantics significantly boosts the grounding accuracy (rows (b-f)). Jointly using visual feature $x^{ROI}$, semantic feature $x^{cls}$, and geometric feature $x^{geo}$ achieves the best accuracy of $49.2\%$, as in row (f).</p>
<p>Table 6: Linear probing accuracy on Nr3D.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Linear probing</th>
<th>Full fine-tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td>non-SAT</td>
<td>35.7%</td>
<td>63.4%</td>
</tr>
<tr>
<td>SAT</td>
<td>60.1%</td>
<td>65.4%</td>
</tr>
<tr>
<td>SAT w/ Sr3D+</td>
<td>61.7%</td>
<td>67.6%</td>
</tr>
</tbody>
</table>
<h2>6 How does SAT help?</h2>
<p>In this section, we analyze how does SAT help 3D visual grounding. We draw three major conclusions: 1) SAT learns a better 3D object representation $F^{O}$ with the assist of 2D semantics $I$, leading to consistent performance improvements over samples with different target classes, number of distractors, query lengths/types, . (Section 6.1). 2) Training with the extra data in Sr3D/Sr3D+ mainly benefits the queries with spatial relationship referring (Section 6.2). 3) The performance gap between SAT and the methods that require extra 2D inputs in inference is small, indicating the effectiveness of SAT in utilizing 2D semantics (Section 6.3). Finally, we present qualitative examples in Section 6.4.</p>
<h3>6.1 Linear probing</h3>
<p>How could SAT achieve the large improvement over the non-SAT baseline and the state of the art? We conjecture that SAT learns a better 3D representation $F^{O}$ for noisy object point clouds with the assist of 2D semantics. Consequently, we observe consistent 3D grounding accuracy improvements on samples with different target object classes, number of distractors, query lengths/types, ., as shown in the performance breakdown in the supplementary material.</p>
<p>We use linear probing <em>[57, 13, 6]</em> to evaluate the quality of the learned 3D object representations $F^{O}$ in different models. Specifically, we keep the pre-trained grounding network fixed and train a linear classifier that maps each proposal feature $F_{m}^{O}$ into one of Nr3D’s 607 object classes. Because no classification annotation is seen during the grounding network training, we evaluate learned representations $F^{O}$ by the object classification accuracy. Similar to the use of linear probing in representation learning <em>[57, 13, 6]</em>, we consider a higher linear probing accuracy the indicator of a better 3D object representation $F^{O}$.</p>
<p>Table 6 shows the linear probing accuracy on Nr3D. SAT improves the linear probing accuracy from $35.7\%$ to $60.1\%$, compared with the non-SAT baseline. The significant improvement supports the conjecture that SAT learns a better 3D object representation with 2D semantics in training. We observe similar improvements in the full fine-tuning setting, where all layers are updated for object classification. It is worth noting that SAT’s effectiveness in generating better 3D object representations may hold the promise of benefiting not only 3D vision-language tasks such as grounding <em>[1, 4]</em> and captioning <em>[5]</em>, but also 3D semantic understanding tasks such as 3D scene graph prediction <em>[2, 44]</em>.</p>
<h3>6.2 Spatial relationship referring</h3>
<p>Our second observation is that the extra data in Sr3D/Sr3D+ helps the queries with spatial relationship referring. On Nr3D’s subset with spatial queries (76.7% of the samples), the extra Sr3D+ training data leads to an 8.4% improvement on “SAT-Nr3D” from 48.4% to 56.8%. In contrast, the improvement is only 3.9% on the remaining samples (from 50.9% to 54.8%). Furthermore, we observe larger improvements on subsets that contain the frequently appeared spatial words in Sr3D/Sr3D+, e.g., “closest” of $+11.5\%$ and “farthest” of $+13.5\%$.</p>
<h3>6.3 2D semantics as extra inputs</h3>
<p>In this subsection, we compare SAT with the methods that require extra 2D inputs in both training and inference. We design two methods that directly use 2D semantics as extra inputs, namely the “2D input aligned” and “2D input unaligned.” Both methods use the same network architecture as SAT and take extra 2D inputs in both training and inference. For “2D input aligned,” we concatenate 2D semantics $I_{m}$ with 3D proposal feature $O_{m}$ and use the extended proposal feature as $O_{m}$ in both training and inference. The input sequence length for “2D input aligned” is $M+K$. We train “2D input aligned” with the main grounding loss $\mathcal{L}<em cls="cls">{VG}^{O}$ and the classification loss $\mathcal{L}</em>$ in Eq. 2 as SAT.}$ in Eq. 2. For “2D input unaligned,” we input 2D semantics $I_{m}$ as extra input tokens to the multi-modal transformer in both training and inference. The input sequence length for “2D input unaligned” is $2M+K$. We train “2D input unaligned” with the same loss $\mathcal{L</p>
<p>Table 7 shows the experiment results on Nr3D with no 2D semantics (upper portion), with 2D inputs only in training (middle portion), and in both training and inference (bottom portion). The “hard” subset contains more than 2 distractors and remaining samples belong to “easy.” We observe a marginal accuracy gap of 1% between “SAT” and “2D input aligned/unaligned” (“overall” in row (e): 49.2%, rows (h,i): 50.3%). The comparable performance indicates SAT’s effectiveness in utilizing 2D semantics to help 3D visual grounding. Meanwhile, SAT does not require extra 2D inputs in inference as “2D input aligned/unaligned,” and thus is easier to use.</p>
<h3>6.4 Qualitative insights</h3>
<p>The left four examples of Figure 3 show representative failure cases of “non-SAT” that can be corrected by “SAT.” We group common cases into three scenarios. 1) Object: SAT improves non-SAT by better recognizing the object classes. Non-SAT occasionally fails to ground the head noun and generates the object prediction in a different class, e.g., “bed” instead of the referred “desk” in Figure 3 (a). 2) Relationship: We observe that SAT is better in modeling relationships in language queries, despite no specific modules are proposed in SAT for relationship understanding. For example, in Figures 3 (b,c), SAT correctly understands the relationship “attach to” and “over.” We conjec</p>
<p>Table 7: The benefit of using 2D semantics in 3D visual grounding. The upper/middle/bottom portion of the table shows the results that do not use 2D semantics/use only in training/use in both training and inference (as extra inputs). The results with extra training data (Sr3D/Sr3D+) are shown in gray. Our SAT (#(e)) shows comparable performance to oracles that require 2D inputs in inference (#(h,i)).</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Extra</th>
<th>2D semantics</th>
<th></th>
<th>Overall</th>
<th>Easy</th>
<th>Hard</th>
<th>View-dep.</th>
<th>View-indep.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>data</td>
<td>Train</td>
<td>Test</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(a)</td>
<td>Ref3DNet [1]</td>
<td>-</td>
<td>✗</td>
<td>✗</td>
<td>35.6$\pm$0.7%</td>
<td>43.6$\pm$0.8%</td>
<td>27.9$\pm$0.7%</td>
<td>32.5$\pm$0.7%</td>
<td>37.1$\pm$0.8%</td>
</tr>
<tr>
<td>(b)</td>
<td>TGNN [17]</td>
<td>-</td>
<td>✗</td>
<td>✗</td>
<td>37.3$\pm$0.3%</td>
<td>44.2$\pm$0.4%</td>
<td>30.6$\pm$0.2%</td>
<td>35.8$\pm$0.2%</td>
<td>38.0$\pm$0.3%</td>
</tr>
<tr>
<td>(c)</td>
<td>IntanceRefer [56]</td>
<td>-</td>
<td>✗</td>
<td>✗</td>
<td>38.8$\pm$0.4%</td>
<td>46.0$\pm$0.5%</td>
<td>31.8$\pm$0.4%</td>
<td>34.5$\pm$0.6%</td>
<td>41.9$\pm$0.4%</td>
</tr>
<tr>
<td>(d)</td>
<td>non-SAT</td>
<td>-</td>
<td>✗</td>
<td>✗</td>
<td>37.7$\pm$0.3%</td>
<td>44.5$\pm$0.5%</td>
<td>31.2$\pm$0.2%</td>
<td>34.1$\pm$0.3%</td>
<td>39.5$\pm$0.4%</td>
</tr>
<tr>
<td>(e)</td>
<td>SAT (Ours)</td>
<td>-</td>
<td>✓</td>
<td>✗</td>
<td>49.2$\pm$0.3%</td>
<td>56.3$\pm$0.5%</td>
<td>42.4$\pm$0.4%</td>
<td>46.9$\pm$0.3%</td>
<td>50.4$\pm$0.3%</td>
</tr>
<tr>
<td>(f)</td>
<td>SAT w/ Sr3D (Ours)</td>
<td>Sr3D</td>
<td>✓</td>
<td>✗</td>
<td>53.9$\pm$0.2%</td>
<td>61.5$\pm$0.1%</td>
<td>46.7$\pm$0.3%</td>
<td>52.7$\pm$0.7%</td>
<td>54.5$\pm$0.3%</td>
</tr>
<tr>
<td>(g)</td>
<td>SAT w/ Sr3D+ (Ours)</td>
<td>Sr3D+</td>
<td>✓</td>
<td>✗</td>
<td>56.5$\pm$0.1%</td>
<td>64.9$\pm$0.2%</td>
<td>48.4$\pm$0.1%</td>
<td>54.4$\pm$0.3%</td>
<td>57.6$\pm$0.1%</td>
</tr>
<tr>
<td>(h)</td>
<td>2D input aligned</td>
<td>-</td>
<td>✓</td>
<td>✓</td>
<td>50.0$\pm$0.1%</td>
<td>62.0$\pm$0.2%</td>
<td>38.5$\pm$0.3%</td>
<td>44.7$\pm$0.3%</td>
<td>52.6$\pm$0.3%</td>
</tr>
<tr>
<td>(i)</td>
<td>2D input unaligned</td>
<td>-</td>
<td>✓</td>
<td>✓</td>
<td>50.3$\pm$0.4%</td>
<td>58.5$\pm$0.7%</td>
<td>42.4$\pm$0.5%</td>
<td>48.1$\pm$0.4%</td>
<td>51.3$\pm$0.5%</td>
</tr>
<tr>
<td>(j)</td>
<td>2D input aligned</td>
<td>Sr3D+</td>
<td>✓</td>
<td>✓</td>
<td>59.7$\pm$0.1%</td>
<td>71.0$\pm$0.3%</td>
<td>48.8$\pm$0.5%</td>
<td>52.9$\pm$0.3%</td>
<td>63.1$\pm$0.2%</td>
</tr>
<tr>
<td>(k)</td>
<td>2D input unaligned</td>
<td>Sr3D+</td>
<td>✓</td>
<td>✓</td>
<td>61.0$\pm$0.3%</td>
<td>69.0$\pm$0.6%</td>
<td>53.2$\pm$0.3%</td>
<td>58.4$\pm$0.3%</td>
<td>62.2$\pm$0.5%</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The failure cases of non-SAT that can be corrected by SAT (the left four examples), and SAT’s representative failure cases (the right two examples). The green/red/yellow colors indicate the correct/incorrect/ground truth boxes. The object class for each box is shown in text next to the 3D box. We provide rendered scenes in first row to better visualize the scene layout. Best viewed zoomed in and in color.</p>
<p>ture that SAT learns a better object representation for both foreground and background objects, which benefits the relationship modeling. 3) Color and shape: SAT also performs better in understanding color and shape-related language queries, e.g., “white and green” in Figure 3 (d).</p>
<p>The right two examples of Figure 3 show SAT’s representative failure cases. Figure 3 (e) shows a failure case that requires understanding “facing the wall.” Although SAT improves both view-dependent and independent samples (c.f. Table 7 “View-dep.” column), view understanding remains an unsolved problem. Figure 3 (f) shows a failure case caused by ambiguous queries. The model predicts the “chair facing the desk” instead of the referred “office chair facing the desk” in the ground truth. We observe that the model and human annotators occasionally confuse objects in similar categories, such as chair/office-chair (Figure 3 (f)), table/coffee-table (Figure 3 (e)), etc.</p>
<h2>7 Conclusion</h2>
<p>We have presented 2D semantics assisted training (SAT) for 3D visual grounding. SAT uses 2D semantics in training to assist 3D visual grounding and eases joint representation learning between the 3D scene and language query. With identical network and inference inputs, SAT beats the non-SAT baseline by 11.5% in absolute accuracy. SAT leads to the new state of the art on multiple datasets and outperforms previous works by large margins. Analyses show that SAT effectively uses 2D semantics to learn a better 3D point cloud object representation that helps 3D visual grounding.</p>
<h2>Acknowledgment</h2>
<p>This work is supported in part by NSF awards IIS-1704337 and IIS-1813709, as well as our corporate sponsors.</p>
<h2>References</h2>
<p>[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In ECCV, pages 422-440. Springer, 2020. 1, 2, 3, 4, 5, 6, 7, $8,12,13,14$
[2] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene graph: A structure for unified semantics, 3d space, and camera. In CVPR, pages 5664-5673, 2019. 7
[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3
[4] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, 2020. 1, 2, 3, 5, 6, 7, 13, 14
[5] Dave Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In CVPR, 2021. 7
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pages 1691-1703. PMLR, 2020. 7
[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5828-5839, 2017. 2, 3, 5
[8] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual grounding with transformers. arXiv preprint arXiv:2104.08541, 2021. 2
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3, 5
[10] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017. 2, 4
[11] Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Ajmal Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. arXiv preprint arXiv:2103.16381, 2021. 2
[12] Qi Feng, Vitaly Ablavsky, and Stan Sclaroff. Cityflow-nl: Tracking and retrieval of vehicles at city scaleby natural language descriptions. arXiv preprint arXiv:2101.04741, 2021. 1
[13] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In ICCV, pages 6391-6400, 2019. 7
[14] Ji Hou, Angela Dai, and Matthias Nießner. 3d-sis: 3d semantic instance segmentation of rgb-d scans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4421-4430, 2019. 2
[15] Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Marcus Rohrbach. Iterative answer prediction with pointer-
augmented multimodal transformers for textvqa. In CVPR, pages 9992-10002, 2020. 5
[16] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. Natural language object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4555-4564, 2016. 1
[17] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. In AAAI, 2021. 1, 2, 3, 5, 6, $8,13,14$
[18] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, ChiWing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In CVPR, pages 4867-4876, 2020. 13, 14
[19] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128-3137, 2015. 2, 4
[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787-798, 2014. 1, 2
[21] Kangsoo Kim, Mark Billinghurst, Gerd Bruder, Henry BeenLirn Duh, and Gregory F Welch. Revisiting trends in augmented reality research: A review of the 2nd decade of ismar (2008-2017). IEEE transactions on visualization and computer graphics, 24(11):2947-2962, 2018. 1
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5
[23] Bernard C Kress and William J Cummings. 11-1: Invited paper: Towards the ultimate mixed reality experience: Hololens display architecture choices. In SID symposium digest of technical papers, volume 48, pages 127-131. Wiley Online Library, 2017. 1
[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32-73, 2017. 3
[25] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander. Joint 3d proposal generation and object detection from view aggregation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1-8. IEEE, 2018. 2
[26] Jean Lahoud and Bernard Ghanem. 2d-driven 3d object detection in rgb-d images. In Proceedings of the IEEE International Conference on Computer Vision, pages 4622-4630, 2017. 2
[27] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. Visual semantic reasoning for image-text matching. In ICCV, pages 4654-4662, 2019. 2, 4
[28] Ruotian Luo and Gregory Shakhnarovich. Comprehensionguided referring expressions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7102-7111, 2017. 3</p>
<p>[29] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11-20, 2016. 2
[30] Vivek Mittal. Attngrounder: Talking to cars with attention. In European Conference on Computer Vision, pages 62-73. Springer, 2020. 1
[31] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641-2649, 2015. 2
[32] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. International journal of computer vision, 123(1):74-93, 2017. 1
[33] Charles R Qi, Xinlei Chen, Or Litany, and Leonidas J Guibas. Imvotenet: Boosting 3d object detection in point clouds with image votes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4404-4413, 2020. 2
[34] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, pages 9277-9286, 2019. 2, 3, 5, 6, 13, 14
[35] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object detection from rgbd data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 918-927, 2018. 2
[36] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017. 3, 5
[37] Junha Roh, Karthik Desingh, Ali Farhadi, and Dieter Fox. Languagerefer: Spatial-language model for 3d visual grounding. arXiv preprint arXiv:2107.03438, 2021. 2
[38] Arka Sadhu, Kan Chen, and Ram Nevatia. Zero-shot grounding of objects from natural language queries. In ICCV, pages 4694-4703, 2019. 2
[39] Arka Sadhu, Kan Chen, and Ram Nevatia. Video object grounding using semantic roles in language description. In CVPR, pages 10417-10427, 2020. 1
[40] Amaia Salvador, Xavier Giró-i Nieto, Ferran Marqués, and Shin'ichi Satoh. Faster r-cnn features for instance search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 9-16, 2016. 3
[41] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In ICCV, pages 9339-9347, 2019. 1
[42] Shuran Song and Jianxiong Xiao. Deep sliding shapes for amodal 3d object detection in rgb-d images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 808-816, 2016. 2
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 2, 3
[44] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3961-3970, 2020. 3, 7
[45] Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martín-Martín, Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6d object pose estimation by iterative dense fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3343-3352, 2019. 2
[46] Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik. Learning two-branch neural networks for image-text matching tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):394-407, 2018. 2, 4
[47] Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5005-5013, 2016. 1
[48] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9068-9079, 2018. 1
[49] Danfei Xu, Dragomir Anguelov, and Ashesh Jain. Pointfusion: Deep sensor fusion for 3d bounding box estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 244-253, 2018. 2
[50] Zhengyuan Yang, Tianlang Chen, Liwei Wang, and Jiebo Luo. Improving one-stage visual grounding by recursive subquery construction. In ECCV, 2020. 2
[51] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate onestage approach to visual grounding. In ICCV, pages 46834693, 2019. 1, 2, 3, 14
[52] Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jingsong Su, and Jiebo Luo. Grounding-tracking-integration. IEEE Transactions on Circuits and Systems for Video Technology, 2020. 1
[53] Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo Luo. Tap: Text-aware pre-training for text-vqa and textcaption. In CVPR, 2021. 5
[54] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1307-1315, 2018. 1, 2
[55] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In European Conference on Computer Vision, pages 69-85. Springer, 2016. 1, 2
[56] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Zhen Li, and Shuguang Cui. Instancerefer: Cooperative holistic</p>
<p>understanding for visual grounding on point clouds through instance multi-level contextual referring. arXiv preprint arXiv:2103.01128, 2021. 1, 2, 5, 6, 8, 13, 14
[57] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In CVPR, pages 1058-1067, 2017. 7
[58] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In CVPR, pages 10668-10677, 2020. 1
[59] Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, Si Liu, et al. Transrefer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding. arXiv preprint arXiv:2108.02388, 2021. 2</p>
<h1>SAT: 2D Semantics Assisted Training for 3D Visual Grounding (Supplementary Material)</h1>
<p>In the first part of the supplementary material, we present additional ablation studies and detailed result analyses. In the second part, we extend our SAT approach to detectorgenerated 3D proposals. We also discuss how 3D proposal quality influences the 3D grounding accuracy.</p>
<h2>A. Experiments</h2>
<h2>A.1. Ablation studies</h2>
<p>Training objectives. We conduct ablation studies on the training objectives introduced in the main paper's Eq. 2. Table 8 shows the experiments on the Nr3D dataset with ground truth proposals. Same as SAT, all compared methods take extra 2D inputs in the training stage and do not require extra inputs in inference.</p>
<p>Row (b) shows the baseline grounding accuracy with the main 3D grounding loss $\mathcal{L}<em _cls="{cls" _text="\text">{V G}^{O}$ and classification loss $\mathcal{L}</em>}}$ only. Despite input to the model during the training, 2D semantics $I$ does not affect the main model ( $Q$ and $O$ ) in this baseline, as $I$ is not attended to and no $I$-related auxiliary losses are included. Therefore, row (b) is equivalent to the main paper's non-SAT baseline and shows a comparable accuracy of $38.0 \%$. SAT's auxiliary objectives of 2D grounding loss $\mathcal{L<em _cor="{cor" _text="\text">{V G}^{I}$ and object correspondence loss $\mathcal{L}</em>$ helpful to both the baseline and the final SAT model, as shown in rows (a,b) and rows (e,f), respectively.}}$ improve the accuracy to $38.5 \%$ and $44.9 \%$, respectively, as shown in rows (c,d). Our proposed SAT jointly applies the two auxiliary objectives and achieves the best accuracy of $49.2 \%$. Furthermore, we find classification loss $\mathcal{L}_{\text {cls }</p>
<h2>A.2. Performance breakdown</h2>
<p>In this subsection, we show the performance breakdown on the Nr3D [1] dataset to better understand SAT's improvement. We report SAT's performance on subsets with different target object classes, numbers of distractors, query lengths/types, spatial relationships, etc. We observe that SAT effectively utilizes the 2D semantics to learn better 3D object representations, and obtains consistent improvements on these subsets.
Numbers of distractors. Table 9 shows the performance on subsets with different numbers of distractors. We compare non-SAT with our SAT in the bottom part of the table. Intuitively, we observe a performance decrease when there exist more distractors in the scene, e.g., SAT's accuracy drops from $56.3 \%$ to $30.6 \%$ when the distractor number increases from 2 to 6 . On the other hand, the relative improvement</p>
<p>Table 8. Ablation studies on the training objectives in the main paper's Eq. 2. Experiments are conducted on the Nr3D dataset with ground truth proposals. We highlight "SAT" by underline.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\mathcal{L}<em c="c" l="l" s="s">{c l s}^{O}+\mathcal{L}</em>$}^{Q</th>
<th style="text-align: center;">$\mathcal{L}_{V G}^{I}$</th>
<th style="text-align: center;">$\mathcal{L}_{\text {cor }}$</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">(a)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$33.8 \pm 0.1 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$38.0 \pm 0.3 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(c)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$38.5 \pm 0.3 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(d)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$44.9 \pm 0.2 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(e)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$46.0 \pm 0.2 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(f)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$49.2 \pm 0.3 \%$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 9. Grounding accuracy on Nr3D's subsets with different numbers of distractors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Percent(\%)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">5.5</td>
</tr>
<tr>
<td style="text-align: left;">non-SAT</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: left;">SAT</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">30.6</td>
</tr>
</tbody>
</table>
<p>Table 10. Grounding accuracy on Nr3D's subsets with different query lengths.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">2-6</th>
<th style="text-align: center;">7-8</th>
<th style="text-align: center;">9-10</th>
<th style="text-align: center;">11-13</th>
<th style="text-align: center;">$14+$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Percent(\%)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">21.2</td>
</tr>
<tr>
<td style="text-align: left;">non-SAT</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: left;">SAT</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">41.8</td>
</tr>
</tbody>
</table>
<p>of SAT over the non-SAT baseline is consistent on subsets with different numbers of distractors.
Numbers of query words. We also examine the influence of query length on the grounding performance to better understand the model's performance in modeling language queries. Table 10 split Nr3D into five sub-sets that are roughly the same size based on query lengths. The results show that longer queries more challenging in general. Therefore, the grounding accuracy decreases on longer queries, e.g., from $54.8 \%$ to $41.8 \%$, when the query length increase from less than 6 words to more than 14 words. Overall, SAT consistently improves the non-SAT accuracy on different subsets.
Target's object class. Table 11 shows the performance on subsets with different target object classes. The upper part of the table shows the percentage of samples in each subset and the subsets' average number of points/distractors. The bottom part compared non-SAT with SAT on different subsets. Overall, we observe consistent improvements on subsets with different target object classes.
Query spatial relationships. As overviewed in the main paper's Section 6.3, training with Sr3D/Sr3D+ mainly benefits the queries with spatial relationship referring. We man-</p>
<p>Table 11: Grounding accuracy on Nr3D’s subsets with different target object classes.</p>
<table>
<thead>
<tr>
<th></th>
<th>Overall</th>
<th>chair</th>
<th>table</th>
<th>window</th>
<th>door</th>
<th>trash can</th>
<th>pillow</th>
<th>monitor</th>
<th>box</th>
<th>shelf</th>
<th>picture</th>
<th>cabinet</th>
</tr>
</thead>
<tbody>
<tr>
<td>Percent(%)</td>
<td>100.0</td>
<td>10.9</td>
<td>7.2</td>
<td>6.1</td>
<td>5.9</td>
<td>5.8</td>
<td>4.1</td>
<td>4.1</td>
<td>3.4</td>
<td>3.2</td>
<td>3.2</td>
<td>3.1</td>
</tr>
<tr>
<td>Avg. #points (K)</td>
<td>3.1</td>
<td>2.2</td>
<td>4.7</td>
<td>4.2</td>
<td>3.7</td>
<td>1.3</td>
<td>0.7</td>
<td>1.2</td>
<td>0.9</td>
<td>7.1</td>
<td>1.8</td>
<td>4.5</td>
</tr>
<tr>
<td>Avg. #distractors</td>
<td>3.0</td>
<td>3.6</td>
<td>3.5</td>
<td>2.6</td>
<td>2.6</td>
<td>2.9</td>
<td>3.6</td>
<td>3.9</td>
<td>3.1</td>
<td>2.6</td>
<td>3.0</td>
<td>2.7</td>
</tr>
<tr>
<td>non-SAT</td>
<td>37.6</td>
<td>30.7</td>
<td>43.2</td>
<td>39.2</td>
<td>34.7</td>
<td>40.0</td>
<td>41.7</td>
<td>38.8</td>
<td>33.5</td>
<td>38.8</td>
<td>44.7</td>
<td>38.3</td>
</tr>
<tr>
<td>SAT</td>
<td>49.0</td>
<td>45.9</td>
<td>52.0</td>
<td>54.9</td>
<td>44.6</td>
<td>53.5</td>
<td>51.1</td>
<td>54.7</td>
<td>40.6</td>
<td>42.1</td>
<td>54.0</td>
<td>53.0</td>
</tr>
</tbody>
</table>
<p>Table 12: Grounding accuracy on Nr3D’s subsets with different spatial referring keywords in language queries.</p>
<table>
<thead>
<tr>
<th></th>
<th>Overall</th>
<th>with spatial</th>
<th>w/o spatial</th>
<th>closest</th>
<th>next to</th>
<th>on the left</th>
<th>on the right</th>
<th>corner</th>
<th>fa(u)rthest</th>
</tr>
</thead>
<tbody>
<tr>
<td>Percent(%)</td>
<td>100.0</td>
<td>76.7</td>
<td>23.3</td>
<td>14.2</td>
<td>8.4</td>
<td>5.4</td>
<td>5.4</td>
<td>5.1</td>
<td>5.0</td>
</tr>
<tr>
<td>SAT</td>
<td>49.0</td>
<td>48.4</td>
<td>50.9</td>
<td>49.5</td>
<td>47.4</td>
<td>56.3</td>
<td>48.1</td>
<td>43.8</td>
<td>37.8</td>
</tr>
<tr>
<td>SAT w/ Sr3D+</td>
<td>56.4</td>
<td>56.8</td>
<td>54.8</td>
<td>61.0</td>
<td>56.1</td>
<td>62.6</td>
<td>58.0</td>
<td>53.8</td>
<td>51.3</td>
</tr>
</tbody>
</table>
<p>ually collect the spatial relationship keywords in Nr3D and show the grounding accuracy on each generated subsets. On the left part of Table 12, we show the overall performance on the subset with and without spatial relationship referring. We find $76.7\%$ of the queries contain at least one spatial relationship keyword, while the remaining samples do not use spatial referring. On the subset with spatial keywords, the extra Sr3D+ training data leads to an $8.4\%$ improvement on “SAT-Nr3D” from $48.4\%$ to $56.8\%$. In contrast, the improvement is only $3.9\%$ on the remaining samples. The right part of Table 12 compares the performance on subsets with specific spatial keywords. We observe larger improvements on the frequently appeared spatial keywords in Sr3D/Sr3D+. For example, the accuracy improves from $49.5\%$ to $61.0\%$ on the keyword “closest,” and from $37.8\%$ to $51.3\%$ on the keyword “farthest/furthest.”</p>
<h2>B. SAT with detector-generated proposals</h2>
<p>In the main paper, we focus on the ground truth proposal setting where we assume the access to $M$ groundtruth object point cloud segments as 3D proposals [1]. SAT is compatible with the setting that uses detector-generated proposals [4]. In this section, we present one implementation of extending SAT with detector-generated proposals. We benchmark our approach on the ScanRef dataset [4].</p>
<h3>B.1. Method</h3>
<p>We obtain $M$ 3D proposals and their feature $O$ with a 3D object detector [34]. It is computationally expensive to project the 3D proposals in each iteration to get the corresponded 2D semantics. Instead, we use the same method introduced in the main paper’s Section 3.2 to cache the ground truth 2D image semantics $I$. The object correspondence between detector-generated 3D proposals $O_{m}$ and groundtruth 2D semantics $I_{n}$ does not naturally exist as in the ground truth 3D proposal experiments. To get the 3D-2D object correspondence in the training stage, we compute the 3D IoU between the generated proposals $m$ and the ground truth boxes $n$ (corresponded to 2D semantics $I_{n}$) and pair 3D proposals with 2D semantics online by selecting the pair with the maximum IoU. We do not apply the object correspondence loss on the pairs with an IoU less than 0.5. With the IoU computation conducted online in each epoch, our implementation supports the end-to-end optimization of the entire framework.</p>
<h3>B.2. Experiment results</h3>
<p>Table 13 shows experiment results on the ScanRef dataset [4]. The upper part of the table contains methods that do not require extra 2D inputs in inference, and the bottom part includes methods that use 2D semantics in both training and inference. The “unique” subset contains samples that do not have distracting objects with the same object class as the target. The remaining samples belong to the “multiple” subset. We note that one previous study [56] simplifies the grounding problem by filtering out proposals that are not in the same object class as the target. We refer to such filtered 3D proposals as “(Filt.)” in the “proposals” column. Consequently, methods with filtered proposals show better performances on the “Unique” subset, which contains no distracting object in the same class as the target. The drawback is that the external object label information is required to perform such filtering.</p>
<p>We focus on the metrics in the “multiple” subset, which best indicates the models’ performance of 3D visual grounding [1]. We draw two major conclusions. 1) SAT significantly outperforms the non-SAT baseline by effectively utilizing the 2D semantics in the training stage (SAT: $37.64\%$ and $25.16\%$, non-SAT: $31.81\%$ and $21.34\%$ ). 2) SAT outperforms the state of the art [56, 17] by large margins (SAT: $37.64\%$ and $25.16\%$, InstanceRefer: $28.83\%$ and $22.92\%$ ). The significant improvements over the nonSAT baseline and the state of the art indicate the effectiveness of our approach in 3D visual grounding.</p>
<p>Furthermore, the state-of-the-art methods [17, 56] find it helpful to replace VoteNet with other proposal generation methods, such as 3D-UNet or PointGroup [18]. We discuss the influence of the proposal quality in Section B.3.</p>
<p>Table 13. 3D visual grounding accuracy on ScanRef [4] with detector-generated proposals. The upper part shows results that do not require extra input in inference, and the bottom part shows methods that use extra inputs. We highlight the best performance that does not use 2D inputs by bold. The "unique" subset contains samples with no distracting objects, and the remaining samples are in the "multiple" subset. "(Filt.)" in the "proposals" column indicates that 3D proposals are first filtered by the object class such that the model only needs to select from the proposals in the same class as the target. "(Filt.)" simplifies the grounding problem by using the external object label information.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Extra <br> 2D input</th>
<th style="text-align: center;">Proposals</th>
<th style="text-align: center;">Unique</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multiple</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc@0.25</td>
<td style="text-align: center;">Acc@0.5</td>
<td style="text-align: center;">Acc@0.25</td>
<td style="text-align: center;">Acc@0.5</td>
<td style="text-align: center;">Acc@0.25</td>
<td style="text-align: center;">Acc@0.5</td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">ScanRef [4]</td>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">VoteNet</td>
<td style="text-align: center;">60.54\%</td>
<td style="text-align: center;">39.19\%</td>
<td style="text-align: center;">26.95\%</td>
<td style="text-align: center;">16.69\%</td>
<td style="text-align: center;">33.47\%</td>
<td style="text-align: center;">21.06\%</td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
<td style="text-align: center;">IntanceRefer [56]</td>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">PointGroup (Filt.)</td>
<td style="text-align: center;">77.13\%</td>
<td style="text-align: center;">66.40\%</td>
<td style="text-align: center;">28.83\%</td>
<td style="text-align: center;">22.92\%</td>
<td style="text-align: center;">38.20\%</td>
<td style="text-align: center;">31.35\%</td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
<td style="text-align: center;">Non-SAT</td>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">VoteNet</td>
<td style="text-align: center;">68.48\%</td>
<td style="text-align: center;">47.38\%</td>
<td style="text-align: center;">31.81\%</td>
<td style="text-align: center;">21.34\%</td>
<td style="text-align: center;">38.92\%</td>
<td style="text-align: center;">26.40\%</td>
</tr>
<tr>
<td style="text-align: center;">(d)</td>
<td style="text-align: center;">SAT (Ours)</td>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">VoteNet</td>
<td style="text-align: center;">73.21\%</td>
<td style="text-align: center;">50.83\%</td>
<td style="text-align: center;">37.64\%</td>
<td style="text-align: center;">25.16\%</td>
<td style="text-align: center;">44.54\%</td>
<td style="text-align: center;">30.14\%</td>
</tr>
<tr>
<td style="text-align: center;">(e)</td>
<td style="text-align: center;">One-stage [51]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">29.32\%</td>
<td style="text-align: center;">22.82\%</td>
<td style="text-align: center;">18.72\%</td>
<td style="text-align: center;">6.49\%</td>
<td style="text-align: center;">20.38\%</td>
<td style="text-align: center;">9.04\%</td>
</tr>
<tr>
<td style="text-align: center;">(f)</td>
<td style="text-align: center;">ScanRef [4]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">VoteNet</td>
<td style="text-align: center;">63.04\%</td>
<td style="text-align: center;">39.95\%</td>
<td style="text-align: center;">28.91\%</td>
<td style="text-align: center;">18.17\%</td>
<td style="text-align: center;">35.53\%</td>
<td style="text-align: center;">22.39\%</td>
</tr>
<tr>
<td style="text-align: center;">(g)</td>
<td style="text-align: center;">TGNN [17]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3D-UNet</td>
<td style="text-align: center;">68.61\%</td>
<td style="text-align: center;">56.80\%</td>
<td style="text-align: center;">29.84\%</td>
<td style="text-align: center;">23.18\%</td>
<td style="text-align: center;">37.37\%</td>
<td style="text-align: center;">29.70\%</td>
</tr>
<tr>
<td style="text-align: center;">(h)</td>
<td style="text-align: center;">IntanceRefer [56]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">PointGroup (Filt.)</td>
<td style="text-align: center;">75.72\%</td>
<td style="text-align: center;">64.66\%</td>
<td style="text-align: center;">29.41\%</td>
<td style="text-align: center;">22.99\%</td>
<td style="text-align: center;">38.40\%</td>
<td style="text-align: center;">31.08\%</td>
</tr>
</tbody>
</table>
<h1>B.3. Discussion</h1>
<p>3D proposal quality. When experimented with the detector-generated 3D proposals, the final grounding accuracy is influenced by two factors, i.e., the quality of generated proposals and the main grounding objective of point-cloud-language modeling. We observe that the current 3D proposal quality is still somewhat limited. When using VoteNet [34] for proposal generation, ScanRef reports an oracle Acc@0.5 of $54.33 \%$, where the best proposal is selected as the final prediction. Because of the imperfect proposal quality, previous studies [17, 56] find it effective to boost the grounding accuracy by simply replacing proposal generation methods [17, 18].</p>
<p>Despite the large influence of proposal quality on grounding accuracy, we argue that the point-cloud-language joint representation learning is the core problem of 3D visual grounding. We expect the fast-growing 3D object detection studies to bring stronger detectors in the future, which alleviates the proposal quality problem. Therefore, in this study, we focus on the unique joint representation learning problem in 3D visual grounding, and evaluate methods with the metrics that best reflect the models' grounding performance. Specifically, we focus on "accuracy" when experimented with ground truth proposals, and the "multiple" accuracy when experimented with detector-generated proposals. For the former, SAT surpasses the state of the art by large margins on Nr3D ( $+10.4 \%$ in absolute accuracy) and Sr3D [1] ( $+9.9 \%$ ). Similarly on ScanRef, SAT-GT achieves an Acc@0.5 of $66.01 \%$, surpassing the state of the art by large margins (ScanRef-GT: $40.06 \%$, InstanceRefGT: $55.37 \%$ ). For the latter, SAT significantly outperforms the state of the art as shown in Table 13's "multiple" column. In summary, SAT effectively uses 2D semantics to assist 3D visual grounding and sets the new state of the art on multiple 3D visual grounding datasets.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2903 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2903</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2903</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268445573</p>
                <p><strong>Paper Title:</strong> LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot</p>
                <p><strong>Paper Abstract:</strong> This paper presents LaSofa, an avant-garde interactive sofa that revolutionizes the concept of human-furniture interaction by integrating fantasy storytelling with conventional furniture design. LaSofa is equipped with pressure sensors that recognize user interactions, which in turn trigger character dialogues and adventure narratives set in an embedded fantasy world. These narratives are dynamically generated by advanced large language models (LLMs) and are delivered through state-of-the-art audio technology to create an enveloping auditory experience. This integration not only embodies the convergence of technology and storytelling within the realm of furniture but also marks a pioneering venture into augmenting human experiences through interactive design. The paper elaborates on the genesis, architecture, and prospective influence of LaSofa in the domains of interactive storytelling and innovative furniture design.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2903.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2903.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIGHT agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agents in the LIGHT text-based fantasy adventure game</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-powered goal-driven agents trained with a combination of pre-training and reinforcement learning to act and communicate within the LIGHT fantasy text-adventure environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LIGHT goal-driven agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents in the LIGHT environment use LLMs to generate actions and dialogue; training combines pre-training and reinforcement learning to produce goal-directed behavior and improved in-game communication.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>LIGHT</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A text-based fantasy adventure environment where agents interact via natural language to perform actions, pursue goals, and communicate with other agents and entities; challenges include long action/state spaces, language-conditioned planning, and interactive dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2903.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2903.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive Simulacra</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system of LLM-driven agents that record experiences in natural language, synthesize those memories into higher-level reflections over time, and dynamically retrieve them to plan future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative Agents: Interactive Simulacra</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents record experiences as natural-language memory entries, periodically summarize/synthesize memories into higher-level reflections, and retrieve relevant memories at decision time to inform planning and behavior generation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / reflection-based (natural-language experience memory with synthesized higher-level reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory consists of natural language entries of agent experiences (episodic traces) that are stored verbatim; an additional synthesis layer periodically condenses related entries into higher-level reflective summaries that capture long-term knowledge and salient events; these stored entries and summaries are then used as context for future planning and behavior generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Dynamically retrieves relevant memories based on the current planning context (paper-level description only; retrieval mechanism specifics not detailed in this paper's text).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Natural-language records of agent experiences (observations, interactions), and synthesized higher-level reflections/summaries derived from those records.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Memory enables agents to plan future behaviors informed by past experiences by synthesizing episodic records into higher-level reflections and retrieving them when relevant, improving behavioral coherence and deliberation (high-level claim reported in related-work summary).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2903.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2903.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaSofa Outline Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LaSofa Planning/Outline memory used by the LaSofa storytelling system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An outline-based memory mechanism embedded in the LaSofa system that stores settings, character definitions, and concise outlines of past dialogues to maintain coherence and re-use context for future dialogue generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LaSofa storytelling agent (Planning / Dialogue / Plot Progression pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A system pipeline with Planning, Dialogue, and Plot Progression modules: GPT-4 generates settings, characters, and coded story outlines in the Planning module; the Dialogue module uses the Planning module as prompt context and, after each interaction, GPT-4 produces a brief outline of key points which is integrated back into the Planning module's Outline for future prompt reuse; Plot Progression advances stories when the same agent is triggered repeatedly.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Not a formal text-game benchmark; an interactive, situated storytelling environment where sensor-driven activations of character modules produce dialogues and evolving narrative arcs within an embedded fantasy world (sofa-as-continent metaphor).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>outline-based episodic/semantic memory (stored dialogue outlines and world/character metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Planning module maintains Settings (world layout), Characters (names, professions, personalities), and an Outline section that stores concise, coded story outlines and summaries of recent dialogues; after each generated dialogue GPT-4 creates an outline of key points which is appended/integrated into the Outline to be re-used as prompt context for future dialogue generation; a Plot Progression module triggers global story advancement when an agent is activated repeatedly.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Prompt-injection/contextual re-use: stored outlines and planning entries are re-inserted into GPT-4 prompts (Planning -> Dialogue) to provide retrieved context; retrieval appears to be driven by which agent/module is activated and the stored Outline entries relevant to that agent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>World settings (weather, terrain, beliefs, history), character definitions (names, occupations, personalities), concise coded story outlines for the world and individual characters, and brief outlines of past dialogues (key points) for re-prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>The outline memory supports dialogue continuity and coherence across interactions, reduces redundant token calls by reusing planning artifacts, helps avoid repetitive content via the Plot Progression trigger, and enhances character portrayal by keeping background/context available for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No quantitative evaluation reported; limitations not empirically analyzed in this paper (capacity, retrieval failure modes, long-term dependency handling not evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra <em>(Rating: 2)</em></li>
                <li>Language Models for Communication Games: An Empirical Study on Werewolf <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2903",
    "paper_id": "paper-268445573",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "LIGHT agents",
            "name_full": "Agents in the LIGHT text-based fantasy adventure game",
            "brief_description": "LLM-powered goal-driven agents trained with a combination of pre-training and reinforcement learning to act and communicate within the LIGHT fantasy text-adventure environment.",
            "citation_title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds",
            "mention_or_use": "mention",
            "agent_name": "LIGHT goal-driven agents",
            "agent_description": "Agents in the LIGHT environment use LLMs to generate actions and dialogue; training combines pre-training and reinforcement learning to produce goal-directed behavior and improved in-game communication.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "LIGHT",
            "text_game_description": "A text-based fantasy adventure environment where agents interact via natural language to perform actions, pursue goals, and communicate with other agents and entities; challenges include long action/state spaces, language-conditioned planning, and interactive dialogue.",
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2903.0",
            "source_info": {
                "paper_title": "LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive Simulacra",
            "brief_description": "A system of LLM-driven agents that record experiences in natural language, synthesize those memories into higher-level reflections over time, and dynamically retrieve them to plan future behavior.",
            "citation_title": "Generative Agents: Interactive Simulacra",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents",
            "agent_description": "Agents record experiences as natural-language memory entries, periodically summarize/synthesize memories into higher-level reflections, and retrieve relevant memories at decision time to inform planning and behavior generation.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "episodic / reflection-based (natural-language experience memory with synthesized higher-level reflections)",
            "memory_architecture": "Memory consists of natural language entries of agent experiences (episodic traces) that are stored verbatim; an additional synthesis layer periodically condenses related entries into higher-level reflective summaries that capture long-term knowledge and salient events; these stored entries and summaries are then used as context for future planning and behavior generation.",
            "memory_retrieval_mechanism": "Dynamically retrieves relevant memories based on the current planning context (paper-level description only; retrieval mechanism specifics not detailed in this paper's text).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Natural-language records of agent experiences (observations, interactions), and synthesized higher-level reflections/summaries derived from those records.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Memory enables agents to plan future behaviors informed by past experiences by synthesizing episodic records into higher-level reflections and retrieving them when relevant, improving behavioral coherence and deliberation (high-level claim reported in related-work summary).",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2903.1",
            "source_info": {
                "paper_title": "LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LaSofa Outline Memory",
            "name_full": "LaSofa Planning/Outline memory used by the LaSofa storytelling system",
            "brief_description": "An outline-based memory mechanism embedded in the LaSofa system that stores settings, character definitions, and concise outlines of past dialogues to maintain coherence and re-use context for future dialogue generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LaSofa storytelling agent (Planning / Dialogue / Plot Progression pipeline)",
            "agent_description": "A system pipeline with Planning, Dialogue, and Plot Progression modules: GPT-4 generates settings, characters, and coded story outlines in the Planning module; the Dialogue module uses the Planning module as prompt context and, after each interaction, GPT-4 produces a brief outline of key points which is integrated back into the Planning module's Outline for future prompt reuse; Plot Progression advances stories when the same agent is triggered repeatedly.",
            "base_llm_model": "GPT-4",
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": "Not a formal text-game benchmark; an interactive, situated storytelling environment where sensor-driven activations of character modules produce dialogues and evolving narrative arcs within an embedded fantasy world (sofa-as-continent metaphor).",
            "uses_memory": true,
            "memory_type": "outline-based episodic/semantic memory (stored dialogue outlines and world/character metadata)",
            "memory_architecture": "Planning module maintains Settings (world layout), Characters (names, professions, personalities), and an Outline section that stores concise, coded story outlines and summaries of recent dialogues; after each generated dialogue GPT-4 creates an outline of key points which is appended/integrated into the Outline to be re-used as prompt context for future dialogue generation; a Plot Progression module triggers global story advancement when an agent is activated repeatedly.",
            "memory_retrieval_mechanism": "Prompt-injection/contextual re-use: stored outlines and planning entries are re-inserted into GPT-4 prompts (Planning -&gt; Dialogue) to provide retrieved context; retrieval appears to be driven by which agent/module is activated and the stored Outline entries relevant to that agent.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "World settings (weather, terrain, beliefs, history), character definitions (names, occupations, personalities), concise coded story outlines for the world and individual characters, and brief outlines of past dialogues (key points) for re-prompting.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "The outline memory supports dialogue continuity and coherence across interactions, reduces redundant token calls by reusing planning artifacts, helps avoid repetitive content via the Plot Progression trigger, and enhances character portrayal by keeping background/context available for generation.",
            "memory_limitations": "No quantitative evaluation reported; limitations not empirically analyzed in this paper (capacity, retrieval failure modes, long-term dependency handling not evaluated).",
            "comparison_with_other_memory_types": null,
            "uuid": "e2903.2",
            "source_info": {
                "paper_title": "LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds",
            "rating": 2,
            "sanitized_title": "how_to_motivate_your_dragon_teaching_goaldriven_agents_to_speak_and_act_in_fantasy_worlds"
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra"
        },
        {
            "paper_title": "Language Models for Communication Games: An Empirical Study on Werewolf",
            "rating": 1,
            "sanitized_title": "language_models_for_communication_games_an_empirical_study_on_werewolf"
        }
    ],
    "cost": 0.008866249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot</p>
<p>Yu Meizhu Tongge 
Ya Chen 
Li 
Deehsiao Lew 
Kejin Yu </p>
<p>Tsinghua University</p>
<p>Tsinghua University Tsinghua University Haidian Qu
BeijingChina</p>
<p>Haidian Qu
BeijingChina</p>
<p>Haidian Qu
BeijingChina</p>
<p>Tsinghua University Tsinghua University Haidian Qu
BeijingChina</p>
<p>Haidian Qu
BeijingChina</p>
<p>LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot
8A20690D107FBD51C7868CB0861F663710.1145/3610978.3640672Human-Robot InteractionStorytelling TechnologySensory ExperienceLarge Language ModelsFurniture Innovation
This paper presents LaSofa, an avant-garde interactive sofa that revolutionizes the concept of human-furniture interaction by integrating fantasy storytelling with conventional furniture design.LaSofa is equipped with pressure sensors that recognize user interactions, which in turn trigger character dialogues and adventure narratives set in an embedded fantasy world.These narratives are dynamically generated by advanced large language models (LLMs) and are delivered through state-of-the-art audio technology to create an enveloping auditory experience.This integration not only embodies the convergence of technology and storytelling within the realm of furniture but also marks a pioneering venture into augmenting human experiences through interactive design.The paper elaborates on the genesis, architecture, and prospective infuence of LaSofa in the domains of interactive storytelling and innovative furniture design.CCS CONCEPTS• Human-centered computing → Human computer interaction (HCI); • Computer systems organization → Robotics; • Computing methodologies → Natural language processing.</p>
<p>INTRODUCTION</p>
<p>People inherently possess a fondness for fantasy worlds, driven by their innate curiosity and desire to explore [4].Fantasy worlds can provide resistance to reality and spiritual regeneration, proving to be particularly valuable in today's high-stress context [14].Sofas, a primary furniture piece for relaxation in everyday life, are often seen in current home design as functional items meant to enhance physical comfort and visual aesthetics [6,9].However, people's daily use of sofas for sleeping, dreaming, watching TV, reading, and chatting transforms them into a space for fantasy and social interaction.Our goal is to integrate the concept of the fantasy world with traditional sofas, thereby altering the experience humans interact with sofas.This approach aims to meet not only physical but also psychological needs, ofering a novel form of human-robot interaction, where sofas are not merely passive objects but active participants in creating immersive, interactive experiences.Addressing this need, our paper introduces LaSofa, a pioneering robotic sofa that seamlessly integrates fantasy storytelling into its functionality.LaSofa is equipped with pressure sensors and is powered by LLMs, enabling it to initiate interactive narratives and dialogues.This integration transforms the user experience, ofering an immersive auditory environment and redefning the sofa from a mere piece of furniture to an interactive storytelling platform.This research contributes to the feld of human-robot interaction by demonstrating how the incorporation of fantasy and storytelling can enhance the functionality and user experience of robotic furniture.</p>
<p>RELATED WORKS</p>
<p>The integration of fantasy elements into interactive technology spans various domains, including narratives, the latest LLMs technology, and robotics.This section provides a concise overview of the relevant literature and technological advancements that underpin the development of LaSofa.</p>
<p>Fantasy Narratives and Interactive Games</p>
<p>A substantial body of research has explored the role of fantasy in novels, comics, movies, and games [5].These studies emphasize how fantasy worlds immerse individuals, sparking curiosity and a desire for exploration.The allure of these worlds lies in their capacity to ofer an escape from reality and to inspire imaginative thought.</p>
<p>Role-playing games (RPGs) are a prime example of how interactive storytelling can engage users.Research [10] notes that RPGs cater to individuals seeking escapism and adventure, ofering an alternative to mundane experiences.These games create intricate worlds where players can explore diferent facets of their personalities and decision-making skills.</p>
<p>Large Language Models in Storytelling and Gaming</p>
<p>Signifcant past research has revealed that Large Language Models (LLMs), a form of deep learning algorithm, possess the potential to create immersive fantasy experiences, like in storytelling[13].</p>
<p>In the text-based fantasy adventure game LIGHT, LLMs are employed to train goal-driven agents.These agents, through a combination of pre-training and reinforcement learning, have demonstrated enhanced behavior and communication within the game environment [1].In the context of Generative Agents, LLMs utilize natural language to comprehensively record an agent's experiences, synthesize these memories into higher-level refections over time, and dynamically retrieve them to plan future behaviors [8].LLMs have also shown impressive performance in communication games like Werewolf[12].</p>
<p>Robotics and Interactive Furniture</p>
<p>While the integration of fantasy in games is well-documented, its application in the feld of robotics, particularly in terms of physical engagement, remains less explored.There are discussions on how robot interactions, involving physical and tactile elements, difer signifcantly from virtual interactions [3].The development of robotic furniture, such as the Lift-Bit sofa, has shown functional advancements but lacks in providing comprehensive interactive experiences [2].Other interactive furniture pieces in the market are exploring various ways to engage users.The "Sparkle Bench" by NunoErin, for example, features LEDs and sensors that change color upon touch, creating an interactive visual experience [7].These designs showcase a trend in furniture towards interactivity and emotional engagement, beyond just functionality.</p>
<p>LASOFA SYSTEM BREAKDOWN</p>
<p>Bridging these gaps, this paper introduces LaSofa, a novel approach to integrating fantasy storytelling with functional furniture.LaSofa is equipped with pressure sensors and is powered by LLMs, enabling it to initiate interactive narratives and dialogues.</p>
<p>Interaction Design</p>
<p>Our interaction consists of two parts: When the "Tail " is picked up and brought close to other agent modules on the sofa, the robot begins to vibrate slightly.This vibration, with a "buzzing" sound, is as if the robot is being awakened.When touching robots of diferent personalities on the backrest, LaSofa provides varied auditory feedback according to the diferent personality identities of the pillow robots.LaSofa integrates RFID 1 technology within its robot modules, strategically concealed in the sofa's "Tail."This sophisticated use of RFID, which enables triggering from a distance, allows users to seamlessly initiate dialogues between the sofa and other agent modules [11].This feature enhances user interaction, making the experience more intuitive and engaging.</p>
<p>When users lie on LaSofa and embrace the "Tail", the sofa back's most pressured module triggers a conversation with the Tail's agent, recounting their adventure stories.If a user's posture presses on multiple modules, LaSofa selects for activation those at the edges of the pressure range, typically under hands and feet in a stretched posture, with up to two modules engaging in separate dialogues with the "Tail".As a result, the agents activated on LaSofa vary with each user's unique lying position.With distinct personalities and professions, these agents are activated at diferent frequencies based on posture-driven interactions, leading to a unique narrative journey for every LaSofa.This design ofers users a range of interactive experiences, forging an immersive adventure.3.2 System Architecture Design 3.2.1 Worldview Revealed.We aim to immerse users in a fantastical, adventure-flled world through our Sofa Robots.In this narrative, the sofa transforms into a continent, with its small robot modules embodying diverse characters, each with unique stories.The Sofa Tail, acting as a protagonist, is central to most narratives, engaging with other characters that have rich social dynamics, including friendships, family ties, and feuds.This interactive storytelling is shaped by user interactions, leading to varied storylines.For instance, when a user interacts with diferent characters like Jack the Soldier 2 and Amy the Farmer 3 , the Large Language Model crafts unique stories, such as their joint quest at an Enchanted Forest.Users can explore diferent plot lines by maintaining positions or shifting interactions, like engaging with Joe the Merchant, ensuring a personalized and immersive experience with each session.</p>
<p>System Design.</p>
<p>We propose an interactive storytelling and dialogue generation system, consisting of Planning, Dialogue, and Plot Progression modules. 2 The main character, whose name, occupation, and personality are generated by GPT-4 3 Agent C, whose name, occupation, and personality are also generated by  The Planning module includes Settings, Characters, and Outline.Settings detail the fantasy world's layout, created using GPT-4 to generate weather, terrain, beliefs, and history.Key locations such as squares, bars, and homes are established as default dialogue orientations.The Characters section uses GPT-4 to develop 8 virtual characters, specifying their names, professions, and personalities, which shape their dialogue topics and tones.The Outline section employs GPT-4 to formulate concise, coded story outlines for the whole fantasy world and each character, laying the framework for dialogue trends.Upon user interaction, LaSofa activates when a user sits and picks up LaSofa's tail, signaled by an RFID sensor module, with the system picks up the value on each pressure sensor and decide which character should be activated.The Planning module is continuously reused to generate dialogue prompts in the Dialogue module.</p>
<p>The Dialogue module consists of Input, Dialogue, and Outline.When a user embraces LaSofa's tail and leans against a specifc module on the sofa's backrest, it activates dialogues between the represented agent and the protagonist, Jack.For instance, Amy could be triggered when the user lies in the middle.The Dialogue module then utilizes the Planning module for prompt input, generating dialogues that align with the story context, as well as the characters' personalities and tones, which are then played for the user.After the dialogue, GPT-4 formulates a brief outline of the key points, integrating them into the Planning module's Outline section for future reference and re-prompting.If the user shifts their position, activating a diferent agent, the module dynamically produces their dialogue.The Outline Section ensures coherence in each interaction, enhancing the narrative arc of each character within the broader story.</p>
<p>The fnal component, the Plot Progression module, activates when users trigger the same agent more than three times.This module prompts GPT-4 to advance plots and write the ending of the current events in the dialogue.The purpose of this module is to avoid repetitive content and user fatigue when the same agent is activated in short succession.</p>
<p>Our robot presents stories in a conversational format.Therefore, compared to similar storytelling algorithms[13], our research places a stronger emphasis on the quality of the generated dialogue.Our system framework provides a more optimized solution.In terms of efciency, it ensures both the continuity of content in each interaction and minimizes the need for token calls.Regarding the entertainment value of the stories, we delve deeper into character personalities and backgrounds to better portray each character's unique traits.Additionally, our Plot Progression module is a crucial design element aimed at enhancing the user experience.It helps prevent content repetition and ensures a fresh experience for users while listening to the story.</p>
<p>Through these three modules, along with the sofa's sensors and auditory system, the system crafts an immersive experience.This experience parallels a Lilliput-like world within the confnes of the sofa, thereby introducing an innovative and captivating interaction form of human-sofa interaction.</p>
<p>Characters</p>
<p>Portrait of Jack and others</p>
<p>Outline</p>
<p>Outline the main plot points.</p>
<p>Plot Progression</p>
<p>Lasofa turns off</p>
<p>Progress the storyline discussed in the current conversation and write the results of the current storyline.</p>
<p>Input</p>
<p>Plan module of Jack and agent C, relevant context</p>
<p>Dialogue</p>
<p>Speaker plays the generated dialogue.</p>
<p>Outline</p>
<p>Outline the main plot points of the dialogues.</p>
<p>Input</p>
<p>Plan module of Jack and agent E, relevant context</p>
<p>Dialogue</p>
<p>Speaker plays the generated dialogue.</p>
<p>Outline</p>
<p>Outline the main plot points of the dialogues.</p>
<p>…</p>
<p>Figure 4: System Design</p>
<p>FIELD STUDY</p>
<p>After developing the LaSofa prototype, we executed a pilot study and recruited fve participants to interact with LaSofa.Their reclining postures were recorded, and their experiences were subsequently interviewed.Observations on reclining postures indicated a natural preference among participants to embrace the "Tail", signifying this as a comfortable and accepted posture.Regarding module activation, the central agents-Agent E and Agent F-were most frequently activated in single-module scenarios, while the peripheral agents-Agent A, Agent C, Agent F, and Agent G-had higher activation in multimodule scenarios.These activation patterns provide insights for future enhancement of agent interrelationships.</p>
<p>Regarding user experience, the majority expressed fascination with the sofa's human-like conversation abilities.A minority desired ambient sounds alongside human voices.In response, we plan to integrate specifc sound efects into dialogues to enrich the immersive experience.Furthermore, we would like to introduce directional sound feld technology to create a spatial dimension in the audio experience, where the volume of voices changes with perceived distance, thus enhancing the sofa's ability to create a more spatially dynamic conversational environment.Looking ahead, we aim to expand LaSofa's human-machine interaction capabilities.Our focus will be on integrating multimodal interaction features, such as visual elements and gesture recognition, to enhance the immersive experience.Inspired by Interactive Drama Games, we also plan to introduce choice-based narrative elements, allowing users to infuence the story through an interactive tablet interface, further personalizing their experience.</p>
<p>We recognize the importance of understanding user experiences with LaSofa.Future studies will delve deeper into what aspects users fnd most appealing, whether it's the conversational abilities or other unique features of LaSofa.These insights will guide our ongoing development and refnement of the technology.</p>
<p>Additionally, we plan to integrate LaSofa into smart homes, enhancing its role as both an interactive storytelling device and an immersive audio platform, utilizing innovative sound feld localization technology for a richer auditory experience.</p>
<p>Figure 1 :
1
Figure 1: LaSofa robot prototype</p>
<p>Figure 2 :
2
Figure 2: Trigger mode When you lie down holding the pillow in the position shown in the left image, it simultaneously triggers "Tail" and agents C and F. The interaction among these three creates a story.If you change your posture to the one shown in the right image, it simultaneously triggers "Tail" and agents A and F, resulting in a diferent story through their mutual interaction.</p>
<p>Figure 3 :
3
Figure 3: Storytelling Example</p>
<p>Figure 5 :
5
Figure 5: Participants' Postures</p>
<p>Dialogue Jack and Joe talk around Plan Lasofa activated
User picking up the TailUser leaning on Agent C moduleUser leaning on Agent E module…User trigged the same agent×3DialogueSettingsWorld setting, weather, etc.Jack and Amy talk around
Radio Frequency Identifcation (RFID) refers to a wireless system comprised of two components: tags and readers.
Amy the FarmerAmyJack: "Amy, are you ready for this?The Enchanted Forest isn't for the faint of heart."Amy: "I know, Jack.After all, it's where we first met."Jack: "Understandable.But remember, the forest is full of mysteries and dangers.We need to stay alert.
Tim Rocktäschel, and Jason Weston. Prithviraj Ammanabrolu, Jakub Urbanek, Maithra Li, Arthur Szlam, arXiv:2010.00685How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds. 2020. 2020arXiv preprint</p>
<p>. Carlo Ratti, Associati , n.d.. Lift-Bit</p>
<p>Communication in Human-Robot Interaction. A Bonarini, 10.1007/s43154-020-00026-1Curr Robot Rep. 12020. 2020</p>
<p>Why imaginary worlds? The psychological foundations and cultural evolution of fctions with imaginary worlds. Edgar Dubourg, Nicolas Baumard, Behavioral and Brain Sciences. 45e2762022. 2022</p>
<p>Exploratory preferences explain the human fascination for imaginary worlds in fctional stories. E Dubourg, V Thouzeau, C De Dampierre, 10.1038/s41598-023-35151-2Sci Rep. 1386572023. 2023</p>
<p>Examining the Efects of the Industrial Revolution on Furniture. Şerif Tolga, Erdem , A+ Arch Design International Journal of Architecture and Design. 52019. 2019</p>
<p>Nunoerin, Sparkle Bench. </p>
<p>An introduction to RFID technology. Ji-Sung Park, John O' Brien, Chengyuan , Julian Cai, Meredith Ringel Morris, ; R Percy, Want, Michael S Liang, Bernstein, 10.1109/MPRV.2006.2Generative Agents: Interactive Simulacra. 5112006. 2023. 2006IEEE Pervasive Computing</p>
<p>Exploring Large Interface Software and Technology. 1-22. ; Y Of Human Behavior, S Xu, P Wang, F Li, X Luo, W Wang, Y Liu, Liu, Proceedings of the 36th Annual ACM Symposium on User. the 36th Annual ACM Symposium on User2023Language Models for Communication Games: An Empirical Study on Werewolf</p>
<p>Furniture design. Jerzy Smardzewski, arXiv:2309.046582015. 2023Springer6arXiv preprint</p>
<p>Re3: Generon their players: a discourse-based look at the evidence. A L Wadum, J Trier-Knudsen ; Kevin, Yuandong Yang, Nanyun Tian, Dan Peng, Klein, arXiv:2210.06774The Role-Playing ating longer stories with recursive reprompting and revision. arXiv preprint Society: Essays on the Popular Infuence of Role-Playing Games. 2015. 2022. 202213Psychological efects of fantasy games</p>
<p>. F Crocco, McFarland.in press</p>
<p>Why fantasy matters too much. Jack Zipes, The Journal of Aesthetic Education. 432009. 2009</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4231 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4231</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4231</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-274141118</p>
                <p><strong>Paper Title:</strong> ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity</p>
                <p><strong>Paper Abstract:</strong> Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a non-profit cloud-based auto finetuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science. The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction. The platform achieves remarkable accuracy with only a small amount of well-annotated articles. This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics. Demo Video</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4231.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4231.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ByteScience</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ByteScience (auto fine-tuned LLM extraction platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloud-based platform that auto-fine-tunes a domain-specific LLM (DARWIN) on small annotated corpora to convert unstructured scientific literature into structured data and synthesize new scientific knowledge at token granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ByteScience auto fine-tuning and extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A two-phase AWS-hosted pipeline: (1) Initial setup builds a domain-specific structured dataset by uploading documents, defining an annotation schema, randomly selecting samples for semi-automatic annotation (LLM auto-labelling) and human correction; (2) Fine-tuning the DARWIN LLM via SageMaker on the corrected, instruction-formatted dataset, deploying the fine-tuned model to a SageMaker endpoint to process new documents into structured JSON stored in MongoDB. The system emphasizes minimal annotated examples (claims as little as a single fully annotated paper or a small set like 10-20 samples for structure learning) and a human-in-the-loop review cycle to iteratively improve the model.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>90 samples (experiment); claims scalability to millions of papers (claimed by authors); cost example: $0.023 per paper for 10,000 articles</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural sciences broadly; experiments reported on materials-related domains (batteries, catalysis, photovoltaics) and a materials/alloy synthesis case study</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical relationships and design principles (e.g., composition-processing-structure-performance relationships), trends and structured factual knowledge suitable for knowledge graphs (entity/relationship extraction), and other empirical generalizations from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) Composition-Processing-Structure-Performance (CPSP) relationships for alloys: mappings from alloy composition plus processing parameters (casting, solution treatment, aging) to resultant microstructure and performance. 2) Trends in alloy synthesis extracted as structured datasets enabling downstream analysis and hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human-in-the-loop review and correction of auto-labels; quantitative evaluation using precision/recall/F1 for information extraction tasks; comparative experiments versus non-LLM baselines (e.g., MatBERT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported extraction accuracy of 80%–90% relative to human extraction; precision/recall/F1 values in the range 0.8–0.9 (reported for models with ≈300 samples in the text); annotation-time reduction of 57% when using 300 training samples; processing speed: ~1 second per 10-page document (claimed).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared experimentally to non-LLM approaches (e.g., MatBERT and other traditional NER/RE/ER pipelines); authors report ByteScience (fine-tuned LLM) outperformed these baselines across NER, relation extraction, and entity resolution with fewer samples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based, domain-adaptive fine-tuning can achieve high-quality structured extraction from scientific texts with relatively few annotated examples; LLMs handle unstructured, context-dependent scientific text more reliably than some traditional models; the platform enables rapid, low-cost large-scale extraction and synthesizing of knowledge (claimed). Human-in-the-loop correction and iterative fine-tuning are central to strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Domain-specific complexity and long-range contextual dependencies in scientific texts require careful schema design and annotation; low initial accuracy/recall may require more annotations or additional corpus data and sequential learning; the paper highlights the need for preprocessing (PDF->text, markup stripping) and does not provide detailed mitigations for LLM hallucination or deep mechanistic validation of synthesized 'laws'. Resource/cost trade-offs and continual retraining to adapt to evolving terminology are noted as practical considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4231.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4231.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DARWIN (domain-specific natural science LLM series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, domain-specific LLM family targeted at natural sciences that serves as the base model for ByteScience fine-tuning and extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DARWIN Series: Domain Specific Large Language Models for Natural Science.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tune DARWIN on instruction-formatted, human-corrected annotations</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>DARWIN is fine-tuned within ByteScience by converting corrected training datasets into the LLM's instruction format and performing SageMaker training jobs; after fine-tuning, the model is deployed to a SageMaker endpoint for large-scale extraction. The workflow uses LLM auto-labeling followed by human correction before fine-tuning to bootstrap performance from few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used in experiments on 90 annotated samples; proposed for large-scale use across millions of documents (claimed).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural science domains (materials science examples provided: batteries, catalysis, photovoltaics, alloy synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Structured factual relations and empirical patterns (entity labels, relations enabling CPSP-style relationships), suitable for downstream knowledge graph construction and empirical generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Same structured CPSP mapping examples as used by the ByteScience pipeline: extraction of composition, processing parameters, resultant structure, and performance entries from alloy literature to form empirical relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluation uses standard IE metrics (precision, recall, F1) on annotated test sets and human review; comparisons to non-LLM baselines are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported precision/recall/F1 in range 0.8–0.9 with ~300 samples (text reports similar figures for LLM fine-tuning scenarios); claims that 10–20 samples can often learn correct structure for certain models in preliminary tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against models like MatBERT and non-LLM pipelines; authors report DARWIN-based fine-tuning outperforms these baselines on extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DARWIN as a domain-specific backbone enables rapid adaptation with few annotated examples and improved handling of complex unstructured scientific text compared to some existing specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper does not disclose DARWIN model sizes or pretraining details in this text; empirical claims rely on relatively small-number experiments and human-corrected annotations, and the paper does not fully quantify failure modes such as hallucination or incorrect causal inference when synthesizing 'knowledge'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4231.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4231.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CPSP relationships</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Composition-Processing-Structure-Performance (CPSP) relationships</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical, multi-step relationships extracted from materials literature that map alloy composition and processing parameters to resulting microstructure and material performance, used as an example of qualitative laws/principles derived by ByteScience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DARWIN (fine-tuned via ByteScience)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Schema-driven extraction of CPSP relations via LLM-assisted annotation and fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Users define an annotation schema that explicitly includes CPSP labels; DARWIN auto-labels texts according to the schema, humans correct labels, and the corrected dataset is used to fine-tune the model so it can extract CPSP triplets (composition, processing parameters, structure, performance) from new papers and assemble them into structured records for downstream analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Demonstrated in a user case (Thomas) with an unspecified personal corpus; experimental extraction tests reported on 90 samples across related domains; authors claim capability to process large corpora quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Materials science, specifically alloy synthesis in the user case.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical design principles and multi-variable empirical relationships (i.e., mappings from composition + processing -> microstructure -> performance), which are qualitative/empirical generalizations rather than mechanistic first-principles laws.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Example (high-level, as reported): extracting that certain compositions combined with specific casting/aging protocols correlate with particular microstructures and improved mechanical/electrochemical performance; the system outputs structured CPSP records enabling trend discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human review/correction of auto-labelled annotations; downstream verification by researchers using the structured dataset (case-study narrative), and standard IE metrics reported for related extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report being able to reach ~80%–90% of human extraction accuracy for extraction tasks after a few hours of annotation; no separate numeric metric solely for CPSP extraction beyond these aggregate extraction figures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to manual curation (slower) and to non-LLM extraction systems; authors state the automated pipeline completes in days what manual curation takes months and outperforms traditional models in extraction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Schema-driven, LLM-assisted extraction can convert literature into structured CPSP datasets enabling rapid trend discovery and hypothesis generation in alloy synthesis; human-in-the-loop correction is essential for high quality.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper provides no detailed quantitative validation of the correctness of inferred CPSP 'laws' beyond extraction-quality metrics; potential oversimplification of nuanced scientific relationships and need for iterative retraining as terminology and methods evolve are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DARWIN Series: Domain Specific Large Language Models for Natural Science. <em>(Rating: 2)</em></li>
                <li>Structured information extraction from complex scientific text with fine-tuned large language models. <em>(Rating: 2)</em></li>
                <li>MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4231",
    "paper_id": "paper-274141118",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "ByteScience",
            "name_full": "ByteScience (auto fine-tuned LLM extraction platform)",
            "brief_description": "A cloud-based platform that auto-fine-tunes a domain-specific LLM (DARWIN) on small annotated corpora to convert unstructured scientific literature into structured data and synthesize new scientific knowledge at token granularity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DARWIN",
            "model_size": null,
            "method_name": "ByteScience auto fine-tuning and extraction pipeline",
            "method_description": "A two-phase AWS-hosted pipeline: (1) Initial setup builds a domain-specific structured dataset by uploading documents, defining an annotation schema, randomly selecting samples for semi-automatic annotation (LLM auto-labelling) and human correction; (2) Fine-tuning the DARWIN LLM via SageMaker on the corrected, instruction-formatted dataset, deploying the fine-tuned model to a SageMaker endpoint to process new documents into structured JSON stored in MongoDB. The system emphasizes minimal annotated examples (claims as little as a single fully annotated paper or a small set like 10-20 samples for structure learning) and a human-in-the-loop review cycle to iteratively improve the model.",
            "number_of_papers": "90 samples (experiment); claims scalability to millions of papers (claimed by authors); cost example: $0.023 per paper for 10,000 articles",
            "domain_or_field": "Natural sciences broadly; experiments reported on materials-related domains (batteries, catalysis, photovoltaics) and a materials/alloy synthesis case study",
            "type_of_laws_extracted": "Empirical relationships and design principles (e.g., composition-processing-structure-performance relationships), trends and structured factual knowledge suitable for knowledge graphs (entity/relationship extraction), and other empirical generalizations from literature.",
            "example_laws_extracted": "1) Composition-Processing-Structure-Performance (CPSP) relationships for alloys: mappings from alloy composition plus processing parameters (casting, solution treatment, aging) to resultant microstructure and performance. 2) Trends in alloy synthesis extracted as structured datasets enabling downstream analysis and hypothesis generation.",
            "evaluation_method": "Human-in-the-loop review and correction of auto-labels; quantitative evaluation using precision/recall/F1 for information extraction tasks; comparative experiments versus non-LLM baselines (e.g., MatBERT).",
            "performance_metrics": "Reported extraction accuracy of 80%–90% relative to human extraction; precision/recall/F1 values in the range 0.8–0.9 (reported for models with ≈300 samples in the text); annotation-time reduction of 57% when using 300 training samples; processing speed: ~1 second per 10-page document (claimed).",
            "comparison_baseline": "Compared experimentally to non-LLM approaches (e.g., MatBERT and other traditional NER/RE/ER pipelines); authors report ByteScience (fine-tuned LLM) outperformed these baselines across NER, relation extraction, and entity resolution with fewer samples.",
            "key_findings": "LLM-based, domain-adaptive fine-tuning can achieve high-quality structured extraction from scientific texts with relatively few annotated examples; LLMs handle unstructured, context-dependent scientific text more reliably than some traditional models; the platform enables rapid, low-cost large-scale extraction and synthesizing of knowledge (claimed). Human-in-the-loop correction and iterative fine-tuning are central to strong performance.",
            "challenges_limitations": "Domain-specific complexity and long-range contextual dependencies in scientific texts require careful schema design and annotation; low initial accuracy/recall may require more annotations or additional corpus data and sequential learning; the paper highlights the need for preprocessing (PDF-&gt;text, markup stripping) and does not provide detailed mitigations for LLM hallucination or deep mechanistic validation of synthesized 'laws'. Resource/cost trade-offs and continual retraining to adapt to evolving terminology are noted as practical considerations.",
            "uuid": "e4231.0",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DARWIN",
            "name_full": "DARWIN (domain-specific natural science LLM series)",
            "brief_description": "An open-source, domain-specific LLM family targeted at natural sciences that serves as the base model for ByteScience fine-tuning and extraction tasks.",
            "citation_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science.",
            "mention_or_use": "use",
            "model_name": "DARWIN",
            "model_size": null,
            "method_name": "Fine-tune DARWIN on instruction-formatted, human-corrected annotations",
            "method_description": "DARWIN is fine-tuned within ByteScience by converting corrected training datasets into the LLM's instruction format and performing SageMaker training jobs; after fine-tuning, the model is deployed to a SageMaker endpoint for large-scale extraction. The workflow uses LLM auto-labeling followed by human correction before fine-tuning to bootstrap performance from few examples.",
            "number_of_papers": "Used in experiments on 90 annotated samples; proposed for large-scale use across millions of documents (claimed).",
            "domain_or_field": "Natural science domains (materials science examples provided: batteries, catalysis, photovoltaics, alloy synthesis).",
            "type_of_laws_extracted": "Structured factual relations and empirical patterns (entity labels, relations enabling CPSP-style relationships), suitable for downstream knowledge graph construction and empirical generalizations.",
            "example_laws_extracted": "Same structured CPSP mapping examples as used by the ByteScience pipeline: extraction of composition, processing parameters, resultant structure, and performance entries from alloy literature to form empirical relationships.",
            "evaluation_method": "Evaluation uses standard IE metrics (precision, recall, F1) on annotated test sets and human review; comparisons to non-LLM baselines are reported.",
            "performance_metrics": "Reported precision/recall/F1 in range 0.8–0.9 with ~300 samples (text reports similar figures for LLM fine-tuning scenarios); claims that 10–20 samples can often learn correct structure for certain models in preliminary tests.",
            "comparison_baseline": "Compared against models like MatBERT and non-LLM pipelines; authors report DARWIN-based fine-tuning outperforms these baselines on extraction tasks.",
            "key_findings": "DARWIN as a domain-specific backbone enables rapid adaptation with few annotated examples and improved handling of complex unstructured scientific text compared to some existing specialized models.",
            "challenges_limitations": "Paper does not disclose DARWIN model sizes or pretraining details in this text; empirical claims rely on relatively small-number experiments and human-corrected annotations, and the paper does not fully quantify failure modes such as hallucination or incorrect causal inference when synthesizing 'knowledge'.",
            "uuid": "e4231.1",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CPSP relationships",
            "name_full": "Composition-Processing-Structure-Performance (CPSP) relationships",
            "brief_description": "Empirical, multi-step relationships extracted from materials literature that map alloy composition and processing parameters to resulting microstructure and material performance, used as an example of qualitative laws/principles derived by ByteScience.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DARWIN (fine-tuned via ByteScience)",
            "model_size": null,
            "method_name": "Schema-driven extraction of CPSP relations via LLM-assisted annotation and fine-tuning",
            "method_description": "Users define an annotation schema that explicitly includes CPSP labels; DARWIN auto-labels texts according to the schema, humans correct labels, and the corrected dataset is used to fine-tune the model so it can extract CPSP triplets (composition, processing parameters, structure, performance) from new papers and assemble them into structured records for downstream analysis.",
            "number_of_papers": "Demonstrated in a user case (Thomas) with an unspecified personal corpus; experimental extraction tests reported on 90 samples across related domains; authors claim capability to process large corpora quickly.",
            "domain_or_field": "Materials science, specifically alloy synthesis in the user case.",
            "type_of_laws_extracted": "Empirical design principles and multi-variable empirical relationships (i.e., mappings from composition + processing -&gt; microstructure -&gt; performance), which are qualitative/empirical generalizations rather than mechanistic first-principles laws.",
            "example_laws_extracted": "Example (high-level, as reported): extracting that certain compositions combined with specific casting/aging protocols correlate with particular microstructures and improved mechanical/electrochemical performance; the system outputs structured CPSP records enabling trend discovery.",
            "evaluation_method": "Human review/correction of auto-labelled annotations; downstream verification by researchers using the structured dataset (case-study narrative), and standard IE metrics reported for related extraction tasks.",
            "performance_metrics": "Authors report being able to reach ~80%–90% of human extraction accuracy for extraction tasks after a few hours of annotation; no separate numeric metric solely for CPSP extraction beyond these aggregate extraction figures.",
            "comparison_baseline": "Compared implicitly to manual curation (slower) and to non-LLM extraction systems; authors state the automated pipeline completes in days what manual curation takes months and outperforms traditional models in extraction quality.",
            "key_findings": "Schema-driven, LLM-assisted extraction can convert literature into structured CPSP datasets enabling rapid trend discovery and hypothesis generation in alloy synthesis; human-in-the-loop correction is essential for high quality.",
            "challenges_limitations": "Paper provides no detailed quantitative validation of the correctness of inferred CPSP 'laws' beyond extraction-quality metrics; potential oversimplification of nuanced scientific relationships and need for iterative retraining as terminology and methods evolve are noted.",
            "uuid": "e4231.2",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science.",
            "rating": 2,
            "sanitized_title": "darwin_series_domain_specific_large_language_models_for_natural_science"
        },
        {
            "paper_title": "Structured information extraction from complex scientific text with fine-tuned large language models.",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_complex_scientific_text_with_finetuned_large_language_models"
        },
        {
            "paper_title": "MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning.",
            "rating": 1,
            "sanitized_title": "matkg_the_largest_knowledge_graph_in_materials_science_entities_relations_and_link_prediction_through_graph_representation_learning"
        }
    ],
    "cost": 0.01054975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity</p>
<p>Tong Xie 
GreenDynamics
KensingtonAustralia</p>
<p>School of Photovoltaic and Renewable Energy Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Hanzhi Zhang hanzhizhang@my.unt.edu 
Dept. of Computer Science and Engineering
University of North Texas
DentonUnited States</p>
<p>Shaozhou Wang shaozhou@greendynamics.com.au 
GreenDynamics
KensingtonAustralia</p>
<p>Yuwei Wan 
Imran Razzak imran.razzak@unsw.edu.au 
GreenDynamics
KensingtonAustralia</p>
<p>School of Computer Science and Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Chunyu Kit ctckit@cityu.edu.hk 
Dept. of Linguistics and Translation
City University of Hong Kong
Hong KongChina</p>
<p>Wenjie Zhang wenjie.zhang@unsw.edu.au 
School of Computer Science and Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Bram Hoex b.hoex@unsw.edu.au 
School of Photovoltaic and Renewable Energy Engineering
University of New South Wales
KensingtonAustralia</p>
<p>ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity
04E441E4B0A441AF9CBD44781A64541910.1109/ICDMW65004.2024.00126componentformattingstylestylinginsert
Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information.However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information.To address this, we introduce ByteScience, a non-profit cloud-based auto finetuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora.The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science.The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction.The platform achieves remarkable accuracy with only a small amount of wellannotated articles.This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics.Demo Video</p>
<p>I. INTRODUCTION</p>
<p>AI has the potential to revolutionize scientific discovery (AI4Science [1]), but challenges remain.Scientific knowledge is scattered across documents, making it hard to fully leverage past research.LLMs offer a promising solution but require structured texts, including converting PDFs and generating fine-tuning examples for NLP tasks.While machine learning models are used in fields like drug discovery [2], protein design [3], and crystal structure generation [4], limited structured data hinders their effectiveness.Databases like Materials Project [5] and NOMAD [6] cover only a fraction of data, leaving much unstructured information untapped.This gap presents an opportunity for AI to accelerate discovery.Although converting documents to markup is well-studied, extracting complex relationships remains challenging but essential for building knowledge graphs and fine-tuning datasets.</p>
<p>• Contextual Dependency: Relationships in scientific texts often depend heavily on context that may span multiple sentences or sections.For instance, a material's properties might be discussed concerning its synthesis method, as described in paragraphs several pages before.Traditional methods like MatKG [7], which define relationships by entity co-occurrence, often miss the nuances of scientific knowledge.While useful, they risk oversimplifying complex relationships.Advanced techniques are needed to better capture this complexity for improved knowledge extraction in AI-driven scientific discovery.Therefore, we introduce ByteScience, a cloud-based platform featuring an auto-finetuned LLM to extract structured scientific data and synthesize new scientific knowledge from extensive scientific corpora.We conclude as follows:</p>
<p>1) Tailored with DARWIN [8], an open-source state-of-the-art nature-science LLM, to provide research focus utilization; 2) Zero-code user-friendly semi-automated annotation and processing for uploaded science documents; 3) A personalized and domain-specific auto fine-tuning LLM that requires only a single fully annotated piece of literature; 4) Time efficiency high-quality science data extraction from millions of papers for less than a second per article.</p>
<p>II. PLATFORM DESIGN</p>
<p>ByteScience is a robust, scalable cloud-based solution leveraging AWS Sagemaker.This architecture ensures high availability, scalability, and performance for processing large scientific documents.Figure 1 illustrates the extraction pipeline for custom model development and data extraction.The pipeline consists of two primary phases:</p>
<p>• Initial Setup (First-time use for a specific field):</p>
<p>Dataset Construction (Green Pipeline): This phase builds a domain-specific corpus of structured scientific data.LLM Fine-tuning (Blue Pipeline): The system fine-tunes a large language model on the constructed dataset to optimize performance for the target scientific domain.• Operational Phase: Once the initial setup is complete, users can directly utilize the fine-tuned LLM stored in AWS to efficiently generate structured datasets from new scientific documents in the same field.This two-phase approach allows ByteScience to quickly adapt to various scientific domains while maintaining high extraction accuracy.The cloud-based architecture enables seamless scaling and ensures users always have access to the latest fine-tuned models, streamlining the conversion of unstructured scientific literature into structured data.Key steps include: 1) Create Database: Users upload scientific documents in JSON, PDF, HTML, or XML formats.Non-JSON text is extracted and saved as JSON, with HTML/XML markup stripped, and PDF conversion done using PDFMiner [9].2) Define Structure: Users define annotation structures, including entity labels and relationships, using pre-built or custom templates.3) Random Selection: A small text subset is randomly selected for initial annotation on first use.4) Auto Labelling: The LLM applies automatic pre-labeling to the selected texts.</p>
<p>III. ARCHITECTURE OF AWS CLOUD-BASED SERVICES</p>
<p>ByteScience utilizes the robust, scalable infrastructure of Amazon Web Services (AWS) to efficiently handle user requests and data processing.Figure 2 shows the detailed architecture of our platform.</p>
<p>A. General Service(Green Pipeline) Infrastructure</p>
<p>The user interaction layer is built on a series of AWS services that ensure high availability, security, and performance:</p>
<p>• DNS Management: AWS Route 53 routes incoming user requests to the appropriate services within the architecture.• Database Services: Amazon Relational Database Service (RDS) supports complex queries and transactions essential for scientific data management.</p>
<p>B. LLM Service (Blue Pipeline) Architecture</p>
<p>The LLM fine-tuning capability is a core function, implemented through a sophisticated pipeline of AWS services:</p>
<p>C. Workflow Integration</p>
<p>ByteScience workflow seamlessly integrates these components: 1) Users interact with the system through Route 53 and the ALB for initial annotation tasks.</p>
<p>2) The DARWIN LLM processes annotated data on a Sage-Maker Endpoint.This architecture enables ByteScience to offer customized, high-performance language models tailored to specific scientific domains, facilitating accurate and efficient structured data extraction from unstructured scientific literature.</p>
<p>IV. STRUCTURED DATA EXTRACTION PERFORMANCE</p>
<p>LLMs significantly improve human-in-the-loop annotation.Using 300 training samples reduced annotation time by 57% compared to a single sample [10].In the GPT-3/Doping-English model, 10-20 samples were enough to learn the correct structure, with precision, recall, and F1 scores reaching 0.8-0.9 with around 300 samples.</p>
<p>In our experiment, we compared non-LLM and LLM methods for structured data extraction on 90 samples covering batteries, catalysis, and photovoltaics, alongside ByteScience's results.As shown in Table I, we evaluated Named Entity Recognition (NER), Relation Extraction (RE), and Entity Resolution (ER).While models like MatBERT performed well, they often produced irrelevant entities, lowering precision.In contrast, LLMs handled unstructured information more reliably, and our system outperformed traditional methods across all tasks with fewer samples.A. Initial Setup: Schema, Semi-Annotation, Model Fine-Tune Thomas configures ByteScience to meet his research needs by designing a custom annotation schema for alloy synthesis, annotating key details like compositions, casting parameters, solution treatment, and aging variables.ByteScience then initiates semi-automatic annotation, where the DARWIN LLM auto-labels papers from his corpus based on this schema.Thomas reviews and corrects the annotations to refine the model's understanding.Afterward, ByteScience fine-tunes the LLM using AWS SageMaker, optimizing it for alloy synthesis data extraction.The fine-tuned model is deployed to a SageMaker Endpoint for efficient, large-scale processing of complex scientific papers.</p>
<p>B. Data Generation: Document Upload, Endpoint Utilization, and Dataset Creation</p>
<p>With the fine-tuned model, Thomas uploads his entire corpus of scientific papers to ByteScience, which processes various formats for comprehensive coverage.He initiates large-scale data extraction via the SageMaker Endpoint, where the model extracts detailed information on alloy compositions, casting processes, solution treatments, and aging procedures.This automation accelerates his research, completing in days what would have taken months manually.The extracted data is structured and stored in MongoDB, allowing Thomas to easily query, analyze, and identify trends in alloy synthesis, uncovering insights that manual review might have missed.</p>
<p>C. Further Dataset Updates and Refinement</p>
<p>As Thomas advances in his research, he updates his dataset with ByteScience, uploading new papers and processing them through the fine-tuned model to continually enrich his dataset.When discrepancies or improvements are needed, he initiates a re-training cycle, reviewing and correcting a subset of the newly processed papers to further fine-tune the model.This iterative process ensures the model stays accurate and adapts to evolving terminologies or methods in alloy synthesis.Through this dynamic interaction, Thomas maintains an up-to-date, accurate dataset, enhancing his research and keeping him at the forefront of alloy synthesis advancements.</p>
<p>VI. SIGNIFICANCE TO SCIENCE</p>
<p>Constructing databases from scholarly literature is crucial for modern research, but traditional methods are timeconsuming and resource-intensive.ByteScience transforms this process by enabling users to create a customized data extraction tool in hours, achieving 80%-90% human accuracy.It can process a 10-page scientific document in one second, compared to the 20-30 minutes it takes a researcher.With an extraction cost of just $0.023 per paper for 10,000 articles, ByteScience makes large-scale data extraction affordable and accessible.Its versatility across scientific fields democratizes access to advanced data extraction, providing computational power equivalent to hundreds of annotators.This accelerates discovery, enhances research decision-making, and fosters innovation across disciplines.</p>
<p>VII. CONCLUSION</p>
<p>ByteScience is leveraging a powerful approach to handle unstructured text by fine-tuning DARWIN, a pre-trained natural science LLM, using a minimal set of annotated articles.Hosted on the AWS cloud, this platform automates the process of extracting structured data from scientific texts, presenting a zero-code solution that could significantly enhance efficiency in natural science research.The key advantage of ByteScience lies in its ability to train the DARWIN model with few annotations, making it exceptionally adaptive and efficient.This capability ensures that the extracted material data is high-quality and highly accurate.ByteScience exemplifies how cutting-edge technology can be harnessed to propel advancements in science, engineering, and research by integrating advanced NLP techniques with cloud computing.This initiative represents a substantial step forward in making vast scientific corpora more accessible and usable, highlighting the transformative potential of AI in scientific data processing.To optimize resource efficiency, we are developing a slicing version that fine-tunes a low-resource inference model using only partial data from extensive content.APPENDIX Figure 3 shows the label-defining function in our system.Users define the structure for annotations, including entity labels and their relationships (visualized by indent).For each label, there is a definition textbox for filling.</p>
<p>Figure 4 shows the annotation function.Users can use automatic pre-labelling to the selected texts.Different colors will visualize the auto-labeled annotations and users can review and correct them, ensuring accuracy and consistency.</p>
<p>Figure 5 shows the data extraction function on example paper.Users can create a customized data extraction tool that achieves 80%-90% of human extraction accuracy after just a few hours of annotation.</p>
<p>911</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Fig. 1 .
1
Fig.1.ByteScience Pipeline.The initial setup for a specific field involves constructing a domain-specific corpus of structured scientific data (Green Pipeline) and fine-tuning an LLM on this dataset to optimize performance for the target scientific domain (Blue Pipeline).Once this setup is complete, users can efficiently generate structured datasets from new scientific documents in the same field by utilizing the fine-tuned LLM stored in AWS.</p>
<p>5 )
5
Correction: Users review and correct the auto-labeled annotations, ensuring accuracy and consistency.6) Training: Corrected annotations are used to train or finetune an LLM, with training done via Amazon SageMaker.7) Fine-Tuned LLM: The training process results in a finetuned LLM customized for the specific annotation task.8) Structured Data Generation: The fine-tuned LLM processes new documents into structured data stored in Mon-goDB as JSON, allowing flexible use and efficient querying.After uploading the training dataset, it is transformed into the LLM's instruction format for fine-tuning.Users should first test a small text subset and assess accuracy and recall.If accuracy is low, add more annotated data and retrain.For low recall, generate more corpus data and use sequential learning to train a new model.</p>
<p>Fig. 2 .
2
Fig. 2. The architecture of ByteScience creates a structured database on AWS cloud with LLM.</p>
<p>3 )
3
The SageMaker Notebook is used for model development and improvement.4) Training datasets on S3 are used to fine-tune the model via SageMaker Training Jobs.5) The resulting model is deployed to a SageMaker Endpoint.6) Users can then perform data extraction tasks, processing requests by the fine-tuned LLM.</p>
<p>Fig. 3 .
3
Fig. 3. Screenshot of label setup.</p>
<p>Fig. 4 .
4
Fig. 4. Screenshot of labeling page.</p>
<p>Fig. 5 .
5
Fig. 5. Screenshot of extraction results of a paper.</p>
<p>TABLE I RESULT
I
OF STRUCTURED DATA EXTRACTION.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
V. BYTESCIENCE IN ACTION: A USER CASE STUDYTo showcase ByteScience's application, we present Thomas, a materials scientist automating alloy synthesis by analyzing literature to establish "Composition-Processing-Structure-Performance" (CPSP) relationships.He designs alloy compositions, develops processing methods, and predicts microstructures using data on casting, solution treatment, and aging.
R Stevens, V Taylor, J Nichols, A B Maccabe, K Yelick, D Brown, AI for Science: Report on the Department of Energy (DOE) Town Halls on Artificial Intelligence (AI) for Science. Argonne, IL (United StatesFeb. 2020Tech. Rep. ANL-20/17</p>
<p>Applications of machine learning in drug discovery and development. J Vamathevan, D Clark, P Czodrowski, I Dunham, E Ferran, G Lee, B Li, A Madabhushi, P Shah, M Spitzer, S Zhao, Nature Reviews Drug Discovery. 186Jun. 2019Nature Publishing Group</p>
<p>Machine learning for functional protein design | Nature Biotechnology. </p>
<p>Opinion Mining by Convolutional Neural Networks for Maximizing Discoverability of Nanomaterials. T Xie, Y Wan, H Wang, I Østrøm, S Wang, M He, R Deng, X Wu, C Grazian, C Kit, B Hoex, 10.1021/acs.jcim.3c00746Journal of Chemical Information and Modeling. 647Apr. 2024</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, Others, APL materials. 112013AIP Publishing</p>
<p>The NOMAD laboratory: from data sharing to artificial intelligence. C Draxl, M Scheffler, Journal of Physics: Materials. 23360012019IOP Publishing</p>
<p>MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning. V Venugopal, S Pai, E Olivetti, arXiv:2210.17340Oct. 2022cond-mat</p>
<p>DARWIN Series: Domain Specific Large Language Models for Natural Science. T Xie, Y Wan, W Huang, Z Yin, Y Liu, S Wang, Q Linghu, C Kit, C Grazian, W Zhang, Others, arXiv:2308.135652023arXiv preprint</p>
<p>Pdfminer: Python pdf parser and analyzer. Y Shinyama, Retrieved on. 112015</p>
<p>Structured information extraction from complex scientific text with fine-tuned large language models. A Dunn, J Dagdelen, N Walker, S Lee, A S Rosen, G Ceder, K Persson, A Jain, arXiv:2212.052382022</p>            </div>
        </div>

    </div>
</body>
</html>
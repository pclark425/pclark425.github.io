<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3435 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3435</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3435</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-268857317</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.01677v2.pdf" target="_blank">Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system’s completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system outperforms previous works by achieving state-of-the-art performances in complex scenarios while maintaining performances in simple scenarios. Besides, we observe that GFaiR is faithful to its reasoning process.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3435.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3435.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GFaiR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalizable and Faithful Reasoner (GFaiR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural, stepwise reasoning system that implements resolution refutation over natural-language theories by iteratively generating intermediate conclusions (via a T5-based composer) guided by XLNet-based selectors and verified with a validity contrastive-loss verifier to improve completeness and faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GFaiR (system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An ensemble reasoning system composed of: (1) a T5-based converter (negates hypotheses and maps NL statements into a Skolem-like NL form), (2) XLNet-based pre-selector and post-selector to pick premise pairs, (3) a T5-based knowledge composer that learns to perform resolution steps at the NL level, and (4) a verifier trained with a validity-contrastive loss to ensure selected premise pairs can produce valid intermediate conclusions. Iterative proof search is bounded by a maximum step N and uses set-of-support + linear-resolution heuristics during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Natural-language first-order logic reasoning (RuleTaker / Hard RuleTaker / Ruletaker-E / NLSAT (GRL, RCL))</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Determine truth value (True/False/Unknown) of a hypothesis relative to a natural-language theory whose semantics are given by an underlying FOL theory; tasks require first-order logical inference including negation, quantifiers (implicit/existential), and multi-step resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Introduce resolution refutation at the natural-language level plus (i) a converter to Skolem-like NL forms, (ii) selector modules to choose premise pairs, (iii) a generative knowledge composer (T5) that produces one-hop resolved conclusions, and (iv) a verifier trained with a validity-contrastive loss to prefer premise pairs that yield logically valid conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Ruletaker-3ext (in-domain) GFaiR reports high performance (entailment accuracy (EA) ≈ 98.1% and full accuracy (FA) ≈ 98.0%). On Hard Ruletaker GFaiR reports substantially better performance than prior stepwise methods (EA ≈ 68.5%, FA ≈ 67.5%). On Hard Ruletaker* GFaiR reports EA ≈ 73.9% and FA ≈ 71.7%. On natural-language satisfiability (GRL and RCL) GFaiR outperforms the T5 two-stage baseline across variable settings (examples shown in paper tables; e.g., GRL rows show GFaiR in the 90s% range for many settings).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Pretrained baselines (RoBERTa-large, T5-large) and chained LLM prompting (ChatGPT/gpt-3.5-turbo with instruct + chain-of-thought) perform worse on the Hard RuleTaker settings; prior stepwise inference systems (FaiRR, IBR, NLProofs) have comparable in-domain EA on easy RuleTaker but much lower EA/FA on Hard Ruletaker. (See Table 1 in paper for reported baseline numbers.)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>GFaiR matches or slightly trails best baselines on the biased/easy RuleTaker-3ext but shows large gains on debiased/harder datasets: e.g., GFaiR EA ≈ 68.5% on Hard RT versus much lower EA for earlier stepwise systems (reported in paper) and ChatGPT; GFaiR exhibits much smaller generalization drop when moving from depth 3→5 (≈1.6% drop in EA) compared to FaiRR (≈14.4%) and NLProofs (≈24.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Because GFaiR is a neural implementation of resolution refutation it has failure modes: (1) accidental contradictions can be produced in both the theory+H and theory+¬H sets leading to a heuristic tie-break (counting reasoning steps) instead of formal resolution outcomes; (2) search is bounded by a maximum step limit N — when reached the system returns Unknown; (3) the knowledge composer may generate invalid or empty outputs when selected premise pairs are unrelated (mitigated but not eliminated by the verifier); (4) training data for intermediate proof steps are extracted by symbolic provers (Prover9 / Stanford CS221 prover) and converted to NL, which can introduce noisy/redundant steps; (5) some reasoning instances have no valid premise pairs (no resolvable pair) and GFaiR halts and reports no contradiction; (6) faithfulness concerns remain for accidental/hallucinatory intermediate steps in a neural composer.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show (1) adding the validity-contrastive verifier to a forward-chaining system (FaiRR+) improves but stays below GFaiR, demonstrating the contribution of resolution refutation; (2) replacing GFaiR's verifier with an alternative (GFaiR-) reduces performance, showing the verifier design matters; (3) GFaiR shows far smaller EA drop when generalizing to depth-5 problems (≈1.6% drop) vs FaiRR/NLProofs (much larger drops), supporting that resolution refutation improves generalization/completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3435.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3435.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large (pretrained transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained masked-language-transformer (RoBERTa) finetuned on RuleTaker-style datasets and used as a baseline classifier for entailment labels and proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained RoBERTa transformer (large variant) fine-tuned on the NL Rule reasoning datasets for label prediction / proof tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker / Hard RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language first-order logic entailment (True/False/Unknown) and proof generation over synthetic rule sets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning a pretrained RoBERTa-large on the target RuleTaker training sets as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as a strong baseline on Ruletaker-3ext for entailment accuracy (paper lists high EA on easy dataset) but substantially lower full-accuracy (proof-correctness) on harder/debiased datasets; exact numbers are reported in Table 1 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>GFaiR shows similar EA on the easy RuleTaker-3ext but superior generalization and much better performance on Hard RuleTaker datasets compared to RoBERTa-large.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>While RoBERTa-large can achieve high label accuracy on biased/easy data, it fails to provide faithful, correct multi-step proofs and generalize to debiased/harder reasoning scenarios requiring multi-step first-order logic.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No dedicated ablation in paper for RoBERTa; it is used as a baseline to show that task-specific reasoning architectures (GFaiR) are needed for complex resolution-style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3435.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3435.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-large (baseline / component)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-large (Text-to-text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large encoder-decoder transformer used both as a baseline (fine-tuned on the task) and as components inside GFaiR (converter and knowledge composer) to generate negations, Skolem-like NL forms, and one-hop resolution conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large encoder-decoder pretrained transformer (T5) used in two roles: (1) as a baseline fine-tuned on RuleTaker-style tasks; (2) inside GFaiR as the converter and the knowledge composer (generative module) that learns resolution steps at NL level.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>large</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker / NLSAT / proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language FOL reasoning and generation of intermediate conclusions (proof steps) via text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on RuleTaker datasets (baseline); trained as a generative module to emulate resolution steps from pairs of NL premises in GFaiR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As a baseline (fine-tuned T5-large) it is reported among pretrained baselines evaluated; as GFaiR's composer it is central to the system's ability to generate valid resolved conclusions. Exact baseline numbers for T5-large are reported in the paper's baseline comparisons (see Table 1); inside GFaiR the T5 composer contributes to the high EA/FA reported for GFaiR.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>As the generative composer inside GFaiR, T5-large enables learning of NL-resolution steps; GFaiR outperforms the vanilla two-stage T5 baseline on NLSAT tasks (GRL/RCL) per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>The generative composer can produce invalid or irrelevant intermediate conclusions (hallucinations) when given unrelated premise pairs; this is mitigated but not eliminated by the validity-contrastive verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper ablations emphasize that verifier design and inclusion of resolution refutation (not just the composer) are necessary — replacing the verifier or removing resolution-refutation reduces performance even with the same generative backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3435.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3435.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose instruction-following LLM (gpt-3.5-turbo) evaluated with instruct + chain-of-thought prompting to produce label and stepwise NL proofs; used as a black-box baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large instruction-tuned autoregressive model (gpt-3.5-turbo) prompted with few-shot instruct+chain-of-thought examples to elicit stepwise reasoning and proofs; evaluated on subsets (~3000 examples) due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker / Hard RuleTaker (instruct + chain-of-thought evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language FOL entailment and proof-generation; chain-of-thought prompting is used to coax intermediate reasoning traces from the model.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Instruct + chain-of-thought few-shot prompting (4–5 shots) restricted to a strict output format so outputs can be automatically evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ChatGPT does not outperform task-specific models; reported entailment accuracy and full-accuracy are lower than specialized systems on these datasets (paper reports that ChatGPT's EA on some datasets is in the mid-50s% and that <10% of outputs were unparsable and excluded). See paper Table 1 for reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable; ChatGPT served as a baseline and was outperformed by task-specific models (including GFaiR) on hard/debiased reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Chain-of-thought outputs were sometimes unparsable (<10% of tested examples were excluded), ChatGPT often hallucinates intermediate steps and is less faithful, and its performance is weaker on complex synthetic FOL reasoning benchmarks compared to specialized systems.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper does not perform ablation on ChatGPT; it reports qualitative failure modes (hallucination, unparsable outputs) and lower EA/FA relative to GFaiR and some fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3435.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3435.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IBR (Iterative Backward Reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A backward-chaining stepwise inference model (from Qu et al., 2022) that iteratively reasons from hypothesis to required facts, adapted here as a baseline for RuleTaker-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interpretable proof generation via iterative backward reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IBR (backward-chaining model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural backward-chaining architecture that predicts the final answer and then generates a backward-style proof trace; adapted by the authors for the open-world RuleTaker variants (strategy prediction removed).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker / Hard RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language FOL entailment and proof generation via backward reasoning from hypotheses to supporting facts/rules.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Iterative backward reasoning architecture trained to predict the answer and then generate proof steps; adapted in this paper removing strategy prediction for open/infinite search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>IBR attains strong EA on the biased/easy RuleTaker-3ext (comparable to other stepwise baselines) and shows mixed performance on hard datasets: the paper notes IBR's EA on some hard datasets is relatively high because IBR first predicts the final answer and only then generates a proof; exact numbers are reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>GFaiR outperforms IBR on the hard/debiased datasets in terms of EA and FA due to completeness improvements from resolution refutation, although IBR can sometimes produce high EA by predicting answers before proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>IBR relies on backward-chaining heuristics that are incomplete under full FOL; it may produce an answer without a correct proof and struggle to generalize to deep resolution-style reasoning required by Hard RuleTaker and NLSAT tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper comments that IBR's approach (predict-then-prove) inflates EA but not FA; no specific IBR ablations are run by the authors beyond adaptation details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3435.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3435.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FaiRR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FaiRR (Faithful and Robust Deductive Reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A forward-chaining stepwise inference model that selects rules whose conditions are satisfied by facts and applies them to derive new conclusions; used as a baseline and as an ablation (FaiRR+ with verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fairr: Faithful and robust deductive reasoning over natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FaiRR (forward-chaining stepwise model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A stepwise forward-chaining architecture that decomposes each reasoning step into selecting rules, selecting facts, and performing the derivation; designed to improve faithfulness via intermediate steps but is incomplete for general FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker / Hard RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language FOL entailment and proof generation by forward application of rules over facts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Forward chaining stepwise inference with learned modules to select rules/facts and generate one-hop conclusions; in ablation experiments the authors add a validity-contrastive verifier to produce FaiRR+.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FaiRR performs competitively in the easy/bias RuleTaker setting but shows severe drops on Hard RuleTaker datasets due to incompleteness; in ablation FaiRR+ (with verifier) improves but remains below GFaiR on hard datasets (see Table 6 in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>GFaiR significantly outperforms (especially on hard/debiased datasets) showing that adding resolution refutation (complete under FOL) improves generalization versus forward-chaining-only systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Forward-chaining is incomplete (requires that all antecedents of a rule be provable from known facts), so it cannot derive conclusions that require refutation-style proofs; lacks completeness for full first-order reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation FaiRR+ (FaiRR + validity-contrastive verifier) shows some gains from better pair selection but still lags GFaiR, highlighting that the core contribution is the resolution-refutation paradigm rather than verifier alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3435.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3435.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLProofs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLProofs (verifier-guided proof search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verifier-guided partial-proof-graph search method for generating NL proofs; used as a stepwise baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating natural language proofs with verifier-guided search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLProofs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural proof-search system that builds partial proof graphs and uses a learned verifier to guide search for valid NL proof chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker / Hard RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>NL proof generation and entailment using verifier-guided search to construct proof graphs; intended to improve faithfulness of generated proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Verifier-guided search over partial proof graphs to produce stepwise proofs from NL theories to hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>NLProofs achieves good in-domain performance on easy RuleTaker but exhibits large drops in entailment accuracy and full-accuracy on the Hard RuleTaker/debiased datasets; GFaiR outperforms NLProofs on these harder datasets (see Table 1 and generalization analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>GFaiR provides better generalization and higher EA/FA on hard datasets via resolution refutation, whereas NLProofs (partial-graph search) is more limited in coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Like other stepwise methods, NLProofs inherits incompleteness relative to full resolution refutation and struggles on tasks requiring deeper, refutation-style multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper compares NLProofs in generalization experiments showing larger drops in EA when increasing reasoning depth than GFaiR, supporting that completeness via resolution refutation matters for strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fairr: Faithful and robust deductive reasoning over natural language <em>(Rating: 2)</em></li>
                <li>Interpretable proof generation via iterative backward reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Pushing the limits of rule reasoning in transformers through natural language satisfiability <em>(Rating: 2)</em></li>
                <li>Generating natural language proofs with verifier-guided search <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3435",
    "paper_id": "paper-268857317",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "GFaiR",
            "name_full": "Generalizable and Faithful Reasoner (GFaiR)",
            "brief_description": "A neural, stepwise reasoning system that implements resolution refutation over natural-language theories by iteratively generating intermediate conclusions (via a T5-based composer) guided by XLNet-based selectors and verified with a validity contrastive-loss verifier to improve completeness and faithfulness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GFaiR (system)",
            "model_description": "An ensemble reasoning system composed of: (1) a T5-based converter (negates hypotheses and maps NL statements into a Skolem-like NL form), (2) XLNet-based pre-selector and post-selector to pick premise pairs, (3) a T5-based knowledge composer that learns to perform resolution steps at the NL level, and (4) a verifier trained with a validity-contrastive loss to ensure selected premise pairs can produce valid intermediate conclusions. Iterative proof search is bounded by a maximum step N and uses set-of-support + linear-resolution heuristics during inference.",
            "model_size": null,
            "reasoning_task_name": "Natural-language first-order logic reasoning (RuleTaker / Hard RuleTaker / Ruletaker-E / NLSAT (GRL, RCL))",
            "reasoning_task_description": "Determine truth value (True/False/Unknown) of a hypothesis relative to a natural-language theory whose semantics are given by an underlying FOL theory; tasks require first-order logical inference including negation, quantifiers (implicit/existential), and multi-step resolution.",
            "method_or_intervention": "Introduce resolution refutation at the natural-language level plus (i) a converter to Skolem-like NL forms, (ii) selector modules to choose premise pairs, (iii) a generative knowledge composer (T5) that produces one-hop resolved conclusions, and (iv) a verifier trained with a validity-contrastive loss to prefer premise pairs that yield logically valid conclusions.",
            "performance": "On Ruletaker-3ext (in-domain) GFaiR reports high performance (entailment accuracy (EA) ≈ 98.1% and full accuracy (FA) ≈ 98.0%). On Hard Ruletaker GFaiR reports substantially better performance than prior stepwise methods (EA ≈ 68.5%, FA ≈ 67.5%). On Hard Ruletaker* GFaiR reports EA ≈ 73.9% and FA ≈ 71.7%. On natural-language satisfiability (GRL and RCL) GFaiR outperforms the T5 two-stage baseline across variable settings (examples shown in paper tables; e.g., GRL rows show GFaiR in the 90s% range for many settings).",
            "baseline_performance": "Pretrained baselines (RoBERTa-large, T5-large) and chained LLM prompting (ChatGPT/gpt-3.5-turbo with instruct + chain-of-thought) perform worse on the Hard RuleTaker settings; prior stepwise inference systems (FaiRR, IBR, NLProofs) have comparable in-domain EA on easy RuleTaker but much lower EA/FA on Hard Ruletaker. (See Table 1 in paper for reported baseline numbers.)",
            "improvement_over_baseline": "GFaiR matches or slightly trails best baselines on the biased/easy RuleTaker-3ext but shows large gains on debiased/harder datasets: e.g., GFaiR EA ≈ 68.5% on Hard RT versus much lower EA for earlier stepwise systems (reported in paper) and ChatGPT; GFaiR exhibits much smaller generalization drop when moving from depth 3→5 (≈1.6% drop in EA) compared to FaiRR (≈14.4%) and NLProofs (≈24.5%).",
            "limitations_or_failures": "Because GFaiR is a neural implementation of resolution refutation it has failure modes: (1) accidental contradictions can be produced in both the theory+H and theory+¬H sets leading to a heuristic tie-break (counting reasoning steps) instead of formal resolution outcomes; (2) search is bounded by a maximum step limit N — when reached the system returns Unknown; (3) the knowledge composer may generate invalid or empty outputs when selected premise pairs are unrelated (mitigated but not eliminated by the verifier); (4) training data for intermediate proof steps are extracted by symbolic provers (Prover9 / Stanford CS221 prover) and converted to NL, which can introduce noisy/redundant steps; (5) some reasoning instances have no valid premise pairs (no resolvable pair) and GFaiR halts and reports no contradiction; (6) faithfulness concerns remain for accidental/hallucinatory intermediate steps in a neural composer.",
            "ablation_or_analysis": "Ablations show (1) adding the validity-contrastive verifier to a forward-chaining system (FaiRR+) improves but stays below GFaiR, demonstrating the contribution of resolution refutation; (2) replacing GFaiR's verifier with an alternative (GFaiR-) reduces performance, showing the verifier design matters; (3) GFaiR shows far smaller EA drop when generalizing to depth-5 problems (≈1.6% drop) vs FaiRR/NLProofs (much larger drops), supporting that resolution refutation improves generalization/completeness.",
            "uuid": "e3435.0",
            "source_info": {
                "paper_title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RoBERTa-large (baseline)",
            "name_full": "RoBERTa-large (pretrained transformer)",
            "brief_description": "A large pretrained masked-language-transformer (RoBERTa) finetuned on RuleTaker-style datasets and used as a baseline classifier for entailment labels and proof generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Pretrained RoBERTa transformer (large variant) fine-tuned on the NL Rule reasoning datasets for label prediction / proof tasks.",
            "model_size": null,
            "reasoning_task_name": "RuleTaker / Hard RuleTaker",
            "reasoning_task_description": "Natural-language first-order logic entailment (True/False/Unknown) and proof generation over synthetic rule sets.",
            "method_or_intervention": "Fine-tuning a pretrained RoBERTa-large on the target RuleTaker training sets as a baseline.",
            "performance": "Reported as a strong baseline on Ruletaker-3ext for entailment accuracy (paper lists high EA on easy dataset) but substantially lower full-accuracy (proof-correctness) on harder/debiased datasets; exact numbers are reported in Table 1 of the paper.",
            "baseline_performance": null,
            "improvement_over_baseline": "GFaiR shows similar EA on the easy RuleTaker-3ext but superior generalization and much better performance on Hard RuleTaker datasets compared to RoBERTa-large.",
            "limitations_or_failures": "While RoBERTa-large can achieve high label accuracy on biased/easy data, it fails to provide faithful, correct multi-step proofs and generalize to debiased/harder reasoning scenarios requiring multi-step first-order logic.",
            "ablation_or_analysis": "No dedicated ablation in paper for RoBERTa; it is used as a baseline to show that task-specific reasoning architectures (GFaiR) are needed for complex resolution-style reasoning.",
            "uuid": "e3435.1",
            "source_info": {
                "paper_title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "T5-large (baseline / component)",
            "name_full": "T5-large (Text-to-text Transfer Transformer)",
            "brief_description": "A large encoder-decoder transformer used both as a baseline (fine-tuned on the task) and as components inside GFaiR (converter and knowledge composer) to generate negations, Skolem-like NL forms, and one-hop resolution conclusions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-large",
            "model_description": "A large encoder-decoder pretrained transformer (T5) used in two roles: (1) as a baseline fine-tuned on RuleTaker-style tasks; (2) inside GFaiR as the converter and the knowledge composer (generative module) that learns resolution steps at NL level.",
            "model_size": "large",
            "reasoning_task_name": "RuleTaker / NLSAT / proof generation",
            "reasoning_task_description": "Natural-language FOL reasoning and generation of intermediate conclusions (proof steps) via text generation.",
            "method_or_intervention": "Fine-tuning on RuleTaker datasets (baseline); trained as a generative module to emulate resolution steps from pairs of NL premises in GFaiR.",
            "performance": "As a baseline (fine-tuned T5-large) it is reported among pretrained baselines evaluated; as GFaiR's composer it is central to the system's ability to generate valid resolved conclusions. Exact baseline numbers for T5-large are reported in the paper's baseline comparisons (see Table 1); inside GFaiR the T5 composer contributes to the high EA/FA reported for GFaiR.",
            "baseline_performance": null,
            "improvement_over_baseline": "As the generative composer inside GFaiR, T5-large enables learning of NL-resolution steps; GFaiR outperforms the vanilla two-stage T5 baseline on NLSAT tasks (GRL/RCL) per the paper.",
            "limitations_or_failures": "The generative composer can produce invalid or irrelevant intermediate conclusions (hallucinations) when given unrelated premise pairs; this is mitigated but not eliminated by the validity-contrastive verifier.",
            "ablation_or_analysis": "Paper ablations emphasize that verifier design and inclusion of resolution refutation (not just the composer) are necessary — replacing the verifier or removing resolution-refutation reduces performance even with the same generative backbone.",
            "uuid": "e3435.2",
            "source_info": {
                "paper_title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChatGPT (gpt-3.5-turbo)",
            "name_full": "ChatGPT (gpt-3.5-turbo)",
            "brief_description": "A general-purpose instruction-following LLM (gpt-3.5-turbo) evaluated with instruct + chain-of-thought prompting to produce label and stepwise NL proofs; used as a black-box baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "A large instruction-tuned autoregressive model (gpt-3.5-turbo) prompted with few-shot instruct+chain-of-thought examples to elicit stepwise reasoning and proofs; evaluated on subsets (~3000 examples) due to cost.",
            "model_size": null,
            "reasoning_task_name": "RuleTaker / Hard RuleTaker (instruct + chain-of-thought evaluation)",
            "reasoning_task_description": "Natural-language FOL entailment and proof-generation; chain-of-thought prompting is used to coax intermediate reasoning traces from the model.",
            "method_or_intervention": "Instruct + chain-of-thought few-shot prompting (4–5 shots) restricted to a strict output format so outputs can be automatically evaluated.",
            "performance": "ChatGPT does not outperform task-specific models; reported entailment accuracy and full-accuracy are lower than specialized systems on these datasets (paper reports that ChatGPT's EA on some datasets is in the mid-50s% and that &lt;10% of outputs were unparsable and excluded). See paper Table 1 for reported numbers.",
            "baseline_performance": null,
            "improvement_over_baseline": "Not applicable; ChatGPT served as a baseline and was outperformed by task-specific models (including GFaiR) on hard/debiased reasoning tasks.",
            "limitations_or_failures": "Chain-of-thought outputs were sometimes unparsable (&lt;10% of tested examples were excluded), ChatGPT often hallucinates intermediate steps and is less faithful, and its performance is weaker on complex synthetic FOL reasoning benchmarks compared to specialized systems.",
            "ablation_or_analysis": "Paper does not perform ablation on ChatGPT; it reports qualitative failure modes (hallucination, unparsable outputs) and lower EA/FA relative to GFaiR and some fine-tuned models.",
            "uuid": "e3435.3",
            "source_info": {
                "paper_title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "IBR",
            "name_full": "IBR (Iterative Backward Reasoner)",
            "brief_description": "A backward-chaining stepwise inference model (from Qu et al., 2022) that iteratively reasons from hypothesis to required facts, adapted here as a baseline for RuleTaker-style tasks.",
            "citation_title": "Interpretable proof generation via iterative backward reasoning",
            "mention_or_use": "use",
            "model_name": "IBR (backward-chaining model)",
            "model_description": "A neural backward-chaining architecture that predicts the final answer and then generates a backward-style proof trace; adapted by the authors for the open-world RuleTaker variants (strategy prediction removed).",
            "model_size": null,
            "reasoning_task_name": "RuleTaker / Hard RuleTaker",
            "reasoning_task_description": "Natural-language FOL entailment and proof generation via backward reasoning from hypotheses to supporting facts/rules.",
            "method_or_intervention": "Iterative backward reasoning architecture trained to predict the answer and then generate proof steps; adapted in this paper removing strategy prediction for open/infinite search spaces.",
            "performance": "IBR attains strong EA on the biased/easy RuleTaker-3ext (comparable to other stepwise baselines) and shows mixed performance on hard datasets: the paper notes IBR's EA on some hard datasets is relatively high because IBR first predicts the final answer and only then generates a proof; exact numbers are reported in Table 1.",
            "baseline_performance": null,
            "improvement_over_baseline": "GFaiR outperforms IBR on the hard/debiased datasets in terms of EA and FA due to completeness improvements from resolution refutation, although IBR can sometimes produce high EA by predicting answers before proofs.",
            "limitations_or_failures": "IBR relies on backward-chaining heuristics that are incomplete under full FOL; it may produce an answer without a correct proof and struggle to generalize to deep resolution-style reasoning required by Hard RuleTaker and NLSAT tasks.",
            "ablation_or_analysis": "Paper comments that IBR's approach (predict-then-prove) inflates EA but not FA; no specific IBR ablations are run by the authors beyond adaptation details.",
            "uuid": "e3435.4",
            "source_info": {
                "paper_title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "FaiRR",
            "name_full": "FaiRR (Faithful and Robust Deductive Reasoner)",
            "brief_description": "A forward-chaining stepwise inference model that selects rules whose conditions are satisfied by facts and applies them to derive new conclusions; used as a baseline and as an ablation (FaiRR+ with verifier).",
            "citation_title": "Fairr: Faithful and robust deductive reasoning over natural language",
            "mention_or_use": "use",
            "model_name": "FaiRR (forward-chaining stepwise model)",
            "model_description": "A stepwise forward-chaining architecture that decomposes each reasoning step into selecting rules, selecting facts, and performing the derivation; designed to improve faithfulness via intermediate steps but is incomplete for general FOL.",
            "model_size": null,
            "reasoning_task_name": "RuleTaker / Hard RuleTaker",
            "reasoning_task_description": "Natural-language FOL entailment and proof generation by forward application of rules over facts.",
            "method_or_intervention": "Forward chaining stepwise inference with learned modules to select rules/facts and generate one-hop conclusions; in ablation experiments the authors add a validity-contrastive verifier to produce FaiRR+.",
            "performance": "FaiRR performs competitively in the easy/bias RuleTaker setting but shows severe drops on Hard RuleTaker datasets due to incompleteness; in ablation FaiRR+ (with verifier) improves but remains below GFaiR on hard datasets (see Table 6 in the paper).",
            "baseline_performance": null,
            "improvement_over_baseline": "GFaiR significantly outperforms (especially on hard/debiased datasets) showing that adding resolution refutation (complete under FOL) improves generalization versus forward-chaining-only systems.",
            "limitations_or_failures": "Forward-chaining is incomplete (requires that all antecedents of a rule be provable from known facts), so it cannot derive conclusions that require refutation-style proofs; lacks completeness for full first-order reasoning.",
            "ablation_or_analysis": "Ablation FaiRR+ (FaiRR + validity-contrastive verifier) shows some gains from better pair selection but still lags GFaiR, highlighting that the core contribution is the resolution-refutation paradigm rather than verifier alone.",
            "uuid": "e3435.5",
            "source_info": {
                "paper_title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "NLProofs",
            "name_full": "NLProofs (verifier-guided proof search)",
            "brief_description": "A verifier-guided partial-proof-graph search method for generating NL proofs; used as a stepwise baseline in comparisons.",
            "citation_title": "Generating natural language proofs with verifier-guided search",
            "mention_or_use": "use",
            "model_name": "NLProofs",
            "model_description": "A neural proof-search system that builds partial proof graphs and uses a learned verifier to guide search for valid NL proof chains.",
            "model_size": null,
            "reasoning_task_name": "RuleTaker / Hard RuleTaker",
            "reasoning_task_description": "NL proof generation and entailment using verifier-guided search to construct proof graphs; intended to improve faithfulness of generated proofs.",
            "method_or_intervention": "Verifier-guided search over partial proof graphs to produce stepwise proofs from NL theories to hypotheses.",
            "performance": "NLProofs achieves good in-domain performance on easy RuleTaker but exhibits large drops in entailment accuracy and full-accuracy on the Hard RuleTaker/debiased datasets; GFaiR outperforms NLProofs on these harder datasets (see Table 1 and generalization analysis).",
            "baseline_performance": null,
            "improvement_over_baseline": "GFaiR provides better generalization and higher EA/FA on hard datasets via resolution refutation, whereas NLProofs (partial-graph search) is more limited in coverage.",
            "limitations_or_failures": "Like other stepwise methods, NLProofs inherits incompleteness relative to full resolution refutation and struggles on tasks requiring deeper, refutation-style multi-step reasoning.",
            "ablation_or_analysis": "Paper compares NLProofs in generalization experiments showing larger drops in EA when increasing reasoning depth than GFaiR, supporting that completeness via resolution refutation matters for strict logical reasoning.",
            "uuid": "e3435.6",
            "source_info": {
                "paper_title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fairr: Faithful and robust deductive reasoning over natural language",
            "rating": 2,
            "sanitized_title": "fairr_faithful_and_robust_deductive_reasoning_over_natural_language"
        },
        {
            "paper_title": "Interpretable proof generation via iterative backward reasoning",
            "rating": 2,
            "sanitized_title": "interpretable_proof_generation_via_iterative_backward_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Pushing the limits of rule reasoning in transformers through natural language satisfiability",
            "rating": 2,
            "sanitized_title": "pushing_the_limits_of_rule_reasoning_in_transformers_through_natural_language_satisfiability"
        },
        {
            "paper_title": "Generating natural language proofs with verifier-guided search",
            "rating": 2,
            "sanitized_title": "generating_natural_language_proofs_with_verifierguided_search"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.018401999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation</p>
<p>Zhouhao Sun zhsun@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
China</p>
<p>Xiao Ding xding@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
China</p>
<p>Du Li duli@baai.ac.cn 
Beijing Academy of Artificial Intelligence
BeijingChina</p>
<p>Bibo Cai bbcai@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
China</p>
<p>Jinglong Gao jlgao@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
China</p>
<p>Ting Liu tliu@ir.hit.edu.cn 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
China</p>
<p>Qin Bing 
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
China</p>
<p>Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation
EC5BB915479E0801385C7C95B1857ED5logic reasoningresolution refutationcompletenessfaithfulnesslarge language models Rule1: Roundkind people are rough
Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks.However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language.This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue.As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability.To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation.Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation.Experimental results demonstrate that our system outperforms previous works by achieving state-of-the-art performances in complex scenarios while maintaining performances in simple scenarios.Besides, we observe that GFaiR is faithful to its reasoning process.</p>
<p>Introduction</p>
<p>The natural language-based logical reasoning task requires the model to understand the abstract logical relationships within statements expressed in natural language to deduce a conclusion.For example, as shown in Figure 1a, the task is to determine the value of the hypothesis (True, False, Unknown) based on a natural language theory (NL Theory) which consists of a set of rules and facts explicitly stated in natural language.This task is increasingly gaining attention (Sun et al., 2021;Kazemi et al., 2023), as it bridges the natural language with abstract logical thinking, which plays a pivotal role in complex problem-solving and cognitive reasoning.</p>
<p>Recently, transformer-based LLMs have achieved significant performance in various natural language reasoning tasks (Wei et al., 2022;Qiao et al., 2022).Theoretical analyses have also demonstrated that transformers have the potential to perform logical reasoning over formal theories (Schlegel et al., 2022;Zhang et al., 2023).However, it still remains challenging for the present LLMs (Pan et al., 2023;Kazemi et al., 2023), even for the State-of-the-Art models including ChatGPT (Bang et al., 2023).This is because of the hallucination problem (Golovneva et al., 2023;Ribeiro et al., 2023) Bob is kind.</p>
<p>¬𝐾𝑖𝑛𝑑(𝑥) 𝐾𝑖𝑛𝑑(𝐵𝑜𝑏)</p>
<p>Everyone is not rough.</p>
<p>Bob is not kind.</p>
<p>Figure 1: (a) Example of an NL Theory and a hypothesis with gold answers.Note that the meaning of these statements are not related to the common sense.(b) For hypothesis 1, the reasoning process using the method of resolution refutation is shown.</p>
<p>The process of refutation is reflected from 'hypothesis' to "Bob is kind" and the grey box represents the process of resolution at the natural language level.</p>
<p>results are not faithful to be trusted (Lyu et al., 2023;Creswell and Shanahan, 2022).Moreover, if regarding large language models as inference sys-arXiv:2404.01677v2[cs.AI] 3 Apr 2024</p>
<p>tems, hallucination will affect their completeness.A complete inference system means that all the hypotheses with determined labels can be inferred by applying valid reasoning rules contained in the inference system.However, the hallucination problem prevents the LLMs from correctly wielding reasoning rules to draw conclusions, thus leading to incompleteness inference systems.</p>
<p>To reduce hallucination and improve faithfulness for LLMs, previous works mainly enhance the reasoning process of LLMs by a stepwise inference paradigm.According to the direction of reasoning, these works can be divided into two groups.The forward chaining approach (Sanyal et al., 2022) starts from known rules to check if there exists any rule whose conditions are all satisfied by the given facts, if so, we apply the reasoning rule of forward chaining to derive a new conclusion, this procedure continues until no new conclusions can be drawn or the hypothesis is proved.The backward chaining approach (Qu et al., 2022) starts from the hypothesis and reasons in an opposite direction to derive a set of facts that need to be satisfied, then querying if these inferred facts overlap the known facts.By introducing intermediate steps, faithfulness can be improved.</p>
<p>However, the performance of these methods in complex logical reasoning scenarios is still unsatisfying.In some cases, their performance may be lower than using LLMs alone, or even lower than random guesses.This is caused by the inherent deficiency of these methods that the forward or backward reasoning method is incomplete.It means that there will be some hypotheses with determined values that are considered Unknown by the model.As a result, it can only accommodate relatively simple scenarios.Take forward chaining as an example, forward chaining is incomplete because it is capable of reasoning if and only if 'all the conditions of a certain rule can be proven to be true based on known facts' (condition 1).However, In the process of reasoning, there are some exceptional cases where forward chaining cannot reason.For the Hypothesis 1 in Figure 1a, forward chaining is unable to complete this type of reasoning since the condition of the rule "kind people" cannot be proven to be true by the facts.Hence, no conclusions can be drawn and hypothesis 1 will be considered Unknown.For the backward chaining, inference also cannot be made since hypothesis 1 "not kind" does not appear at the right hand of the rule.Hence the hypothesis will also be considered Unknown.</p>
<p>Inspired by the logical reasoning methods in the field of symbolic logic, we attempt to introduce a complete logical reasoning paradigm (under firstorder logic) resolution refutation (Russell, 2010) whose reasoning procedure is not constrained by the condition 1 to improve completeness, and propose a novel reasoning framework GFaiR.Figure 1b illustrates the reasoning process of our model.For hypothesis 1, by utilizing the reasoning rule of resolution, we can derive 'Everyone is not kind' step by step from the known information by performing resolution at the natural language level.Then by refutation, 'kind' appears in the known information so we can finally prove that hypothesis 1 is True.As a result, the combining of resolution refutation enables the model to handle more complex reasoning scenarios and enhances its generalization ability.Because the process of resolution refutation is complex, so we detail them in Section 2.</p>
<p>To combine resolution, we need to first select two theories and then utilize a reasoning model to perform resolution over them at the natural language level.However, the previous (Sanyal et al., 2022) transformers-based selection module only considers selecting which theories are more likely to infer the target hypothesis, without taking into account whether these two theories are logically related.This leads to scenarios where the selected theories are completely unrelated, which further causes the failure of resolution and the generation of invalid conclusions which may result in hallucinations.As a result, we use a validity contrastive loss-based verifier to distinguish valid conditions from illogical statements.This ensures that a valid conclusion can be drawn from the selected theories through logical reasoning, thereby providing guarantees for resolution and improving faithfulness by reducing hallucinations.</p>
<p>We validate our method on the widely adopted Ruletaker dataset and a more challenging Hard Ruletaker dataset, as well as the natural language satisfiability (NLSAT).Experimental results show that our approach is faithful to its reasoning process and has maintained in-domain inference accuracy, meanwhile demonstrating stronger zero-shot generalization performances1 .</p>
<p>Background</p>
<p>Natural Language Reasoning with First-Order Logic We follow the task definition proposed by Han et al. (2022).Given a hypothesis H and an NL Theory N LT (including a series of facts and rules expressed in natural language) without contradiction, the goal is to determine the value of H: True, False, or Unknown.Note that N LT and H are annotated with parallel FOL (first-order logic) Theory and FOL hypothesis, and the value is determined by the FOL reasoning result of the FOL Theory and FOL hypothesis.If the value is True or False, it is expected to give a reasoning process, which consists of a series of reasoning steps (p 1 , p 2 , ..., p n ), and each reasoning step p i includes selected rules or facts s i along with reasoning conclusion c i .Resolution Refutation Resolution refutation (Nawaz et al., 2019) is a commonly used and complete reasoning method under first-order logic, i.e. for a hypothesis whose label is True or False under the semantics of full FOL, applying the reasoning method of resolution refutation can infer the label of the hypothesis.Let F be the FOL formula set of the given premises, and Q be the hypothesis, then the process of proving that Q is True by resolution refutation is shown as follows: 1) Negate Q to get ¬Q, and merge it into the formula set F to get {F, ¬Q}.2) Transform {F, ¬Q} into a clause set in Skolem normal form.(Skolem standardization) 3) Apply resolution principle (Robinson, 1965) to resolve clauses in the clause set, each resolution step generates a resolved clause, which is then added to the clause set.This process is repeated iteratively.If an empty clause is obtained during the resolution step, it indicates a contradiction in the clause set and proves that Q is True.</p>
<p>The process of proving that Q is False is similar.Therefore, when dealing with our target task, we can determine the value of H by applying a reasoning model to the theory set T 1 composed of N LT and H and the theory set T 2 composed of N LT and ¬H at the same time, where the reasoning model implicitly performs resolution at the natural language level.If there is no contradiction in the theory set T 1 and there is a contradiction in the theory set T 2 , it proves that H is True.On the contrary, it proves that H is False.if there are no contradictions in two theory sets, H is Unknown.an example of resolution refutation reasoning procedure can be seen in Section 3.3.</p>
<p>Method</p>
<p>Overview</p>
<p>Although improved faithfulness compared with vanilla LLMs, existing stepwise inference methods based on forward or backward chaining are incomplete, which makes them unable to generalize to complex reasoning scenarios.</p>
<p>In this paper, we propose a novel reasoning framework GFaiR.As shown in Figure 2, GFaiR introduces resolution refutation to improve the completeness.</p>
<p>Specifically, GFaiR is composed of five modules: (1) A converter for augmenting the given NL Theory with the negated hypothesis and to convert the representations of natural language for resolution at the natural language level in the following reasoning process.(2) A pre-selector to select a theory for drawing intermediate conclusions.(3) A post-selector to select another theory by explicitly modeling the relationship between the theory selected by the pre-selector and the remaining ones.(4) A knowledge composer to generate a novel conclusion by applying the resolution rule at the natural language level.(5) A verifier to ensure that a valid conclusion can be drawn from the selected theories through logical reasoning, thereby providing guarantees for resolution and improving faithfulness.</p>
<p>In the following sections, we will first introduce the architecture of GFaiR, and then explain the inference and training procedure of GFaiR.</p>
<p>Architecture</p>
<p>Overall, GFaiR is an iterative model where the onehop intermediate conclusions are generated stepby-step.Our model is shown in Figure 2. Specifically, we have the following five modules: Converter Given the NL Theory and hypothesis, before directly performing reasoning, we first employ a T5-based converter to automatically convert the hypothesis into its negation form for refutation in the inference process (not reflected in Figure 2).Additionally, because our knowledge composer mimics the resolution step which cannot deal with existential quantifiers and some implicit logical relationships such as →, we need a step to convert the implicit logical relationships and existential quantifiers while retaining as much of the original text as possible.The convertor can also perform this step by imitating the Skolem standardization step in resolution refutation, which transforms NL Theory and hypothesis (or its negation form) into natural language representations similar to the Skolem normal form.As shown in Figure 2, the converter converts 'Round, kind people are rough' to 'Everyone is not kind or not round or rough'.The converted NL Theory and hypothesis will be taken as inputs for the following reasoning process.Pre-Selector (Pre-S) The pre-selector is an XLNET-based (Yang et al., 2019) classification model that takes the concatenated theories in the theory set as input (including intermediate conclusions, converted NL Theory and hypothesis), and selects a theory for generating new conclusions in the current iterative steps.Taking theory set T = {t 1 , t 2 , ..., t n } in Figure 2 as an example, we concatenate and separate them with the [SEP] token to form the input
[CLS] [t i [SEP ]] n ([ ] n de- notes continued concatenation).
The output is a one-dimensional vector denoted as u, which is obtained by classifying each [SEP] token embedding via a linear binary classification layer.During iteration, we select the theory in front of the [SEP] token corresponding to the maximum value in the vector u.The example in Figure 2</p>
<p>Knowledge Composer (KC)</p>
<p>The knowledge composer is a generative transformer T5 that can learn the resolution rule implicitly from data, and apply the learned resolution rule at the natural language level to generate a novel conclusion.As shown in Figure 2, the input is two theories selected by the pre-selector and post-selector (t n and t 1 ), and the output t n+1 is an intermediate conclusion expressed in natural language, which will be merged into the theory set.</p>
<p>Verifier The previous (Sanyal et al., 2022) transformers-based selection module is not accurate enough for resolution refutation, which leads to scenarios where the selected theories are unrelated.This causes the failure of resolution and further the generation of invalid conclusions which may result in hallucinations.As a result, we use a validity contrastive loss-based verifier to verify two theories selected by the pre-selector and postselector to ensure that a valid conclusion can be drawn from them through logical reasoning, thus providing guarantees for resolution and improving faithfulness by reducing hallucinations.The validity contrastive loss is shown in Figure 2:</p>
<p>To facilitate explanation, we establish the following definitions: A theory pair (t i , t j ) composed of two theories t i and t j is valid if and only if a valid conclusion can be drawn from them through logical reasoning.Because our knowledge composer emulates the resolution step at the natural language level to draw intermediate conclusions, the criterion for determining if the theory pair is valid lies in whether the FOL expressions corresponding to these two theories can be used for resolution.</p>
<p>We consider all the theory pairs composed of the theory selected by pre-selector and the remaining ones, and then devise validity contrastive loss by maximizing the cosine similarity of valid theory pairs (pink and blue in Figure 2) while minimizing the cosine similarity of invalid theory pairs (green and blue in the Figure 2).Please refer to 3.4 for the loss function.</p>
<p>When verifying the theory selected by the postselector t k , the verifier first calculates the cosine similarity between t k and the theory selected by the pre-selector t m .If the similarity score is above 0 (similarity score is within the range of -1 to 1), the theory pair is deemed valid and selected as input for the knowledge composer.Conversely, it is invalid and the post-selector will select a new theory for verification.This process continues until a theory is selected that can form a valid theory pair with t m .</p>
<p>Inference</p>
<p>During inference, the converter first converts the NL Theory and hypothesis into two theory sets represented in natural language similar to the skolem normal form.One of them consists of the NL Theory and hypothesis, the other consists of the NL Theory and the negation of the hypothesis.Then we apply our reasoning model (shown in Figure 2) to two theory sets separately to infer if there exists a contradiction, which determines the value of the hypothesis (referring to the background for more details).Since our model is a neural network model rather than a symbolic reasoning system, there are accidental conditions that contradiction exists in both theory sets.In such cases, we employ a heuristic approach to determine the value of the hypothesis (according to the number of reasoning steps).Below we will explain how to infer a contradiction in a theory set.</p>
<p>For a specific theory set T , the pre-selector first selects a theory t i .Then, under the guidance of the verifier, the post-selector selects a theory t j that can form a valid theory pair with t i .If it does not exist, stop and conclude that there are no contradictions in the theory set.Conversely, the knowledge composer composes two selected theories to generate a new conclusion.If the conclusion is an empty string (corresponding to the empty clause in the process of resolution refutation), it indicates that there is a contradiction in the theory set and stops the iteration.Otherwise, the newly generated conclusion is placed in the theory set T to participate in the following reasoning process.For the example in Figure 2, the reasoning model will first derive 't n+1 : Bob is not round or rough.' by resolving 't 1 : Everyone is not kind or not round or rough.' and 't n : Bob is kind.'.Then, by combining t n+1 and 't n−2 Everyone is not rough.',the model can derive 't n+2 : Bob is not round.'.Finially, we can derive an empty string from t n+2 and 't n−1 : Everyone is round.',which indicates a contradiction in the theory set and illustrated that the hypothesis is True.Due to the infinite search space of first-order logical reasoning, we design a maximum number of reasoning steps N. When it is reached, we assume that there are no contradictions in the theory set and stop iteration.There may be cases where no theories are selected to form a valid theory pair.For example, the theory set is: {Bob is kind.Bob is tall.Bob is happy.}.In this situation, we are unable to derive a valid conclusion based on any theory pair.Therefore, we cannot derive any valid conclusions, and we will halt the search and consider that there are no contradictions in this theory set.</p>
<p>Training</p>
<p>Each component of our model is trained separately.</p>
<p>The training data of the converter is every fact and rule in the NL Theory and hypothesis as well as their corresponding natural language representations similar to the skolem normal form (or the negation).The following mainly introduces the training methods of the other four modules.</p>
<p>From Background, we know that the resolution refutation process for proving a hypothesis is True or False involves proving a theory set is contradictory.And each step for proving a theory set T = {t 1 , ..., t n } is contradictory can be represented as (t i , t j , t k ), which means that the intermediate conclusion t k is generated based on t i and t j already existed in the theory set T (intermediate conclusions generated by previous reasoning steps have been merged into the theory set T ).Then, for a theory set T with contradiction and one of its reasoning steps {t i , t j , t k }, we can generate four training samples for training Pre-Selector, Post-Selector, and Knowledge Composer, respectively: P re-S Input = {T } ; P re-S Output = {t i , t j } P ost-S Input = {T, t i } ; P ost-S Output = {t j } P ost-S Input = {T, t j } ; P ost-S Output = {t i }
KC Input = {t i , t j } ; KC Output = {t k }
The generative knowledge composer can learn the resolution rule implicitly after training by language modeling loss.The pre-selector and post-selector are classification models, so that their output is converted to class labels instead of text.We use binary cross entropy loss to train these two modules.</p>
<p>To train the verifier, we utilize the output of XL-NET in post-selector, specifically the vector representation corresponding to the [SEP] token via a linear layer, as the vector representations of the theories for simplicity.So the verifier and postselector are trained jointly, with their loss function combined using a hyperparameter α.For the example in Figure 2, T = {t 1 , ..., t n } is the current theory set, V = {v 1 , ..., v n } is the corresponding vector representations, t n is the theory selected by the pre-selector.Assume that P = {p 1 , ..., p k } represents the indices of theories that can form a valid theory pair with t n , which are considered as positive examples, R = {r 1 , ..., r m } represents the indices of theories that cannot, which are considered as negative examples.The specific definition of the validity contrastive loss (VCE) is shown as follows, where the maximum similarity between positive examples is constrained to be 0.8 to prevent model collapse:
L vce = − 1 k k j=1 log exp(max(sim(v n , v pj ), 0.8)) m i=1 exp(sim(v n , v ri ))</p>
<p>Experiment Setup</p>
<p>Tasks and Datasets Following Richardson and Sabharwal (2022), we trained and evaluated on the easy Ruletaker-depth-3ext dataset (Tafjord et al., 2021) To train GFaiR, we first get each data's FOL representation and then employ a resolution refutation based FOL prover to automatically derive the intermediate reasoning process.Finally, we transform it from FOL representations into natural language representations by using natural language templates.More details can be seen in Appendix A. Baselines For the first task, we compare GFaiR with two kinds of methods:</p>
<p>(1) Pretrained Language Model Based Methods: We use Roberta-large (Liu et al., 2019), T5-large and ChatGPT (gpt-3.5-turbo)as baselines.For Roberta-large and T5-large, we finetune them on the Ruletaker-3ext and Hard Ruletaker** datasets.For ChatGPT, we use the method of instruct and chain-of-thought prompt to evaluate its performance.Due to cost reasons, we respectively tested 3000 pieces of data in three datasets.</p>
<p>(2) Stepwise Inference Methods: we mainly compare GFaiR with the model combined with forward chaining FaiRR (Sanyal et al., 2022) 2021), when the model predicts Unknown, no proof will be generated and we think the proof is right when the gold label is Unknown.Note that our method for evaluating the reasoning process is more flexible compared to the evaluation method proposed in previous work (Saha et al., 2020), which relies on precisely matching between the gold proof and the predicted proof.Instead, our evaluation method is able to take different reasoning paths into account.However, our method still will not evaluate incorrect reasoning processes as correct ones ensured by symbolized logical reasoning.</p>
<p>Experiment Results</p>
<p>Main results</p>
<p>To investigate different methods' in-domain performance on easy problems and zero-shot generaliza-  1, from which we can observe that:</p>
<p>(1) Compared to pretrained language model based methods (T5-large, Roberta-large, and Chat-GPT), we can find that stepwise inference methods are more faithful than ChatGPT from the difference between the value of EA and FA.</p>
<p>(2) Compared to stepwise inference methods IBR, FaiRR, and NLProofs, GFaiR shows comparable performance on the biased RuleTaker-3ext dataset, and significantly outperforms on two debiased hard datasets, which demonstrates stronger zero-shot generalization performance according to EA and FA.This suggests that by introducing resolution refutation to improve completeness, the stepwise inference methods can generalize to complex logical reasoning scenarios.In contrast, previous stepwise inference methods IBR, FaiRR, and NL-Proofs are incomplete and often classify hypotheses that can be inferred as True or False as Unknown, so they exhibit unsatisfied when dealing with complex logical reasoning scenarios.Hence GFaiR's ability to generalize to complex logical reasoning scenarios is better.</p>
<p>(3) From the difference between the value of EA and FA, we can observe that our model is faithful.Though this difference of FaiRR as well as GFaiR on two hard datasets is much smaller, their entailment accuracy is relatively low so there is no point in only considering their faithfulness.However, GFaiR both achieves higher entailment accuracy and maintains faithfulness by combining resolution refutation and introducing a validity contrastive loss-based verifier.</p>
<p>(4) ChatGPT does not outperform other models significantly and even performs worse than them.On the one hand, this reflects the difficulty of this task.On the other hand, this is because Chat-GPT is a general-purpose model.However, the performance of some relatively small task-specific models far exceeds ChatGPT, demonstrating the immense potential of transformers in mastering logical operation rules and the necessity of equipping the data-driven chatGPT with the logical rules for enhancing the performance on complex rule reasoning tasks such as math or coding.Note that the EA of ChatGPT on Hard Ruletaker is slightly higher than on Ruletaker-3ext, this is because the labels in Hard Ruletaker are only True or False and we exclude data that ChatGPT considers Unknown (less than 10%).Though this may overestimate the performance, it does not affect our conclusion.Additionally, the EA of IBR on hard datasets is much higher than FaiRR and equal to Roberta.This is because IBR first predicts the final answer and then gives a reasoning process, and only the reasoning process is derived by stepwise backward inference.</p>
<p>Generalization to Higher Depths</p>
<p>In this section, we experiment with a setting where models are trained on reasoning depths less than or equal to 3 and tested on Ruletaker-D5 which contains problems that require reasoning up to depth 5.The reasoning depth are defined based on the minimal reasoning depth using the forwardchaining reasoning method (Tafjord et al., 2021).But we use resolution refutation which is different from forward-chaining in principle and thus leads to different minimal reasoning depth for the same instance.However, in a statistical sense, data with higher reasoning depth for forward-chaining is generally higher for resolution refutation.So it can also be a reference to compare the generalization ability of different methods using the depth defined by previous work.</p>
<p>From Table 2, we can find that the performance drop of GFaiR is smaller with the increasing reasoning depth.For example, considering the performance drops between d = 3 to d = 5, GFaiR has 1.6% drop in entailment accuracy.In contrast, FaiRR and NLProofs drop 14.4% and 24.5% in entailment accuracy, respectively.This indicates that our model's ability to generalize to higher reasoning</p>
<p>In-domain Performance on Complex Reasoning Scenarios</p>
<p>To investigate the in-domain performance of different methods in complex reasoning scenarios, we evaluate different methods on Hard Ruletaker** dataset.Experimental results are shown in Table 3, from which we can find that compared to IBR, FaiRR, and NLProofs, GFaiR achieves better performance.Combining the experimental results in Table 1, we can conclude that GFaiR is more effective in handling complex logical reasoning scenarios by introducing resolution refutation.</p>
<p>Performance on Ruletaker-E</p>
<p>We also wish to see the performance on scenerios with implicitly expressed existence quantifiers.</p>
<p>To do this, we evaluate different method's performances on the Ruletaker-E dataset.Experimental results are shown in Table 3, from which we can find that compared to FaiRR, IBR, and NLProofs, GFaiR achieves better performance, which indicates that it is also effective in handling implicitly expressed existence quantifiers by combining resolution refutation.Additionally, the difference between EA and FA also indicates that our model is faithful in scenarios with implicitly expressed existence quantifiers.</p>
<p>Performance on Natural Language Satisfiability Task</p>
<p>We further evaluate GFaiR on natural language satisfiability (NLSAT) task, whose aim is to determine whether there is a contradiction in the given NL Theory.In this task, we do not need the process of refutation, so the converter only needs to convert the NL Theory into natural language representations similar to the Skolem normal form, and then directly use our reasoning model to infer whether there is a contradiction in the given NL Theory.Specially, there are two datasets available in this task, Grounded Rule Language (GRL) dataset and Relative Clause Fragment (RCL) dataset.These datasets are more challenging compared to Hard RuleTaker (Richardson and Sabharwal, 2022).This is because these datasets demand the model to reason only based on rules and the number of reasoning steps required to solve the problem significantly exceeds that of Hard Ruletaker.So we use these datasets to further investigate the performance of our approach in more complex reasoning scenarios.Because there are no facts available on these datasets and models designed with forward or backward chaining rely on facts during inference.Therefore, we cannot apply these models to such tasks.Instead, we compare GFaiR with the T5large two-stage fine-tuning method (Richardson and Sabharwal, 2022).</p>
<p>Experimental results are shown in Table 4 and  5, from which we can observe that GFaiR outperforms the baseline methods on both datasets.Consequently, GFaiR is capable of handling more complex reasoning scenarios by combining the stepwise inference method and resolution refutation.</p>
<p>Ablation Study</p>
<p>To respectively explore the effects of resolution refutation and validity contrastive loss-based verifier in our model, we consider the following ablations: 1) FaiRR+: add the validity contrastive loss-based verifier to the FaiRR model.So comparing FaiRR+ with GFaiR can show the impact of resolution refutation; 2) GFaiR-: replace the validity contrastive loss-based verifier with the verifier proposed by Yang et al. (2022) to check its impact.</p>
<p>Results on Ruletaker-3ext and Hard Ruletaker<em> datasets are given in table 6.From these results, we can know that even if adding a verifier to FaiRR, the performance on Hard Ruletaker</em> dataset is lower than GFaiR, which signifies the effectiveness of combining resolution refutation.Furthermore, we can know that GFaiR's performance is better than GFaiR-, which shows the effectiveness of the validity contrastive loss-based verifier.</p>
<p>Related Work</p>
<p>Natural Language Reasoning with First-Order Logic First-order logic has a wide range of cov-erage.For example, it includes the majority of reasoning situations in commonsense reasoning (Davis, 2017).Additionally, it can represent most problems in mathematics and domains such as Euclidean geometry, making it widely used in automated theorem provers (Nawaz et al., 2019).As a result, FOL reasoning ability is a fundamental reasoning ability (Davis, 2017) widely used in existing reasoning benchmarks.For example, LogiQA (Liu et al., 2021) and ReClor (Yu et al., 2020) are two benchmarks widely used in logical reasoning.However, Tian et al. (2021) points out that FOL reasoning ability is not disentangled from other reasoning abilities such as commonsense reasoning in these benchmarks.So even if a model performs poorly on these datasets, it can't be concluded that the model lacks the reasoning ability.Starting from Clark et al. (2021), there are a series of novel benchmarks which measure logical reasoning independently of other forms of reasoning.We focus on these benchmarks to check our model's FOL reasoning ability.Since our method focuses on first-order logic reasoning based on natural language, it can easily be adapted to other forms of natural language-based reasoning problems.Proof Generation One of our task's goals is to give a reasoning process, which is similar to the task of proof generation.Proof generation focuses on generating a reasoning chain from the given NL Theory to the conclusion, which aims at improving the model's interpretability (Rudin, 2019;Hase and Bansal, 2020).Recently, some works have been working on the problem of proof generation.Prover (Saha et al., 2020) trains a RoBERTabased model that predicts nodes and edges of the proof graph.ProofWriter (Tafjord et al., 2021) is a T5-based model that iteratively generates onehop conclusions and proofs from the NL Theory.FaiRR (Sanyal et al., 2022) further decomposes each reasoning step into selecting rules, selecting facts and reasoning based on selected rules and facts, which is similar to the reasoning process of forward reasoning.IBR (Qu et al., 2022) draws inspiration from backward reasoning and designs an iterative backward reasoning model.NU (Picco et al., 2021) also employs backward reasoning but it cannot generate a reasoning process.NLProofs (Yang et al., 2022) is also a stepwise reasoning method that using verifier-guided search.However, the validity contrastive loss-based verifier is more suitable for the reasoning scenerios of resolution.Another work MultiProver (Saha et al., 2021) aims at generating multiple proofs for a hypothesis.</p>
<p>Conclusion</p>
<p>In this paper, we propose GFaiR, a faithful and generalizable model capable of handling complex logical reasoning scenarios by introducing a validity contrastive loss-based verifier and resolution refutation.Experimental results also shows that GFaiR achieves better performance especially on Hard RuleTaker and Hard RuleTaker* datasets.</p>
<p>A. Dataset details</p>
<p>Due to the need for intermediate reasoning processes when training our model, we employed the FOL prover provided in the Stanford CS221 course page and Prover9 to automatically extract the reasoning process for these two types of tasks respectively.Below we describe our method to extract the reasoning process.Since the dataset is a synthetic dataset, regular expressions can be used to convert each data back to its corresponding FOL representation.Then we apply Prover9 or the FOL prover provided in the Stanford CS221 course page to each data and obtain its intermediate reasoning process of FOL representations.Finally, we transform the intermediate reasoning process from FOL representations into natural language representations by using natural language templates.Although this approach introduces some noise limited by the prover we used (redundant and excessively long reasoning steps), it does not hinder our model from achieving excellent generalization performance across all tasks.</p>
<p>Additionally, Richardson and Sabharwal (2022) illustrated that they found around 1% mismatched labels on the Ruletaker-3ext dataset.However, they only correct the train and dev set of the Ruletaker-3ext-sat dataset.As a result, we correct the test set of the Ruletaker-3ext dataset and the Ruletaker-D5 dataset for our experiments using the same method as Richardson and Sabharwal (2022).</p>
<p>B. Baselines details B.1. ChatGPT Baseline</p>
<p>In order to automatically evaluate the accuracy of the reasoning process generated by ChatGPT, we need to know that the intermediate conclusions are derived from which theories are selected from the theory set.Therefore, we use the form of instruct+chain-of-thought prompt to strictly restrict its output form, specifically, we use 4-shot for Hard Ruletaker and 5-shot for Ruletaker-3ext-sat and Hard Ruletaker* because there is no label Unknown in Hard Ruletaker and we need one more example for the condition Unknown when testing on other datasets.However, there will still be a small portion of data (less than 10%) that we cannot parse the output of ChatGPT, so we exclude this portion of data.We only tested 3000 pieces of data in three datasets respectively using gpt-3.5-turbodue to cost reasons.</p>
<p>B.2. IBR Baseline</p>
<p>Since IBR targets the problems in the Close World Assumption, we made simple modifications to adapt to our target task.Specifically, the QA prediction module still first predicts the answer but we remove the strategy prediction module along with the strategy loss.This is because the search space of our target tasks is infinite so we can not generate proof when the answer is Unknown and the strategy is always Proof.As a result, if the QA prediction module predicts Unknown, we stop and return the results.On the contrary, we will apply other modules in IBR to get the reasoning process.</p>
<p>During training, all the modules of IBR are trained together with three types of losses: parent prediction loss, child prediction loss, and QA prediction loss (strategic prediction loss has been removed).However, when the gold answer is Unknown, there is no parent prediction loss and child prediction loss, which will introduce some noise.As a result, we implement the QA prediction module apart from two other modules.</p>
<p>C. implementation details</p>
<p>To reduce the search space and improve the inference efficiency of our model, we combine two complete inference strategies specifically designed for resolution refutation when experimenting with the natural language reasoning with first-order logic task, including set of support strategy and linear resolution strategy.However, when experimenting with the natural language satisfiability task, we can not combine these inference strategies because the task is different.In addition, we use beam search with beam size 5 for RuleTaker-E and 2 for other datasets.</p>
<p>Previous work (Buss, 1998) has shown that using linear resolution strategy and set of support strategy together will not affect the completeness of resolution refutation under first order logic.Set of support strategy requires that at least one of the two clauses involved in each resolution step is the negation of the inference target (hypothesis or the negation of the hypothesis) or a descendant of the negation of the inference target.Linear resolution strategy requires that one of the two clauses involved in each resolution step (except the first step) is the clause derived from the previous resolution step.Combining these two strategies, we can know that one of the two clauses involved in the first step is the negation of the inference target (from the set of support strategy), and one of the two clauses involved in the other steps is the clause derived from the previous resolution step (from the linear resolution strategy).From these we can know that one of the two clauses involved in each resolution step is determined.So we can remove the xlnet-based pre-selector while regarding that the pre-selector always choose the negation of the inference target in the first step, and choose the clause derived from the previous resolution step in the other resolution steps.</p>
<p>, i.e., LLMs may hallucinate incorrect intermediate reasoning steps to draw the final conclusions.As a result, the inference</p>
<p>illustrates the selection theories  1 , …,     , </p>
<p>Figure 2 :
2
Figure2: Architecture of GFaiR.We mark the converter in orange, the selector (consisting of pre-selector and post-selector) in green, the verifier with its validity contrastive loss in yellow, and the knowledge composer in blue, respectively.</p>
<p>and the
ModelRuletaker-3ext Hard RTHard RT<em>EAFAEAFAEAFAT597.7-57.3 -57.5 -Roberta 98.9-59.6 -59.7 -ChatGPT 56.542.857.0 2.7 38.9 6.9IBR98.998.159.6 12.1 59.7 29.6FaiRR99.098.414.1 12.2 41.1 39.8NLProofs 99.399.214.3 13.8 41.8 41.4GFaiR98.198.068.5 67.5 73.9 71.7Table 1: Comparison of GFaiR with baselines whentrained on Ruletaker-3ext and tested on Ruletaker-3ext and two hard datasets. EA, FA, Hard RT, andHard RT</em> refer to entailment accuracy, full accuracy,Hard Ruletaker, and Hard Ruletaker* respectively.model combined with backward chaining IBR (Quet al., 2022). And we also compare GFaiR withNLProofs (Yang et al., 2022) which conducts proofsearch on partial proof graphs. More details canbe seen in Appendix B.
Evaluation protocolFollowing Qu et al. (2022), We consider two main aspects for evaluating the model's performance in our study: (1) Entailment accuracy (EA) measures how accurately the model is able to predict the label of the hypothesis.(2)Full accuracy (FA) measures how accurately the model can simultaneously predict the label and the valid proof (i.e. the reasoning process) of the hypothesis.For a reasoning process P = (p 1 , p 2 . ..p n ), it is valid if and only if every reasoning step p i is correct.A reasoning step p i includes selected rules or facts s i , along with reasoning conclusion c i .To check if p i is right, we use the FOL format expression of s i and c i , denoting as f s i and f c i .And we consider p i is right if f c i can be directly derived by f s i using a valid reasoning rule under FOL.FollowingTafjord et al. (</p>
<p>Table 4 :
4
Model performance on GRL dataset.
Model5var8var10var12varT595.587.882.380.9GFaiR95.591.390.189.4Model 16,21v 25,32v 35,48v 60,70vT588.287.482.977.4GFaiR93.692.491.791.3</p>
<p>Table 5 :
5
Model performance on RCL dataset.
depth is better.</p>
<p>Table 6 :
6
Results of ablation study.
ModelRuletaker-3ext Hard Ruletaker*EAFAEAFAFaiRR99.098.441.139.8FaiRR+ 98.498.341.541.4GFaiR-97.597.272.468.6GFaiR98.198.073.971.7
The source code of GFaiR has been made available at https://github.com/spirit-moon-fly/GFaiR.
AcknowledgmentsWe thank the anonymous reviewers for their constructive comments and gratefully acknowledge the National Natural Science Foundation of China (U22B2059, 62176079), and the Natural Science Foundation of Heilongjiang Province (Y02022F005).
Bibliographical References, Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>An introduction to proof theory. Handbook of proof theory. Samuel R Buss, 1998137</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Logical formalizations of commonsense reasoning: A survey. Ernest Davis, Journal of Artificial Intelligence Research. 592017</p>
<p>ROSCOE: A suite of metrics for scoring stepby-step reasoning. Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Evaluating explainable ai: Which algorithmic explanations help users predict model behavior?. Peter Hase, Mohit Bansal, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Lambada: Backward chaining for automated reasoning in natural language. Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Learning deductive reasoning from synthetic corpus based on formal logic. Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Moin Saqib Nawaz, Yi Malik, Meng Li, Sun, Lali, arXiv:1912.03028A survey on theorem provers in formal methods. 2019arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Neural unification for logic reasoning over natural language. Gabriele Picco, Thanh Lam Hoang, Marco Luca Sbodio, Vanessa Lopez, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, arXiv:2212.09597Reasoning with language model prompting: A survey. 2022arXiv preprint</p>
<p>Interpretable proof generation via iterative backward reasoning. Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, Ruifeng Xu, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022</p>
<p>STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANA-TION BENCHMARK. Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Yang Wang, George Karypis, Bing Xiang, Dan Roth, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Pushing the limits of rule reasoning in transformers through natural language satisfiability. Kyle Richardson, Ashish Sabharwal, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>A machine-oriented logic based on the resolution principle. John Alan, Robinson , Journal of the ACM (JACM). 1211965</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, Nature machine intelligence. 152019</p>
<p>Artificial intelligence a modern approach. Russell Stuart, 2010Pearson Education, Inc</p>
<p>Prover: Proof generation for interpretable reasoning over rules. Swarnadeep Saha, Sayan Ghosh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020Shashank Srivastava, and Mohit Bansal</p>
<ol>
<li>multiprover: Generating multiple proofs for improved interpretability in rule reasoning. Swarnadeep Saha, Prateek Yadav, Mohit Bansal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies</li>
</ol>
<p>Fairr: Faithful and robust deductive reasoning over natural language. Soumya Sanyal, Harman Singh, Xiang Ren, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Can transformers reason in fragments of natural language?. Viktor Schlegel, Kamen Pavlov, Ian Pratt-Hartmann, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Probabilistic graph reasoning for natural proof generation. Changzhi Sun, Xinbo Zhang, Jiangjie Chen, Chun Gan, Yuanbin Wu, Jiaze Chen, Hao Zhou, Lei Li, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Diagnosing the first-order logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Generating natural language proofs with verifierguided search. Kaiyu Yang, Jia Deng, Danqi Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in neural information processing systems. 2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-232023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1902 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1902</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1902</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-276094900</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.01218v2.pdf" target="_blank">Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1902.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1902.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AcTOL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Temporal Coherence Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language pretraining method that enforces temporal ordering (Vision-Language Ordering loss) and local continuity (Brownian-bridge regularizer) on video frame embeddings to produce ordered, continuous multi-modal representations for language-conditioned embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AcTOL (initialized from CLIP ResNet-50)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ResNet-50 visual backbone + CLIP transformer language encoder initialized from CLIP; encoders are further multimodally pretrained with two new losses: a Vision-Language Ordering (VLO) time-contrastive style loss and a local Brownian-bridge continuity regularizer; after pretraining encoders are frozen and a lightweight MLP policy is trained for downstream LC-IL.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal video-text pretraining (vision-language pretraining on human action videos with temporal contrastive + Brownian-bridge continuity)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on EPIC-KITCHEN-100: third-person/egocentric human action video clips with coarse textual annotations describing actions (verbs, objects, task-level instructions, human-object interactions and affordances); noisy, coarse annotations containing action verbs and object references but not low-level robot control signals.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (LC-IL / LCBC)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on real-world Unitree D1 6-DoF end-effector displacement + gripper control (continuous 6-DoF displacement vector + gripper state at 20 Hz) in a third-person office scene (tasks: pick cup, open [X] drawer, close [X] drawer) and in simulation on Franka Kitchen and Metaworld manipulation tasks (various household interactions, 9-DoF Franka in randomized kitchen scenes; Metaworld multi-task benchmarks). Policies are learned via behavior cloning from limited expert demonstrations; visual input from a camera plus proprioception.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Yes — explicitly studied. AcTOL's VLO loss enforces that vision-language similarity differences reflect frame temporal distance (ordering), and Brownian-bridge encourages smooth per-frame embeddings (continuity). Pretraining data (EPIC-KITCHEN) contains overlap in object types and action verbs (kitchen objects, open/close, pick, manipulate) relevant to downstream manipulation; paper argues improved alignment vs goal-reaching pretraining which assumes final-frame goals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Real robot: AcTOL success rates — pick cup: 50%, open [X] drawer: 80%, close [X] drawer: 90% (success rate over 10 trials, Table 2). Simulation (Franka Kitchen average across tasks): 5 demos: 42.60 ± 0.53% success; 15 demos: 61.80 ± 2.54%; 25 demos: 64.60 ± 0.57% (Table 1). Metaworld (average): 5 demos: 53.81 ± 3.89%; 15 demos: 74.13 ± 1.59%; 25 demos: 81.13 ± 1.59% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baselines include CLIP initialization, R3M, LIV, DecisionNCE and an ablation 'AcTOL w/o BB'. Example comparisons (Franka Kitchen, 5 demos): CLIP 11.67 ± 0.95% avg success; R3M 28.60 ± 1.39%; LIV 23.40 ± 0.78%; DecisionNCE 25.33 ± 1.30%; AcTOL w/o BB 32.80 ± 1.23%; AcTOL 42.60 ± 0.53% (Table 1). Real-robot baselines (Table 2): CLIP [0%,20%,30%], R3M [10%,40%,40%], LIV [20%,30%,50%], DecisionNCE [20%,40%,60%] for pick/open/close tasks respectively (AcTOL [50%,80%,90%]).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Yes — measured by using small demonstration set sizes (5, 15, 25) in simulation and 60 real demonstrations for real robot (authors note typical prior work uses ~100). Example: on Franka with 5 demos AcTOL (42.6%) outperforms DecisionNCE (25.3%) and CLIP (11.7%), representing ~1.7× and ~3.6× relative success rates respectively; AcTOL achieves large relative gains at low demo counts (Table 1 and Tables 7–12).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention-map analysis reported. The paper does not present attention visualizations or per-pixel attention that would show focus on affordances or object parts.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes — qualitative and quantitative evidence: t-SNE visualizations (Figure 5) show that AcTOL's visual trajectories across time are more temporally continuous and better clustered by action semantics than baselines; paper reports that this improved continuity stabilizes VL alignment and enables dense language-conditioned rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — multiple lines of evidence: (1) language-conditioned dense reward curves generated from pretrained AcTOL features accurately localize action boundaries and semantics in real robot videos (Figures 4, 9), including distinguishing consecutive/opposing actions; (2) real-robot task performance (correct drawer index grounding and pick-cup handle grounding) indicates the pretrained features ground instructions to visual locations/affordances; (3) robustness to linguistic perturbations (little drop in success rates under paraphrases) indicates stable language-perception grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No detailed multi-scale / layerwise analysis of low-level vs high-level feature benefits presented; the paper demonstrates that temporal continuity benefits 'video feature trajectories' and semantic alignment, but does not decompose gains into low- vs high-level visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer works best when pretraining video domains contain coherent temporal action structure and shared objects/actions with downstream tasks (EPIC-KITCHEN -> kitchen manipulation). Failure modes: repetitive/cyclic actions (dishwashing) may violate ordering assumption; domain gap between human hands and robot arms can reduce transfer; AcTOL is robust to linguistic paraphrase but benefits from temporal coherence in pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No explicit quantitative split reported comparing objects/actions seen vs unseen during pretraining; qualitative experiments include tasks like distinguishing colored cups and dice orientation where model could succeed, but no per-object seen/unseen numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot: primary evaluations are few-shot (5/15/25 demonstrations). AcTOL shows few-shot improvements (e.g., 5-demo Franka: 42.6% vs CLIP 11.7%). No strict zero-shot policy transfer (zero demonstrations) reported for control policies, although pretrained features were used to generate reward signals on videos in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Partial: encoders are frozen after pretraining and only the MLP policy is trained (design choice). Ablation 'AcTOL w/o BB' (removing Brownian-bridge term) shows reduced performance, demonstrating the importance of the continuity regularizer; no full per-layer probing or fine-grained layer ablations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — discussed qualitatively. The authors note AcTOL's ordering assumption may hurt for repetitive/cyclic actions and that distribution gaps (human videos -> robot visuals/kinematics) can negatively affect transfer; no numeric magnitude for negative transfer is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Indirect: CLIP (image-text pretraining) is used as a vanilla baseline (vision-language image-text pretraining) and is substantially weaker than AcTOL on these embodied tasks; pure vision-only (ImageNet-only) results are not reported. R3M (video-based representation pretrained on Ego4D) performs better than CLIP but worse than AcTOL in many low-data settings.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Yes — both conceptually and empirically: AcTOL enforces that semantic alignment evolves smoothly over time (VLO + BB). Empirically, Brownian-bridge loss (L_BB) converges faster than VLO and the resulting continuous embeddings lead to stable reward signals and improved early learning; reward curves show temporally localized peaks at action boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No rigorous dimensionality metrics (PCA intrinsic dimension) reported; only t-SNE visualizations and theoretical bounds on continuity/ordering are provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1902.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1902.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Transferable Visual Models from Natural Language Supervision (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large image-text contrastive model (ResNet-50 backbone in this work) used as initialization for vision and text encoders; serves as the vanilla baseline and initialization for AcTOL pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (ResNet-50 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image-text contrastive model with a ResNet-50 image encoder and a transformer text encoder trained on large-scale image-caption pairs to align visual and text embeddings in a shared space; provides frozen encoder initialization or baseline embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs (web-scale image-caption pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale image-caption pairs from web sources (contains object labels, scene descriptions, and some action verbs but not structured video temporal dynamics or dense human-object interaction sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (baseline evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same downstream LC-IL tasks described above (Unitree D1 real robot pick/open/close tasks and Franka Kitchen / Metaworld simulation); CLIP used as a frozen encoder baseline feeding into MLP policy.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Partial overlap: CLIP encodes object and some action semantics from image-text pairs but lacks temporal action progression cues; paper demonstrates CLIP baseline performs poorly on few-demo embodied tasks relative to video-based pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>As baseline: Franka Kitchen (5 demos) average success 11.67 ± 0.95%; 15 demos 27.47 ± 1.01%; 25 demos 31.20 ± 2.62% (Table 1). Metaworld values also reported (e.g., 5 demos ~42.29% on some Metaworld tasks as per Table 1 aggregate columns). Real robot: pick/open/close reported approximately [0%, 20%, 30%] respectively (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable here (CLIP itself is a pretrained VL model); the paper does not report a purely random-initialization vision-only control baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>CLIP shows much lower sample efficiency in these embodied tasks — with 5 demos CLIP achieves ~11.7% avg success on Franka versus AcTOL's 42.6%; no explicit sample-efficiency curve for CLIP beyond tabulated demo counts.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map analysis for CLIP within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>CLIP embeddings are used as a starting point; t-SNE results indicate CLIP retains discriminative power but exhibits less temporal continuity than AcTOL (Figure 5 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Minimal — CLIP baseline underperforms on tasks requiring temporal grounding and precise action localization (e.g., identifying handle or drawer index).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No layerwise analysis of CLIP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>CLIP transfers poorly when temporal structure and fine-grained action progression are important; better for static object recognition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly analyzed for CLIP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot: CLIP baseline evaluated with same small-demo counts; no zero-shot control reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>CLIP encoders used as initialization and frozen for downstream policy; no ablation beyond that.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>CLIP performs worse than video-based pretraining methods in low-demo embodied tasks, indicating limited transfer for temporally structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP is a vision-language model (image-text); direct comparison to vision-only ImageNet models not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not explicitly captured by CLIP since CLIP is image-based.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality analysis for CLIP in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1902.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1902.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that learns universal visual representations for robot manipulation by leveraging video pretraining and language/reward shaping; used as a competitive baseline (pretrained on Ego4D).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R3M: A universal visual representation for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Video-pretrained visual representation intended for robotic manipulation, combining visual encoders with task-aware objectives and optionally language/reward shaping; in this paper used as a pretrained baseline (weights from prior R3M trained on Ego4D).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>video-based pretraining (multimodal / video representation pretraining, R3M trained on Ego4D in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Ego4D human egocentric video dataset (very large-scale egocentric videos of human activities) with dense human-object interactions, actions and affordances; contains many action verbs and object interactions relevant to robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same real and simulated manipulation tasks (Unitree D1 real robot tasks and Franka Kitchen / Metaworld), evaluated as a transfer baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>R3M pretraining on Ego4D provides better temporal and action-alignment priors than static image-text models, improving performance relative to CLIP, though it does not use AcTOL's explicit ordering+continuity losses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Franka Kitchen (5 demos): ~28.60 ± 1.39% average success (Table 1). Real robot: R3M reported [10%, 40%, 40%] success for pick/open/close tasks (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Paper does not show R3M un-pretrained; R3M serves as pretrained baseline compared to CLIP and AcTOL.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>R3M outperforms CLIP at low demos but is generally outperformed by AcTOL especially at very low demonstration counts (5 demos).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention visualizations for R3M provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No direct embedding-space plots provided for R3M in this manuscript, but baselines are compared via downstream success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>R3M shows some grounding via higher baseline performance than CLIP; paper uses it as evidence that video pretraining helps, but AcTOL's ordering/continuity further improves grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>R3M benefits from large-scale egocentric video pretraining (Ego4D) that shares action types with robotic tasks; transfer still sensitive to domain gap between human and robot embodiment.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly evaluated in paper for R3M.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot improvements observed relative to CLIP but inferior to AcTOL at same demo counts.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here (used as fixed baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported in this paper for R3M specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>R3M (video) > CLIP (image-text) in many downstream embodied few-shot settings reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>R3M captures temporal structure via video pretraining but does not enforce explicit ordering/continuity as AcTOL does.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided in this paper for R3M.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1902.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1902.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LIV</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior vision-language pretraining method that treats language as a goal and aligns it to the final frame of video clips (goal-reaching semantic alignment); used as a baseline and compared against AcTOL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LIV: language-image representations and rewards for robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LIV</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that aligns language embeddings to goal (final) video frame embeddings to form vision-language representations suited for reward generation and control; typically uses CLIP initialization and goal-reaching time-contrastive losses.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>video-text pretraining with goal-reaching time-contrastive objective (align language to final/future frames)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on human action video datasets (authors of this paper used EPIC-KITCHEN-100 for fair comparison), containing action verbs and object interactions but possibly noisy goal annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same downstream tasks (Unitree D1 real robot and Franka Kitchen/Metaworld sim) used for comparison; LIV provides pretrained encoders for LCBC.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>LIV enforces goal-directed alignment between the language and future/final frames; authors argue this can be misled by noisy or coarse video clips where final frame may not correspond to completion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Franka Kitchen (5 demos): LIV 23.40 ± 0.78% average success (Table 1). Real robot: LIV reported [20%, 30%, 50%] success for pick/open/close tasks (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable; LIV is a pretrained approach compared to other pretraining strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>LIV improves over CLIP but is outperformed by AcTOL at low demo counts (e.g., 5 demos Franka: LIV 23.4% vs AcTOL 42.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis provided for LIV in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No direct embedding-space visualization for LIV in this paper; discussed conceptually as final-frame alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>LIV produces reward signals and performs moderate downstream grounding but authors show LIV can produce reward curves that continue increasing beyond relevant action segments in noisy videos (Figure 4 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>LIV's goal-reaching assumption can fail when videos are coarse or final frames are irrelevant; transfer degrades in presence of noisy action boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly analyzed for LIV in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot evaluation reported; no zero-shot control.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Authors highlight failure modes where final-frame alignment misleads semantics in noisy video clips; shown empirically via reward misalignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>LIV (video-language) > CLIP (image-text) in many tasks but < AcTOL which models ordering+continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>LIV focuses on aligning to final frame (goal dynamics) rather than enforcing smooth, ordered trajectories across all frames, which AcTOL targets explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1902.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1902.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecisionNCE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DecisionNCE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal representation method that aligns language with transitions from initial to final frames (time-contrastive / goal-reaching style); included as a primary baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decisionnce: Embodied multimodal representations via implicit preference learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DecisionNCE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A time-contrastive multimodal pretraining method that aligns language to transitions (initial->final frames) to capture goal-directed semantics; used (with same architecture/dataset) as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>video-text pretraining with time-contrastive / goal-transition objectives</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on human action videos (here compared using EPIC-KITCHEN-100 as the common pretraining dataset), containing task-level verbs and object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same downstream LCBC tasks (Unitree D1 real robot and Franka Kitchen & Metaworld simulations). DecisionNCE provides pretrained encoders for downstream policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>DecisionNCE enforces alignment of language to action transitions (initial->final) — authors note this can overemphasize future frames and be misled by videos with noisy or irrelevant endings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Franka Kitchen (5 demos): DecisionNCE 25.33 ± 1.30% avg success (Table 1). Real robot: DecisionNCE reported [20%, 40%, 60%] success for pick/open/close tasks (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable; DecisionNCE is a pretrained method used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>DecisionNCE improves over CLIP and LIV in some settings but is outperformed by AcTOL at low-demo regimes (e.g., 5-demo Franka: DecisionNCE 25.3% vs AcTOL 42.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis provided for DecisionNCE within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit embedding-space visualizations for DecisionNCE presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>DecisionNCE can generate some language-conditioned rewards and achieve moderate downstream control performance but is more sensitive to noisy final frames than AcTOL.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not examined in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works better when goal-reaching assumption holds; degrades when video clips are noisy or end frames are irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot reported; no zero-shot control results.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Empirical sensitivity to noisy / misaligned final frames reported in narrative and reward-curve comparisons (can produce reward signals that incorrectly grow beyond relevant segments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>DecisionNCE (video-language) surpasses CLIP in many few-shot tasks, but AcTOL (ordering+continuity) further improves robustness and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>DecisionNCE focuses on transition (initial->final) dynamics rather than continuous ordered trajectories across the whole clip.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>R3M: A universal visual representation for robot manipulation <em>(Rating: 2)</em></li>
                <li>LIV: language-image representations and rewards for robotic control <em>(Rating: 2)</em></li>
                <li>Decisionnce: Embodied multimodal representations via implicit preference learning <em>(Rating: 2)</em></li>
                <li>Scaling egocentric vision: The EPIC-KITCHENS dataset <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1902",
    "paper_id": "paper-276094900",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "AcTOL",
            "name_full": "Action Temporal Coherence Learning",
            "brief_description": "A vision-language pretraining method that enforces temporal ordering (Vision-Language Ordering loss) and local continuity (Brownian-bridge regularizer) on video frame embeddings to produce ordered, continuous multi-modal representations for language-conditioned embodied control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AcTOL (initialized from CLIP ResNet-50)",
            "model_description": "ResNet-50 visual backbone + CLIP transformer language encoder initialized from CLIP; encoders are further multimodally pretrained with two new losses: a Vision-Language Ordering (VLO) time-contrastive style loss and a local Brownian-bridge continuity regularizer; after pretraining encoders are frozen and a lightweight MLP policy is trained for downstream LC-IL.",
            "pretraining_type": "multimodal video-text pretraining (vision-language pretraining on human action videos with temporal contrastive + Brownian-bridge continuity)",
            "pretraining_data_description": "Pretrained on EPIC-KITCHEN-100: third-person/egocentric human action video clips with coarse textual annotations describing actions (verbs, objects, task-level instructions, human-object interactions and affordances); noisy, coarse annotations containing action verbs and object references but not low-level robot control signals.",
            "target_task_name": "Language-conditioned robotic manipulation (LC-IL / LCBC)",
            "target_task_description": "Evaluated on real-world Unitree D1 6-DoF end-effector displacement + gripper control (continuous 6-DoF displacement vector + gripper state at 20 Hz) in a third-person office scene (tasks: pick cup, open [X] drawer, close [X] drawer) and in simulation on Franka Kitchen and Metaworld manipulation tasks (various household interactions, 9-DoF Franka in randomized kitchen scenes; Metaworld multi-task benchmarks). Policies are learned via behavior cloning from limited expert demonstrations; visual input from a camera plus proprioception.",
            "semantic_alignment": "Yes — explicitly studied. AcTOL's VLO loss enforces that vision-language similarity differences reflect frame temporal distance (ordering), and Brownian-bridge encourages smooth per-frame embeddings (continuity). Pretraining data (EPIC-KITCHEN) contains overlap in object types and action verbs (kitchen objects, open/close, pick, manipulate) relevant to downstream manipulation; paper argues improved alignment vs goal-reaching pretraining which assumes final-frame goals.",
            "performance_with_language_pretraining": "Real robot: AcTOL success rates — pick cup: 50%, open [X] drawer: 80%, close [X] drawer: 90% (success rate over 10 trials, Table 2). Simulation (Franka Kitchen average across tasks): 5 demos: 42.60 ± 0.53% success; 15 demos: 61.80 ± 2.54%; 25 demos: 64.60 ± 0.57% (Table 1). Metaworld (average): 5 demos: 53.81 ± 3.89%; 15 demos: 74.13 ± 1.59%; 25 demos: 81.13 ± 1.59% (Table 1).",
            "performance_without_language_pretraining": "Baselines include CLIP initialization, R3M, LIV, DecisionNCE and an ablation 'AcTOL w/o BB'. Example comparisons (Franka Kitchen, 5 demos): CLIP 11.67 ± 0.95% avg success; R3M 28.60 ± 1.39%; LIV 23.40 ± 0.78%; DecisionNCE 25.33 ± 1.30%; AcTOL w/o BB 32.80 ± 1.23%; AcTOL 42.60 ± 0.53% (Table 1). Real-robot baselines (Table 2): CLIP [0%,20%,30%], R3M [10%,40%,40%], LIV [20%,30%,50%], DecisionNCE [20%,40%,60%] for pick/open/close tasks respectively (AcTOL [50%,80%,90%]).",
            "sample_efficiency_comparison": "Yes — measured by using small demonstration set sizes (5, 15, 25) in simulation and 60 real demonstrations for real robot (authors note typical prior work uses ~100). Example: on Franka with 5 demos AcTOL (42.6%) outperforms DecisionNCE (25.3%) and CLIP (11.7%), representing ~1.7× and ~3.6× relative success rates respectively; AcTOL achieves large relative gains at low demo counts (Table 1 and Tables 7–12).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No explicit attention-map analysis reported. The paper does not present attention visualizations or per-pixel attention that would show focus on affordances or object parts.",
            "embedding_space_analysis": "Yes — qualitative and quantitative evidence: t-SNE visualizations (Figure 5) show that AcTOL's visual trajectories across time are more temporally continuous and better clustered by action semantics than baselines; paper reports that this improved continuity stabilizes VL alignment and enables dense language-conditioned rewards.",
            "action_grounding_evidence": "Yes — multiple lines of evidence: (1) language-conditioned dense reward curves generated from pretrained AcTOL features accurately localize action boundaries and semantics in real robot videos (Figures 4, 9), including distinguishing consecutive/opposing actions; (2) real-robot task performance (correct drawer index grounding and pick-cup handle grounding) indicates the pretrained features ground instructions to visual locations/affordances; (3) robustness to linguistic perturbations (little drop in success rates under paraphrases) indicates stable language-perception grounding.",
            "hierarchical_features_evidence": "No detailed multi-scale / layerwise analysis of low-level vs high-level feature benefits presented; the paper demonstrates that temporal continuity benefits 'video feature trajectories' and semantic alignment, but does not decompose gains into low- vs high-level visual features.",
            "transfer_conditions": "Transfer works best when pretraining video domains contain coherent temporal action structure and shared objects/actions with downstream tasks (EPIC-KITCHEN -&gt; kitchen manipulation). Failure modes: repetitive/cyclic actions (dishwashing) may violate ordering assumption; domain gap between human hands and robot arms can reduce transfer; AcTOL is robust to linguistic paraphrase but benefits from temporal coherence in pretraining data.",
            "novel_vs_familiar_objects": "No explicit quantitative split reported comparing objects/actions seen vs unseen during pretraining; qualitative experiments include tasks like distinguishing colored cups and dice orientation where model could succeed, but no per-object seen/unseen numbers reported.",
            "zero_shot_or_few_shot": "Few-shot: primary evaluations are few-shot (5/15/25 demonstrations). AcTOL shows few-shot improvements (e.g., 5-demo Franka: 42.6% vs CLIP 11.7%). No strict zero-shot policy transfer (zero demonstrations) reported for control policies, although pretrained features were used to generate reward signals on videos in a zero-shot manner.",
            "layer_analysis": "Partial: encoders are frozen after pretraining and only the MLP policy is trained (design choice). Ablation 'AcTOL w/o BB' (removing Brownian-bridge term) shows reduced performance, demonstrating the importance of the continuity regularizer; no full per-layer probing or fine-grained layer ablations reported.",
            "negative_transfer_evidence": "Yes — discussed qualitatively. The authors note AcTOL's ordering assumption may hurt for repetitive/cyclic actions and that distribution gaps (human videos -&gt; robot visuals/kinematics) can negatively affect transfer; no numeric magnitude for negative transfer is provided.",
            "comparison_to_vision_only": "Indirect: CLIP (image-text pretraining) is used as a vanilla baseline (vision-language image-text pretraining) and is substantially weaker than AcTOL on these embodied tasks; pure vision-only (ImageNet-only) results are not reported. R3M (video-based representation pretrained on Ego4D) performs better than CLIP but worse than AcTOL in many low-data settings.",
            "temporal_dynamics": "Yes — both conceptually and empirically: AcTOL enforces that semantic alignment evolves smoothly over time (VLO + BB). Empirically, Brownian-bridge loss (L_BB) converges faster than VLO and the resulting continuous embeddings lead to stable reward signals and improved early learning; reward curves show temporally localized peaks at action boundaries.",
            "dimensionality_analysis": "No rigorous dimensionality metrics (PCA intrinsic dimension) reported; only t-SNE visualizations and theoretical bounds on continuity/ordering are provided.",
            "uuid": "e1902.0"
        },
        {
            "name_short": "CLIP",
            "name_full": "Learning Transferable Visual Models from Natural Language Supervision (CLIP)",
            "brief_description": "A large image-text contrastive model (ResNet-50 backbone in this work) used as initialization for vision and text encoders; serves as the vanilla baseline and initialization for AcTOL pretraining.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP (ResNet-50 backbone)",
            "model_description": "Image-text contrastive model with a ResNet-50 image encoder and a transformer text encoder trained on large-scale image-caption pairs to align visual and text embeddings in a shared space; provides frozen encoder initialization or baseline embeddings.",
            "pretraining_type": "vision-language on image-text pairs (web-scale image-caption pretraining)",
            "pretraining_data_description": "Large-scale image-caption pairs from web sources (contains object labels, scene descriptions, and some action verbs but not structured video temporal dynamics or dense human-object interaction sequences).",
            "target_task_name": "Language-conditioned robotic manipulation (baseline evaluations)",
            "target_task_description": "Same downstream LC-IL tasks described above (Unitree D1 real robot pick/open/close tasks and Franka Kitchen / Metaworld simulation); CLIP used as a frozen encoder baseline feeding into MLP policy.",
            "semantic_alignment": "Partial overlap: CLIP encodes object and some action semantics from image-text pairs but lacks temporal action progression cues; paper demonstrates CLIP baseline performs poorly on few-demo embodied tasks relative to video-based pretraining.",
            "performance_with_language_pretraining": "As baseline: Franka Kitchen (5 demos) average success 11.67 ± 0.95%; 15 demos 27.47 ± 1.01%; 25 demos 31.20 ± 2.62% (Table 1). Metaworld values also reported (e.g., 5 demos ~42.29% on some Metaworld tasks as per Table 1 aggregate columns). Real robot: pick/open/close reported approximately [0%, 20%, 30%] respectively (Table 2).",
            "performance_without_language_pretraining": "Not applicable here (CLIP itself is a pretrained VL model); the paper does not report a purely random-initialization vision-only control baseline.",
            "sample_efficiency_comparison": "CLIP shows much lower sample efficiency in these embodied tasks — with 5 demos CLIP achieves ~11.7% avg success on Franka versus AcTOL's 42.6%; no explicit sample-efficiency curve for CLIP beyond tabulated demo counts.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention-map analysis for CLIP within this paper.",
            "embedding_space_analysis": "CLIP embeddings are used as a starting point; t-SNE results indicate CLIP retains discriminative power but exhibits less temporal continuity than AcTOL (Figure 5 comparisons).",
            "action_grounding_evidence": "Minimal — CLIP baseline underperforms on tasks requiring temporal grounding and precise action localization (e.g., identifying handle or drawer index).",
            "hierarchical_features_evidence": "No layerwise analysis of CLIP in this paper.",
            "transfer_conditions": "CLIP transfers poorly when temporal structure and fine-grained action progression are important; better for static object recognition tasks.",
            "novel_vs_familiar_objects": "Not explicitly analyzed for CLIP in this paper.",
            "zero_shot_or_few_shot": "Few-shot: CLIP baseline evaluated with same small-demo counts; no zero-shot control reported.",
            "layer_analysis": "CLIP encoders used as initialization and frozen for downstream policy; no ablation beyond that.",
            "negative_transfer_evidence": "CLIP performs worse than video-based pretraining methods in low-demo embodied tasks, indicating limited transfer for temporally structured tasks.",
            "comparison_to_vision_only": "CLIP is a vision-language model (image-text); direct comparison to vision-only ImageNet models not provided.",
            "temporal_dynamics": "Not explicitly captured by CLIP since CLIP is image-based.",
            "dimensionality_analysis": "No explicit dimensionality analysis for CLIP in this paper.",
            "uuid": "e1902.1"
        },
        {
            "name_short": "R3M",
            "name_full": "R3M",
            "brief_description": "A prior method that learns universal visual representations for robot manipulation by leveraging video pretraining and language/reward shaping; used as a competitive baseline (pretrained on Ego4D).",
            "citation_title": "R3M: A universal visual representation for robot manipulation",
            "mention_or_use": "use",
            "model_name": "R3M",
            "model_description": "Video-pretrained visual representation intended for robotic manipulation, combining visual encoders with task-aware objectives and optionally language/reward shaping; in this paper used as a pretrained baseline (weights from prior R3M trained on Ego4D).",
            "pretraining_type": "video-based pretraining (multimodal / video representation pretraining, R3M trained on Ego4D in original work)",
            "pretraining_data_description": "Ego4D human egocentric video dataset (very large-scale egocentric videos of human activities) with dense human-object interactions, actions and affordances; contains many action verbs and object interactions relevant to robotics.",
            "target_task_name": "Language-conditioned robotic manipulation (baseline)",
            "target_task_description": "Same real and simulated manipulation tasks (Unitree D1 real robot tasks and Franka Kitchen / Metaworld), evaluated as a transfer baseline.",
            "semantic_alignment": "R3M pretraining on Ego4D provides better temporal and action-alignment priors than static image-text models, improving performance relative to CLIP, though it does not use AcTOL's explicit ordering+continuity losses.",
            "performance_with_language_pretraining": "Franka Kitchen (5 demos): ~28.60 ± 1.39% average success (Table 1). Real robot: R3M reported [10%, 40%, 40%] success for pick/open/close tasks (Table 2).",
            "performance_without_language_pretraining": "Paper does not show R3M un-pretrained; R3M serves as pretrained baseline compared to CLIP and AcTOL.",
            "sample_efficiency_comparison": "R3M outperforms CLIP at low demos but is generally outperformed by AcTOL especially at very low demonstration counts (5 demos).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention visualizations for R3M provided in this paper.",
            "embedding_space_analysis": "No direct embedding-space plots provided for R3M in this manuscript, but baselines are compared via downstream success rates.",
            "action_grounding_evidence": "R3M shows some grounding via higher baseline performance than CLIP; paper uses it as evidence that video pretraining helps, but AcTOL's ordering/continuity further improves grounding.",
            "hierarchical_features_evidence": "Not analyzed in this paper.",
            "transfer_conditions": "R3M benefits from large-scale egocentric video pretraining (Ego4D) that shares action types with robotic tasks; transfer still sensitive to domain gap between human and robot embodiment.",
            "novel_vs_familiar_objects": "Not explicitly evaluated in paper for R3M.",
            "zero_shot_or_few_shot": "Few-shot improvements observed relative to CLIP but inferior to AcTOL at same demo counts.",
            "layer_analysis": "Not reported here (used as fixed baseline).",
            "negative_transfer_evidence": "Not reported in this paper for R3M specifically.",
            "comparison_to_vision_only": "R3M (video) &gt; CLIP (image-text) in many downstream embodied few-shot settings reported here.",
            "temporal_dynamics": "R3M captures temporal structure via video pretraining but does not enforce explicit ordering/continuity as AcTOL does.",
            "dimensionality_analysis": "Not provided in this paper for R3M.",
            "uuid": "e1902.2"
        },
        {
            "name_short": "LIV",
            "name_full": "LIV",
            "brief_description": "A prior vision-language pretraining method that treats language as a goal and aligns it to the final frame of video clips (goal-reaching semantic alignment); used as a baseline and compared against AcTOL.",
            "citation_title": "LIV: language-image representations and rewards for robotic control",
            "mention_or_use": "use",
            "model_name": "LIV",
            "model_description": "Method that aligns language embeddings to goal (final) video frame embeddings to form vision-language representations suited for reward generation and control; typically uses CLIP initialization and goal-reaching time-contrastive losses.",
            "pretraining_type": "video-text pretraining with goal-reaching time-contrastive objective (align language to final/future frames)",
            "pretraining_data_description": "Pretrained on human action video datasets (authors of this paper used EPIC-KITCHEN-100 for fair comparison), containing action verbs and object interactions but possibly noisy goal annotations.",
            "target_task_name": "Language-conditioned robotic manipulation (baseline)",
            "target_task_description": "Same downstream tasks (Unitree D1 real robot and Franka Kitchen/Metaworld sim) used for comparison; LIV provides pretrained encoders for LCBC.",
            "semantic_alignment": "LIV enforces goal-directed alignment between the language and future/final frames; authors argue this can be misled by noisy or coarse video clips where final frame may not correspond to completion.",
            "performance_with_language_pretraining": "Franka Kitchen (5 demos): LIV 23.40 ± 0.78% average success (Table 1). Real robot: LIV reported [20%, 30%, 50%] success for pick/open/close tasks (Table 2).",
            "performance_without_language_pretraining": "Not applicable; LIV is a pretrained approach compared to other pretraining strategies.",
            "sample_efficiency_comparison": "LIV improves over CLIP but is outperformed by AcTOL at low demo counts (e.g., 5 demos Franka: LIV 23.4% vs AcTOL 42.6%).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention analysis provided for LIV in this paper.",
            "embedding_space_analysis": "No direct embedding-space visualization for LIV in this paper; discussed conceptually as final-frame alignment.",
            "action_grounding_evidence": "LIV produces reward signals and performs moderate downstream grounding but authors show LIV can produce reward curves that continue increasing beyond relevant action segments in noisy videos (Figure 4 comparisons).",
            "hierarchical_features_evidence": "Not analyzed in this paper.",
            "transfer_conditions": "LIV's goal-reaching assumption can fail when videos are coarse or final frames are irrelevant; transfer degrades in presence of noisy action boundaries.",
            "novel_vs_familiar_objects": "Not explicitly analyzed for LIV in this paper.",
            "zero_shot_or_few_shot": "Few-shot evaluation reported; no zero-shot control.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Authors highlight failure modes where final-frame alignment misleads semantics in noisy video clips; shown empirically via reward misalignment.",
            "comparison_to_vision_only": "LIV (video-language) &gt; CLIP (image-text) in many tasks but &lt; AcTOL which models ordering+continuity.",
            "temporal_dynamics": "LIV focuses on aligning to final frame (goal dynamics) rather than enforcing smooth, ordered trajectories across all frames, which AcTOL targets explicitly.",
            "dimensionality_analysis": "Not provided in this paper.",
            "uuid": "e1902.3"
        },
        {
            "name_short": "DecisionNCE",
            "name_full": "DecisionNCE",
            "brief_description": "An embodied multimodal representation method that aligns language with transitions from initial to final frames (time-contrastive / goal-reaching style); included as a primary baseline for comparison.",
            "citation_title": "Decisionnce: Embodied multimodal representations via implicit preference learning",
            "mention_or_use": "use",
            "model_name": "DecisionNCE",
            "model_description": "A time-contrastive multimodal pretraining method that aligns language to transitions (initial-&gt;final frames) to capture goal-directed semantics; used (with same architecture/dataset) as a baseline in experiments.",
            "pretraining_type": "video-text pretraining with time-contrastive / goal-transition objectives",
            "pretraining_data_description": "Pretrained on human action videos (here compared using EPIC-KITCHEN-100 as the common pretraining dataset), containing task-level verbs and object interactions.",
            "target_task_name": "Language-conditioned robotic manipulation (baseline)",
            "target_task_description": "Same downstream LCBC tasks (Unitree D1 real robot and Franka Kitchen & Metaworld simulations). DecisionNCE provides pretrained encoders for downstream policy learning.",
            "semantic_alignment": "DecisionNCE enforces alignment of language to action transitions (initial-&gt;final) — authors note this can overemphasize future frames and be misled by videos with noisy or irrelevant endings.",
            "performance_with_language_pretraining": "Franka Kitchen (5 demos): DecisionNCE 25.33 ± 1.30% avg success (Table 1). Real robot: DecisionNCE reported [20%, 40%, 60%] success for pick/open/close tasks (Table 2).",
            "performance_without_language_pretraining": "Not applicable; DecisionNCE is a pretrained method used as baseline.",
            "sample_efficiency_comparison": "DecisionNCE improves over CLIP and LIV in some settings but is outperformed by AcTOL at low-demo regimes (e.g., 5-demo Franka: DecisionNCE 25.3% vs AcTOL 42.6%).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention analysis provided for DecisionNCE within this paper.",
            "embedding_space_analysis": "No explicit embedding-space visualizations for DecisionNCE presented here.",
            "action_grounding_evidence": "DecisionNCE can generate some language-conditioned rewards and achieve moderate downstream control performance but is more sensitive to noisy final frames than AcTOL.",
            "hierarchical_features_evidence": "Not examined in this paper.",
            "transfer_conditions": "Works better when goal-reaching assumption holds; degrades when video clips are noisy or end frames are irrelevant.",
            "novel_vs_familiar_objects": "Not evaluated explicitly.",
            "zero_shot_or_few_shot": "Few-shot reported; no zero-shot control results.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Empirical sensitivity to noisy / misaligned final frames reported in narrative and reward-curve comparisons (can produce reward signals that incorrectly grow beyond relevant segments).",
            "comparison_to_vision_only": "DecisionNCE (video-language) surpasses CLIP in many few-shot tasks, but AcTOL (ordering+continuity) further improves robustness and performance.",
            "temporal_dynamics": "DecisionNCE focuses on transition (initial-&gt;final) dynamics rather than continuous ordered trajectories across the whole clip.",
            "dimensionality_analysis": "Not provided in this paper.",
            "uuid": "e1902.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "R3M: A universal visual representation for robot manipulation",
            "rating": 2
        },
        {
            "paper_title": "LIV: language-image representations and rewards for robotic control",
            "rating": 2
        },
        {
            "paper_title": "Decisionnce: Embodied multimodal representations via implicit preference learning",
            "rating": 2
        },
        {
            "paper_title": "Scaling egocentric vision: The EPIC-KITCHENS dataset",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        }
    ],
    "cost": 0.0208705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents
22 May 2025</p>
<p>Zhizhen Zhang zhizhen.zhang@uq.edu.au 
Lei Zhu 
Zhen Fang zhen.fang@uts.edu.au 
Helen Huang helen.huang@uq.edu.au 
Yadan Luo y.luo@uq.edu.au </p>
<p>University of Queensland Brisbane
Australia</p>
<p>Tongji University Shanghai
China</p>
<p>University of Technology Sydney Sydney
Australia</p>
<p>University of Queensland Brisbane
Australia</p>
<p>University of Queensland Brisbane
Australia</p>
<p>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents
22 May 20259DECAAA30D340C21F41A146B5FAF4348arXiv:2502.01218v2[cs.RO]
Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents.However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame.This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end.To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint.AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames.Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.Our code and demo videos are available here.</p>
<p>Our learned multi-modal representations can be effectively transferred to downstream language-conditioned robot manipulation tasks, exhibiting robustness to diverse instruction and linguistic variations.</p>
<p>Introduction</p>
<p>The long-term vision for embodied intelligence [26,22] is to create systems that seamlessly perceive and interact with the world around them.Achieving this requires agents that integrate vision and language to understand their surroundings, interpret human instructions, and autonomously plan actions for complex tasks.Current end-to-end approaches achieve policy learning through direct vision-language-action mapping [43,11,5,19,3].However, the inherent unpredictability of physical environments, including unseen scenarios and dynamic object interactions, constrains these solutions by requiring massive, high-quality robotic trajectories with action annotations, which are costly to collect.To mitigate this, recent research has leveraged large-scale, readily available egocentric human action videos [12,8,13] for pre-training.Although these out-of-domain videos often lack lowlevel action details and contain noise, their diverse human-object interactions and task instructions provide valuable prior knowledge.This enables the pre-trained representations to be more effectively transferred to novel tasks with fewer demonstrations, reducing reliance on large-scale robotic datasets while preserving strong generalization capabilities.</p>
<p>A promising approach for vision-language pre-training from human action videos leverages the concept of time contrastive learning [33] to capture temporally consistent visual representations, where language serves as the guiding goal, with semantic alignment between the language and chronologically later frames in the video [27,23,20].However, this goal-reaching semantic alignment approach relies on a rigid assumption that action videos adhere to a specific principle: actions progressively approach the target instruction from the initial frame to the final one.Such assumption can be easily violated in egocentric human action videos, which are typically annotated at a coarsegrained level and riddled with noise.Figure 1 shows an example video-instruction pair, where the end of the video clip does not correspond to the actual end of the action.As a result, existing methods suffer from misleading semantic alignment, which hampers their ability to learn accurate vision-language relationships.</p>
<p>Given the challenges outlined above, a more natural and flexible pre-training strategy without rigid assumptions is needed to enhance vision-language representations for better policy learning.Building solely on the intrinsic temporal consistency of human action videos, we argue that the ordering and continuity of pre-trained vision-language representations play a crucial role in ensuring the effectiveness of policy learning.Ordering refers to the need for visual features to align with the underlying action logic required by the language instruction.For instance, as the task progresses, visual representations closer to the completion of the action should exhibit stronger alignment with the language instruction.This ensures that each step in the sequence is meaningfully associated with the corresponding instruction, enabling the model to effectively capture the dynamic progression of the task.Continuity, on the other hand, emphasizes that both visual features and their alignment with the language should evolve smoothly over time, with gradual transitions rather than abrupt changes.This is crucial because actions in the real world are not discrete but unfold continuously in time.</p>
<p>Moreover, the alignment between visual and instruction should also be fluid, ensuring that as the action progresses, the visual representations consistently align with the target language instruction.</p>
<p>To address the aforementioned issues, as illustrated in Figure 2, we propose Action Temporal Coherence Learning (AcTOL), a novel approach designed to implicitly capture the ordering and continuity of video actions without relying on rigid assumptions, while providing strong theoretical guarantees.Unlike previous approaches that focus on goal-directed semantic alignment, AcTOL introduces a Vision-Language Ordering (VLO) loss.This loss leverages the intrinsic temporal coherence of videos, contrasting frames against each other based on their relative temporal distance, theoretically ensuring that the semantic alignment between frames reflects their temporal ordering and continuity throughout the entire sequence.However, the VLO loss does not explicitly enforce the continuity of the visual features themselves, and under conditions with variations in frame content and noise, it can lead to suboptimal local consistency of the visual features.To address this, AcTOL introduces a Brownian bridge constraint over the video, treating video frames as a Brownian bridge process.This approach imposes a structured, continuous flow on the visual representations, ensuring that the model learns more consistent and stable intermediate states, further enhancing the continuity of the visual representations and improving the stability of their alignment with language instruction.Further theoretical analysis suggests that these properties also contribute to the model's resilience to language perturbations, a crucial trait for real-world applications.To evaluate the generalization ability of AcTOL on embodied agents, we conducted extensive language-conditioned imitation learning experiments using both the real-world Unitree D1 robotic arm and two simulation environments.The results demonstrate that AcTOL significantly outperforms prior methods with a limited number of expert demonstrations.Additionally, AcTOL can generate language-conditioned visual rewards from real-world robot videos and remains robust to complex linguistic perturbations, highlighting its potential as a generalizable solution for real-world embodied agents.</p>
<p>Preliminaries</p>
<p>We first set up notations and mathematically formulate tasks.</p>
<p>Language-Conditioned Imitation Learning (LC-IL).</p>
<p>The task of LC-IL aims to train an agent to mimic expert behaviors from a given demonstration set
D d = {(τ i , l i )} N i=1
, where l i ∈ L represents a task-specific language instruction.Each trajectory τ i ∈ T consists of a sequence of state-action pairs τ i = {(s j , a j )} T j=1 of the horizon length T .In robot manipulation tasks, action a j ∈ A corresponds to the control commands executed by the agent and state s j = [p j ; v j ] ∈ S records proprioceptive data p j (e.g., joint positions, velocities) and visual inputs o j ∈ O (e.g., camera images) at the time step j.The objective of LC-IL is to find an optimal language-conditioned policy π * (a|s, l) : S × L → A via solving the supervised optimization as follows,
π * ∈ arg min π E (τi,li)∼T   1 T (sj ,aj )∼τi ℓ(π(â j , s j |l i ), a j )   ,
where ℓ(•, •) is a task-specific loss, such as mean squared error or cross-entropy.Training the policy π θ in an end-to-end fashion may require hundreds of high-quality expert demonstrations to converge, primarily due to the high variance of visual inputs o and language instructions l.</p>
<p>Vision-language Pre-training.Address such scalability issues can be achieved by leveraging large-scale, easily accessible human action video datasets [8,13]
D p = {(O i , l i )} M i=1
, where O i = {o j } T j=1 represents a video clip with T frames and l i the corresponding description.Pretraining on such datasets enables policies to rapidly learn visual-language correspondences with minimal expert demonstrations.Mainstream pretraining methods employ time contrastive learning [33] to fine-tune a visual encoder ϕ and a text encoder φ, which project frames and descriptions into a shared d-dimensional embedding space, i.e., v j = ϕ(o j ) ∈ R d and l i = φ(l i ) ∈ R d .To provide a unified perspective on various pretraining approaches, we formulate them within the objective L tNCE (ϕ, φ):
L tNCE = −E o + ∼P(Oi) log exp(R(v + , l i )) E o − ∼N (Oi) exp(R(v − , l i ))
, where v +/− = ϕ(o +/− ).Different pretraining strategies differ in their selection of (1) the positive frame set P(O i ), (2) negative frame set N (O i ); and (3) the semantic alignment scoring function R(v, l i ) measuring the gap of VL similarities.</p>
<p>As motivated by goal-conditioned RL [1], current approaches explicitly select future frames (e.g., R3M, DecisionNCE) or the last frame (e.g., LIV) as the goal within the positive frame set, enforcing their visual embedding to align with the semantics.Likewise, the scoring functions R are often designed to maximize this transition direction.However, the pretraining action videos are noisy as actions may terminate early or include irrelevant subsequent actions, which may mislead the encoders and result in inaccurate vision-language association.As detecting precise action boundaries is non-trivial, we argue for a more flexible approach that leverages intrinsic characteristics of actions to guide pertaining.</p>
<p>Our Approach: AcTOL</p>
<p>We introduce an action temporal coherence learning (AcTOL) to capture two temporal properties of video actions: ordering and continuity.Ordering was ensured in the vision-language ordering loss (Section 3.1), where the semantic difference between frames reflects their temporal distance, with closer frames exhibiting smaller differences than those further apart.Continuity requires smooth visual transitions between adjacent frames, avoiding abrupt changes and high variance.To achieve this, we model sampled frame intervals as a Brownian bridge process (Section 3.2), penalizing deviations from the expected trajectories.Different from prior works that relies on setting explicit goal frames, the proposed approach implicitly explore the global and local structure of actions without imposing rigid constraints.</p>
<p>Visual-Language Ordering</p>
<p>To capture the temporal coherence of video actions, we first propose a vision-language ordering (VLO) loss that ensures the semantic alignment between frames reflects their temporal order.Consider an anchor frame o i ∈ O with an index n(i) corresponding to its position in the original video.For any given frame pair (o i , o j ), we first define the semantic alignment score R to quantify differences in their VL similarities w.r.t a language description l as:
R(v i , v j , l) = −∥ sim(v i , l) − sim(v j , l)∥ 2 ,(1)
where v i = ϕ(o i ), l = φ(l).The function sim(•, •) computes the VL similarity using cosine similarity.</p>
<p>To ensure the proposed R adhere to the temporal ordering of frames, we construct a negative set N i,j by selecting o k ∈ O correspond to frames that are temporally more distant from o i than o j :
N i,j = {o k | k ̸ = i, |n(i) − n(k)| ≥ |n(i) − n(j)|},
This formulation allows us to reformulate L tNCE by enforcing that the VL similarity difference between frames i and j should be smaller than that between frame i and any negative frame k within the video O:
L VLO = −E (oi,oj )∼O log exp (R(v i , v j , l)) o k ∈Ni,j exp (R(v i , v k , l))
.</p>
<p>Notably, our VLO loss does not strictly require o j to be from a future timestep for goal-reaching.Instead, we leverage the inherent temporal dynamics in videos, allowing the model to learn the natural ordering in an unsupervised manner.</p>
<p>Vision-Language Continuity</p>
<p>While the VLO property provides a strong global constraint on the structural alignment of VL pretraining, optimizing triplet relationships alone can be unstable.Variations in frame content and noise often lead to suboptimal local consistency.To mitigate this, we introduce an additional local continuity constraint inspired by the Brownian bridge [32].This stochastic process models transitions between two fixed endpoints over by any sampled local video interval [n(i), n(j)].For any time step t ∈ [n(i), n(j)] within this interval, the transition density of Brownian Bridge process B(t) follows a time-dependent Gaussian distribution:
N v i + t − n(i) n(j) − n(i) (v j − v i ), t(n(j) − n(i)) − t 2 ) n(j) − n(i) ,
where v i , v j ∈ R d are the visual embeddings of the first and last frames in the sampled interval.</p>
<p>The mean trajectory E[B(t)] linearly interpolates between the two endpoints, while the variance Var[B(t)] provides uncertainty modeling that peaks in the middle of the interval.To enforce this local continuity, the Brownian bridge loss L BB is formulated as,
L BB = 1 T T t=1 1 2Var[B(t)] ∥v t − E[B(t)]∥ 2 2 .(2)
This loss encourages local consistency by penalizing deviations from expected trajectories, ensuring consistency across short temporal spans.</p>
<p>Overall Objective.The final training objective integrates both global and local constraints to achieve temporal coherence simultaneously:
L AcTOL = L VLO + λL BB ,(3)
where λ is empirically set to balance two components.</p>
<p>Theoretical Analysis</p>
<p>In this section, we theoretically prove the vision-language ordering and continuity, as well as extend the robustness of linguistic perturbations of representations learned by AcTOL.All proofs are provided in Appendix 10 for reference.</p>
<p>Vision-Language Ordering.Ordering and sorting properties are well-established in self-supervised learning [35,16,41].Building upon these insights, we formalize the concept of vision-language ordering (VLO) below.</p>
<p>Definition 1 (VLO Representations).Let {o i } i∈[T ] be a sequence of video frames and l the corresponding language description.The representations of the frames are said to satisfy the VLO property for any 0 &lt; δ &lt; 1 if ∀i ∈ [T ], and distinct frames j, k ∈ [T ]{i}, the following conditions hold:
R i,j,l &gt; R i,k,l + 1/δ, if d i,j &lt; d i,k , |R i,j,l − R i,k,l | &lt; δ, if d i,j = d i,k , R i,j,l &lt; R i,k,l − 1/δ, if d i,j &gt; d i,k ,
where R i,j,l denotes R (v i , v j , l) and d i,j denotes |n(i) − n(j)|.</p>
<p>Implications of the VLO Property.The VLO property enforces a structured representation of video frames, ensuring that temporally adjacent frames have consistent and predictable semantic differences.When two frames have equal temporal distances from an anchor frame, their semantic gaps should be similar, fostering smooth transitions.In contrast, frames that are farther apart should exhibit larger semantic gaps, thus preserving the chronological order.</p>
<p>To formalize the temporal ordering constraints, we define the unique sorted set of frame distances from frame i as
{D i,1 &lt; D i,2 &lt; • • • &lt; D i,Mi }, where each D i,m , m ∈ [M i ] is obtained by sorting the set {d i,j | j ∈ [T ] \ {i}}.
Additionally, we define the count of frames at each distance level as:
n i,m := |{j | d i,j = D i,m , j ∈ [T ] \ {i}}|,(4)
which denotes the number of frames whose temporal distance from frame i equals D i,m .The VLO property is satisfied when the proposed L VLO approaches its theoretical lower bound, which is given by:
L * := 1 T (T − 1) T i=1 Mi m=1 n i,m log n i,m .(5)
This bound characterizes the optimal alignment of VL similarities, ensuring that the learned representations preserve the inherent temporal structure within the video sequence, as guaranteed by the following theorem:</p>
<p>Theorem 1 (Vision-Language Ordering).L * is a tight lower bound of L VLO , i.e., L VLO ≥ L * , and for any ϵ &gt; 0, there exists feature embeddings such that L VLO &lt; L * + ϵ.Furthermore, for any 0 &lt; δ &lt; 1, there exist ϵ &gt; 0 such that if L VLO &lt; L * + ϵ, the learned representations satisfy the VLO property.</p>
<p>Vision-Language Continuity.We establish the following theoretical result to rigorously describe continuity preservation in vision-language representations:</p>
<p>Theorem 2 (Vision-Language Continuity).Let v k , v l be visual representations at arbitrary time steps within a Brownian Bridge-regularized interval [n(i), n(j)], and let l ∈ L be a language embedding.If the VL similarity function sim(•) is Lipschitz continuous with constant C, then for any ϵ &gt; 0, there exists δ &gt; 0 such that:
∥v k − v l ∥ 2 &lt; δ ⇒ |R(v k , v l , l)| &lt; ϵ.
This result follows from two key observations: (i) Brownian Bridge regularization constrains each embedding to remain close to a linear interpolation between anchor frames, with deviations governed by a time-dependent variance; and (ii) under this constraint, the distance between temporally close frames admits an explicit upper bound.Combining this with the Lipschitz continuity of the visionlanguage similarity function ensures that small changes in frame embeddings lead to proportionally bounded changes in alignment scores.</p>
<p>Building upon the continuity result, we further demonstrate that the semantic alignment score remains stable under small perturbations in language input: Theorem 3 (Robustness to Language Variations).Let l ′ be a perturbed language embedding such that ∥l − l ′ ∥ ≤ δ l .Then the semantic alignment score R satisfies:
|R(v i , v j , l ′ ) − R(v i , v j , l)| ≤ 2Cδ l .
This second result guarantees that small shifts in the language representation (e.g., synonym substitution or phrasing variation) lead to bounded changes in the alignment score.Together, Theorems 2 and 3 formalize the local stability of semantic grounding across both time and modality, providing a theoretical basis for continuity-aware vision-language learning.</p>
<p>Experiment</p>
<p>In our experiments, we aim to evaluate the effectiveness of ordered and continuous vision-language representations for robotic control.First, we conduct extensive Language-Conditioned Behavior Cloning (LCBC) experiments on both real and simulated robots to validate the importance of ordering and continuity for imitation learning.Second, we assess the utility of the learned representations as reward functions on multiple real-world action videos.The results demonstrate that the ordered and continuous representations enable our method to accurately identify action boundaries and generate dense rewards aligned with the given instructions.Finally, we evaluate the robustness of our method under language perturbations, showcasing its strong generalization capability for application in real-world daily scenarios.</p>
<p>Experimental Setups.Figure 3 shows the experimental environments.For real-world robot evaluation, we deploy the Unitree D1 robot arm to perform three challenging manipulation tasks: pick cup, open [X] drawer and close [X] drawer, where [X] is the drawer index specified by the instruction.The pick cup task requires the model to accurately identify the cup handle, while the open/close [X] drawer tasks demand grounding of language instructions to visual observations, enabling the model to interact with the correct drawer.To isolate manipulation performance, the Unitree Go2 quadruped remains lying down and stationary throughout the evaluation.We use a web camera to capture a third-person view as visual observation.The action space consists of a 6-DoF end-effector displacement vector and gripper state, executed at a control frequency of 20 Hz.For each task, we collect 60 demonstrations via remote control using the Unitree Go app, which is significantly fewer than the 100 trajectories typically used in prior work [23,20].For simulation, we choose two widely used simulation environments for evaluation: Franka Kitchen [14,10] and Metaworld [39].</p>
<p>For Franka Kitchen, we evaluate five tasks: sliding a cabinet, opening the left door, opening the microwave, turning on the stove, and switching on the light.For Metaworld, we focus on learning five tasks: hammering a nail, pressing a button, picking and placing a block, assembling a ring onto a peg, and opening a drawer.Detailed environment setup can be found at Appendix 9.1.Baselines.Since our model is initialized with CLIP [30], a state-of-the-art image-text representation widely applied in various embodied tasks [7,18,34,37], it is a natural choice to include CLIP as a vanilla baseline for comparison.Our primary baselines are LIV [23] and DecisionNCE [20], as we use the same model architecture and dataset for pre-training.We also compare against R3M [27] pretrained on Ego4D [13], a dataset containing roughly 36× longer videos than EPIC-KITCHEN-100.We also include an ablation variant of AcTOL where the Brownian Bridge loss is removed, referred to as AcTOL w/o BB.</p>
<p>Implementation Details.We initialize our model with the weights of CLIP [30] with ResNet-50 vision backbone and further pre-train it on human action video dataset EPIC-KITCHEN-100 [8,9].For hyperparameter selection, we uniformly sample 10 frames of each video per batch.The loss weight λ to 0.1.Other hyperparameters, such as temperature,s follow the default value used in CLIP [30].More details of pre-training and hyperparameter sensitivity can be found in Appendix 8.</p>
<p>Language-Conditioned Behavior Cloning</p>
<p>For LCBC policy learning, we keep the pre-trained vision-language encoders frozen and feed their output representations into a lightweight MLP, which is trained as a policy network.Real Robot results.Table 2 shows the real robot comparison results.AcTOL consistently outperforms all baseline models across the three tasks.Among them, the pick cup task yields relatively lower performance, as it requires the model to precisely identify and grasp the cup handle, demanding stronger spatial perception capabilities.For the open/close [X] drawer tasks, AcTOL is able to accurately interpret the drawer number specified in the language instruction, align it with the corresponding location in the visual observation, and execute continuous actions on the correct drawer to complete the task.These results highlight the effectiveness of AcTOL's learned visual-language representations in real-world manipulation tasks.By learning semantically smooth visual representations, our model further enables the use of semantic trajectories as effective task rewards.</p>
<p>Language-Conditioned Visual Rewards</p>
<p>To illustrate this, we first demonstrate the continuity of purely visual representations.In Figure 5, we visualize the learned visual representation trajectories for three tasks, each with ten video clips, using t-SNE.The results show that AcTOL significantly improves the temporal continuity of video feature trajectories while retaining CLIP's discriminative ability to distinguish between actions associated with different instructions.As discussed in Section 3.2, the visual continuity can stabilize learning ordered vision-language alignment.Building on this foundation, we define a dense reward signal based on the semantic alignment between the current visual state and the language goal.Specifically, at each time step i, we define the reward cosine(v i , l) as the similarity between the current visual state and the language goal.While prior work [23,20] focused primarily on single-action video clips, we evaluate reward quality on three clips, each containing two consecutive actions, to assess whether the model can reliably capture fine-grained action semantics., where human and robot actors perform opposite actions.Only our method consistently produces symmetric and instruction-aligned reward curves, accurately identifying both action boundaries and semantics.</p>
<p>Robustness Study under Linguistic Perturbations</p>
<p>In the EPIC-KITCHEN-100 dataset, textual annotations are often concise, such as "open cupboard".In the default setting of LCBC, we employ similarly structured simple instructions.In this experiment, to validate the robustness of the representations our method learns in real-world scenarios, we introduce minor modifications to the language instructions.Specifically, we transform each original instruction into four conversational variants by varying lexical choices (e.g., verbs and nouns) and incorporating ChatGPT-4o [28] generated complex instructions.Details can be found in Appendix 9.4.We then evaluate the imitation learning performance conditioned on these modified instructions in the Franka Kitchen environment.For comparison, we select LIV and DecisionNCE, which are also pre-trained on EPIC-KITCHEN-100.As shown in Figure 6, the success rates of LIV and DecisionNCE dropped by 11.9% and 2.7% on average, respectively, while our method maintained a success rate comparable to that before language perturbation.This result demonstrates the robustness of our learned representations, which generalize more effectively to real-world scenarios.</p>
<p>Related Work</p>
<p>Given the success of large-scale pre-training in the vision and language research communities [4,21], many studies have attempted to extend this paradigm to the field of robotics.Some work leverage massive robotic trajectory data [6] for pre-training, aiming to establish unified vision-langauge-action models [43,5,19,3,11,36,29].However, collecting large amounts of high-quality robot trajectory data is extremely costly and time-consuming.Consequently, many studies have begun to explore the use of large-scale, readily available, out-of-domain human action video data to learn generalizable representations that can be transferred to robotic tasks [33,24,31,27,17,23,25,38,40,20].Among these, TCN [33], VIP [24], MVP [31], and VC-1 [25] focus solely on studying unimodal visual representations, limiting their performance when understanding language instructions is required.R3M [27] employs language and reward models to shape progressive visual representations, while Voltron [17] and MPI [40] model the transition from the current state to the goal state conditioned on language.However, during training, these approaches freeze the language encoder, using it only to aid in the training of visual representations.As a result, they do not effectively achieve multi-modal representation learning.LIV [23] and DecisionNCE [20] have attempted to leverage CLIP [30] to train embodied multi-modal representations.LIV treats language as the goal of video actions, aligning it with the final frame, while DecisionNCE aligns language with the transition from the initial to final frame.Both rely on a goal-reaching assumption, which can lead to suboptimal results in noisy real-world videos.In contrast, our approach avoids rigid assumptions by enforcing semantic alignment that follows the intrinsic temporal continuity of videos, leading to more robust and generalizable vision-language representations.This property also benefits methods like UVD [42], which rely on pretrained visual features to detect phase changes and decompose long-horizon tasks.</p>
<p>Our method more reliably identifies action phases, enabling stronger progress rewards and improving suitability for such goal-conditioned downstream tasks.</p>
<p>Conclusion</p>
<p>We present Action Temporal Coherence Learning (AcTOL) as a promising vision-language pretraining solution for generalizable embodied agents.By learning action consistency from a large corpus of human action videos, AcTOL theoretically ensures the ordering and continuity of visionlanguage representations, as well as robustness to language perturbations.Extensive experiments across various environments demonstrate that AcTOL effectively generalizes to complex robotic manipulation tasks.One limitation of AcTOL is its reliance on the assumption that actions follow a coherent temporal order.While this holds for most tasks, repetitive or cyclic actions such as dishwashing or window cleaning may violate this assumption and affect performance.Future work could adapt AcTOL to better handle such unordered action patterns.</p>
<p>Appendix 8 Pre-training Details</p>
<p>Following [23,20], we use a modified ResNet-50 [15] from CLIP [30] for the vision encoder and a CLIP transformer for the language encoder.We initialize our model with CLIP and train them on EPIC-KITCHEN-100 [8,9].The training hyperparameters used during the pre-training are listed in Table 3.For L BB , due to the large number of video frames, we apply a logarithmic scaling to the variance term.The training was conducted on two NVIDIA A800 GPUs taking approximately 30 hours.For hyperparameter sensitivity, we report the model performance under varying numbers of sampled frames and different values of the loss weight λ.As shown in Figure 7, increasing the number of sampled frames leads to higher success rates, likely because it better preserves the temporal ordering and continuity in the video sequence.The model shows low sensitivity to λ, as we observe that L BB converges much faster than L V LO due to its unimodal nature.As a result, L BB primarily serves as a constraint during training rather than a dominant optimization objective.9 Evaluation Details</p>
<p>Simulation Environment</p>
<p>We follow [27] for the specific simulation environment setup and code details.</p>
<p>Franka Kitchen.The Franka Kitchen environment [14,10] is based on the 9 degrees of freedom Franka robot.The Franka robot is placed in a kitchen environment containing several common household items: a microwave, a kettle, an overhead light, cabinets, and an oven.Following [27], the Franka Kitchen environments used in this paper are modified from their original design.Specifically, we introduce additional randomization to the scene by randomly altering the kitchen's position between episodes.This modification makes the tasks significantly more challenging in terms of both perception and control.</p>
<p>Metaworld.The Metaworld environment [39] is an open-source simulated benchmark for robot learning.In our settings, the target object position is randomized between episodes in all tasks.</p>
<p>We present the specific default language instructions for each tasks in Table 4.</p>
<p>Real Robot Environment</p>
<p>Our real robot environment is a real-world office scene where the Unitree D1 robot arm can interact with a cup and a drawer.The pick cup task requires the robot to accurately identify the handle of the cup, while the open/close [X] drawer task requires the robot to understand the drawer index specified in the language instruction and align it with the visual observation.As shown in Figure 8, we use the Unitree Go app interface to remotely control the robotic arm for action data collection.Visual observations are collected using a third-person perspective web camera in a same frequency (20Hz) with action.During control, the whole system, including AcTOL and the policy MLP, runs on a GeForce GTX 880M GPU.</p>
<p>Language-Conditioned Behavior Cloning Hyperparameters</p>
<p>We present the LCBC imitation learning hyperparameters in Table 5.For each distinct task in simulation, we run an evaluation episode every 1,000 gradient steps by running 50 roll-outs and computing their average success rate.Over a total of 10,000 gradient steps, we conduct this evaluation 10 times.The highest success rate among these 10 evaluations is reported as the final result.To ensure robustness, we average the results across two different camera viewpoints and three independent random seeds.In total, we run: 9 (tasks) * 2 (views) * 3 (demosizes) * 3 (seeds) * 6 (models)=972 (episodes), each episode takes approximately 2 hours on our workstation with a 24-core CPU, resulting in a total of roughly 1, 944 hours for the simulated LCBC experiments.For each task on the real robot, we use the final checkpoint and perform 10 evaluation runs with a fixed random seed, due to the cost of real-world policy evaluation.</p>
<p>Linguistic Perturbation Results</p>
<p>To assess the robustness of AcTOL under language perturbations, we perform extensive experiments across four instruction variants.Instructions 1 and 2 transform the original action into more con-     55.0 ± 3.5 93.0 ± 1.0 100.0 ± 0.0 78.7 ± 3.5 AcTOL 93.5 ± 3.4 66.0 ± 2.8 76.5 ± 4.9 88.5 ± 3.9 100.0 ± 0.0 84.9 ± 1.6</p>
<p>searches for the target.Correspondingly, the reward grows gradually.Once the robot interacts with the object and completes the task, our method captures the distinct semantic changes in the action, leading to a rapid reward increase.In Figures 9(e)-(f), we test two complex actions and instructions to explore the limits of our method.In Figure 9(e), the model is required to accurately distinguish between the blue and red cups to complete the task.In Figure 9(f), the model needs to differentiate the orientation and face values of two dice.These scenarios impose high demands on the model's visual and semantic understanding.Our method successfully produces the correct rewards in both tasks, showcasing its potential for application in real-world, complex scenarios.For the proof of Theorem 1, we closely follow the approaches presented in [41] and adapted to our triplet case.We prove the theorem in three steps:</p>
<p>(1) L * := (2) L * is tight, i.e., for any ϵ &gt; 0, there exists representations such that L VLO &lt; L * + ϵ.</p>
<p>(3) For any 0 &lt; δ &lt; 1, there exist ϵ &gt; 0, such that if L VLO &lt; L * + ϵ, then the learned representations satisfy VLO property.</p>
<p>(1) Recall that
L VLO = 1 T T i=1 1 T −1 T j=1,j̸ =i − log exp(R i,j,l ) v k ∈N i,j exp(R i,k,l )
, where N i,j = {v k |k ̸ = i, d i,j &lt; d i,k }, we rewrite it as
L VLO = − 1 T (T − 1) T i=1 j∈[T ]{i} log exp (R i,j,l ) k∈[T ]{i},d i,k ≥di,j exp (R i,k,l ) = − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log exp (R i,j,l ) k∈[T ]{i},d i,k ≥Di,m exp (R i,k,l ) = − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log 1 k∈[T ]{i},d i,k ≥Di,m exp (R i,k,l − R i,j,l ) = − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log 1 k∈[T ]{i},d i,k =Di,m exp (R i,k,l − R i,j,l ) − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log k∈[T ]{i},d i,k =Di,m exp (R i,k,l − R i,j,l ) k∈[T ]{i},d i,k ≥Di,m exp (R i,k,l − R i,j,l ) = − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log exp (R i,j,l ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) + 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log   1 + k∈[T ]{i},d i,k &gt;Di,m exp (R i,k,l − R i,j,l ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l − R i,j,l )    &gt; − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log exp (R i,j,l ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) .(6)
∀i ∈ [T ], m ∈ [M i ], from Jensen's Inequality we have
− j∈[T ]{i},di,j =Di,m log exp (R i,j,l ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) ≥ −n i,m log    1 n i,m j∈[T ]{i},di,j =Di,m exp (R i,j,l ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l )    = n i,m log n i,m .(7)
Thus, by plugging Eq. ( 7) into Eq.( 6), we have
L VLO &gt; 1 T (T − 1) T i=1 Mi m=1 n i,m log n i,m = L ⋆(8)
(2) We will show for ∀ϵ &gt; 0, there is a set of representations where
R i,j,l &gt; R i,k,l + γ if d i,j &lt; d i,k R i,j,l = R i,k,l if d i,j = d i,k
and γ := log
T min i∈[T ],m∈[M i ] ni,mϵ , ∀i ∈ [T ], j, k ∈ [T ]{i}, such that L VLO &lt; L ⋆ + ϵ. For such a set of representations, ∀i ∈ [T ], m ∈ [M i ] , j ∈ {[T ]{i} | d i,j = D i,m }, − log exp (R i,j,l ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) = log n i,m(9)
since R i,k,l = R i,j,l for all k such that d i,k = D i,m = d i,j , and
log 1 + k∈[T ]{i},d i,k &gt;D i,m exp(R i,k,l −R i,j,l ) k∈[T ]{i},d i,k =D i,m exp(R i,k,l −R i,j,l ) &lt; log 1 + T exp(−γ) ni,m &lt; T exp(−γ) ni,m ≤ ϵ.(10)
As R i,k,l − R i,j,l &lt; −γ for all k such that d i,k &gt; D i,m = d i,j and R i,k,l − R i,j,l = 0 for all k such that d i,k = D i,m = d i,j .From Eq. ( 6) we have
L VLO = − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i} di,j =Di,m log exp (R i,j,l ) k∈[T ]{i} d i,k =Di,m exp (R i,k,l ) + 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i} di,j =Di,m log      1 + k∈[T ]{i} d i,k &gt;Di,m exp (R i,k,l − R i,j,l ) k∈[T ]{i} d i,k =Di,m exp (R i,k,l − R i,j,l )     (11)
By plugging Eq. ( 9) and Eq. ( 10) into Eq.( 11) we have
L VLO &lt; 1 T (T − 1) T i=1 Mi m=1 n i,m log n i,m + ϵ = L ⋆ + ϵ(12)
(3) We will show ∀0 &lt; δ &lt; 1, there is a
ϵ = 1 T (T − 1) min min i∈[T ],m∈[Mi] log 1 + 1 n i,m exp δ + 1 δ , 2 log 1 + exp(δ) 2 − δ &gt; 0,
such that when L VLO &lt; L * + ϵ, the representations satisfy VLO property.We first show that
|R i,j,l − R i,k,l | &lt; δ if d i,j = d i,k , i ∈ [T ], j, k ∈ [T ]{i} when L VLO &lt; L * + ϵ.
From Eq. ( 6) we have
L VLO &gt; − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i},di,j =Di,m log exp (R i,j,l ) k∈[T ]{i},d i,k −Di,m exp (R i,k,l )(13)
Let p i,m := arg min
j∈[T ]{i},di,j =Di,m R i,j,l , q i,m := arg max j∈[T ]{i},di,j =Di,m R i,j,l , ζ i,m := R i,pi,m,l , η i,m := s i,qi,m,l − s i,pi,m,l , ∀i ∈ [T ], m ∈ [M i ],
by splitting out the maximum term and the minimum term we have
L VLO &gt; − 1 T (T − 1) T i=1 Mi m=1      log exp (ζ i,m ) k∈[T ]{i},di,i=Di,m exp (R i,k,l ) + log exp (ζ i,m + η i,m ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) + log exp j∈[T ]{i,pi,m,qi,m},di,j =Di,m R i,j,l k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) ni,m−2              . (14) Let θ i,m := 1 ni,m−2 j∈[T ]{i,pi,m,qi,m},di,j =Di,j exp (R i,j,l − ζ i,m ), we have − log exp (ζ i,m ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) = log (1 + exp (η i,m ) + (n i,m − 2) θ i,m )(15)
and
− log exp (ζ i,m + η i,m ) k∈[T ]{i},d i,k =Di,m exp (R i,k,l ) = log (1 + exp (η i,m ) + (n i,m − 2) θ i,m ) − η i,m(16)
Then, from Jensen's inequality, we know
exp j∈[T ]{i,pi,m,qi,m} di,j =Di,m R i,j,l ≤     1 n i,m − 2 j∈[T ]{i,pi,m,qi,m} di,j =Di,m exp(R i,j,l )     ni,m−2(17)
thus
− log exp j∈[T ]{i,pi,m,qi,m} di,j =Di,m R i,j,l k∈[T ]{i} d i,k =Di,m exp(R i,k,l ) ni,m−2 ≥ (n i,m − 2) log (1 + exp(η i,m ) + (n i,m − 2)θ i,m ) − (n i,m − 2) log(θ i,m )(18)
By plugging Eq. ( 15), Eq. ( 16) and Eq. ( 18) into Eq.( 14), we have , exp (η i,m ) , thus
L VLO &gt; 1 T (T − 1) T i=1 Mi m=1 n i,m log (1 + exp(η i,m ) + (n i,m − 2)θ i,m ) − η i,m − (n i,m − 2) log(θ i,m ) (19) Let h(θ) := n i,m log (1 + exp (η i,m ) + (n i,m − 2) θ) − η i,m − (n i,m − 2) log(θ).h(θ) ≥ h 1 + exp (η i,m ) 2 = n i,m log n i,m + 2 log 1 + exp (η i,m ) 2 − η i,m .(20)
By plugging Eq. ( 20) into Eq.( 19), we have
L VLO &gt; 1 T (T − 1) T i=1 Mi m=1 n i,m log n i,m + 2 log 1 + exp (η i,m ) 2 − η i,m = L ⋆ + 1 T (T − 1) T i=1 Mi m=1 2 log 1 + exp (η i,m ) 2 − η i,m(21)
Then, since η i,m ≥ 0, we have 2 log
1+exp(ηi,m) 2 − η i,m ≥ 0. Thus, ∀i ∈ [T ], m ∈ [M i ], L VLO &gt; L ⋆ + 1 T (T − 1) 2 log 1 + exp (η i,m ) 2 − η i,m(22)
If
L VLO &lt; L ⋆ + ϵ ≤ L ⋆ + 1 T (T −1) 2 log 1+exp(δ) 2 − δ , then 2 log 1 + exp (η i,m ) 2 − η i,m &lt; 2 log 1 + exp(δ) 2 − δ(23)
Since y(x) = 2 log 1+exp(x) 2 − x increases monotonically when x &gt; 0, we have η i,m &lt; δ.Hence 6) we have
∀i ∈ [T ], j, k ∈ [T ]{i}, if d i,j = d i,k = D i,m , |R i,j,l − R i,k,l | ≤ η i,m &lt; δ. Next, we show R i,j,l &gt; R i,k,l + δ if d i,j &lt; d i,k when L VLO &lt; L ⋆ + ϵ. From Eq. (L VLO = − 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i} di,j =Di,m log exp(R i,j,l ) k∈[T ]{i} d i,k =Di,m exp(R i,k,l ) + 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i} di,j =Di,m log      1 + k∈[T ]{i} d i,k =Di,m exp(R i,k,l − R i,j,l ) k∈[T ]{i} d i,k &gt;Di,m exp(R i,k,l − R i,j,l )     (24)
and combining it with Eq. ( 7) we have
L VLO ≥ L ⋆ + 1 T (T − 1) T i=1 Mi m=1 j∈[T ]{i} di,j =Di,m log      1 + k∈[T ]{i} d i,k &gt;Di,m exp(R i,k,l − R i,j,l ) k∈[T ]{i} d i,k =Di,m exp(R i,k,l − R i,j,l )      &gt; L ⋆ + 1 T (T − 1) log      1 + exp(R i,k,l − R i,j,l ) h∈[T ]{i} d i,h =di,j exp(R i,h,l − R i,j,l )     (25)
Since the mean trajectory E[B(t)] is linear within the interval [n(i), n(j)], we have:
∥E[B(k)] − E[B(l)]∥ 2 ≤ |k − l| n(j) − n(i) ∥v j − v i ∥ 2 .
Combining these bounds, now we can rewrite into the following inequality:
∥v k − v l ∥ 2 ≤ |k − l| n(j) − n(i)
∥v j − v i ∥ 2 + (k − n(i))(n(j) − k) n(j) − n(i) + (l − n(i))(n(j) − l) n(j) − n(i) .</p>
<p>For the variance terms, the Brownian Bridge process achieves its maximum variance at the midpoint t = n(i)+n(j)</p>
<p>2</p>
<p>. This gives us,
Var[B(t max) ] = n(j) − n(i) 4 , ∥v k − v l ∥ 2 ≤ 2 |k − l| n(j) − n(i)
+ (n(j) − n(i)).</p>
<p>Bounding Semantic Alignment Score.Finally, by substituting this bound into the Lipschitz continuity of sim, we obtain,
|R(v k , v l , l)| ≤ C • 2|k − l| n(j) − n(i)
+ (n(j) − n(i)) .</p>
<p>To ensure |R(v k , v l , l)| &lt; ϵ, we require:
C • 2 |k − l| n(j) − + n(j) − n(i) &lt; ϵ.
Here, we consider these two terms respectively:
2C |k − l| n(j) − n(i) &lt; ϵ 2 , C n(j) − n(i) &lt; ϵ 2 ,
which gives:
|k − l| &lt; δ 1 = ϵ • (n(j) − n(i)) 4C , n(j) − n(i) &lt; ϵ 2C 2 .
Combining these conditions, we choose:
δ = min ϵ • (n(j) − n(i)) 4C , ϵ 2 4C 2 .
Final Conclusion.For any given ϵ &gt; 0, setting δ = min ϵ•(n(j)−n(i))</p>
<p>4C</p>
<p>, ϵ 2 4C 2 ensures:
∥v k − v l ∥ 2 &lt; δ ⇒ |R(v k , v l , l)| &lt; ϵ.</p>
<p>Proofs of Theorem 3</p>
<p>From the definition of the semantic alignment score, we have:
R(v i , v j , l) = −| sim(v i , l) − sim(v j , l)|, R(v i , v j , l ′ ) = −| sim(v i , l ′ ) − sim(v j , l ′ )|.
The difference in scores can be bounded using the reverse triangle inequality:
|R(v i , v j , l ′ ) − R(v i , v j , l)| ≤ |(sim(v i , l ′ ) − sim(v j , l ′ )) − (sim(v i , l) − sim(v j , l))|.
Simplifying the inequalities above, it gives us:
|R(v i , v j , l ′ ) − R(v i , v j , l)| ≤ | sim(v i , l ′ ) − sim(v i , l)| + | sim(v j , l ′ ) − sim(v j , l)|.
By the Lipschitz continuity of sim, we have: for some constant C &gt; 0,
| sim(v i , l ′ ) − sim(v i , l)| ≤ C∥l ′ − l∥ 2 , | sim(v j , l ′ ) − sim(v j , l)| ≤ C∥l ′ − l∥ 2 .
Substituting these bounds and considering ∥l ′ − l∥ 2 ≤ δ l
|R(v i , v j , l ′ ) − R(v i , v j , l)| ≤ 2C∥l ′ − l∥ 2 ≤ 2Cδ l . (30)
11 Broader Impacts and Limitations Broader Impacts.We introduce Action Temporal Coherence Learning (AcTOL), a vision-language pretraining framework aimed at improving the generalization capabilities of embodied agents in a variety of manipulation tasks.By learning from large-scale human action videos, AcTOL helps agents acquire temporally consistent representations aligned with natural language, which can support more flexible and data-efficient robotic learning.However, some potential risks should be acknowledged.If AcTOL is trained on video data that contains societal biases or stereotypes, those patterns may be reflected in the model's behavior.For if certain groups or actions are underrepresented or portrayed inaccurately, the resulting agents could behave in ways that are inappropriate or unreliable in diverse real-world settings.While these challenges are common across many data-driven systems in robotics and vision-language learning, we believe future work should explore strategies such as dataset auditing, fairness-aware training, and improved transparency to support more responsible and robust deployment.</p>
<p>Limitations.Our proposed method presents several limitations.First, while the temporal ordering of actions provides a strong inductive bias for many goal-directed tasks, it may not align well with tasks that involve ambiguous, repetitive, or cyclic behaviors.In such cases, the assumption of coherent progression can break down, potentially affecting the reliability of the model.Future work could explore adapting AcTOL to handle such repetitive action sequences.Second, since AcTOL is trained on human action videos, it may face a distribution gap when applied to real-world robotic tasks.The dynamics of human actions in videos may differ from robotic interactions in physical environments, especially considering the visual differences between human hands and robot arms.These differences in dexterity, size, and appearance could lead to challenges in transferring the learned representations from human demonstrations to robotic execution.Future work could focus on bridging this gap by incorporating robotic-specific data or fine-tuning the model with real-world robotic demonstrations to improve its transferability.</p>
<p>Figure 1 :
1
Figure 1: Pretraining on Internet human action videos for robot control, where the video-instruction pairs are noisy and often include irrelevant frames.The red vision-language reward curve demonstrates AcTOL learns to correctly align instruction with action, outperforming previous goal-reaching methods in the presence of distracting content.</p>
<p>Figure 2 :
2
Figure 2: Comparison of existing goal-reaching pre-training strategies and the proposed AcTOL approach.Our learned multi-modal representations can be effectively transferred to downstream language-conditioned robot manipulation tasks, exhibiting robustness to diverse instruction and linguistic variations.</p>
<p>Figure 3 :
3
Figure 3: Policy learning environments, including 3 tasks with a real-world Unitree D1 robot arm and 5 tasks each in two simulation environments, i.e., Franka Kitchen and Metaworld.</p>
<p>Figure 4 :
4
Figure 4: Visualization of the normalized learned reward corresponding to different actions.Our representations effectively help capture the correct temporal order of actions in the instruction.For more results, please refer to Appendix 9.6.</p>
<p>Figure 5 :
5
Figure 5: Visual trajectory visualization.</p>
<p>Figure 4 (
4
a) presents an in-distribution evaluation using a video from EPIC-KITCHEN-100.Our model produces a clear reward peak aligned with the completion of the "open cupboard" action, followed by a decline-indicating successful temporal localization of the instructed behavior.In contrast, R3M and DecisionNCE rewards continue increasing beyond the relevant action segment.</p>
<p>Figure 6 :
6
Figure 6: Success rate fluctuation across tasks in Franka Kitchen for different instruction variants.</p>
<p>Figure 7 :
7
Figure 7: Hyper-parameters sensitivity.</p>
<p>Table 4 :Figure 8 :
48
Figure 8: Action space of Unitree D1 arm and the remote control interface on Unitree Go app.</p>
<p>7 ± 3 . 8
738
100.0 ± 0.0 55.7 ± 2.8 AcTOL w/o BB 66.8 ± 1.4 39.0 ± 16.8 20.7 ± 1.5 74.7 ± 1.5 100.0 ± 0.0 60.2 ± 5.1 AcTOL 62.8 ± 6.0 41.0 ± 6.3 42.0 ± 4.5 69.5 ± 0.7 100.0 ± 0.0 63.1 ± 3.9</p>
<p>Figure 9 :
9
Figure 9: Reward plots for exemplar robot action videos.</p>
<p>m log n i,m is a lower bound of L VLO , i.e., L VLO &gt; L * .</p>
<p>From derivative analysis we know h(θ) decreases monotonically when θ ∈ 1,</p>
<p>Table 1 :
1Method5 demosFRANKA KITCHEN 15 demos25 demos5 demosMETAWORLD 15 demos25 demosCLIP11.67 ± 0.95 27.47 ± 1.01 31.20 ± 2.62 42.29 ± 2.65 60.33 ± 1.32 62.54 ± 4.36R3M28.60 ± 1.39 42.20 ± 1.00 51.13 ± 2.83 46.83 ± 3.85 56.50 ± 5.20 60.08 ± 3.62LIV23.40 ± 0.78 42.73 ± 1.17 51.93 ± 0.95 46.95 ± 2.07 64.33 ± 3.63 66.67 ± 1.49DecisionNCE25.33 ± 1.30 43.20 ± 2.25 50.87 ± 2.95 44.58 ± 2.79 59.08 ± 1.77 69.75 ± 3.90AcTOL w/o BB 32.80 ± 1.23 54.20 ± 0.85 60.80 ± 0.87 50.29 ± 4.05 70.83 ± 4.21 73.33 ± 2.83AcTOL42.60 ± 0.53 61.80 ± 2.54 64.60 ± 0.57 53.81 ± 3.89 74.13 ± 1.59 81.13 ± 1.59(+48.95%)(+43.06%)(+24.40%)(+14.61%)(+15.23%)(+16.32%)
Comparison in simulation environments with varying amounts of demonstrations.Each result reports the success rate over 50 roll-outs, averaged across 2 camera views and 3 random seeds.We also report the relative performance gain in green compared to the strongest baseline.</p>
<p>Table 2 :
2
Performance comparison on Unitree D1 arm.Success rates are reported over 10 trials.
MethodPick Cup Open [X] Drawer Close [X] DrawerCLIP0%20%30%R3M10%40%40%LIV20%30%50%DecisionNCE20%40%60%AcTOL50%80%90%
ronments and dataset sizes, averaged over camera views and seeds.Detailed comparison results for each task can be referred to Appendix 9.5.Table1presents the comparison results, demonstrating that AcTOL achieves significantly enhanced performance relative to baseline methods across all evaluated datasets and environments.This superiority is particularly pronounced in the complex</p>
<p>Table 3 :
3
Hyper-parameters for pre-training.
ConfigValueTraining epochs1000OptimizerAdamLearning rate1 × 10 −5Batch size128Frames per video10Loss weight λ0.1Weight decay0.001Momentum (β1, β2) 0.9, 0.999AugmentationRandomCropResize</p>
<p>Table 5 :
5
Hyper-parameters for LCBC.
Franka Kitchen Metaworld Real robotMLP achitecture[256,256][256,256][256,256]Non-linear activationReLUReLUReLUOptimizerAdamAdamAdamGradient Steps10K10K50KLearning rate1 × 10 −31 × 10 −31 × 10 −3Batch size323232Horizon50100100Proprioception94No</p>
<p>Table 7 :
7
LCBC results when dataset size= 5 on Franka Kitchen.
MethodSlide Cabinet Open Left Door Open Microwave Turn On Stove Switch On LightAverageCLIP38.7 ± 5.12.0 ± 1.03.0 ± 0.07.0 ± 2.67.7 ± 1.511.7 ± 0.9R3M68.7 ± 0.618.3 ± 4.07.7 ± 3.219.3 ± 7.629.0 ± 6.128.6 ± 1.4LIV55.0 ± 1.06.0 ± 2.97.0 ± 0.613.0 ± 0.622.0 ± 2.620.6 ± 0.7DecisionNCE59.3 ± 6.89.7 ± 1.57.0 ± 2.026.3 ± 4.524.3 ± 2.525.3 ± 1.3AcTOL w/o BB71.5 ± 3.511.5 ± 0.710.5 ± 0.723.5 ± 6.447.0 ± 4.232.8 ± 2.8AcTOL85.5 ± 0.720.0 ± 2.118.3 ± 4.924.7 ± 4.962.3 ± 2.842.6 ± 0.3</p>
<p>Table 8 :
8
LCBC results when dataset size= 15 on Franka Kitchen.
MethodSlide Cabinet Open Left Door Open Microwave Turn On Stove Switch On LightAverageCLIP71.0 ± 3.68.0 ± 2.015.7 ± 2.114.7 ± 0.628.0 ± 1.027.5 ± 1.0R3M81.0 ± 1.031.0 ± 1.722.0 ± 2.619.3 ± 4.757.7 ± 3.842.2 ± 1.0LIV85.0 ± 5.619.0 ± 3.028.3 ± 2.929.7 ± 3.551.7 ± 2.342.7 ± 1.2DecisionNCE92.0 ± 6.618.7 ± 4.527.0 ± 4.033.3 ± 3.545.0 ± 7.543.2 ± 2.3AcTOL w/o BB84.5 ± 3.529.5 ± 0.729.5 ± 2.154.0 ± 2.873.5 ± 2.154.2 ± 0.8AcTOL99.5 ± 0.737.5 ± 5.637.0 ± 4.253.5 ± 3.581.5 ± 2.161.8 ± 2.5</p>
<p>Table 9 :
9
LCBC results when dataset size= 25 on Franka Kitchen.
MethodSlide Cabinet Open Left Door Open Microwave Turn On Stove Switch On LightAverageCLIP66.3 ± 7.58.7 ± 1.218.7 ± 1.523.7 ± 3.138.7 ± 2.331.2 ± 2.6R3M84.7 ± 6.835.3 ± 4.040.0 ± 1.034.0 ± 5.361.7 ± 10.751.1 ± 2.8LIV91.7 ± 5.926.0 ± 2.635.0 ± 4.645.3 ± 0.661.7 ± 3.251.9 ± 0.9DecisionNCE91.7 ± 1.527.0 ± 10.437.0 ± 1.747.3 ± 1.251.3 ± 4.050.9 ± 2.9AcTOL w/o BB92.0 ± 2.437.0 ± 5.440.0 ± 2.457.0 ± 1.578.0 ± 6.260.8 ± 1.3AcTOL100.0 ± 0.037.0 ± 7.142.5 ± 2.162.5 ± 2.181.0 ± 4.264.6 ± 0.6</p>
<p>Table 10 :
10
LCBC results when dataset size= 5 on Metaworld.
MethodAssemblyPick binPress buttonHammerOpen drawerAverageCLIP48.3 ± 5.735.3 ± 2.334.3 ± 4.951.2 ± 2.891.0 ± 1.052.0 ± 2.7R3M63.5 ± 5.633.3 ± 5.127.3 ± 5.163.2 ± 7.192.3 ± 0.655.9 ± 3.9LIV61.8 ± 6.532.3 ± 9.</p>
<p>Table 11 :
11
LCBC results when dataset size= 15 on Metaworld.
MethodAssemblyPick binPress buttonHammerOpen drawerAverageCLIP73.0 ± 7.840.3 ± 5.552.0 ± 7.976.0 ± 5.096.7 ± 0.667.6 ± 1.5R3M80.7 ± 7.6 17.0 ± 12.345.0 ± 4.683.3 ± 4.594.0 ± 1.064.0 ± 5.2LIV84.3 ± 2.537.0 ± 8.754.7 ± 3.881.3 ± 5.9 100.0 ± 0.0 71.4 ± 3.6DecisionNCE73.3 ± 10.8 36.7 ± 5.043.3 ± 2.183.0 ± 6.0 100.0 ± 0.0 67.3 ± 1.8AcTOL w/o BB 94.0 ± 3.0 50.3 ± 18.648.3 ± 1.590.7 ± 1.2 100.0 ± 0.0 76.7 ± 5.3AcTOL82.5 ± 0.764.5 ± 3.265.5 ± 3.984.0 ± 2.1 100.0 ± 0.0 79.3 ± 1.6</p>
<p>Table 12 :
12
LCBC results when dataset size= 25 on Metaworld.
MethodAssemblyPick binPress buttonHammerOpen drawerAverageCLIP69.3 ± 5.7 36.0 ± 11.866.0 ± 2.578.8 ± 4.999.3 ± 0.669.9 ± 4.4R3M87.7 ± 2.4 14.7 ± 11.648.3 ± 2.189.7 ± 3.5 100.0 ± 0.0 68.1 ± 3.6LIV87.3 ± 5.523.7 ± 6.866.0 ± 6.889.7 ± 2.5 100.0 ± 0.0 73.3 ± 1.5DecisionNCE85.7 ± 4.9 47.0 ± 12.858.0 ± 7.888.3 ± 6.7 100.0 ± 0.0 75.8 ± 3.9AcTOL w/o BB 93.7 ± 0.6 51.7 ± 11.9
versational forms.Instruction 3 introduces vocabulary diversity by varying the verbs and nouns used.Instruction 4 further extends Instruction 3 by incorporating linguistically complex expressions generated using ChatGPT-4o.We present the comparison results obtained from experiments in the Franka Kitchen environment, with a data size of 5.As shown in Table6, AcTOL outperforms the baselines in most instruction perturbation scenarios, thereby validating its robustness.Language-Conditioned Behavior Cloning ResultsIn Table7-12, we report detailed Language-Conditioned Behavior Cloning results for different task and dataset size.The results demonstrate that our method achieves significant improvements across different simulation environments, varying dataset sizes, and diverse robotic manipulation tasks.Language-Conditioned Visual Reward ResultsAs shown in Figure9, we present more visualizations of Language-Conditioned Visual Reward on real-world robot manipulation videos from[2].In Figure9(a), the robot performs two consecutive and opposing actions.Our method effectively identifies the action boundaries and generates the correct reward sequence, increasing first and then decreasing, in alignment with the given instructions.In Figures9(b)-(d), where the robot performs a single action, the robot initially moves slowly as it, which derives R i,h,l − R i,j,l &lt; δ and thus exp (R i,h,l − R i,j,l ) &lt; exp(δ).By putting this into Eq.(24), we have ∀i ∈where r i,j ∈ [M i ] is the index such that D i,ri,j = d i,j .Further, given, we haveProofs of Theorem 2Setup and Assumptions.To provide the vision-language continuity, we first assume that the frame embeddings {v t }, where t ∈ [1, T ] are regularized under a Brownian Bridge process B(t) as discussed in Section 3.2, where the transition density for any intermediate time t ∈ [n(i), n(j)] within a sampled interval is given as:with:All time steps t ∈ [1, T ] are covered by at least one sampled interval, ensuring the entire video sequence satisfies the Brownian Bridge regularization.Now, let v k , v l ∈ R d be arbitrary embeddings, not necessarily the endpoints v i and v j of a sampled interval.These embeddings fall within the union U of all sampled local intervals.Without loss of generality, here we can identify the interval [n(i), n(j)] ∈ U from the union containing v k and v l .Bounding Local Continuity.Recall that semantic alignment score R(v k , v l , l) is defined as:where sim(•) is Lipschitz continuous with constant C &gt; 0 when embeddings are normalized as unit vectors.By the Lipschitz continuity of sim(•), we have:To ensure the continuity of R, we must bound ∥v k − v l ∥ 2 .Under the Brownian Bridge regularization, the embeddings are aligned with the mean trajectory E[B(t)], and deviations are constrained by the variance Var[B(t)].Specifically:where λ &gt; 0 depends on the strength of the Brownian Bridge loss L BB .Below we omit λ for simplicty.Substituting the variance:Var[B(t)] = (t − n(i))(n(j) − t) n(j) − n(i) .Bounding Pairwise Distance.The total pairwise distance between v k and v l can be expressed as:
Hindsight experience replay. Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, 2017</p>
<p>Human-to-robot imitation in the wild. Shikhar Bahl, Abhinav Gupta, Deepak Pathak, Robotics: Science and Systems XVIII. Kris Hauser, Dylan A Shell, Shoudong Huang, New York City, NY, USAJune 27 -July 1, 2022, 2022</p>
<p>A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky, CoRR, abs/2410.2416420240</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Ilya Sutskever, and Dario Amodei</p>
<p>GR-2: A generative video-language-action model with web-scale knowledge for robot manipulation. Chilam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, Minzhao Zhu, CoRR, abs/2410.061582024</p>
<p>X Open, Embodiment CollaborationAcorn Abhishek Padalkar, Embodiment CollaborationAjinkya Pooley, Embodiment CollaborationAlex Jain, Embodiment CollaborationAlexander Bewley, Embodiment CollaborationAlex Herzog, Embodiment CollaborationAlexander Irpan, Embodiment CollaborationAnant Khazatsky, Embodiment CollaborationAnikait Raj, Embodiment CollaborationAnthony Singh, Embodiment CollaborationAntonin Brohan, Embodiment CollaborationAyzaan Raffin, Embodiment CollaborationBen Wahid, Embodiment CollaborationBeomjoon Burgess-Limerick, Embodiment CollaborationBernhard Kim, Embodiment CollaborationBrian Schölkopf, Embodiment CollaborationCewu Ichter, Embodiment CollaborationCharles Lu, Embodiment CollaborationChelsea Xu, Embodiment CollaborationChenfeng Finn, Embodiment CollaborationCheng Xu, Embodiment CollaborationChenguang Chi, Embodiment CollaborationChristine Huang, Embodiment CollaborationChuer Chan, Embodiment CollaborationChuyuan Pan, Embodiment CollaborationColine Fu, Embodiment CollaborationDanny Devin, Embodiment CollaborationDeepak Driess, Embodiment CollaborationDhruv Pathak, Embodiment CollaborationDieter Shah, Embodiment CollaborationDmitry Büchler, Embodiment CollaborationDorsa Kalashnikov, Embodiment CollaborationEdward Sadigh, Embodiment CollaborationFederico Johns, Embodiment CollaborationFei Ceola, Embodiment CollaborationFreek Xia, Embodiment CollaborationGaoyue Stulp, Embodiment CollaborationGaurav S Zhou, Embodiment CollaborationGautam Sukhatme, Embodiment CollaborationGe Salhotra, Embodiment CollaborationGiulio Yan, Embodiment CollaborationGregory Schiavi, Embodiment CollaborationHao Kahn, Embodiment CollaborationHaoshu Su, Embodiment CollaborationHaochen Fang, Embodiment CollaborationHeni Shi, Embodiment CollaborationHenrik I Ben Amor, Embodiment CollaborationHiroki Christensen, Embodiment CollaborationHomer Furuta, Embodiment CollaborationWalke, Embodiment CollaborationCoRR, abs/2310.08864Hongjie Fang, Igor Mordatch, Ilija Radosavovic, and et al. Open x-embodiment: Robotic learning datasets and RT-X models. 2023</p>
<p>Can foundation models perform zero-shot task specification for robot manipulation?. Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, Aravind Rajeswaran, Learning for Dynamics and Control Conference. Roya Firoozi, Negar Mehr, Esen Yel, Rika Antonova, Jeannette Bohg, Mac Schwager, Mykel J Kochenderfer, PMLR2022168of Proceedings of Machine Learning Research</p>
<p>Scaling egocentric vision: The EPIC-KITCHENS dataset. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray, CoRR, abs/1804.027482018</p>
<p>. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray, 2006.13256. 2020</p>
<p>D4RL: datasets for deep data-driven reinforcement learning. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine, CoRR, abs/2004.072192020</p>
<p>Octo: An open-source generalist robot policy. Dibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Quan Vuong, Ted Xiao, R Pannag, Dorsa Sanketi, Chelsea Sadigh, Sergey Finn, Levine, Robotics: Science and Systems XX. Dana Kulic, Gentiane Venture, Kostas E Bekris, Enrique Coronado, Delft, The NetherlandsJuly 15-19, 2024, 2024</p>
<p>The "something something" video database for learning and evaluating visual common sense. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fründ, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic, IEEE International Conference on Computer Vision, ICCV 2017. Venice, ItalyIEEE Computer SocietyOctober 22-29, 2017. 2017</p>
<p>Ego4d: Around the world in 3, 000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina González, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Kolár, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Lorenzo Torresani, Mingfei Yan, Jitendra Malik, Pablo Arbeláez, David Crandall; Richard A. Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio TorralbaIEEE2022</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol Hausman, of Proceedings of Machine Learning Research. Leslie Pack Kaelbling, Danica Kragic, Komei Sugiura, PMLR20191003rd Annual Conference on Robot Learning</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition, (CVPR). IEEE Computer Society2016</p>
<p>Contrast and order representations for video self-supervised learning. Kai Hu, Jie Shao, Yuan Liu, Bhiksha Raj, Marios Savvides, Zhiqiang Shen, 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaIEEEOctober 10-17, 2021. 2021</p>
<p>Language-driven representation learning for robotics. Siddharth Karamcheti, RSSSuraj Nair, RSSAnnie S Chen, RSSThomas Kollar, RSSChelsea Finn, RSSDorsa Sadigh, RSSPercy Liang, RSSRobotics: Science and Systems XIX. Kostas E Bekris, Kris Hauser, Sylvia L Herbert, Jingjin Yu, 2023</p>
<p>Simple but effective: CLIP embeddings for embodied AI. Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, Aniruddha Kembhavi, IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE2022</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Paul Rafailov, Grace Foster, Pannag Lam, Quan Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, CoRR, abs/2406.092462024</p>
<p>Decisionnce: Embodied multimodal representations via implicit preference learning. Jianxiong Li, Jinliang Zheng, Yinan Zheng, Liyuan Mao, Xiao Hu, Sijie Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan, Forty-first International Conference on Machine Learning. 2024OpenReview.net</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, 2023</p>
<p>Aligning cyber space with physical world: A comprehensive survey on embodied AI. Yang Liu, Weixing Chen, Yongjie Bai, Guanbin Li, Wen Gao, Liang Lin, CoRR, abs/2407.068862024</p>
<p>LIV: language-image representations and rewards for robotic control. Jason Yecheng, Vikash Ma, Amy Kumar, Osbert Zhang, Dinesh Bastani, Jayaraman, International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, PMLR2023202of Proceedings of Machine Learning Research</p>
<p>VIP: towards universal visual reward and representation via value-implicit pretraining. Jason Yecheng, Shagun Ma, Dinesh Sodhani, Osbert Jayaraman, Vikash Bastani, Amy Kumar, Zhang, The Eleventh International Conference on Learning Representations. ICLR). OpenReview.net. 2023</p>
<p>Where are we in the search for an artificial visual cortex for embodied intelligence?. Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Yecheng, Claire Ma, Sneha Chen, Aryan Silwal, Vincent-Pierre Jain, Tingfan Berges, Jay Wu, Pieter Vakil, Jitendra Abbeel, Dhruv Malik, Yixin Batra, Oleksandr Lin, Aravind Maksymets, Franziska Rajeswaran, Meier, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, 2023</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, Advances in Neural Information Processing Systems (NeurIPS). Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, 2023</p>
<p>R3M: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, of Proceedings of Machine Learning Research. Karen Liu, Dana Kulic, Jeffrey Ichnowski, PMLR2022205Conference on Robot Learning</p>
<p>Gpt-4o technical report. 2024OpenAI</p>
<p>Spatialvla: Exploring spatial representations for visual-language-action model. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, Xuelong Li, CoRR, abs/2501.158302025</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning. Marina Meila, Tong Zhang, the 38th International Conference on Machine LearningPMLR2021139of Proceedings of Machine Learning Research</p>
<p>Real-world robot learning with masked visual pre-training. Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, Trevor Darrell, of Proceedings of Machine Learning Research. Karen Liu, Dana Kulic, Jeffrey Ichnowski, PMLR2022205Conference on Robot Learning</p>
<p>Continuous martingales and Brownian motion. Daniel Revuz, Marc Yor, Springer Science &amp; Business Media. 2932013</p>
<p>Time-contrastive networks: Self-supervised learning from video. Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, IEEE International Conference on Robotics and Automation. 2018. 2018IEEEICRA)</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, of Proceedings of Machine Learning Research. Aleksandra Faust, David Hsu, Gerhard Neumann, PMLR2021164Conference on Robot Learning</p>
<p>Learning by sorting: Self-supervised learning with group ordering constraints. Nina Shvetsova, Felix Petersen, Anna Kukleva, Bernt Schiele, Hilde Kuehne, IEEE/CVF International Conference on Computer Vision, ICCV 2023. Paris, FranceIEEEOctober 1-6, 2023. 2023</p>
<p>Grounding multimodal large language models in actions. Andrew Szot, Bogdan Mazoure, R Devon Harsh Agrawal, Zsolt Hjelm, Alexander Kira, Toshev, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, M Jakub, Cheng Tomczak, Zhang, NeurIPS; Vancouver, BC, Canada2024. 2024. December 10 -15, 2024, 2024</p>
<p>Semantic exploration from language abstractions and pretrained representations. Allison C Tam, Neil C Rabinowitz, Andrew K Lampinen, Nicholas A Roy, Stephanie C Y Chan, Jane Strouse, Andrea Wang, Felix Banino, Hill, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, 2022</p>
<p>Latent action pretraining from videos. Seonghyeon Ye, Joel Jang, Byeongguk Jeon, June Se, Jianwei Joo, Baolin Yang, Ajay Peng, Reuben Mandlekar, Yu-Wei Tan, Bill Chao, Lars Yuchen Lin, Kimin Liden, Jianfeng Lee, Luke Gao, Dieter Zettlemoyer, Minjoon Fox, Seo, CoRR, abs/2410.117582024</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, of Proceedings of Machine Learning Research. Leslie Pack Kaelbling, Danica Kragic, Komei Sugiura, PMLR20191003rd Annual Conference on Robot Learning</p>
<p>Learning manipulation by predicting interaction. Jia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Chen, Hao Dong, Haoming Song, Dong Wang, Di Hu, Ping Luo, Heming Cui, Bin Zhao, Xuelong Li, Yu Qiao, Hongyang Li, CoRR, abs/2406.004392024</p>
<p>Rank-n-contrast: Learning continuous representations for regression. Kaiwen Zha, Peng Cao, Jeany Son, Yuzhe Yang, Dina Katabi, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS), LA, USADecember 10 -16, 2023, 2023</p>
<p>Universal visual decomposer: Long-horizon manipulation made easy. Zichen Zhang, Yunshuang Li, Osbert Bastani, Abhishek Gupta, Dinesh Jayaraman, Yecheng , Jason Ma, Luca Weihs, IEEE International Conference on Robotics and Automation, ICRA 2024. Yokohama, JapanIEEEMay 13-17, 2024. 2024</p>
<p>RT-2: vision-language-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, R Pannag, Grecia Sanketi, Michael S Salazar, Krista Ryoo, Kanishka Reymann, Karl Rao, Igor Pertsch, Henryk Mordatch, Yao Michalewski, Sergey Lu, Lisa Levine, Tsang-Wei Edward Lee, Isabel Lee, Yuheng Leal, Dmitry Kuang, Ryan Kalashnikov, Nikhil J Julian, Alex Joshi, Brian Irpan, Jasmine Ichter, Alexander Hsu, Karol Herzog, Keerthana Hausman, Chuyuan Gopalakrishnan, Pete Fu, Chelsea Florence, Finn, Avinava Kumar, Danny Dubey, Tianli Driess, Ding, Marcin Krzysztof, Xi Choromanski, Chen, of Proceedings of Machine Learning Research. Jie Tan, Marc Toussaint, Kourosh Darvish, PMLR2023229Conference on Robot Learning</p>            </div>
        </div>

    </div>
</body>
</html>
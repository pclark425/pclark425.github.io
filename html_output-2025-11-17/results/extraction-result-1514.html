<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1514 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1514</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1514</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-cda5d891ae6262c55c8711ef5b18ccc1fe748d7a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cda5d891ae6262c55c8711ef5b18ccc1fe748d7a" target="_blank">iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes</a></p>
                <p><strong>Paper Venue:</strong> IEEE/RJS International Conference on Intelligent RObots and Systems</p>
                <p><strong>Paper TL;DR:</strong> It is shown that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of human demonstrated (mobile) manipulation behaviors.</p>
                <p><strong>Paper Abstract:</strong> We present iGibson 1.0, a novel simulation environment to develop robotic solutions for interactive tasks in large-scale realistic scenes. Our environment contains 15 fully interactive home-sized scenes with 108 rooms populated with rigid and articulated objects. The scenes are replicas of real-world homes, with distribution and the layout of objects aligned to those of the real world. iGibson 1.0 integrates several key features to facilitate the study of interactive tasks: i) generation of high-quality virtual sensor signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain randomization to change the materials of the objects (both visual and physical) and/or their shapes, iii) integrated sampling-based motion planners to generate collision-free trajectories for robot bases and arms, and iv) intuitive human-iGibson interface that enables efficient collection of human demonstrations. Through experiments, we show that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks. We also show that iGibson features enable the generalization of navigation agents, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of human demonstrated (mobile) manipulation behaviors. iGibson 1.0 is open-source, equipped with comprehensive examples and documentation. For more information, visit our project website: http://svl.stanford.edu/igibson/.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1514.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1514.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iGibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iGibson 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, high‑fidelity simulation environment for training embodied agents on navigation, manipulation and mobile-manipulation in large, realistic, fully-interactive indoor scenes with PBR rendering, LiDAR, domain randomization and integrated motion planners.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>iGibson (custom environment built on physics engines)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A simulation environment that integrates a physics simulator (PyBullet by default), a physics-based renderer (PBR), high-quality virtual sensors (RGB, depth, LiDAR, flow, segmentation), 15 fully interactive real-world-derived scenes (108 rooms) plus compatibility with thousands more layouts, domain randomization for visuals/dynamics, articulated object models and integrated sampling-based motion planners.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied robotics / perception (robot navigation and manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity for embodied robotics: continuous rigid-body and articulated-object physics, realistic sensor simulation (PBR, LiDAR), and per-part material/dynamics properties; not a fluid/thermodynamics or circuit simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes continuous rigid-body dynamics, articulated joints, collision meshes, center-of-mass and inertial properties, friction/dynamics tied to material annotations, physics timestep 1/120s, PBR with roughness/metallic/normal maps, LiDAR noise dropout model, integrated motion planners; does not model fluids/thermodynamics/electrical circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Multiple: PointGoal navigation policies, object navigation policies, behavioral cloning policies for manipulation, DQN-based push policies, U-Net interaction predictor, SAC-based RL agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agents (SAC, DQN), imitation learning / behavioral cloning networks (MLPs, CNNs), U-Net visual encoder for interaction prediction; distributed multi-GPU RL training pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Embodied navigation (PointGoal), object navigation, pick-and-place mobile/manipulation, push-to-close drawers/cabinets, and learning visual affordance maps predicting where interactions will succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Depth-based PointGoal: with domain randomization overall SPL 0.40 and success rate 44.75% (800k steps); Object navigation (RGB) overall success rate 57.5% with randomization (200k); Imitation pick-and-place: 98% success (100 eval episodes); Mobile-manipulation imitation: 70% success (20 eval episodes); LiDAR PointGoal in-sim success 33% (Rs_int).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Held-out unseen simulated scenes (zero-shot generalization) and real-world robot deployment (real apartment counterpart Rs_int for sim2real LiDAR transfer); also held-out visuals/textures.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>LiDAR PointGoal trained in iGibson (Rs_int): 33% success in simulation, 24% success in the real-world apartment (zero-shot transfer) — ~9 percentage-point drop; depth/RGB policies show improved generalization to held-out simulated scenes when trained with domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper compares policies trained with and without domain randomization (visual/dynamics/object instance randomization) and contrasts continuous Rigid-Body Physics (RBP) vs Predefined Actions (PA) approaches used by other simulators; domain-randomized training produced substantially better generalization (e.g., PointGoal depth SPL from 0.27 to 0.40 and success from 31.25% to 44.75%; object navigation success from 49.75% to 57.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that continuous RBP (full continuous physics of objects and robots), realistic sensor simulation (e.g., LiDAR, PBR), and domain randomization are important for robust policies and sim2real transfer; they claim PA-only simulators limit access to task granularities and impede transfer, implying these features are minimally necessary for mobile manipulation sim2real.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Sim2real LiDAR transfer had a ~9% drop in success (33% sim → 24% real); some failure cases concentrated on the same initial/goal pairs. Two simulated pick-and-place failures were caused by premature gripper opening. Also, authors note that scenes or objects that are not fully interactive (e.g., non-interactable subset) can cause false negatives during pretraining and hurt representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1514.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson Environment (Gibson v1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale dataset/ environment of 3D reconstructions of real-world floors for embodied perception and navigation that uses static scene meshes (non-interactive) and PyBullet for robot navigation collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gibson env: Real-world perception for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gibson (environment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Environment comprised of >1400 real-world 3D reconstructed floors and static meshes; originally used for navigation/perception tasks; physics limited to robot navigation collisions against static mesh rather than continuous interactive object physics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied robotics / perception (navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium / low for interaction: high visual realism for perception, but low interaction fidelity because scenes are static single meshes (no per-object dynamics or articulation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>High visual realism from reconstructions; physics limited to collision with static scene mesh (no articulated object dynamics or object manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Primarily perceptual navigation tasks (e.g., visual navigation, localization); not designed for manipulation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Perception and navigation algorithms to real-world data/robots (visual generalization), but not intended for interaction sim2real for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes Gibson's use of static meshes prevents interactive tasks and limits sim2real transfer for manipulation, implying that static-scene fidelity is insufficient for manipulation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not interactive: cannot simulate manipulation; unsuitable for end-to-end sensorimotor control loops that require object dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1514.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fast simulation platform for embodied AI research optimized for high rendering speed and large-scale navigation experiments, using static scene assets and focusing on navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Habitat: A Platform for Embodied AI Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Habitat (environment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>High-throughput renderer and environment for training navigation agents on static scene reconstructions (Gibson v1 / Matterport assets); emphasizes speed and large-scale RL training rather than interactive physics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied robotics / perception (navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity and very high rendering throughput for perception tasks, but low interaction fidelity (non-interactive assets), limiting manipulation simulation realism.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fast PBR rendering, supports RGB/D sensors; does not model continuous object interactions in its standard datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Visual navigation and perception-oriented embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Generalization to unseen environments and perception; less emphasis on manipulation sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors contrast Habitat's speed and non-interactive assets with iGibson's interactive continuous physics for manipulation; suggest Habitat's fidelity is insufficient for manipulation sim2real.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Because assets are non-interactive, Habitat is unsuitable for end-to-end manipulation transfer scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1514.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sapien</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sapien</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated part-based interactive environment focusing on articulated object interaction and part-level mobility using PhysX and articulated-object models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sapien: A simulated part-based interactive environment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Sapien (environment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An environment for articulated-object interaction and part-level simulation, supporting rigid-body physics (RBP) without predefined action abstractions and using PhysX as physics engine.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / articulated-object interaction (robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity for articulated-object simulation within smaller scenes: continuous RBP (PhysX) with emphasis on articulated parts and part-level dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports articulated object dynamics, per-part collision and mobility; smaller scenes focused on object interactions; does not provide the large-scale home scenes suite of iGibson.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Articulation-centered manipulation and interaction reasoning (e.g., part movement prediction, grasping).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Interactive manipulation research and algorithms; potential sim2real for articulated-object tasks but not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Mentioned as an environment that, like iGibson, uses continuous physics (RBP) rather than only predefined actions; authors highlight Sapien's articulated focus but smaller scene scale.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1514.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive 3D environment that uses predefined actions (symbolic state transitions) and Unity rendering to support object state changes and task planning at a higher abstraction level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai2-thor: An interactive 3d environment for visual ai</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AI2-THOR (environment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A Unity-based environment offering object states and predefined actions (RBPA) that change object states symbolically (e.g., open/close) rather than simulating continuous object physics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied AI / task planning (symbolic interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Low-to-medium interaction fidelity for physics: uses rigid-body with predefined actions abstraction (instant state transitions) appropriate for symbolic task planning but not detailed physics-based manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Predefined action transitions (RBPA) permit fast symbolic interactions but lack continuous contact dynamics and low-level force/trajectory simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>High-level task planning, symbolic interaction reasoning, and object-state based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Symbolic task planners and high-level policy research; limited direct sim2real for low-level manipulation due to lack of continuous physics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper contrasts AI2-THOR's PA abstraction with continuous-physics simulators and argues PA can limit low-level manipulation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Predefined-action abstraction limits access to task granularities and impedes sim2real transfer for full robotics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1514.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VirtualHome</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VirtualHome</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation/dataset for household activities represented as programs, using predefined actions to simulate high-level activity sequences rather than continuous physics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Virtualhome: Simulating household activities via programs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>VirtualHome (environment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Platform that simulates household activities through scripted programs and predefined actions (RBPA) for object state transitions, focusing on symbolic task composition.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / task planning (symbolic household activities)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Low interaction fidelity for continuous physics; high-level task/programmatic simulation suitable for symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Uses predefined actions and symbolic state changes; not intended for continuous force/trajectory-based manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Programmatic household activity simulation, high-level task planning and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Symbolic planners and program-based activity reasoning research.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes RBPA approaches like VirtualHome help symbolic reasoning but are inadequate for full robotics sim2real.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Cannot capture low-level dynamics needed to learn force/trajectory-dependent manipulation behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1514.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TDW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ThreeDWorld (TDW)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform for interactive multi-modal physical simulation (Unity & Flex) that supports continuous rigid-body physics and particle/fluids simulation and audio modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Threadworld: A platform for interactive multi-modal physical simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ThreeDWorld (TDW)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulator offering multi-modal simulation (visual, physics, audio) with support for rigid-body physics and particle/fluids using Unity and NVIDIA Flex; emphasizes multi-modal, multi-physics interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / multi-physics (rigid-body, particles, fluids) / embodied AI</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-to-high fidelity for multi-modal physics including particle/fluids (depending on Flex configuration); supports continuous interactions and additional modalities (audio).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports RBP and particle/fluids; may include simplifications for grasping via avatars; rendering PBR supported; fidelity depends on module (Flex for fluids can be approximate).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Interactive multi-modal physical reasoning tasks including fluid/particle interactions and manipulation under multi-physics conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Research exploring multi-modal physics interactions and more complex interaction phenomena; transfer specifics not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Mentioned as capable of continuous RBP and fluids, offering more physical phenomena than simulators limited to rigid bodies.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1514.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bullet Physics Library</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source rigid-body physics engine widely used for simulating collision, contact and dynamics in robotics and game research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bullet physics library</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Bullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A general-purpose rigid-body physics engine providing collision detection, contact resolution, articulation support and dynamics integration; used by multiple robotics simulation environments including iGibson and Gibson.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / rigid-body dynamics (robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-fidelity rigid-body dynamics engine: simulates collisions, contact, articulated joints, and basic friction; lacks native fluid/thermodynamics modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Discrete-time integration (configurable timestep, e.g., 1/120s used in iGibson), contact and friction modelling, support for articulated chains; does not model fluids or electrical phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Low-level physics simulation for robotic control, collision-avoiding motion planning and manipulation dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Provides physically plausible dynamics for sim-to-real robotics training when coupled with appropriate sensors/domain randomization; specific transfer metrics depend on environment and policies (e.g., iGibson + Bullet used for LiDAR sim2real).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Used as a backbone in iGibson; authors attribute successful sim2real (with LiDAR) partly to realistic physics modeling and sensor simulation built on Bullet.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1514.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics engine designed for model-based control and efficient, accurate simulation of articulated systems often used in robotics research and RL benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mujoco: A physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A high-performance physics engine optimized for articulated rigid-body simulation, contact dynamics and control research; commonly used in manipulation/locomotion benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / articulated rigid-body dynamics (robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-high fidelity for articulated dynamics and control; focuses on accurate dynamics and efficient computation; does not natively model fluids/thermodynamics/electrical circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Accurate articulated dynamics, contact handling and fast integrators; often used for control research and RL due to speed and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Robot control, locomotion and manipulation policy training; not used for thermodynamics/circuits/biology in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1514.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1514.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhysX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NVIDIA PhysX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial physics SDK used for real-time rigid-body and soft-body simulation, referenced here as the engine used by some interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PhysX</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A real-time physics engine capable of rigid-body dynamics and articulated systems, used by some simulators (e.g., Sapien) to simulate object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / rigid-body dynamics (robotics/game physics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-fidelity real-time physics suitable for interactive simulation; not specialized for fluids/thermodynamics/electrical circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Real-time contact and articulated dynamics, GPU acceleration in some implementations; fidelity depends on configuration and numerical parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Interactive object manipulation and articulated dynamics simulation in research platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Mentioned as an engine choice for other simulation environments (Sapien).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gibson env: Real-world perception for embodied agents <em>(Rating: 2)</em></li>
                <li>Habitat: A Platform for Embodied AI Research <em>(Rating: 2)</em></li>
                <li>Sapien: A simulated part-based interactive environment <em>(Rating: 2)</em></li>
                <li>Ai2-thor: An interactive 3d environment for visual ai <em>(Rating: 2)</em></li>
                <li>Virtualhome: Simulating household activities via programs <em>(Rating: 1)</em></li>
                <li>Threadworld: A platform for interactive multi-modal physical simulation <em>(Rating: 1)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Bullet physics library <em>(Rating: 1)</em></li>
                <li>Mujoco: A physics engine for model-based control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1514",
    "paper_id": "paper-cda5d891ae6262c55c8711ef5b18ccc1fe748d7a",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "iGibson",
            "name_full": "iGibson 1.0",
            "brief_description": "An open-source, high‑fidelity simulation environment for training embodied agents on navigation, manipulation and mobile-manipulation in large, realistic, fully-interactive indoor scenes with PBR rendering, LiDAR, domain randomization and integrated motion planners.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "iGibson (custom environment built on physics engines)",
            "simulator_description": "A simulation environment that integrates a physics simulator (PyBullet by default), a physics-based renderer (PBR), high-quality virtual sensors (RGB, depth, LiDAR, flow, segmentation), 15 fully interactive real-world-derived scenes (108 rooms) plus compatibility with thousands more layouts, domain randomization for visuals/dynamics, articulated object models and integrated sampling-based motion planners.",
            "scientific_domain": "mechanics / embodied robotics / perception (robot navigation and manipulation)",
            "fidelity_level": "High-fidelity for embodied robotics: continuous rigid-body and articulated-object physics, realistic sensor simulation (PBR, LiDAR), and per-part material/dynamics properties; not a fluid/thermodynamics or circuit simulator.",
            "fidelity_characteristics": "Includes continuous rigid-body dynamics, articulated joints, collision meshes, center-of-mass and inertial properties, friction/dynamics tied to material annotations, physics timestep 1/120s, PBR with roughness/metallic/normal maps, LiDAR noise dropout model, integrated motion planners; does not model fluids/thermodynamics/electrical circuits.",
            "model_or_agent_name": "Multiple: PointGoal navigation policies, object navigation policies, behavioral cloning policies for manipulation, DQN-based push policies, U-Net interaction predictor, SAC-based RL agents",
            "model_description": "Reinforcement learning agents (SAC, DQN), imitation learning / behavioral cloning networks (MLPs, CNNs), U-Net visual encoder for interaction prediction; distributed multi-GPU RL training pipelines.",
            "reasoning_task": "Embodied navigation (PointGoal), object navigation, pick-and-place mobile/manipulation, push-to-close drawers/cabinets, and learning visual affordance maps predicting where interactions will succeed.",
            "training_performance": "Depth-based PointGoal: with domain randomization overall SPL 0.40 and success rate 44.75% (800k steps); Object navigation (RGB) overall success rate 57.5% with randomization (200k); Imitation pick-and-place: 98% success (100 eval episodes); Mobile-manipulation imitation: 70% success (20 eval episodes); LiDAR PointGoal in-sim success 33% (Rs_int).",
            "transfer_target": "Held-out unseen simulated scenes (zero-shot generalization) and real-world robot deployment (real apartment counterpart Rs_int for sim2real LiDAR transfer); also held-out visuals/textures.",
            "transfer_performance": "LiDAR PointGoal trained in iGibson (Rs_int): 33% success in simulation, 24% success in the real-world apartment (zero-shot transfer) — ~9 percentage-point drop; depth/RGB policies show improved generalization to held-out simulated scenes when trained with domain randomization.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Paper compares policies trained with and without domain randomization (visual/dynamics/object instance randomization) and contrasts continuous Rigid-Body Physics (RBP) vs Predefined Actions (PA) approaches used by other simulators; domain-randomized training produced substantially better generalization (e.g., PointGoal depth SPL from 0.27 to 0.40 and success from 31.25% to 44.75%; object navigation success from 49.75% to 57.5%).",
            "minimal_fidelity_discussion": "Authors argue that continuous RBP (full continuous physics of objects and robots), realistic sensor simulation (e.g., LiDAR, PBR), and domain randomization are important for robust policies and sim2real transfer; they claim PA-only simulators limit access to task granularities and impede transfer, implying these features are minimally necessary for mobile manipulation sim2real.",
            "failure_cases": "Sim2real LiDAR transfer had a ~9% drop in success (33% sim → 24% real); some failure cases concentrated on the same initial/goal pairs. Two simulated pick-and-place failures were caused by premature gripper opening. Also, authors note that scenes or objects that are not fully interactive (e.g., non-interactable subset) can cause false negatives during pretraining and hurt representation learning.",
            "uuid": "e1514.0",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Gibson",
            "name_full": "Gibson Environment (Gibson v1)",
            "brief_description": "A large-scale dataset/ environment of 3D reconstructions of real-world floors for embodied perception and navigation that uses static scene meshes (non-interactive) and PyBullet for robot navigation collisions.",
            "citation_title": "Gibson env: Real-world perception for embodied agents",
            "mention_or_use": "mention",
            "simulator_name": "Gibson (environment)",
            "simulator_description": "Environment comprised of &gt;1400 real-world 3D reconstructed floors and static meshes; originally used for navigation/perception tasks; physics limited to robot navigation collisions against static mesh rather than continuous interactive object physics.",
            "scientific_domain": "mechanics / embodied robotics / perception (navigation)",
            "fidelity_level": "Medium / low for interaction: high visual realism for perception, but low interaction fidelity because scenes are static single meshes (no per-object dynamics or articulation).",
            "fidelity_characteristics": "High visual realism from reconstructions; physics limited to collision with static scene mesh (no articulated object dynamics or object manipulation).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Primarily perceptual navigation tasks (e.g., visual navigation, localization); not designed for manipulation reasoning.",
            "training_performance": null,
            "transfer_target": "Perception and navigation algorithms to real-world data/robots (visual generalization), but not intended for interaction sim2real for manipulation.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes Gibson's use of static meshes prevents interactive tasks and limits sim2real transfer for manipulation, implying that static-scene fidelity is insufficient for manipulation transfer.",
            "failure_cases": "Not interactive: cannot simulate manipulation; unsuitable for end-to-end sensorimotor control loops that require object dynamics.",
            "uuid": "e1514.1",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Habitat",
            "name_full": "Habitat",
            "brief_description": "A fast simulation platform for embodied AI research optimized for high rendering speed and large-scale navigation experiments, using static scene assets and focusing on navigation tasks.",
            "citation_title": "Habitat: A Platform for Embodied AI Research",
            "mention_or_use": "mention",
            "simulator_name": "Habitat (environment)",
            "simulator_description": "High-throughput renderer and environment for training navigation agents on static scene reconstructions (Gibson v1 / Matterport assets); emphasizes speed and large-scale RL training rather than interactive physics.",
            "scientific_domain": "mechanics / embodied robotics / perception (navigation)",
            "fidelity_level": "High visual fidelity and very high rendering throughput for perception tasks, but low interaction fidelity (non-interactive assets), limiting manipulation simulation realism.",
            "fidelity_characteristics": "Fast PBR rendering, supports RGB/D sensors; does not model continuous object interactions in its standard datasets.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Visual navigation and perception-oriented embodied tasks.",
            "training_performance": null,
            "transfer_target": "Generalization to unseen environments and perception; less emphasis on manipulation sim2real transfer.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors contrast Habitat's speed and non-interactive assets with iGibson's interactive continuous physics for manipulation; suggest Habitat's fidelity is insufficient for manipulation sim2real.",
            "failure_cases": "Because assets are non-interactive, Habitat is unsuitable for end-to-end manipulation transfer scenarios.",
            "uuid": "e1514.2",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Sapien",
            "name_full": "Sapien",
            "brief_description": "A simulated part-based interactive environment focusing on articulated object interaction and part-level mobility using PhysX and articulated-object models.",
            "citation_title": "Sapien: A simulated part-based interactive environment",
            "mention_or_use": "mention",
            "simulator_name": "Sapien (environment)",
            "simulator_description": "An environment for articulated-object interaction and part-level simulation, supporting rigid-body physics (RBP) without predefined action abstractions and using PhysX as physics engine.",
            "scientific_domain": "mechanics / articulated-object interaction (robotics)",
            "fidelity_level": "High-fidelity for articulated-object simulation within smaller scenes: continuous RBP (PhysX) with emphasis on articulated parts and part-level dynamics.",
            "fidelity_characteristics": "Supports articulated object dynamics, per-part collision and mobility; smaller scenes focused on object interactions; does not provide the large-scale home scenes suite of iGibson.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Articulation-centered manipulation and interaction reasoning (e.g., part movement prediction, grasping).",
            "training_performance": null,
            "transfer_target": "Interactive manipulation research and algorithms; potential sim2real for articulated-object tasks but not evaluated here.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Mentioned as an environment that, like iGibson, uses continuous physics (RBP) rather than only predefined actions; authors highlight Sapien's articulated focus but smaller scene scale.",
            "failure_cases": null,
            "uuid": "e1514.3",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "AI2-THOR",
            "name_full": "AI2-THOR",
            "brief_description": "An interactive 3D environment that uses predefined actions (symbolic state transitions) and Unity rendering to support object state changes and task planning at a higher abstraction level.",
            "citation_title": "Ai2-thor: An interactive 3d environment for visual ai",
            "mention_or_use": "mention",
            "simulator_name": "AI2-THOR (environment)",
            "simulator_description": "A Unity-based environment offering object states and predefined actions (RBPA) that change object states symbolically (e.g., open/close) rather than simulating continuous object physics.",
            "scientific_domain": "mechanics / embodied AI / task planning (symbolic interactions)",
            "fidelity_level": "Low-to-medium interaction fidelity for physics: uses rigid-body with predefined actions abstraction (instant state transitions) appropriate for symbolic task planning but not detailed physics-based manipulation.",
            "fidelity_characteristics": "Predefined action transitions (RBPA) permit fast symbolic interactions but lack continuous contact dynamics and low-level force/trajectory simulation.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "High-level task planning, symbolic interaction reasoning, and object-state based tasks.",
            "training_performance": null,
            "transfer_target": "Symbolic task planners and high-level policy research; limited direct sim2real for low-level manipulation due to lack of continuous physics.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper contrasts AI2-THOR's PA abstraction with continuous-physics simulators and argues PA can limit low-level manipulation transfer.",
            "failure_cases": "Predefined-action abstraction limits access to task granularities and impedes sim2real transfer for full robotics tasks.",
            "uuid": "e1514.4",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "VirtualHome",
            "name_full": "VirtualHome",
            "brief_description": "A simulation/dataset for household activities represented as programs, using predefined actions to simulate high-level activity sequences rather than continuous physics.",
            "citation_title": "Virtualhome: Simulating household activities via programs",
            "mention_or_use": "mention",
            "simulator_name": "VirtualHome (environment)",
            "simulator_description": "Platform that simulates household activities through scripted programs and predefined actions (RBPA) for object state transitions, focusing on symbolic task composition.",
            "scientific_domain": "embodied AI / task planning (symbolic household activities)",
            "fidelity_level": "Low interaction fidelity for continuous physics; high-level task/programmatic simulation suitable for symbolic reasoning.",
            "fidelity_characteristics": "Uses predefined actions and symbolic state changes; not intended for continuous force/trajectory-based manipulation.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Programmatic household activity simulation, high-level task planning and reasoning.",
            "training_performance": null,
            "transfer_target": "Symbolic planners and program-based activity reasoning research.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes RBPA approaches like VirtualHome help symbolic reasoning but are inadequate for full robotics sim2real.",
            "failure_cases": "Cannot capture low-level dynamics needed to learn force/trajectory-dependent manipulation behaviors.",
            "uuid": "e1514.5",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "TDW",
            "name_full": "ThreeDWorld (TDW)",
            "brief_description": "A platform for interactive multi-modal physical simulation (Unity & Flex) that supports continuous rigid-body physics and particle/fluids simulation and audio modalities.",
            "citation_title": "Threadworld: A platform for interactive multi-modal physical simulation",
            "mention_or_use": "mention",
            "simulator_name": "ThreeDWorld (TDW)",
            "simulator_description": "Simulator offering multi-modal simulation (visual, physics, audio) with support for rigid-body physics and particle/fluids using Unity and NVIDIA Flex; emphasizes multi-modal, multi-physics interactions.",
            "scientific_domain": "mechanics / multi-physics (rigid-body, particles, fluids) / embodied AI",
            "fidelity_level": "Medium-to-high fidelity for multi-modal physics including particle/fluids (depending on Flex configuration); supports continuous interactions and additional modalities (audio).",
            "fidelity_characteristics": "Supports RBP and particle/fluids; may include simplifications for grasping via avatars; rendering PBR supported; fidelity depends on module (Flex for fluids can be approximate).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Interactive multi-modal physical reasoning tasks including fluid/particle interactions and manipulation under multi-physics conditions.",
            "training_performance": null,
            "transfer_target": "Research exploring multi-modal physics interactions and more complex interaction phenomena; transfer specifics not reported in this paper.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Mentioned as capable of continuous RBP and fluids, offering more physical phenomena than simulators limited to rigid bodies.",
            "failure_cases": null,
            "uuid": "e1514.6",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Bullet",
            "name_full": "Bullet Physics Library",
            "brief_description": "An open-source rigid-body physics engine widely used for simulating collision, contact and dynamics in robotics and game research.",
            "citation_title": "Bullet physics library",
            "mention_or_use": "mention",
            "simulator_name": "Bullet",
            "simulator_description": "A general-purpose rigid-body physics engine providing collision detection, contact resolution, articulation support and dynamics integration; used by multiple robotics simulation environments including iGibson and Gibson.",
            "scientific_domain": "mechanics / rigid-body dynamics (robotics)",
            "fidelity_level": "Medium-fidelity rigid-body dynamics engine: simulates collisions, contact, articulated joints, and basic friction; lacks native fluid/thermodynamics modeling.",
            "fidelity_characteristics": "Discrete-time integration (configurable timestep, e.g., 1/120s used in iGibson), contact and friction modelling, support for articulated chains; does not model fluids or electrical phenomena.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Low-level physics simulation for robotic control, collision-avoiding motion planning and manipulation dynamics.",
            "training_performance": null,
            "transfer_target": "Provides physically plausible dynamics for sim-to-real robotics training when coupled with appropriate sensors/domain randomization; specific transfer metrics depend on environment and policies (e.g., iGibson + Bullet used for LiDAR sim2real).",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Used as a backbone in iGibson; authors attribute successful sim2real (with LiDAR) partly to realistic physics modeling and sensor simulation built on Bullet.",
            "failure_cases": null,
            "uuid": "e1514.7",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo",
            "brief_description": "A physics engine designed for model-based control and efficient, accurate simulation of articulated systems often used in robotics research and RL benchmarks.",
            "citation_title": "Mujoco: A physics engine for model-based control",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "A high-performance physics engine optimized for articulated rigid-body simulation, contact dynamics and control research; commonly used in manipulation/locomotion benchmarks.",
            "scientific_domain": "mechanics / articulated rigid-body dynamics (robotics)",
            "fidelity_level": "Medium-high fidelity for articulated dynamics and control; focuses on accurate dynamics and efficient computation; does not natively model fluids/thermodynamics/electrical circuits.",
            "fidelity_characteristics": "Accurate articulated dynamics, contact handling and fast integrators; often used for control research and RL due to speed and stability.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Robot control, locomotion and manipulation policy training; not used for thermodynamics/circuits/biology in this paper.",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1514.8",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "PhysX",
            "name_full": "NVIDIA PhysX",
            "brief_description": "A commercial physics SDK used for real-time rigid-body and soft-body simulation, referenced here as the engine used by some interactive environments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "PhysX",
            "simulator_description": "A real-time physics engine capable of rigid-body dynamics and articulated systems, used by some simulators (e.g., Sapien) to simulate object interactions.",
            "scientific_domain": "mechanics / rigid-body dynamics (robotics/game physics)",
            "fidelity_level": "Medium-fidelity real-time physics suitable for interactive simulation; not specialized for fluids/thermodynamics/electrical circuits.",
            "fidelity_characteristics": "Real-time contact and articulated dynamics, GPU acceleration in some implementations; fidelity depends on configuration and numerical parameters.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Interactive object manipulation and articulated dynamics simulation in research platforms.",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Mentioned as an engine choice for other simulation environments (Sapien).",
            "failure_cases": null,
            "uuid": "e1514.9",
            "source_info": {
                "paper_title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gibson env: Real-world perception for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Habitat: A Platform for Embodied AI Research",
            "rating": 2
        },
        {
            "paper_title": "Sapien: A simulated part-based interactive environment",
            "rating": 2
        },
        {
            "paper_title": "Ai2-thor: An interactive 3d environment for visual ai",
            "rating": 2
        },
        {
            "paper_title": "Virtualhome: Simulating household activities via programs",
            "rating": 1
        },
        {
            "paper_title": "Threadworld: A platform for interactive multi-modal physical simulation",
            "rating": 1
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Bullet physics library",
            "rating": 1
        },
        {
            "paper_title": "Mujoco: A physics engine for model-based control",
            "rating": 1
        }
    ],
    "cost": 0.017671,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes</h1>
<p>Bokui Shen<em>, Fei Xia</em>, Chengshu Li<em>, Roberto Martín-Martín</em>, Linxi Fan, Guanzhi Wang, Claudia Pérez-D'Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, Micael Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, Silvio Savarese</p>
<h4>Abstract</h4>
<p>We present iGibson 1.0, a novel simulation environment to develop robotic solutions for interactive tasks in large-scale realistic scenes. Our environment contains 15 fully interactive home-sized scenes with 108 rooms populated with rigid and articulated objects. The scenes are replicas of realworld homes, with distribution and the layout of objects aligned to those of the real world. iGibson 1.0 integrates several key features to facilitate the study of interactive tasks: i) generation of high-quality virtual sensor signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain randomization to change the materials of the objects (both visual and physical) and/or their shapes, iii) integrated sampling-based motion planners to generate collision-free trajectories for robot bases and arms, and iv) intuitive human-iGibson interface that enables efficient collection of human demonstrations. Through experiments, we show that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks. We also show that iGibson features enable the generalization of navigation agents, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of human demonstrated (mobile) manipulation behaviors. iGibson 1.0 is open-source, equipped with comprehensive examples and documentation. For more information, visit our project website: http://svl.stanford.edu/igibson/.</p>
<h2>I. INTRODUCTION</h2>
<p>Simulation environments have proliferated over the last few years as a way to train robots and interactive agents in a rapid and safe manner. In these environments, agents learn to engage in physical interactions [1, 2], navigate based on sensor signals [3, 4, 5, 6, 7], or plan long-horizon tasks [8, $9,10,11]$. In simulation, agents learn to perform interactions that actively change the input sensor signals and the state of the environment towards a desired configuration, capabilities at the core of what an embodied agent needs to achieve.</p>
<p>However, existing simulation environments that combine physics simulation and robotic tasks often cater to a narrow set of tasks and include only clean, small-scale scenes [12, $13,14,15,16,17]$. The few simulation environments that include large scenes such as homes or offices either disable the possibility of interacting with the scene, focusing only on navigation (e.g. Habitat [18]), or use simplified modes of interaction (e.g. AI2Thor [19], VirtualHome [20]). These simulators do not support the development of end-to-end sensorimotor control loops for tasks that require rich, continuous interaction with the scene. Such tasks are difficult to accomplish in the aforementioned simulators, and the simplified modes of interaction lead to difficulties in transferring the learned policy onto real robots.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Robot performs an interactive task in iGibson 1.0. It operates in the kitchen of one of iGibson's fully interactive scenes, planning an interaction with the arm using a integrated sampling-based motion planner and receiving first-person view. Bottom: The same scene can be randomized with different materials and/or object models</p>
<p>We present iGibson 1.0 (alternative called just iGibson in this manuscript), a novel simulation environment that enables the development of embodied agents for interactive tasks in large-scale realistic scenes (Fig. 1). Interactivity is achieved by leveraging a physics engine processing all elements in the scene, enabling manipulation of rigid and articulated objects as well as mobility. iGibson 1.0 aims at unifying several aspects of robot simulation that are often available in different software tools, such as physics simulation for interaction with objects and robot control, high-quality simulated sensors, integration with reinforcement learning frameworks, and realistic indoor scenes that reflect the objects distribution of real homes. This integration allows fully physics based simulation of robot tasks (i.e. simulating the full complexity of the task) and allows developing task and motion planning, reinforcement learning or imitation learning solutions for those tasks with virtual sensor signals.
iGibson 1.0 contains 15 fully interactive and visuallyrealistic scenes with a total of 108 rooms. These scenes were generated by annotating 3D reconstructions of realworld scans (static scenes represented by a single mesh) and converting them into fully interactive scene models (scenes filled with articulated object models). In this process, we respect the original object-instance layout and object-</p>
<p>category distribution. The object models are extended from open-source datasets [21, 22, 15] enriched with annotations of material and dynamic properties. iGibson's physics-based renderer leverages the extra information provided in the material annotation (maps of metallic, roughness and normals) to generate high-quality virtual images. To further facilitate the training of more robust visuomotor agents, iGibson 1.0 offers domain randomization procedures for materials (both visual appearances and dynamics properties) and object shapes while respecting the distribution of object placements and preserving interactability. iGibson 1.0 is also equipped with a graphical user interface that allows human users to easily interact with the scenes, enabling efficient collection of human demonstrations for imitation learning.</p>
<p>In summary, iGibson 1.0 provides the following novel features that facilitate developing and training robotic solutions:</p>
<p>1) Fifteen fully interactive scenes containing 108 rooms modelled after real world homes with articulated object models annotated with materials and dynamics properties. Additionally, we support importing CubiCasa5K [23] and 3D-Front [24] layouts, giving access to more than 12000 additional fully interactive scenes.
2) Realistic virtual sensor signals, including a physics-based renderer (PBR) for RGB images, rendering of normals, depth, point clouds, virtual LiDAR signals, and optical/scene flow. We further integrate domain randomization functionality (visual texture, dynamics properties and object instances) that facilitates generalization to unseen scenes.
3) Useful tooling for developing robotics solutions in simulation, such as a human-computer interface for humans to provide interactive demonstrations, and sampling-based motion planners for navigation and manipulation.</p>
<p>We demonstrate the benefits of these novel features in a comprehensive set of experiments in which visual agents are trained for navigation and interactive tasks. Our experiments show that iGibson 1.0 enables researchers to 1) train more robust and generalizable sensorimotor policies thanks to its realistic virtual sensor signals (including LiDAR) and domain randomization mechanisms, 2) collect human demonstrations and train imitation learning policies for manipulation and mobile manipulation tasks, and 3) learn intermediate visual representations linked to interactability of the scene that accelerate training of downstream manipulation tasks. iGibson 1.0 is open-source and academically developed, and available on our website http://sv1.stanford.edu/igibson/.</p>
<h2>II. Related Work</h2>
<p>The use of simulation in robotics has significantly increased in recent years and the research community has proposed a number of simulators for robotics and embodied AI. Here, we use the terms physics simulator and simulation environment as follows. A physics simulator is an engine capable of computing the physical effect of actions on an environment (e.g. motion of bodies when a force is applied, or flow of liquid particles when being poured) [27, 28, 29, $30,31,32]$. On the other hand, a simulation environment is a framework that includes a physics simulator, a renderer of
virtual signals, and a set of assets (i.e. models of scenes, objects, robots) ready to be used to study and develop solutions for different tasks. Both components are crucial for advancing embodied AI and robotics. Here, we focus on the discussion of simulation environments.</p>
<p>Several simulation environments have been proposed recently to study manipulation with stationary arms [13, 12, $17,33,16]$. Most of them are based on Bullet [27] or MuJoCo [28] for physics simulation, and render images with the default renderer or a Unity [30] plugin. Different from these simulation environments, iGibson focuses on largescale (house-size) scenes and includes fifteen fully interactive scenes with 108 rooms - such as kitchens and bedrooms - where researchers can develop solutions for navigation, manipulation and mobile manipulation.</p>
<p>Closer to iGibson are simulation environments that include large-scale realistic scenes (e.g. homes or offices), which we summarize and compare in Table I. Gibson [25] (now Gibson v1) was the precursor of iGibson. It includes over 1400 3D-reconstructed floors of homes and offices with real-world object distribution and layout. Although Gibson incorporates PyBullet as its physics engine for simulating robot navigation, each scene is one single fully rigid object (static mesh). Thus, it does not allow agents to interact with the scenes other than collisions with the mesh, restricting its use to only navigation. A similar environment is Habitat [18]. Despite its high rendering speed, Habitat uses the noninteractive assets from Gibson v1 [25] and Matterport [34] and therefore only supports navigation tasks and simplified rearrangement of added objects. Recent work [35] introduced an extension to the Gibson v1 environment to support Interactive Navigation [35] where parts of the reconstructions corresponding to five object classes (chairs, tables, desks, sofas, and doors) in several Gibson static models were segmented and replaced with interactive versions. This enabled navigation agents to interact with the scene, and thus allowed for the first benchmark for Interactive Navigation. However, the simulators above are not interactive [25, 18] or partially interactive [35]. iGibson encapsulates the functionalities in all previous versions of Gibson with backwards compatibility for the static environments.</p>
<p>A variety of simulation environments have been proposed recently for scene-level interactive tasks, such as Sapien [15], AI2Thor [19], VirtualHome [20], and ThreeDWorld (TDW [26]). These simulators adopt different ways of agent-world interactions. Predefined Actions (PA) consist in the set of actions that can be performed for each object type. When the agent is close enough to an object and the object is in the right state (precondition), the agent can select a predefined action, and the object is "transitioned" to the next state (postcondition). In Table I we refer to this technique as Rigid Bodies with Predefined Actions (RBPA). It is possible to combine Rigid Bodies with Predefined Actions (RBPA) and Rigid Body Physics (RBP), such as first using RBPA to grasp an object and then using RBP after releasing it. AI2Thor and VirtualHome use predefined actions (PA) as an abstraction of physical interactions and allow agents to</p>
<p>TABLE I: Comparison of Simulation Environments</p>
<table>
<thead>
<tr>
<th></th>
<th>iGibson 1.0 (ours)</th>
<th>Gibson [25]</th>
<th>Habitat [18]</th>
<th>Sapien [15]</th>
<th>AI2Thor [19]</th>
<th>VirtualH [20]</th>
<th>TDW [26]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Provided Large Scenes</td>
<td>15 homes</td>
<td>1400 / -</td>
<td>-</td>
<td>-</td>
<td>- / 120 rooms</td>
<td>- / 7</td>
<td>- / 25</td>
</tr>
<tr>
<td>Real-World / Designed</td>
<td>(108 rooms) / -</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Provided Objects</td>
<td>570 / Yes</td>
<td>- / -</td>
<td>- / -</td>
<td>2346 / No</td>
<td>609 / Yes</td>
<td>308 / No</td>
<td>200 / Yes</td>
</tr>
<tr>
<td>Number / Materials</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Agent/World Interaction</td>
<td>F</td>
<td>-</td>
<td>-</td>
<td>F</td>
<td>F &amp; PA</td>
<td>F &amp; PA</td>
<td>F</td>
</tr>
<tr>
<td>Forces, Predefined Actions</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Physics Engine</td>
<td>Bullet</td>
<td>Bullet</td>
<td>Bullet</td>
<td>PhysX</td>
<td>Unity</td>
<td>Unity</td>
<td>Unity &amp; Flex</td>
</tr>
<tr>
<td>Type of Simulation</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Rigid Bodies Physics,</td>
<td>RBP</td>
<td>RBP</td>
<td>RBP</td>
<td>RBP</td>
<td>RBP&amp;RBPA</td>
<td>RBPA</td>
<td>RBP&amp;PF</td>
</tr>
<tr>
<td>Rigid Bodies with PA,</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Particles and Fluids</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Supported Task</td>
<td>Nav.&amp;Manip.</td>
<td>Nav.</td>
<td>Nav.</td>
<td>Nav.&amp;Manip.</td>
<td>Nav.&amp;Manip.</td>
<td>Nav.&amp;Manip.</td>
<td>Nav.&amp;Manip.</td>
</tr>
<tr>
<td>Type of Rendering</td>
<td>PBR</td>
<td>IBR</td>
<td>PBR</td>
<td>PBR,RTX</td>
<td>PBR</td>
<td>PBR</td>
<td>PBR</td>
</tr>
<tr>
<td>Virtual Sensor Signals</td>
<td>RGB,D,N</td>
<td>RGB,D</td>
<td>RGB,D,SS,S</td>
<td>RGB,D,SS</td>
<td>RGB,D,SS,S</td>
<td>RGB,D</td>
<td>RGB,D,SS,S</td>
</tr>
<tr>
<td></td>
<td>SS,FL,LiDAR</td>
<td>N,SS</td>
<td></td>
<td></td>
<td></td>
<td>SS,FL</td>
<td></td>
</tr>
<tr>
<td>Domain Randomization</td>
<td>S,O,M</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>S</td>
<td>S</td>
<td>S,O</td>
</tr>
<tr>
<td>Scene,Object,Materials</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Speed</td>
<td>++</td>
<td>+</td>
<td>+++</td>
<td>++(PBR)/-(RTX)</td>
<td>+</td>
<td>+</td>
<td>+</td>
</tr>
<tr>
<td>Human Interface</td>
<td>Mouse</td>
<td>-</td>
<td>Mouse</td>
<td>-</td>
<td>Mouse</td>
<td>Natural</td>
<td>Virtual</td>
</tr>
<tr>
<td></td>
<td>Keyboard</td>
<td>-</td>
<td>Keyboard</td>
<td>-</td>
<td>Keyboard</td>
<td>Language</td>
<td>Reality</td>
</tr>
<tr>
<td>Integrated Motion Planner</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Specialty</td>
<td>Phys. Int. in</td>
<td>Nav.</td>
<td>Fast,</td>
<td>Articulation,</td>
<td>Object States,</td>
<td>Object States,</td>
<td>Audio,</td>
</tr>
<tr>
<td></td>
<td>Large Scenes</td>
<td></td>
<td>Nav.</td>
<td>Ray Tracing</td>
<td>Task Planning</td>
<td>Task Planning</td>
<td>Fluids</td>
</tr>
</tbody>
</table>
<p>Type of rendering: PBR:Physics-Based Rendering, IBR:Image-Based Rendering, RTX:Ray Tracing Virtual sensor signals: RGB: Color Images, D:Depth, N:Normals, SS:Semantic Segmentation, LiDAR:Lidar, FL:Flow (optical and/or scene), S: Sounds</p>
<p>modify the object states instantaneously (e.g. open a closed cabinet), suitable to study high-level task planning. While the availability of PA helps focusing on symbolic reasoning, the full robotics problems require RBP to simulate the tasks in all its complexity. PA limits access to all granularities of the task, which impedes robot learning and sim2real transfer. With the purpose of simulating full robotics tasks, iGibson uses RBP, simulating the physics behavior of all objects continuously, with embodiment of real robots. This is crucial if the learned policy is to be deployed in the real world. Similar to iGibson, Sapiens also uses RBP without PA, but with smaller scenes, focusing on interaction with articulated objects. In iGibson, we enrich the articulated objects models from the PartNet-Mobility dataset introduced by Sapien with materials and dynamics properties. TDW is also capable of continuous RBP, with simplifications that facilitate grasping using robot avatars and not real robot platforms.</p>
<p>While other simulation environments focus on particular aspects of embodied agent simulation (Table. I), iGibson <strong>uniquely</strong> unifies a set of important tools for robotics that together enable robot learning in large scenes: support of LiDAR and PBR rendering, speed that enable reinforcement learning, robot integration (URDF, controllers, motion planners), and continuous physics simulation of agents and objects. This integration of features enables tasks such as mobile manipulation, illustrated in Fig. 2, including physics-based robot simulation across the entire task.</p>
<h3>III. iGibson Simulation Environment</h3>
<p>In this section, we discuss the main structure, properties and features of iGibson that support training of robust sensor-guided policies for navigation and manipulation.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Simulated Fetch Robot performing a mobile manipulation task in a cluttered environment.</p>
<h4>A. Simulation Characteristics and API</h4>
<p>At the highest level, iGibson follows the OpenAI Gym [36] convention. The environment receives an action and returns a new observation, reward and additional meta-information (e.g. if the episode has ended). Environments are specified with config files that determine scenes, tasks, robot embodiments, sensors, etc. Given a config file, iGibson creates an Environment that contains a Task and a Simulator. The Simulator contains a Scene, with a list of interactive Objects and one or more Robot instances. It also contains a Renderer that generates virtual visual signals from any point of view, such as a camera mounted on a robot or an external third person view. The Task defines the reward, initial and final conditions for the scene and the agents. While modular and easy to extend, most users may only need to interface with Environment after instantiating it with the appropriate config files.</p>
<p>iGibson comes with multiple easy-to-use configs, demos and Docker [37] files. It has been extensively adopted to train visuo-motor policies that successfully transfer to the real world [38, 39, 40, 41], and was the platform for iGibson Sim2Real Challenge at CVPR20 [42] and iGibson Challenge at CVPR21 [43]. The provided virtual LiDAR</p>
<p>sensor has been used for robotics research in planning and reinforcement learning for social navigation [44] and mobile manipulation [10]. iGibson is easily parallelizable and supports off-screen rendering on clusters.</p>
<h2>B. Fully Interactive Assets</h2>
<p>iGibson provides fifteen high quality fully interactive scenes with 108 rooms (see Fig. 3), populated with interactable objects. The scenes are interactive versions of fifteen 3D reconstructed scenes included in the Gibson v1 dataset. To preserve the real-world layout and distribution of objects, we follow a semi-automatic annotation procedure. This process is radically different from the annotation we performed for the Interactive Gibson Benchmark [35]. Instead of segmenting the original scene and replacing part of the meshes with interactive object models, we create fully interactive counterparts of the 3D reconstruction from scratch. This eliminates the need to fix artifacts in the original mesh due to reconstruction noise or segmentation error, and allows us to improve the overall quality of the scenes.</p>
<p>The scene generation process is composed of two annotation phases. First, the layout of the scene is annotated with floors, walls, doors and window openings. Then, all objects are annotated with 3D bounding boxes and class labels. We annotate bounding boxes for 57 different object classes, including all furniture types (doors, chairs, tables, cabinets, TVs, shelves, stoves, sinks, etc) and some small objects (plants, laptops, speakers, etc); see project website for the complete list. Annotating class-labeled bounding boxes allows us to scale and use different models of the same object class, while maintaining the real-world distribution of objects in the scene. In this way, we are able generate realistic randomized versions of the scenes (see Sec. IIID). To achieve the highest quality, for each class-labeled bounding box, we select a best fitting object model. The scene is also annotated with lights, with which we generate light probes for physics-based rendering (see Sec. III-C). We also bake in a realistic ray-traced ambient light and other light effects in the walls, floors and ceilings.</p>
<p>The object models are curated from open-source datasets: ShapeNet [21], PartNet Mobility [15, 45], and SketchFab. To preserve visual realism of the original reconstruction, we improve the object visual quality by annotating different parts of the models with photorealistic materials, which are then used by iGibson's physics-based renderer. We utilize materials from CC0Texture, including wood, marble, metal, etc. To achieve a high degree of physics realism, we curate a mapping from visual materials to friction coefficients. We additionally compute the collision mesh, center of mass and inertia frame for each link of all objects. To assign realistic mass and density for different objects, we take the the median values of the top 20 search results from Amazon.</p>
<p>Additionally, we provide compatibility with CubiCasa5K [23] and 3D-Front [24] repositories of home scenes. We use their scene layouts and populate them with our annotated object models, leading to additional more than 12000 interactive home scenes. These scenes contain fewer
objects than the fifteen iGibson scenes, but provide a very large number of additional models to train tasks.</p>
<p>The fully interactive scenes we include in iGibson enable learning of interactive tasks in large realistic home scenes; in Sec. IV-C we show that the scenes can be used to learn a useful visual representation that accelerates the learning of downstream manipulation tasks.</p>
<h2>C. Virtual Sensors</h2>
<p>A crucial component of iGibson is the generation of high quality virtual sensor signals, i.e. images and point clouds, for the simulated robots. In the following, we summarize the most relevant of these signal generators (Fig. 4).</p>
<p>Physics Based Rendering: In iGibson, we include an open-source physics-based renderer, which implements an approximation of BRDF models [46] with spatially varying material maps including roughness, metallic and tangentspace surface normals, extending [47].</p>
<p>LiDAR Sensing: Many real-world robots are equipped with LiDAR sensors for obstacle detection. In iGibson, we support virtual LiDAR signals, with both 1 beam (e.g. Hokuyo) and 16 beams (e.g. Velodyne VLP-16). We include a simple drop-out sensor noise model to emulate the common failure case in real sensors in which some of the laser pulses do not return. Additionally, we provide the functionality to turn the 1D LiDAR scans into local occupancy maps, which are bird's-eye view images with three types of pixels indicating free, occupied, or unknown space.</p>
<p>Additional Visual Channels: In addition to RGB and LiDAR, we support a wide range of visual modalities, such as depth maps, optical/scene flow and normals, segmentation of semantic class, instance, material and movable parts. These modalities can support research topics such as: depth/segmentation/normal/affordance prediction [48, 49, 50], action-conditioned flow prediction [51], multi-modal pose estimation [52, 53, 54], and visuomotor policy training assuming perfect vision systems [35, 55].</p>
<h2>D. Domain Randomization</h2>
<p>It is standard practice for robot learning to partially randomize the environment's parameters in order to make the policy more robust [56, 57, 58, 59]. With the model being trained in a wide distribution of environments, it will be more likely to generalize to unknown evaluation environments. The evaluation environment may be the real world if we aim to train in simulation and transfer the policy to a real robot. In iGibson, we include domain randomization that leads to an endless variation of visual appearance, dynamics properties and object instances with the same scene layout.</p>
<p>First, we provide object randomization. The original 3D reconstructions are annotated with class-labeled object bounding boxes. These labels can be used to instantiate any object model of the corresponding class into the given bounding box (e.g. a bounding box labeled as "table" can be filled with any table model). This randomization maintains the semantic layout of the scenes (i.e. the object categories remain at the same 3D locations) while enabling near-infinite</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Fifteen interactive iGibson 1.0 scenes modelled after real-world reconstructions, preserving layout, distribution and size of objects.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Robot interacting in iGibson 1.0 (large picture: 3rd person view) and virtual sensor signals generated. Policies and solutions can make use of the following channels: (from top to bottom, left to right) RGB, depth, semantic/instance segmentation, normals, 16D LiDAR (point cloud), 1D LiDAR (also as occupancy map). Not depicted: optical/scene flow, joint encoders for robot's and objects' joints, poses, wrenches, contact points, and map localization.</p>
<p>combinations of object instances. It provides strong variation in depth maps and LiDAR signals that helps robustify policies based on these observations (see Sec. IV-A).</p>
<p>Second, we provide material randomization. In addition to high-quality material annotation for object and scene models, we provide a mechanism to randomize the specific material model associated with each object part (e.g. associating a different type of wood or metal). The effect is a stark color randomization that still represents plausible material combinations. This randomization generates strong variations in the RGB images and helps robustify policies based on this observation (see Sec. IV-A). Moreover, the dynamics properties of all object links can be randomized based on a curated mapping from visual materials to dynamics properties.</p>
<h3><em>E. Motion Planning</em></h3>
<p>Motion planners provide collision-free trajectories to move a robot from an initial to a final configuration [60]. They can be used to generate collision-free navigation paths for robot bases and collision-free motion paths for robot arms. In iGibson, we include implementations of the most popular sampling-based motion planners: rapidly growing random trees (RRT [61]) and its bidirectional variant (BiRRT [62]), and lazy probabilistic road-maps (lazyPRM [63]), adapted from [64]. Sampling-based motion planners can have rather suboptimal and intricate paths. To alleviate this, we include acceleration-bounded shortcuts [65] for smoother paths.</p>
<h3><em>F. Human-iGibson Interface</em></h3>
<p>We provide a human-iGibson interface that enables users to navigate and interact in iGibson scenes using mouse and key commands on a viewer window. The user can navigate and interact with (pull, push, pick and place) objects. While a virtual reality or a 3D mouse interface may provide a more intuitive experience, most users do not have the necessary hardware. This interface offers a natural and simple way to demonstrations for imitation learning, evaluate the difficulty or feasibility of a task, or change the scene into a better initial state, for example. This interface is also integrated with the motion planner to command the robot to desired base and/or arm configurations. We verify this interface facilitates efficient development of interactive robotic solutions in Sec. IV-B.</p>
<h3>IV. EXPERIMENTS</h3>
<p>The goal of our experiments is to study how iGibson's features help to develop AI agents. Specifically, we examine:</p>
<ul>
<li>(Sec.IV-A) does iGibson's domain randomization and realistic virtual sensor signals (including LiDAR) allow navigation agents to generalize to unseen scenes (including the real world)?</li>
<li>(Sec.IV-B) can the human-iGibson interface be used to efficiently train imitation learning agents for manipulation and mobile manipulation tasks?</li>
<li>(Sec.IV-C) does the full interactivity in the scenes allow agents to learn visual representations that accelerate learning of downstream manipulation tasks?</li>
</ul>
<h3><em>A. Domain Randomization and Realistic Virtual Sensor Signals for Robot Navigation</em></h3>
<p>In the first three experiments, we evaluate the generalization benefits brought by our realistic virtual sensor signals (including LiDAR) and domain randomization. First, we compare the generalization capabilities of vision-based reinforcement learning policies trained with and without domain randomization. Concretely, we evaluate the performance of</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Robot navigating the real-world counterpart of the iGibson 1.0 scene Rs_int. The robot executes a policy trained in simulation with virtual LiDAR signals, without domain adaptation. The quality and realism of iGibson facilitates zero-shot policy transfer.</p>
<p>Policies trained in iGibson for PointGoal tasks [66] in held-out scenes with held-out visual textures. The observations for the policy include the depth maps, the robot's linear and angular velocities, the goal location in the robot's reference frame, and the next 10 waypoints in the shortest path between the robot's current location and the goal location, separated by 0.2 m. The waypoints are computed only based on the room layout, not the objects, so the robot mainly relies on depth maps for obstacle avoidance.</p>
<p>Second, we evaluate the performance of robot policies trained in iGibson to navigate to a target object (a lamp) using virtual RGB images also in held-out scenes with held-out visual textures. The task goal is to get at least 5% of the image occupied by the pixels of the target object. The observation for the policy only includes RGB images. In the above two experiments, we train in eleven scenes and evaluate in four held-out scenes with held-out visual textures.</p>
<p>Third, we evaluate the performance of the policies trained in iGibson for a PointGoal task [66] using virtual LiDAR signals with no vision inputs, and examine how well those policies transfer to the real world without adaptation, a hard test for generalization. The observations for the policy include a 1D LiDAR scan with 512 laser rays, the robot's linear and angular velocities, and the goal location in the robot's reference frame. To focus on sim2real transferability, we train in our scene Rs_int, for which we have access to the real-world counterpart (see Fig. 5).</p>
<p><em>Results:</em> In the first two experiments, we observe better generalization capabilities in policies using iGibson's domain randomization. For PointGoal navigation based on depth images, the performance goes from 0.27 to 0.40 SPL [66] and from 31.25% to 44.75% success rate when using randomization, indicating that the larger variety of shapes observed in the training process generates more robust depth-based policies. For object navigation based on RGB images, the performance goes from 49.75% to 57.5% success rate, indicating that material randomization helps in obtaining RGB-based policies that are more generalizable to unseen scenes and textures. Finally, for PointGoal navigation based on LiDAR signals, the policy achieves 33% success rate in Rs_int in iGibson, and 24% success rate in the real-world apartment. With only a 9% drop in performance and the failures mostly occurring in the same episodes (same pairs of the initial and goal locations in iGibson and real</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Imitation learning from human demonstration. <em>Top:</em> third-person view of a robot performing a pick-and-place task. The policy trained using demonstrations collected with our human-iGibson interface achieves 98% success rate. <em>Bottom:</em> first-person view of a robot performing a mobile manipulation task. The policy trained using teleoperated demonstrations achieved 70% success rate.</p>
<p>world), this experiment indicates that the LiDAR signals generated in iGibson are realistic enough to facilitate zero-shot policy transfer. In summary, as shown in Table. I, iGibson provides unique support to train with realistic virtual sensor signals (e.g., LiDAR) and domain randomization, which leads to more robust robot navigation policies that successfully transfer to novel scenes.</p>
<h3><em>B. Imitation Learning of Human Demonstrated (Mobile) Manipulation</em></h3>
<p>In the second set of experiments, we evaluate iGibson as platform to train robots to perform manipulation and mobile manipulation tasks with Imitation Learning. First, we test the usability of the human-iGibson interface to efficiently collect demonstrations of manipulation-only tasks. We collect 50 demonstrations of pick-and-place operations with 20 mug models: pick a mug and place in the sink (Fig. 6), and store pairs of state (object position) and action (desired end-effector translation). We use the demonstrations to train a behavioral cloning policy that maps states to actions at 20 Hz. The action space consists of two parts: a 3-dimensional continuous space for desired end-effector translation, and a 1-dimensional discrete space for gripper opening. The desired end-effector translation is computed using inverse kinematics (IK) and executed with a joint position controller. The evaluation is conducted using a simulated mobile manipulator (Fetch robot), and generalization is tested with 5 unseen mugs.</p>
<p>Second, we collect demonstrations for imitation learning through continuous teleoperation using a system similar to Roboturk [67] for mobile-manipulation tasks. We collect 200 demonstrations with a simulated Fetch robot for search-and-pick operations: the robot must navigate and interact with a cabinet to open drawers, find a bowl and pick it (Fig. 6 (bottom)). The bowl and robot initial poses are randomized between episodes. Using these demonstrations, we train a behavioral cloning policy mapping observations to actions at 20 Hz. The observation space includes the robot</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: <em>Left:</em> Example result of interaction pretraining. The agent receives RGB input and predicts if the pixels are pushable (red: higher probability; blue: lower) (Sec. IV-C). The model learns to associate edge of doors as the most pushable points. <em>Right:</em> Training curves for two interactive tasks (PushDrawer, PushCabinet) with and without interaction pretraining.</p>
<p>proprioceptive information (joint positions and velocities) and RGB-D images from virtual cameras on robot's head and wrist. The action space consists of four parts: a 1-dimensional discrete value indicating whether to extend the arm, 2-dimensional continuous values representing the robot base's linear and angular velocities, 6-dimensional continuous values representing the desired pose change of the end-effector, and 1-dimensional discrete value indicating whether to open the gripper.</p>
<p><em>Results:</em> In our first experiment, training manipulation-only policies with imitation learning, we observe 98% success rate over 100 evaluation episodes. This experiment showcases that the human-iGibson interface enables easy collection of effective demonstrations for imitation learning, and the integrated motion planner is helpful for policy training. These two integrated features, as shown in Table. I, are novel combinations offered by iGibson. In the second experiment, training mobile-manipulation policies with imitation learning, we observe 70% success rate over 20 evaluation episodes. This experiment indicates that we can leverage iGibson to train imitation learning algorithms for mobile manipulation tasks.</p>
<h3><em>C. Pretraining in Fully Interactive Scenes</em></h3>
<p>In the third and final set of experiments, we evaluate the potential of using iGibson's fully interactive scenes to learn an intermediate visual representation that encodes the expected outcome of interactions with different objects. Such an intermediate visual representation may be used to accelerate robot learning of manipulation tasks, since they typically require the agent to associate visual observations with promising areas of interaction to change the state of the scene towards a manipulation goal.</p>
<p>To learn such representations, we set up a virtual agent that interacts with random points in the scenes and learns to predict the outcome of these interactions. The interaction is parameterized as a coordinate in the virtual agent's image observation space. We emulate a pushing interaction by displacing the corresponding 3D location of the selected pixel by 30 cm in the opposite direction of the surface normal, applying a maximum force of 60 N (a common payload of commercial robots). A motion of the point for more than 10 cm is considered a success. We sample 10 random pushes at each location, 4,000 locations in each scene. We use the images annotated with interaction successes/failures to train a U-Net [68]-based visual encoder that predicts heatmaps of expected interaction success from RGB input.</p>
<p>For the second phase, we train two policy networks for two manipulation tasks respectively (PushDrawer, PushCabinet). The goal is to close the drawers or the cabinets. The policy outputs points to interact (push) that are given to one of our integrated motion planners to generate an arm motion [10]. We use DQN [69] as policy learning algorithm. The predicted interaction heatmaps are used to gate the Q-value maps predicted by the network.</p>
<p><em>Results:</em> Fig. 7 (left) depicts the result of the pretrained visual model. We observe that the heatmap has stronger activation at the edge of the door than in the area closer to the hinge, and weak activation on closed cabinets. The model learns to identify the best areas to push to cause motion (further visualizations on project website). For both downstream tasks, we observe that using the pre-trained representation significantly accelerates training (Fig. 7 (right)). This suggests that the full interactability of iGibson can help agents learn useful visual representation for downstream mobile manipulation tasks. Having a subset of objects not physically interactable will lead to false negatives during pretraining, and prevents successful representation learning. As shown in Table. I and discussed in Sec. II, fully interactive scenes with continuous robot actions is a specialty of iGibson.</p>
<h3>V. CONCLUSION</h3>
<p>We presented iGibson, a novel simulation environment for developing interactive robotic agents in large-scale realistic scenes. iGibson includes 15 fully interactive scenes with 108 rooms, and novel capabilities to generate high-quality virtual sensor signals, domain randomization, integration with motion planners, and human-iGibson interface. Through experiments, we showcased that iGibson helps to develop robust policies for navigation and manipulation. We hope that iGibson can aid researchers in solving complex robotics problems in large-scale realistic scenes.</p>
<h3>VI. ACKNOWLEDGEMENT</h3>
<p>We thank NVIDIA, Google, ONR MURI (N00014-14-1-0671), ONR (1165419-10-TDAUZ), Panasonic (1192707-1-GWMSX), Qualcomm and Samsung for their support.</p>
<h3>REFERENCES</h3>
<ul>
<li>[1] T. P. Lillicrap <em>et al.</em>, "Continuous control with deep reinforcement learning," <em>ICLR</em>, 2016.</li>
<li>[2] S. Levine, C. Finn, T. Darrell, and P. Abbeel, "End-to-end training of deep visuomotor policies," <em>JMLR</em>, 2016.</li>
<li>[3] P. Mirowski <em>et al.</em>, "Learning to navigate in complex environments," <em>ICLR</em>, 2017.</li>
<li>[4] E. Parisotto and R. Salakhutdinov, "Neural map: Structured memory for deep reinforcement learning," <em>ICLR</em>, 2018.</li>
<li>[5] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, "Target-driven visual navigation in indoor scenes using deep reinforcement learning," in <em>IEEE ICRA</em>, 2017.</li>
<li>[6] W. B. Shen <em>et al.</em>, "Situational fusion of visual representation for visual navigation," in <em>IEEE ICCV</em>, 2019.</li>
<li>[7] K. Chen, J. P. de Vicente, G. Sepulveda, F. Xia, A. Soto, M. Vázquez, and S. Savarese, "A behavioral approach to visual navigation with graph localization networks," <em>RSS</em>, 2019.</li>
<li>[8] C. R. Garrett <em>et al.</em>, "Ffrob: Leveraging symbolic planning for efficient task and motion planning," <em>IJRR</em>, 2018.</li>
</ul>
<p>[9] D. Xu et al., "Neural task programming: Learning to generalize across hierarchical tasks," in IEEE ICRA, 2018.
[10] F. Xia, C. Li, R. Martín-Martín, O. Litany, A. Toshev, and S. Savarese, "Relmogen: Leveraging motion generation in reinforcement learning for mobile manipulation," IEEE ICRA, 2021.
[11] E. Li, R. Martín-Martín, F. Xia, and S. Savarese, "Hrl4in: Hierarchical reinforcement learning forinteractive navigation with mobile manipulators," in CoRL, 2019.
[12] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, "Rlbench: The robot learning benchmark \&amp; learning environment," IEEE RA-L, 2020.
[13] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning," in CoRL, 2020.
[14] Y. Urakami, A. Hodgkinson, C. Carlin, R. Leu, L. Rigazio, and P. Abbeel, "Doorgym: A scalable door opening environment and baseline agent," arxiv:1908.01887, 2019.
[15] F. Xiang et al., "Sapien: A simulated part-based interactive environment," in IEEE CVPR, 2020.
[16] Y. Lee, E. S. Hu, Z. Yang, A. Yin, and J. J. Lim, "IKEA furniture assembly environment for long-horizon complex manipulation tasks," IEEE ICRA, 2021.
[17] Y. Zhu, J. Wong, A. Mandlekar, and R. Martín-Martín, "robosuite: A modular simulation framework and benchmark for robot learning," arxiv:2009.12293, 2020.
[18] Manolis Savva<em>, Abhishek Kadian</em>, Oleksandr Maksymets*, Y. Zhao, E. Wijnsens, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and O. Batra, "Habitat: A Platform for Embodied AI Research," in IEEE ICCV, 2019.
[19] E. Kolve et al., "Ai2-thor: An interactive 3d environment for visual ai," arxiv:1712.05474, 2017.
[20] X. Puig et al., "Virtualhome: Simulating household activities via programs," in IEEE CVPR, 2018.
[21] A. X. Chang et al., "Shapenet: An information-rich 3d model repository," arxiv:1512.03012, 2015.
[22] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, and K. Xu, "Shape2motion: Joint analysis of motion parts and attributes from 3d shapes," in IEEE CVPR, 2019.
[23] A. Kalervo, J. Ylioinas, M. Häikiö, A. Karhu, and J. Kannala, "Cubicasa5k: A dataset and an improved multi-task model for floorplan image analysis," in Scandinavian Conference on Image Analysis, 2019.
[24] H. Fu et al., "3d-front: 3d furnished rooms with layouts and semantics," arxiv:2011.09127, 2020.
[25] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, "Gibson env: Real-world perception for embodied agents," in CVPR, 2018.
[26] C. Gan et al., "Threadworld: A platform for interactive multi-modal physical simulation," arxiv:2007.04954, 2020.
[27] E. Coumans et al., "Bullet physics library," Open source: bulletphysics. org, 2013.
[28] E. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model-based control," in IROS, 2012.
[29] J. Lee, M. X. Grey, S. Ha, T. Kunz, S. Jain, Y. Ye, S. S. Srinivasa, M. Stilman, and C. K. Liu, "Dart: Dynamic animation and robotics toolkit," Journal of Open Source Software, 2018.
[30] "Unity," https://unity.com/, accessed: 2020-10-30.
[31] J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox, "Gpu-accelerated robotic simulation for distributed reinforcement learning," CoRL, 2018.
[32] "Ode," https://ode.org/, accessed: 2020-10-30.
[33] Z. Erickson et al., "Assistive gym: A physics simulation framework for assistive robotics," in IEEE ICRA, 2020.
[34] A. Chang et al., "Matterport3d: Learning from rgb-d data in indoor environments," in 3DV, 2017.
[35] F. Xia et al., "Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments," IEEE RA-L, 2020.
[36] G. Brockman et al., "Openai gym," arxiv:1606.01540, 2016.
[37] D. Merkel, "Docker: lightweight linux containers for consistent development and deployment," Linux journal, 2014.
[38] X. Meng, N. Ratliff, Y. Xiang, and D. Fox, "Scaling local control to large-scale topological navigation," in IEEE ICRA, 2020.
[39] ——, "Neural autonomous navigation with riemannian motion policy," in IEEE ICRA, 2019.
[40] K. Kang et al., "Generalization through simulation: Integrating simulated and real data into deep reinforcement learning for vision-based autonomous flight," IEEE ICRA, 2019.
[41] N. Hirose, F. Xia, R. Martín-Martín, A. Sadeghian, and S. Savarese,
"Deep visual mpc-policy learning for navigation," IEEE RA-L, 2019.
[42] "iGibson Sim2real Challenge at CVPR2020." [Online]. Available: http://svl.stanford.edu/igibson/challenge2020.html
[43] "iGibson Challenge at CVPR2021." [Online]. Available: http: //svl.stanford.edu/igibson/challenge.html
[44] C. Pérez-D'Arpino, C. Liu, P. Goebel, R. Martín-Martín, and S. Savarese, "Robot navigation in constrained pedestrian environments using reinforcement learning," IEEE ICRA, 2021.
[45] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su, "Partner: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding," in IEEE CVPR, 2019.
[46] C. Schlick, "An inexpensive bnff model for physically-based rendering," in Computer graphics forum, 1994.
[47] "Nadrin/pbr," https://github.com/Nadrin/PBR, accessed: 2020-10-30.
[48] L. Porzi, S. R. Bulo, A. Penate-Sanchez, E. Ricci, and F. MorenoNoguer, "Learning depth-aware deep representations for robotic perception," IEEE RA-L, 2016.
[49] V. Casser, S. Pirk, R. Mahjourian, and A. Angelova, "Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos," in AAAI, 2019.
[50] C. Xie, Y. Xiang, A. Mousavian, and D. Fox, "The best of both modes: Separately leveraging rgb and depth for unseen object instance segmentation," in CoRL, 2020.
[51] I. Nematollahi, O. Mees, L. Hermann, and W. Burgard, "Hindsight for foresight: Unsupervised structured dynamics models from physical interaction," IEEE IROS, 2020.
[52] C. Choi and H. I. Christensen, "Rgb-d object pose estimation in unstructured environments," Robotics and Autonomous Systems, 2016.
[53] C. Wang et al., "6-pack: Category-level 6d pose tracker with anchorbased keypoints," in IEEE ICRA, 2020.
[54] Y. Hu, J. Hugonot, P. Fua, and M. Salzmann, "Segmentation-driven 6d object pose estimation," in IEEE CVPR, 2019.
[55] M. Yan, Q. Sun, I. Frosio, S. Tyree, and J. Kautz, "How to close sim-real gap? transfer with segmentation!" arxiv:2005.07695, 2020.
[56] F. Sadeghi and S. Levine, "Cad2rl: Real single-image flight without a single real image," RSS, 2016.
[57] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in IROS, 2017.
[58] S. James et al., "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks," in IEEE CVPR, 2019.
[59] M. Andrychowicz et al., "Learning dexterous in-hand manipulation," IJRR, 2020.
[60] S. M. LaValle, Planning algorithms. Cambridge university press, 2006.
[61] S. M. Lavalle, "Rapidly-exploring random trees: A new tool for path planning," Department of Computer Science; Iowa State University, Tech. Rep., 1998.
[62] A. H. Qureshi and Y. Ayaz, "Intelligent bidirectional rapidly-exploring random trees for optimal motion planning in complex cluttered environments," Robotics and Autonomous Systems, 2015.
[63] R. Bohlin and L. E. Kavraki, "Path planning using lazy prm," in IEEE ICRA, 2000.
[64] Caelan Reed Garrett, "PyBullet Planning." https://pypi.org/project/ pybullet-planning/, 2018.
[65] K. Hauser and V. Ng-Thow-Hing, "Fast smoothing of manipulator trajectories using optimal bounded-acceleration shortcuts," in IEEE ICRA, 2010.
[66] P. Anderson et al., "On evaluation of embodied navigation agents," arxiv:1807.06757, 2018.
[67] A. Mandlekar et al., "Roboturk: A crowdsourcing platform for robotic skill learning through imitation," in CoRL, 2018.
[68] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," in International Conference on MICCAI, 2015.
[69] H. Van Hasselt, A. Guez, and D. Silver, "Deep reinforcement learning with double q-learning," in AAAI, 2016.
[70] S. Qiao, H. Wang, C. Liu, W. Shen, and A. Yuille, "Weight standardization," arxiv:1903.10520, 2019.
[71] T. Haarnoja et al., "Soft actor-critic algorithms and applications," arxiv:1812.05905, 2018.
[72] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng, "Ros: an open-source robot operating system," in ICRA workshop on open source software, 2009.</p>
<p>[73] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” <em>ICLR</em>, 2015.</p>
<h2>APPENDIX</h2>
<h3><em>A. Details on the Experimental Evaluation</em></h3>
<p>In this section, we provide additional information on the experiments presented in Sec. IV such as training processes and architectures, or system characteristics.</p>
<h4><em>1) Domain Randomization for Visual Navigation</em></h4>
<p>For PointGoal navigation, as mentioned in the main paper, the observations for the policy include the depth maps, the robot's linear and angular velocities, the goal location in the robot's reference frame, and the location of the next 10 waypoints in the shortest path between the robot's current location and the goal location. We concatenate the robot's linear and angular velocities, the goal location, and the location of the next 10 waypoints together and denote them by sensor observations. For both depth maps and sensor observations, we adopt a frame stack of 4. The encoder of the policy network consists of 3 parts: (a) a 9-layer ResNet with Weight Standardization [70] and GroupNorm, followed by a 3-layer Conv1D block to encode depth maps; (b) a 2-layer MLP to encode sensor observations; (c) a 2-layer fusion MLP to encode the concatenation of depth maps embedding and sensor observations embedding. The learning rate for SAC is 1e-4. For object navigation, the observation only includes a frame stack of 4 RGB images. The encoder of the policy network is a 9-layer ResNet with Weight Standardization and GroupNorm, followed by a 3-layer Conv1D block to encode RGB images. The learning rate for SAC is 5e-4. To accelerate the training of our visual policies, we implement a multi-GPU distributed reinforcement learning pipeline based on SAC [71]. iGibson can be easily parallelized and deployed on large computing clusters, which makes the training highly efficient.</p>
<p>Tables II and III include a breakdown analysis of the results of the experiment for different scenes and phases of the training process.</p>
<h4><em>2) LiDAR-Based Point-to-Point Navigation</em></h4>
<p>As mentioned in the main paper, the observations for the policy include a 1D LiDAR scan with 512 laser rays, the robot's linear and angular velocities, and the goal location in the robot's reference frame. Similar to PointGoal navigation in Sec. VI-A.1, we concatenate the robot's linear velocity, angular velocity and the goal location and denote them by sensor observations. The controller runs at 10Hz. For both LiDAR scans and sensor observations, we adopt a frame stack of 8. The encoder of the policy network consists of 3 parts: (a) a 3-layer MLP to encode LiDAR scans; (b) a 3-layer MLP to encode sensor observations; (c) a 3-layer fusion MLP to encode the concatenation of the flattened (along temporal dimension) LiDAR scans embedding and sensor observations embedding. The learning rate for SAC is 1e-4.</p>
<p>As real robot platform we use a Locobot, a non-holonomic mobile base, with an additionally mounted Hokuyo 1 beam LiDAR sensor (see Fig. 5, left). The simulated agents matches the characteristics of the real robot and sensor. The robot is controlled via ROS [72]. Processing the images requires more computation than the one provided by the onboard computer of the Locobot; thus, we send the images to a desktop computer that hosts the policy and generate commands that are sent back to the robot. We use the system we developed for the CVPR Challenge “Sim2Real with iGibson” [42].</p>
<h4><em>3) Imitation Learning: Human Demonstrated Manipulation</em></h4>
<p>We collected 50 demonstrations of pick-and-place operations demonstrated with our Human-iGibson interface to pick a mug and place in the sink (Fig. 6, right) with 20 different mug models. We split the demos into train/val/test set with 35, 10, 5 demos and 1095, 304, 157 pairs of state and action, respectively. The state includes the object position and the action includes the desired delta translation to add to the object position in the next step and whether</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8: Execution of (a) PushDrawer task. The agent successfully push the top drawers in. (b) PushCabinet task. The agent push and close the cabinet doors.</p>
<p>to open gripper. The starting position of the mug is randomized within a 50cm by 10cm area.</p>
<p>We trained a policy using behavioral cloning to map states to actions. The policy network is composed by three MLP layers and ReLU activation. We used MSE loss for the delta translation action and cross entropy loss for the binary gripper action. We used ADAM optimizer [73] with learning rate 0.1 and trained the policy for 1000 epochs after validation loss plateaus.</p>
<p>For evaluation, we deployed the policy on the simulated Fetch robot and achieved 98% success rate for 100 evaluation episodes. The two failed episodes are caused by the robot prematurely opening its gripper and the mug being dropped outside the sink. The robot has a time budget of 500 action steps (25 seconds) to accomplish the task.</p>
<h4><em>4) Pretraining in Fully Interactive Scenes</em></h4>
<p>In this section we show the details of the network used in pretraining and the details for policy training in downstream tasks. The network used in pretraining is a UNet structure. The input to the UNet is an RGB image of size 128×128, and the output is a binary mask indicating which area is interactable. The UNet consists of an encoder with ResNet9 architecture, 4 (Upsampling, Conv) blocks and finally a readout module consisting of 2 Conv layers predicting the binary mask.</p>
<p>For the downstream task, we focused on pushing task. The observation space is RGB images of size 128×128, and the action space is a point on the image. To give readers an intuitive sense, an example trajectory of the two tasks are shown in Fig. 8. The baseline model for these tasks are DQN with dense Q-value prediction. The policy network uses a 6-layer convolutional neural network to predict an array of Q-values, with the same shape as the input image. The agent picks the pixel with the highest associated Q-value, and use a motion planner to plan a push motion. The method that integrates pretraining modifies the baseline method, by adding a mask to the predicted Q-values with predicted interaction mask, only pixels that are predicted to be interactable keep the Q-values, and the rest are zeroed out. Both setups uses Q-learning algorithm with ϵ-greedy to update the network, we use a learning rate of 1×10<sup>−3</sup>, discount factor of 0.99, ϵ = 0.2 for exploration, and the policies are trained for 5×10<sup>4</sup> steps.</p>
<p>As shown in the main paper, with pretraining, the agent learns faster. Fig. 8 depicts different stages of the interactions learned by an agent using our learned intermediate representation.</p>
<h3><em>B. Physics Based Rendering</em></h3>
<p>In this section we include additional information about the shading model and rendering process we use in iGibson.</p>
<p><strong>Shading Models</strong> To improve the realism when rendering images of objects, we represent the properties of the surface material of each object using four layers, 1) metallic layer (single channel</p>
<p>TABLE II: Quantitative results on PointGoal navigation.</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Training</th>
<th>Rs_int</th>
<th>Beechwood_0_int</th>
<th>Merom_0_int</th>
<th>Pomaria_0_int</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>Randomization</td>
<td>Steps</td>
<td>SPL</td>
<td>SR</td>
<td>SPL</td>
<td>SR</td>
<td>SPL</td>
</tr>
<tr>
<td>w/o</td>
<td>400k</td>
<td>0.41</td>
<td>42%</td>
<td>0.21</td>
<td>21%</td>
<td>0.22</td>
</tr>
<tr>
<td>w/</td>
<td>400k</td>
<td>0.35</td>
<td>36%</td>
<td>0.25</td>
<td>26%</td>
<td>0.32</td>
</tr>
<tr>
<td>w/o</td>
<td>800k</td>
<td>0.50</td>
<td>56%</td>
<td>0.14</td>
<td>17%</td>
<td>0.22</td>
</tr>
<tr>
<td>w/</td>
<td>800k</td>
<td>0.60</td>
<td>67%</td>
<td>0.37</td>
<td>40%</td>
<td>0.29</td>
</tr>
</tbody>
</table>
<p>TABLE III: Quantitative results on object navigation.</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Training</th>
<th>Rs_int</th>
<th>Beechwood_0_int</th>
<th>Merom_0_int</th>
<th>Pomaria_0_int</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>Randomization</td>
<td>Steps</td>
<td>SR</td>
<td>SR</td>
<td>SR</td>
<td>SR</td>
<td>SR</td>
</tr>
<tr>
<td>w/o</td>
<td>100k</td>
<td>43%</td>
<td>34%</td>
<td>47%</td>
<td>45%</td>
<td>42.25%</td>
</tr>
<tr>
<td>w/</td>
<td>100k</td>
<td>40%</td>
<td>45%</td>
<td>52%</td>
<td>51%</td>
<td>47%</td>
</tr>
<tr>
<td>w/o</td>
<td>200k</td>
<td>50%</td>
<td>46%</td>
<td>48%</td>
<td>55%</td>
<td>49.75%</td>
</tr>
<tr>
<td>w/</td>
<td>200k</td>
<td>55%</td>
<td>54%</td>
<td>64%</td>
<td>57%</td>
<td>57.5%</td>
</tr>
</tbody>
</table>
<p>image), 2) roughness layer (single channel image), 3) albedo (three channel image), and 4) tangent-space normal mapping (three channel image). The information of the layers is combined by our physics-based rendering model. To generate correct light effects on the objects' surfaces, we create environment maps that light the scene. We also pre-integrate the Cook-Torrance specular bidirectional scattering distribution function (BRDF) for varying roughness and viewing directions to accelerate and enable real time rendering. The results are saved into 2-dimensional look up table texture, which is used to scale and add bias to Fresnel reflectance at normal incidence (F₀) during rendering. The F₀ is then used to calculate Shlick's approximation of the Fresnel factor as follows:</p>
<p>$$F(\theta) = F_0 + (1 - F_0)(1 - \cos\theta)^5$$</p>
<p>To calculate the specular highlights, we pre-filter environment cube map using GGX normal distribution function importance sampling. The results are saved as an image for later faster retrieval. For diffuse effect we use quasi Monte Carlo sampling with Hammersley sequence to approximate the integration.</p>
<p><strong>Light Probe Generation</strong> To generate light probes, we use Blender to bake high resolution high dynamic range (HDR) environment textures within iGibson scenes. The light sources are artistically designed and placed on the ceiling.</p>
<p><strong>Shadow Mapping</strong> For generating shadows we use shadow mapping techniques and simulate orthogonal uniform light in +z direction. All the objects in the scenes are shadow casters except for the ceiling. The shadows don't necessarily match the real world indoor lights, but create a realistic enough effect that help perceiving depth.</p>
<h3><em>C. iGibson Performance</em></h3>
<p>In this section we evaluate the speed of the iGibson simulation environment, analyzing the time for rendering and physics simulation, and its combination.</p>
<h4><em>1) Rendering Performance</em></h4>
<p>We evaluate the performance and speed of our novel iGibson renderer with different working settings. There are many configurable options in iGibson to control the rendering quality. We benchmark two typical use cases. The first one is Reinforcement Learning (VisualRL), where the goal is to generate simulated sensor data (low resolution) as fast as possible. The second one is when higher quality images, as photorealistic as possible, are required, but maintaining a minimum level of rendering speed (HighFidelity). This use case is common when training perceptual solutions (e.g., segmentation, object detection) and also to support collecting human demonstrations with photorealistic enough images and effects such as shadows.</p>
<p><strong>TABLE IV: Rendering Speed of iGibson Renderer [fps]</strong></p>
<table>
<thead>
<tr>
<th>Preset</th>
<th>Modality</th>
<th>Mean</th>
<th>Max</th>
<th>Min</th>
</tr>
</thead>
<tbody>
<tr>
<td>VisualRL</td>
<td>RGB</td>
<td>409.8</td>
<td>1142.7</td>
<td>270.4</td>
</tr>
<tr>
<td></td>
<td>Normal</td>
<td>530.9</td>
<td>1142.0</td>
<td>283.7</td>
</tr>
<tr>
<td></td>
<td>Point Cloud</td>
<td>530.3</td>
<td>1129.0</td>
<td>282.1</td>
</tr>
<tr>
<td></td>
<td>Semantic Mask</td>
<td>529.4</td>
<td>1140.4</td>
<td>281.7</td>
</tr>
<tr>
<td></td>
<td>Optical Flow</td>
<td>528.1</td>
<td>1129.1</td>
<td>281.6</td>
</tr>
<tr>
<td></td>
<td>Scene Flow</td>
<td>526.4</td>
<td>1129.9</td>
<td>280.1</td>
</tr>
<tr>
<td>HighFidelity</td>
<td>RGB</td>
<td>188.4</td>
<td>289.3</td>
<td>114.3</td>
</tr>
<tr>
<td></td>
<td>Normal</td>
<td>219.6</td>
<td>310.1</td>
<td>148.9</td>
</tr>
<tr>
<td></td>
<td>Point Cloud</td>
<td>240.4</td>
<td>345.1</td>
<td>174.7</td>
</tr>
<tr>
<td></td>
<td>Semantic Mask</td>
<td>221.5</td>
<td>313.8</td>
<td>163.5</td>
</tr>
<tr>
<td></td>
<td>Optical Flow</td>
<td>240.1</td>
<td>348.9</td>
<td>170.8</td>
</tr>
<tr>
<td></td>
<td>Scene Flow</td>
<td>240.5</td>
<td>345.9</td>
<td>173.9</td>
</tr>
</tbody>
</table>
<p><strong>TABLE V: Simulator Step Speed and Full Step Speed [Hz]</strong></p>
<table>
<thead>
<tr>
<th>Simulator Physics Step</th>
<th>Robot</th>
<th>Mean</th>
<th>Max</th>
<th>Min</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>With robot</td>
<td>175</td>
<td>311</td>
<td>130</td>
</tr>
<tr>
<td></td>
<td>Scene only</td>
<td>310</td>
<td>797</td>
<td>92</td>
</tr>
<tr>
<td>Simulator Full Step</td>
<td>With robot</td>
<td>100</td>
<td>150</td>
<td>68</td>
</tr>
<tr>
<td></td>
<td>Scene only</td>
<td>136</td>
<td>230</td>
<td>84</td>
</tr>
</tbody>
</table>
<p>For the first use case, VisualRL, we render 128 × 128 images, with physically based rendering on, multi sample anti-aliasing off, and shadow mapping off. For the second case, HighFidelity, we render 512 × 512 images, with physically based rendering on, multi sample anti-aliasing on, shadow mapping on. The results of our benchmark of the rendering time for different modalities with these settings are shown in Table IV.</p>
<h4><em>2) Physics Simulator and Full Simulator Performance</em></h4>
<p>The size and number of objects included in our scenes are beyond what is typically included in other simulator or other projects based on PyBullet. We benchmarked the performance of the physics simulator and the final performance of iGibson, combining physics and rendering. In our experiments, the physics simulation timestep is set to 1/120s. Each simulator step includes four steps of the physics simulator and one rendering pass, corresponding to rendering at 30 fps in simulation. The results of our analysis are shown in Table V. We achieve an average of 100 Hz with robot for simulator full step, and 136 Hz without robot, which correspond to 3.33× and 4.53× real-time respectively.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9: Examples of iGibson scenes created based on the CubiCasa5K and 3D-Front annotations. We provide over 12000 additional fully interactive iGibson scenes based the two datasets.</p>
<h3><em>D. Integration of Additional Datasets of Scenes</em></h3>
<p><em>1) Integration of CubiCasa5K:</em> CubiCasa5K [23] is a dataset of five thousand annotated floor plans of real world homes in Finland. The annotated floor plans are semi-automatically generated and include the structural elements (walls, doors, windows) and position and size of fixed furniture items (closets, toilets, benches, embedded cabinets, counters, sinks, …). We convert this annotations into iGibson 3D fully interactive scenes with a two-step procedure: 1) generate a building based on the annotation of structural elements, and 2) populate the building with object models from our dataset based on the description of poses and sizes from CubiCasa5K. Some of the floor plans in CubiCasa5K included two separate floors; since we do not include outdoor navigation in iGibson, we split them into two separate indoor scenes. In total, we offer 6297 scene in iGibson based on the real-world layouts of CubiCasa5K. Fig. 9 depicts some examples of the scenes included created based on the CubiCasa5K dataset.</p>
<p><em>2) Integration of 3D-Front:</em> 3D-Front [24] is a large dataset of layouts with room models populated with furniture. The layouts have been created by artists and interior designers. It includes 18,797 rooms, around 10,000 houses, and 7,302 furniture models. We convert 3D-Front static scenes into iGibson 3D fully interactive scenes with a two-step procedure: 1) we keep the original building structural meshes as visual meshes while procedurally generating collision meshes for structural elements that approximate their geometry, and 2) we populate the building with object models from our dataset based on the description of poses and sizes from 3D-Front.</p>
<p>There are four challenges we faced when integrating 3D-Front to convert their scenes into interactive scenes for iGibson. First, some 3D-Front scenes contain objects of undefined categories, corresponding to one-of-a-kind pieces. We skip these scenes, since we cannot generate appropriate low-poly collision meshes for these objects. Second, some of the 3D data within 3D-Front is corrupted or contain shape errors (see reported issue in this link). Third, the kitchen cabinets in 3D-Front are not annotated as objects, but instead the entire kitchen furniture is a single object with each panel of the furniture (front and lateral panels, internal shelves) annotated as a separate part. This impedes us to generate interactive versions of the kitchen cabinets. We include two alternative versions of the scenes: a) a version with non-interactive kitchen cabinets, and b) a version without any kitchen cabinets. We expect this problem to be solved in future annotations of 3D-Front. Fourth, while 3D-Front dataset includes a layout description of rooms and elements, including their position and size, the furniture pieces are sometimes defined as overlapping significantly with each other (see reported issue in this link). This has a severe effect in our physics simulation as it tries to solve the penetrating contact. To alleviate this issue, we remove objects that overlap more than 80% of volume with others. For objects that overlap less than 80%, we reduce their size (with a threshold of reducing no more than 20%) until the overlapping is</p>
<p>resolve. After all the aforementioned filtering and fixing processes, we obtained 6049 scenes with correct object placements without overlapping, ready to be used in interactive tasks.</p>
<p><em>3) Comparing iGibson, CubiCasa5K and 3D-Front Scenes:</em> There are two significant differences between the 15 fully interactive iGibson scenes, and the ones obtained from the integration of Cubicasa5K and 3D-Front. The first one is in density of objects. Our iGibson curated scenes contain a density of 75 objects per room. This is more realistic and significantly higher density than the one in 3D-Front rooms (37) and CubiCasa5K rooms (32). A second difference between the scenes is in the albedo and material effects of walls and floors. In the 15 iGibson scenes provided, we bake the effects of the ambient lighting and the light sources on walls, floors and ceilings into a RGB texture map. This provides a more realistic lighting effect. Differently, the structural elements of CubiCasa5K and 3D-Front do not present this enhanced diffuse color channel. To bake lighting sources in CubiCasa5K and 3D-Front scenes, we need lighting information that are not contained as part of the datasets. It would require a manual annotation of light location and strength in thousands of scenes, an effort that is beyond our attempt to make these datasets available to the community. Despite these differences, we believe getting access to these two large datasets of scenes significantly complements the lower number (15) but higher quality scenes provided with iGibson.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution. All authors are with the Stanford Vision \&amp; Learning Laboratory, Stanford University.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
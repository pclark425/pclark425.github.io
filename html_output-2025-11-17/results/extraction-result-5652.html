<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5652 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5652</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5652</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268187341</p>
                <p><strong>Paper Title:</strong> Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning</p>
                <p><strong>Paper Abstract:</strong> This perspective piece explores the transformative potential and associated challenges of large language models (LLMs) in education and how those challenges might be addressed utilizing playful and game-based learning. While providing many opportunities, the stochastic elements incorporated in how present LLMs process text, requires domain expertise for a critical evaluation and responsible use of the generated output. Yet, due to their low opportunity cost, LLMs in education may pose some risk of over-reliance, potentially and unintendedly limiting the development of such expertise. Education is thus faced with the challenge of preserving reliable expertise development while not losing out on emergent opportunities. To address this challenge, we first propose a playful approach focusing on skill practice and human judgment. Drawing from game-based learning research, we then go beyond this playful account by reflecting on the potential of well-designed games to foster a willingness to practice, and thus nurturing domain-specific expertise. We finally give some perspective on how a new pedagogy of learning with AI might utilize LLMs for learning by generating games and gamifying learning materials, leveraging the full potential of human-AI interaction in education.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5652.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5652.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The iterative practice of crafting and refining textual inputs (prompts) to steer LLM outputs toward desired content, style, or behavior; described as an emerging expertise and digital literacy skill across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs; e.g., ChatGPT, Bard)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Diverse domain tasks (writing, software development, entrepreneurship, science, healthcare, game generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A heterogenous set of generation and assistance tasks where users request text, code, explanations, or game artefacts from LLMs across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Iterative textual input refinement incorporating multiple modifiers and patterns (role/persona assignment, explicit style/tone/length constraints, question rephrasing, chain-of-thought instructions, flipped interaction patterns, emotional cues, stepwise decomposition, etc.). Prompts may be organized into templates and staged sequences for multi-phase tasks (idea generation → prototyping → evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Narrative claim: optimizing prompts has been shown in cited work to substantially improve output accuracy and usefulness; no quantitative values provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompt engineering improves outputs by increasing contextual tenability and steering model behaviour; it is itself a skill that requires domain expertise and knowledge of model/system settings; effective prompting plus domain-critical review together yield reliable results.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5652.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5652.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting (intermediate reasoning elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that instructs the model to produce intermediate reasoning steps (step-by-step solutions) rather than only final answers, intended to improve complex reasoning and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning and multi-step problem solving (examples given generically)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that benefit from multi-step reasoning, e.g., multi-step math, code explanation, or complex decision tasks where intermediate logic aids final correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Explicit instruction to generate intermediate steps or to 'work step-by-step' (chain-of-thought prompts); may include exemplar stepwise demonstrations or explicit cues like 'Take a deep breath and work on this problem step-by-step'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicitly compared to standard (non-chain-of-thought) prompts where only final answer is requested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper cites other work indicating chain-of-thought style prompting can improve accuracy but provides no numerical metrics itself.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Eliciting intermediate reasoning helps the model produce more accurate, coherent solutions by making internal reasoning explicit and guiding multi-step decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5652.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5652.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Role/persona prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role or persona prompting (persona modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting pattern that assigns the LLM a specific role or persona (e.g., 'act as an expert X') to bias style, depth, and perspective of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The artificially intelligent entrepreneur: ChatGPT, prompt engineering, and entrepreneurial rhetoric creation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Domain-specific advisory, content generation, instructional design, and other tasks where expertise framing is relevant</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that benefit from an expert stance or particular viewpoint (e.g., requesting an LLM to adopt 'instructional designer' role for game design suggestions).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Include role assignment in prompt (e.g., 'You are an expert X, provide...'), sometimes combined with other constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly to neutral or unspecified-role prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports role prompting as a common and effective prompt engineering technique per cited literature; no numeric performance reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Specifying a role increases contextual tenability and guides the model to produce content matching domain expectations and desired style.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5652.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5652.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emotional prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emotional prompting (emotionally salient cues in prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategy that embeds emotionally salient language (e.g., stressing importance to user) to influence model responses and possibly enhance model engagement or emphasis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models understand and can be enhanced by emotional stimuli.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tasks sensitive to tone, persuasion, or motivational framing (broadly described)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generation tasks where emotional framing might alter emphasis, persuasiveness, or style of output.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Add emotionally loaded phrases or stakes to prompt (e.g., 'This is very important for my career') to bias model output.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly to neutral prompts without emotional cues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Cited research suggests emotional cues can enhance model outputs in some respects; the present paper gives no quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Emotional stimuli change model behavior by providing additional contextual signals that the model uses to prioritize or style responses.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5652.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5652.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flipped interaction / question-first prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flipped interaction prompting (requesting questions rather than elaboration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt pattern that flips the usual interaction by asking the LLM to generate questions (e.g., Socratic questioning) or take an alternate interaction mode instead of immediately producing an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A prompt pattern catalog to enhance prompt engineering with ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Idea exploration, tutoring, iterative design (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where eliciting probing questions or alternative perspectives from the model can improve exploration or scaffold learning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Request alternative interaction patterns such as 'ask me clarifying questions' or 'generate diagnostic questions' rather than providing final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to standard answer-provision prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper cites flipped interaction as a useful pattern and notes such patterns generalize across domains; no numeric performance reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Flipped patterns change the dialogue dynamics, encouraging deeper exploration and iterative refinement, which can lead to higher-quality collaborative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5652.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5652.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formatting and constraint modifiers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formatting, style, length and language constraints in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of explicit formatting instructions, style/tone directives, length limits, and coding-language constraints in prompts to control output form and suitability for specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (e.g., ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Writing, code generation, and domain-specific content creation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where output format, code language, or stylistic conventions matter (e.g., asking for Python code vs. pseudocode, or academic tone vs. lay explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Add explicit formatting tokens, style instructions, length limits, or specify coding language and output structure in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly to unconstrained, free-form prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper states such modifiers are part of prompt engineering and can materially affect output quality; no quantitative results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicit format and constraint instructions reduce ambiguity and better align model outputs with user needs; nevertheless, domain expertise is still needed to judge correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5652.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5652.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented generation for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Technique that augments a generative model with an external retrieval system (documents, databases) to ground responses in up-to-date and factual sources, improving timeliness and factual reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for large language models: A survey.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs augmented with retrieval modules</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factual QA, time-sensitive information retrieval, knowledge-grounded generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring up-to-date or verifiable factual content where the base LLM's training cutoff or hallucination risk is problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Combine retrieval step (fetching relevant documents) with generation step so the model conditions on retrieved context when producing its answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly to vanilla generation without retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper cites RAG as an ongoing development that improves factual reliability and timeliness; no numeric performance figures in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>RAG supplies models with external evidence, reducing hallucinations and extending factual currency beyond model training data.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5652.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5652.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting pedagogy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting pedagogy (this paper's term)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pedagogical framework proposed by the authors that integrates prompt engineering into learning workflows, emphasizing scaffolds (templates), phased prompting activities, and critical evaluation to teach students to generate games and learning artefacts with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (used as tools in learning-by-making games)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Learning-by-generating-games, gamification of learning materials, multi-phase design tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Educational design and game-making workflows where students use LLMs to prototype and iterate on game ideas, requiring staged prompting (idea generation → design → prototyping → assessment).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Sequenced prompt templates, support materials, and phase-specific prompting activities tailored to each step of game creation and evaluation; encourages dialogic, iterative prompting and critical review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Teaching prompt engineering as pedagogy can build students' ability to use LLMs constructively while preserving domain expertise through iterative critical evaluation; framing prompting as a practiced skill parallels constructionist approaches (e.g., Papert).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A prompt pattern catalog to enhance prompt engineering with ChatGPT. <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers. <em>(Rating: 2)</em></li>
                <li>Large language models understand and can be enhanced by emotional stimuli. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey. <em>(Rating: 2)</em></li>
                <li>Prompt engineering for healthcare: Methodologies and applications. <em>(Rating: 1)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering. <em>(Rating: 1)</em></li>
                <li>Prompt engineering as an important emerging skill for medical professionals. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5652",
    "paper_id": "paper-268187341",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Prompt engineering",
            "name_full": "Prompt engineering (general)",
            "brief_description": "The iterative practice of crafting and refining textual inputs (prompts) to steer LLM outputs toward desired content, style, or behavior; described as an emerging expertise and digital literacy skill across domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs; e.g., ChatGPT, Bard)",
            "model_size": null,
            "task_name": "Diverse domain tasks (writing, software development, entrepreneurship, science, healthcare, game generation)",
            "task_description": "A heterogenous set of generation and assistance tasks where users request text, code, explanations, or game artefacts from LLMs across domains.",
            "problem_format": "Iterative textual input refinement incorporating multiple modifiers and patterns (role/persona assignment, explicit style/tone/length constraints, question rephrasing, chain-of-thought instructions, flipped interaction patterns, emotional cues, stepwise decomposition, etc.). Prompts may be organized into templates and staged sequences for multi-phase tasks (idea generation → prototyping → evaluation).",
            "comparison_format": null,
            "performance": "Narrative claim: optimizing prompts has been shown in cited work to substantially improve output accuracy and usefulness; no quantitative values provided in this paper.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Prompt engineering improves outputs by increasing contextual tenability and steering model behaviour; it is itself a skill that requires domain expertise and knowledge of model/system settings; effective prompting plus domain-critical review together yield reliable results.",
            "counterexample_or_null_result": null,
            "uuid": "e5652.0",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Chain-of-thought prompting",
            "name_full": "Chain-of-thought prompting (intermediate reasoning elicitation)",
            "brief_description": "A prompting technique that instructs the model to produce intermediate reasoning steps (step-by-step solutions) rather than only final answers, intended to improve complex reasoning and accuracy.",
            "citation_title": "Large language models as optimizers.",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs)",
            "model_size": null,
            "task_name": "Reasoning and multi-step problem solving (examples given generically)",
            "task_description": "Tasks that benefit from multi-step reasoning, e.g., multi-step math, code explanation, or complex decision tasks where intermediate logic aids final correctness.",
            "problem_format": "Explicit instruction to generate intermediate steps or to 'work step-by-step' (chain-of-thought prompts); may include exemplar stepwise demonstrations or explicit cues like 'Take a deep breath and work on this problem step-by-step'.",
            "comparison_format": "Implicitly compared to standard (non-chain-of-thought) prompts where only final answer is requested.",
            "performance": "Paper cites other work indicating chain-of-thought style prompting can improve accuracy but provides no numerical metrics itself.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Eliciting intermediate reasoning helps the model produce more accurate, coherent solutions by making internal reasoning explicit and guiding multi-step decomposition.",
            "counterexample_or_null_result": null,
            "uuid": "e5652.1",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Role/persona prompting",
            "name_full": "Role or persona prompting (persona modeling)",
            "brief_description": "Prompting pattern that assigns the LLM a specific role or persona (e.g., 'act as an expert X') to bias style, depth, and perspective of outputs.",
            "citation_title": "The artificially intelligent entrepreneur: ChatGPT, prompt engineering, and entrepreneurial rhetoric creation.",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs)",
            "model_size": null,
            "task_name": "Domain-specific advisory, content generation, instructional design, and other tasks where expertise framing is relevant",
            "task_description": "Tasks that benefit from an expert stance or particular viewpoint (e.g., requesting an LLM to adopt 'instructional designer' role for game design suggestions).",
            "problem_format": "Include role assignment in prompt (e.g., 'You are an expert X, provide...'), sometimes combined with other constraints.",
            "comparison_format": "Compared implicitly to neutral or unspecified-role prompts.",
            "performance": "Paper reports role prompting as a common and effective prompt engineering technique per cited literature; no numeric performance reported.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Specifying a role increases contextual tenability and guides the model to produce content matching domain expectations and desired style.",
            "counterexample_or_null_result": null,
            "uuid": "e5652.2",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Emotional prompting",
            "name_full": "Emotional prompting (emotionally salient cues in prompts)",
            "brief_description": "Prompting strategy that embeds emotionally salient language (e.g., stressing importance to user) to influence model responses and possibly enhance model engagement or emphasis.",
            "citation_title": "Large language models understand and can be enhanced by emotional stimuli.",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs)",
            "model_size": null,
            "task_name": "Tasks sensitive to tone, persuasion, or motivational framing (broadly described)",
            "task_description": "Generation tasks where emotional framing might alter emphasis, persuasiveness, or style of output.",
            "problem_format": "Add emotionally loaded phrases or stakes to prompt (e.g., 'This is very important for my career') to bias model output.",
            "comparison_format": "Compared implicitly to neutral prompts without emotional cues.",
            "performance": "Cited research suggests emotional cues can enhance model outputs in some respects; the present paper gives no quantitative results.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Emotional stimuli change model behavior by providing additional contextual signals that the model uses to prioritize or style responses.",
            "counterexample_or_null_result": null,
            "uuid": "e5652.3",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Flipped interaction / question-first prompting",
            "name_full": "Flipped interaction prompting (requesting questions rather than elaboration)",
            "brief_description": "A prompt pattern that flips the usual interaction by asking the LLM to generate questions (e.g., Socratic questioning) or take an alternate interaction mode instead of immediately producing an answer.",
            "citation_title": "A prompt pattern catalog to enhance prompt engineering with ChatGPT.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / LLMs",
            "model_size": null,
            "task_name": "Idea exploration, tutoring, iterative design (general)",
            "task_description": "Tasks where eliciting probing questions or alternative perspectives from the model can improve exploration or scaffold learning.",
            "problem_format": "Request alternative interaction patterns such as 'ask me clarifying questions' or 'generate diagnostic questions' rather than providing final answers.",
            "comparison_format": "Compared to standard answer-provision prompts.",
            "performance": "Paper cites flipped interaction as a useful pattern and notes such patterns generalize across domains; no numeric performance reported here.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Flipped patterns change the dialogue dynamics, encouraging deeper exploration and iterative refinement, which can lead to higher-quality collaborative outputs.",
            "counterexample_or_null_result": null,
            "uuid": "e5652.4",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Formatting and constraint modifiers",
            "name_full": "Formatting, style, length and language constraints in prompts",
            "brief_description": "Use of explicit formatting instructions, style/tone directives, length limits, and coding-language constraints in prompts to control output form and suitability for specific tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (e.g., ChatGPT)",
            "model_size": null,
            "task_name": "Writing, code generation, and domain-specific content creation",
            "task_description": "Tasks where output format, code language, or stylistic conventions matter (e.g., asking for Python code vs. pseudocode, or academic tone vs. lay explanation).",
            "problem_format": "Add explicit formatting tokens, style instructions, length limits, or specify coding language and output structure in the prompt.",
            "comparison_format": "Compared implicitly to unconstrained, free-form prompts.",
            "performance": "Paper states such modifiers are part of prompt engineering and can materially affect output quality; no quantitative results provided.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Explicit format and constraint instructions reduce ambiguity and better align model outputs with user needs; nevertheless, domain expertise is still needed to judge correctness.",
            "counterexample_or_null_result": null,
            "uuid": "e5652.5",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Retrieval-augmented generation (RAG)",
            "name_full": "Retrieval-augmented generation for LLMs",
            "brief_description": "Technique that augments a generative model with an external retrieval system (documents, databases) to ground responses in up-to-date and factual sources, improving timeliness and factual reliability.",
            "citation_title": "Retrieval-augmented generation for large language models: A survey.",
            "mention_or_use": "mention",
            "model_name": "LLMs augmented with retrieval modules",
            "model_size": null,
            "task_name": "Factual QA, time-sensitive information retrieval, knowledge-grounded generation",
            "task_description": "Tasks requiring up-to-date or verifiable factual content where the base LLM's training cutoff or hallucination risk is problematic.",
            "problem_format": "Combine retrieval step (fetching relevant documents) with generation step so the model conditions on retrieved context when producing its answer.",
            "comparison_format": "Compared implicitly to vanilla generation without retrieval augmentation.",
            "performance": "Paper cites RAG as an ongoing development that improves factual reliability and timeliness; no numeric performance figures in this paper.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "RAG supplies models with external evidence, reducing hallucinations and extending factual currency beyond model training data.",
            "counterexample_or_null_result": null,
            "uuid": "e5652.6",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Prompting pedagogy",
            "name_full": "Prompting pedagogy (this paper's term)",
            "brief_description": "A pedagogical framework proposed by the authors that integrates prompt engineering into learning workflows, emphasizing scaffolds (templates), phased prompting activities, and critical evaluation to teach students to generate games and learning artefacts with LLMs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLMs (used as tools in learning-by-making games)",
            "model_size": null,
            "task_name": "Learning-by-generating-games, gamification of learning materials, multi-phase design tasks",
            "task_description": "Educational design and game-making workflows where students use LLMs to prototype and iterate on game ideas, requiring staged prompting (idea generation → design → prototyping → assessment).",
            "problem_format": "Sequenced prompt templates, support materials, and phase-specific prompting activities tailored to each step of game creation and evaluation; encourages dialogic, iterative prompting and critical review.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": null,
            "explanation_or_hypothesis": "Teaching prompt engineering as pedagogy can build students' ability to use LLMs constructively while preserving domain expertise through iterative critical evaluation; framing prompting as a practiced skill parallels constructionist approaches (e.g., Papert).",
            "counterexample_or_null_result": null,
            "uuid": "e5652.7",
            "source_info": {
                "paper_title": "Leveraging the Potential of Large Language Models in Education Through Playful and Game‑Based Learning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A prompt pattern catalog to enhance prompt engineering with ChatGPT.",
            "rating": 2,
            "sanitized_title": "a_prompt_pattern_catalog_to_enhance_prompt_engineering_with_chatgpt"
        },
        {
            "paper_title": "Large language models as optimizers.",
            "rating": 2,
            "sanitized_title": "large_language_models_as_optimizers"
        },
        {
            "paper_title": "Large language models understand and can be enhanced by emotional stimuli.",
            "rating": 2,
            "sanitized_title": "large_language_models_understand_and_can_be_enhanced_by_emotional_stimuli"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Prompt engineering for healthcare: Methodologies and applications.",
            "rating": 1,
            "sanitized_title": "prompt_engineering_for_healthcare_methodologies_and_applications"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering.",
            "rating": 1,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Prompt engineering as an important emerging skill for medical professionals.",
            "rating": 1,
            "sanitized_title": "prompt_engineering_as_an_important_emerging_skill_for_medical_professionals"
        }
    ],
    "cost": 0.014905249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Leveraging the Potential of Large Language Models in Education Through Playful and Game-Based Learning
27 February 2024</p>
<p>Stefan E Hube 0000-0001-5057-0376
Germany</p>
<p>Kristian Kiili 0000-0002-4664-8430
Germany</p>
<p>Ste Nebel 0000-0001-5831-8140
Germany</p>
<p>Richard M Ryan 0000-0002-2355-6154
Germany</p>
<p>Michael Sailer 0000-0001-6831-5429
Germany</p>
<p>Manuel Ninaus manuel.ninaus@uni-graz.at 0000-0002-4664-8430
Germany</p>
<p>Leveraging the Potential of Large Language Models in Education Through Playful and Game-Based Learning
27 February 2024F71E70E84857F68EC8971D9FCA674E8A10.1007/s10648-024-09868-zAccepted: 9 February 2024 /Large language modelsGenerative artificial intelligenceEducationPlayful learningGamificationGame-based learning
This perspective piece explores the transformative potential and associated challenges of large language models (LLMs) in education and how those challenges might be addressed utilizing playful and game-based learning.While providing many opportunities, the stochastic elements incorporated in how present LLMs process text, requires domain expertise for a critical evaluation and responsible use of the generated output.Yet, due to their low opportunity cost, LLMs in education may pose some risk of over-reliance, potentially and unintendedly limiting the development of such expertise.Education is thus faced with the challenge of preserving reliable expertise development while not losing out on emergent opportunities.To address this challenge, we first propose a playful approach focusing on skill practice and human judgment.Drawing from game-based learning research, we then go beyond this playful account by reflecting on the potential of well-designed games to foster a willingness to practice, and thus nurturing domain-specific expertise.We finally give some perspective on how a new pedagogy of learning with AI might utilize LLMs for learning by generating games and gamifying learning materials, leveraging the full potential of human-AI interaction in education.</p>
<p>Introduction</p>
<p>Large language models (LLMs) and their recently increased accessibility via chatbots like ChatGPT (OpenAI, 2023), Bard (Google, 2023), or Bing Chat (Microsoft, 2023) provide both new opportunities and challenges for education.On the one hand, they legitimately promise effective ways to assist with many tasks involved in both teaching and learning (Bernabei et al., 2023;Kohnke et al., 2023), to provide scalable, personalized learning material (Abd-alrazaq et al., 2023;Sallam, 2023), Extended author information available on the last page of the article 25 Page 2 of 20 and thus easy and scalable opportunities for exercise (Kasneci et al., 2023).On the other hand, they come with the educational challenge to avoid becoming overly or naïvely reliant on their support (Abd-alrazaq et al., 2023;Bernabei et al., 2023;Kasneci et al., 2023;Kohnke et al., 2023;Shue et al., 2023;Zhu et al., 2023), and thus to prevent inadvertently adopting inherent biases (Abd-alrazaq et al., 2023;Bernabei et al., 2023;Dwivedi et al., 2023;Kasneci et al., 2023;Zhu et al., 2023) or losing out on opportunities for reflection and practice for developing domain expertise and judgment competence (Dwivedi et al., 2023;Krügel et al., 2023).These are, however, especially needed for the responsible use of present LLMs, because, due to their inherent random mechanisms utilized during text generation (Wolfram, 2023), mistakes or fabricated information cannot be entirely ruled out.Hence, at least for the time being, the output generated by LLMs definitely requires domain expertise for critical revision and evaluation.Education thus finds itself currently faced with the challenge to find an appropriate balance between seizing new and welcome opportunities and protecting against inadvertent risks of losing out on the development of required expertise at the same time.</p>
<p>In this perspective piece, we propose that-in a first step-a more exploratory, playful approach towards the use of LLMs may help with finding such an appropriate balance.Such an approach has already been utilized in the form of prompt engineering in various domains (Oppenlaender et al., 2023;Polak &amp; Morgan, 2023;Short &amp; Short, 2023;Shue et al., 2023;Wang et al., 2023;White et al., 2023;Zhu et al., 2023).Beyond those accounts, we further suggest that-in a second stepgoing the full way to a game-based education could eventually provide a new pedagogy of learning with artificial intelligence (AI) leveraging the full potential within a well-balanced cooperation between human and machine intelligence.We further argue that this second step allows utilization of LLMs for devising appropriate game-based learning environments, such that LLMs may eventually serve to overcome exactly those challenges they pose for education in the first place.</p>
<p>To serve a systematic development of our arguments, the article is organized as follows: first, we briefly illustrate both opportunities and challenges posed by the usage of LLMs in educational contexts.In a second section, we argue how a more playful approach to the usage of LLMs in education may already help to resolve some of the tension between opportunities, challenges, and risks.In a final section, we outline our proposition how game-based learning can extend the limits of said playful approaches, paving the way for a prolific co-operation between human and artificial intelligence in education.</p>
<p>LLMs in Education-Opportunities and Challenges</p>
<p>Generally, LLMs are a recently developed form of AI (i.e., algorithms historically devised to mimic, extend, or replace parts of human cognition or behavior).More specifically, they are a form of generative AI, representing algorithms capable of generating new media like images or text.</p>
<p>Recent LLMs (like those provided via ChatGPT) use large datasets of text in conjunction with artificial neural networks with billions of parameters to process and generate text.Chat-like interfaces allow the user to obtain human-like responses in conversational style upon entering arbitrary prompts.While earlier language models like Wordtune, Paperpal, or Generate (Hutson, 2022) could help writers restructure a sentence, more recent versions like ChatGPT can help with devising entire manuscripts, providing feedback, finding limitations (Zimmerman, 2023), or devising specialized text like computer code (Shue et al., 2023).</p>
<p>The essential core principles have, however, remained similar (Wolfram, 2023): the computation of likely continuations of the user-provided prompt based on identified relations between text elements in the vast amount of training data.An important ingredient in the computation is the fact that not always the most likely, but sometimes a less likely continuation is chosen.While this serves the impression of an especially spontaneous, human-like, fluently emergent text, it also is the reason why the information provided by present LLMs can be misleading or erroneous and thus requires continuous supervision and critical evaluation.</p>
<p>Opportunities</p>
<p>Given their capabilities, LLMs provide a wide range of opportunities for education (Kasneci et al., 2023).LLMs can assist with management tasks (e.g., development of teaching units, curricula, or personalized study plans), with assessment and evaluation, and with program monitoring and review (Abd-alrazaq et al., 2023).They can take the roles of content providers (Abd-alrazaq et al., 2023;Jeon &amp; Lee, 2023;Sarsa et al., 2022), temporary interlocutors, teaching assistants, and evaluators (Jeon &amp; Lee, 2023).They can assist with writing tasks of both teachers and learners (Bernabei et al., 2023), regarding not only content creation, but also basic information retrieval (Zhu et al., 2023) and literature review (Abd-alrazaq et al., 2023).</p>
<p>LLMs can further assist teachers in orchestrating a continuously growing plethora of teaching resources, making the teachers' resources (bound to developing and revising learning material in earlier times) more available for designing creative, wellorganized, and engaging lessons (Jeon &amp; Lee, 2023).They enable personalized learning (Abd-alrazaq et al., 2023;Sallam, 2023) and may benefit learners' understanding of topics (Bernabei et al., 2023;Sarsa et al., 2022;Zhu et al., 2023).If used carefully, they can enhance critical thinking and problem-based learning (Bernabei et al., 2023;Sallam, 2023;Shue et al., 2023), emphasize the role the role of students as active investigators, and raise ethical awareness regarding the use of AI (Jeon &amp; Lee, 2023).</p>
<p>Challenges</p>
<p>However, careful use of LLMs also presents a challenge to both teachers and learners (Kasneci et al., 2023).This is related to a variety of shortcomings of LLMs that have not yet been entirely resolved.These include the possibility of mistakes or fabricated information; the lack of recent, state-of-the-art, domain knowledge; the lack of originality; inherent (social or gender) biases; various ethical and legal issues like 25 Page 4 of 20 copyright, plagiarism, and false citations; lacks of transparency and accountability; cybersecurity issues; and the risk of infodemics (Sallam, 2023;Zhu et al., 2023).</p>
<p>In contrast to pocket calculators, present LLMs are not designed to yield reliably the same deterministic output upon the same given prompt.A stochastic element in generation of such output is a part of how and why they work so astonishingly well in producing seemingly human-like responses (Wolfram, 2023).This, however, has also the consequence that their output definitely requires critical evaluation and careful revision by domain experts (Ali et al., 2023;Biswas, 2023;Hosseini et al., 2023;Howard et al., 2023;Kasneci et al., 2023;Mogali, 2023;Salvagno et al., 2023;Van Dis et al., 2023;Zhu et al., 2023).Especially when it is about decisions that should guide human action, the support provided by LLMs should be supervised by human expertise (Molenaar, 2021).</p>
<p>Expertise as a Crucial Factor in Human-AI Systems</p>
<p>This resonates well with the general assertion that the quality of decisions by human-AI systems depends crucially on the human expertise within such systems (Ninaus &amp; Sailer, 2022).However, both the development and preservation of expertise require practicing domain-specific problem-solving capabilities (Elvira et al., 2017;Tynjälä, 2008;Tynjälä et al., 2006).</p>
<p>As novices advance from easier to more difficult problems, they continuously engage in three learning processes.First, they transform conceptual knowledge into experiential knowledge when, for instance, applying general concepts to specific problems in particular contexts.Second, they also need to explicate experiential into conceptual knowledge to, for instance, make tacit knowledge (Patterson et al., 2010) accessible to other people as well as to metacognitive processes like reflection.Reflecting on experiential and conceptual knowledge finally allows for improving problem-solving strategies, further supports the transfer of tacit to explicable knowledge, and facilitates the development of learning strategies, metacognitive, and self-regulatory skills (Elvira et al., 2017).</p>
<p>All three processes have in common that continuous practice in integrating conceptual, experiential, and self-reflective knowledge during problem-solving utilizes already existing expertise and contributes to its further development.Although modern theories on expertise acknowledge that many factors besides practice contribute to expertise development (Hambrick et al., 2016), they do not deny the relevance or even necessity of (deliberate) practice (Campitelli &amp; Gobet, 2011;Ericsson et al., 1993;Hambrick et al., 2014).</p>
<p>Interaction Between Use of LLMs and Expertise Development</p>
<p>In formal education, which lays the foundations for the development of expertise, practice sometimes requires that learners engage in effortful or even strenuous tasks.That is, learners need to regulate their attention and efforts toward a task that might be associated with aversive feelings and also to resist engaging in more pleasurable activities (Kurzban et al., 2013;Miller et al., 2012).</p>
<p>However, the convenience and low opportunity cost that LLMs bring for certain tasks, bears the risk of over-reliance (Kasneci et al., 2023) or over-trust (Morris et al., 2023), which has also been recognized as a hindrance for critical thinking (Shue et al., 2023), learning, and reflection (Zhu et al., 2023).In addition to that, learners (and sometimes also teachers) can feel tempted by the authoritative nature of the responses to take them at face value without critically evaluating and processing them further (Kohnke et al., 2023).Lastly, learners can be tempted to outsource the activity.While such outsourcing might be appropriate for tasks that are merely means to an end, it becomes problematic when tasks represent essential learning opportunities for skills that a person should have even without AI support (Salomon et al., 1991).Over-reliance on LLMs in educational contexts is thus associated with some risk of losing out on essential ingredients for the development and preservation of expertise, potentially and inadvertently providing also a risk of deskilling (Morris et al., 2023), and consequentially of automation bias, reduced human autonomy and judgment competence (Dwivedi et al., 2023;Deutscher Ethikrat, 2023).</p>
<p>It is important, however, to note that an eventual shift in what is considered an essential skill is not problematic per se.As with every new useful tool, LLMs also bring about a shift in what is considered essential expertise.While in earlier times, doing a statistical analysis might have involved manually integrating a normal curve to determine a p-value, this would hardly suggest that a social scientist not knowing anymore how to do this has not developed any statistical expertise (we thank the anonymous reviewer for providing this example).The advent of the digital computer has changed the outline of the skill set determining the meaning of statistical expertise.</p>
<p>Ongoing developments of generative AI technology like retrieval augmented generation, improving on both factual reliability and timeliness of responses provided by LLMs (Gao et al., 2023), are likely to push the boundaries of what kind of expertise may be called essential even further.The critical point remains that high-quality decisions of human-AI systems presuppose some human expertise (Ninaus &amp; Sailer, 2022).And it is difficult to judge in advance which skill sets will remain essential in the future.As Dwivedi et al. (2023) note, we as educators must ask ourselves first: which skills are still needed?Once these are identified, a second question remains: how can we devise new, appropriate ways of developing and practicing these skills in a new pedagogy of learning with AI?</p>
<p>Banning LLMs?</p>
<p>One response addressing this challenge are calls for more closely regulating the use of LLMs, ranging from simply requiring disclosure (Stokel-Walker, 2023) over adaption of examination procedures (Dwivedi et al., 2023) to complete bans (Johnson, 2023;Rosenzweig-Ziff, 2023).Yet attempts at external control face at least one very pragmatic issue: It can be difficult, if possible at all, to distinguish between human-and AI-produced material (Ariyaratne et al., 2023;Dunn et al., 2023;Else, 2023).Although tools are developed that allow (at least temporarily) AI-support detection to some extent (Bernabei et al., 2023;Else, 2023), we also think that research and higher education needs to devise ways to use LLMs ethically, transparently, and responsibly (Van Dis et al., 2023).Furthermore, "it makes no sense to ban the technology for students that will live in a world where this technology will play a major role" (Dwivedi et al., 2023, p. 9).</p>
<p>A completely different response to the outlined challenge originates long before the most recent advent of AI in the form of LLMs.It involves a more playful stance towards the new possibilities that come with new technology.</p>
<p>On Playful Approaches to Integrate New Technology in Education</p>
<p>As early as in the 1960s, Papert (1980) developed a pedagogical approach which allowed to utilize computers to facilitate children's understanding of geometry.However, instead of thinking of ways to use computers just as providers of more sophisticated, digital teaching or learning material, children were enabled to build up their geometrical understanding by providing them with a tool to let computers do something meaningful to them.For this purpose, the programing language Logo was developed (Papert, 1980) which allowed children to control the movement of a virtual turtle which left behind lines as it moved over the screen.By understanding how to draw geometrical shapes by controlling the turtle, and further, how simple geometrical shapes constitute more complex images, a gradually improving understanding of geometry allowed the children to draw more beautiful and complex images.Playful experimentation with the Logo language allowed to build up experiential knowledge by applying basic, conceptual knowledge of how to draw squares, triangles, and so forth.At the same time, purposeful drawing of more complex, composite objects (like a house with a door, windows, and a roof) required translating experiential knowledge into conceptual knowledge by the necessity to provide specific commands.Learning by purposive doing and by engaging in discovery via the natural processes of trial and error would further provide ample opportunity to reflect on both, experiential and conceptual knowledge to further improve drawing capabilities and thus, understanding geometry.Papert's pedagogical approach (1980), hence, naturally nurtured all three learning processes involved in developing expertise (Elvira et al., 2017;Tynjälä, 2008;Tynjälä et al., 2006).Not only became children able to produce images and experiences of meaning for themselves, but they did so just inasmuch as they improved in their geometrical understanding, programing capabilities, and computational thinking.Furthermore, new technology, i.e., the digital computer, which could have just been programmed to do the same geometrical operations much more efficiently, was instead utilized to promote education (Papert, 1980).</p>
<p>Yet, why did Papert come up with his playful, constructionist approach to learning in the first place?In fact, he was inspired by constructivist theory of how children construct new schemas by interacting with their environment (Piaget, 1962).In Piaget's theory of cognitive development (1962), play facilitates children's cognitive development by activating basic units for organizing knowledge and behavior, known as schemas.Play allows both the practice of existing schemas, and thus of existing skills and knowledge, and the development of new ones by combining elements of existing ones in ways that transcend existing knowledge.</p>
<p>Social development theory (Vygotsky, 1967), scrutinizing also the developmental importance of play, adds the notion that the crucial point of play for learning is its capability to provide children with opportunities to explore outcomes beyond their current abilities.Play allows players to experience and simulate potential outcomes without the real-life costs (Homer et al., 2020).It allows to probe their capabilities, and by that, it allows them to grow beyond their current limitations.Although highlighting somewhat different aspects, both theories of play highlight their potential for facilitating learning and development.</p>
<p>More recently, research within self-determination theory (SDT; Ryan &amp; Deci, 2017) has specifically highlighted the importance of intrinsic motivation, the enjoyment of the activity itself, as critical to learning across development (Reeve, 2023).That is, much if not most of human learning (both within and outside formal education) occurs because of our interest and curiosity in activities, from which we acquire knowledge and skills.Research in SDT suggests that sustained playful learning involves experiencing a sense of autonomy and competence, which are often richly afforded within game environments (Rigby &amp; Ryan, 2011).</p>
<p>Carefully applying these concepts to the challenge posed by LLMs for expertise development may turn the outlined risks into promising learning opportunities.The idea is the same as the one exemplified by Papert's approach (1980) to utilize computers as educational tools.Instead of seeing LLMs as possibilities to outsource task accomplishment, they are understood as tools that can be utilized to engage in a meaningful activity.The interface, which has been the Logo language in Papert's case (1980), now is, for instance, ChatGPT, allowing to provide prompts that steer the underlying LLM in the desired direction.In this case, the meaningful product, is not necessarily an image, but can be a manuscript, some computer code, or any piece of text.The specific expertise required to be acquired to make LLMs work in such a useful way has become known as prompt engineering.</p>
<p>Prompt Engineering as a Form of Playful Interaction with LLMs</p>
<p>Prompt engineering generally refers to the iterative process in which users fine-tune their textual inputs to achieve a desired output from the LLM (Meskó, 2023).It has been recognized as an essential competence within future digital literacy (Eager &amp; Brunton, 2023;Korzynski et al., 2023), eventually enabling to fully harness LLMs' potential to provide personalized learning, unlimited practice opportunities, and interactive engagement with immediate feedback (Heston &amp; Khun, 2023).It has been successfully applied in diverse domains including software development (White et al., 2023), entrepreneurship (Short &amp; Short, 2023), art (Oppenlaender et al., 2023), science (Polak &amp; Morgan, 2023), and healthcare (Wang et al., 2023).</p>
<p>Prompt engineering may involve role play or persona modeling (letting the LLM adopt a specific role such as a domain expert in a certain field; Short &amp; Short, 2023), text format, style or tone (Zhu et al., 2023), length and (coding) language restrictions (Shue et al., 2023), question refinement or alternative approaches requests, 25 Page 8 of 20 flipped interaction patterns (e.g., requesting questions rather than elaboration from the LLM; White et al., 2023), chain-of-thought-prompting (generating intermediate outputs; e.g., "Take a deep breath and work on this problem step-by-step"; Yang et al., 2023), or emotional prompting (e.g., "This is very important for my career"; Li et al., 2023) among many more possible techniques.Noteworthy, identified functional prompt patterns have been found to be generalizable over many different domains (White et al., 2023).</p>
<p>Although optimizing prompts has been shown to be capable of vastly improving the accuracy of outputs generated by LLMs (Li et al., 2023;Yang et al., 2023), the fact remains that the critical evaluation of resulting outputs still requires domain expertise.Critically reviewing the resulting output is just as important as optimizing the prompts (Shue et al., 2023).</p>
<p>Prompt engineering itself can actually be regarded as an expert skill requiring not only expertise within the domain (for the selection of appropriate keywords and prompt content) but also of prompt modifiers and the training data and system configuration settings of the specific LLMs (Oppenlaender et al., 2023).Becoming proficient in prompt engineering thus has an analogous meaning for a user of an LLM as becoming proficient in the Logo language for Papert's (1980) students.It not only allows one to make use of LLMs efficiently, but in order for it to work, i.e., to result in reliable and useful output, it entails practicing exactly that domain expertise which it presupposes.Given the necessary expertise, prompt engineering can thus become a form of playful interaction with LLMs, exploring various aspects of a topic by varying prompt patterns and techniques.Under those circumstances, the domain expert's intrinsic interest in the reliability and usefulness of results produced in cooperation with LLMs might provide some protection from over-reliance on a single output and associated risks of more narrowly directed LLM employments.</p>
<p>However, such risks might be more severe for learners who are not yet domain experts but are presently on their way to developing such expertise.Their primary goals may be less intrinsically motivated but eventually correspond rather to the mere accomplishment of educational tasks like the submission of seminar papers, homework, or sample calculations.In light of the especially low opportunity costs of LLMs, supporting a playful approach for working with them also under those circumstances may require more than to appeal to individual integrity and virtue.Such support, however, may then be accomplished by providing a learning environment in which playing becomes a natural form of activity (Plass et al., 2020) and a designed pathway to learning.That means, such support may be provided by a pedagogy of learning based on games.</p>
<p>Game-Based Learning as a Way to Harness the Full Potential of Human-AI Interaction in Education</p>
<p>Games, in both non-digital and digital forms, have repeatedly proven valuable for learning, training, and education (Dillon et al., 2017;Pahor et al., 2022;Pasqualotto et al., 2022).They provide space for playful learning experiences, allow room for experimentation, and provide safe spaces for graceful failure, a crucial component for learning with games, allowing the players to learn from mistakes and motivating them to practice until feeling confident (Plass et al., 2015).</p>
<p>Due to their capabilities in capturing and holding people's attention and in fostering sustained engagement and long-term loyalties, games have further become role models for engaging learners (Rigby, 2014) and citizens to solve complex scientific problems (Cooper et al., 2010;Spiers et al., 2023).Well-designed games can indeed promote both the required persistence in activities for practice and high quality of engagement that can foster deep human learning and problem solving (Barz et al., 2023;Hu et al., 2022;Ryan &amp; Rigby, 2020).The extension of SDT (Ryan &amp; Deci, 2000, 2017) based on research on video games (Ryan et al., 2006), technology design (Calvo &amp; Peters, 2014), or digital learning (Sørebø et al., 2009) has shown in which ways psychological satisfactions for autonomy, competence, and relatedness can be evoked or undermined and thus affect players' intrinsic motivation and sustained engagement (Ryan &amp; Rigby, 2020).In games, a complex set of skills is challenged in a constrained environment in which those skills can be explored, analyzed, manipulated, extended (Ryan &amp; Rigby, 2020), or in other words: practiced.Thereby, ample opportunities allow experiences of autonomy, competence, and relatedness fuelling intrinsic motivation."In a well-designed game, the learning becomes its own reward" (Ryan &amp; Rigby, 2020, p. 169).</p>
<p>The problem-based gaming model (Kiili, 2007) further emphasizes the meaning of experiential learning and reflection in educational games.It is argued that the ability to reflect may be the main factor determining who learns effectively from experience (Kiili, 2007).This is especially true for games that require problem-solving (e.g., simulation games).In the model, the level of reflection concerns whether the player considers the consequences of their actions and the changes in the game world to create better playing strategies (double-loop learning) or merely applies the previously formed playing strategy (single-loop learning).Games that trigger double-loop learning are effective because they persuade players to test different kinds of hypotheses and consider the learning content deeply from several perspectives.The challenge of educational game design is to design game mechanics that trigger such meaningful reflection practices.</p>
<p>Games as a Culture Medium for the Development of Expertise</p>
<p>Games naturally serve all three learning processes facilitating the development of expertise.By providing ample space for playful engagement, they support the transformation of experiential into conceptual knowledge.By being-in contrast to free-form play-yet structured by explicit rule sets and specific goals (Deterding et al., 2011), they also require and thus facilitate the transformation of conceptual into experiential knowledge.Finally, as outlined above, they invite diverse forms of reflection serving the further development of problem-solving strategies as well as metacognitive and self-regulatory skills.</p>
<p>The capabilities of games to invite reflection are further emphasized by the fact that successful games have repeatedly been identified as sources of spontaneously emergent culture.Affinity groups (Gee, 2005) may emerge (online or offline) in 25 Page 10 of 20 which players meet to communicate, reflect, and influence game rules, extend new game content, and contribute to game development (Brown, 2016), engage in theorycrafting (Choontanom &amp; Nardi, 2012) and peer-to-peer apprenticeship (Steinkuehler &amp; Oh, 2012).Both the explication of experiential knowledge into conceptual knowledge and reflecting on both knowledge types happen naturally in such spontaneously forming collaborative spaces.</p>
<p>The emergence of those spaces is not induced by top-down mechanisms (e.g., by game developers) but happens horizontally within the game community (Steinkuehler &amp; Tsaasan, 2020).For instance, in the Just Press Play project (Decker &amp; Lawley, 2013), investigating the effect of gamification on undergraduate experience in computer science, students spontaneously requested access to computer labs for tutoring other students for free, on their own time and out of their own desire.In addition, a lively community of educators emerged, constantly creating new learning environments and trying to include the game in the class room against all technical and bureaucratic odds.After the release of Minecraft, communities emerged, modifying the game and creating content way beyond the games' original intended meaning and functionalities (Nebel et al., 2016).Users-and mostly pupils-used the games' mechanics to create functioning CPUs, landscapes of their favorite books or sustainable environments, all in their free time.Those are both unforeseen and astonishing results.Not only provide they examples of what the notion of "learning outcomes" in game-based learning can actually encompass: the spontaneous emergence of teachers or experts from a community of students or novices (Steinkuehler &amp; Tsaasan, 2020).They also provide examples of what potential game-based learning might bear for education.</p>
<p>Furthermore, they provide examples of how games can foster spontaneous profound engagement with the learning material far beyond a mere accomplishment of tasks.When within well-designed games, in which the basic needs of autonomy, competence, and relatedness are met, learning becomes its own reward (Ryan &amp; Rigby, 2020), the option to outsource cognitive efforts to LLMs becomes less tempting.Instead, well-designed games might even foster the motivation to utilize LLMs for engaging deeper with the content and finding out more.That means, game environments might provide novices with a flavor of that kind of intrinsic interest that may protect domain experts from over-reliance and associated risks.</p>
<p>Yet Where Are All the Educational Games?</p>
<p>However, if games hold such an educational potential, the question needs to be addressed: Why have they not become much more abundant in schools and universities?One simple reason is that making good games, i.e., games that satisfy basic psychological needs (Ryan &amp; Rigby, 2020), is tough.Even established developers in the entertainment game industry, i.e., in the business of manufacturing fun, repeatedly fail to deliver and are regularly hit with closures and layoffs (Hodent, 2018), whereas some of the most successful games started as low-budget side projects.Educational games face many additional challenges.</p>
<p>On a socio-cultural dimension (Fernández-Manjón et al., 2015), an issue is social rejection of games, which may be reduced by improving society's understanding of games as another form of cultural good, and informing stakeholders (students, educators, and parents) about the social potential and positive effects of video games (Granic et al., 2014) and their usefulness in education (Bourgonjon et al., 2010).At the same time, violence, sexism, and discrimination are advised to be avoided in the design of educational games (Fernández-Manjón et al., 2015).</p>
<p>Along an educational dimension, limited accessibility to educational games can prevent their further adoption in education (Fernández-Manjón et al., 2015).Whereas creating and maintaining user manuals and best practice guides are ways to facilitate accessibility (Fernández-Manjón et al., 2015), both require further structural support.The latter can be provided by simultaneous support and creation of communities of practice (Wenger, 1998) allowing participation in development processes (Moreno-Ger et al., 2008) and knowledge production and transfer between educators, developers, and researchers (Fernández-Manjón et al., 2015;Hébert et al., 2021).</p>
<p>Along a technological dimension, limited accessibility to technology is an issue (Hébert et al., 2021).Lowering development costs and developing environments that allow educators some game development without requiring substantial programming skills and specific game development expertise are regarded as necessary steps to address this issue (Fernández-Manjón et al., 2015).</p>
<p>LLMs as an Opportunity for Harnessing the Potential of Games Within Education</p>
<p>In this context, LLMs or more generally generative AI tools have the potential to transform game-based learning practices and-again similarly to the use of computers in Papert's class (1980)-could even become once more part of their own remedy regarding the challenge they pose for education.This, however, warrants a new pedagogy of learning with artificial intelligence.</p>
<p>In particular, we identified two use scenarios in which generative AI tools can boost the use of games in educational settings.First, generative AI tools provide new ways to implement making games for learning approaches (Kafai &amp; Burke, 2015), in which students learn educational content by designing and making games.Second, teachers and educators can utilize AI tools to gamify their learning materials or even create fully-fledged learning games for their students.In the following, we consider how LLMs can be utilized in these scenarios.</p>
<p>Learning by Generating Games</p>
<p>Making games for learning is another prime example of a constructionist learning activity (Kafai &amp; Burke, 2015) similar to Papert's (1980) early use of computers in the classroom discussed above.Kafai and Burke (2015) argue that we are witnessing a paradigmatic shift toward constructionist gaming, in which students design games for learning instead of just consuming games created by professional developers.We believe that generative AI tools will further accelerate this shift.LLMs have the potential to make game creation more accessible for novices in a similar way as block-based visual programming environments like Scratch (Resnick et al., 2009) lowered the demands to program interactive stories and animations in educational settings.The pedagogical idea behind learning by generating games relies mostly on the assumption that game-making activities help students reformulate their understanding of the subject matter (educational content) and express their personal understanding and ideas about the subject (Kafai, 2006).In addition, generating games using AI's technical backup can be open and creative, allowing for experiences of autonomy and competence essential to sustained interest and intrinsic motivation (Ryan &amp; Rigby, 2020).As the technicalities of programming can be largely outsourced to LLMs, students can focus more on the topic and game design aspects.</p>
<p>A recent study indicates that game-designing activities can be even more beneficial, especially for the long-term retention of knowledge, than learning by playing games (Chen &amp; Liu, 2023).Furthermore, Resnick et al., (2009) have emphasized that digital fluency requires more than just interacting with media; it requires an ability to collaboratively design, create, and invent with media.Similar abilities are needed when creating games with the help of LLMs and seem now more important than ever.However, making games with LLMs also imposes unique requirements for students as well as for teachers who are orchestrating the game-making activities.</p>
<p>We coined the term prompting pedagogy to capture fundamental pedagogical practices involved in generating games or other digital outputs with the help of LLMs going beyond prompt engineering as discussed above and constituting one aspect of a new pedagogy of learning with AI.While prompt engineering will be a crucial competence for harnessing the potential of AI in education (Eager &amp; Brunton, 2023), we also want to emphasize that the ability to critically evaluate generated outputs and its facilitation by existing (domain) expertise are equally important (Dwivedi et al., 2023;Krügel et al., 2023).This critical evaluation informs the crafting of prompts leading to meaningful and constructive dialogue with LLMs.Such cumulative and continuous dialog is crucial when using LLMs in complex tasks like game-making.Moreover, using LLMs in such a reflective and critical manner enhances critical thinking and problem-based learning (Bernabei et al., 2023;Sallam, 2023;Shue et al., 2023).</p>
<p>It is evident that effective prompting is challenging, and students need support to develop adequate prompting skills to generate games with LLMs.Prompting pedagogy for game-making also involves the preparation of support materials (e.g., prompting templates for different purposes) and sequencing the prompting activities to specific phases (e.g., idea generation, core design, prototyping, and assessment).</p>
<p>Even though the use of LLMs plays a crucial role in the suggested learning by generating games approach, the design and production activities need to be integrated into a meaningful teaching process.For example, the creative thinking spiral process (imagine, create, play, share, reflect, imagine, and so forth) can be adapted to the learning by generating games approach (Resnick, 2009).According to Resnick (2009, p. 1), in this process, "people imagine what they want to do, create a project based on their ideas, play with their creations, share their ideas and creations with others, and reflect on their experiences-all of which leads them to imagine new ideas and new projects."This thus provides a way to emphasize playtesting with peers (sharing and testing game prototypes and games) as well as reflective discussion sessions about prompting and game design strategies in LLMs-based game-making projects.</p>
<p>Overall, learning by generating games promotes a creative, experimental, playful, and inclusive learning culture that aims to support the learning of academic content while preparing students for utilizing generative AI tools effectively and creatively in different contexts.As teachers have a significant role in this approach, a starting point for them may be to generate at least one learning game with an LLM before applying the learning approach in their teaching.Such first-hand experience can facilitate perceiving the affordances that LLMs provide, preparing support materials for students, and planning the workflow of activities.</p>
<p>The use of generative AI for developing learning games may also help to decrease the barriers to the creation of low-budget game productions and educational games.One problem with educational games is that we have become more and more accustomed to big-budget releases.Many educational games seem degraded by comparison (e.g., poor graphics and mechanics) and are thus perceived as boring or unappealing.A reasonable utilization of LLMs for game development could eventually help to close this gap.Moreover, since the activities are learner generated, they may well engender a different kind of interest and sense of ownership than studio produced educational outputs.</p>
<p>Gamifying Learning Materials</p>
<p>Generative AI provides many low-threshold possibilities for educators to gamify their teaching or to generate learning games for teaching.That is, LLMs and generative AI might establish themselves as a useful tool for developing (educational) games, for instance, by supporting the generation of artwork, code, or game levels (Nasir &amp; Togelius, 2023;Todd et al., 2023).LLMs can further assist educators in the analysis, design, evaluation, and development phases of game creation projects, allowing, for instance, the adaption of popular board games such as Monopoly for specific learning purposes (Gatti Junior et al., 2023).</p>
<p>It may eventually not matter whether the game makers are students or educators; the generation of games with LLMs requires a playful, experimental, and iterative style of engagement in which game makers continually reassess their goals, explore new solutions, and imagine new possibilities based on the generated outputs and dialogue with LLMs.Resnick and Rosenbaum (Resnick &amp; Rosenbaum, 2013) called such a bottom-up approach "tinkering."As highlighted above, one of the key skills of the successful generation of games with LLMs is prompting and critical evaluation of generated outputs, which requires expertise such tinkering might enhance.</p>
<p>As game development is usually a highly interdisciplinary process requiring expertise in various areas, LLMs might be used to complement individuals' skills in a particular area.For instance, it might allow an educator with expertise in the pedagogical approach for a given problem and an idea for the game design to implement a working prototype of an educational game, which would have been significantly more difficult for the educator without using generative AI technologies.Furthermore, as game design is a very complex activity, it is important to break complex prompts into a series of small, specific steps and phases, starting from the idea generation and identification of instructional approaches and core game mechanics.For example, chain-of-thought prompting (generating intermediate outputs) or role prompting (giving the LLM, e.g., the specific role of an instructional designer or target group player) can increase the model's contextual tenability and enhance the quality of outputs.</p>
<p>Conclusions and General Remarks</p>
<p>On balance, implementing insights from game-based learning in educational contexts is far from a straightforward task.However, game-based learning research has revealed that well-designed games indeed address, challenge, and promote players holistically, incorporating all cognitive (Mayer, 2020), affective (Loderer et al., 2020), motivational (Ryan &amp; Rigby, 2020), and sociocultural (Steinkuehler &amp; Tsaasan, 2020) aspects of the human condition.Applications of game-based learning in science, technology, engineering, and mathematics (Klopfer &amp; Thompson, 2020), or the development of educational games for critical thinking (Butcher et al., 2017) or social problem-solving (Ang et al., 2017) indicate at least the potential games may have for fostering deep engagement with the learning material and continuous practice of expertise.Utilizing LLMs for learning by generating games and purposefully gamifying learning materials may allow educators to fully harness the potential of games toward a new pedagogy of learning with AI.</p>
<p>The potential of playful and game-based learning we see for education is strongly related to games' motivational and engaging power, that "in play, the aim is play itself" (Flanagan, 2009).Even if the activities associated with the playful engagement encountered in games could be delegated to AI support, who would want this-because in this context, it would be outsourcing the fun and intrinsic satisfactions of play.That would be like delegating joy to a robot.Even if we could, why should we want that?</p>
<p>The notion of (good) practice has since Aristotle (2020) involved the aspect of bearing its meaning in itself, a quality which practice has, according to Rousseau and Schiller (Greipl et al., 2020), in common with play.It seems as if the advent of AI challenges us as educators to remember and revive research and its teaching, as such practice calls for the creation and cultivation of playful spaces within education.While this perspective is certainly not about advocating that we redesign each class into a game promising enjoyment or entertainment, we think that game-based learning could be especially valuable in taking advantage of the educational capabilities of AI, which themselves require capable human partnership.</p>
<p>25</p>
<p>Page 12 of 20</p>
<p>Funding</p>
<p>Open access funding provided by University of Graz.The authors acknowledge the financial support of the University of Graz.Kristian Kiili was supported by the Strategic Research Council (SRC) established within the Research Council of Finland [Grants: 335625, 358250].</p>
<p>Page 6 of 20
Data Availability Not applicable to this article as no datasets were generated or analysed during the current study.Authors and Affiliations
Large language models in medical education: Opportunities, challenges, and future directions. A Abd-Alrazaq, R Alsaad, D Alhuwail, A Ahmed, P M Healy, S Latifi, S Aziz, R Damseh, S Alabed Alrazak, J Sheikh, 10.2196/48291JMIR Medical Education. 92023</p>
<p>Using ChatGPT to write patient clinic letters. The Lancet Digital Health. S R Ali, T D Dobbs, H A Hutchings, I S Whitaker, 10.1016/S2589-7500(23)00048-120235</p>
<p>A game-based approach to teaching social problem-solving skills. R P Ang, J L Tan, D H Goh, V S Huan, Y P Ooi, J S T Boon, Handbook of research on serious games for educational applications. R Z Zheng, M K Gardner, Beresford, TransIGI Global. Aristotle2017. 2020The Nicomachean Ethics. Penguin Classics. (Original work published ca. 350 B.C.E.</p>
<p>A comparison of Chat-GPT-generated articles with human-written articles. S Ariyaratne, K P Iyengar, N Nischal, N Chittibabu, R Botchu, 10.1007/s00256-023-04340-5Skeletal Radiology. 2023</p>
<p>The effect of digital game-based learning interventions on cognitive, metacognitive, and affective-motivational learning outcomes in school: A meta-analysis. N Barz, M Benick, L Dörrenbächer-Ulrich, F Perels, 10.3102/00346543231167795Review of Educational Research. 54323952023. 003465432311677</p>
<p>Students' use of large language models in engineering education: A case study on technology acceptance, perceptions, efficacy, and detection chances. M Bernabei, S Colabianchi, A Falegnami, F Costantino, 10.1016/j.caeai.2023.100172Computers and Education: Artificial Intelligence. 51001722023</p>
<p>Potential use of Chat GPT in global warming. S S Biswas, 10.1007/s10439-023-03171-8Annals of Biomedical Engineering. 2023</p>
<p>Students' perceptions about the use of video games in the classroom. J Bourgonjon, M Valcke, R Soetaert, T Schellens, 10.1016/j.compedu.2009.10.022Computers and Education. 5442010</p>
<p>J K Brown, To literacy and beyond: The poetics of Disney Infinity 3.0 as facilitators of new literacy practices. 2016University of California, IrvineMaster's thesis</p>
<p>Dino Lab: Designing and developing an educational game for critical thinking. K R Butcher, M Runburg, R Altizer, Handbook of research on serious games for educational applications. R Z Zheng, M K Gardner, IGI Global2017</p>
<p>Positive computing: Technology for wellbeing and human potential. R A Calvo, D Peters, 2014MIT Press</p>
<p>Deliberate practice: Necessary but not sufficient. G Campitelli, F Gobet, 10.1177/0963721411421922Current Directions in Psychological Science. 2052011</p>
<p>Learning by designing or learning by playing? A comparative study of the effects of game-based learning on learning motivation and on short-term and long-term 25 Page 16 of 20 conversational gains. S Chen, Y.-T Liu, 10.1080/10494820.2021.1961159Interactive Learning Environments. 317592023. 19611</p>
<p>Theorycrafting: The art and science of using numbers to interpret the world. T Choontanom, B Nardi, 10.1017/CBO9781139031127.017Games, Learning, and Society. C Steinkuehler, K Squire, &amp; S Barab, Cambridge University Press20121st ed.</p>
<p>Predicting protein structures with a multiplayer online game. S Cooper, F Khatib, A Treuille, J Barbero, J Lee, M Beenen, A Leaver-Fay, D Baker, Z Popović, F Players, 10.1038/nature09304Nature. 46673072010</p>
<p>Life's a game and the game of life: How making a game out of it can change student behavior. A Decker, E L Lawley, 10.1145/2445196.2445269Proceeding of the 44th ACM Technical Symposium on Computer Science Education. eeding of the 44th ACM Technical Symposium on Computer Science Education2013</p>
<p>From game design elements to gamefulness: Defining "gamification. S Deterding, D Dixon, R Khaled, L Nacke, 10.1145/2181037.2181040Proceedings of the 15th International Academic MindTrek Conference: Envisioning Future Media Environments. the 15th International Academic MindTrek Conference: Envisioning Future Media Environments2011</p>
<p>Deutscher Ethikrat, Mensch und Maschine -Herausforderungen durch Künstliche Intelligenz. Stellungnahme. Deutscher Ethikrat. 2023. May 2023</p>
<p>Cognitive science in the field: A preschool intervention durably enhances intuitive but not formal mathematics. M R Dillon, H Kannan, J T Dean, E S Spelke, E Duflo, 10.1126/science.aal4724Science. 35763462017</p>
<p>Artificial intelligence-derived dermatology case reports are indistinguishable from those written by humans: A single-blinded observer study. C Dunn, J Hunter, W Steffes, Z Whitney, M Foss, J Mammino, A Leavitt, S D Hawkins, A Dane, M Yungmann, R Nathoo, 10.1016/j.jaad.2023.04.005S019096222300587XJournal of the American Academy of Dermatology. 2023</p>
<p>Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. Y K Dwivedi, N Kshetri, L Hughes, E L Slade, A Jeyaraj, A K Kar, A M Baabdullah, A Koohang, V Raghavan, M Ahuja, H Albanna, M A Albashrawi, A S Al-Busaidi, J Balakrishnan, Y Barlette, S Basu, I Bose, L Brooks, D Buhalis, R Wright, 10.1016/j.ijinfomgt.2023.102642International Journal of Information Management. 711026422023Opinion paper: "So what if ChatGPT wrote it?</p>
<p>Prompting higher education towards AI-augmented teaching and learning practice. B Eager, R Brunton, 10.53761/1.20.5.02Journal of University Teaching and Learning Practice. 5202023</p>
<p>Abstracts written by ChatGPT fool scientists. H Else, 10.1038/d41586-023-00056-7Nature. 61379442023</p>
<p>Designing education for professional expertise development. Q Elvira, J Imants, B Dankbaar, M Segers, 10.1080/00313831.2015.1119729Scandinavian Journal of Educational Research. 612292017</p>
<p>The role of deliberate practice in the acquisition of expert performance. K A Ericsson, R T Krampe, C Tesch-Römer, 10.1037/0033-295X.100.3.363Psychological Review. 10031993</p>
<p>B Fernández-Manjón, P Moreno-Ger, I Martinez-Ortiz, M Freire, 10.4108/eai.5-11-2015.150611Challenges of serious games. EAI Endorsed Transactions on Game-Based Learning. 20152150611</p>
<p>Critical play: Radical game design. M Flanagan, 2009MIT Press</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, Q Guo, M Wang, H Wang, 10.48550/ARXIV.2312.109972023</p>
<p>How ChatGPT can inspire and improve serious board game design. W Gatti Junior, E Marasco, B Kim, L Behjat, M Eggermont, 10.17083/ijsg.v10i4.645International Journal of Serious Games. 1042023</p>
<p>Semiotic social spaces and affinity spaces: From The Age of Mythology to today's schools. J P Gee, 10.1017/CBO9780511610554.012Beyond communities of practice. D Barton, K Tusting, Cambridge University Press20051st ed.</p>
<p>. Google, 2023. 6 Dec 2023large language model</p>
<p>The benefits of playing video games. I Granic, A Lobel, R C M E Engels, 10.1037/a0034857American Psychologist. 6912014</p>
<p>Potential and limits of game-based learning. S Greipl, K Moeller, M Ninaus, 10.1504/IJTEL.2020.110047International Journal of Technology Enhanced Learning. 1243632020</p>
<p>Beyond born versus made. D Z Hambrick, B N Macnamara, G Campitelli, F Ullén, M A Mosing, 10.1016/bs.plm.2015.09.001Psychology of learning and motivation. Elsevier201664</p>
<p>Deliberate practice: Is that all it takes to become an expert?. D Z Hambrick, F L Oswald, E M Altmann, E J Meinz, F Gobet, G Campitelli, 10.1016/j.intell.2013.04.001Intelligence. 452014</p>
<p>Access to technology is the major challenge. C Hébert, J Jenson, T Terzopoulos, 10.1177/2042753021995315Teacher perspectives on barriers to DGBL in K-12 classrooms. E-Learning and Digital Media. 202118995315</p>
<p>Prompt engineering in medical education. T F Heston, C Khun, 10.3390/ime2030019International Medical Education. 232023</p>
<p>The gamer's brain. C Hodent, 2018CRC Press</p>
<p>Games as playful learning: Implications of developmental theory for game-based learning. B D Homer, C Raffaele, H Henderson, Handbook of game-based learning. J L Plass, R E Mayer, B D Homer, MIT Press2020</p>
<p>An exploratory survey about using ChatGPT in education, healthcare, and research. M Hosseini, C A Gao, D M Liebovitz, A M Carvalho, F S Ahmad, Y Luo, N Macdonald, K L Holmes, A Kho, 10.1101/2023.03.31.23287979Medical Ethics. 9792023Preprint</p>
<p>ChatGPT and antimicrobial advice: The end of the consulting infection doctor? The Lancet Infectious Diseases. A Howard, W Hope, A Gerada, 10.1016/S1473-3099(23)00113-5202323</p>
<p>Game-based learning has good chemistry with chemistry education: A three-level meta-analysis. Y Hu, T Gallagher, P Wouters, M Van Der Schaaf, L Kester, 10.1002/tea.21765Journal of Research in Science Teaching. 5992022</p>
<p>Could AI help you to write your next paper?. M Hutson, 10.1038/d41586-022-03479-wNature. 61179342022</p>
<p>Large language models in education: A focus on the complementary relationship between human teachers and ChatGPT. Education and Information Technologies. J Jeon, S Lee, 10.1007/s10639-023-11834-1202328</p>
<p>ChatGPT in schools: Here's where it's banned-And how it could potentially help students. A Johnson, 2023. 6 Dec 2023chatg pt-in-schoo lsheres-where-its-banned-and-how-it-could-poten tially-help-stude nts</p>
<p>Playing and making games for learning: Instructionist and constructionist perspectives for game studies. Y B Kafai, 10.1177/1555412005281767Games and Culture. 112006</p>
<p>Constructionist gaming: Understanding the benefits of making games for learning. Y B Kafai, Q Burke, 10.1080/00461520.2015.1124022Educational Psychologist. 504222015</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, S Krusche, G Kutyniok, T Michaeli, C Nerdel, J Pfeffer, O Poquet, M Sailer, A Schmidt, T Seidel, G Kasneci, 10.1016/j.lindif.2023.1022742023103102274</p>
<p>Foundation for problem-based gaming. K Kiili, 10.1111/j.1467-8535.2007.00704.xBritish Journal of Educational Technology. 3832007</p>
<p>Game-based learning in science, technology, engineering, and mathematics. E Klopfer, M Thompson, Handbook of game-based learning. J L Plass, R E Mayer, B D Homer, MIT Press2020</p>
<p>ChatGPT for language teaching and learning. L Kohnke, B L Moorhouse, D Zou, 10.1177/00336882231162868RELC Journal. 542682023</p>
<p>Artificial intelligence prompt engineering as a new digital competence: Analysis of generative AI technologies such as ChatGPT. P Korzynski, G Mazurek, P Krzypkowska, A Kurasinski, 10.15678/EBER.2023.110302Entrepreneurial Business and Economics Review. 1132023</p>
<p>ChatGPT's inconsistent moral advice influences users' judgment. S Krügel, A Ostermaier, M Uhl, 10.1038/s41598-023-31341-0Scientific Reports. 13145692023</p>
<p>An opportunity cost model of subjective effort and task performance. R Kurzban, A Duckworth, J W Kable, J Myers, 10.1017/S0140525X12003196Behavioral and Brain Sciences. 3662013</p>
<p>Large language models understand and can be enhanced by emotional stimuli. C Li, J Wang, Y Zhang, K Zhu, W Hou, J Lian, F Luo, Q Yang, X Xie, 10.48550/ARXIV.2307.117602023</p>
<p>Emotional foundations of game-based learning. K Loderer, R Pekrun, J L Plass, Handbook of game-based learning. J L Plass, R E Mayer, B D Homer, MIT Press2020</p>
<p>Cognitive foundations of game-based learning. R E Mayer, Handbook of game-based learning. J L Plass, R E Mayer, B D Homer, MIT Press2020</p>
<p>Prompt engineering as an important emerging skill for medical professionals. B Meskó, 10.2196/50638Tutorial. Journal of Medical Internet Research. 25e506382023</p>
<p>Theories of willpower affect sustained learning. E M Miller, G M Walton, C S Dweck, V Job, K H Trzesniewski, S M Mcclure, 10.1371/journal.pone.0038680PLoS One. 76802023. 6 Dec 2023. 2012MicrosoftBing Chat</p>
<p>Initial impressions of ChatGPt for anatomy education. S R Mogali, 10.1002/ase.2261Anatomical Sciences Education. 2023</p>
<p>Personalisation of learning: Towards hybrid human-AI learning technologies. I Molenaar, 10.1787/2cc25e37-en?format=htmlOECD digital education outlook 2021: Pushing the frontiers with artificial intelligence, blockchain and robots. OECD Publishing2021. 26 Jun 2023</p>
<p>A content-centric development process model. P Moreno-Ger, I Martinez-Ortiz, J L Sierra, B Fernandez-Manjon, 10.1109/MC.2008.73Computer. 4132008</p>
<p>M R Morris, J Sohl-Dickstein, N Fiedel, T Warkentin, A Dafoe, A Faust, C Farabet, S Legg, 10.48550/ARXIV.2311.02462Levels of AGI: Operationalizing progress on the path to AGI. 2023</p>
<p>M U Nasir, J Togelius, 10.48550/ARXIV.2305.18243Practical PCG through large language models. 2023</p>
<p>Mining learning and crafting scientific experiments: A literature review on the use of minecraft in education and research. S Nebel, S Schneider, G D Rey, Journal of Educational Technology and Society. 1922016</p>
<p>Closing the loop -The human role in artificial intelligence for education. M Ninaus, M Sailer, 10.3389/fpsyg.2022.956798Frontiers in Psychology. 139567982022</p>
<p>. Openai, 2023. 6 Dec 2023large language model</p>
<p>Prompting AI art: An investigation into the creative skill of prompt engineering. J Oppenlaender, R Linder, J Silvennoinen, 10.48550/ARXIV.2303.135342023</p>
<p>Near transfer to an unrelated N-back task mediates the effect of N-back working memory training on matrix reasoning. A Pahor, A R Seitz, S M Jaeggi, 10.1038/s41562-022-01384-wNature Human Behaviour. 692022</p>
<p>Mindstorms. Children, computers and powerful ideas. S Papert, 1980Basic Books</p>
<p>Enhancing reading skills through a video game mixing action mechanics and cognitive training. A Pasqualotto, I Altarelli, A De Angeli, Z Menestrina, D Bavelier, P Venuti, 10.1038/s41562-021-01254-xNature Human Behaviour. 642022</p>
<p>Implicit learning, tacit knowledge, expertise development, and naturalistic decision making. R E Patterson, B J Pierce, H H Bell, G Klein, 10.1177/155534341000400403Journal of Cognitive Engineering and Decision Making. 444032010</p>
<p>Play, dreams, and imitation in childhood. J Piaget, 1962Norton</p>
<p>Foundations of game-based learning. J L Plass, B D Homer, C K Kinzer, 10.1080/00461520.2015.1122533Educational Psychologist. 504332015. 2015. 11225</p>
<p>Theoretical foundations of game-based and playful learning. J L Plass, B D Homer, R E Mayer, C K Kinzer, Handbook of game-based learning. J L Plass, R E Mayer, B D Homer, MIT Press2020</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.48550/ARXIV.2303.053522023</p>
<p>Cognitive evaluation theory: The seedling that keeps self-determination theory growing. J Reeve, 10.1093/oxfordhb/9780197600047.013.3R. M. Ryan2023Oxford University PressThe Oxford handbook of self-determination theory. 1st ed.</p>
<p>Sowing the seeds for a more creative society. M Resnick, 10.1145/1518701.2167142Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. CHI '09: CHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing Systems. CHI '09: CHI Conference on Human Factors in Computing SystemsBoston MA USA2009. April 4</p>
<p>Scratch: Programming for all. M Resnick, J Maloney, A Monroy-Hernández, N Rusk, E Eastmond, K Brennan, A Millner, E Rosenbaum, J Silver, B Silverman, Y Kafai, 10.1145/1592761.1592779Communications of the ACM. 52112009</p>
<p>Design, make, play: Growing the next generation of STEM innovators. M Resnick, E Rosenbaum, M. Honey2013RoutledgeDesigning for tinkerability</p>
<p>Gamification and motivation. C S Rigby, The gameful world. S P Walz, S Deterding, MIT Press2014</p>
<p>Glued to games: How video games draw us in and hold us spellbound. C S Rigby, R M Ryan, 2011Praeger</p>
<p>New York City blocks use of the ChatGPT bot in its schools. D Rosenzweig-Ziff, The Washington Post. 2023. 6 Dec 2023</p>
<p>Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being. R M Ryan, E L Deci, 10.1037/0003-066X.55.1.68American Psychologist. 5512000</p>
<p>Self-determination theory: Basic psychological needs in motivation, development, and wellness. R M Ryan, E L Deci, 2017Guilford Press</p>
<p>Motivational foundations of game-based learning. R M Ryan, C S Rigby, Handbook of game-based learning. J L Plass, R E Mayer, B D Homer, MIT Press2020</p>
<p>The motivational pull of video games: A self-determination theory approach. R M Ryan, C S Rigby, A Przybylski, 10.1007/s11031-006-9051-8Motivation and Emotion. 3042006</p>
<p>ChatGPT utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns. M Sallam, 10.3390/healthcare11060887Healthcare. 1168872023</p>
<p>Partners in cognition: Extending human intelligence with intelligent technologies. G Salomon, D N Perkins, T Globerson, 10.3102/0013189X020003002Educational Researcher. 2031991</p>
<p>Can artificial intelligence help for scientific writing?. M Salvagno, F S Taccone, A G Gerli, 10.1186/s13054-023-04380-2Critical Care. 271752023</p>
<p>Automatic generation of programming exercises and code explanations using large language models. S Sarsa, P Denny, A Hellas, J Leinonen, 10.1145/3501385.3543957Proceedings of the 2022 ACM Conference on International Computing Education Research -Volume. the 2022 ACM Conference on International Computing Education Research -Volume20221</p>
<p>The artificially intelligent entrepreneur: ChatGPT, prompt engineering, and entrepreneurial rhetoric creation. C E Short, J C Short, 10.1016/j.jbvi.2023.e00388Journal of Business Venturing Insights. 19e003882023</p>
<p>E Shue, L Liu, B Li, Z Feng, X Li, G Hu, 10.1101/2023.03.07.531414Empowering beginners in bioinformatics with ChatGPT. 2023Preprint</p>
<p>. 10.1101/2023.03.07.531414Bioinformatics. </p>
<p>The role of self-determination theory in explaining teachers' motivation to continue to use e-learning technology. Ø Sørebø, H Halvari, V F Gulli, R Kristiansen, 10.1016/j.compedu.2009.06.001Computers and Education. 5342009</p>
<p>Explaining world-wide variation in navigation ability from millions of people: Citizen science project sea hero quest. H J Spiers, A Coutrot, M Hornberger, 10.1111/tops.12590Topics in Cognitive Science. 1512023</p>
<p>Apprenticeship in massively multiplayer online games. C Steinkuehler, Y Oh, 10.1017/CBO9781139031127.017Games, learning, and society: Learning and meaning in the digital age. C Steinkuehler, K Squire, &amp; S Barab, Cambridge University Press2012</p>
<p>Sociocultural foundations of game-based learning. C Steinkuehler, A M Tsaasan, Handbook of game-based learning. J L Plass, R E Mayer, B D Homer, MIT Press2020</p>
<p>ChatGPT listed as author on research papers: Many scientists disapprove. C Stokel-Walker, 10.1038/d41586-023-00107-zNature. 61379452023</p>
<p>Level Generation Through Large Language Models. G Todd, S Earle, M U Nasir, M C Green, J Togelius, 10.1145/3582437.3587211Proceedings of the 18th International Conference on the Foundations of Digital Games. the 18th International Conference on the Foundations of Digital Games2023</p>
<p>Perspectives into learning at the workplace. P Tynjälä, 10.1016/j.edurev.2007.12.001Educational Research Review. 322008</p>
<p>From university to working life: Graduates' workplace skills in practice. P Tynjälä, V Slotte, J Nieminen, K Lonka, E Olkinuora, Higher education and working life: Collaborations, confrontations and challenges. P Tynjälä, J Välimaa, G Boulton-Lewis, Elsevier Earli2006</p>
<p>ChatGPT: Five priorities for research. E A M Van Dis, J Bollen, W Zuidema, R Van Rooij, C L Bockting, 10.1038/d41586-023-00288-7Nature. 61479472023</p>
<p>Play and its role in the mental development of the child. L S Vygotsky, 10.2753/RPO1061-040505036Soviet Psychology. 531967</p>
<p>J Wang, E Shi, S Yu, Z Wu, C Ma, H Dai, Q Yang, Y Kang, J Wu, H Hu, C Yue, H Zhang, Y Liu, X Li, B Ge, D Zhu, Y Yuan, D Shen, T Liu, S Zhang, 10.48550/ARXIV.2304.14670Prompt engineering for healthcare: Methodologies and applications. 2023</p>
<p>Communities of practice: Learning, meaning, and identity. E Wenger, 1998Cambridge University Press</p>
<p>A prompt pattern catalog to enhance prompt engineering with ChatGPT. J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, 10.48550/ARXIV.2302.113822023</p>
<p>What is ChatGPT doing and why does it work. S Wolfram, 2023Wolfram Media</p>
<p>C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, 10.48550/ARXIV.2309.03409Large language models as optimizers. 2023</p>
<p>ChatGPT and environmental research. J.-J Zhu, J Jiang, M Yang, Z J Ren, 10.1021/acs.est.3c01818acs.est.3c01818Environmental Science &amp; Technology. 2023</p>
<p>A ghostwriter for the masses: ChatGPT and the future of writing. A Zimmerman, 10.1245/s10434-023-13436-0s10434-023-13436-0Annals of Surgical Oncology. 2023</p>
<p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. </p>            </div>
        </div>

    </div>
</body>
</html>
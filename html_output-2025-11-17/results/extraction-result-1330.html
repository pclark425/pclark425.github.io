<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1330 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1330</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1330</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-216867917</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2004.14684v1.pdf" target="_blank">Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Transferring learning-based models to the real world remains one of the hardest problems in model-free control theory. Due to the cost of data collection on a real robot and the limited sample efficiency of Deep Reinforcement Learning algorithms, models are usually trained in a simulator which theoretically provides an infinite amount of data. Despite offering unbounded trial and error runs, the reality gap between simulation and the physical world brings little guarantee about the policy behavior in real operation. Depending on the problem, expensive real fine-tuning and/or a complex domain randomization strategy may be required to produce a relevant policy. In this paper, a Soft-Actor Critic (SAC) training strategy using incremental environment complexity is proposed to drastically reduce the need for additional training in the real world. The application addressed is depth-based mapless navigation, where a mobile robot should reach a given waypoint in a cluttered environment with no prior mapping information. Experimental results in simulated and real environments are presented to assess quantitatively the efficiency of the proposed approach, which demonstrated a success rate twice higher than a naive strategy.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1330.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1330.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gazebo simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source robotics simulator used to emulate robot dynamics, sensors and environment interactions; used here as the training environment for the RL navigation agent. Integrated with ROS and uses URDF robot models and sensor plugins.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A multi-robot open-source simulator that provides physics-based simulation (via physics engines), sensor plugins, collision detection, and environment models; used here with a URDF robot model and RealSense sensor plugin to emulate a Wifibot and depth sensor at 10 Hz.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / mechanics (robot navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity physics and sensor emulation (approximate robot dynamics via URDF and physics engine; simulated depth sensor plugin; collision detection), intended to be representative of the real robot but not a perfect physical replica.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Uses URDF representation for robot kinematics/dynamics (representative model), physics engine handling collisions and dynamics, sensor plugin (Intel RealSense R200 model) to emulate depth at 10 fps, Gazebo collision bumper plugin for collisions; simplifies some aspects (discrete timestep, simplified obstacle/motion dynamics, limited sensor noise modeling noted indirectly via reality gap discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SAC-based navigation agent (Soft Actor-Critic policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agent implementing Soft Actor-Critic with three fully-connected neural networks (actor, soft Q, soft V); state includes sampled sparse depth values, relative target position, heading and last action; continuous linear/angular velocity outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Depth-based mapless robot navigation: reach a target waypoint in a cluttered room without a prior map while avoiding collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world Wifibot Lab V4 mobile robot experiments</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Best transferred model (trained with incremental Env1-2-3 and observation vector o3 = last two frames + difference) achieved 47% mean success rate and mean reward 38.751 over five real-world sessions (5×100 episodes); other trained variants transferred with lower success rates (examples: [F_t] Env1-2-3 32% success; [F_t] Env2 21%; [F_t] Env3 29%; [F_t;F_{t-1};F_{t-2}] Env1-2-3 42%; etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that a simulator representative of real-world conditions (URDF robot dynamics + realistic depth sensor model) was sufficient for useful sim-to-real transfer in depth-based navigation; they note that heavy domain randomization or extensive real fine-tuning was not required with their incremental-complexity training approach.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper reports the 'reality gap' in general and that naive training strategies (training only on Env2 or Env3) transferred worse to the real robot; also notes that the learned policies would likely fail in substantially different environments (e.g., labyrinths) because observations would differ from training data. No explicit low-level physics features singled out as the sole cause of failure.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1330.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>URDF model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified Robot Description Format (URDF) model of the robot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kinematic/dynamic robot description used inside Gazebo to represent the Wifibot Lab v4 dynamics and geometry for simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>URDF robot model (used within Gazebo)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A descriptive model encoding robot geometry, joints and approximate inertial/kinematic parameters used by Gazebo to simulate robot motion and collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>representative kinematic/dynamic model (medium-fidelity) — intended to approximate true robot dynamics but simplified compared to full high-fidelity multibody dynamics and detailed contact/elastic effects.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Captures robot geometry, joint relationships and approximate inertial parameters; used with Gazebo physics engine for collision and motion, but lacks fine-grained modeling of wheel-ground micro-contacts, electrical motor dynamics, and detailed compliance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SAC-based navigation agent (trained using URDF-based simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agent interacting with a simulated robot whose dynamics are provided by the URDF-based model inside Gazebo.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Depth-based mapless navigation (same as Gazebo entry)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robot</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>See Gazebo entry: best transferred success 47% for the incremental-trained model; URDF-based simulation was the basis for that training.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors indicate that a URDF model 'representative of the true robot dynamics' together with realistic sensor emulation was sufficient for transfer, implying high precision URDF details beyond representative values were not necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit URDF-specific failure cases reported, but general reality-gap effects are discussed; degraded transfer expected if environment/observations differ strongly (e.g., labyrinth).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1330.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intel RealSense R200 sensor model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R200 sensor model from the Intel RealSense ROS package (simulated depth sensor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated depth sensor plugin used in Gazebo to emulate the Intel RealSense depth camera output during training (set to 10 fps), providing depth frames used by the RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>R200 sensor model (Gazebo/ROS sensor plugin)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A software sensor model that emulates depth images produced by an Intel RealSense device, exposing configurable framerate and field-of-view characteristics to the simulation environment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / perception (sensor emulation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity sensor emulation (emulates basic depth output, frame rate and field-of-view; likely simplified noise characteristics relative to real sensor).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simulates depth frames at specified FPS (used at 10 fps here), modeled field-of-view properties consistent with the physical sensor; paper does not claim detailed sensor noise modeling or time-varying artifacts beyond basic emulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SAC-based navigation agent (consumes depth frames)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agent that samples 10 depth values from a particular row of each simulated depth frame to form sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Perception for depth-based navigation: obstacle detection and distance estimation from depth readings.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real Intel RealSense D435 on Wifibot in the real world</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>When trained using the simulated R200 model, the transferred policies achieved a range of real-world success rates up to 47% (best model); indicates the simulated sensor model sufficed for partial transfer without heavy DR.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors imply that a realistic-enough depth sensor emulation (field-of-view and depth sampling) combined with incremental complexity training was adequate; heavy sensor domain randomization was not necessary in their task.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure due to sensor-model mismatch reported, though general transfer limitations and worse performance for some naive training setups are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1330.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gazebo collision bumper plugin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gazebo collision bumper plugin</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gazebo plugin used to detect collisions in simulation during training episodes, serving as the collision signal for the RL reward and episode termination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gazebo collision bumper plugin (within Gazebo)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A collision-detection plugin that signals contact events between simulated links/objects to the ROS/Gazebo nodes; used to produce negative rewards and stop episodes upon collision.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / simulation tooling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>basic collision detection (binary contact events); does not model fine contact forces, deformation, or detailed frictional contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides binary collision detection for episode termination and reward shaping; simplifies contact physics (no detailed force sensing or elastic deformation modeling reported).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SAC-based navigation agent (uses collision events for reward shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agent whose reward uses collision detection (r_collision = -550) to penalize contact in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Avoid collisions while navigating to a target (safety-oriented behavior learned via reward shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world experiments where collisions are detected by a supervisor (Optitrack + safety stack)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Policies trained using the simulated collision plugin were evaluated on real robot with supervisor-based collision detection; best transferred success rates up to 47%.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper relies on basic collision event emulation for training; does not claim that high-fidelity contact dynamics are required.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure attributed solely to collision-plugin simplifications; general reality-gap caveats apply.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1330.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gym-Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gym-Gazebo (OpenAI Gym extension for Gazebo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit integrating OpenAI Gym APIs with Gazebo for reinforcement learning experiments in robotics; cited as prior work where Gazebo was used for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gym-Gazebo (wrapper around Gazebo)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An integration layer that exposes Gazebo simulated robots/environments through standard Gym interfaces for RL training and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>inherits Gazebo fidelity (medium-fidelity); provides standardized RL interfaces rather than changing underlying fidelity characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Facilitates episodic RL experiments using Gazebo; fidelity depends on the underlying Gazebo models and plugins used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1330.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViZDoom</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViZDoom (Doom-based AI research platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gaming-based visual RL environment (Doom engine) cited as an example of environments where RL has seen success; mentioned in related work as an environment with similar input/control spaces to robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ViZDoom (game-engine based simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A simulated first-person visual environment built on the Doom engine for visual reinforcement learning research; provides visuals and game dynamics rather than physical robot dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>artificial intelligence / simulated games (not physical sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity physical realism (game-based), high for visual complexity but not representative of robot physics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Focuses on visual observations and game dynamics; not intended to model real robot sensors or mechanics accurately.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1330.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Offworld Gym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Offworld Gym (open-access physical robotics environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform cited in related work for real-world reinforcement learning benchmarks that provides physical robot access; mentioned as prior art for real-world RL testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Offworld Gym (real-world robotics benchmark infrastructure, not a simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An open-access platform for real-world reinforcement learning experiments that provides remote access to physical robots/environments for benchmarking (not a simulated environment).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / experimental benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>real-world (highest practical fidelity since experiments are run on physical robots), not a simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides real sensors, actuators and physical interactions; avoids simulator simplifications but introduces real-world noise and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1330.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-sim (learning to generate synthetic datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited related work describing techniques to learn or synthesize simulated datasets to improve sim-to-real transfer, particularly by generating higher-quality synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Meta-sim (synthetic data / simulation generation method)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Approach/pipeline that learns to generate synthetic datasets or simulated scenes to better match real-world distributions (used in vision and perception tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer vision / simulation data generation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>variable — aims to increase fidelity of synthetic data to match real distributions (medium-to-high depending on learned generation), but specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Focus on appearance and dataset realism rather than detailed physical dynamics; may model visual properties, textures, and object placement statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1330.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1330.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot Operating System (ROS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Middleware used to connect the training code and Gazebo simulator and to run the real robot supervision stack; facilitated integration of training, simulation and real-hardware components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ROS (middleware, not a simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Robotics middleware providing communication primitives (topics/services) used to connect the PyTorch training node, Gazebo simulator, and real robot components.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / software infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not applicable (software middleware)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides consistent communication and tooling to reduce implementation mismatch between sim and real setups; helps reproducibility and rapid transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Design and use paradigms for Gazebo, an open-source multi-robot simulator <em>(Rating: 2)</em></li>
                <li>Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real robot learning from pixels with progressive nets <em>(Rating: 2)</em></li>
                <li>Learning to simulate <em>(Rating: 2)</em></li>
                <li>Meta-sim: Learning to generate synthetic datasets <em>(Rating: 1)</em></li>
                <li>ViZDoom: A Doom-based AI research platform for visual reinforcement learning <em>(Rating: 1)</em></li>
                <li>Offworld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1330",
    "paper_id": "paper-216867917",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Gazebo",
            "name_full": "Gazebo simulator",
            "brief_description": "Open-source robotics simulator used to emulate robot dynamics, sensors and environment interactions; used here as the training environment for the RL navigation agent. Integrated with ROS and uses URDF robot models and sensor plugins.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "use",
            "simulator_name": "Gazebo",
            "simulator_description": "A multi-robot open-source simulator that provides physics-based simulation (via physics engines), sensor plugins, collision detection, and environment models; used here with a URDF robot model and RealSense sensor plugin to emulate a Wifibot and depth sensor at 10 Hz.",
            "scientific_domain": "robotics / mechanics (robot navigation)",
            "fidelity_level": "medium-fidelity physics and sensor emulation (approximate robot dynamics via URDF and physics engine; simulated depth sensor plugin; collision detection), intended to be representative of the real robot but not a perfect physical replica.",
            "fidelity_characteristics": "Uses URDF representation for robot kinematics/dynamics (representative model), physics engine handling collisions and dynamics, sensor plugin (Intel RealSense R200 model) to emulate depth at 10 fps, Gazebo collision bumper plugin for collisions; simplifies some aspects (discrete timestep, simplified obstacle/motion dynamics, limited sensor noise modeling noted indirectly via reality gap discussion).",
            "model_or_agent_name": "SAC-based navigation agent (Soft Actor-Critic policy)",
            "model_description": "Reinforcement learning agent implementing Soft Actor-Critic with three fully-connected neural networks (actor, soft Q, soft V); state includes sampled sparse depth values, relative target position, heading and last action; continuous linear/angular velocity outputs.",
            "reasoning_task": "Depth-based mapless robot navigation: reach a target waypoint in a cluttered room without a prior map while avoiding collisions.",
            "training_performance": null,
            "transfer_target": "real-world Wifibot Lab V4 mobile robot experiments",
            "transfer_performance": "Best transferred model (trained with incremental Env1-2-3 and observation vector o3 = last two frames + difference) achieved 47% mean success rate and mean reward 38.751 over five real-world sessions (5×100 episodes); other trained variants transferred with lower success rates (examples: [F_t] Env1-2-3 32% success; [F_t] Env2 21%; [F_t] Env3 29%; [F_t;F_{t-1};F_{t-2}] Env1-2-3 42%; etc.).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors argue that a simulator representative of real-world conditions (URDF robot dynamics + realistic depth sensor model) was sufficient for useful sim-to-real transfer in depth-based navigation; they note that heavy domain randomization or extensive real fine-tuning was not required with their incremental-complexity training approach.",
            "failure_cases": "Paper reports the 'reality gap' in general and that naive training strategies (training only on Env2 or Env3) transferred worse to the real robot; also notes that the learned policies would likely fail in substantially different environments (e.g., labyrinths) because observations would differ from training data. No explicit low-level physics features singled out as the sole cause of failure.",
            "uuid": "e1330.0"
        },
        {
            "name_short": "URDF model",
            "name_full": "Unified Robot Description Format (URDF) model of the robot",
            "brief_description": "A kinematic/dynamic robot description used inside Gazebo to represent the Wifibot Lab v4 dynamics and geometry for simulation.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "use",
            "simulator_name": "URDF robot model (used within Gazebo)",
            "simulator_description": "A descriptive model encoding robot geometry, joints and approximate inertial/kinematic parameters used by Gazebo to simulate robot motion and collisions.",
            "scientific_domain": "robotics / mechanics",
            "fidelity_level": "representative kinematic/dynamic model (medium-fidelity) — intended to approximate true robot dynamics but simplified compared to full high-fidelity multibody dynamics and detailed contact/elastic effects.",
            "fidelity_characteristics": "Captures robot geometry, joint relationships and approximate inertial parameters; used with Gazebo physics engine for collision and motion, but lacks fine-grained modeling of wheel-ground micro-contacts, electrical motor dynamics, and detailed compliance.",
            "model_or_agent_name": "SAC-based navigation agent (trained using URDF-based simulation)",
            "model_description": "RL agent interacting with a simulated robot whose dynamics are provided by the URDF-based model inside Gazebo.",
            "reasoning_task": "Depth-based mapless navigation (same as Gazebo entry)",
            "training_performance": null,
            "transfer_target": "real-world robot",
            "transfer_performance": "See Gazebo entry: best transferred success 47% for the incremental-trained model; URDF-based simulation was the basis for that training.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors indicate that a URDF model 'representative of the true robot dynamics' together with realistic sensor emulation was sufficient for transfer, implying high precision URDF details beyond representative values were not necessary.",
            "failure_cases": "No explicit URDF-specific failure cases reported, but general reality-gap effects are discussed; degraded transfer expected if environment/observations differ strongly (e.g., labyrinth).",
            "uuid": "e1330.1"
        },
        {
            "name_short": "Intel RealSense R200 sensor model",
            "name_full": "R200 sensor model from the Intel RealSense ROS package (simulated depth sensor)",
            "brief_description": "Simulated depth sensor plugin used in Gazebo to emulate the Intel RealSense depth camera output during training (set to 10 fps), providing depth frames used by the RL agent.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "use",
            "simulator_name": "R200 sensor model (Gazebo/ROS sensor plugin)",
            "simulator_description": "A software sensor model that emulates depth images produced by an Intel RealSense device, exposing configurable framerate and field-of-view characteristics to the simulation environment.",
            "scientific_domain": "robotics / perception (sensor emulation)",
            "fidelity_level": "medium-fidelity sensor emulation (emulates basic depth output, frame rate and field-of-view; likely simplified noise characteristics relative to real sensor).",
            "fidelity_characteristics": "Simulates depth frames at specified FPS (used at 10 fps here), modeled field-of-view properties consistent with the physical sensor; paper does not claim detailed sensor noise modeling or time-varying artifacts beyond basic emulation.",
            "model_or_agent_name": "SAC-based navigation agent (consumes depth frames)",
            "model_description": "RL agent that samples 10 depth values from a particular row of each simulated depth frame to form sensory input.",
            "reasoning_task": "Perception for depth-based navigation: obstacle detection and distance estimation from depth readings.",
            "training_performance": null,
            "transfer_target": "real Intel RealSense D435 on Wifibot in the real world",
            "transfer_performance": "When trained using the simulated R200 model, the transferred policies achieved a range of real-world success rates up to 47% (best model); indicates the simulated sensor model sufficed for partial transfer without heavy DR.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors imply that a realistic-enough depth sensor emulation (field-of-view and depth sampling) combined with incremental complexity training was adequate; heavy sensor domain randomization was not necessary in their task.",
            "failure_cases": "No explicit failure due to sensor-model mismatch reported, though general transfer limitations and worse performance for some naive training setups are reported.",
            "uuid": "e1330.2"
        },
        {
            "name_short": "Gazebo collision bumper plugin",
            "name_full": "Gazebo collision bumper plugin",
            "brief_description": "Gazebo plugin used to detect collisions in simulation during training episodes, serving as the collision signal for the RL reward and episode termination.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "use",
            "simulator_name": "Gazebo collision bumper plugin (within Gazebo)",
            "simulator_description": "A collision-detection plugin that signals contact events between simulated links/objects to the ROS/Gazebo nodes; used to produce negative rewards and stop episodes upon collision.",
            "scientific_domain": "robotics / simulation tooling",
            "fidelity_level": "basic collision detection (binary contact events); does not model fine contact forces, deformation, or detailed frictional contact dynamics.",
            "fidelity_characteristics": "Provides binary collision detection for episode termination and reward shaping; simplifies contact physics (no detailed force sensing or elastic deformation modeling reported).",
            "model_or_agent_name": "SAC-based navigation agent (uses collision events for reward shaping)",
            "model_description": "RL agent whose reward uses collision detection (r_collision = -550) to penalize contact in simulation.",
            "reasoning_task": "Avoid collisions while navigating to a target (safety-oriented behavior learned via reward shaping)",
            "training_performance": null,
            "transfer_target": "real-world experiments where collisions are detected by a supervisor (Optitrack + safety stack)",
            "transfer_performance": "Policies trained using the simulated collision plugin were evaluated on real robot with supervisor-based collision detection; best transferred success rates up to 47%.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper relies on basic collision event emulation for training; does not claim that high-fidelity contact dynamics are required.",
            "failure_cases": "No explicit failure attributed solely to collision-plugin simplifications; general reality-gap caveats apply.",
            "uuid": "e1330.3"
        },
        {
            "name_short": "Gym-Gazebo",
            "name_full": "Gym-Gazebo (OpenAI Gym extension for Gazebo)",
            "brief_description": "A toolkit integrating OpenAI Gym APIs with Gazebo for reinforcement learning experiments in robotics; cited as prior work where Gazebo was used for RL.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "mention",
            "simulator_name": "Gym-Gazebo (wrapper around Gazebo)",
            "simulator_description": "An integration layer that exposes Gazebo simulated robots/environments through standard Gym interfaces for RL training and benchmarking.",
            "scientific_domain": "robotics / reinforcement learning",
            "fidelity_level": "inherits Gazebo fidelity (medium-fidelity); provides standardized RL interfaces rather than changing underlying fidelity characteristics.",
            "fidelity_characteristics": "Facilitates episodic RL experiments using Gazebo; fidelity depends on the underlying Gazebo models and plugins used.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": null,
            "failure_cases": "",
            "uuid": "e1330.4"
        },
        {
            "name_short": "ViZDoom",
            "name_full": "ViZDoom (Doom-based AI research platform)",
            "brief_description": "A gaming-based visual RL environment (Doom engine) cited as an example of environments where RL has seen success; mentioned in related work as an environment with similar input/control spaces to robotics.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "mention",
            "simulator_name": "ViZDoom (game-engine based simulator)",
            "simulator_description": "A simulated first-person visual environment built on the Doom engine for visual reinforcement learning research; provides visuals and game dynamics rather than physical robot dynamics.",
            "scientific_domain": "artificial intelligence / simulated games (not physical sciences)",
            "fidelity_level": "low-fidelity physical realism (game-based), high for visual complexity but not representative of robot physics.",
            "fidelity_characteristics": "Focuses on visual observations and game dynamics; not intended to model real robot sensors or mechanics accurately.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": null,
            "failure_cases": "",
            "uuid": "e1330.5"
        },
        {
            "name_short": "Offworld Gym",
            "name_full": "Offworld Gym (open-access physical robotics environment)",
            "brief_description": "A platform cited in related work for real-world reinforcement learning benchmarks that provides physical robot access; mentioned as prior art for real-world RL testing.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "mention",
            "simulator_name": "Offworld Gym (real-world robotics benchmark infrastructure, not a simulator)",
            "simulator_description": "An open-access platform for real-world reinforcement learning experiments that provides remote access to physical robots/environments for benchmarking (not a simulated environment).",
            "scientific_domain": "robotics / experimental benchmarks",
            "fidelity_level": "real-world (highest practical fidelity since experiments are run on physical robots), not a simulator.",
            "fidelity_characteristics": "Provides real sensors, actuators and physical interactions; avoids simulator simplifications but introduces real-world noise and constraints.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": null,
            "failure_cases": "",
            "uuid": "e1330.6"
        },
        {
            "name_short": "Meta-sim",
            "name_full": "Meta-sim (learning to generate synthetic datasets)",
            "brief_description": "Cited related work describing techniques to learn or synthesize simulated datasets to improve sim-to-real transfer, particularly by generating higher-quality synthetic data.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "mention",
            "simulator_name": "Meta-sim (synthetic data / simulation generation method)",
            "simulator_description": "Approach/pipeline that learns to generate synthetic datasets or simulated scenes to better match real-world distributions (used in vision and perception tasks).",
            "scientific_domain": "computer vision / simulation data generation",
            "fidelity_level": "variable — aims to increase fidelity of synthetic data to match real distributions (medium-to-high depending on learned generation), but specifics not provided in this paper.",
            "fidelity_characteristics": "Focus on appearance and dataset realism rather than detailed physical dynamics; may model visual properties, textures, and object placement statistics.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": null,
            "failure_cases": "",
            "uuid": "e1330.7"
        },
        {
            "name_short": "ROS",
            "name_full": "Robot Operating System (ROS)",
            "brief_description": "Middleware used to connect the training code and Gazebo simulator and to run the real robot supervision stack; facilitated integration of training, simulation and real-hardware components.",
            "citation_title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
            "mention_or_use": "use",
            "simulator_name": "ROS (middleware, not a simulator)",
            "simulator_description": "Robotics middleware providing communication primitives (topics/services) used to connect the PyTorch training node, Gazebo simulator, and real robot components.",
            "scientific_domain": "robotics / software infrastructure",
            "fidelity_level": "not applicable (software middleware)",
            "fidelity_characteristics": "Provides consistent communication and tooling to reduce implementation mismatch between sim and real setups; helps reproducibility and rapid transfer.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": null,
            "failure_cases": "",
            "uuid": "e1330.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Design and use paradigms for Gazebo, an open-source multi-robot simulator",
            "rating": 2,
            "sanitized_title": "design_and_use_paradigms_for_gazebo_an_opensource_multirobot_simulator"
        },
        {
            "paper_title": "Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo",
            "rating": 2,
            "sanitized_title": "extending_the_openai_gym_for_robotics_a_toolkit_for_reinforcement_learning_using_ros_and_gazebo"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real robot learning from pixels with progressive nets",
            "rating": 2,
            "sanitized_title": "simtoreal_robot_learning_from_pixels_with_progressive_nets"
        },
        {
            "paper_title": "Learning to simulate",
            "rating": 2,
            "sanitized_title": "learning_to_simulate"
        },
        {
            "paper_title": "Meta-sim: Learning to generate synthetic datasets",
            "rating": 1,
            "sanitized_title": "metasim_learning_to_generate_synthetic_datasets"
        },
        {
            "paper_title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning",
            "rating": 1,
            "sanitized_title": "vizdoom_a_doombased_ai_research_platform_for_visual_reinforcement_learning"
        },
        {
            "paper_title": "Offworld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research",
            "rating": 1,
            "sanitized_title": "offworld_gym_openaccess_physical_robotics_environment_for_realworld_reinforcement_learning_benchmark_and_research"
        }
    ],
    "cost": 0.01638275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation</p>
<p>Thomas Chaffre 
Lab-STICC UMR CNRS 6285
ENSTA Bretagne
BrestFrance</p>
<p>School of Computer Science, Engineering and Mathematics
Flinders University
AdelaideSAAustralia</p>
<p>Julien Moras julien.moras@onera.fr 
DTIS
ONERA -The French Aerospace Lab
Université Paris Saclay
F-91123PalaiseauFrance</p>
<p>Adrien Chan-Hon-Tong adrien.chanhontong@onera.fr 
DTIS
ONERA -The French Aerospace Lab
Université Paris Saclay
F-91123PalaiseauFrance</p>
<p>Julien Marzat julien.marzat@onera.fr 
DTIS
ONERA -The French Aerospace Lab
Université Paris Saclay
F-91123PalaiseauFrance</p>
<p>Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation
Reinforcement LearningSim-to-Real TransferAutonomous Robot Navigation
Transferring learning-based models to the real world remains one of the hardest problems in model-free control theory. Due to the cost of data collection on a real robot and the limited sample efficiency of Deep Reinforcement Learning algorithms, models are usually trained in a simulator which theoretically provides an infinite amount of data. Despite offering unbounded trial and error runs, the reality gap between simulation and the physical world brings little guarantee about the policy behavior in real operation. Depending on the problem, expensive real fine-tuning and/or a complex domain randomization strategy may be required to produce a relevant policy. In this paper, a Soft-Actor Critic (SAC) training strategy using incremental environment complexity is proposed to drastically reduce the need for additional training in the real world. The application addressed is depth-based mapless navigation, where a mobile robot should reach a given waypoint in a cluttered environment with no prior mapping information. Experimental results in simulated and real environments are presented to assess quantitatively the efficiency of the proposed approach, which demonstrated a success rate twice higher than a naive strategy.</p>
<p>INTRODUCTION</p>
<p>State-of-the-art algorithms are nowadays able to provide solutions to most elementary robotic problems like exploration, mapless navigation or Simultaneous Localization And Mapping (SLAM), under reasonable assumptions (Cadena et al., 2016). However, robotic pipelines are usually an assembly of several modules, each one dealing with an elementary function (e.g. control, planning, localization, mapping) dedicated to one technical aspect of the task. Each of these modules usually requires expert knowledge to be integrated, calibrated, and tuned. Combining several elementary functions into a single grey box module is a challenge but is an extremely interesting alternative in order to reduce calibration needs or expertise dependency. Some of the elementary functions can raise issues in hard cases (e.g., computer vision in weakly textured environment or varying illumination conditions). Splitting the system into a nearly optimal control module processing a coarse computer vision mapping output may result in a poorer pipeline than directly using a map-and-command module which could achieve a better performance trade-off.</p>
<p>For this reason, there is a large academic effort to try to combine several robotic functions into learningbased modules, in particular using a deep reinforcement strategy as in (Zamora et al., 2016). A limitation of this approach is that the resulting module is task-dependent, thus usually not reusable for other purposes even if this could be moderated by multitask learning. A more serious limit is that learning such a function requires a large amount of trial and error. Training entirely with real robots is consequently unrealistic in practice considering the required time (even omitting physical safety of the platform during such learning process where starting behavior is almost random). On the other hand, due to the reality gap between the simulator and the real world, a policy trained exclusively in simulation is most likely to fail in real conditions (Dulac-Arnold et al., 2019). Hence, depending on the problem, expensive real fine-tuning, and/or a complex domain randomization strategy may be required to produce a relevant policy. The explainability and evaluation of safety guarantees provided by such learning approaches compared to conventional methods also remain challenging issues (Juozapaitis et al., 2019).</p>
<p>In this work, we address the problem of robot navigation in an uncharted cluttered area with Deep Reinforcement Learning. In this context we consider as a benchmark the task where a robotic agent (here a Wifibot Lab v4 mobile robot, see Figure 1) has to reach a given target position in a cluttered room. At the beginning of each episode, the robot starts at the same location but with a random orientation and gets the target position coordinates. The robot is equipped with a perception sensor providing a dense depth map (here an Intel RealSense D435), it has access to its current position in the environment and has to control the speed of its wheels (which are the continuous outputs of the proposed learning algorithm). The proposed training method is based on the Soft Actor-Critic method (Haarnoja et al., 2018) coupled with incremental environment complexity. The latter refers to a technique which consists in splitting the desired mission requirements into several environments, each one representing a different degree of complexity of the global mission. Furthermore, an experimental setup is also proposed in this paper for testing or learning continuation in the real environment without human supervision. The paper is organized as follows. Section 2 presents related work in robotic navigation and reinforcement learning. Section 3 details the proposed method, particularly the agent structure, the reward generation and the learning procedure. Finally, experiments and results in both simulated and real environments are presented in Section 4.</p>
<p>RELATED WORK</p>
<p>Classical methods for robot autonomous navigation are usually based on a set of several model-based blocks that perform SLAM (Mur-Artal and Tardós, 2017;Engel et al., 2015;Wurm et al., 2010) and waypoint navigation algorithms (Ş ucan et al., 2012;Kamel et al., 2017;Bak et al., 2001). The latter use either reactive methods, which are limited in horizon and sometimes trapped in local minima, or a combination of a trajectory planner using an additional mapping algorithm and a tracking controller. Trajectory planning is usually highly time-consuming and has difficulties to adapt to real-time computational constraints. It could be hinted that learning-based strategies will be able to achieve an implicit computational and informational trade-off between these techniques. In (Koltun, 2019), classic and learning-based navigation systems have been compared. The modular navigation pipeline they proposed divides the navigation process into 4 sub-tasks: mapping, localization, planning and locomotion. They demonstrated that the classical system outperforms the learned agent when taking as input RBG-D values. In contrast, the classical method is very sensitive to the global set of modalities (i.e. without depth information, it fails catastrophically). Although model-based 1 methods usually work well in practice, they demand expert knowledge and do not allow to tackle missions requiring great interaction with the environment. On the other hand, model-free approaches (in particular reinforcement learning) have shown impressive progress in gaming (Silver et al., 2017), and are beginning to be widely applied for robotic tasks involving computer vision (see (Carrio et al., 2017) for a review). This paradigm has also been successfully applied to video game environments (Mnih et al., 2013;Kempka et al., 2016;Lample and Chaplot, 2017), where the input space is similar to the control space of a robot. However these results cannot be readily applied in robotics, because these strategies have been learned and tested in the same virtual environment and moreover the reward (game score) is explicit.</p>
<p>1 Here the term model does not correspond to the one used in Section 3 but refers to whether or not a dynamical model of the controlled system is used in the control strategy whereas in RL, it is the model of the environment that would be provided to the RL algorithm.   Dedicated Deep Reinforcement Learning strategies have already been applied to different robotic platforms and functions. In (Chiang et al., 2019), pointto-point and path-following navigation behaviors that avoid moving obstacles were learned using a deep reinforcement learning approach, with validation in a real office environment. In (Xie et al., 2017), a dueling architecture based on a deep double-Q network (D3QN) was proposed for obstacle avoidance, using only monocular RGB vision as input signal. A convolutional neural network was constructed to predict depth from raw RGB images, followed by a Deep Q-Network consisting of a convolutional network and a dueling network to predict the Q-Value of angular and linear actions in parallel. They demonstrated the feasibility of transferring visual knowledge from virtual to real and the high performance of obstacle avoidance using monocular vision only. In (Zamora et al., 2016), a robot patrolling task was successfully learned in simulation. However, this strategy puts the emphasis on not hitting any obstacle more than anything else, therefore the system is not strongly forced to take risks (which is required when heading to a designated destination). Visual object recovery (Sampedro et al., 2019) has also been considered, the task being defined as reaching an object described by an appearance where all objectives are described in a common vocabulary: proximity to obstacle and proximity of target are embedded in the visual space directly. These two approaches have only been validated in a simulation environment, therefore an undetermined amount of work remains to transfer them to a real robot and environment. In (Kulhanek et al., 2019), a navigation task to an image-defined target was achieved using an advantage actor-critic strategy, and a learning process with increased environment complexity was described. The method we propose is similar in spirit to this one, but our contribution addresses mapless navigation as in (Tai et al., 2017), which forces the system to take more risks: the system fails if it does not reach the target sufficiently fast, so going closer to obstacles (without hitting them) should be considered. Also, in this task, the system has to process both metric and visual information: distance to obstacles should be perceived from sensor measurements (image, depth), while the target is given as a coordinate. We propose a new learning strategy to tackle this problem, similar to Curriculum Learning (CL) (Elman, 1993;Bengio et al., 2009) but easier to implement. CL aims at training a neural network more efficiently by using the concept of curriculum, a powerful tool inspired by how humans progressively learn from simple concepts to harder problems. Recent studies (Zaremba and Sutskever, 2014;Rusu et al., 2017;OpenAI et al., 2019) on the application of this method in the robotic field have shown promising outcomes. A drawback of these approaches is the need for heavy simulation, however there is little alternative: fine-tuning in real life seems to be a candidate, but as the fine-tuning database may be quite limited, it is hard (with a deep model) to avoid overfitting and/or catastrophic forgetting. In this paper, we study the behavior of the policy transferred from a simulated to a real environment, with a dedicated hardware setup for unsupervised real testing with a mobile robot. It turns out that depth-based mapless navigation does not seem to require a too heavy domain randomization or fine-tuning procedure with the proposed framework based on incremental complexity.</p>
<p>SAC-BASED NAVIGATION FRAMEWORK</p>
<p>Preliminaries</p>
<p>For completeness, we recall here the Policy Gradient (Sutton et al., 1999) point of view in which we aim at modeling and optimizing the policy directly. More formally, a policy function π is defined as follows:
π θ : S → A
Where θ is a vector of parameters, S is the state space and A is the action space. The vector θ is optimized and thus modified in training mode, while it is fixed in testing mode. The performance of the learned behavior is commonly measured in terms of success rate (number of successful runs over total number of runs). Typically for mapless navigation, a successful run happens if the robot reaches the targeted point without hitting any obstacle in some allowed duration. In training mode, the objective is to optimize θ such that the success rate during testing is high. However, trying to directly optimize θ with respect to the testing success rate is usually sample inefficient (it could be achieved using e.g. CMA-ES (Salimans et al., 2017)). Thus, the problem is instead modelled as a Markov process with state transitions associated to a reward. The objective of the training is therefore to maximize the expected (discounted) total reward:
min θ ∑ t∈1,...,T ∑ τ∈t,...,T r τ γ τ−t log (π θ (a t |s t ))
Direct maximization of the expected reward is a possible approach, another one consists in estimating the expected reward for each state, and they can be combined to improve performance. A turning point in the expansion of RL algorithms to continuous action spaces appeared in (Lillicrap et al., 2015) where Deep Deterministic Policy Gradient (DDPG) was introduced, an actor-critic model-free algorithm that expanded Deep Q-Learning to the continuous domain. This approach has then be improved in (Haarnoja et al., 2018) where the Soft Actor Critic (SAC) algorithm was proposed: it corresponds to an actor-critic strategy which adds a measure of the policy entropy into the reward to encourage exploration. Therefore, the policy consists in maximizing simultaneously the expected return and the entropy:
J(θ) = T ∑ t=1 E (s t ,a t )∼ρ π θ [r(s t , a t ) + αH(π θ (.|s t ))] (1) where H(π θ (.|s)) = − ∑ a∈A π θ (a) log π θ (a|s)(2)
The term H(π θ ) is the entropy measure of policy π θ and α is a temperature parameter that determines the relative importance of the entropy term. Entropy maximization leads to policies that have better exploration capabilities, with an equal probability to select nearoptimal strategies. SAC aims to learn three functions, a policy π θ (a t |s t ), a soft Q-value function Q w (s t , a t ) parameterized by w and a soft state-value function V Ψ (s t ) parameterized by Ψ. The Q-value and soft state-value functions are defined as follows:
Q w (s t , a t ) = r(s t , a t ) + γE s t+1 ∼ρ π (s) [V Ψ (s t+1 )] (3) V Ψ (s t ) = E a t ∼π [Q w (s t , a t ) − α log π θ (a t |s t )] (4)
Theoretically, we can derive V Ψ by knowing Q w and π θ but in practice, trying to also estimate the statevalue function helps stabilizing the training process. The terms ρ π (s) and ρ π (s, a) denote the state and the state-action marginals of the state distribution induced by the policy π(a|s). The Q-value function is trained to minimize the soft Bellman residual:
J Q (w) = E (s t ,a t )∼R <a href="5"> 1 2 (Q w (s t , a t ) − (r(s t , a t ) + γE s t+1 ρ π (s) [VΨ(s t+1 )])) 2 </a>
The state-value function is trained to minimize the mean squared error:
J V (Ψ) = E s t ∼R <a href="6"> 1 2 (V Ψ (s t ) − E[Q w (s t , a t ) − log π θ (a t , s t )]) 2 </a>
The policy is updated to minimize the Kullback-Leibler divergence:
π new = arg min π ∈∏ D KL (π (.|s t ), exp(Q π old (s t , .) − log Z π old (s t )))(7)
We use the partition function Z π old (s t ) to normalize the distribution and while it is intractable in general, it does not contribute to the gradient with respect to the new policy and can thus be neglected. This update guarantees that Q π new (s t , a t ) ≥ Q π old (s t , a t ), the proof of this lemma can be found in the Appendix B.2 of (Haarnoja et al., 2018). Despite performing well in simulation, the transfer of the obtained policy to a real platform is often problematic due to the reality gap between the simulator and the physical world (which is triggered by an inconsistency between physical parameters and incorrect physical modeling). Recently proposed approaches have tried to either strengthen the mathematical model (simulator) or increase the generalization capacities of the model (Ruiz et al., 2019;Kar et al., 2019). Among the existing techniques that facilitate model transfer, domain randomization (DR) is an unsupervised approach which requires little or no real data. It aims at training a policy across many virtual environments, as diverse as possible. By monitoring a set of N environment parameters with a configuration Σ (sampled from a randomization space, Σ ∈ Ξ ∈ R N ), the policy π θ can then use episode samples collected among a variety of configurations and as a result learn to better generalize. The policy parameter θ is trained to maximize the expected reward R (of a finite trajectory) averaged across a distribution of configurations:
θ * = arg max θ E Σ∼Ξ E π θ ,τ∼e Σ <a href="8">R(τ)</a>
where τ is a trajectory collected in the environment randomized by the configuration Σ. In (Vuong et al., 2019), domain randomization has been coupled with a simple iterative gradient-free stochastic optimization method (Cross Entropy) to solve (8). Assuming the randomization configuration is sampled from a distribution parameterized by φ, Σ ∼ P φ (Σ), the optimization process consists in learning a distribution on which a policy can achieve maximal performance in the real environment e real :
φ * = arg min φ L(π θ * (φ) ; e real ),(9)
where
θ * (φ) = arg min φ E Σ∼P φ (Σ) <a href="10">L(π θ ; e Σ )</a>
The term L(π,e) refers to the loss function of policy π evaluated in environment e. Since the ranges for the parameters are hand-picked in this uniform DR, it can be seen as a manual optimization process to tune φ for the optimal L(π θ ;e real ). The effectiveness of DR lies in the choice of the randomization parameters. In its original version (Sadeghi and Levine, 2016;Tobin et al., 2017), each randomization parameter Φ i was restricted to an inter-
val Φ i ∈ [Φ low i ; Φ high i ], i = 1, . . . , N.
The randomization parameters can control appearance or dynamics of the training environment.</p>
<p>Proposed Learning Architecture</p>
<p>State and observation vectors</p>
<p>The problem considered is to learn a policy to drive a mobile robot (with linear and angular velocities as continuous outputs) in a cluttered environment, using the knowledge of its current position and destination (external inputs) and the measurements acquired by its embedded depth sensor. The considered state is defined as:
s t = (o t , p t , h t , a t−1 )(11)
where o t is the observation of the environment from the depth sensor, p t and h t are respectively the relative position and heading of the robot toward the target, a t−1 are the last actions achieved (linear and angular velocities). The elementary observation vector o t is composed of depth values from the embedded Intel RealSense D435 sensor. The depth output resolution is 640 × 480 pixels with a maximum frame rate of 60 fps. Since the depth field of view of this sensor is limited to 87°± 3°× 58°± 1°× 95°± 3°, the environment is only partially observable. To limit the amount of values kept from this sensor, we decided to sample 10 values from a specific row of the depth map (denoted as δ). By doing this, we sample the environment along the ( X; Y ) orthonormal plane, similarly to a LIDAR sensor (but within an angle of 58°). In the following, the vector containing these 10 depth values captured from a frame at timestep t is denoted by F t . To be able to avoid obstacles, it seems natural to consider a period of observation longer than the current frame. For this reason, three different observation vectors have been evaluated:</p>
<p>• The current frame only, o 1 t = [F t ]. • The three last frames, o 2 t = [F t ; F t−1 ; F t−2 ] • The last two frames and their difference,
o 3 t = [F t ; F t−1 ; (F t − F t−1 )]
The sampling rate for the training and prediction of a policy is a critical parameter. It refers to the average number of s t obtained in one second. If it is too high, the long-term effects of the actions on the agent state cannot be captured, whereas a too low value would most likely lead to a sub-optimal policy. A good practice is at the very least to synchronize the sampling process with the robot slowest sensor (by doing this, every state s t contains new information). This was the depth sensor in our case, which is also the main contributor to the observation vector.</p>
<p>Policy structure</p>
<p>The architecture of networks encoding the policy seemed to have little impact, therefore we did not put a lot of emphasis on this part and a single one has been selected. In order to optimize the functions introduced in Section 3.1, three fully-connected neural networks are used as shown in Figure 2. The ndimensional depth range findings, the relative target position and the last action achieved are merged together as a (n + 4)-dimensional state vector s t . The sparse depth range findings are sampled from the raw depth findings and are normalized between 0 and 1. The 2-dimensional relative target position is represented in polar coordinates with respect to the robot coordinate frame. The last action performed takes the form of the last linear and angular velocities that are respectively expressed in m.s −1 and rad.s −1 .</p>
<p>Reward shaping</p>
<p>Reinforcement learning algorithms are very sensitive to the reward function, which seems to be the most critical component before model transfer strategy. A straightforward sparse reward function (positive on success, negative on failure) would most likely lead to failure when working with a physical agent. On the other hand, a too specific reward function seems too hard to be learned. However, we describe below how the reward shaping approach (Laud, 2004) could lead to an interesting success rate in simulation. At the beginning of each episode, the robot is placed at an initial position P with a randomized orientation θ. The goal for the robot is to reach a target position T whose coordinates (x T , y T ) change at each episode (the target relative position from the robot is an input of the models). Reaching the target (considered achieved when the robot is below some distance threshold d min from the target) produces a positive reward r reached , while touching an element of the environment is considered as failing and for this reason produces a negative reward r collision . The episode is stopped if one of these events occurs. Otherwise, the reward is based on the difference dR t between d t (the Euclidean distance from the target at timestep t) and d t−1 . If dR t is positive, the reward is equal to this quantity multiplied by a hyper-parameter C and reduced by a velocity factor V r (function of the current velocity v t and d t ). On the other hand, if dR t is negative (which means the robot moved away from the target during the last time step), the instant reward is equal to r recede . The corresponding reward function is thus:
r(s t , a t ) =          C × dR t ×V r if dR t &gt; 0 r recede if dR t ≤ 0 r reached if d t &lt; d min r collision if collision detected (12) where V r = (1 − max(v t , 0.1)) 1/max(d t ,0
.1) , r reached = 500, r collision = −550 and r recede = −10.</p>
<p>Without this velocity reduction factor V r , we observed during training that the agent was heading toward the target even though an object was in its field of view (which led to a collision). The reward signal based only on the distance rate dR t was too strong com- Figure 2: The network structure for our implementation of the SAC algorithm. Each layer is represented by its type, output size and activation function. The dense layer represents a fully-connected neural network. The models use the same learning rate l r = 3e −4 , optimizer (Adam) (Kingma and Ba, 2014) and activation function (Leaky Relu, (Maas, 2013)). The target smoothing coefficient τ is set to 5e −2 for the soft update and 1 for the hard update. pared to the collision signal. With this proposed reward function, we encourage the robot to get closer to the target and to decrease its velocity while it gets to the goal. In addition, it is important to relate the nonterminal reward to the distance of the current state to the target. This way, it is linked to a state function (a potential function) which is known to keep the optimal policy unchanged. More precisely, if the reward was simply defined as γd t+1 − d t , then the optimal policy would be the same with or without the shaping (which just fastens the convergence). Here, the shaping is a little more complicated and may change the optimal policy but it is still based on d t (see (Badnava and Mozayani, 2019) for more details on reward shaping and its benefits).</p>
<p>Incremental complexity vs naive</p>
<p>Sim-to-Real transfer</p>
<p>The mission was divided into three distinct environments as shown in Figure 3. The first one (Env1) is an empty room. By starting training in this context, we try to force the agent to learn how to simply move toward the target. The second environment (Env2) incorporates eight identical static obstacles uniformly spread in the room. Training in these conditions should allow the agent to learn how to avoid obstacles on its way to the target. The last environment (Env3) includes both static and mobile obstacles. Two identical large static obstacles are placed near the initial position of the robot while four other identical mobile obstacles are randomly distributed in the room at the beginning of each episode. Transition from an environment to another is based on the success rate S rate for the last 100 episodes. If this value exceeds a specific threshold, the agent will move to the next environment or will be sent back to the previous one. Transition from one environment to another is related to the local performance of the policy and is done during the current training session, ensuring the use of samples collected from various conditions to improve generalization. As illustrated in Figure 3, α 1 and β 1 rule transitions between Env1 and Env2 while α 2 and β 2 rule transition between Env2 and Env3. For this study, these parameters were set to α 1 = 90%, α 2 = 80%, and β 1 = β 2 = 50%. In the following, the "naive" strategy refers to training using only either Env2 or Env3. The training of all the models consisted of 5000 episodes with a maximum step size of 500 each. It was observed that learning with incremental complexity does not improve performance in simulation but has a critical impact in real life. It is relevant, since this domain randomization technique can be easily implemented for many other problems. </p>
<p>EXPERIMENTS</p>
<p>Training or evaluating a robotic agent interacting with a real environment is not straightforward. Indeed, both the training (or at least the fine-tuning) and the evaluation require a lot of task runs. So in this work, we used both simulation and real-world experiments and particularly studied the behavior of the transferred policy from the former to the latter. To do so, a simulation environment representative of the realworld conditions was built, and the real world environment was also instrumented to carry out unsupervised intensive experiments.</p>
<p>Simulation experiments</p>
<p>The proposed approach has been implemented using the Robot Operating System (ROS) middleware and the Gazebo simulator (Koenig and Howard, 2004). Some previous works already used Gazebo for reinforcement learning like Gym-Gazebo (Zamora et al., 2016;Kumar et al., 2019). An URDF model representative of the true robot dynamics has been generated. The R200 sensor model from the Intel RealSense ROS package was used to emulate the depth sensor (at 10 fps), and the Gazebo collision bumper plugin served to detect collisions. We created several environments that shared a common base, a room containing multiple obstacles (some fixed, others with their positions randomised at each episode). The training process was implemented with Pytorch (Paszke et al., 2019) as a ROS node communicating with the Gazebo node using topics and services. Both the simulator and the training code ran on the same desktop computer equipped with an Intel Xeon E5-1620 (4C-8T, 3.5Ghz), 16GB of memory and a GPU Nvidia GTX 1080, allowing us to perform the training of one model in approximately 6 hours in the Cuda frame-work (Ghorpade et al., 2012). The communication between the learning agent and the environment was done using a set of ROS topics and services, which facilitated transposition to the real robot.</p>
<p>Real-world experiments</p>
<p>The real world experiment took place into a closed room measuring 7 by 7 meters. The room was equipped with a motion capture system (Optitrack) used by the robot and by a supervision stack (described in what follows). Four obstacles (boxes) were placed into the room at the front or the side of the robot starting point. The same desktop computer processed the supervisor and the agent. The robot used was a Wifibot Lab V4 robotic platform which communicated with the ground station using WiFi. It carried an Intel RealSense D435 depth sensor and an onboard computer (Intel NUC 7), on which the prediction was computed using the learned policy. Since the number of runs needed for training and validation is large, this raises some practical issues:</p>
<p>• A long operation time is not possible with usual mobile robots due to their battery autonomy.</p>
<p>• Different risks of damaging the robotic platform can occur on its way to the target with obstacle avoidance. To tackle these issues, we instrumented the environment with two components. First, the room setup allowed the robot to be constantly plugged into a power outlet without disturbing its movements. Secondly, we developed a supervisor node to detect collisions, stop the current episode, and replace autonomously the robot to its starting location at the beginning of a new episode. As detailed in Figure 5, the supervisor multiplexes the command to the robot embedded low-level controller (angular and linear speeds).</p>
<p>During the learning phase, it uses the command coming from the SAC node and during the resetting phase it uses the command coming from a motion planner node. The motion planner node defines a safe return trajectory using a PRM* path planner (Ş ucan et al., 2012) and a trajectory tracking controller (Bak et al., 2001). No data was collected for learning during this return phase. During the episode, the supervisor node ( Figure 5) takes as input the linear and angular velocities estimated by our SAC model to send them to the robot. Whenever the episode is stopped, the supervisor takes as input the commands generated by the motion planner based on the mapping stack to make the robot move to a new starting position, without colliding with any element of the environment. Since the map is fixed, we built a ground truth 3D map (Figure 4) of the test environment before starting the experiment by manually moving the robot and integrating the depth sensor into an Octomap (Wurm et al., 2010) model (any other ground truth mapping technique would be suitable). Thanks to this infrastructure, we were able to run a large number of runtimes with a minimal need for human supervision.</p>
<p>Obviously, the duration of each real-life run is large (vs simulation), but the unsupervised evaluation of 100 runs can be performed in roughly ∼ 30 minutes, which is practical for evaluating the Sim-to-Real policy transfer.</p>
<p>Results</p>
<p>Experimental results for the approach proposed in the previous section are provided for the different observation vector configurations considered 2 . This evaluation consisted in a total of 5 sessions of 100 episodes each, conducted with the real robot thanks to the supervision stack described in Section 4.2. Let us stress that these performances are conservative due to safety margins included in the supervision stack but comparable for all models. It took us roughly 45 minutes to test one model under these conditions. Performances of the trained policies were finally assessed and compared in terms of mean success rate and mean reward over the 5 sessions. These results are provided in Tables 1 and 2 for the distinct cases outlined in Section 3.2. In these tables, we designate by F n the 10 depth values kept in the frame captured at time step n. This means that the first column indicates which observation vector o i t is used in the state s t . The second column specifies which environment has been used to train the models as shown in Section 3.3. It can observed that the models trained by using the incremental method (i.e. Env1-2-3 in the tables) obtain the best 2 A video can be found at https://tinyurl.com/sim2real-drl-robotnav performances in terms of mean success rate as well as in mean reward over the 5 sessions. The best one among the models trained incrementally is the model whose observation vector consisted of the last two frames and their difference (o 3 t ) with a success rate of 47% and a mean reward of 38.751. The performance can thus be scaled twice using the incremental complexity sim-to-real strategy coupled with the SAC reinforcement learning strategy. This result is not trivial as depth-based mapless navigation is harder than mapless patrolling (Zamora et al., 2016) or visual object recovery (Sampedro et al., 2019), which do not need to go close to obstacles (and these methods were only tested in simulated environments). It could be noted that even the naive learned-in-sim policy achieves a non-trivial success rate. The success rate could most probably be improved by carrying out a fine-tuning training session in the real-world experiment, however this is beyond the scope of this paper.</p>
<p>CONCLUSIONS</p>
<p>In this paper, we have proposed a mapless navigation planner trained end-to-end with Deep Reinforcement Learning. A domain randomization method was applied in order to increase the generalization capacities of the policy without additional training or finetuning in the real world. By taking as inputs only two 
[F t ] Env2 21% [F t ] Env3 29% [F t ] Env1-2-3 32% [F t ; F t−1 ; F t−2 ] Env2 38% [F t ; F t−1 ; F t−2 ] Env3 17% [F t ; F t−1 ; F t−2 ] Env1-2-3 42% [F t ; F t−1 ; F t − F t−1 ] Env2 24% [F t ; F t−1 ; F t − F t−1 ] Env3 33% [F t ; F t−1 ; F t − F t−1 ]
Env1-2-3 47% successive frames of 10 depth values and the target position relative to the mobile robot coordinate frame combined with a new incremental complexity training method, the given policy is able to accomplish depthbased navigation with a mobile robot in the real world even though it has only been trained in a ROS-Gazebo simulator. When compared to a naive training setup, this approach proved to be more robust to the transfer on the real platform. The models trained in this study were able to achieve the mission in an open environment containing box-size obstacles and should be able to perform well in similar indoor contexts with obstacles of different shapes. However, they would most likely fail in environments such as labyrinths because the observation inputs o t will be too different.</p>
<p>A direct improvement could be to include a final reference heading which can be easily considered since the Wifibot Lab V4 robotic platform is able to spin around. Future work will focus on the fair comparison between model-based methods and such learning algorithms for autonomous robot navigation, as well as addressing more complex robotics tasks.</p>
<p>(a) Robot learning in the ROS-Gazebo simulated environment.(b) Cable-powered Wifibot with depth sensor.</p>
<p>(c) Wifibot navigating in the real environment.</p>
<p>Figure 1 :
1Illustration of simulated and real environments for reinforcement learning of depth-based robot navigation.</p>
<p>Figure 3 :
3Illustration of the incremental complexity strategy. The policy is trained on multiple environments, each one representing an increment of subtasks (more complex obstacles) contributing to the global mission.</p>
<p>Figure 4 :
4Octomap ground truth of the environment. The frame denotes the robot position and the red ball the target position.</p>
<p>Figure 5 :
5Supervision stack for learning and testing in the real world.</p>
<p>Table 1 :
1Success rate (in %). F t designates depth measurements taken at time t.Observation 
vector (o t ) </p>
<p>Training 
environments </p>
<p>Success 
Rate </p>
<p>Table 2 :
2Mean reward values. F t designates depth measurements taken at time t.[F t ; F t−1 ; F t−2 ] Env2 -100.662 [F t ; F t−1 ; F t−2 ] Env3 -300.124 [F t ; F t−1 ; F t−2 ] Env1-2-3 22.412 [F t ; F t−1 ; F t − F t−1 ] Env2 -217.843 [F t ; F t−1 ; F t − F t−1 ] Env3 -56.288 [F t ; F t−1 ; F t − F t−1 ]Observation 
vector (o t ) </p>
<p>Training 
environments </p>
<p>Mean 
reward </p>
<p>[F t ] 
Env2 
-248.892 
[F t ] 
Env3 
-189.68 
[F t ] 
Env1-2-3 
-95.623 
Env1-2-3 
38.751 </p>
<p>A new potentialbased reward shaping for reinforcement learning agent. B Badnava, N Mozayani, abs/1902.06239ArXiv. Badnava, B. and Mozayani, N. (2019). A new potential- based reward shaping for reinforcement learning agent. ArXiv, abs/1902.06239.</p>
<p>Path following mobile robot in the presence of velocity constraints. IMM, Informatics and Mathematical Modelling. M Bak, N K Poulsen, O Ravn, The Technical University of DenmarkBak, M., Poulsen, N. K., and Ravn, O. (2001). Path fol- lowing mobile robot in the presence of velocity con- straints. IMM, Informatics and Mathematical Mod- elling, The Technical University of Denmark.</p>
<p>Y Bengio, J Louradour, R Collobert, Weston , J , Curriculum learning. In ICML '09. Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In ICML '09.</p>
<p>Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. C Cadena, L Carlone, H Carrillo, Y Latif, D Scaramuzza, J Neira, I D Reid, J J Leonard, IEEE Transactions on Robotics. 32Cadena, C., Carlone, L., Carrillo, H., Latif, Y., Scara- muzza, D., Neira, J., Reid, I. D., and Leonard, J. J. (2016). Past, present, and future of simultaneous lo- calization and mapping: Toward the robust-perception age. IEEE Transactions on Robotics, 32:1309-1332.</p>
<p>A review of deep learning methods and applications for unmanned aerial vehicles. A Carrio, C Sampedro, A Rodriguez-Ramos, P Campoy, Journal of Sensors. 13Carrio, A., Sampedro, C., Rodriguez-Ramos, A., and Cam- poy, P. (2017). A review of deep learning methods and applications for unmanned aerial vehicles. Journal of Sensors, 2017:13.</p>
<p>Learning navigation behaviors end-to-end with autoRL. H.-T L Chiang, A Faust, M Fiser, Francis , A , IEEE Robotics and Automation Letters. 42Chiang, H.-T. L., Faust, A., Fiser, M., and Francis, A. (2019). Learning navigation behaviors end-to-end with autoRL. IEEE Robotics and Automation Letters, 4(2):2007-2014.</p>
<p>Challenges of real-world reinforcement learning. G Dulac-Arnold, D J Mankowitz, T Hester, ArXiv, abs/1904.12901Dulac-Arnold, G., Mankowitz, D. J., and Hester, T. (2019). Challenges of real-world reinforcement learn- ing. ArXiv, abs/1904.12901.</p>
<p>Learning and development in neural networks: the importance of starting small. J L Elman, Cognition. 48Elman, J. L. (1993). Learning and development in neural networks: the importance of starting small. Cognition, 48:71-99.</p>
<p>Large-scale direct SLAM with stereo cameras. J Engel, J Stückler, D Cremers, IEEE/RSJ International Conference on Intelligent Robots and Systems. Hamburg, GermanyEngel, J., Stückler, J., and Cremers, D. (2015). Large-scale direct SLAM with stereo cameras. In IEEE/RSJ In- ternational Conference on Intelligent Robots and Sys- tems, Hamburg, Germany, pages 1935-1942.</p>
<p>Gpgpu processing in cuda architecture. J Ghorpade, J Parande, M Kulkarni, A Bawaskar, abs/1202.4347ArXiv. Ghorpade, J., Parande, J., Kulkarni, M., and Bawaskar, A. (2012). Gpgpu processing in cuda architecture. ArXiv, abs/1202.4347.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, arXiv:1801.01290arXiv preprintHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.</p>
<p>Explainable reinforcement learning via reward decomposition. Z Juozapaitis, A Koul, A Fern, M Erwig, F Doshi-Velez, Proceedings of the IJ-CAI 2019 Workshop on Explainable Artificial Intelligence. the IJ-CAI 2019 Workshop on Explainable Artificial IntelligenceJuozapaitis, Z., Koul, A., Fern, A., Erwig, M., and Doshi- Velez, F. (2019). Explainable reinforcement learning via reward decomposition. In Proceedings of the IJ- CAI 2019 Workshop on Explainable Artificial Intelli- gence, pages 47-53.</p>
<p>Model predictive control for trajectory tracking of unmanned aerial vehicles using robot operating system. M Kamel, T Stastny, K Alexis, R Siegwart, Robot Operating System (ROS). Kamel, M., Stastny, T., Alexis, K., and Siegwart, R. (2017). Model predictive control for trajectory tracking of unmanned aerial vehicles using robot operating sys- tem. In Robot Operating System (ROS), pages 3-39.</p>
<p>. Cham Springer, Springer, Cham.</p>
<p>Meta-sim: Learning to generate synthetic datasets. A Kar, A Prakash, M.-Y Liu, E Cameracci, J Yuan, M Rusiniak, D Acuna, A Torralba, S Fidler, Kar, A., Prakash, A., Liu, M.-Y., Cameracci, E., Yuan, J., Rusiniak, M., Acuna, D., Torralba, A., and Fidler, S. (2019). Meta-sim: Learning to generate synthetic datasets.</p>
<p>ViZDoom: A Doom-based AI research platform for visual reinforcement learning. M Kempka, M Wydmuch, G Runc, J Toczek, Jaśkowski , W , IEEE Conference on Computational Intelligence and Games (CIG). Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Jaśkowski, W. (2016). ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games (CIG), pages 1-8.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, abs/1412.6980CoRRKingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. CoRR, abs/1412.6980.</p>
<p>Design and use paradigms for Gazebo, an open-source multi-robot simulator. N P Koenig, A Howard, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 3Koenig, N. P. and Howard, A. (2004). Design and use paradigms for Gazebo, an open-source multi-robot simulator. IEEE/RSJ International Conference on In- telligent Robots and Systems (IROS), 3:2149-2154 vol.3.</p>
<p>Benchmarking classic and learned navigation in complex 3d environments. D M A D V Koltun, IEEE Robotics and Automation Letters. Koltun, D. M. A. D. V. (2019). Benchmarking classic and learned navigation in complex 3d environments. IEEE Robotics and Automation Letters.</p>
<p>Vision-based navigation using deep reinforcement learning. J Kulhanek, E Derner, T De Bruin, R Babuska, 9th European Conference on Mobile Robots. Kulhanek, J., Derner, E., de Bruin, T., and Babuska, R. (2019). Vision-based navigation using deep reinforce- ment learning. In 9th European Conference on Mobile Robots.</p>
<p>Offworld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research. A Kumar, T Buckley, Q Wang, A Kavelaars, I Kuzovkin, arXiv:1910.08639arXiv preprintKumar, A., Buckley, T., Wang, Q., Kavelaars, A., and Ku- zovkin, I. (2019). Offworld Gym: open-access phys- ical robotics environment for real-world reinforce- ment learning benchmark and research. arXiv preprint arXiv:1910.08639.</p>
<p>Playing FPS games with deep reinforcement learning. G Lample, D S Chaplot, Thirty-First AAAI Conference on Artificial Intelligence. Lample, G. and Chaplot, D. S. (2017). Playing FPS games with deep reinforcement learning. In Thirty-First AAAI Conference on Artificial Intelligence.</p>
<p>Theory and application of reward shaping in reinforcement learning. A D Laud, Technical reportLaud, A. D. (2004). Theory and application of reward shap- ing in reinforcement learning. Technical report.</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Contin- uous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.</p>
<p>Rectifier nonlinearities improve neural network acoustic models. A L Maas, Maas, A. L. (2013). Rectifier nonlinearities improve neural network acoustic models.</p>
<p>V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602Playing Atari with deep reinforcement learning. arXiv preprintMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing Atari with deep reinforcement learn- ing. arXiv preprint arXiv:1312.5602.</p>
<p>ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras. R Mur-Artal, J D Tardós, IEEE Transactions on Robotics. 335Mur-Artal, R. and Tardós, J. D. (2017). ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras. IEEE Transactions on Robotics, 33(5):1255-1262.</p>
<p>. Akkaya Openai, I Andrychowicz, M Chociej, M Litwin, M Mcgrew, B Petron, A Paino, A Plappert, M Powell, G Ribas, R Schneider, J Tezak, N Tworek, J Welinder, P Weng, L Yuan, Q.-M Zaremba, W Zhang, L , Solving rubik's cube with a robot hand. ArXiv, abs/1910.07113OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plap- pert, M., Powell, G., Ribas, R., Schneider, J., Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q.-M., Zaremba, W., and Zhang, L. (2019). Solving rubik's cube with a robot hand. ArXiv, abs/1910.07113.</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z De-Vito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R.Curran Associates, Inc32Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., De- Vito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Gar- nett, R., editors, Advances in Neural Information Pro- cessing Systems 32, pages 8024-8035. Curran Asso- ciates, Inc.</p>
<p>Learning to simulate. N Ruiz, S Schulter, M Chandraker, ICLRRuiz, N., Schulter, S., and Chandraker, M. (2019). Learning to simulate. ICLR.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerík, T Rothörl, N M O Heess, R Pascanu, R Hadsell, CoRL. Rusu, A. A., Vecerík, M., Rothörl, T., Heess, N. M. O., Pascanu, R., and Hadsell, R. (2017). Sim-to-real robot learning from pixels with progressive nets. In CoRL.</p>
<p>Cad2rl: Real singleimage flight without a single real image. F Sadeghi, S Levine, arXiv:1611.04201arXiv preprintSadeghi, F. and Levine, S. (2016). Cad2rl: Real single- image flight without a single real image. arXiv preprint arXiv:1611.04201.</p>
<p>Evolution strategies as a scalable alternative to reinforcement learning. T Salimans, J Ho, X Chen, S Sidor, I Sutskever, arXiv:1703.03864arXiv preprintSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution strategies as a scalable al- ternative to reinforcement learning. arXiv preprint arXiv:1703.03864.</p>
<p>A fullyautonomous aerial robot for search and rescue applications in indoor environments using learning-based techniques. C Sampedro, A Rodriguez-Ramos, H Bavle, A Carrio, P De La Puente, P Campoy, Journal of Intelligent &amp; Robotic Systems. 952Sampedro, C., Rodriguez-Ramos, A., Bavle, H., Carrio, A., de la Puente, P., and Campoy, P. (2019). A fully- autonomous aerial robot for search and rescue appli- cations in indoor environments using learning-based techniques. Journal of Intelligent &amp; Robotic Systems, 95(2):601-627.</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 5507676Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. Nature, 550(7676):354- 359.</p>
<p>The Open Motion Planning Library. I A Ş Ucan, M Moll, L E Kavraki, IEEE Robotics &amp; Automation Magazine. 194Ş ucan, I. A., Moll, M., and Kavraki, L. E. (2012). The Open Motion Planning Library. IEEE Robotics &amp; Automa- tion Magazine, 19(4):72-82.</p>
<p>Policy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, NIPS. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. In NIPS.</p>
<p>Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation. L Tai, G Paolo, M Liu, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Tai, L., Paolo, G., and Liu, M. (2017). Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation. In IEEE/RSJ Interna- tional Conference on Intelligent Robots and Systems (IROS), pages 31-36.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, Abbeel , P , IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. (2017). Domain randomization for trans- ferring deep neural networks from simulation to the real world. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 23-30.</p>
<p>How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?. Q Vuong, S Vikram, H Su, S Gao, H I Christensen, arXiv:1903.11774arXiv preprintVuong, Q., Vikram, S., Su, H., Gao, S., and Christensen, H. I. (2019). How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies? arXiv preprint arXiv:1903.11774.</p>
<p>Octomap: A probabilistic, flexible, and compact 3d map representation for robotic systems. K M Wurm, A Hornung, M Bennewitz, C Stachniss, W Burgard, ICRA 2010 workshop on best practice in 3D perception and modeling for mobile manipulation. 2Wurm, K. M., Hornung, A., Bennewitz, M., Stachniss, C., and Burgard, W. (2010). Octomap: A probabilis- tic, flexible, and compact 3d map representation for robotic systems. In ICRA 2010 workshop on best practice in 3D perception and modeling for mobile manipulation, volume 2.</p>
<p>Towards monocular vision based obstacle avoidance through deep reinforcement learning. L Xie, S Wang, A Markham, N Trigoni, arXiv:1706.09829arXiv preprintXie, L., Wang, S., Markham, A., and Trigoni, N. (2017). Towards monocular vision based obstacle avoidance through deep reinforcement learning. arXiv preprint arXiv:1706.09829.</p>
<p>Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo. I Zamora, N G Lopez, V M Vilches, A H Cordero, arXiv:1608.05742arXiv preprintZamora, I., Lopez, N. G., Vilches, V. M., and Cordero, A. H. (2016). Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo. arXiv preprint arXiv:1608.05742.</p>
<p>Learning to execute. ArXiv. W Zaremba, I Sutskever, abs/1410.4615Zaremba, W. and Sutskever, I. (2014). Learning to execute. ArXiv, abs/1410.4615.</p>            </div>
        </div>

    </div>
</body>
</html>
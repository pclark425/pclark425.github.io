<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5491 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5491</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5491</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1" target="_blank">Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</a></p>
                <p><strong>Paper Venue:</strong> Conference on Computational Natural Language Learning</p>
                <p><strong>Paper Abstract:</strong> To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs’ robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.</p>
                <p><strong>Cost:</strong> 0.036</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5491.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5491.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion parameter base large language model (base-LLM) evaluated in this study; trained via self-supervised language modeling (described as a base-LLM in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base-LLM (text-completion style); described in the paper as a 7B parameter model from Penedo et al. (2023), trained via self-supervision (base-LLM paradigm).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (first-order and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief (social reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic first- and second-order false-belief narrative tests (SA1: where will Sally look for the ball?; SA2: second-order belief about what Sally thinks Anne believes). Administered in original and two rewritten deviations; scored on experimental answer (correct/incorrect) and motivation (0-2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Falcon-7B performed poorly; classified among smaller models that tended to perform worse — struggled on first-order less than some larger base models and failed on almost all second-order questions (performance at or below child level; dropped substantially on deviation levels).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, age 7-8) used as baseline: SA1 mean = 0.45, SA2 mean = 0.225 (scores averaged and scaled to 0-1 as described in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline on second-order ToM and generally below or at child level overall; performed worse than larger base models (BLOOM, Davinci, LLaMA-30B) on SA1 and notably worse on SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Smaller model size and base (non-instruction) training; prompts were provided as text-completion (because base-LLMs do not follow instruction prompts well), which may have affected comparability; poor robustness to deviations and second-order items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5491.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 30-billion parameter base LLM (base-LLM) evaluated on ToM tasks in the paper; trained via self-supervised objectives as a foundation model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (30B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base-LLM (30B), described in the paper as a 30B parameter model (Touvron et al., 2023); used with text-completion prompting consistent with base-LLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>SA1 first-order false-belief and SA2 second-order false-belief narratives, with original and two deviation variants; scoring as in Methods.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLaMA-30B performed above child level on first-order ToM (SA1) — one of the base-LLMs that did well on SA1 — but fell at or below child level on second-order ToM (SA2), with performance decreasing for deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Matched or exceeded child performance on SA1 but underperformed relative to children on SA2; exhibited the common pattern of base-LLMs: reasonable on SA1, weak on SA2 and degraded with deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>As a base-LLM, prompted differently (text-completion) than instruct-LLMs; may have advantages/disadvantages from this format; struggled with generalization to deviated formulations and recursive ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5491.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-davinci (text-davinci-002/003 family; base/instruction variants as used)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175-billion-parameter model from the GPT family included among base-LLMs in the study (table lists GPT-davinci as base-LLM, 175B).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Listed as a 175B base-LLM in the paper's table (Brown et al. lineage); used with text-completion style prompts for base-LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief narratives with deviations; scored on answer and motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-davinci (as a large base-LLM) performed above child level on SA1 (first-order), but fell to at or below child level on SA2. It showed some improvement over smaller base models, but struggled with second-order recursion and deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperformed many base-LLMs on SA1 (above child level) but did not reliably exceed child performance on SA2; generalization to deviations was limited.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Prompting as base text-completion (rather than instruction-following) may have altered behavior; the paper notes GPT-3 (instruct vs. base variants) behaved differently depending on prompt style, suggesting format sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5491.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOOM-176B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM (176B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open 176-billion parameter base LLM evaluated in the study; a large base model that in some tasks outperformed smaller base models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base-LLM listed at ~176B parameters (Scao et al., 2022); used with text-completion prompting consistent with other base models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic first- and second-order false-belief tasks with deviations; scored on experimental answer and motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>BLOOM performed above child level on first-order SA1 (one of the larger base models that did well on SA1) but fell at or below child level on second-order SA2, with decreased performance under deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Matched or exceeded child performance on SA1 but underperformed on SA2 relative to children; similar pattern to other large base models (better on SA1, poor generalization on SA2).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>As with other base-LLMs, prompts used completion format; the paper notes base-LLMs generally underperform versus instruct-LLMs and are sensitive to prompt formatting and prompt length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5491.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned variant of Falcon (7B) evaluated as an instruct-LLM in the study; smaller instruction-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-instruct (Falcon-7B-I)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 7B model (the instruct variant of Falcon); described as an instruct-LLM in the paper's table.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief tasks (SA1, SA2) with original and rewritten deviations; instruct-LLMs were prompted in question-answering format like children.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Falcon-7B-I struggled: the chat-optimized version failed on all second-order questions and was among the worst-performing instruct-LLMs on SA tasks, showing poor performance across deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children on SA2 and generally performed worse than larger instruct-LLMs; did not match child performance on second-order ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Small model size despite instruction-tuning; instruction-tuning did not compensate for limited capacity on recursive ToM in this case.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5491.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 11-billion parameter instruction-tuned model evaluated among instruct-LLMs; trained/finetuned for instruction-following tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM (Flan-T5, 11B) used as an instruct-LLM; described in paper as performing variably across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>SA1/SA2 narratives with deviations; instruct-LLMs prompted in QA format and motivations elicited by inserting model answers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FLAN-T5, as a smaller instruct-LLM, tended to perform worse on SA tasks: it was among smaller models that performed worse overall and struggled especially on SA2 and with deviations (fell at or below child level on more complex/deviated SA2).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to best instruct-LLMs (GPT-3.5, GPT-4) and generally at or below child level on second-order ToM; better on some Strange Stories items (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Smaller instruct model; showed some strengths on non-literal language tasks (Strange Stories) relative to base models, but limited recursion/robustness on SA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5491.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 / text-davinci-003 (instruction-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 family instruct-tuned model (text-davinci-003) evaluated in the study; 175B parameter family with instruction tuning per Ouyang et al. (2022).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3 instruct variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 family model (text-davinci-003), listed as an instruct-LLM in the paper; attributed to Ouyang et al. (2022) and 175B parameter scale referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief tests with deviations; instruct-LLMs were prompted in question-answering format similar to children.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 (text-davinci-003) displayed structurally low scores on SA in this study when prompted with open questions (contrasting with Kosinski (2023) results that used text-completion prompts). It performed closer to child level on some Strange Stories items but was degraded on SA2 and deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Did not reliably exceed child performance on SA tasks in this prompting regime; noted as a striking exception (structurally low scores) compared to prior reports that used different prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Prompt format sensitivity: the paper highlights that GPT-3's performance depended on whether it was prompted as text-completion (as in Kosinski) vs open QA (as used here); this complicates direct comparisons and suggests format-dependent capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5491.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-ish parameter instruction-tuned GPT-family model (GPT-3.5) evaluated across ToM tests; presented as an instruct-LLM that often performed near top-tier models except on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-family model (GPT-3.5), cited as 175B in the paper's table; used in question-answering prompt format for instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>SA1 and SA2 narratives with deviations; motivations elicited by reinserting model answers into prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 performed well above child level on first-order SA1 and remained above child level on SA2 (an exception among models) though its performance degraded with deviation level 2; on Imposing Memory it performed worst among instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225. Imposing Memory baseline: children (n=36, 9-10y) with intentionality showing a drop after level 2 (no single mean reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperformed many models on SA1 and uniquely remained above child level on SA2 (with some degradation under deviation), but underperformed on the Imposing Memory task compared to GPT-4 and sometimes to children on recursive intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Good at SA relative to peers but failed to generalize robustly to heavily deviated second-order contexts and struggled on recursive intentionality tasks (IM); demonstrates divergence across task types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5491.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM 2 family model (large instruct-LLM, size estimated 175-340B) evaluated on ToM tasks; included both PaLM2 and PaLM2-chat variants in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruct-LLM (PaLM2) with estimated parameter range 175-340B per paper; used in instruction/Q&A format for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Estimated 175-340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>SA1 and SA2 with deviations; instructed to respond concisely and accurately in QA format.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2 performed well above child level on SA1 for first-order ToM but degraded on SA2; PaLM2-chat in particular showed perturbation on second-order items (PaLM2-chat was noted as being perturbed by second-order ToM).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2 and variants outperformed many base models on first-order ToM but did not consistently exceed child performance on second-order ToM; PaLM2-chat showed more sensitivity to second-order items and deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Large instruct models still showed degradation on second-order ToM and on deviation 2, suggesting limits to recursive ToM generalization despite instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5491.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 (chat variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chat-optimized instruct-tuned variant of PaLM2 evaluated in the study; included to compare conversation-optimized models on ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-optimized instruct-LLM variant of PaLM2 (estimated 175-340B); evaluated in QA prompt format mirroring child testing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Estimated 175-340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / social cognition (ToM-related)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven Strange Stories items probing non-literal language understanding (irony, white lie, sarcasm, pretend, double bluff, etc.); experimental question plus motivation scored (0-3 per item aggregated). Deviations applied.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2-chat matched or surpassed child level earlier than PaLM2 on Strange Stories and maintained strong performance across items; deviation levels had little effect on large instruct-LLMs including PaLM2-chat.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): performance declines as items increase in complexity (no single mean reported); dashed lines in figures indicate child level used as comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2-chat generally matched or exceeded child performance on Strange Stories across items and was robust to deviations; outperformed many base-LLMs on non-literal language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Although strong on Strange Stories, PaLM2-chat was noted to be more sensitive to second-order Sally-Anne items; strengths appear aligned with training data rich in non-literal language and with instruction/chat tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5491.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A >340-billion-parameter instruction-tuned GPT-family model from OpenAI; the top-performing model in this study across ToM-related tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned transformer (OpenAI GPT-4), listed as >340B parameters in the table; prompted in question-answering format like the children's tests, with deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B (paper's estimate)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief tasks (original + deviations); motivation elicited and rated as 0-2 per item.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 outperformed other models and often outperformed children: near-perfect on Strange Stories, remained above child level on SA1 and comparatively resilient on SA2 for some deviations, but performance decreased on the most deviated SA2 items; excelled on Imposing Memory, being the only model to pass well on that challenging test.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children: SA group (n=37, 7-8y) SA1 mean = 0.45, SA2 mean = 0.225; IM group (n=36, 9-10y) showed intentionality drop after level 2 and memory questions >0.85 across levels (except a dip at level 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 often surpassed child performance across tests (notably SS and IM) and outperformed most other LLMs; however, even GPT-4's performance degraded with large deviations on second-order SA (deviation 2) indicating limits to generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Strongest model but sensitive to deviations in some recursive scenarios; able to handle non-literal language and higher-order intentionality better than others but still not a claim of human-like ToM; results time-bound to Spring 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e5491.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sally-Anne Test (SA1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sally-Anne first-order false-belief test (SA1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized first-order false-belief narrative test (Sally-Anne) assessing understanding that others can hold beliefs about the world that differ from reality; used here on LLMs and children.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (see individual model entries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>NA (this entry represents the cognitive test rather than a single model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne first-order (SA1)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief (social reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Narrative where Sally leaves a ball in her box and Anne moves it; participant asked 'Where will Sally look for the ball?' Correct = 'in her box'. Follow-up 'Why?' for motivation. Administered in original and two rewritten deviations to probe robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Varied by model: many instruct-LLMs (GPT-4, GPT-3.5, PaLM2 variants) performed well above child level; some large base-LLMs (BLOOM, GPT-davinci, LLaMA-30B) also performed above child level on SA1; smaller models and several base-LLMs underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA1 mean = 0.45 (averaged score between 0 and 1 as per paper scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Several large instruct-LLMs and some large base-LLMs outperformed the child baseline on SA1; smaller models and many base models underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Performance sensitive to prompt format and deviations; SA1 easier for LLMs than second-order SA2; addition of motivation question makes the task more demanding for both children and models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e5491.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sally-Anne Test (SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sally-Anne second-order false-belief test (SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized second-order false-belief narrative (Perner & Wimmer) requiring judgment about one character's belief about another character's belief; used on LLMs and children.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (see individual model entries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>NA (test entry).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne second-order (SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / recursive belief reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Narrative assessing what Sally thinks Anne believes about location of ice-cream truck; requires reasoning about beliefs about beliefs; followed by motivation question. Administered with deviations 0-2.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most models (both base and instruct) struggled with SA2; many fell at or below child level. Exceptions: GPT-4 and GPT-3.5 remained above child level on SA2 in some conditions but degraded substantially under deviation 2. Some large base models already declined on deviation 0.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Majority of LLMs underperformed relative to child baseline on SA2; a few large instruct-LLMs matched or exceeded child performance in limited conditions but were sensitive to deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Second-order recursive ToM is a clear weakness for many LLMs; deviation from canonical formulations further reduces performance, indicating limited generalization for recursive belief reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e5491.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strange Stories test (Happé, 1994) - seven non-literal language vignettes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A battery of seven social vignettes probing non-literal language (white lies, sarcasm, pretend-play, double bluff, etc.) that require inferring intentions and mental states; used here for LLMs and children.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (see individual model entries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>NA (test entry).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / social cognition (ToM-related)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven items each depicting a social situation with potential for misinterpretation (lie, white lie, sarcasm, double bluff, etc.). Experimental question (e.g., 'Is what the girl says true?') plus motivation question; items increase in difficulty. Deviation variants applied.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Large instruct-LLMs (GPT-4, GPT-3.5, PaLM2 variants) generally matched or exceeded child performance; GPT-4 approached near-perfect scores. Some smaller instruct-LLMs (FLAN-T5) outperformed larger PaLM variants on harder items. Base-LLMs generally performed below child level except on the most complex items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): performance declines as items increase in complexity; no single aggregate mean reported, but used as dashed-line baseline in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Many large instruct-LLMs matched or surpassed child baselines on Strange Stories and were robust to deviations, whereas base-LLMs generally underperformed relative to children.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Performance on non-literal language generalizes better across deviations for large instruct-LLMs; likely aided by training data rich in books and forums with irony and sarcasm; base-LLMs may possess knowledge but fail to surface it due to prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e5491.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imposing Memory test (Kinderman et al. adaptations) - recursive intentionality and memory questions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A revised Imposing Memory test probing recursive intentionality (multiple embedded mental states) and matched memory questions; used here (adapted for age 7-10) to test LLMs and older children (9-10y).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs (see individual model entries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>NA (test entry).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality (Theory of Mind) and episodic memory (matched control)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Two stories followed by true/false questions: 10 'intentionality' questions (recursive mental-state reasoning up to level 5) and 12 'memory' questions (recall of story facts matched in complexity). Used to distinguish limits on recursive ToM vs memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most LLMs (both base and instruct) performed below child level on IM; GPT-4 was a notable exception, performing consistently well and passing the task. Some large base models (BLOOM, GPT-davinci) improved on higher recursion levels but still below children overall. FLAN-T5 increased performance with recursion and reached child level on later levels; GPT-3.5 performed worst among instruct-LLMs on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory questions remained high (>0.85 across levels except a dip at level 4); intentionality questions showed a significant drop after level two (difference between level 2 and 1: β = -0.222, p < .05), after which performance remained low.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Majority of LLMs underperformed relative to child baseline on intentionality questions (recursive ToM); GPT-4 outperformed and was above child level on IM, while most others were below children, especially for recursive intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>IM was the most challenging test; paper's IM adaptation had not been published previously (robustness test); LLMs' failures concentrated on intentionality (recursive reasoning) rather than memory, mirroring human patterns but at lower performance levels for most models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e5491.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (7B) - Strange Stories evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Falcon-7B base-LLM evaluated on Strange Stories; base-LLM prompt format used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base-LLM (7B) using text-completion prompting for Strange Stories items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / social cognition</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven vignettes probing non-literal language and intentions, with deviations included.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Falcon-7B (base) performed generally below child level on Strange Stories; little impact of deviations for most base-LLMs but overall underperformance relative to children and instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): show declining performance across items; exact aggregate not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline and large instruct-LLMs; did not match the strong Strange Stories performance of GPT-4 and several instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base prompt format may have limited expression of knowledge relevant to non-literal language despite possible exposure in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e5491.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B) - Strange Stories evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA-30B base model evaluated on Strange Stories; compared to child baseline and instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>30B base-LLM used with text-completion prompting for Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / social cognition</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven Strange Stories items with original and deviation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLaMA-30B performed below child level on Strange Stories generally, with little variation across deviations; larger base models showed limited ability to match instruct-LLMs on this test.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): decreasing performance with item difficulty; no single mean reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children and instruct-LLMs (notably GPT-4) on Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base-LLM prompting format and inability to follow instruction-style QA likely constrained performance on nuanced non-literal language items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e5491.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-davinci (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-davinci (176/175B) - Strange Stories evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large base GPT-davinci evaluated on Strange Stories; compared with child baseline and instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>175B-class base model used in text-completion prompt format for Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM-related</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven non-literal language vignettes with QA and motivation prompts (with deviations).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-davinci (base) performed below child level on Strange Stories in general, consistent with base-LLM trend; larger base models sometimes improved on complex items but still tended to lag behind instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): performance declines with item difficulty; used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to instruct-LLMs like GPT-4 and some PaLM variants on Strange Stories; some improvement on complex items but overall below children.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base completion-style prompting may have reduced performance relative to instruct-tuned versions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e5491.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOOM (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM (176B) - Strange Stories evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large open base model BLOOM evaluated on Strange Stories; showed below-child performance on most items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>176B base-LLM used in text-completion style prompts for Strange Stories items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM-related</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven vignettes probing intentions and non-literal language; deviations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>BLOOM performed below child level across Strange Stories items in the study, consistent with general base-LLM trends.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): baseline declines across items.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline and to instruct-LLMs (GPT-4, PaLM2 variants).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Despite being large, as a base model BLOOM may not manifest instruction-following behaviors needed for QA-format Strange Stories evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e5491.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B-I (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (7B) - Strange Stories evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned small model Falcon-7B-I evaluated on Strange Stories; performed poorly among instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 7B Falcon variant used in QA prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM-related</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven items probing non-literal language and intentions, with deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Falcon-7B-I performed overall worst among instruct-LLMs on Strange Stories and failed to match child baseline on most items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): baseline declines across items.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children and to larger instruct-LLMs such as GPT-4 and GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Small parameter count limits despite instruction-tuning; instruction-tuning alone insufficient to reach child-level performance on this battery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e5491.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5 (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11B) - Strange Stories evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 11B instruction-tuned model that performed relatively well on Strange Stories, sometimes exceeding larger models on harder items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>11B instruction-tuned model (Flan-T5) evaluated in QA prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / social cognition</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven-item Strange Stories battery with deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FLAN-T5 surpassed child level earlier than some models and outperformed larger PaLM variants on more difficult items; generally robust to deviations for Strange Stories relative to size.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): declining performance with item difficulty used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Matched or exceeded child performance on several Strange Stories items, outperforming some larger instruct-LLMs on harder items.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Unusually strong Strange Stories performance relative to model size; contrast with poor SA2/IM performance indicates task-specific strengths in non-literal language.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.21">
                <h3 class="extraction-instance">Extracted Data Instance 21 (e5491.21)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003 (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (GPT-3 instruct) - Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The instruction-tuned GPT-3 variant evaluated on Strange Stories; performed at or above child level on several items but with decline across item difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 175B-class GPT-3 variant used in QA prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM-related</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven social vignettes probing intentions and non-literal language with motivations elicited.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 (text-davinci-003) performed at or close to child level on item 1, with performance somewhat declining on later items but staying well above child level overall on many items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y): baseline declines across items.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Generally performed at or above child baseline on several Strange Stories items but trailed GPT-4 on aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Performance depends on prompt style; the paper notes GPT-3 performed differently in other studies depending on text-completion vs QA prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.22">
                <h3 class="extraction-instance">Extracted Data Instance 22 (e5491.22)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo - Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 instruction-tuned model evaluated on Strange Stories; performed well overall, often above child level but below GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-family model (approx. 175B) used in QA prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM-related</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven Strange Stories scenarios with motivation questions; deviations included.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 performed at or close to child level on item 1 and then somewhat declined while staying well above child level overall on many items; robust to deviations for larger instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Generally matched or exceeded child performance on many Strange Stories items but was outperformed by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Stable across deviations for Strange Stories but less stable on recursive SA and IM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.23">
                <h3 class="extraction-instance">Extracted Data Instance 23 (e5491.23)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2 (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM2 - Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM2 evaluated on Strange Stories; performed well, though PaLM2-chat sometimes surpassed it on earlier items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruct-LLM (estimated 175-340B) used in QA prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Estimated 175-340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM-related</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven non-literal language vignettes probing intentions and sarcasm etc.; deviations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2 performed well on Strange Stories, though PaLM2-chat and FLAN-T5 sometimes surpassed it on earlier/harder items; large models were largely robust to deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Matched or exceeded child baseline on many items but generally not as strong as GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Robust to deviations but relative performance varied compared to PaLM2-chat and FLAN-T5 depending on item difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.24">
                <h3 class="extraction-instance">Extracted Data Instance 24 (e5491.24)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 - Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 achieved near-perfect performance on Strange Stories, robust to deviations and outperforming children and most other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned model (>340B estimate) evaluated in QA prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM-related</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven-item Strange Stories battery with motivation follow-ups and deviation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Approached perfect scores throughout Strange Stories; robust across deviation levels and consistently above child performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=37, 7-8y) whose performance declines with item difficulty; used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 surpassed child baseline and outperformed nearly all other LLMs on non-literal language items.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Strongest across this battery, but exceptional Strange Stories performance does not imply human-equivalent ToM across other domains (e.g., recursive SA2/IM challenges remain for some deviations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.25">
                <h3 class="extraction-instance">Extracted Data Instance 25 (e5491.25)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (7B) - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small base model evaluated on IM; performed below child baseline on both intentionality and memory questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B base-LLM (text-completion prompts) evaluated on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality (ToM) and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Stories followed by true/false intentionality and memory questions across five levels of recursion.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Falcon-7B performed below child level on both intentionality and memory questions, consistent with base-LLM group trend on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory > 0.85 across levels (except dip at level 4); intentionality drops after level 2 (statistically significant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline on IM, particularly on recursive intentionality questions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Prompt length and context window constraints may have disproportionately affected base-LLMs on the IM test (long scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.26">
                <h3 class="extraction-instance">Extracted Data Instance 26 (e5491.26)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B) - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>30B base LLaMA evaluated on IM; performed below child baseline though larger base models showed some improvement on higher recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>30B base-LLM evaluated on IM using text-completion prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Stories plus true/false questions probing multiple nested levels of intentionality and matched memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLaMA-30B performed below child level overall on IM; larger base models (including LLaMA-30B) showed some gains at higher recursion levels but did not reach child-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory > 0.85; intentionality declines after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children on intentionality; partial improvement at higher recursion levels but insufficient to match children.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base prompting and context-length sensitivity may limit IM performance for base-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.27">
                <h3 class="extraction-instance">Extracted Data Instance 27 (e5491.27)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-davinci (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-davinci (175B) - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large base GPT-davinci evaluated on IM; improved on higher recursion relative to smaller bases but still below child baseline overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>175B base model used with text-completion style prompts for IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>IM with five recursion levels for intentionality and matched memory questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-davinci showed somewhat better performance on higher recursion levels compared to smaller base models, but remained below child level overall on intentionality questions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory >0.85; intentionality shows drop after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children on recursive intentionality; better than smaller base models but not reaching instructional models like GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base completion prompt style and long prompt lengths likely constrained performance on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.28">
                <h3 class="extraction-instance">Extracted Data Instance 28 (e5491.28)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOOM (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM (176B) - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large base model evaluated on IM; performed below child baseline though some improvement at higher recursion levels noted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>176B base-LLM used in text-completion prompts for IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>IM with intentionality and memory true/false questions across recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>BLOOM performed below child level overall on IM; larger base models including BLOOM showed improvement on higher recursion levels but remained inferior to children and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): baseline as above (memory high, intentionality drop after level 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline on intentionality; partial gains at higher recursion levels did not match children's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>As with other base models, format and prompt length may have impacted results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.29">
                <h3 class="extraction-instance">Extracted Data Instance 29 (e5491.29)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B-I (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (7B) - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small instruct model evaluated on IM; performed poorly among instruct-LLMs and below child baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 7B Falcon variant evaluated on IM; QA prompting used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>IM with recursive intentionality and matched memory questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Falcon-7B-I performed below child level on IM; among worst instruct-LLMs on this challenging test.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory >0.85; intentionality drops after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children and to larger instruct-LLMs (notably GPT-4) on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Small parameter count limited ability to handle long context and recursive reasoning required by IM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.30">
                <h3 class="extraction-instance">Extracted Data Instance 30 (e5491.30)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5 (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11B) - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>11B instruction-tuned FLAN-T5 evaluated on IM; showed an unusual increase in performance with recursion and reached child-level by the final levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>11B instruction-tuned LLM evaluated in QA prompt format on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Stories plus intentionality and memory true/false questions across recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FLAN-T5 increased performance as recursion levels went up and ended at child level on IM, a notable exception among instruct-LLMs; overall performance trend was upward across recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory high (>0.85) and intentionality drop after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reached child-level performance at higher recursion levels for IM, contrasting with most other LLMs which stayed below child baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Small model with atypical upward trend in IM performance; reasons for this pattern unclear and warrant further investigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.31">
                <h3 class="extraction-instance">Extracted Data Instance 31 (e5491.31)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003 (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (GPT-3 instruct) - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant evaluated on IM; performed worse than top instruct-LLMs and below child baseline on recursive intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 175B-class GPT-3 variant used in QA prompt format on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>IM with intentionality and memory true/false questions across recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 (text-davinci-003) performed below child baseline on IM, with substantial weaknesses on recursive intentionality questions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory >0.85; intentionality drops after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children and to GPT-4 on IM; not among top instruct-LLMs for recursive ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Demonstrates limitations of earlier instruct-tuned models on deeply recursive ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.32">
                <h3 class="extraction-instance">Extracted Data Instance 32 (e5491.32)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 evaluated on IM and found to perform worst among instruct-LLMs on this task, underperforming relative to children and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 model used in QA prompt format for IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>IM with intentionality and memory questions across recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 performed worst of the instruct-LLMs on IM and was below child performance on recursive intentionality questions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory >0.85; intentionality declines after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline and significantly worse than GPT-4 on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Highlights heterogeneity among instruct-LLMs: not all instruction-tuned models handle recursive ToM well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.33">
                <h3 class="extraction-instance">Extracted Data Instance 33 (e5491.33)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2 (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large PaLM2 evaluated on IM; performed below child baseline on intentionality, though larger instruct models generally fared better than base models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned model (estimated 175-340B) evaluated on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Estimated 175-340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>IM test with recursive intentionality and matched memory questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2 performed below child baseline on IM intentionality questions; larger instruction-tuned variants tended to do better than base models but not as well as GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory >0.85; intentionality drops after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children on intentionality; better than many base-LLMs but inferior to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Instruction tuning helps but is not sufficient to match top-performing GPT-family models on recursive ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.34">
                <h3 class="extraction-instance">Extracted Data Instance 34 (e5491.34)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2-chat (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 chat variant - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chat-optimized PaLM2 variant evaluated on IM; showed perturbation on second-order tasks and did not match GPT-4 on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-optimized instruct-LLM variant of PaLM2 used in QA format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Estimated 175-340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>IM battery with intentionality and memory questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2-chat showed perturbation on second-order ToM and performed below child baseline on IM intentionality; did not reach GPT-4-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): memory >0.85; intentionality drop after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to children on recursive intentionality and below GPT-4 on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Chat optimization did not guarantee robust recursive ToM abilities on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5491.35">
                <h3 class="extraction-instance">Extracted Data Instance 35 (e5491.35)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 - Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was the only model that passed the Imposing Memory test well, performing consistently and staying above child level after second-order intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned transformer (>340B estimate) evaluated in QA format on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Stories with intentionality and memory true/false questions across recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 performed consistently well on IM and was the only model to 'pass' this challenging robustness test, staying above child level after second-order intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (n=36, 9-10y): intentionality drop after level 2; memory >0.85 across most levels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 surpassed child baseline on IM where most other LLMs did not; uniquely strong on recursive intentionality and memory tasks among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>GPT-4's superior IM performance highlights model heterogeneity; nevertheless, authors caution against equating model success with human-like ToM given grounding differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>Neural theory-of-mind? on the limits of social intelligence in large LMs <em>(Rating: 2)</em></li>
                <li>Clever Hans or neural theory of mind? stress testing social reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4 <em>(Rating: 1)</em></li>
                <li>Do conversational agents have a theory of mind? a single case study of chatgpt with the hinting, false beliefs and false photographs, and strange stories paradigms <em>(Rating: 1)</em></li>
                <li>MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5491",
    "paper_id": "paper-86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "Falcon-7B",
            "name_full": "Falcon (7B)",
            "brief_description": "A 7-billion parameter base large language model (base-LLM) evaluated in this study; trained via self-supervised language modeling (described as a base-LLM in the paper).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-7B",
            "model_description": "Base-LLM (text-completion style); described in the paper as a 7B parameter model from Penedo et al. (2023), trained via self-supervision (base-LLM paradigm).",
            "model_size": "7B",
            "cognitive_test_name": "Sally-Anne (first-order and second-order)",
            "cognitive_test_type": "Theory of Mind / false-belief (social reasoning)",
            "cognitive_test_description": "Classic first- and second-order false-belief narrative tests (SA1: where will Sally look for the ball?; SA2: second-order belief about what Sally thinks Anne believes). Administered in original and two rewritten deviations; scored on experimental answer (correct/incorrect) and motivation (0-2).",
            "llm_performance": "Falcon-7B performed poorly; classified among smaller models that tended to perform worse — struggled on first-order less than some larger base models and failed on almost all second-order questions (performance at or below child level; dropped substantially on deviation levels).",
            "human_baseline_performance": "Children (n=37, age 7-8) used as baseline: SA1 mean = 0.45, SA2 mean = 0.225 (scores averaged and scaled to 0-1 as described in paper).",
            "performance_comparison": "Underperformed relative to child baseline on second-order ToM and generally below or at child level overall; performed worse than larger base models (BLOOM, Davinci, LLaMA-30B) on SA1 and notably worse on SA2.",
            "notable_differences_or_limitations": "Smaller model size and base (non-instruction) training; prompts were provided as text-completion (because base-LLMs do not follow instruction prompts well), which may have affected comparability; poor robustness to deviations and second-order items.",
            "uuid": "e5491.0",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA (30B)",
            "brief_description": "A 30-billion parameter base LLM (base-LLM) evaluated on ToM tasks in the paper; trained via self-supervised objectives as a foundation model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA (30B)",
            "model_description": "Base-LLM (30B), described in the paper as a 30B parameter model (Touvron et al., 2023); used with text-completion prompting consistent with base-LLM behavior.",
            "model_size": "30B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "SA1 first-order false-belief and SA2 second-order false-belief narratives, with original and two deviation variants; scoring as in Methods.",
            "llm_performance": "LLaMA-30B performed above child level on first-order ToM (SA1) — one of the base-LLMs that did well on SA1 — but fell at or below child level on second-order ToM (SA2), with performance decreasing for deviations.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.",
            "performance_comparison": "Matched or exceeded child performance on SA1 but underperformed relative to children on SA2; exhibited the common pattern of base-LLMs: reasonable on SA1, weak on SA2 and degraded with deviations.",
            "notable_differences_or_limitations": "As a base-LLM, prompted differently (text-completion) than instruct-LLMs; may have advantages/disadvantages from this format; struggled with generalization to deviated formulations and recursive ToM.",
            "uuid": "e5491.1",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-davinci",
            "name_full": "GPT-davinci (text-davinci-002/003 family; base/instruction variants as used)",
            "brief_description": "A 175-billion-parameter model from the GPT family included among base-LLMs in the study (table lists GPT-davinci as base-LLM, 175B).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-davinci",
            "model_description": "Listed as a 175B base-LLM in the paper's table (Brown et al. lineage); used with text-completion style prompts for base-LLM evaluation.",
            "model_size": "175B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief narratives with deviations; scored on answer and motivation.",
            "llm_performance": "GPT-davinci (as a large base-LLM) performed above child level on SA1 (first-order), but fell to at or below child level on SA2. It showed some improvement over smaller base models, but struggled with second-order recursion and deviations.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.",
            "performance_comparison": "Outperformed many base-LLMs on SA1 (above child level) but did not reliably exceed child performance on SA2; generalization to deviations was limited.",
            "notable_differences_or_limitations": "Prompting as base text-completion (rather than instruction-following) may have altered behavior; the paper notes GPT-3 (instruct vs. base variants) behaved differently depending on prompt style, suggesting format sensitivity.",
            "uuid": "e5491.2",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLOOM-176B",
            "name_full": "BLOOM (176B)",
            "brief_description": "An open 176-billion parameter base LLM evaluated in the study; a large base model that in some tasks outperformed smaller base models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BLOOM",
            "model_description": "Base-LLM listed at ~176B parameters (Scao et al., 2022); used with text-completion prompting consistent with other base models.",
            "model_size": "176B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "Classic first- and second-order false-belief tasks with deviations; scored on experimental answer and motivation.",
            "llm_performance": "BLOOM performed above child level on first-order SA1 (one of the larger base models that did well on SA1) but fell at or below child level on second-order SA2, with decreased performance under deviations.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.",
            "performance_comparison": "Matched or exceeded child performance on SA1 but underperformed on SA2 relative to children; similar pattern to other large base models (better on SA1, poor generalization on SA2).",
            "notable_differences_or_limitations": "As with other base-LLMs, prompts used completion format; the paper notes base-LLMs generally underperform versus instruct-LLMs and are sensitive to prompt formatting and prompt length.",
            "uuid": "e5491.3",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B-I",
            "name_full": "Falcon-instruct (7B)",
            "brief_description": "Instruction-tuned variant of Falcon (7B) evaluated as an instruct-LLM in the study; smaller instruction-tuned model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-instruct (Falcon-7B-I)",
            "model_description": "Instruction-tuned 7B model (the instruct variant of Falcon); described as an instruct-LLM in the paper's table.",
            "model_size": "7B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief tasks (SA1, SA2) with original and rewritten deviations; instruct-LLMs were prompted in question-answering format like children.",
            "llm_performance": "Falcon-7B-I struggled: the chat-optimized version failed on all second-order questions and was among the worst-performing instruct-LLMs on SA tasks, showing poor performance across deviations.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.",
            "performance_comparison": "Underperformed relative to children on SA2 and generally performed worse than larger instruct-LLMs; did not match child performance on second-order ToM.",
            "notable_differences_or_limitations": "Small model size despite instruction-tuning; instruction-tuning did not compensate for limited capacity on recursive ToM in this case.",
            "uuid": "e5491.4",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "FLAN-T5",
            "name_full": "FLAN-T5 (11B)",
            "brief_description": "An 11-billion parameter instruction-tuned model evaluated among instruct-LLMs; trained/finetuned for instruction-following tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (11B)",
            "model_description": "Instruction-tuned LLM (Flan-T5, 11B) used as an instruct-LLM; described in paper as performing variably across tasks.",
            "model_size": "11B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "SA1/SA2 narratives with deviations; instruct-LLMs prompted in QA format and motivations elicited by inserting model answers.",
            "llm_performance": "FLAN-T5, as a smaller instruct-LLM, tended to perform worse on SA tasks: it was among smaller models that performed worse overall and struggled especially on SA2 and with deviations (fell at or below child level on more complex/deviated SA2).",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.",
            "performance_comparison": "Underperformed relative to best instruct-LLMs (GPT-3.5, GPT-4) and generally at or below child level on second-order ToM; better on some Strange Stories items (see separate entry).",
            "notable_differences_or_limitations": "Smaller instruct model; showed some strengths on non-literal language tasks (Strange Stories) relative to base models, but limited recursion/robustness on SA.",
            "uuid": "e5491.5",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-003)",
            "name_full": "GPT-3 / text-davinci-003 (instruction-tuned variant)",
            "brief_description": "GPT-3 family instruct-tuned model (text-davinci-003) evaluated in the study; 175B parameter family with instruction tuning per Ouyang et al. (2022).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3 instruct variant)",
            "model_description": "Instruction-tuned GPT-3 family model (text-davinci-003), listed as an instruct-LLM in the paper; attributed to Ouyang et al. (2022) and 175B parameter scale referenced.",
            "model_size": "175B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief tests with deviations; instruct-LLMs were prompted in question-answering format similar to children.",
            "llm_performance": "GPT-3 (text-davinci-003) displayed structurally low scores on SA in this study when prompted with open questions (contrasting with Kosinski (2023) results that used text-completion prompts). It performed closer to child level on some Strange Stories items but was degraded on SA2 and deviations.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.",
            "performance_comparison": "Did not reliably exceed child performance on SA tasks in this prompting regime; noted as a striking exception (structurally low scores) compared to prior reports that used different prompting.",
            "notable_differences_or_limitations": "Prompt format sensitivity: the paper highlights that GPT-3's performance depended on whether it was prompted as text-completion (as in Kosinski) vs open QA (as used here); this complicates direct comparisons and suggests format-dependent capabilities.",
            "uuid": "e5491.6",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "A 175B-ish parameter instruction-tuned GPT-family model (GPT-3.5) evaluated across ToM tests; presented as an instruct-LLM that often performed near top-tier models except on some tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned GPT-family model (GPT-3.5), cited as 175B in the paper's table; used in question-answering prompt format for instruct-LLMs.",
            "model_size": "175B (approx.)",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "SA1 and SA2 narratives with deviations; motivations elicited by reinserting model answers into prompts.",
            "llm_performance": "GPT-3.5 performed well above child level on first-order SA1 and remained above child level on SA2 (an exception among models) though its performance degraded with deviation level 2; on Imposing Memory it performed worst among instruct-LLMs.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225. Imposing Memory baseline: children (n=36, 9-10y) with intentionality showing a drop after level 2 (no single mean reported).",
            "performance_comparison": "Outperformed many models on SA1 and uniquely remained above child level on SA2 (with some degradation under deviation), but underperformed on the Imposing Memory task compared to GPT-4 and sometimes to children on recursive intentionality.",
            "notable_differences_or_limitations": "Good at SA relative to peers but failed to generalize robustly to heavily deviated second-order contexts and struggled on recursive intentionality tasks (IM); demonstrates divergence across task types.",
            "uuid": "e5491.7",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2",
            "name_full": "PaLM 2",
            "brief_description": "PaLM 2 family model (large instruct-LLM, size estimated 175-340B) evaluated on ToM tasks; included both PaLM2 and PaLM2-chat variants in the study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM2",
            "model_description": "Large instruct-LLM (PaLM2) with estimated parameter range 175-340B per paper; used in instruction/Q&A format for evaluation.",
            "model_size": "Estimated 175-340B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "SA1 and SA2 with deviations; instructed to respond concisely and accurately in QA format.",
            "llm_performance": "PaLM2 performed well above child level on SA1 for first-order ToM but degraded on SA2; PaLM2-chat in particular showed perturbation on second-order items (PaLM2-chat was noted as being perturbed by second-order ToM).",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45; SA2 mean = 0.225.",
            "performance_comparison": "PaLM2 and variants outperformed many base models on first-order ToM but did not consistently exceed child performance on second-order ToM; PaLM2-chat showed more sensitivity to second-order items and deviations.",
            "notable_differences_or_limitations": "Large instruct models still showed degradation on second-order ToM and on deviation 2, suggesting limits to recursive ToM generalization despite instruction tuning.",
            "uuid": "e5491.8",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2-chat",
            "name_full": "PaLM 2 (chat variant)",
            "brief_description": "Chat-optimized instruct-tuned variant of PaLM2 evaluated in the study; included to compare conversation-optimized models on ToM tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM2-chat",
            "model_description": "Chat-optimized instruct-LLM variant of PaLM2 (estimated 175-340B); evaluated in QA prompt format mirroring child testing.",
            "model_size": "Estimated 175-340B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / social cognition (ToM-related)",
            "cognitive_test_description": "Seven Strange Stories items probing non-literal language understanding (irony, white lie, sarcasm, pretend, double bluff, etc.); experimental question plus motivation scored (0-3 per item aggregated). Deviations applied.",
            "llm_performance": "PaLM2-chat matched or surpassed child level earlier than PaLM2 on Strange Stories and maintained strong performance across items; deviation levels had little effect on large instruct-LLMs including PaLM2-chat.",
            "human_baseline_performance": "Children (n=37, 7-8y): performance declines as items increase in complexity (no single mean reported); dashed lines in figures indicate child level used as comparison.",
            "performance_comparison": "PaLM2-chat generally matched or exceeded child performance on Strange Stories across items and was robust to deviations; outperformed many base-LLMs on non-literal language understanding.",
            "notable_differences_or_limitations": "Although strong on Strange Stories, PaLM2-chat was noted to be more sensitive to second-order Sally-Anne items; strengths appear aligned with training data rich in non-literal language and with instruction/chat tuning.",
            "uuid": "e5491.9",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A &gt;340-billion-parameter instruction-tuned GPT-family model from OpenAI; the top-performing model in this study across ToM-related tests.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned transformer (OpenAI GPT-4), listed as &gt;340B parameters in the table; prompted in question-answering format like the children's tests, with deterministic decoding.",
            "model_size": "&gt;340B (paper's estimate)",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief tasks (original + deviations); motivation elicited and rated as 0-2 per item.",
            "llm_performance": "GPT-4 outperformed other models and often outperformed children: near-perfect on Strange Stories, remained above child level on SA1 and comparatively resilient on SA2 for some deviations, but performance decreased on the most deviated SA2 items; excelled on Imposing Memory, being the only model to pass well on that challenging test.",
            "human_baseline_performance": "Children: SA group (n=37, 7-8y) SA1 mean = 0.45, SA2 mean = 0.225; IM group (n=36, 9-10y) showed intentionality drop after level 2 and memory questions &gt;0.85 across levels (except a dip at level 4).",
            "performance_comparison": "GPT-4 often surpassed child performance across tests (notably SS and IM) and outperformed most other LLMs; however, even GPT-4's performance degraded with large deviations on second-order SA (deviation 2) indicating limits to generalization.",
            "notable_differences_or_limitations": "Strongest model but sensitive to deviations in some recursive scenarios; able to handle non-literal language and higher-order intentionality better than others but still not a claim of human-like ToM; results time-bound to Spring 2023.",
            "uuid": "e5491.10",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Sally-Anne Test (SA1)",
            "name_full": "Sally-Anne first-order false-belief test (SA1)",
            "brief_description": "A standardized first-order false-belief narrative test (Sally-Anne) assessing understanding that others can hold beliefs about the world that differ from reality; used here on LLMs and children.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (see individual model entries)",
            "model_description": "NA (this entry represents the cognitive test rather than a single model).",
            "model_size": null,
            "cognitive_test_name": "Sally-Anne first-order (SA1)",
            "cognitive_test_type": "Theory of Mind / false-belief (social reasoning)",
            "cognitive_test_description": "Narrative where Sally leaves a ball in her box and Anne moves it; participant asked 'Where will Sally look for the ball?' Correct = 'in her box'. Follow-up 'Why?' for motivation. Administered in original and two rewritten deviations to probe robustness.",
            "llm_performance": "Varied by model: many instruct-LLMs (GPT-4, GPT-3.5, PaLM2 variants) performed well above child level; some large base-LLMs (BLOOM, GPT-davinci, LLaMA-30B) also performed above child level on SA1; smaller models and several base-LLMs underperformed.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA1 mean = 0.45 (averaged score between 0 and 1 as per paper scoring).",
            "performance_comparison": "Several large instruct-LLMs and some large base-LLMs outperformed the child baseline on SA1; smaller models and many base models underperformed.",
            "notable_differences_or_limitations": "Performance sensitive to prompt format and deviations; SA1 easier for LLMs than second-order SA2; addition of motivation question makes the task more demanding for both children and models.",
            "uuid": "e5491.11",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Sally-Anne Test (SA2)",
            "name_full": "Sally-Anne second-order false-belief test (SA2)",
            "brief_description": "A standardized second-order false-belief narrative (Perner & Wimmer) requiring judgment about one character's belief about another character's belief; used on LLMs and children.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (see individual model entries)",
            "model_description": "NA (test entry).",
            "model_size": null,
            "cognitive_test_name": "Sally-Anne second-order (SA2)",
            "cognitive_test_type": "Theory of Mind / recursive belief reasoning",
            "cognitive_test_description": "Narrative assessing what Sally thinks Anne believes about location of ice-cream truck; requires reasoning about beliefs about beliefs; followed by motivation question. Administered with deviations 0-2.",
            "llm_performance": "Most models (both base and instruct) struggled with SA2; many fell at or below child level. Exceptions: GPT-4 and GPT-3.5 remained above child level on SA2 in some conditions but degraded substantially under deviation 2. Some large base models already declined on deviation 0.",
            "human_baseline_performance": "Children (n=37, 7-8y): SA2 mean = 0.225.",
            "performance_comparison": "Majority of LLMs underperformed relative to child baseline on SA2; a few large instruct-LLMs matched or exceeded child performance in limited conditions but were sensitive to deviations.",
            "notable_differences_or_limitations": "Second-order recursive ToM is a clear weakness for many LLMs; deviation from canonical formulations further reduces performance, indicating limited generalization for recursive belief reasoning.",
            "uuid": "e5491.12",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Strange Stories",
            "name_full": "Strange Stories test (Happé, 1994) - seven non-literal language vignettes",
            "brief_description": "A battery of seven social vignettes probing non-literal language (white lies, sarcasm, pretend-play, double bluff, etc.) that require inferring intentions and mental states; used here for LLMs and children.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (see individual model entries)",
            "model_description": "NA (test entry).",
            "model_size": null,
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / social cognition (ToM-related)",
            "cognitive_test_description": "Seven items each depicting a social situation with potential for misinterpretation (lie, white lie, sarcasm, double bluff, etc.). Experimental question (e.g., 'Is what the girl says true?') plus motivation question; items increase in difficulty. Deviation variants applied.",
            "llm_performance": "Large instruct-LLMs (GPT-4, GPT-3.5, PaLM2 variants) generally matched or exceeded child performance; GPT-4 approached near-perfect scores. Some smaller instruct-LLMs (FLAN-T5) outperformed larger PaLM variants on harder items. Base-LLMs generally performed below child level except on the most complex items.",
            "human_baseline_performance": "Children (n=37, 7-8y): performance declines as items increase in complexity; no single aggregate mean reported, but used as dashed-line baseline in figures.",
            "performance_comparison": "Many large instruct-LLMs matched or surpassed child baselines on Strange Stories and were robust to deviations, whereas base-LLMs generally underperformed relative to children.",
            "notable_differences_or_limitations": "Performance on non-literal language generalizes better across deviations for large instruct-LLMs; likely aided by training data rich in books and forums with irony and sarcasm; base-LLMs may possess knowledge but fail to surface it due to prompt format.",
            "uuid": "e5491.13",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Imposing Memory (IM)",
            "name_full": "Imposing Memory test (Kinderman et al. adaptations) - recursive intentionality and memory questions",
            "brief_description": "A revised Imposing Memory test probing recursive intentionality (multiple embedded mental states) and matched memory questions; used here (adapted for age 7-10) to test LLMs and older children (9-10y).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple LLMs (see individual model entries)",
            "model_description": "NA (test entry).",
            "model_size": null,
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality (Theory of Mind) and episodic memory (matched control)",
            "cognitive_test_description": "Two stories followed by true/false questions: 10 'intentionality' questions (recursive mental-state reasoning up to level 5) and 12 'memory' questions (recall of story facts matched in complexity). Used to distinguish limits on recursive ToM vs memory.",
            "llm_performance": "Most LLMs (both base and instruct) performed below child level on IM; GPT-4 was a notable exception, performing consistently well and passing the task. Some large base models (BLOOM, GPT-davinci) improved on higher recursion levels but still below children overall. FLAN-T5 increased performance with recursion and reached child level on later levels; GPT-3.5 performed worst among instruct-LLMs on IM.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory questions remained high (&gt;0.85 across levels except a dip at level 4); intentionality questions showed a significant drop after level two (difference between level 2 and 1: β = -0.222, p &lt; .05), after which performance remained low.",
            "performance_comparison": "Majority of LLMs underperformed relative to child baseline on intentionality questions (recursive ToM); GPT-4 outperformed and was above child level on IM, while most others were below children, especially for recursive intentionality.",
            "notable_differences_or_limitations": "IM was the most challenging test; paper's IM adaptation had not been published previously (robustness test); LLMs' failures concentrated on intentionality (recursive reasoning) rather than memory, mirroring human patterns but at lower performance levels for most models.",
            "uuid": "e5491.14",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B (SS)",
            "name_full": "Falcon (7B) - Strange Stories evaluation",
            "brief_description": "Falcon-7B base-LLM evaluated on Strange Stories; base-LLM prompt format used.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-7B",
            "model_description": "Base-LLM (7B) using text-completion prompting for Strange Stories items.",
            "model_size": "7B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / social cognition",
            "cognitive_test_description": "Seven vignettes probing non-literal language and intentions, with deviations included.",
            "llm_performance": "Falcon-7B (base) performed generally below child level on Strange Stories; little impact of deviations for most base-LLMs but overall underperformance relative to children and instruct-LLMs.",
            "human_baseline_performance": "Children (n=37, 7-8y): show declining performance across items; exact aggregate not reported.",
            "performance_comparison": "Underperformed relative to child baseline and large instruct-LLMs; did not match the strong Strange Stories performance of GPT-4 and several instruct-LLMs.",
            "notable_differences_or_limitations": "Base prompt format may have limited expression of knowledge relevant to non-literal language despite possible exposure in training data.",
            "uuid": "e5491.15",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B (SS)",
            "name_full": "LLaMA (30B) - Strange Stories evaluation",
            "brief_description": "LLaMA-30B base model evaluated on Strange Stories; compared to child baseline and instruct-LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B",
            "model_description": "30B base-LLM used with text-completion prompting for Strange Stories.",
            "model_size": "30B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / social cognition",
            "cognitive_test_description": "Seven Strange Stories items with original and deviation variants.",
            "llm_performance": "LLaMA-30B performed below child level on Strange Stories generally, with little variation across deviations; larger base models showed limited ability to match instruct-LLMs on this test.",
            "human_baseline_performance": "Children (n=37, 7-8y): decreasing performance with item difficulty; no single mean reported.",
            "performance_comparison": "Underperformed relative to children and instruct-LLMs (notably GPT-4) on Strange Stories.",
            "notable_differences_or_limitations": "Base-LLM prompting format and inability to follow instruction-style QA likely constrained performance on nuanced non-literal language items.",
            "uuid": "e5491.16",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-davinci (SS)",
            "name_full": "GPT-davinci (176/175B) - Strange Stories evaluation",
            "brief_description": "Large base GPT-davinci evaluated on Strange Stories; compared with child baseline and instruct-LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-davinci",
            "model_description": "175B-class base model used in text-completion prompt format for Strange Stories.",
            "model_size": "175B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM-related",
            "cognitive_test_description": "Seven non-literal language vignettes with QA and motivation prompts (with deviations).",
            "llm_performance": "GPT-davinci (base) performed below child level on Strange Stories in general, consistent with base-LLM trend; larger base models sometimes improved on complex items but still tended to lag behind instruct-LLMs.",
            "human_baseline_performance": "Children (n=37, 7-8y): performance declines with item difficulty; used as baseline.",
            "performance_comparison": "Underperformed relative to instruct-LLMs like GPT-4 and some PaLM variants on Strange Stories; some improvement on complex items but overall below children.",
            "notable_differences_or_limitations": "Base completion-style prompting may have reduced performance relative to instruct-tuned versions.",
            "uuid": "e5491.17",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLOOM (SS)",
            "name_full": "BLOOM (176B) - Strange Stories evaluation",
            "brief_description": "Large open base model BLOOM evaluated on Strange Stories; showed below-child performance on most items.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BLOOM",
            "model_description": "176B base-LLM used in text-completion style prompts for Strange Stories items.",
            "model_size": "176B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM-related",
            "cognitive_test_description": "Seven vignettes probing intentions and non-literal language; deviations tested.",
            "llm_performance": "BLOOM performed below child level across Strange Stories items in the study, consistent with general base-LLM trends.",
            "human_baseline_performance": "Children (n=37, 7-8y): baseline declines across items.",
            "performance_comparison": "Underperformed relative to child baseline and to instruct-LLMs (GPT-4, PaLM2 variants).",
            "notable_differences_or_limitations": "Despite being large, as a base model BLOOM may not manifest instruction-following behaviors needed for QA-format Strange Stories evaluation.",
            "uuid": "e5491.18",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B-I (SS)",
            "name_full": "Falcon-instruct (7B) - Strange Stories evaluation",
            "brief_description": "Instruction-tuned small model Falcon-7B-I evaluated on Strange Stories; performed poorly among instruct-LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-7B-I",
            "model_description": "Instruction-tuned 7B Falcon variant used in QA prompt format.",
            "model_size": "7B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM-related",
            "cognitive_test_description": "Seven items probing non-literal language and intentions, with deviations.",
            "llm_performance": "Falcon-7B-I performed overall worst among instruct-LLMs on Strange Stories and failed to match child baseline on most items.",
            "human_baseline_performance": "Children (n=37, 7-8y): baseline declines across items.",
            "performance_comparison": "Underperformed relative to children and to larger instruct-LLMs such as GPT-4 and GPT-3.5.",
            "notable_differences_or_limitations": "Small parameter count limits despite instruction-tuning; instruction-tuning alone insufficient to reach child-level performance on this battery.",
            "uuid": "e5491.19",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "FLAN-T5 (SS)",
            "name_full": "FLAN-T5 (11B) - Strange Stories evaluation",
            "brief_description": "An 11B instruction-tuned model that performed relatively well on Strange Stories, sometimes exceeding larger models on harder items.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "11B instruction-tuned model (Flan-T5) evaluated in QA prompt format.",
            "model_size": "11B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / social cognition",
            "cognitive_test_description": "Seven-item Strange Stories battery with deviations.",
            "llm_performance": "FLAN-T5 surpassed child level earlier than some models and outperformed larger PaLM variants on more difficult items; generally robust to deviations for Strange Stories relative to size.",
            "human_baseline_performance": "Children (n=37, 7-8y): declining performance with item difficulty used as baseline.",
            "performance_comparison": "Matched or exceeded child performance on several Strange Stories items, outperforming some larger instruct-LLMs on harder items.",
            "notable_differences_or_limitations": "Unusually strong Strange Stories performance relative to model size; contrast with poor SA2/IM performance indicates task-specific strengths in non-literal language.",
            "uuid": "e5491.20",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "text-davinci-003 (SS)",
            "name_full": "text-davinci-003 (GPT-3 instruct) - Strange Stories",
            "brief_description": "The instruction-tuned GPT-3 variant evaluated on Strange Stories; performed at or above child level on several items but with decline across item difficulty.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "Instruction-tuned 175B-class GPT-3 variant used in QA prompt format.",
            "model_size": "175B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM-related",
            "cognitive_test_description": "Seven social vignettes probing intentions and non-literal language with motivations elicited.",
            "llm_performance": "GPT-3 (text-davinci-003) performed at or close to child level on item 1, with performance somewhat declining on later items but staying well above child level overall on many items.",
            "human_baseline_performance": "Children (n=37, 7-8y): baseline declines across items.",
            "performance_comparison": "Generally performed at or above child baseline on several Strange Stories items but trailed GPT-4 on aggregate.",
            "notable_differences_or_limitations": "Performance depends on prompt style; the paper notes GPT-3 performed differently in other studies depending on text-completion vs QA prompting.",
            "uuid": "e5491.21",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 (SS)",
            "name_full": "GPT-3.5-turbo - Strange Stories",
            "brief_description": "GPT-3.5 instruction-tuned model evaluated on Strange Stories; performed well overall, often above child level but below GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned GPT-family model (approx. 175B) used in QA prompt format.",
            "model_size": "~175B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM-related",
            "cognitive_test_description": "Seven Strange Stories scenarios with motivation questions; deviations included.",
            "llm_performance": "GPT-3.5 performed at or close to child level on item 1 and then somewhat declined while staying well above child level overall on many items; robust to deviations for larger instruct-LLMs.",
            "human_baseline_performance": "Children (n=37, 7-8y).",
            "performance_comparison": "Generally matched or exceeded child performance on many Strange Stories items but was outperformed by GPT-4.",
            "notable_differences_or_limitations": "Stable across deviations for Strange Stories but less stable on recursive SA and IM tasks.",
            "uuid": "e5491.22",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2 (SS)",
            "name_full": "PaLM2 - Strange Stories",
            "brief_description": "PaLM2 evaluated on Strange Stories; performed well, though PaLM2-chat sometimes surpassed it on earlier items.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM2",
            "model_description": "Large instruct-LLM (estimated 175-340B) used in QA prompt format.",
            "model_size": "Estimated 175-340B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM-related",
            "cognitive_test_description": "Seven non-literal language vignettes probing intentions and sarcasm etc.; deviations tested.",
            "llm_performance": "PaLM2 performed well on Strange Stories, though PaLM2-chat and FLAN-T5 sometimes surpassed it on earlier/harder items; large models were largely robust to deviations.",
            "human_baseline_performance": "Children (n=37, 7-8y).",
            "performance_comparison": "Matched or exceeded child baseline on many items but generally not as strong as GPT-4.",
            "notable_differences_or_limitations": "Robust to deviations but relative performance varied compared to PaLM2-chat and FLAN-T5 depending on item difficulty.",
            "uuid": "e5491.23",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (SS)",
            "name_full": "GPT-4 - Strange Stories",
            "brief_description": "GPT-4 achieved near-perfect performance on Strange Stories, robust to deviations and outperforming children and most other models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned model (&gt;340B estimate) evaluated in QA prompt format.",
            "model_size": "&gt;340B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM-related",
            "cognitive_test_description": "Seven-item Strange Stories battery with motivation follow-ups and deviation variants.",
            "llm_performance": "Approached perfect scores throughout Strange Stories; robust across deviation levels and consistently above child performance.",
            "human_baseline_performance": "Children (n=37, 7-8y) whose performance declines with item difficulty; used as baseline.",
            "performance_comparison": "GPT-4 surpassed child baseline and outperformed nearly all other LLMs on non-literal language items.",
            "notable_differences_or_limitations": "Strongest across this battery, but exceptional Strange Stories performance does not imply human-equivalent ToM across other domains (e.g., recursive SA2/IM challenges remain for some deviations).",
            "uuid": "e5491.24",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B (IM)",
            "name_full": "Falcon (7B) - Imposing Memory",
            "brief_description": "Small base model evaluated on IM; performed below child baseline on both intentionality and memory questions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-7B",
            "model_description": "7B base-LLM (text-completion prompts) evaluated on IM.",
            "model_size": "7B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality (ToM) and memory",
            "cognitive_test_description": "Stories followed by true/false intentionality and memory questions across five levels of recursion.",
            "llm_performance": "Falcon-7B performed below child level on both intentionality and memory questions, consistent with base-LLM group trend on IM.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt; 0.85 across levels (except dip at level 4); intentionality drops after level 2 (statistically significant).",
            "performance_comparison": "Underperformed relative to child baseline on IM, particularly on recursive intentionality questions.",
            "notable_differences_or_limitations": "Prompt length and context window constraints may have disproportionately affected base-LLMs on the IM test (long scenarios).",
            "uuid": "e5491.25",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B (IM)",
            "name_full": "LLaMA (30B) - Imposing Memory",
            "brief_description": "30B base LLaMA evaluated on IM; performed below child baseline though larger base models showed some improvement on higher recursion levels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B",
            "model_description": "30B base-LLM evaluated on IM using text-completion prompts.",
            "model_size": "30B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "Stories plus true/false questions probing multiple nested levels of intentionality and matched memory.",
            "llm_performance": "LLaMA-30B performed below child level overall on IM; larger base models (including LLaMA-30B) showed some gains at higher recursion levels but did not reach child-level performance.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt; 0.85; intentionality declines after level 2.",
            "performance_comparison": "Underperformed relative to children on intentionality; partial improvement at higher recursion levels but insufficient to match children.",
            "notable_differences_or_limitations": "Base prompting and context-length sensitivity may limit IM performance for base-LLMs.",
            "uuid": "e5491.26",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-davinci (IM)",
            "name_full": "GPT-davinci (175B) - Imposing Memory",
            "brief_description": "Large base GPT-davinci evaluated on IM; improved on higher recursion relative to smaller bases but still below child baseline overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-davinci",
            "model_description": "175B base model used with text-completion style prompts for IM.",
            "model_size": "175B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "IM with five recursion levels for intentionality and matched memory questions.",
            "llm_performance": "GPT-davinci showed somewhat better performance on higher recursion levels compared to smaller base models, but remained below child level overall on intentionality questions.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt;0.85; intentionality shows drop after level 2.",
            "performance_comparison": "Underperformed relative to children on recursive intentionality; better than smaller base models but not reaching instructional models like GPT-4.",
            "notable_differences_or_limitations": "Base completion prompt style and long prompt lengths likely constrained performance on IM.",
            "uuid": "e5491.27",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLOOM (IM)",
            "name_full": "BLOOM (176B) - Imposing Memory",
            "brief_description": "Large base model evaluated on IM; performed below child baseline though some improvement at higher recursion levels noted.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BLOOM",
            "model_description": "176B base-LLM used in text-completion prompts for IM.",
            "model_size": "176B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "IM with intentionality and memory true/false questions across recursion levels.",
            "llm_performance": "BLOOM performed below child level overall on IM; larger base models including BLOOM showed improvement on higher recursion levels but remained inferior to children and GPT-4.",
            "human_baseline_performance": "Children (n=36, 9-10y): baseline as above (memory high, intentionality drop after level 2).",
            "performance_comparison": "Underperformed relative to child baseline on intentionality; partial gains at higher recursion levels did not match children's performance.",
            "notable_differences_or_limitations": "As with other base models, format and prompt length may have impacted results.",
            "uuid": "e5491.28",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B-I (IM)",
            "name_full": "Falcon-instruct (7B) - Imposing Memory",
            "brief_description": "Small instruct model evaluated on IM; performed poorly among instruct-LLMs and below child baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-7B-I",
            "model_description": "Instruction-tuned 7B Falcon variant evaluated on IM; QA prompting used.",
            "model_size": "7B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "IM with recursive intentionality and matched memory questions.",
            "llm_performance": "Falcon-7B-I performed below child level on IM; among worst instruct-LLMs on this challenging test.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt;0.85; intentionality drops after level 2.",
            "performance_comparison": "Underperformed relative to children and to larger instruct-LLMs (notably GPT-4) on IM.",
            "notable_differences_or_limitations": "Small parameter count limited ability to handle long context and recursive reasoning required by IM.",
            "uuid": "e5491.29",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "FLAN-T5 (IM)",
            "name_full": "FLAN-T5 (11B) - Imposing Memory",
            "brief_description": "11B instruction-tuned FLAN-T5 evaluated on IM; showed an unusual increase in performance with recursion and reached child-level by the final levels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "11B instruction-tuned LLM evaluated in QA prompt format on IM.",
            "model_size": "11B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "Stories plus intentionality and memory true/false questions across recursion levels.",
            "llm_performance": "FLAN-T5 increased performance as recursion levels went up and ended at child level on IM, a notable exception among instruct-LLMs; overall performance trend was upward across recursion levels.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory high (&gt;0.85) and intentionality drop after level 2.",
            "performance_comparison": "Reached child-level performance at higher recursion levels for IM, contrasting with most other LLMs which stayed below child baseline.",
            "notable_differences_or_limitations": "Small model with atypical upward trend in IM performance; reasons for this pattern unclear and warrant further investigation.",
            "uuid": "e5491.30",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "text-davinci-003 (IM)",
            "name_full": "text-davinci-003 (GPT-3 instruct) - Imposing Memory",
            "brief_description": "Instruction-tuned GPT-3 variant evaluated on IM; performed worse than top instruct-LLMs and below child baseline on recursive intentionality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "Instruction-tuned 175B-class GPT-3 variant used in QA prompt format on IM.",
            "model_size": "175B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "IM with intentionality and memory true/false questions across recursion levels.",
            "llm_performance": "GPT-3 (text-davinci-003) performed below child baseline on IM, with substantial weaknesses on recursive intentionality questions.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt;0.85; intentionality drops after level 2.",
            "performance_comparison": "Underperformed relative to children and to GPT-4 on IM; not among top instruct-LLMs for recursive ToM.",
            "notable_differences_or_limitations": "Demonstrates limitations of earlier instruct-tuned models on deeply recursive ToM tasks.",
            "uuid": "e5491.31",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 (IM)",
            "name_full": "GPT-3.5-turbo - Imposing Memory",
            "brief_description": "GPT-3.5 evaluated on IM and found to perform worst among instruct-LLMs on this task, underperforming relative to children and GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned GPT-3.5 model used in QA prompt format for IM.",
            "model_size": "~175B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "IM with intentionality and memory questions across recursion levels.",
            "llm_performance": "GPT-3.5 performed worst of the instruct-LLMs on IM and was below child performance on recursive intentionality questions.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt;0.85; intentionality declines after level 2.",
            "performance_comparison": "Underperformed relative to child baseline and significantly worse than GPT-4 on IM.",
            "notable_differences_or_limitations": "Highlights heterogeneity among instruct-LLMs: not all instruction-tuned models handle recursive ToM well.",
            "uuid": "e5491.32",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2 (IM)",
            "name_full": "PaLM 2 - Imposing Memory",
            "brief_description": "Large PaLM2 evaluated on IM; performed below child baseline on intentionality, though larger instruct models generally fared better than base models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM2",
            "model_description": "Large instruction-tuned model (estimated 175-340B) evaluated on IM.",
            "model_size": "Estimated 175-340B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "IM test with recursive intentionality and matched memory questions.",
            "llm_performance": "PaLM2 performed below child baseline on IM intentionality questions; larger instruction-tuned variants tended to do better than base models but not as well as GPT-4.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt;0.85; intentionality drops after level 2.",
            "performance_comparison": "Underperformed relative to children on intentionality; better than many base-LLMs but inferior to GPT-4.",
            "notable_differences_or_limitations": "Instruction tuning helps but is not sufficient to match top-performing GPT-family models on recursive ToM tasks.",
            "uuid": "e5491.33",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2-chat (IM)",
            "name_full": "PaLM 2 chat variant - Imposing Memory",
            "brief_description": "Chat-optimized PaLM2 variant evaluated on IM; showed perturbation on second-order tasks and did not match GPT-4 on IM.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM2-chat",
            "model_description": "Chat-optimized instruct-LLM variant of PaLM2 used in QA format.",
            "model_size": "Estimated 175-340B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "IM battery with intentionality and memory questions.",
            "llm_performance": "PaLM2-chat showed perturbation on second-order ToM and performed below child baseline on IM intentionality; did not reach GPT-4-level performance.",
            "human_baseline_performance": "Children (n=36, 9-10y): memory &gt;0.85; intentionality drop after level 2.",
            "performance_comparison": "Underperformed relative to children on recursive intentionality and below GPT-4 on IM.",
            "notable_differences_or_limitations": "Chat optimization did not guarantee robust recursive ToM abilities on IM.",
            "uuid": "e5491.34",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (IM)",
            "name_full": "GPT-4 - Imposing Memory",
            "brief_description": "GPT-4 was the only model that passed the Imposing Memory test well, performing consistently and staying above child level after second-order intentionality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned transformer (&gt;340B estimate) evaluated in QA format on IM.",
            "model_size": "&gt;340B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "Stories with intentionality and memory true/false questions across recursion levels.",
            "llm_performance": "GPT-4 performed consistently well on IM and was the only model to 'pass' this challenging robustness test, staying above child level after second-order intentionality.",
            "human_baseline_performance": "Children (n=36, 9-10y): intentionality drop after level 2; memory &gt;0.85 across most levels.",
            "performance_comparison": "GPT-4 surpassed child baseline on IM where most other LLMs did not; uniquely strong on recursive intentionality and memory tasks among evaluated models.",
            "notable_differences_or_limitations": "GPT-4's superior IM performance highlights model heterogeneity; nevertheless, authors caution against equating model success with human-like ToM given grounding differences.",
            "uuid": "e5491.35",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2
        },
        {
            "paper_title": "Neural theory-of-mind? on the limits of social intelligence in large LMs",
            "rating": 2
        },
        {
            "paper_title": "Clever Hans or neural theory of mind? stress testing social reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "rating": 1
        },
        {
            "paper_title": "Do conversational agents have a theory of mind? a single case study of chatgpt with the hinting, false beliefs and false photographs, and strange stories paradigms",
            "rating": 1
        },
        {
            "paper_title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic",
            "rating": 2
        }
    ],
    "cost": 0.0357475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</h1>
<p>Max van Duijn ${ }^{1 <em>}$, Bram van Dijk ${ }^{1 </em>}$, Tom Kouwenhoven ${ }^{1 *}$, Werner de Valk ${ }^{1}$, Marco Spruit ${ }^{1,2}$, and Peter van der Putten ${ }^{1}$<br>${ }^{1}$ Leiden Institute of Advanced Computer Science<br>${ }^{2}$ Leiden University Medical Centre<br>Corresponding author: m.j.van.duijn@liacs.leidenuniv.nl</p>
<h4>Abstract</h4>
<p>To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including nonliteral language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instructiontuned LLMs from the GPT family outperform other models, and often also children. BaseLLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.</p>
<h2>1 Introduction</h2>
<p>Machines that can think like us have always triggered our imagination. Contemplation of such machines can be traced as far back as antiquity (Liveley and Thomas, 2020), and peaked with the advent of all kinds of 'automata' in the early days of the Industrial Revolution (Voskuhl, 2013) before settling in computer science from the 1950s (Turing, 1950). Currently people around the world can interact with powerful chatbots driven by Large Language Models (LLMs), such as OpenAI's ChatGPT (OpenAI, 2023), and wonder to what degree such systems are capable of thought.</p>
<p>LLMs are large-scale deep neural networks, trained on massive amounts of text from the web.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>They are vastly complex systems: even if all details about their architecture, training data, and optional fine-tuning procedures are known (which is currently not the case for the most competitive models), it is very difficult to oversee their capabilities and predict how they will perform on a variety of tasks. Researchers from linguistics (Manning et al., 2020), psychology (Binz and Schulz, 2023b; Kosinski, 2023; Webb et al., 2023), psychiatry (Kjell et al., 2023), epistemology (Sileo and Lernould, 2023), logic (Creswell et al., 2022), and other fields, have therefore started to study LLMs as new, 'alien' entities, with their own sort of intelligence, that needs to be probed with experiments, an endeavour recently described as 'machine psychology' (Hagendorff, 2023). This not only yields knowledge about what LLMs are capable of, but also provides a unique opportunity to shed new light on questions surrounding our own intelligence (Dillion et al., 2023; Binz and Schulz, 2023a).</p>
<p>Here we focus on attempts to determine to what degree LLMs demonstrate a capacity for Theory of Mind (ToM), defined as the ability to work with beliefs, intentions, desires, and other mental states, to anticipate and explain behaviour in social settings (Apperly, 2010). We first address the question how LLMs perform on standardized, language-based tasks used to assess ToM capabilities in humans. We extend existing work in this area, surveyed in Section 2, in four ways: by (i) testing 11 models (see Table 1) for a broader suite of capabilities relevant to ToM beyond just the dominant falsebelief paradigm, including non-literal language understanding and recursive intentionality (A wants B to believe that C intends...); (ii) using newly written versions of standardized tests with varying degrees of deviation from the originals; (iii) including open questions besides closed ones; and (iv) benchmarking LLM performance against that of children aged 7-8 ( $\mathrm{n}=37$ ) and 9-10 ( $\mathrm{n}=36$ ) on the same tasks. Section 3 contains details of our</p>
<p>test procedures for both children and LLMs. After reporting the results in Section 4, we turn to the question how variation in performance of the LLMs we tested can be explained in Section 5. We conclude by placing our findings in the broader context of strong links between language and ToM in human development and evolution, and tentatively interpret what it means for an LLM to pass (or fail) ToM tests.</p>
<p>We are aware of issues regarding LLM training and deployment, for example regarding the biases they inherit (Lucy and Bamman, 2021; Bender et al., 2021), problems for educators (Sparrow, 2022), and ethical concerns in obtaining human feedback (Perrigo, 2023). Ongoing reflection on the use of LLMs is necessary, but outside the scope of this paper.</p>
<h2>2 Background</h2>
<h3>2.1 Large Language Models</h3>
<p>The field of Natural Language Processing (NLP) has been revolutionized by the advent of Transformer models (Vaswani et al., 2017; Devlin et al., 2019), deep neural networks that can induce language structures through self-supervised learning. During training, such models iteratively predict masked words from context in large sets of natural language data. They improve at this task by building representations of the many morphological, lexical, and syntactic rules governing human language production and understanding (Manning et al., 2020; Rogers et al., 2021; Grand et al., 2022). Models exclusively trained through such self-supervision constitute what we refer to as 'base-LLMs' in this paper.</p>
<p>Base-LLMs can generate natural language when prompted with completion queries ('A mouse is an ...'). They can also be leveraged successfully for an array of other challenges, such as questionanswering and translation, which often requires task-specific fine-tuning or prompting with specific examples, known as few-shot-learning (Brown et al., 2020). This makes them different from a new generation of LLMs that we refer to as 'instruct-LLMs' in this paper, and to which the currently most competitive models belong. In instruction-tuning, various forms of human feedback are collected, such as ranking most suitable responses, which then forms the reward-signal for further aligning these models to human preferences through reinforcement learning (Ouyang
et al., 2022). The resulting LLMs can be prompted with natural language in the form of instructions to perform a wide variety of tasks directly, amounting to zero-shot learning (Wei et al., 2022).</p>
<p>A key realization is thus that LLMs are given either no explicitly labelled data at all, or, in the case of instruct-LLMs, data with human labels pertaining to relatively general aspects of communicative interaction. As such they are part of a completely different paradigm than earlier language models that were trained on, for example, data sets of human-annotated language structures (e.g. Nivre et al., 2016). This means that when LLMs are capable of such tasks as solving co-reference relationships or identifying word classes (Manning et al., 2020), this arises as an emergent property of the model's architecture and training on different objectives. Given that such emergent linguistic capabilities have been observed (Reif et al., 2019; Grand et al., 2022), it is a legitimate empirical question which other capacities LLMs may have acquired as 'by-catch'.</p>
<h3>2.2 Theory of Mind in Humans and LLMs</h3>
<p>ToM, also known as 'mindreading', is classically defined as the capacity to attribute mental states to others (and oneself), in order to explain and anticipate behaviour. The concept goes back to research in ethology in which Premack and Woodruff (1978) famously studied chimpanzees' abilities to anticipate behaviour of caretakers. When focus shifted to ToM in humans, tests were developed that present a scenario in which a character behaves according to its false beliefs about a situation, and not according to the reality of the situation itself-which a successful participant, having the benefit of spectatorsight, can work out (see Section 3.1).</p>
<p>Initial consensus that children could pass versions of this test from the age of 4 was followed by scepticism about additional abilities it presumed, including language skills and executive functioning, which led to the development of simplified false-belief tests based on eye-gaze that even 15 month-olds were found to 'pass' (Onishi and Baillargeon, 2005). While this line of research also met important criticism (for a review see Barone et al., 2019), it highlights two key distinctions in debate from the past decades: implicit-behavioural versus explicit-representational and innate versus learned components of ToM. Some researchers see results from eye-gaze paradigms as evidence for a</p>
<p>native or very early developing capacity for beliefattribution in humans (Carruthers, 2013) and hold that performance on more complex tests is initially 'masked' by a lack of expressive skills (cf. also Fodor, 1992). Others have attempted to explain eyegaze results in terms of lower-level cognitive mechanisms (Heyes, 2014) and argued that the capacity for belief-attribution itself develops gradually in interaction with more general social, linguistic, and narrative competencies (Heyes and Frith, 2014; Milligan et al., 2007; Hutto, 2008). Two-systems approaches (Apperly, 2010) essentially reconcile both sides by positing that our mindreading capacity encompasses both a basic, fast, and early developing component and a more advanced and flexible component that develops later.</p>
<p>In computational cognitive research, a variety of approaches to modelling ToM have been proposed (e.g. Baker and Saxe, 2011; Arslan et al., 2017). More recently neural agents (Rabinowitz et al., 2018) have been implemented, along with an increasing number of deep-learning paradigms aimed at testing first- and second-order ToM via question-answering. Initially this was done with recurrent memory networks (Grant et al., 2017; Nematzadeh et al., 2018) using data sets of classic false-belief tests from psychology, but after issues surfaced with simple heuristics for solving such tasks, scenarios were made more varied and challenging (Le et al., 2019). From the inception of BERT as one of the first LLMs (Devlin et al., 2019), we have seen roughly two approaches for testing ToM in LLMs: many different ToM scenarios integrated in large benchmark suites (e.g. Sap et al., 2022; Srivastava et al., 2023; Sileo and Lernould, 2023; Ma et al., 2023; Shapira et al., 2023), and studies that modified standardized ToM tests as used in developmental and clinical research for prompting LLMs (e.g. Kosinski, 2023; Ullman, 2023; Bubeck et al., 2023; Brunet-Gouet et al., 2023; Chowdhery et al., 2022; Moghaddam and Honey, 2023; Marchetti et al., 2023). This paper adds to the latter tradition in four respects, as listed in the introduction.</p>
<h2>3 Methodology</h2>
<p>Here we describe our tasks and procedures for testing LLMs and children; all code, materials, and data are on OSF: https://shorturl.at/FQR34.</p>
<h3>3.1 ToM Tests</h3>
<p>Sally-Anne test, first-order (SA1) — The SallyAnne test (Wimmer and Perner, 1983; BaronCohen et al., 1985) is a classic first-order false belief test. It relies on a narrative in which Sally and Anne stand behind a table with a box and a basket on it. When Anne is still present, Sally puts a ball in her box. When Sally leaves, Anne retrieves the ball from the box and puts it in her own basket. The story ends when Sally returns and the participant is asked the experimental question 'Where will Sally look for the ball?' The correct answer is that she will look in her box. We followed up by asking a motivation question, 'Why?', to prompt an explanation to the effect of 'she (falsely) believes the object is where she left it'.</p>
<p>Sally-Anne test, second-order (SA2) — While SA1 targets the participant's judgement of what a character believes about the location of an unexpectedly displaced object, in SA2 the participant needs to judge what a character believes that another character believes about the location of an ice-cream truck (Perner and Wimmer, 1985). Sally and Anne are in a park this time, where an icecream man is positioned next to the fountain. Anne runs home to get her wallet just while the ice-cream man decides to move his truck to the swings. He tells Sally about this, but unknown to her, he meets Anne on the way and tells her too. Sally then runs after Anne, and finds her mother at home, who says that Anne picked up the wallet and went to buy ice cream. The experimental question now is 'Where does Sally think Anne went to buy ice cream?', with as correct answer 'to the fountain', also followed up with 'Why?', to prompt an explanation to the effect of 'Sally doesn't know that the ice-cream man told Anne that he was moving to the swings'.</p>
<p>Strange Stories test (SS) — The Strange Stories test (Happé, 1994; Kaland et al., 2005) depicts seven social situations with non-literal language use that can easily be misinterpreted, but causes no problems to typically developed adults. To understand the situations, subjects must infer the characters' intentions, applying ToM. For example, in one of the items a girl wants a rabbit for Christmas. When she opens her present, wrapped in a big enough box, it turns out that she received a pile of books. She says that she is really happy with her gift, after which subjects are asked the experimental question 'Is what the girl says true?', with correct answer 'No'. They can motivate their</p>
<p>answer after the question 'Why does she say this?', with as correct answer 'to avoid her parents' feelings being hurt'. Items increase in difficulty and cover a lie, pretend-play scenario, practical joke, white lie (example above), misunderstanding, sarcasm, and double bluff.</p>
<p>Imposing Memory test (IM) — The Imposing Memory test was originally developed by Kinderman et al. (1998), but the test has been revised several times; we rely on an unpublished version created by Anneke Haddad and Robin Dunbar (van Duijn, 2016), originally for adolescents, which we adapted thoroughly to make it suitable for children aged 7-10. Our version features two different stories, followed by true/false questions, 10 of which are 'intentionality' and 12 are 'memory' questions. For instance, in one story Sam has just moved to a new town. He asks one of his new classmates, Helen, where he can buy post stamps for a birthday card for his granny. When Helen initially sends him to the wrong location, Sam wonders whether she was playing a prank on him or just got confused about the whereabouts of the shop herself. He goes and asks another classmate, Pete, for help. As in the original IM, the intentionality questions involve reasoning about different levels of recursively embedded mental states (e.g., at third-level: 'Helen thought Sam did not believe that she knew the location of the store that sells post stamps'), whereas the memory questions require just remembering facts presented in the story (e.g., to match third-level intentionality questions, three elements from the story are combined: 'Sam was looking for a store where they sell post stamps. He told Pete that he had asked Helen about this').</p>
<h3>3.2 Scoring Test Answers</h3>
<p>Test scores for both children and LLMs were determined in the following way. For each of the SA1 and SA2 items, as well as for the seven SS items, a correct answer to the experimental question yielded 1 point. These answers were discrete and thus easy to assess ('box', 'fountain', 'no', etc.). For the motivation question a consensus score was obtained from two expert raters, on a range from $0-2$, with 0 meaning a missing, irrelevant, or wrong motivation, 1 meaning a partly appropriate motivation, and 2 meaning a completely appropriate motivation that fully explained why the character in each scenario did or said something, or had a mental or emotional mind state. Thus, the maximum score for the SA1,</p>
<p>SA2, and SS was 3 points per item, which were averaged to obtain a score between 0 and 1 . For each correct answer to a true/false question in the IM, 1 point was given. All scores and ratings can be found on OSF.</p>
<h3>3.3 Deviations</h3>
<p>We tested the LLMs on the original SA and SS scenarios, but also on manually created deviations that increasingly stray from their original formulations, to prevent LLMs from leveraging heuristics and memorizing relevant patterns from the training data. Thus, deviations probe the degree to which performance on ToM tests in LLMs generalizes. Deviation 0 was always the original test scenario (likely present in the training data); deviation 1 was a superficial variation on the original with only e.g., objects and names changed (similar to Kosinski (2023)), whereas deviation 2 was a completely new scenario where only the ToM-phenomenon at issue was kept constant (e.g., 'second-order false belief' or 'irony'). Since our adaptation of the IM test has hitherto not been used or published, we did not include deviations for this test.</p>
<h3>3.4 Test Procedures for LLMs</h3>
<p>We leveraged 11 state-of-the-art LLMs: 4 baseLLMs and 7 instruct-LLMs (see Table 1). Inference parameters were set such that their output was as deterministic as possible (i.e. a temperature $\approx$ zero or zero where possible) improving reproducibility. Each inference was done independently to avoid in-context learning or memory leakage between questions. This means that for each question, the prompt repeated the following general structure: [instruction] + [test scenario] + [question].</p>
<p>Instruct-LLMs were prompted in a questionanswering format that stayed as close as possible to the questionnaires given to children, without any further custom prompting or provision of examples. Instructions were also similar to those given to children (e.g. 'You will be asked a question. Please respond to it as accurately as possible without using many words.'). The 'Why'-questions in SA1 and SA2 were created by inserting the experimental question and answer the LLM gave into the prompt: [instruction] + [test scenario] + [experimental question] + [LLM answer] +['Why?']. This was not necessary for SS, given that experimental and motivation questions could be answered independently.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Base-LLMs</th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Falcon</td>
<td style="text-align: center;">Penedo et al. (2023)</td>
<td style="text-align: center;">7B</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">Touvron et al. (2023)</td>
<td style="text-align: center;">30B</td>
</tr>
<tr>
<td style="text-align: center;">GPT-davinci</td>
<td style="text-align: center;">Brown et al. (2020)</td>
<td style="text-align: center;">175B</td>
</tr>
<tr>
<td style="text-align: center;">BLOOM</td>
<td style="text-align: center;">Scao et al. (2022)</td>
<td style="text-align: center;">176B</td>
</tr>
<tr>
<td style="text-align: center;">Instruct-LLMs</td>
<td style="text-align: center;">"</td>
<td style="text-align: center;">"</td>
</tr>
<tr>
<td style="text-align: center;">Falcon-instruct</td>
<td style="text-align: center;">Penedo et al. (2023)</td>
<td style="text-align: center;">7B</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5</td>
<td style="text-align: center;">Chung et al. (2022)</td>
<td style="text-align: center;">11B</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(text-davinci-003)</td>
<td style="text-align: center;">Ouyang et al. (2022)</td>
<td style="text-align: center;">175B</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">Ouyang et al. (2022)</td>
<td style="text-align: center;">175B</td>
</tr>
<tr>
<td style="text-align: center;">PaLM2</td>
<td style="text-align: center;">Anil et al. (2023)</td>
<td style="text-align: center;">$175-340$ B</td>
</tr>
<tr>
<td style="text-align: center;">PaLM2-chat</td>
<td style="text-align: center;">Anil et al. (2023)</td>
<td style="text-align: center;">$175-340$ B</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">OpenAI (2023)</td>
<td style="text-align: center;">$&gt;340$ B</td>
</tr>
</tbody>
</table>
<p>Table 1: LLMs used in this study. Model sizes are undisclosed for GPT-4 and for PaLM2 and PaLM2-chat, thus we base ourselves on secondary sources for estimations; Knight (2023) and Elias (2023), respectively.</p>
<p>For base-LLMs, known to continue prompts rather than follow instructions, staying this close to the children's questionnaires was not feasible. For the SA and SS we therefore fed base-LLMs the scenario as described before, but formulated the questions as text-completion exercises (e.g. 'Sally will look for the ball in the '). Additionally, when creating the motivation questions for SA1 and SA2, we inserted the correct answer to the experimental question, instead of the LLM's answer. This was because base-LLMs so often derailed in their output that the method described for instruct-LLMs did not yield sensible prompts. Base-LLMs thus had an advantage here over children and instructLLMs, who were potentially providing a motivation following up on an incorrect answer they gave to the experimental question.</p>
<p>For the closed questions in the IM we attempted to streamline the output of base-LLMs by including two example continuations in the desired answer format. These examples were based on trivial information we added to the scenarios, unrelated to the actual experimental questions. For example: 'Helen: I wear a blue jumper today. This is [incorrect]', where it was added in the story that Helen wears a green jumper. This pushed nearly all baseLLM responses towards starting with '[correct]' or '[incorrect]', which we then assessed as answers to the true/false questions. We considered a similar prompt structure for SA and SS, amounting to adopting few-shot learning for base-LLMs throughout (Brown et al., 2020), but given that reformulating questions as text-completion exercises was by itself effective to get the desired output format, we refrained from inserting further differences from
how instruct-LLMs are prompted. It is important to note that our prompts were in general not optimized for maximal test performance, but rather designed to stay as uniform and close to the way children were tested as possible, enabling a fair comparison among LLMs and with child performance.</p>
<h3>3.5 Test Procedures for Children</h3>
<p>Children were recruited from one Dutch and one international school in the South-West of the Netherlands: 37 children in the younger group (7-8y) and 36 children in the older group (9-10y). Children were administered digital versions of the SA and SS for the younger group, and of the IM for the older group, which they completed individually on tablets or PCs equipped with a touch screen. Test scenarios and questions were presented in a self-paced text format and all SA and SS questions were followed by an open text field in which they had to type their answer. As the IM features long scenarios, voice-overs of the text were included to alleviate reading fatigue. Here children had to answer by pressing yes/no after each question. To reduce memory bottlenecks, accompanying drawings were inserted (see OSF) and navigating back and forth throughout the tests was enabled. Informed consent for each child was obtained from caretakers, and the study was approved by the Leiden University Science Ethics Committee (ref. no. 2021-18). Test answers were evaluated and scored parallel to the approach for LLMs (Section 3.2).</p>
<h2>4 Results</h2>
<h3>4.1 Sally-Anne</h3>
<p>Overall performance on SA1 versus SA2 is given in Figure 1, left column. Most base-LLMs perform above child level on first-order ToM (BLOOM, Davinci, LLaMA-30B) but fall at or or below child level on second-order ToM. A similar pattern is visible for instruct-LLMs: most models perform well above child level on first-order (GPT-4, GPT3.5, PaLM2-chat, PaLM2), but not on second-order ToM. Exceptions are GPT-4 and GPT-3.5: while degrading on second-order, they remain above child level. For both base- and instruct-LLMs, smaller models tend to perform worse (Falcon-7B, Falcon-7B-I, FLAN-T5) with GPT-3's structurally low scores as striking exception. This is inconsistent with results reported by (Kosinski, 2023) for GPT3, which is probably due to the fact that Kosinski applied a text-completion approach whereas we</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance on Sally-Anne tests for base-LLMs (top row) and instruct-LLMs (bottom row). Left column depicts performance on first- and second-order ToM (i.e. SA1 vs. SA2), averaged over the original and rewritten test versions. Middle and left columns depict performance for SA1 and SA2 over levels of deviation from the original test ( 0,1 , and 2 ; see Section 3.3). Dashed lines indicate child performance ( $\mathrm{n}=37$, age $7-8$ years).
prompted GPT-3 with open questions.
When we consider the performance on SA1 and SA2 over deviations (middle and right columns in Figure 1), we see once more that almost all LLMs struggle with second-order ToM, since performance decreases already on deviation 0 (i.e. the original test scenario), except for GPT-3.5 and GPT-4. Yet, it is the combination of second-order ToM and deviation 2 that pushes also GPT-3.5 and GPT-4 substantially below child levels, except for Falcon-7B, although the chat-optimized version of this model (Falcon-7B-I) fails on all second-order questions.</p>
<h3>4.2 Strange Stories</h3>
<p>General performance on SS is given in Figure 2, left column. Whereas child performance declines as items become more complex (from 1 to 7 ; see Section 3.1), this is overall less the case for LLM performance. For instruct-LLMs, we see that GPT4 approaches perfect scores throughout. GPT-3 and GPT-3.5 perform at or close to child level on item 1, after which their performance somewhat declines, while staying well above child level. Other instructLLMs show a mixed picture: PaLM2-chat and FLAN-T5 surpass child level earlier than PaLM2. Interestingly, smaller FLAN-T5 outperforms large PaLM and PaLM2-chat on more difficult items. Falcon-7B-I, as smallest instruct-LLM, performs overall worst.</p>
<p>If performance is plotted over deviations (right column in Figure 2) we see little impact on most base-LLMs. For instruct-LLMs, it is striking
that deviation levels have almost no effect on the larger models (GPT-4, PaLM2, PaLM2-chat, GPT3, GPT-3.5), but do more dramatically lower performance of smaller models (FLAN-T5, Falcon-7B-I). In sum, base-LLMs perform below child level, except for the most complex items. Several large instruct-LLMs match or surpass child level throughout, others only for more complex items. Unlike for SA, deviation levels seem to have little negative impact.</p>
<h3>4.3 Imposing Memory</h3>
<p>The classical finding for the IM test is that error rates go up significantly for questions involving higher levels of recursive intentionality, but not for memory questions on matched levels of complexity, suggesting a limit to the capacity for recursive ToM specifically (Stiller and Dunbar, 2007). ${ }^{1}$ We verified this for our child data $(\mathrm{n}=36)$ with two mixed linear models for memory and intentional questions with random intercepts. We included five predictors that were contrast-coded such that each predictor indicated the difference in average performance with the previous level. For intentional questions, only the difference between level two and one was significant $(\beta=-0.222, p&lt;.05)$, marking a cutoff point after which performance remained consistently low. For memory questions, performance</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance on Strange Stories for base-LLMs (top row) and instruct-LLMs (bottom row). Left column shows overall performance, averaged over levels of deviation from the original test. Right column shows performance over deviation levels, averaged over items. Dashed lines indicate child performance ( $\mathrm{n}=37,7-8 \mathrm{y}$ ).
remained high across all levels ( $&gt;.85$ ), except for level four, where scores were significantly lower than at level three $(\beta=-0.292, p&lt;.00)$, but went up again at level five $(\beta=0.208, p&lt;.00)$. Thus, in line with earlier work, we find a cut-off point after which scores on intentionality questions remained consistently low, compared to scores on matched memory questions. We have no clear explanation for the dip in performance on memory questions at level four, but observe that it is driven by low scores on only one specific question out of a total of four for this level, which children may have found confusing.</p>
<p>In Figure 3 we see that all base-LLMs perform below child level, in general and on both intentionality and memory questions, and there is little variation in performance, except that larger baseLLMs (BLOOM, GPT-davinci) improve on higher levels of recursion. Regarding instruct-LLMs, we see largely the same picture, as they almost all perform below child level, in general and on both types of questions. The exception is GPT-4, which performs consistently well on all levels and stays above child level after second-order intentionality. For the difference between memory and intentional questions, instruct-LLMs perform better on easier memory questions, and drop towards the end, while on intentional questions, they already start lower and stay relatively constant. Lastly, it is remarkable that FLAN-T5, as one of the smallest instructLLMs, overall increases performance as recursion levels go up, and ends at child level. For GPT-3.5, which performs worst of all instruct-LLMs on this task, we see the exact opposite.</p>
<h3>4.4 Notes on Child Performance</h3>
<p>It can be observed that performance for SA was overall low compared to what could be expected from children aged 7-8 years: $\bar{x}=0.45$ for SA1 and $\bar{x}=0.225$ for SA2. We have two complementary explanations for this. Firstly, as discussed in Section 3.5, children had to read the tests on a screen, after which they had to type answers in open text fields. This is a challenging task by itself that relies on additional skills including language proficiency, conscientiousness, digital literacy, and more. Secondly, whereas 'passing' originally only means that a child can work out where Sally will look (for the ball, or for Anne on her way to buy ice cream), we also asked for a motivation, which makes the test more demanding. For the SS, completed by the same group of children, we see the expected pattern that scores show a downward tendency as test items increase in difficulty. The older group, aged 9-10, completed the IM. As discussed in Section 4.3, scores resonate with earlier work. Given that we see child performance not as the central phenomenon under observation in this paper, but rather as a reference for LLM performance, further discussion is outside our scope.</p>
<h2>5 Discussion</h2>
<p>Summing up the results for the Sally-Anne tests, while it is less surprising that base-LLMs and smaller instruct-LLMs struggle with increasing test complexity and deviations, it is striking that second-order ToM immediately perturbs some large instruct-LLMs (e.g. PaLM2-chat), and that adding deviations from the original test formula-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance on Imposing Memory test for base-LLMs (top row) and instruct-LLMs (bottom row). Left column depicts overall performance over five levels of recursion, averaged over deviations. Middle and left columns depict performance for Memory and Intentional questions. Dashed lines indicate child performance (n=36, 9-10y).
tions pushed performance of even the most competitive models down (e.g. GPT-4, GPT-3.5). This initially suggests that performance on ToM tasks does not generalize well beyond a few standard contexts in LLMs, in line with earlier work (Sap et al., 2022; Shapira et al., 2023; Ullman, 2023).</p>
<p>For the Strange Stories we saw that base-LLMs perform generally below child level. Most instructLLMs perform close to or above child level, particularly as items become more complex and child performance drops much more dramatically than LLM performance. Levels of deviation from the original test formulation seem to have made almost no impact for the SS, suggesting that the capacity to deal with non-literal language targeted by the Strange Stories test does generalize to novel contexts. We conclude that instruct-LLMs are quite capable at interpreting non-literal language, a skill that in humans involves ToM. Since the training data of LLMs includes numerous books and fora, which are typically rich in irony, misunderstanding, jokes, sarcasm, and similar figures of speech, we tentatively suggest that LLMs are in general wellequipped to handle the sort of scenarios covered in the Strange Stories. This should in theory include base-LLMs, but it could be that their knowledge does not surface due to the test format, even after specialized prompting. Going one step further, we hypothesize that Sally-Ann is generally harder for LLMs given that this test relies less on a very specific sort of advanced language ability, but more on a type of behaviourally-situated reasoning that LLMs have limited access to during training (see also Mahowald et al., 2023).</p>
<p>The Imposing Memory test was the most chal-
lenging for both base- and instruct-LLMs. Since our version of it was never published before, it constitutes another robustness test, which only GPT-4 as largest instruct-LLM seems to pass well.</p>
<p>The gap between base- and instruct-LLMs is best summarized in Figure 4. Here we see that no baseLLM achieves child level: all LLMs approaching or exceeding child performance are larger instructLLMs. Our adapted prompts and insertion of correct answers for motivation questions did not make a difference. We suggest that another issue for baseLLMs, besides the prompt format, was prompt length. This was highest for IM, which can explain why they struggled most with this test. Prompt length, in relation to the models' varying context window sizes and ability to engage in what Hagendorff et al. (2023) call chain-of-thought reasoning, merits further research (see also Liu et al., 2023). We tested whether there was a difference between model performance on closed versus open questions across all three tasks, but found no signal: the models that struggled with closed questions were also those that performed low on open questions (for more details and additional information on prompting, see Appendix A on OSF).</p>
<p>Evidence is emerging that most LLM capacities are learned during self-supervised pre-training (Gudibande et al., 2023; Ye et al., 2023), which suggests that base-LLMs are essentially 'complete' models. Yet instruction-tuning, even in small amounts (Zhou et al., 2023), adds adherence to the desired interaction format and teaches LLMs, as it were, to apply their knowledge appropriately. We see a parallel between instruction-tuning and the role for rewarding cooperative communication</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Grand mean performance (stars) of all mean test scores (dots) for children and LLMs.
in human evolution and development. It has been argued extensively that human communication is fundamentally cooperative in that it relies on a basic ability and willingness to engage in mental coordination (e.g Verhagen, 2015; Grice, 1975). It is a key characteristic of the socio-cultural niche in which we evolved that, when growing up, we are constantly being rewarded for showing such willingness and cooperating with others to achieve successful communicative interactions (Tomasello, 2008). Reversely, if we do not, we are being punished, explicitly or implicitly via increasing social exclusion (David-Barrett and Dunbar, 2016). This brings us back to our context: instruction-tuning essentially rewards similar cooperative principles, but punishes the opposite, which may amount to an enhanced capacity for coordinating with an interaction partner's perspective, in humans and LLMs alike. This is reflected in performance on ToM tasks, which are banking on this capacity too.</p>
<p>Finally, we do not claim that LLMs that performed well also have ToM in the way that humans have it. Validity of cognitive tests such as those used in ToM research is a general issue (e.g. van Duijn, 2016). Yet for humans ToM tests are validated 'quick probes': decades of research have shown that proficiency on such tests correlates with an array of real-world social and cognitive abilities (Beaudoin et al., 2020). For LLMs we are in a very early stage of figuring out what is entailed by proficon ToM tests: on the one hand it is impressive that some models show a degree of robust performance, without explicit training on ToM. On the other hand it remains an open question whether this amounts to any actual capacities in the social-cognitive domain, in which they are clearly very differently
grounded (if at all) compared to humans.
For future research we believe in the format of testing models that differ in other respects than just size, on a varied array of tasks, with multiple tests per test item, to gain further insight into the aspects that explain variability in performance. For this, more openness about architecture and training procedures of current and future LLMs is imperative. In addition, we believe to have contributed to the debate by benchmarking LLM results on child data, but more of this is needed. We had limited samples and age distributions, and tests were not presented in optimal ways (see Section 3.5).</p>
<p>We emphasize that our results need to be seen within the time frame of late Spring 2023. The fast pace with which LLMs are currently released and, in some cases, updated, makes them a moving target. There are indications that specific capacities of models from the GPT-family have declined over time, perhaps as a result of such updates (e.g., handling math problems and producing code; Chen et al., 2023). Future studies need to address how such developments impact the capacities assessed in this paper.</p>
<h2>6 Conclusion</h2>
<p>We have shown that a majority of recent Large Language Models operate below performance of children aged 7-10 on three standardized tests relevant to Theory of Mind. Yet those that are largest in terms of parameters, and most heavily instructiontuned, surpass children, with GPT-4 well above all other models, including more recent competitors like PaLM2-chat and PaLM2 (see Figure 4). We have interpreted these findings by drawing a parallel between instruction-tuning and rewarding cooperative interaction in human evolution. We concede that researching the degree to which LLMs are capable of anything like thought in the human sense has only just begun, which leaves the field with exciting challenges ahead.</p>
<h2>Acknowledgements</h2>
<p>This research took place in the context of the project A Telling Story, financed by the Dutch Research Council NWO (VLVeni.191C.051). We are grateful to the children and their caregivers and teachers for participating in our research, and we thank Li Kloostra, Lola Vandame, and three anonymous reviewers for their help and constructive feedback.</p>
<h2>References</h2>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Iltycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2 Technical Report. arXiv preprint arXiv:2305.10403v3.</p>
<p>Ian Apperly. 2010. Mindreaders: the Cognitive Basis of "Theory of Mind". Psychology Press.</p>
<p>Burcu Arslan, Niels A Taatgen, and Rineke Verbrugge. 2017. Five-year-olds' systematic errors in secondorder false belief tasks are due to first-order theory of mind strategy selection: A computational modeling study. Frontiers in psychology, 8:275.</p>
<p>Chris Baker and Rebecca Saxe. 2011. Bayesian theory of mind: Modeling joint belief-desire attribution. Proceedings of the Thirty-Third Annual Conference of the Cognitive Science Society.</p>
<p>Simon Baron-Cohen, Alan M Leslie, and Uta Frith. 1985. Does the autistic child have a "theory of mind"? Cognition, 21(1):37-46.</p>
<p>Pamela Barone, Guido Corradi, and Antoni Gomila. 2019. Infants' performance in spontaneous-response false belief tasks: A review and meta-analysis. Infant Behavior and Development, 57:101350.</p>
<p>Cindy Beaudoin, Élizabel Leblanc, Charlotte Gagner, and Miriam H Beauchamp. 2020. Systematic review and inventory of theory of mind measures for young children. Frontiers in psychology, 10:2905.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623.</p>
<p>Marcel Binz and Eric Schulz. 2023a. Turning large language models into cognitive models. arXiv preprint arXiv:2306.03917.</p>
<p>Marcel Binz and Eric Schulz. 2023b. Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901.</p>
<p>Eric Brunet-Gouet, Nathan Vidal, and Paul Roux. 2023. Do conversational agents have a theory of mind? a single case study of chatgpt with the hinting, false beliefs and false photographs, and strange stories paradigms.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with GPT-4.</p>
<p>Peter Carruthers. 2013. Mindreading in infancy. Mind \&amp; Language, 28(2):141-172.</p>
<p>Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is ChatGPT's behavior changing over time? arXiv preprint arXiv:2307.09009.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.</p>
<p>Tamas David-Barrett and Robin I. M. Dunbar. 2016. Language as a coordination tool evolves slowly. $R$. Soc. open sci., 3:160259.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding.</p>
<p>Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. 2023. Can AI language models replace human participants? Trends in Cognitive Sciences, 27(7):597-600.</p>
<p>Jennifer Elias. 2023. Google's newest A.I. model uses nearly five times more text data for training than its predecessor. Accessed on: 2023-05-30.
J.A. Fodor. 1992. A theory of the child's theory of mind. Cognition, 44(3):283-296.</p>
<p>Gabriel Grand, Idan Asher Blank, Francisco Pereira, and Evelina Fedorenko. 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nature human behaviour, 6(7):975-987.</p>
<p>Erin Grant, Aida Nematzadeh, and Thomas L Griffiths. 2017. How can memory-augmented neural networks pass a false-belief task? In CogSci.</p>
<p>Paul Grice. 1975. Logic and conversation. In Peter Cole and Jerry Morgan, editors, Syntax and semantics. Vol. 3: Speech acts, pages 41-58. Academic Press, New York.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms.
T. Hagendorff, S. Fabi, and M. Kosinski. 2023. Humanlike intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Nature Compututer Science.</p>
<p>Thilo Hagendorff. 2023. Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. arXiv preprint arXiv:2303.13988.</p>
<p>Francesca G.E. Happé. 1994. An advanced test of theory of mind: Understanding of story characters' thoughts and feelings by able autistic, mentally handicapped, and normal children and adults. Journal of autism and Developmental disorders, 24(2):129-154.</p>
<p>Cecilia Heyes. 2014. False belief in infancy: a fresh look. Developmental Science, 17(5):647-659.</p>
<p>Cecilia M. Heyes and Chris D. Frith. 2014. The cultural evolution of mind reading. Science, 344(6190):1243091.</p>
<p>Daniel D. Hutto. 2008. Folk Psychological Narratives: The Sociocultural Basis of Understanding Reasons. The MIT Press.</p>
<p>Nils Kaland, Annette Møller-Nielsen, Lars Smith, Erik Lykke Mortensen, Kirsten Callesen, and Dorte Gottlieb. 2005. The Strange Stories test - a replication study of children and adolescents with Asperger syndrome. European child \&amp; adolescent psychiatry, 14(2):73-82.
P. Kinderman, R. Dunbar, and R. P. Bentall. 1998. Theory-of-mind deficits and causal attributions. British Journal of Psychology, (2):191-204.</p>
<p>Oscar Kjell, Katarina Kjell, and H Andrew Schwartz. 2023. Ai-based large language models are ready to transform psychological health assessment.</p>
<p>Will Knight. 2023. A new chip cluster will make massive ai models possible. Accessed on: 2023-05-30.</p>
<p>Michal Kosinski. 2023. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083.</p>
<p>Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5872-5877, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Penelope A. Lewis, Amy Birch, Alexander Hall, and Robin I. M. Dunbar. 2017. Higher order intentionality tasks are cognitively more demanding. Social Cognitive and Affective Neuroscience, 12(7):10631071.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.</p>
<p>Genevieve Liveley and Sam Thomas. 2020. Homer's intelligent machines: AI in antiquity.</p>
<p>Li Lucy and David Bamman. 2021. Gender and Representation Bias in GPT-3 Generated Stories. In Proceedings of the Third Workshop on Narrative Understanding, pages 48-55, Virtual. Association for Computational Linguistics.</p>
<p>Xiaomeng Ma, Lingyu Gao, and Qihui Xu. 2023. Tomchallenges: A principle-guided dataset and diverse evaluation tasks for exploring theory of mind. arXiv preprint arXiv:2305.15068.</p>
<p>Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627.</p>
<p>Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. 2020. Emergent linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the National Academy of Sciences, 117(48):30046-30054.</p>
<p>Antonella Marchetti, Cinzia Di Dio, Angelo Cangelosi, Federico Manzi, and Davide Massaro. 2023. Developing chatgpt's theory of mind. Frontiers in Robotics and AI, 10 .</p>
<p>Karen Milligan, Janet Wilde Astington, and Lisa Ain Dack. 2007. Language and theory of mind: Metaanalysis of the relation between language ability and false-belief understanding. Child development, 78(2):622-646.</p>
<p>Shima Rahimi Moghaddam and Christopher J. Honey. 2023. Boosting theory-of-mind performance in large language models via prompting.</p>
<p>Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Tom Griffiths. 2018. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392-2400, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al. 2016. Universal dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages $1659-1666$.</p>
<p>Kristine H. Onishi and Renée Baillargeon. 2005. Do 15-month-old infants understand false beliefs? Science, 308(5719):255-258.</p>
<p>OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.</p>
<p>Josef Perner and Heinz Wimmer. 1985. "John thinks that Mary thinks that. . ." attribution of second-order beliefs by 5-to 10-year-old children. Journal of experimental child psychology, 39(3):437-471.</p>
<p>Billy Perrigo. 2023. Exclusive: OpenAI Used Kenyan Workers on Less Than \$2 Per Hour to Make ChatGPT Less Toxic. Accessed on: 2023-01-25.</p>
<p>David Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515-526.</p>
<p>Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S. M. Ali Eslami, and Matthew Botvinick. 2018. Machine theory of mind. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4218-4227. PMLR.</p>
<p>Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. 2019. Visualizing and measuring the geometry of BERT. Advances in Neural Information Processing Systems, 32.</p>
<p>Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842-866.</p>
<p>Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762-3780, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. BLOOM: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever Hans or neural theory of mind? stress testing social reasoning in large language models.</p>
<p>Damien Sileo and Antoine Lernould. 2023. MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. arXiv preprint arXiv.2305.03353.</p>
<p>Jeff Sparrow. 2022. 'Full-on robot writing': the artificial intelligence challenge facing universities. Accessed on: 2023-01-25.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex</p>
<p>Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris CallisonBurch, Christopher Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C. Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodolà, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Francis Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose HernandezOrallo, Joseph Boudeman, Joseph Guerr, Joseph</p>
<p>Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-Colón, Luke Metz, Lütfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta GurAri Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter W Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Russ Salakhutdinov, Ryan Andrew Chi, Seungjae Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel Stern Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Zivic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber, Summer Mish-</p>
<p>erghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>James Stiller and Robin IM Dunbar. 2007. Perspectivetaking and memory capacity predict social network size. Social Networks, 29(1):93-104.</p>
<p>Michael Tomasello. 2008. Origins of Human Communication. MIT Press, Cambridge, MA.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.</p>
<p>Alan M. Turing. 1950. Computing machinery and intelligence. Mind, LIX:433-460.</p>
<p>Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.</p>
<p>Max J van Duijn. 2016. The lazy mindreader: a humanities perspective on mindreading and multiple-order intentionality. Ph.D. thesis, Leiden University.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30 .</p>
<p>Arie Verhagen. 2015. Grammar and cooperative communication. In Ewa Dabrowska and Dagmar Divjak, editors, Handbook of Cognitive Linguistics, pages 232-252. De Gruyter Mouton, Berlin, München, Boston.</p>
<p>Adelheid Voskuhl. 2013. One introduction: Androids, enlightenment, and the human-machine boundary.</p>
<p>Taylor Webb, Keith J. Holyoak, and Hongjing Lu. 2023. Emergent analogical reasoning in large language models.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners.</p>
<p>Robert Wilson, Alexander Hruby, Daniel Perez-Zapata, Sanne W. van der Kleij, and Ian A. Apperly. 2023. Is recursive "mindreading" really an exception to limitations on recursive thinking? Journal of Experimental Psychology: General, 152(5):1454-1468.</p>
<p>Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Cognition, 13(1):103-128.</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023. A comprehensive capability analysis of GPT-3 and GPT-3.5 series models. arXiv preprint arXiv:2303.10420.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ While there is consensus in the literature that higher levels of intentionality are significantly harder for participants than lower levels, by various measures, there is debate about the difference with memory questions; see e.g. Lewis et al. (2017). For a critical discussion of measuring recursive intentionality in general, see Wilson et al. (2023).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
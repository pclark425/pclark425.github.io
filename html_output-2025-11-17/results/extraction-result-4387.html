<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4387 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4387</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4387</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-280561585</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.05660v1.pdf" target="_blank">Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review</a></p>
                <p><strong>Paper Abstract:</strong> The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries with vector search offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates. We present an agentic approach that encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1) dynamically selecting between GraphRAG and VectorRAG for each query, (2) adapting instruction-tuned generation in real time to researcher needs, and (3) quantifying uncertainty during inference. This dynamic orchestration improves relevance, reduces hallucinations, and promotes reproducibility. Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2 model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking). Instruction tuning refines domain-specific generation, and bootstrapped evaluation yields standard deviation for evaluation metrics. On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned Agent with Direct Preference Optimization (DPO) outperforms the baseline, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision. These results highlight the system's improved reasoning over heterogeneous sources and establish a scalable framework for autonomous, agentic scientific discovery.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4387.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4387.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentic Hybrid RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-Source Agentic Hybrid Retrieval-Augmented Generation Framework for Scientific Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source agentic framework that dynamically orchestrates graph-based (GraphRAG) and vector-based (VectorRAG) retrieval, grounding LLM generation in heterogeneous scholarly sources (KG + full-text) with runtime uncertainty quantification and preference optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentic Hybrid RAG Framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A Python-based, Dockerizable pipeline that (1) ingests bibliometric records from PubMed, ArXiv and Google Scholar; (2) constructs a Neo4j knowledge graph containing papers, citations, authors and extracted keywords; (3) parses open-access full-text PDFs into overlapping chunks and embeds them with all-MiniLM-L6-v2 into a FAISS vector store; (4) exposes two retrieval functions (Cypher-based GraphRAG and an ensemble VectorRAG combining BM25 keyword and dense semantic search) and a reranking step (Cohere rerank-english-v3.0); (5) runs an LLM agent (LLaMA-3.3-70B-versatile) that dynamically selects the retrieval mode per query using few-shot tool-selection exemplars; and (6) generates final answers with an instruction-tuned generator (Mistral-7B-Instruct-v0.3) further optimized via Direct Preference Optimization (DPO). The system also produces bootstrapped uncertainty estimates and supports instruction tuning and few-shot Cypher generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLaMA-3.3-70B-versatile (agent for planning/tool selection); Mistral-7B-Instruct-v0.3 (response generator); all-MiniLM-L6-v2 (embeddings); Cohere rerank-english-v3.0 (reranker).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Hybrid extraction: (a) structured Cypher queries over a Neo4j knowledge graph (metadata, citations, keyword nodes) generated from natural-language queries (GraphRAG); (b) chunk-level embedding-based retrieval (FAISS) using all-MiniLM-L6-v2 plus BM25 keyword retrieval, merged and reranked by Cohere (VectorRAG); few-shot examples guide tool choice.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Instruction-tuned generation conditioned on retrieved contexts; the agent dynamically selects GraphRAG or VectorRAG per query and composes evidence from retrieved chunks and KG results into concise answers; DPO trains the generator to prefer context-grounded outputs; bootstrapped evaluation informs uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature with emphasis on biomedical and clinical literature (ingests PubMed, ArXiv, Google Scholar records); evaluated on synthetic benchmarks derived from papers about Multimodal LLMs in healthcare (2023-2025).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Context-grounded answers to research queries, literature exploration summaries, KG query results, uncertainty estimates; supporting artifacts include retrieved context chunks and Cypher results.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Faithfulness (F), Answer Relevance (AR), Context Precision (CP), Context Recall (CR); bootstrapped means and standard deviations computed across 12 resamples; task-specific KS/VS (KG vs VS) breakdowns (e.g., VS Context Recall).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Relative improvements over a non-agentic baseline: +0.63 VS Context Recall, +0.56 overall Context Precision, +0.24 VS Faithfulness, +0.12 VS Precision and KG Answer Relevance, +0.11 overall Faithfulness, +0.05 KG Context Recall, +0.04 VS Answer Relevance and overall Precision. Latency: ~2 minutes/query on a consumer machine, ~10 sec with GPU-enabled server.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Non-Agentic RAG: joint semantic search over both vector store and KG (fixed pipeline baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>The instruction-tuned agent with DPO substantially outperformed the non-agentic baseline across most metrics (see reported numeric gains above), with modest declines in a few KG-specific precision/faithfulness measures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamically selecting between GraphRAG and VectorRAG per query improves retrieval relevance and downstream generation; combining KG-structured queries and dense full-text retrieval leverages complementary strengths (KG for metadata/multi-hop relations, VS for nuanced full-text content); instruction tuning and small-data DPO meaningfully increase faithfulness and context utilization; bootstrapped evaluation provides transparent uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cypher translation via few-shot prompting can misinterpret complex queries leading to KG errors; synthetic benchmark may not reflect full real-world complexity (figures/tables, modalities); dependence on external APIs (rate limits, data availability); OCR for scanned PDFs not integrated; computational/latency costs on consumer hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>No explicit empirical scaling curves reported; authors note latency reduces with GPU server deployment and anticipate RL-based policy optimization or continuous ingestion to improve performance; DPO showed benefits even with 15 preference pairs, but effects of corpus size or larger model sizes are discussed qualitatively rather than quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4387.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4387.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented agent that answers research queries by retrieving and synthesizing relevant excerpts from a corpus of journal articles, reported to achieve expert-level performance on multi-document benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as an agentic RAG system applied to a corpus of journal articles that retrieves relevant passages and synthesizes them to answer research queries; cited as prior work demonstrating expert-level multi-document question answering in scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Passage retrieval from a corpus of journal articles (RAG-style retriever).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Generative synthesis conditioned on retrieved excerpts (multi-document answer synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific research articles (general scientific literature).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers to research queries / multi-document summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported as expert-level performance on multi-document benchmarks (specific metrics not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG-style agents can achieve high performance on multi-document question answering tasks in scientific domains when grounded in retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4387.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4387.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (Lewis et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The RAG paradigm couples a retriever over an external corpus with a generative model conditioned on retrieved passages to improve factual accuracy and provenance in knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A two-stage architecture where a retriever fetches relevant passages (non-parametric memory) and a sequence-to-sequence generator conditions on those passages to answer queries; originally demonstrated with a neural retriever over Wikipedia.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>sequence-to-sequence generator (unspecified in this paper's description of prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Neural passage retrieval over an external corpus (e.g., Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Generation conditioned on retrieved passages (context-grounded generation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Open-domain / knowledge-intensive QA (original demonstration used Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Context-grounded answers / QA responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Standard QA metrics used in the originating work (not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grounding generation in retrieved documents improves factuality and traceability compared to standalone LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4387.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4387.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HybridRAG (Sarmah et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach integrating knowledge graphs with vector retrieval in a RAG pipeline to improve information extraction efficiency and exploit structured and unstructured evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HybridRAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as prior work that combines knowledge-graph queries and vector retrieval within a RAG framework to leverage structured relations and dense semantic content for information extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Graph-based queries (KG) plus embedding-based vector retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Combining structured KG outputs and retrieved text to inform generation (hybrid RAG synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Information extraction over mixed structured/unstructured corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extracted information / context-grounded responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hybridizing KG and vector retrieval can improve retrieval robustness by leveraging complementary modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4387.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4387.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DualRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-process approach that integrates reasoning and retrieval mechanisms to handle multi-hop question answering by combining distinct retrieval strategies and reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DualRAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an approach that integrates separate retrieval and reasoning processes (dual-process) to better support multi-hop QA, implying a specialized orchestration between retrieval modules and reasoning components.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-hop retrieval strategies (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Integrated reasoning over retrieved evidence across hops.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-hop question answering (general QA).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers requiring multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dual-process integration of retrieval and reasoning can better address multi-hop questions (as referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4387.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4387.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PolyG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GraphRAG variant focusing on adaptive graph traversal strategies to make graph-based retrieval within RAG more effective and efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PolyG (GraphRAG variant)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as work on GraphRAG that employs adaptive graph traversal to improve the efficiency and effectiveness of graph-based retrieval in RAG pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Adaptive traversal of knowledge graphs to retrieve multi-hop relational evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Graph-grounded generation after adaptive retrieval (details not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Graph-based retrieval for QA/information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Graph-grounded answers / retrieval outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive graph traversal can make GraphRAG approaches more efficient and better at multi-hop retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4387.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4387.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SemanticScholar/Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Scholar and Elicit (examples of static hybrid RAG pipelines / commercial tools)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary scholarly search and literature-assistant tools that integrate neural retrieval and summarization but typically rely on fixed retrieval pipelines and proprietary LLM APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semantic Scholar / Elicit (representative tools)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as examples of existing literature tools that combine neural retrieval and summarization; characterized as using static, fixed retrieval pipelines and often depending on proprietary LLM APIs without runtime uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Proprietary neural retrieval and indexing (likely dense retrieval + metadata search; specifics not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Neural summarization/retrieval-augmented outputs (commercial implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic literature search and summarization (multi-domain).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries, search results, literature overviews.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Existing commercial tools often use single-modality retrieval pipelines and lack adaptive retrieval orchestration and uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Proprietary/opaque implementations, static pipelines, limited uncertainty estimates, potential reproducibility concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4387.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4387.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentic RAG Survey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentic Retrieval-Augmented Generation: A Survey on Agentic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A survey paper summarizing agentic approaches that orchestrate retrieval and generation components (agents that choose tools) in RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentic Retrieval-Augmented Generation: A Survey on Agentic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentic Retrieval-Augmented Generation (survey / approach)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a survey of agentic methods where an LLM-based agent plans, decomposes queries, and calls specialized tools (retrievers, KG queries, etc.) to achieve complex tasks in RAG pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Agent-driven selection and orchestration of retrieval tools (GraphRAG, VectorRAG, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical planning and tool composition followed by context-grounded generation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General RAG/agentic literature (survey across approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey insights; categorizations of agentic RAG methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agentic orchestration offers flexible, tool-aware pipelines that can dynamically select retrieval modalities and improve task performance versus static pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction <em>(Rating: 2)</em></li>
                <li>DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering <em>(Rating: 2)</em></li>
                <li>PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal <em>(Rating: 2)</em></li>
                <li>Agentic Retrieval-Augmented Generation: A Survey on Agentic <em>(Rating: 2)</em></li>
                <li>Retraining language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>Ragas: Automated Evaluation of Retrieval Augmented Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4387",
    "paper_id": "paper-280561585",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Agentic Hybrid RAG",
            "name_full": "Open-Source Agentic Hybrid Retrieval-Augmented Generation Framework for Scientific Literature Review",
            "brief_description": "An open-source agentic framework that dynamically orchestrates graph-based (GraphRAG) and vector-based (VectorRAG) retrieval, grounding LLM generation in heterogeneous scholarly sources (KG + full-text) with runtime uncertainty quantification and preference optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Agentic Hybrid RAG Framework",
            "system_description": "A Python-based, Dockerizable pipeline that (1) ingests bibliometric records from PubMed, ArXiv and Google Scholar; (2) constructs a Neo4j knowledge graph containing papers, citations, authors and extracted keywords; (3) parses open-access full-text PDFs into overlapping chunks and embeds them with all-MiniLM-L6-v2 into a FAISS vector store; (4) exposes two retrieval functions (Cypher-based GraphRAG and an ensemble VectorRAG combining BM25 keyword and dense semantic search) and a reranking step (Cohere rerank-english-v3.0); (5) runs an LLM agent (LLaMA-3.3-70B-versatile) that dynamically selects the retrieval mode per query using few-shot tool-selection exemplars; and (6) generates final answers with an instruction-tuned generator (Mistral-7B-Instruct-v0.3) further optimized via Direct Preference Optimization (DPO). The system also produces bootstrapped uncertainty estimates and supports instruction tuning and few-shot Cypher generation.",
            "llm_model_used": "LLaMA-3.3-70B-versatile (agent for planning/tool selection); Mistral-7B-Instruct-v0.3 (response generator); all-MiniLM-L6-v2 (embeddings); Cohere rerank-english-v3.0 (reranker).",
            "extraction_technique": "Hybrid extraction: (a) structured Cypher queries over a Neo4j knowledge graph (metadata, citations, keyword nodes) generated from natural-language queries (GraphRAG); (b) chunk-level embedding-based retrieval (FAISS) using all-MiniLM-L6-v2 plus BM25 keyword retrieval, merged and reranked by Cohere (VectorRAG); few-shot examples guide tool choice.",
            "synthesis_technique": "Instruction-tuned generation conditioned on retrieved contexts; the agent dynamically selects GraphRAG or VectorRAG per query and composes evidence from retrieved chunks and KG results into concise answers; DPO trains the generator to prefer context-grounded outputs; bootstrapped evaluation informs uncertainty.",
            "number_of_papers": null,
            "domain_or_topic": "General scientific literature with emphasis on biomedical and clinical literature (ingests PubMed, ArXiv, Google Scholar records); evaluated on synthetic benchmarks derived from papers about Multimodal LLMs in healthcare (2023-2025).",
            "output_type": "Context-grounded answers to research queries, literature exploration summaries, KG query results, uncertainty estimates; supporting artifacts include retrieved context chunks and Cypher results.",
            "evaluation_metrics": "Faithfulness (F), Answer Relevance (AR), Context Precision (CP), Context Recall (CR); bootstrapped means and standard deviations computed across 12 resamples; task-specific KS/VS (KG vs VS) breakdowns (e.g., VS Context Recall).",
            "performance_results": "Relative improvements over a non-agentic baseline: +0.63 VS Context Recall, +0.56 overall Context Precision, +0.24 VS Faithfulness, +0.12 VS Precision and KG Answer Relevance, +0.11 overall Faithfulness, +0.05 KG Context Recall, +0.04 VS Answer Relevance and overall Precision. Latency: ~2 minutes/query on a consumer machine, ~10 sec with GPU-enabled server.",
            "comparison_baseline": "Non-Agentic RAG: joint semantic search over both vector store and KG (fixed pipeline baseline).",
            "performance_vs_baseline": "The instruction-tuned agent with DPO substantially outperformed the non-agentic baseline across most metrics (see reported numeric gains above), with modest declines in a few KG-specific precision/faithfulness measures.",
            "key_findings": "Dynamically selecting between GraphRAG and VectorRAG per query improves retrieval relevance and downstream generation; combining KG-structured queries and dense full-text retrieval leverages complementary strengths (KG for metadata/multi-hop relations, VS for nuanced full-text content); instruction tuning and small-data DPO meaningfully increase faithfulness and context utilization; bootstrapped evaluation provides transparent uncertainty estimates.",
            "limitations_challenges": "Cypher translation via few-shot prompting can misinterpret complex queries leading to KG errors; synthetic benchmark may not reflect full real-world complexity (figures/tables, modalities); dependence on external APIs (rate limits, data availability); OCR for scanned PDFs not integrated; computational/latency costs on consumer hardware.",
            "scaling_behavior": "No explicit empirical scaling curves reported; authors note latency reduces with GPU server deployment and anticipate RL-based policy optimization or continuous ingestion to improve performance; DPO showed benefits even with 15 preference pairs, but effects of corpus size or larger model sizes are discussed qualitatively rather than quantified.",
            "uuid": "e4387.0",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA",
            "name_full": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "brief_description": "A retrieval-augmented agent that answers research queries by retrieving and synthesizing relevant excerpts from a corpus of journal articles, reported to achieve expert-level performance on multi-document benchmarks.",
            "citation_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "mention_or_use": "mention",
            "system_name": "PaperQA",
            "system_description": "Described as an agentic RAG system applied to a corpus of journal articles that retrieves relevant passages and synthesizes them to answer research queries; cited as prior work demonstrating expert-level multi-document question answering in scientific domains.",
            "llm_model_used": null,
            "extraction_technique": "Passage retrieval from a corpus of journal articles (RAG-style retriever).",
            "synthesis_technique": "Generative synthesis conditioned on retrieved excerpts (multi-document answer synthesis).",
            "number_of_papers": null,
            "domain_or_topic": "Scientific research articles (general scientific literature).",
            "output_type": "Answers to research queries / multi-document summaries.",
            "evaluation_metrics": "Reported as expert-level performance on multi-document benchmarks (specific metrics not provided in this paper).",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "RAG-style agents can achieve high performance on multi-document question answering tasks in scientific domains when grounded in retrieved passages.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4387.1",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "RAG (Lewis et al.)",
            "name_full": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "brief_description": "The RAG paradigm couples a retriever over an external corpus with a generative model conditioned on retrieved passages to improve factual accuracy and provenance in knowledge-intensive tasks.",
            "citation_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "A two-stage architecture where a retriever fetches relevant passages (non-parametric memory) and a sequence-to-sequence generator conditions on those passages to answer queries; originally demonstrated with a neural retriever over Wikipedia.",
            "llm_model_used": "sequence-to-sequence generator (unspecified in this paper's description of prior work)",
            "extraction_technique": "Neural passage retrieval over an external corpus (e.g., Wikipedia).",
            "synthesis_technique": "Generation conditioned on retrieved passages (context-grounded generation).",
            "number_of_papers": null,
            "domain_or_topic": "Open-domain / knowledge-intensive QA (original demonstration used Wikipedia).",
            "output_type": "Context-grounded answers / QA responses.",
            "evaluation_metrics": "Standard QA metrics used in the originating work (not enumerated here).",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Grounding generation in retrieved documents improves factuality and traceability compared to standalone LLMs.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4387.2",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "HybridRAG (Sarmah et al.)",
            "name_full": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
            "brief_description": "A hybrid approach integrating knowledge graphs with vector retrieval in a RAG pipeline to improve information extraction efficiency and exploit structured and unstructured evidence.",
            "citation_title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
            "mention_or_use": "mention",
            "system_name": "HybridRAG",
            "system_description": "Referenced as prior work that combines knowledge-graph queries and vector retrieval within a RAG framework to leverage structured relations and dense semantic content for information extraction tasks.",
            "llm_model_used": null,
            "extraction_technique": "Graph-based queries (KG) plus embedding-based vector retrieval.",
            "synthesis_technique": "Combining structured KG outputs and retrieved text to inform generation (hybrid RAG synthesis).",
            "number_of_papers": null,
            "domain_or_topic": "Information extraction over mixed structured/unstructured corpora.",
            "output_type": "Extracted information / context-grounded responses.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Hybridizing KG and vector retrieval can improve retrieval robustness by leveraging complementary modalities.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4387.3",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "DualRAG",
            "name_full": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering",
            "brief_description": "A dual-process approach that integrates reasoning and retrieval mechanisms to handle multi-hop question answering by combining distinct retrieval strategies and reasoning steps.",
            "citation_title": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering",
            "mention_or_use": "mention",
            "system_name": "DualRAG",
            "system_description": "Cited as an approach that integrates separate retrieval and reasoning processes (dual-process) to better support multi-hop QA, implying a specialized orchestration between retrieval modules and reasoning components.",
            "llm_model_used": null,
            "extraction_technique": "Multi-hop retrieval strategies (details not provided in this paper).",
            "synthesis_technique": "Integrated reasoning over retrieved evidence across hops.",
            "number_of_papers": null,
            "domain_or_topic": "Multi-hop question answering (general QA).",
            "output_type": "Answers requiring multi-hop reasoning.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Dual-process integration of retrieval and reasoning can better address multi-hop questions (as referenced).",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4387.4",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PolyG",
            "name_full": "PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal",
            "brief_description": "A GraphRAG variant focusing on adaptive graph traversal strategies to make graph-based retrieval within RAG more effective and efficient.",
            "citation_title": "PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal",
            "mention_or_use": "mention",
            "system_name": "PolyG (GraphRAG variant)",
            "system_description": "Referenced as work on GraphRAG that employs adaptive graph traversal to improve the efficiency and effectiveness of graph-based retrieval in RAG pipelines.",
            "llm_model_used": null,
            "extraction_technique": "Adaptive traversal of knowledge graphs to retrieve multi-hop relational evidence.",
            "synthesis_technique": "Graph-grounded generation after adaptive retrieval (details not provided here).",
            "number_of_papers": null,
            "domain_or_topic": "Graph-based retrieval for QA/information extraction.",
            "output_type": "Graph-grounded answers / retrieval outputs.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Adaptive graph traversal can make GraphRAG approaches more efficient and better at multi-hop retrieval.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4387.5",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "SemanticScholar/Elicit",
            "name_full": "Semantic Scholar and Elicit (examples of static hybrid RAG pipelines / commercial tools)",
            "brief_description": "Proprietary scholarly search and literature-assistant tools that integrate neural retrieval and summarization but typically rely on fixed retrieval pipelines and proprietary LLM APIs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Semantic Scholar / Elicit (representative tools)",
            "system_description": "Cited as examples of existing literature tools that combine neural retrieval and summarization; characterized as using static, fixed retrieval pipelines and often depending on proprietary LLM APIs without runtime uncertainty quantification.",
            "llm_model_used": null,
            "extraction_technique": "Proprietary neural retrieval and indexing (likely dense retrieval + metadata search; specifics not provided).",
            "synthesis_technique": "Neural summarization/retrieval-augmented outputs (commercial implementations).",
            "number_of_papers": null,
            "domain_or_topic": "Academic literature search and summarization (multi-domain).",
            "output_type": "Summaries, search results, literature overviews.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Existing commercial tools often use single-modality retrieval pipelines and lack adaptive retrieval orchestration and uncertainty quantification.",
            "limitations_challenges": "Proprietary/opaque implementations, static pipelines, limited uncertainty estimates, potential reproducibility concerns.",
            "scaling_behavior": null,
            "uuid": "e4387.6",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Agentic RAG Survey",
            "name_full": "Agentic Retrieval-Augmented Generation: A Survey on Agentic",
            "brief_description": "A survey paper summarizing agentic approaches that orchestrate retrieval and generation components (agents that choose tools) in RAG systems.",
            "citation_title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic",
            "mention_or_use": "mention",
            "system_name": "Agentic Retrieval-Augmented Generation (survey / approach)",
            "system_description": "Cited as a survey of agentic methods where an LLM-based agent plans, decomposes queries, and calls specialized tools (retrievers, KG queries, etc.) to achieve complex tasks in RAG pipelines.",
            "llm_model_used": null,
            "extraction_technique": "Agent-driven selection and orchestration of retrieval tools (GraphRAG, VectorRAG, etc.).",
            "synthesis_technique": "Hierarchical planning and tool composition followed by context-grounded generation.",
            "number_of_papers": null,
            "domain_or_topic": "General RAG/agentic literature (survey across approaches).",
            "output_type": "Survey insights; categorizations of agentic RAG methods.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Agentic orchestration offers flexible, tool-aware pipelines that can dynamically select retrieval modalities and improve task performance versus static pipelines.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4387.7",
            "source_info": {
                "paper_title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
            "rating": 2,
            "sanitized_title": "hybridrag_integrating_knowledge_graphs_and_vector_retrieval_augmented_generation_for_efficient_information_extraction"
        },
        {
            "paper_title": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering",
            "rating": 2,
            "sanitized_title": "dualrag_a_dualprocess_approach_to_integrate_reasoning_and_retrieval_for_multihop_question_answering"
        },
        {
            "paper_title": "PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal",
            "rating": 2,
            "sanitized_title": "polyg_effective_and_efficient_graphrag_with_adaptive_graph_traversal"
        },
        {
            "paper_title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic",
            "rating": 2,
            "sanitized_title": "agentic_retrievalaugmented_generation_a_survey_on_agentic"
        },
        {
            "paper_title": "Retraining language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "retraining_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Ragas: Automated Evaluation of Retrieval Augmented Generation",
            "rating": 1,
            "sanitized_title": "ragas_automated_evaluation_of_retrieval_augmented_generation"
        }
    ],
    "cost": 0.016065,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review
30 Jul 2025</p>
<p>Aditya Nagori 
Ricardo Accorsi Casonatto 
Abhinav Manikantha 
Sai Cheruvu 
Rishikesan Kamaleswaran </p>
<p>Department of Surgery
Department of Anesthesiology
Duke University School of Medicine Durham
North Carolina
United States</p>
<p>Department of Surgery
Department of Anesthesiology
Faculty of Technology
Duke University School of Medicine Durham
North Carolina
United States</p>
<p>Federal District
University of Brasilia Brasilia
Brazil Ayush Gautam</p>
<p>Department of Surgery
Department of Anesthesiology
Duke University School of Medicine Durham
North Carolina
United States</p>
<p>Indian Institute of Technology Goa Goa
India</p>
<p>Department of Surgery
Department of Anesthesiology
Duke University School of Medicine Durham
North Carolina
United States</p>
<p>Birla Institute of Technology &amp; Science Pilani Hyderabad
India</p>
<p>Department of Electrical and Computer Engineering
Department of Surgery
Department of Anesthesiology
Duke University Pratt School of Engineering
Duke University School of Medicine Durham
North Carolina
United States</p>
<p>Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review
30 Jul 2025F68216A043B3B9B217CD96B49E167B81arXiv:2508.05660v1[cs.IR]CCS CONCEPTSInformation systems  Information retrieval;Computing methodologies  Artificial intelligence AI Agent, Literature Review, Graph Database, Retrieval Augmented Generation (RAG), Instruction Tuning, Synthetic Benchmarks
The surge in scientific publications challenges traditional reviews, requiring tools that integrate structured metadata and full-text analysis.Hybrid retrieval augmented generation (RAG) systems blend graph queries with vector search but are typically static (fixed pipelines per prompt), depend on proprietary services, and omit uncertainty estimates.In this work we have developed an agentic approach which encapsulates the hybrid RAG pipeline within an autonomous agent that can (1) dynamically reason about which retrieval mode-GraphRAG or VectorRAG-is best suited for each query, (2) adjust instruction-tuned generation on the fly to the researcher's needs, and (3) incorporate uncertainty quantification at runtime.This dynamic orchestration improves relevance, mitigates hallucinations, and ensures transparent, reproducible workflows.Our pipeline ingests bibliometric open-access data from PubMed, ArXiv and Google Scholar APIs, constructs a Neo4j knowledge graph (KG) with citation relationships, and embeds publicly available full-text PDFs into a FAISS vector store (VS) using all-MiniLM-L6-v2 model.A Llama-3.3-70B-versatile agent dynamically selects between GraphRAG, which translates user queries into Cypher searches over the KG, and VectorRAG, which combines sparse and dense retrieval methods with re-ranking to provide relevant information.Instruction tuning refines domain-specific generation, and bootstrapped evaluation (12 resamples) yields standard deviation for evaluation metrics.The efficacy of the agentic system is evaluated using synthetic benchmarks tailored to mimic real-world scenarios.The Instruction-Tuned Agent with Direct Preference Optimization (DPO) substantially outperforms the baseline retrieval system, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision.Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision.These improvements highlight the model's enhanced ability to retrieve, reason over, and integrate information from heterogeneous sources.By dynamically selecting the optimal retrieval pathway and quantifying uncertainty, our open-source framework substantially enhances the accuracy and robustness of literature exploration, laying a scalable foundation for autonomous agentic scientific knowledge discovery.</p>
<p>INTRODUCTION</p>
<p>The volume of scientific literature is growing at an unprecedented rate, making it increasingly difficult for researchers to stay informed.Recent bibliometric studies have confirmed that scientific publication output grows exponentially, roughly doubling every 15 years [3].Traditional manual literature reviews are time-consuming and prone to information overload, as no individual can comprehensively read and synthesize millions of papers published annually.Efforts to ease this burden have begun to leverage artificial intelligence for automating parts of the literature review process.For example, text-mining and machine learning techniques have been applied to assist systematic reviews by prioritizing relevant citations or even auto-generating summaries [1,8,37], but they were limited in scope and often relied on keyword matching or topic modeling rather than true content understanding.</p>
<p>Meanwhile, Large Language Models (LLMs) have recently emerged as powerful aids for text understanding and generation.LLMs can, in principle, read and summarize large collections of papers, helping researchers identify themes and connections given sufficient context.Previous works demonstrate a growing interest in using LLMs to streamline scholarly workflows [26].However, significant caveats remain.LLMs operating in isolation -i.e., without access to external knowledge -often suffer from hallucinations [9,17].These observations underscore that relying on standalone LLMs for literature review is perilous, and integrating them with reliable retrieval of actual literature is essential for accuracy and trustworthiness.</p>
<p>Another major area of development has been Retrieval-Augmented Generation (RAG), which enhances language models by grounding their outputs in retrieved documents.This approach has been shown to improve factual accuracy and offer source provenance.For instance, [22] introduced a RAG model that conditioned a sequenceto-sequence generator on Wikipedia passages, yielding state-of-theart results on open-domain question answering.Similarly, tools like Semantic Scholar have begun integrating neural retrieval and summarization, reflecting a broader shift towards augmenting search with generative capabilities.Despite these advances, current systems often rely on fixed, one-shot retrieval pipelines and support only a single retrieval modality -typically semantic vector search [5].This rigidity limits their effectiveness for complex scientific queries, which may require combining different strategies, such as citation graph traversal or full-text semantic search.Moreover, many AI-powered literature tools are proprietary or opaque, raising concerns around reproducibility and trust.</p>
<p>Accordingly, this work is motivated by three identified key gaps: the accelerating volume of scientific literature, the limitations of static pipelines and manual review methods, and the untapped opportunity of combining structured knowledge with adaptive language models to create a robust literature review assistant.The objective is to design a literature review framework that is precise, interpretable, and adaptive -capable of dynamically orchestrating distinct retrieval strategies, grounding outputs in up-to-date sources, quantifying uncertainty, and functioning with minimal human intervention.</p>
<p>The paper begins with a background and review of related work in literature review automation, retrieval-augmented generation, and language model agents.The methods section then introduces the proposed system, detailing both the hybrid retrieval components and the agentic orchestration framework.Experimental procedures describe the evaluation setup used to test the system's performance across various scientific information tasks.Results are then presented and analyzed, followed by a discussion of key findings, limitations, and potential directions for improvement.The paper concludes by outlining broader implications for future research support systems.</p>
<p>BACKGROUND AND RELATED WORK 2.1 Traditional Literature Review Methods</p>
<p>Manual literature reviews, including systematic reviews and metaanalyses, have long been the gold standard for synthesizing evidence.Researchers meticulously search databases, screen hundreds or thousands of titles and abstracts, and extract and compare data from relevant studies under well-defined protocols.While rigorous, this process is time-consuming and prone to information overload.The volume of scientific publications has grown exponentially, to the point that "researchers cannot keep up with the volume of articles being published each year" [19].Even systematic reviewers following established guidelines struggle with the sheer amount of new literature that needs to be incorporated; one study noted that traditional manual methods cannot keep pace with the "ensuing workload," leading most systematic reviews to become partially outdated by the time of publication [19].This overload not only slows down the review process but also raises the risk of missing pertinent information.</p>
<p>In response, semi-automated strategies have emerged to assist with literature reviews.Reference management software (e.g., End-Note, Zotero) and academic search engines with advanced query capabilities help organize citations and perform keyword searches more efficiently.Tools for automatic title/abstract screening and study selection have also been developed-for example, applications like Rayyan and Abstrackr use machine learning to prioritize relevant abstracts for review.These tools can expedite specific steps, such as filtering out obviously irrelevant papers, but significant challenges remain.A scoping review of systematic review automation tools found numerous prototypes (e.g., LitSuggest, RobotAnalyst) that show potential to assist in screening and data extraction, "however, they are not without limitations" [15].Many require technical expertise or only handle a portion of the workflow.Critical higher-level tasks-namely synthesizing findings across studies and drawing nuanced conclusions-still demand intensive human effort.Furthermore, human error and bias continue to be concerns: crafting comprehensive search queries is difficult, and even small omissions can lead to missing key studies.In fact, an analysis of published systematic reviews found that 92.7% had search strategy errors, with the most common issue being missing relevant terms (affecting recall in 78% of reviews) [16].Such findings underscore that current manual and semi-automated methods often yield reviews that are labor-intensive, error-prone, and potentially incomplete.</p>
<p>Graph Databases in Scholarly Research</p>
<p>Graph databases have become an important tool for modeling and exploring relationships in scholarly data.In a graph-based representation of the literature, nodes can represent entities such as papers, authors, journals, or key scientific concepts, while edges capture relations like "Paper A cites Paper B, " "Author X co-authored with Author Y, " or "Concept Z is related to Concept Y. " This format naturally mirrors the structure of scholarly knowledge.For example, citation networks form directed graphs, and author collaboration networks form co-authorship graphs.Knowledge graphs (KGs) can also encode higher-level connections: a "topic graph" might link papers to the scientific topics or methodologies they involve.Graph databases (such as Neo4j) are well-suited for storing and querying such networks due to their efficient relationship traversal capabilities.</p>
<p>The use of graph-based models in scholarly research offers powerful capabilities for literature mining and knowledge discovery.Iancarelli et al. (2022) constructed a citation graph of the human aggression research literature to map its thematic structure; their analysis discovered distinct clusters (e.g., media and video games, hormonal influences) and pinpointed key "bridging" papers linking communities [15].Projects like the Open Research Knowledge Graph (ORKG) represent core contributions of papers in a structured form, making scholarly knowledge machine-actionable [16].By curating papers into a KG of concepts and relationships, one can query questions like "which trials used method X and achieved outcome Y?"-a task that is difficult with unstructured text.</p>
<p>RAG</p>
<p>LLMs have shown impressive ability to generate fluent text, but their knowledge is bounded by training data, leading to hallucinations.RAG grounds LLM outputs in external documents.In a RAG system, a retriever first fetches relevant passages from a corpus, and a generator conditions on those passages to produce responses [22].This combination of non-parametric and parametric memory spaces yields significant improvements in factual accuracy and traceability.Lewis et al. demonstrated RAG by coupling a neural passage retriever over Wikipedia with a sequence-to-sequence generator, enabling on-the-fly fact lookup [22].Empirical evaluations show that RAG models generate more specific, diverse, and factual language than comparable non-retrieval models.</p>
<p>In scientific domains, RAG has been applied to literature questionanswering and summarization.The PaperQA agent uses a corpus of journal articles to answer research queries by retrieving and synthesizing relevant excerpts, achieving expert-level performance on multi-document benchmarks [21].Han et al. survey the potential of RAG for automating systematic literature reviews, highlighting its ability to retrieve and summarize key studies with fidelity to source material [14].</p>
<p>Instruction Tuning in Natural Language Processing</p>
<p>Instruction tuning fine-tunes pretrained LLMs on datasets of instruction-response pairs to improve adherence to user directives.Wei et al. introduced FLAN, fine-tuning T5 on a mix of tasks as instructions, yielding strong zero-shot performance across diverse benchmarks [38].Sanh et al. released T0, an 11B-parameter instructiontuned model trained on open-source prompts, demonstrating robust cross-task generalization [30].Alpaca, built on LLaMA-7B, replicated GPT-3.5-likeinstruction-following capabilities with only 52K self-generated examples [36].Such models better execute scholarly tasks like summarization and comparison in scientific contexts.</p>
<p>Related Approaches</p>
<p>Static hybrid RAG pipelines-employed by tools like Elicit and Semantic Scholar-combine retrieval and generation in fixed sequences, often relying on proprietary LLM APIs and lacking uncertainty quantification.These systems cannot adapt when initial retrieval is insufficient and do not provide confidence estimates.Our Agentic Hybrid RAG framework addresses these gaps by: (1) Dynamically selecting between graph-based and vectorbased retrieval modes.( 2) Using an instruction-tuned agent to plan and decompose queries into tool calls (e.g., graph queries, vector search).( 3) Estimating uncertainty at runtime to flag low-confidence or conflicting findings [23].This agentic approach enables flexible, transparent, and trustworthy AI-assisted literature synthesis.</p>
<p>METHODOLOGY</p>
<p>A novel fully open source Python-based interface was developed to extract and structure data from multiple electronic databases, organizing it simultaneously into a KG and a vector store (VS).This hybrid storage architecture was designed to leverage the strengths of both Cypher-based querying, as well as vector search, enabling an AI tool agent to dynamically select the most appropriate strategy based on a given prompt.</p>
<p>The proposed pipeline encompasses the steps of collection, filtering, preprocessing and analysis of publications through a questionanswering (QA) format, resulting in the automated gathering of relevant insights from academic literature after a search query with optional date range is shared by the researcher.The general concept of the pipeline is shown in Figure 1.Firstly, publicly available bibliometric data is collected from the PubMed, ArXiv, and Google Scholar APIs.The initial data extracted from the publications indexed in these databases include DOI, title, abstract, publication year, authors, PDF url and source database.The obtained results are then consolidated into a single dataframe, and entries with missing values are discarded.Subsequently, duplicate records based on the DOI and title subset are also removed.</p>
<p>During the filtering stage, five keywords are extracted from each article by applying a TF-IDF vectorizer to the concatenated text of the title and abstract that accounts for both unigrams and bigrams.The extracted keywords are subsequently processed through lemmatization and lowercasing and are used for two distinct purposes: (1) to compute their cosine similarity (CS) score against the keywords of the initial search query, and (2) to be incorporated into the KG as individual nodes, establishing first-degree connections with their respective source articles.</p>
<p>CS scores are calculated as:
 (,  ) =      (1)
where In that way, only studies with a CS score above the third quartile -i.e., those in the top 25% of the distribution -were retained to ensure the selection of the most relevant studies for the research.The use of CS scores for this purpose was motivated by its widespread application in the literature for text comparison and relevance filtering [10,13,20,29].
   =  =1    
After selecting the publications, the pipeline downloads and parses open-access full texts into chunks, storing them as vectors in a VS.Additionally, the initial extracted bibliometric data is structured as a node in the KG, with a direct relationship to its source article.The exception goes for title, DOI and abstract, which are stored as attributes inside the paper node they belong to.For the references section within the full texts, the algorithm collects the DOIs available and also stores them as individual nodes in the KG, enabling efficient tracking of the cited sources and facilitating the detection of co-citation networks between the reviewed papers.Once the setup is concluded, a chat interface is built upon an agent powered by the LLaMA-3.3-70B-versatilemodel, which is capable of invoking two distinct functions: Cypher-based retrieval and similarity-based retrieval.To ensure accurate tool selection, we employed a total of 10 few-shot examples-5 for each method-each illustrating the full reasoning chain: the initial user question, the corresponding retrieval tool, the retrieved context, and the final answer generated.This structured prompting strategy guides the agent's decision-making and promotes consistent performance across different query types.</p>
<p>The Cypher retrieval function is able to convert a question into a Cypher query and then apply it to the KG.Meanwhile, the similarity retrieval function performs an ensemble approach that combines and reranks both semantic search and keyword search to find the chunks that are most closely related to the user's question.Based on the agent's decision, the appropriate function retrieves the necessary context and directly generates a meaningful answer through Mistral-7B-Instruct-v0.3 model, reducing the risk of hallucination and ensuring relevance to the original question.On a consumergrade machine, this results in a latency of approximately 2 minutes per query, primarily due to hardware limitations.However, when deployed on suitable server infrastructure with GPU acceleration, the latency is significantly reduced to around 10 seconds per query.</p>
<p>Graph Database Integration</p>
<p>A KG structure was chosen due to its high efficiency in mining, organizing and managing knowledge from large-scaled data [4].That architecture leverages the existing relationships and hierarchies within the data to enhance multi-hop reasoning and contextual enrichment, which is particularly useful for tasks requiring relational understanding [32].Figure 2  Through that schema, nodes for author, database, related keywords, publication year and citation can be shared across multiple articles, ensuring the creation of interconnected networks within the documents.Furthermore, the Publication Node contains individual internal attributes, such as the DOI, title and abstract, which can be retrieved for further analysis.By using bibliometric data in the KG, the pipeline expands its capabilities for global queries over structural and descriptive attributes, such as author collaboration patterns, database distribution, publication timelines, and citation relationships, offering a broader overview of the research landscape.</p>
<p>Vector Database Integration</p>
<p>VSs have been widely integrated into RAG systems due to their ability to enhance semantic understanding and context-awareness through the use of dense retrieval models [32].This retrieval process is fundamentally similarity-based, wherein similarity scores are computed to quantify the degree of resemblance between feature vectors [28].</p>
<p>Following this principle and based on previous approaches, the proposed pipeline recursively segments the texts extracted from the papers into batches of 2,024 characters with an overlap of 50 characters between consecutive chunks [31].These segments are then embedded using all-MiniLM-L6-v2 model, and the resulting vectors are stored and indexed through the FAISS library, enabling efficient similarity-based retrieval.The integration of a vector database complements the KG's relational perspective by supporting semantic-level queries while also facilitating a deeper, contentfocused exploration of individual articles.</p>
<p>RAG</p>
<p>This work uses an intermediary tool agent capable of dynamically selecting between graph-based retrieval and a hybrid strategy that combines dense and sparse vector retrieval with reranking.By leveraging the strengths of GraphRAG and VectorRAG, the approach enhances both retrieval and generation performance, adapting the method to the nature of each prompt.The natural language query is supplied to the LLM together with the KG schema, along with a set of thirty input-output example pairs used to guide the generation of the corresponding Cypher query.This strategy, known as few-shot learning, facilitates quicker in-context learning, thereby producing better task-specific performances [33].Once generated, Cypher query is executed on the KG, which is hosted on a Neo4j instance, and the retrieved results are subsequently passed back to the LLM for final response formatting.</p>
<p>VectorRAG.</p>
<p>Meanwhile, the VectorRAG functionality employs an ensemble retrieval strategy that combines a keywordbased retriever with a semantic search retriever.The retrieved text chunks are merged and then reranked based on their relevance using Cohere's rerank-english-v3.0 model.At this stage, the query and retrieved passages are concatenated and processed by Cohere's transformer-based reranker, pre-trained on a large-scale corpus.This model assigns refined relevance scores through deep crossattention mechanisms, improving retrieval quality by prioritizing the most informative results [39].The top-ranked passages are then provided to the LLM as contextual input, which generates and formats the final response.Figure 4 illustrates the workflow.</p>
<p>The keyword search applies the BM25 algorithm, which treats queries and documents as sparse bag-of-words vectors and matches them in token-level.Considering a given query , the scores for each chunk  are calculated as follows:
BM25(, ) =    IDF()   (, )  ( 1 + 1)  (, ) +  1  1   +   | | avgdl (2)
where  (, ) denotes the frequency of the word  in the chunk , | | represents the total number of words in the chunk, and  is the average chunk length across the collection.The parameters  1 and  are hyperparameters, while  () is the inverse document frequency of the word w, designed to down-weight common terms.Higher BM25 scores reflect a greater estimated relevance between the query and the candidate chunk.While BM25 efficiently captures lexical similarity through exact term matching, it often struggles to account for semantic variations in language.To address this limitation and enhance retrieval robustness, sparse and dense retrievers are often combined, as they tend to complement each other's strengths [7].Accordingly, alongside the sparse retrieval approach, semantic search is employed to compute similarity based on the 2 score, also referred to as the Euclidean distance, between two dense, continuous semantic vectors.The 2 distance between vectors  and  is defined as:</p>
<p>Natural Language Query
Semantic Search Keyword Search Ensemble Retrieval Rerank LLM Interpret Generated Response Format2(x, y) =   =1 (     ) 2(3)
Lower 2 scores indicate higher semantic similarity, meaning that the vector representations of the query and the candidate chunk are positioned closer together in the embedding space.By leveraging both sparse and dense retrieval strategies, the framework combines the exact lexical matching capabilities of sparse search with the contextual understanding provided by dense semantic representations.In this setup, both methods independently return the top five candidate chunks, which are subsequently ranked and compressed into a final set of results through the previously mentioned re-ranking process.</p>
<p>Direct Preference Optimization (DPO)</p>
<p>We applied Direct Preference Optimization (DPO) directly to the response generator in our RAG pipeline, using just 15 high-quality, human-annotated preference pairs.This led to clear improvements in both faithfulness and context retrieval metrics compared to models trained without DPO.</p>
<p>DPO explicitly teaches the response generator to prefer answers grounded in the retrieved context, directly aligning model outputs with human judgment.This reduces hallucinations and encourages the model to rely on external evidence rather than internal knowledge.Even with a small dataset, DPO efficiently guides the generator to produce more accurate and contextually relevant responses, explaining the observed gains in both faithfulness and context utilization.</p>
<p>Synthetic Benchmark Evaluation</p>
<p>To the best of our knowledge, and consistent with [31], no publicly available benchmark datasets currently exist for evaluating both VectorRAG and GraphRAG approaches in general domains.Consequently, a custom dataset was developed.Synthetic data was selected due to its capacity to control question difficulty and cover a broad range of retrieval scenarios while also addressing the challenges associated with limited availability of real-world data at relatively lower costs [18].</p>
<p>The synthetic benchmark was designed to evaluate the agentic framework across both types of retrieval.It consists of questions that can only be answered through the correct tool call -i.e., answers available exclusively in either VS or KG.By inputting the search query '("Multimodal Large Language Model" OR MLLM OR MM-LLM<em> OR "Information Fusion" OR "Multimodal Learn</em>" OR "Joint Learn<em>" OR "Cross Learn</em>") AND (Healthcare OR Medicine OR Health)' into the pipeline and selecting the 2023-2025 date range, the retrieved papers were used to generate 40 question-answer pairs -20 tailored specifically for VectorRAG and 20 for GraphRAG -thereby ensuring a balanced assessment across both retrieval methods.</p>
<p>EXPERIMENTAL SETUP</p>
<p>This section provides a detailed description of the experimental setup, including the characteristics of the evaluative questions, the metrics employed, and the procedure followed during the experiment.</p>
<p>Datasets and Benchmarks</p>
<p>4.1.1VectorRAG Benchmark.To validate the pipeline's VectorRAG capability, a sample of 20 text chunks was randomly selected from the final batch of generated chunks extracted from the papers' full text.Each chunk was then provided as input to a LLaMa 3.3 instance, which was asked to generate a question given the prompt "Generate a question that can only be answered from the given context.Don't create generic questions.Don't mention specific figures, tables, sections or even the actual document provided.Focus only on its content and its main ideas." accompanied by three output examples.As a result, the generated questions were stored alongside their corresponding chunks for further analysis.</p>
<p>GraphRAG Benchmark.</p>
<p>A knowledge graph  = {(, , ) | ,   ,   } is composed of factual triples, where  denotes the subject,  the predicate, and  the object.The set of all subjects and objects defines the entity set , while the collection of all predicates forms the relation set . Building upon this structure, five different types of questions were designed to comprehensively evaluate the pipeline's GraphRAG capabilities, extending the four original variations proposed by [24].The question types are described in Table 1.</p>
<p>A total of four samples for each question type were generated by randomly selecting nodes and relationships, then querying the results using template Cypher queries tailored to each question pattern in the KG.The resulting questions and answers were then combined with the 20 VectorRAG data points to consolidate the final test set.</p>
<p>Evaluation Metrics</p>
<p>In order to assess the efficacy of the proposed framework, a comparative analysis was conducted among three approaches in a controlled experimental setup: (i) Non-Agentic RAG, which performs vector search on both VS and KG and combines its results (serving as the baseline); (ii) Agentic RAG; and (iii) Fine Tuned Agentic RAG.A comprehensive set of evaluation metrics was implemented to capture various aspects of agent's output quality, with a particular focus on faithfulness, answer relevance, context precision and context recall [12].Each metric provides distinct insights into the system's strengths and limitations.</p>
<p>Faithfulness (F) measures the extent to which the generated answer remains grounded in the provided context.To compute this metric, statements are extracted from the generated answer and compared against the retrieved context.The final score is calculated as  = Answer relevance (AR) quantifies how effectively the answer addresses the original question.The methodology involves generating auxiliary questions (  ) solely based on the answer, then computing their similarity scores to the original question ().AR is ultimately computed as  = 1   =1 cos(,   ), aiming to assess how closely the generated answer aligns with the initial inquiry.</p>
<p>Context precision (CP) evaluates the relevance of the context elements retrieved to support the generated answer.Specifically, it measures the proportion of relevant items among the top-ranked context chunks.Context precision at  is defined as follows, where  represents the number of context chunks considered and   is the relevance indicator at rank :
CP =  =1 (Precision@k    )
Total number of relevant items in the top  results (</p>
<p>Precision@k = true positives@k true positives@k + false positives@k (</p>
<p>Finally, context recall (CR) checks if relevant information has been fully addressed, identifying whether any important elements have been omitted.Specifically, it assesses how many statements from the ground truth are supported by the retrieved context.CR is computed as  =</p>
<p>Experimental Procedures</p>
<p>To ensure the statistical significance of the results, the evaluation pipeline employed the bootstrap technique, a resampling method that repeatedly draws samples from the original data to better estimate the underlying distribution [25].The analysis was conducted over 12 resamplings, each consisting of 20 randomly selected questions -10 corresponding to VectorRAG and 10 to GraphRAG -with the significance level () set at 0.05.Following resampling, the mean and standard deviation of the results were computed, and the margin of error was estimated using the t-distribution, accounting for the finite sample size.Specifically, the margin of error (ME) was calculated as:
ME =  /2,     (6)
Table 1: Types of Questions Used for GraphRAG Evaluation.Adapted from [24].</p>
<p>Question Type Description</p>
<p>Subject Centered (&lt; , * , * &gt;) Questions targeting the subject without considering specific predicates or connected objects.The goal is to answer questions based solely on the node properties.Example: "What is the paper 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review' about?"Object Discovery (&lt; , , * &gt;)</p>
<p>Questions targeting an object, given a subject and its predicate.The goal is to identify the node connected to the subject through the defined relationship.Example: "In which year was the paper 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review' published?"Predicate Discovery (&lt; , * ,  &gt;)</p>
<p>Questions targeting the predicate, given a subject and an object.The goal is to identify the relationship between two entities.Example: "How is ArXiv related to the paper 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review'?" Fact Check (&lt; , ,  &gt;)</p>
<p>Questions targeting the existence of a predicate given a subject and an object.The goal is to validate the connection between two entities.Example: "Is the paper 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review' represented by the keyword 'healthcare'?"Indirect Relationship Discovery
(&lt;  1 , * , _, * ,  2 &gt;)
Questions targeting the existence of a connection between two entities, which are indirectly related through an intermediate node.Example: "Is the keyword 'agent' associated with any paper published in 2025?"</p>
<p>where  /2,   is the t-critical value for a two-tailed test at the given significance level and degrees of freedom   =   1,  is the standard deviation of the bootstrap estimates, and  is the number of bootstrap samples.</p>
<p>RESULTS AND ANALYSIS</p>
<p>The evaluation metrics are presented in Figure 5, providing a comparative analysis among three configurations: the baseline, the proposed agentic framework, and an enhanced fine-tuned agentic version using DPO.The baseline is defined as a non-agentic approach that performs a joint semantic search over both the VS and the KG, integrating the retrieved results to generate a final response.The agentic framework introduces dynamic tool selection guided by an AI agent, while the DPO-enhanced variant refines the final output based on preference-aligned optimization for improved response quality.For clarity, the evaluation is segmented by question scope: KG-specific, VS-specific, and a combined category referred to as overall.The introduction of the fine-tuned agentic model (with direct preference optimization), in green, led to measurable improvements over the original agentic approach (orange).Specifically, it yielded a 0.19 gain in KG F score, a 0.09 gain in overall F, along with enhancements of 0.08 in KG CR, 0.07 in KG P, and 0.05 in KG AR.</p>
<p>Notably, these improvements suggest a more accurate and complete generated response.On the VS side, the CR metric remained unchanged, while slight reductions were observed: 0.10 in P, 0.02 in F, and 0.01 in AR.</p>
<p>When compared to the baseline approach, the fine-tuned agentic model demonstrates substantial performance gains across nearly all evaluation metrics.Most notably, it achieves a 0.63 improvement in VS CR and a 0.56 gain in overall CP, highlighting its enhanced ability to effectively retrieve and integrate information from both retrievers.Additional improvements include a 0.24 increase in VS F, 0.12 in both VS P and KG AR, and 0.11 in overall F. Modest yet consistent gains were also observed in KG CR (0.05), VS AR (0.04), and overall P (0.04).These results underscore the model's superiority in producing coherent and contextually accurate responses when compared to the non-agentic RAG baseline.Despite these advances, slight decreases were recorded in KG P (0.04) and KG F (0.03), suggesting that while symbolic reasoning has generally improved, there may still be room for refinement in generating precise Cypher outputs.</p>
<p>DISCUSSION</p>
<p>Our results demonstrate that encapsulating hybrid RAG within an agentic framework-dynamically selecting between GraphRAG and VectorRAG-yields substantial improvements over static, nonagentic baselines.Instruction tuning of the Mistral-7B-Instruct-v0.3 model further enhances precision and recall by aligning generation with domain-specific QA tasks, while bootstrapped evaluation provides transparent error estimates and confirms the statistical significance of our gains (Standard Error  0.10) [2,11,25].</p>
<p>Based on the results obtained, it becomes evident that agentic approaches hold substantial potential for enhancing literature review processes.By significantly outperforming traditional vectorsearch RAG pipelines, our framework enables research questions to be explored in a more automated, efficient, and context-aware manner-ultimately delivering richer insights to researchers with reduced manual effort [32].</p>
<p>A key strength of our approach is its ability to leverage complementary retrieval modalities.GraphRAG excels at structured, metadata-driven queries (e.g., author collaborations, publication timelines), whereas VectorRAG better captures nuanced, full-text information [31].The agent's dynamic reasoning about which mode to invoke per query avoids the "one-size-fits-all" limitations of fixed pipelines and mitigates hallucinations by grounding each response in the most appropriate source.</p>
<p>The incorporation of Direct Preference Optimization (DPO) yielded encouraging results, particularly in tasks requiring structured querying.However, the reduced performance in metrics (Faithfulness and Precision) on KG-specific questions suggests that additional instruction tuning or DPO-focused examples may be necessary to fully unlock the pipeline's capabilities in information retrieval [27].This highlights an opportunity to further tailor the model's decisionmaking process in favor of more accurate and context-sensitive retrieval strategies.</p>
<p>Despite these advances, several avenues exist for further enhancement:</p>
<p>Fine-Tuning the Cypher Translator.GraphRAG currently uses few-shot prompting to generate Cypher, which can misinterpret complex queries.Fine-tuning a model on a curated set of (NLQ, Cypher) pairs-or leveraging LLM function-calling APIs-should reduce translation errors and boost multi-hop recall [33].</p>
<p>Integrating Optical Character Recognition for Broader Coverage.Incorporating an Optical Character Recognition (OCR) pipeline (e.g., Tesseract [34]) would allow processing of scanned or nondigitally structured documents, expanding context coverage and enhancing the KG.</p>
<p>Reinforcement Learning for Policy Optimization.RL (e.g., RLHF or reward-model fine-tuning) could learn an optimal mix of GraphRAG and VectorRAG based on end-task rewards [6,35].While full RL is compute-heavy, offline RL or model distillation techniques may enable these gains with moderate resources.</p>
<p>Limitations and Future Directions.Our synthetic benchmark-while balanced between KG-and VS-specific questions-may not capture the full complexity of real scientific inquiries, such as multi-modal reasoning over figures and tables [7].Dependence on external APIs (PubMed, ArXiv, Google Scholar) also introduces variability in data availability and rate limits.Future work should validate performance on domain-specific corpora (e.g., clinical trials, patent literature), integrate table-and figure-aware retrieval, and support interactive, user-in-the-loop refinement.Continuous ingestion of new publications and active learning for query routing could further enhance adaptability and coverage [18].</p>
<p>In summary, our open-source agentic hybrid RAG framework represents a significant advance toward autonomous, reliable scientific literature review.By combining dynamic retrieval selection, instruction tuning, DPO insights and rigorous uncertainty quantification-and by charting a path toward RL-based policy optimization, we establish a scalable foundation for next-generation knowledge discovery tools.</p>
<p>CONCLUSION</p>
<p>In this work, we introduced an open-source, agentic hybrid RAG framework that marries a Neo4j knowledge graph, a FAISS vector store, LLaMA-3.3-70B-versatile, and Direct Preference Optimization on Mistral-7B-Instruct-v0.3 to automate scientific literature review.By dynamically choosing between GraphRAG and Vector-RAG for each query and quantifying uncertainty via bootstrapped evaluation, the Instruction-Tuned Agent with Direct Preference Optimization (DPO) delivered substantial gains over the non-agentic baseline: a +0.63 increase in VS Context Recall and a +0.56 increase in overall Context Precision, alongside improvements of +0.24 in VS Faithfulness, +0.12 in both VS Precision and KG Answer Relevance, +0.11 in overall Faithfulness, +0.05 in KG Context Recall, and +0.04 in both VS Answer Relevance and overall Precision.</p>
<p>The bootstrapped confidence estimates confirm the robustness of these gains, while our fully Python-based, Dockerizable implementation ensures transparency and reproducibility.Looking ahead, we plan to refine Cypher translation, expand instruction-tuning datasets, integrate OCR for non-structured sources, and explore lightweight reinforcement learning to further optimize retrieval and generation policies, paving the way for truly autonomous, scalable knowledge discovery.</p>
<p>SAFE AND RESPONSIBLE INNOVATION STATEMENT</p>
<p>Our agentic RAG framework processes only publicly available bibliometric records and respects all API usage policies, thereby safeguarding data privacy.We proactively mitigate bias by retaining diverse publication sources and quantifying uncertainty to flag low-confidence outputs.</p>
<p>The system is open-source to promote transparency and community auditing.To prevent misuse, such as automated propagation of incorrect summaries-we integrate rigorous bootstrapped evaluation and encourage human oversight in deployment.We also design for inclusivity by supporting multiple research domains and languages where APIs permit.By embedding these safeguards, we aim to foster ethical and equitable innovation in agentic literature review tools.</p>
<p>Figure 1 :
1
Figure 1: Overview of the end-to-end pipeline illustrating how the user query is processed and a response is generated.</p>
<p> 2 
2
represents the dot product of the two vectors,   =   =1  2  is the norm of  , and   =   =1 is the norm of  .CS values lie in the range [1, 1], where  = 1 denotes perfect alignment and  = 1 indicates diametrically opposite directions.</p>
<p>Figure 2 :
2
Figure 2: KG structure overview highlighting metadata entities and relationships captured in the pipeline.</p>
<ol>
<li>3 . 1
31
GraphRAG.The GraphRAG mechanism operates by leveraging a Cypher-based function that translates natural language queries into Cypher queries-a declarative graph query language used to interact with graph databases-which are then executed against the KG database.The overall process is illustrated in Figure3.</li>
</ol>
<p>Figure 3 :
3
Figure 3: Workflow of KG retrieval, translating natural queries into Cypher, querying the graph, and generating answers via LLM.</p>
<p>Figure 4 :
4
Figure 4: VS retrieval workflow combining keyword and semantic search, followed by reranking and LLM-based answer generation.</p>
<p>where | | denotes the number of statements supported by the context, and | | is the total number of statements.</p>
<p>where |  | denotes the number of ground truth statements supported by the retrieved context, and | | represents the total number of ground truth statements.</p>
<p>Figure 5 :
5
Figure 5: Model comparison on faithfulness (F), answer relevance (AR), context precision (CP), and context recall (CR) for baseline, agentic, and fine-tuned agentic setups.</p>
<p>CODE AVAILABILITYOur up-to-date code is available in the Github project repository https://github.com/Kamaleswaran-Lab/Agentic-Hybrid-Rag
Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error. Alexandra Bannach-Brown, Piotr Przybya, James Thomas, Sophia Andrew Sc Rice, Jing Ananiadou, Malcolm Robert Liao, Macleod, Systematic reviews. 82019. 2019</p>
<p>An empirical investigation of statistical significance in NLP. Taylor Berg-Kirkpatrick, David Burkett, Dan Klein, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningAssociation for Computational Linguistics2012</p>
<p>Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Robin Lutz Bornmann, Rdiger Haunschild, Mutz, Humanities and Social Sciences Communications. 82021. 2021</p>
<p>A review: Knowledge reasoning over knowledge graph. Xiaojun Chen, Shengbin Jia, Yang Xiang, 10.1016/j.eswa.2019.112948Expert Systems with Applications. 1411129482020. March 2020</p>
<p>DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering. Rong Cheng, Jinyi Liu, Yan Zheng, Fei Ni, Jiazhen Du, Hangyu Mao, Fuzheng Zhang, Bo Wang, Jianye Hao, arXiv:2504.182432025. 2025arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Christiano Paul, Advances in Neural Information Processing Systems. 2017. 2017</p>
<p>Yung-Sung Chuang, Wei Fang, Shang-Wen, Wen-Tau Li, James Yih, Glass, 10.48550/arXiv.2305.17080arXiv:2305.17080Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering. 2023</p>
<p>Enhancing Industrial Productivity Through AI-Driven Systematic Literature Reviews. Jaqueline Coelho, Guilherme Bispo, Guilherme Vergara, Gabriela Saiki, Andr Serrano, Li Weigang, Clovis Neumann, Patricia Martins, Welber Santos De Oliveira, Angela Albarello, Ricardo Casonatto, Patrcia Missel, Roberto Medeiros Junior, Jefferson Gomes, Carlos Rosano-Pea, Caroline F Da Costa, 10.5220/0012235000003584Proceedings of the 19th International Conference on Web Information Systems and Technologies -WEBIST. the 19th International Conference on Web Information Systems and Technologies -WEBISTINSTICC, SciTePress2023</p>
<p>A systematic review of the limitations and associated opportunities of ChatGPT. Ali Ngo Cong-Lem, Diki Soyoof, Tsering, International Journal of Human-Computer Interaction. 412025. 2025</p>
<p>A Cosine-Similarity Mutual-Information Approach for Feature Selection on High Dimensional Datasets. Vimal Kumar, Dubey , Amit Kumar Saxena, 10.4018/JITR.2017010102Journal of Information Technology Research. 1012017. Jan. 2017</p>
<p>An Introduction to the Bootstrap. Bradley Efron, Robert J Tibshirani, 1993Chapman &amp; Hall/CRCNew York</p>
<p>Ragas: Automated Evaluation of Retrieval Augmented Generation. Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert, 10.48550/arXiv.2309.15217arXiv:2309.152172025</p>
<p>The Implementation of Cosine Similarity to Calculate Text Relevance between Two Documents. C A Gunawan, M Sembiring, Budiman, 10.1088/1742-6596/978/1/012120Journal of Physics: Conference Series. 978121202018. March 2018</p>
<p>Automating Systematic Literature Reviews with Retrieval-Augmented Generation: A Comprehensive Overview. Binglan Han, Thomas Susnjak, Anuj Mathrani, 10.3390/app14199103Applied Sciences. 1491032024. 2024</p>
<p>Using citation network analysis to enhance scholarship in psychological science: A case study of the human aggression literature. Alessia Iancarelli, Chun-An Thomas F Denson, Ajay B Chou, Satpute, 10.1371/journal.pone.0266513PLOS ONE. 172022. 2022</p>
<p>Mohamad Yaser, Jaradeh , Allard Oelen, Manuel Prinz, Markus Stocker, Sren Auer, arXiv:2206.01439Open Research Knowledge Graph: A System Walkthrough. 2022. 2022arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM computing surveys. 552023. 2023</p>
<p>Replicant framework for synthetic data generation. Emily Kenul, Margaret Black, Drew Massey, Zachary Havelka, Mawia Henkai, Kyle Gavin, Luke Shellhorn, ; Applications, I I , Kimberly E Manser, 10.1117/12.3013826Synthetic Data for Artificial Intelligence and Machine Learning: Tools, Techniques, and. M Raghuveer, Christopher L Rao, Howell, National Harbor, United States202450Celso De MeloSPIE</p>
<p>Tools to support the automation of systematic reviews: a scoping review. Hanan Khalil, Daniel Ameen, Armita Zarnegar, 10.1016/j.jclinepi.2021.12.005Journal of Clinical Epidemiology. 1442022. 2022</p>
<p>Cosine similarity based filter technique for feature selection. Vimal Kumar, Dubey , Amit Kumar Saxena, 10.1109/ICCCCM.2016.79182222016 International Conference on Control, Computing, Communication and Materials (ICCCCM). IEEE, Allahbad, India. 2016</p>
<p>Jakub Lla, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.07559PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. 2023. 2023arXiv preprint</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Sebastian Riedel, arXiv:2005.11401Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 2020. 2020arXiv preprint</p>
<p>Linyu Liu, Yu Pan, Xiaocheng Li, Guanting Chen, arXiv:2404.15993Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach. 2024. 2024arXiv preprint</p>
<p>PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal. Renjie Liu, Haitian Jiang, Xiao Yan, Bo Tang, Jinyang Li, 10.48550/arXiv.2504.02112arXiv:2504.021122025</p>
<p>Bootstrap Method of Eco-Efficiency in the Brazilian Agricultural Industry. Andr Luiz, Marques Serrano, Gabriela Mayumi Saiki, Carlos Rosano-Pen, Gabriel Arquelau, Pimenta Rodrigues, Robson De Oliveira Albuquerque, Luis Javier, Garca Villalba, 10.3390/systems12040136Systems. 121362024. April 2024</p>
<p>Evaluating Literature Reviews Conducted by Humans Versus ChatGPT: Comparative Study. Mehrnaz Mostafapour, Jacqueline H Fortier, Karen Pacheco, Heather Murray, Gary Garber, Jmir ai. 3e565372024. 2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, arXiv:2203.021552022. 2022arXiv preprint</p>
<p>Vector Database Management Techniques and Systems. James Jie Pan, Jianguo Wang, Guoliang Li, 10.1145/3626246.3654691Companion of the 2024 International Conference on Management of Data. Santiago AA ChileACM2024</p>
<p>Relevance Filtering for Embedding-based Retrieval. Nicholas Rossi, Juexin Lin, Feng Liu, Zhen Yang, Tony Lee, Alessandro Magnani, Ciya Liao, 10.1145/3627673.3680095Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge ManagementBoise ID USAACM2024</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Tian-Jian Chang, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Fevry, Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M Rush, arXiv:2110.08207Multitask Prompted Training Enables Zero-Shot Task Generalization. 2021. 2021arXiv preprint</p>
<p>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction. Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, Dhagash Mehta, 10.48550/arXiv.2408.04948arXiv:2408.049482024</p>
<p>Agentic Retrieval-Augmented Generation: A Survey on Agentic. Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei, Khoei , 10.48550/arXiv.2501.09136arXiv:2501.091362025</p>
<p>Alan Karthikesalingam, and Vivek Natarajan. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera Y Arcas, Dale Webster, Greg S Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, 10.48550/arXiv.2212.13138arXiv:2212.13138Large Language Models Encode Clinical Knowledge. Alvin Rajkomar, Joelle Barral, Christopher Semturs2022</p>
<p>An overview of the Tesseract OCR engine. Ray Smith, Proceedings of the Ninth International Conference on Document Analysis and Recognition. the Ninth International Conference on Document Analysis and Recognition2007. 2007ICDAR 2007</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Advances in Neural Information Processing Systems. 2020</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Alpaca: A Strong, Replicable Instruction-Following Model. 2023</p>
<p>Automated screening of research studies for systematic reviews using study characteristics. Guy Tsafnat, Paul Glasziou, George Karystianis, Enrico Coiera, Systematic reviews. 72018. 2018</p>
<p>Finetuned Language Models Are Zero-Shot Learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652International Conference on Learning Representations (ICLR). 2022</p>
<p>Hybrid and Collaborative Passage Reranking. Zongmeng Zhang, Wengang Zhou, Jiaxin Shi, Houqiang Li, 10.18653/v1/2023.findings-acl.880arXiv:2305.09313Findings of the Association for Computational Linguistics: ACL 2023. 14003-14021. 2023880</p>            </div>
        </div>

    </div>
</body>
</html>
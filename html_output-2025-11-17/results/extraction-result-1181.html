<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1181 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1181</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1181</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-260900048</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.07498v1.pdf" target="_blank">DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation</a></p>
                <p><strong>Paper Abstract:</strong> VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions. It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose DREAMWALKER -- a world model based VLN-CE agent. The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation. DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, DREAMWALKER is able to make strategic planning through large amounts of ``mental experiments.'' Moreover, the imagined future scenarios reflect our agent's intention, making its decision-making process more transparent. Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1181.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1181.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DREAMWALKER WM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DREAMWALKER world model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit, hybrid world model combining a discrete Environment Graph (episodic, structured memory of visited and predicted waypoints) with a parametric Scene Synthesizer that generates plausible panoramic RGBD observations at unvisited nearby waypoints; used for Monte Carlo Tree Search (MCTS) mental planning in continuous vision-language navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DREAMWALKER world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid architecture with two components: (1) Environment Graph (EG) — an episodic, discrete graph G=(V,E) where node embeddings are panoramic observation embeddings Y_p for visited waypoints and edges encode topological/geometric relations (cosθ, sinθ, Δp); (2) Scene Synthesizer (SS) — a parametric generative network that takes an observed panoramic RGBD Y_p and a geometry-aligned reprojection Y_{p→p'} and synthesizes a full-resolution panoramic RGBD Ŷ_{p'} at a neighboring unvisited waypoint. The EG is updated online during navigation; SS is learned offline from training scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid model (explicit discrete graph + parametric neural scene synthesizer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Vision-Language Navigation in continuous 3D indoor environments (VLN-CE)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Fréchet Inception Distance (FID) between synthesized panoramic RGBD views and ground-truth panoramic observations; distance estimation error for the learnable distance function F_d (L2 / prediction error reported as curves)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>FID increases (worsens) with rollout length (no single scalar reported); synthesized-view error accumulates over prediction steps (paper reports decreasing usefulness beyond horizon ≈4). No absolute numeric FID value provided in main text; distance estimation experiments show that with perfect distance estimates SR → 100%, and replacing predicted distances with ground-truth at only 20% probability still yields ≈70% success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High — the model produces explicit synthesized panoramic observations and discrete future waypoint graphs, and the MCTS search tree (nodes = world states, edges = waypoint actions) can be visualized and colored by value/Q-values to show why particular actions/plans are chosen; imagined scenes provide human-understandable explanations of agent intent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of synthesized panoramic views at imagined waypoints, visualization of MCTS search tree nodes/edges colored by V(s)/Q(s,a), and inspection of branch outcomes (correct vs wrong branches) to explain decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>World-model components trained on Matterport3D-derived training split (61 scenes); overall system trained/run with components: ResNet-18/50 feature encoders (pretrained and frozen), SS two-stage generator (training schedule as in cited prior work), distance F_d trained with AdamW; online mental planning uses MCTS with 50 search iterations and planning horizon 4 by default; typical response time for mental planning ≈1.5 s per decision. Training and SS/WP pretraining details follow prior cited works; system trained/implemented on a single NVIDIA RTX 3090 for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to model-free baselines, DREAMWALKER achieves noticeably higher Success Rate without substantial extra travel distance; mental planning (50 iterations, horizon 4) is reported as real-time (≈1.5s), and search iterations scale runtime roughly linearly. The paper reports diminishing returns beyond 50 iterations and beyond horizon 4, motivating the chosen hyperparameters as an efficiency tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Outperforms prior model-free VLN-CE baselines: e.g., Success Rate (SR) reported as 59% (val seen), 49% (val unseen), 49% (test) compared with top prior (BridgingGap) 50%/44%/42% respectively. Using a 'perfect imagination' oracle improves SR further (val seen SR reported ≈64%).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The world model directly enables planning via MCTS and improves strategic behavior — higher-fidelity synthesized observations and more accurate distance estimates increase SR; however, fidelity degradation over long rollouts reduces planning utility, and distance estimator F_d accuracy is a central bottleneck (perfect distance estimates give upper bound of 100% SR). The model prioritizes task-relevant abstractions (discrete waypoints + predicted observations) rather than full scene reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Observed trade-offs include: longer mental-horizon increases runtime and accumulated synthesis error (worse FID), which can hurt performance; more MCTS iterations improve planning up to ≈50 iterations but increase per-decision latency linearly; higher fidelity SS would help but is limited by training data diversity (61 scenes) and current generative model capacity. Interpretability is improved by explicit discrete structure, at the cost of needing an SS and EG maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: discrete/structured EG for interpretability and tractable MCTS; parametric SS trained offline to capture general room-layout/transition priors; waypoint action space (heatmap of angles/distances) to map low-level controls to high-level discrete decisions; learnable GAT-based distance estimator F_d to compute rewards in mental planning; MCTS with UCT selection and rollout policy using F_d for fast playouts; default hyperparameters chosen as 50 search iterations and planning horizon 4 to balance fidelity and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to model-free VLN-CE agents (existing baselines), DREAMWALKER yields higher SR with modest additional travel length and offers interpretable imagined futures. Compared to 'copy memory' (memorization baseline) DREAMWALKER greatly outperforms it. 'Perfect imagination' oracle establishes an upper bound and shows that improving SS fidelity could further raise performance. The paper also contrasts with prior viewpoint-synthesis-only world models which either pre-access full topology or do not support sampling-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends (and uses) MCTS search rounds ≈50 and mental planning horizon ≈4 as practical optima: beyond these values gains are marginal or negative due to fidelity limits and compute cost. Authors recommend improving SS capacity/data (more scenes, diffusion-based synthesis) and better distance estimation to push the world model toward the 'perfect imagination' upper bound.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1181.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1181.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Environment Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An episodic, discrete graph (G=(V,E)) that stores visited and detected-but-unvisited waypoints as nodes with panoramic observation embeddings, and encodes geometric/topological relations (angle, distance) as edge features; used as the structured memory of the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Environment Graph (EG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Structured discrete graph where node v embedding = embedding of panoramic observation Y_p at waypoint p, and edge e_{u,v} embedding = (cosθ_{u,v}, sinθ_{u,v}, Δp_{u,v}). An edge exists only if both waypoints detect each other via the waypoint predictor heatmap. EG evolves online by adding visited waypoints and is used as the root state for MCTS tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit discrete structured memory / symbolic-leaning representation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>VLN-CE (continuous vision-language navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not applicable in isolation; fidelity of EG is tied to accuracy of waypoint prediction and of SS synthesized observations used to populate EG during imagination. Downstream performance measured by navigation SR and distance function regression L2 loss when EGs are built during training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No standalone numeric fidelity metric reported for EG; the paper trains distance function F_d on progressively constructed EGs and reports downstream improvements in planning performance. EG quality contributes to DREAMWALKER's SR gains (see task_performance of full model).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability: discrete nodes correspond to concrete waypoints and stored panoramic observations; edges encode explicit geometric relations, enabling human-understandable visualizations of imagined graph expansions and MCTS branches.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of graph nodes/edges, mapping nodes to synthesized/real panoramic images, and inspection of graph-based distance estimates (GAT outputs) used to justify actions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>EG maintenance is lightweight (graph operations, node embeddings). EG is updated per visited waypoint and used in MCTS expansions; main compute costs come from SS synthesis and GAT distance estimation during planning rather than graph bookkeeping itself.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>EG gives more compact, discrete planning state than attempting to plan in full continuous observation space; compared to model-free latent states, EG is explicit and supports transparent MCTS at modest computational overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As a core component of DREAMWALKER, EG supports the improved SR reported (59% val seen etc.). The paper shows that planning in this discrete EG enables superior strategic navigation relative to greedy model-free baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>EG provides a tractable abstraction for MCTS that preserves task-relevant topology/observations; its discreteness enables many fast mental rollouts. However, the utility depends on the correctness of detected waypoints and quality of synthesized observations when imagining unvisited nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete abstraction improves interpretability and tractability but discards some continuous information; if EG is sparse or waypoints are poorly predicted, planning utility falls.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of a waypoint heatmap with NMS to generate up to 5 candidate waypoints per observation; node embedding = panoramic feature; edges encode cos/sin angle + Δp; connectivity only when mutual detection exists.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Alternative (model-free) methods rely on compressed latent state vectors without an explicit graph; EG provides clearer structure for sampling-based planning and allows explicit visualization, at a moderate computational and design complexity cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>EG is designed to be episodic and compact; authors recommend updating EG on the fly and limiting candidate waypoints (NMS to 5) to keep planning tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1181.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1181.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scene Synthesizer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned, parametric two-stage generative network that, given a panoramic RGBD observation at p and a geometry-aligned reprojection of that observation into an unvisited nearby viewpoint p', synthesizes a plausible full-resolution panoramic RGBD (and semantic) observation at p'; encodes statistical priors about layout and transition dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Scene Synthesizer (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage generator that first projects the observed RGBD at p to a 3D point cloud and reprojects it into the frame at p' to form sparse geometry-aligned guidance images (RGB, depth, semantic), then inputs the guidance images plus original observation into a neural generator to synthesize panoramic RGBD and semantic maps for p'. The SS is trained offline on training scenes and encodes general rules of room layout and transition dynamics in its weights.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural generative world model (view synthesis / predictive renderer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Indoor scene prediction for VLN-CE (panoramic RGBD synthesis at nearby waypoints)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Fréchet Inception Distance (FID) between synthesized panoramic views and ground-truth; qualitative visual inspection; downstream planning utility (SR) as indirect fidelity measure.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>FID reported as increasing with prediction step (quantitative curve shown in paper), indicating accumulation of error over rollouts; no single absolute FID reported in main text. Authors note synthesized views are blurry in occluded or sparsely guided cases.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: synthesized images are human-interpretable and used to explain internal intentions, but the internal parameters are a black-box neural generator.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Direct visualization of synthesized panoramic RGBD (and semantic) images at imagined waypoints; replacing some ground-truth observations with synthesized ones during training builds robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SS is computationally heavier than EG operations: synthesis involves point-cloud projection and neural generation; trained following prior two-stage generator schedules; exact training time and parameter counts not reported. Online synthesis is used inside MCTS rollouts; number of syntheses scales with search iterations and planning horizon, contributing to per-decision latency (~1.5s at chosen hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides richer imagined visual input than trivial memory-copy baselines at the cost of additional compute; compared to methods that pre-access full environment typology, SS allows online imagination without full pre-exploration but is less accurate than a 'perfect imagination' oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables planning that improves SR; ablation variants show that replacing SS outputs with ground-truth ('perfect imagination') improves SR further (demonstrating SS is a key bottleneck). Copy-memory baseline (no imagination) performs worst.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SS improves the quality of mental experiments and hence planning, but its limited fidelity (especially across longer rollouts) constrains maximal planning horizon and overall gains. The model trades off generation fidelity vs. speed and generalization given limited training scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher-fidelity synthesis (longer/higher-capacity SS) would help planning but would increase training data needs and compute; synthesis error accumulates with rollout depth, limiting effective planning horizon to ≈4.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Two-stage generator; use of 3D point-cloud reprojection to create geometry-aligned guidance images; training with mixtures of ground-truth and synthesized observations to make distance estimator robust to SS noise; use of semantic guidance (Red-Net) to aid synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to viewpoint-synthesis-only prior works that may pre-access full topology or only use synthesized observations as sub-goals, this SS is integrated into an explicit discrete world model to support MCTS-based planning. Compared to a naive 'copy memory' approach, SS produces imagined unseen views enabling planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend keeping rollout horizons short (≈4) to limit error accumulation, increasing SS training data/diversity, and consider diffusion-based synthesis for higher-quality generation in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1181.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1181.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perfect Imagination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perfect imagination oracle variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental oracle variant where the world model's imagined future scenarios are replaced by the actual future observations from the environment (i.e., perfect predictions) to estimate an upper bound on performance attributable to the world-model/planning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Perfect imagination (oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A counterfactual variant used in ablation: during mental planning, synthesized predictions from SS are replaced by the true future panoramic observations (as if the agent could perfectly predict the future). All other components (EG, MCTS, distance function) remain the same.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>oracle / ground-truth augmented world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>VLN-CE evaluation of world-model upper bound</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>By construction, perfect fidelity for synthesized observations; downstream metrics are navigation SR, NE etc.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Leads to a solid performance boost compared to DREAMWALKER with learned SS; example: val seen SR increases from 59% (DREAMWALKER) to ≈64% (perfect imagination) per reported table, demonstrating headroom from SS fidelity improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Highest possible interpretability for imagined scenarios (they match true observations), and thus planning decisions are directly explainable against actual scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not applicable beyond using ground-truth visuals for imagined futures; used to demonstrate interpretability/upper bound.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Does not reflect practical runtime cost because it requires access to actual future observations (unrealistic online).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly comparable as an oracle; demonstrates how much SS fidelity limits current performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improves SR over learned SS baseline (e.g., val seen SR: ≈64% vs 59%; similar improvements on val unseen/test reported), indicating that better synthesis/knowledge of future would significantly improve navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows that high-fidelity world model predictions translate to substantial gains in navigation performance; thus fidelity is a key lever for task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Oracle performance is unattainable in practice without true future access; motivates investment in higher-capacity SS and better distance estimation to approximate oracle benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Used as an experimental upper-bound analysis rather than a deployable design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to COPY MEMORY and learned SS, perfect imagination yields the best task performance, highlighting the cost of imperfect synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Serves to indicate target direction: approach SS fidelity close to 'perfect imagination' to approach the upper bound; paper suggests improving SS (more scenes, better generative models) as next steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1181.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1181.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy memory (lazy world model) variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple ablation baseline that 'memorizes' past observations and generates future predictions by copying the nearest memory rather than synthesizing new views; used to measure the importance of active imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Copy memory</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A non-generative baseline which, when asked to imagine a future waypoint, returns the closest stored/past observation from episodic memory instead of synthesizing a new view via learned SS.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>memory-based baseline (non-generative world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>VLN-CE ablation to test utility of generative imagination</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream navigation SR and NE as proxy for utility/fidelity of the 'predictions'.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Per paper, yields poor performance (e.g., val seen SR ≈35%), substantially worse than DREAMWALKER with SS (59% val seen), indicating low utility of naive memory-copy predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low: copying nearest past observation gives limited and often misleading imagined futures; not helpful for explanation of intended consequences for unvisited areas.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Comparison of trajectories/decisions between this baseline and the full model in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Very cheap (near-zero synthesis cost) but yields poor task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Computationally efficient at inference relative to generative SS but dramatically worse in task performance; demonstrates that cheap memory copying is not an effective world model for planning in VLN-CE.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Low SR and higher NE compared to learned SS model (e.g., reported val seen SR 35% vs 59% for DREAMWALKER).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows naive memorization fails to provide the necessary imaginative capacity for strategic planning in unseen or partially observed continuous environments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Minimal compute cost vs large drop in performance; confirms need for a generative/predictive SS rather than naive memory replay.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Used as a control in experiments to show the contribution of predictive synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs worse than learned SS and much worse than perfect imagination; demonstrates the importance of learned generalization rather than pure memory.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not recommended; serves only as a baseline showing what not to do.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pathdreamer: A world model for indoor navigation <em>(Rating: 2)</em></li>
                <li>Mastering Atari with Discrete World Models <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Imagination-augmented agents for deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Vision-only robot navigation in a neural radiance world <em>(Rating: 1)</em></li>
                <li>World-consistent video-to-video synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1181",
    "paper_id": "paper-260900048",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DREAMWALKER WM",
            "name_full": "DREAMWALKER world model",
            "brief_description": "An explicit, hybrid world model combining a discrete Environment Graph (episodic, structured memory of visited and predicted waypoints) with a parametric Scene Synthesizer that generates plausible panoramic RGBD observations at unvisited nearby waypoints; used for Monte Carlo Tree Search (MCTS) mental planning in continuous vision-language navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DREAMWALKER world model",
            "model_description": "Hybrid architecture with two components: (1) Environment Graph (EG) — an episodic, discrete graph G=(V,E) where node embeddings are panoramic observation embeddings Y_p for visited waypoints and edges encode topological/geometric relations (cosθ, sinθ, Δp); (2) Scene Synthesizer (SS) — a parametric generative network that takes an observed panoramic RGBD Y_p and a geometry-aligned reprojection Y_{p→p'} and synthesizes a full-resolution panoramic RGBD Ŷ_{p'} at a neighboring unvisited waypoint. The EG is updated online during navigation; SS is learned offline from training scenes.",
            "model_type": "hybrid model (explicit discrete graph + parametric neural scene synthesizer)",
            "task_domain": "Vision-Language Navigation in continuous 3D indoor environments (VLN-CE)",
            "fidelity_metric": "Fréchet Inception Distance (FID) between synthesized panoramic RGBD views and ground-truth panoramic observations; distance estimation error for the learnable distance function F_d (L2 / prediction error reported as curves)",
            "fidelity_performance": "FID increases (worsens) with rollout length (no single scalar reported); synthesized-view error accumulates over prediction steps (paper reports decreasing usefulness beyond horizon ≈4). No absolute numeric FID value provided in main text; distance estimation experiments show that with perfect distance estimates SR → 100%, and replacing predicted distances with ground-truth at only 20% probability still yields ≈70% success rate.",
            "interpretability_assessment": "High — the model produces explicit synthesized panoramic observations and discrete future waypoint graphs, and the MCTS search tree (nodes = world states, edges = waypoint actions) can be visualized and colored by value/Q-values to show why particular actions/plans are chosen; imagined scenes provide human-understandable explanations of agent intent.",
            "interpretability_method": "Visualization of synthesized panoramic views at imagined waypoints, visualization of MCTS search tree nodes/edges colored by V(s)/Q(s,a), and inspection of branch outcomes (correct vs wrong branches) to explain decisions.",
            "computational_cost": "World-model components trained on Matterport3D-derived training split (61 scenes); overall system trained/run with components: ResNet-18/50 feature encoders (pretrained and frozen), SS two-stage generator (training schedule as in cited prior work), distance F_d trained with AdamW; online mental planning uses MCTS with 50 search iterations and planning horizon 4 by default; typical response time for mental planning ≈1.5 s per decision. Training and SS/WP pretraining details follow prior cited works; system trained/implemented on a single NVIDIA RTX 3090 for experiments.",
            "efficiency_comparison": "Compared to model-free baselines, DREAMWALKER achieves noticeably higher Success Rate without substantial extra travel distance; mental planning (50 iterations, horizon 4) is reported as real-time (≈1.5s), and search iterations scale runtime roughly linearly. The paper reports diminishing returns beyond 50 iterations and beyond horizon 4, motivating the chosen hyperparameters as an efficiency tradeoff.",
            "task_performance": "Outperforms prior model-free VLN-CE baselines: e.g., Success Rate (SR) reported as 59% (val seen), 49% (val unseen), 49% (test) compared with top prior (BridgingGap) 50%/44%/42% respectively. Using a 'perfect imagination' oracle improves SR further (val seen SR reported ≈64%).",
            "task_utility_analysis": "The world model directly enables planning via MCTS and improves strategic behavior — higher-fidelity synthesized observations and more accurate distance estimates increase SR; however, fidelity degradation over long rollouts reduces planning utility, and distance estimator F_d accuracy is a central bottleneck (perfect distance estimates give upper bound of 100% SR). The model prioritizes task-relevant abstractions (discrete waypoints + predicted observations) rather than full scene reconstruction.",
            "tradeoffs_observed": "Observed trade-offs include: longer mental-horizon increases runtime and accumulated synthesis error (worse FID), which can hurt performance; more MCTS iterations improve planning up to ≈50 iterations but increase per-decision latency linearly; higher fidelity SS would help but is limited by training data diversity (61 scenes) and current generative model capacity. Interpretability is improved by explicit discrete structure, at the cost of needing an SS and EG maintenance.",
            "design_choices": "Key choices: discrete/structured EG for interpretability and tractable MCTS; parametric SS trained offline to capture general room-layout/transition priors; waypoint action space (heatmap of angles/distances) to map low-level controls to high-level discrete decisions; learnable GAT-based distance estimator F_d to compute rewards in mental planning; MCTS with UCT selection and rollout policy using F_d for fast playouts; default hyperparameters chosen as 50 search iterations and planning horizon 4 to balance fidelity and compute.",
            "comparison_to_alternatives": "Compared to model-free VLN-CE agents (existing baselines), DREAMWALKER yields higher SR with modest additional travel length and offers interpretable imagined futures. Compared to 'copy memory' (memorization baseline) DREAMWALKER greatly outperforms it. 'Perfect imagination' oracle establishes an upper bound and shows that improving SS fidelity could further raise performance. The paper also contrasts with prior viewpoint-synthesis-only world models which either pre-access full topology or do not support sampling-based planning.",
            "optimal_configuration": "Paper recommends (and uses) MCTS search rounds ≈50 and mental planning horizon ≈4 as practical optima: beyond these values gains are marginal or negative due to fidelity limits and compute cost. Authors recommend improving SS capacity/data (more scenes, diffusion-based synthesis) and better distance estimation to push the world model toward the 'perfect imagination' upper bound.",
            "uuid": "e1181.0",
            "source_info": {
                "paper_title": "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "EG",
            "name_full": "Environment Graph",
            "brief_description": "An episodic, discrete graph (G=(V,E)) that stores visited and detected-but-unvisited waypoints as nodes with panoramic observation embeddings, and encodes geometric/topological relations (angle, distance) as edge features; used as the structured memory of the world model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Environment Graph (EG)",
            "model_description": "Structured discrete graph where node v embedding = embedding of panoramic observation Y_p at waypoint p, and edge e_{u,v} embedding = (cosθ_{u,v}, sinθ_{u,v}, Δp_{u,v}). An edge exists only if both waypoints detect each other via the waypoint predictor heatmap. EG evolves online by adding visited waypoints and is used as the root state for MCTS tree search.",
            "model_type": "explicit discrete structured memory / symbolic-leaning representation",
            "task_domain": "VLN-CE (continuous vision-language navigation)",
            "fidelity_metric": "Not applicable in isolation; fidelity of EG is tied to accuracy of waypoint prediction and of SS synthesized observations used to populate EG during imagination. Downstream performance measured by navigation SR and distance function regression L2 loss when EGs are built during training.",
            "fidelity_performance": "No standalone numeric fidelity metric reported for EG; the paper trains distance function F_d on progressively constructed EGs and reports downstream improvements in planning performance. EG quality contributes to DREAMWALKER's SR gains (see task_performance of full model).",
            "interpretability_assessment": "High interpretability: discrete nodes correspond to concrete waypoints and stored panoramic observations; edges encode explicit geometric relations, enabling human-understandable visualizations of imagined graph expansions and MCTS branches.",
            "interpretability_method": "Visualization of graph nodes/edges, mapping nodes to synthesized/real panoramic images, and inspection of graph-based distance estimates (GAT outputs) used to justify actions.",
            "computational_cost": "EG maintenance is lightweight (graph operations, node embeddings). EG is updated per visited waypoint and used in MCTS expansions; main compute costs come from SS synthesis and GAT distance estimation during planning rather than graph bookkeeping itself.",
            "efficiency_comparison": "EG gives more compact, discrete planning state than attempting to plan in full continuous observation space; compared to model-free latent states, EG is explicit and supports transparent MCTS at modest computational overhead.",
            "task_performance": "As a core component of DREAMWALKER, EG supports the improved SR reported (59% val seen etc.). The paper shows that planning in this discrete EG enables superior strategic navigation relative to greedy model-free baselines.",
            "task_utility_analysis": "EG provides a tractable abstraction for MCTS that preserves task-relevant topology/observations; its discreteness enables many fast mental rollouts. However, the utility depends on the correctness of detected waypoints and quality of synthesized observations when imagining unvisited nodes.",
            "tradeoffs_observed": "Discrete abstraction improves interpretability and tractability but discards some continuous information; if EG is sparse or waypoints are poorly predicted, planning utility falls.",
            "design_choices": "Use of a waypoint heatmap with NMS to generate up to 5 candidate waypoints per observation; node embedding = panoramic feature; edges encode cos/sin angle + Δp; connectivity only when mutual detection exists.",
            "comparison_to_alternatives": "Alternative (model-free) methods rely on compressed latent state vectors without an explicit graph; EG provides clearer structure for sampling-based planning and allows explicit visualization, at a moderate computational and design complexity cost.",
            "optimal_configuration": "EG is designed to be episodic and compact; authors recommend updating EG on the fly and limiting candidate waypoints (NMS to 5) to keep planning tractable.",
            "uuid": "e1181.1",
            "source_info": {
                "paper_title": "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SS",
            "name_full": "Scene Synthesizer",
            "brief_description": "A learned, parametric two-stage generative network that, given a panoramic RGBD observation at p and a geometry-aligned reprojection of that observation into an unvisited nearby viewpoint p', synthesizes a plausible full-resolution panoramic RGBD (and semantic) observation at p'; encodes statistical priors about layout and transition dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Scene Synthesizer (SS)",
            "model_description": "Two-stage generator that first projects the observed RGBD at p to a 3D point cloud and reprojects it into the frame at p' to form sparse geometry-aligned guidance images (RGB, depth, semantic), then inputs the guidance images plus original observation into a neural generator to synthesize panoramic RGBD and semantic maps for p'. The SS is trained offline on training scenes and encodes general rules of room layout and transition dynamics in its weights.",
            "model_type": "neural generative world model (view synthesis / predictive renderer)",
            "task_domain": "Indoor scene prediction for VLN-CE (panoramic RGBD synthesis at nearby waypoints)",
            "fidelity_metric": "Fréchet Inception Distance (FID) between synthesized panoramic views and ground-truth; qualitative visual inspection; downstream planning utility (SR) as indirect fidelity measure.",
            "fidelity_performance": "FID reported as increasing with prediction step (quantitative curve shown in paper), indicating accumulation of error over rollouts; no single absolute FID reported in main text. Authors note synthesized views are blurry in occluded or sparsely guided cases.",
            "interpretability_assessment": "Moderate: synthesized images are human-interpretable and used to explain internal intentions, but the internal parameters are a black-box neural generator.",
            "interpretability_method": "Direct visualization of synthesized panoramic RGBD (and semantic) images at imagined waypoints; replacing some ground-truth observations with synthesized ones during training builds robustness.",
            "computational_cost": "SS is computationally heavier than EG operations: synthesis involves point-cloud projection and neural generation; trained following prior two-stage generator schedules; exact training time and parameter counts not reported. Online synthesis is used inside MCTS rollouts; number of syntheses scales with search iterations and planning horizon, contributing to per-decision latency (~1.5s at chosen hyperparameters).",
            "efficiency_comparison": "Provides richer imagined visual input than trivial memory-copy baselines at the cost of additional compute; compared to methods that pre-access full environment typology, SS allows online imagination without full pre-exploration but is less accurate than a 'perfect imagination' oracle.",
            "task_performance": "Enables planning that improves SR; ablation variants show that replacing SS outputs with ground-truth ('perfect imagination') improves SR further (demonstrating SS is a key bottleneck). Copy-memory baseline (no imagination) performs worst.",
            "task_utility_analysis": "SS improves the quality of mental experiments and hence planning, but its limited fidelity (especially across longer rollouts) constrains maximal planning horizon and overall gains. The model trades off generation fidelity vs. speed and generalization given limited training scenes.",
            "tradeoffs_observed": "Higher-fidelity synthesis (longer/higher-capacity SS) would help planning but would increase training data needs and compute; synthesis error accumulates with rollout depth, limiting effective planning horizon to ≈4.",
            "design_choices": "Two-stage generator; use of 3D point-cloud reprojection to create geometry-aligned guidance images; training with mixtures of ground-truth and synthesized observations to make distance estimator robust to SS noise; use of semantic guidance (Red-Net) to aid synthesis.",
            "comparison_to_alternatives": "Compared to viewpoint-synthesis-only prior works that may pre-access full topology or only use synthesized observations as sub-goals, this SS is integrated into an explicit discrete world model to support MCTS-based planning. Compared to a naive 'copy memory' approach, SS produces imagined unseen views enabling planning.",
            "optimal_configuration": "Authors recommend keeping rollout horizons short (≈4) to limit error accumulation, increasing SS training data/diversity, and consider diffusion-based synthesis for higher-quality generation in future work.",
            "uuid": "e1181.2",
            "source_info": {
                "paper_title": "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Perfect Imagination",
            "name_full": "Perfect imagination oracle variant",
            "brief_description": "An experimental oracle variant where the world model's imagined future scenarios are replaced by the actual future observations from the environment (i.e., perfect predictions) to estimate an upper bound on performance attributable to the world-model/planning pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Perfect imagination (oracle)",
            "model_description": "A counterfactual variant used in ablation: during mental planning, synthesized predictions from SS are replaced by the true future panoramic observations (as if the agent could perfectly predict the future). All other components (EG, MCTS, distance function) remain the same.",
            "model_type": "oracle / ground-truth augmented world model",
            "task_domain": "VLN-CE evaluation of world-model upper bound",
            "fidelity_metric": "By construction, perfect fidelity for synthesized observations; downstream metrics are navigation SR, NE etc.",
            "fidelity_performance": "Leads to a solid performance boost compared to DREAMWALKER with learned SS; example: val seen SR increases from 59% (DREAMWALKER) to ≈64% (perfect imagination) per reported table, demonstrating headroom from SS fidelity improvements.",
            "interpretability_assessment": "Highest possible interpretability for imagined scenarios (they match true observations), and thus planning decisions are directly explainable against actual scenes.",
            "interpretability_method": "Not applicable beyond using ground-truth visuals for imagined futures; used to demonstrate interpretability/upper bound.",
            "computational_cost": "Does not reflect practical runtime cost because it requires access to actual future observations (unrealistic online).",
            "efficiency_comparison": "Not directly comparable as an oracle; demonstrates how much SS fidelity limits current performance.",
            "task_performance": "Improves SR over learned SS baseline (e.g., val seen SR: ≈64% vs 59%; similar improvements on val unseen/test reported), indicating that better synthesis/knowledge of future would significantly improve navigation.",
            "task_utility_analysis": "Shows that high-fidelity world model predictions translate to substantial gains in navigation performance; thus fidelity is a key lever for task utility.",
            "tradeoffs_observed": "Oracle performance is unattainable in practice without true future access; motivates investment in higher-capacity SS and better distance estimation to approximate oracle benefits.",
            "design_choices": "Used as an experimental upper-bound analysis rather than a deployable design.",
            "comparison_to_alternatives": "Compared to COPY MEMORY and learned SS, perfect imagination yields the best task performance, highlighting the cost of imperfect synthesis.",
            "optimal_configuration": "Serves to indicate target direction: approach SS fidelity close to 'perfect imagination' to approach the upper bound; paper suggests improving SS (more scenes, better generative models) as next steps.",
            "uuid": "e1181.3",
            "source_info": {
                "paper_title": "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Copy Memory",
            "name_full": "Copy memory (lazy world model) variant",
            "brief_description": "A simple ablation baseline that 'memorizes' past observations and generates future predictions by copying the nearest memory rather than synthesizing new views; used to measure the importance of active imagination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Copy memory",
            "model_description": "A non-generative baseline which, when asked to imagine a future waypoint, returns the closest stored/past observation from episodic memory instead of synthesizing a new view via learned SS.",
            "model_type": "memory-based baseline (non-generative world model)",
            "task_domain": "VLN-CE ablation to test utility of generative imagination",
            "fidelity_metric": "Downstream navigation SR and NE as proxy for utility/fidelity of the 'predictions'.",
            "fidelity_performance": "Per paper, yields poor performance (e.g., val seen SR ≈35%), substantially worse than DREAMWALKER with SS (59% val seen), indicating low utility of naive memory-copy predictions.",
            "interpretability_assessment": "Low: copying nearest past observation gives limited and often misleading imagined futures; not helpful for explanation of intended consequences for unvisited areas.",
            "interpretability_method": "Comparison of trajectories/decisions between this baseline and the full model in ablations.",
            "computational_cost": "Very cheap (near-zero synthesis cost) but yields poor task performance.",
            "efficiency_comparison": "Computationally efficient at inference relative to generative SS but dramatically worse in task performance; demonstrates that cheap memory copying is not an effective world model for planning in VLN-CE.",
            "task_performance": "Low SR and higher NE compared to learned SS model (e.g., reported val seen SR 35% vs 59% for DREAMWALKER).",
            "task_utility_analysis": "Shows naive memorization fails to provide the necessary imaginative capacity for strategic planning in unseen or partially observed continuous environments.",
            "tradeoffs_observed": "Minimal compute cost vs large drop in performance; confirms need for a generative/predictive SS rather than naive memory replay.",
            "design_choices": "Used as a control in experiments to show the contribution of predictive synthesis.",
            "comparison_to_alternatives": "Performs worse than learned SS and much worse than perfect imagination; demonstrates the importance of learned generalization rather than pure memory.",
            "optimal_configuration": "Not recommended; serves only as a baseline showing what not to do.",
            "uuid": "e1181.4",
            "source_info": {
                "paper_title": "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pathdreamer: A world model for indoor navigation",
            "rating": 2,
            "sanitized_title": "pathdreamer_a_world_model_for_indoor_navigation"
        },
        {
            "paper_title": "Mastering Atari with Discrete World Models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Imagination-augmented agents for deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "imaginationaugmented_agents_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Vision-only robot navigation in a neural radiance world",
            "rating": 1,
            "sanitized_title": "visiononly_robot_navigation_in_a_neural_radiance_world"
        },
        {
            "paper_title": "World-consistent video-to-video synthesis",
            "rating": 1,
            "sanitized_title": "worldconsistent_videotovideo_synthesis"
        }
    ],
    "cost": 0.0164335,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation
14 Aug 2023</p>
<p>Hanqing Wang 
Beijing Institute of Technology</p>
<p>Computer Vision Lab
ETH Zurich</p>
<p>Wei Liang 
Beijing Institute of Technology</p>
<p>Yangtze Delta Region Academy of Beijing Institute of Technology
Jiaxing</p>
<p>Luc Van Gool 
Computer Vision Lab
ETH Zurich</p>
<p>Wenguan Wang 
ReLER
CCAI
Zhejiang University</p>
<p>DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation
14 Aug 2023A6C03BEB1274940E3C2A412AFB40D04BarXiv:2308.07498v1[cs.CV]
VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions.It poses great challenges due to the huge space of possible strategies.Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose DREAMWALKER -a world model based VLN-CE agent.The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation.DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions.As opposed to existing modelfree VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, DREAMWALKER is able to make strategic planning through large amounts of "mental experiments."Moreover, the imagined future scenarios reflect our agent's intention, making its decision-making process more transparent.Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.</p>
<p>Introduction</p>
<p>For decades, the AI community has strived to develop intelligent robots that can understand human instructions and carry them out.As a small step towards this long-held goal, vision-language navigation (VLN) [6] -the task of entailing autonomous agents to navigate in never-before-seen 3D environments with language instructions -gained growing attention.In the standard VLN setting, agent's movement is constrained to a small set of pre-defined sparse locations.As pointed out by [47], such over-simplified, discrete task setup involves many unrealistic assumptions such as known   topology, perfect localization, and deterministic transition.</p>
<p>To better reflect the challenges of real world navigation, Krantz et al. [47] update the discrete VLN to a continuous version -VLN-CE (VLN in continuous environments), where the agent is free to traverse any unobstructed location with low-level actions.VLN-CE proved much more challenging than its discrete counterpart: the performance gap between the state-of-the-arts in the two settings is more than 20%, in terms of episode success rate.The main challenge posed by VLN-CE lies in the demand of strategic planning in continuous environments with low-level actions.</p>
<p>As a direct response, we developed a world model based VLN-CE agent, called DREAMWALKER.Previous studies in cognitive science [17,36,37] suggest that humans build a mental model of the local surrounding, based on our limited senses.This internal world model summarizes our knowledge about the environment and serves as the basis for many high-level meta-skills, e.g., reasoning, planning, decisionmaking, and interpretation.The world model theory is one source of the idea of model-based Reinforcement Learning (RL) [76] and promotes many recent advances in robot control [72,65,61,94].Keeping this grand idea in head, we let DREAMWALKER explicitly abstract crucial characteristics of its continuous surrounding environment to a discrete, structured representation (Fig. 1).This allows DREAMWALKER to "imagine" a lot of future possible navigation plans and evaluate the corresponding consequences entirely in the mind, before taking actual low-level actions in the real world.In this way, DREAMWALKER takes the challenge of VLN-CE head-on: mental planning with discrete world model enables efficient navigation behavior in continuous environments.</p>
<p>Technically, the world model is built upon agent's past experiences and can make predictions about the future.It contains two parts: i) An environment graph (EG) is constructed as a composition of selected or predicted waypoints and their typological relations.EG collects agent's temporary knowledge about its surrounding.ii) A learnable scene synthesizer (SS) predicts future observations from a waypoint with multiple steps.SS embeds agent's stable knowledge about environments, such as general room layout rules and transition dynamics, into its network parameters.Based on the world model, DREAMWALKER synthesizes various future navigation trajectories, and assesses their progress towards the final target location.Then, the best mental plan is found by Monte Carlo Tree Search [43] and executed in the continuous world with low-level actions.With the navigation proceeds, EG is further updated for making a new round of mental planning.</p>
<p>Notably, our DREAMWALKER significantly distinguishes itself from prior VLN-CE solutions [70,30,45,46] in the following aspects: i) Recent advanced solutions are essentially model-free methods.While in principle a representation of the environment could be implicitly learned through model-free RL, the reinforcement signal may be too weak to quickly learn such a representation and how to make use of it.In contrast, our agent plans its actions within an explicit, and abstract model of the continuous environment.ii) Existing agents navigate by greedily and reactively choosing between a small set of nearby waypoints, based on their hidden state which compresses past observations.They tend to be shortsighted, due to the absence of reliable strategies for capturing information for achieving the future [23].Yet DREAMWALKER can use the world model to anticipate the impacts of possible actions and plan strategic behavior.iii) The future scenarios created by the world model explain the intention of DREAMWALKER in a way that human can understand, making its behaviors more interpretable [9,82].</p>
<p>Extensive experiments on VLN-CE dataset [47] confirm that our DREAMWALKER gains promising performance with the appealing ability of real-time behavioral interpretation.This work is expected to foster future research in developing more strategic, robust, and interpretable VLN-CE agents.</p>
<p>Related Work</p>
<p>VLN in Discrete Environments.The release of R2R dataset [6] has stimulated the emergence of variousVLN systems and datasets.In particular, early VLN agents are built upon recurrent neural network based sequence-to-sequence model [6,18,80,91,89,32,55,96,40,56,87,85,2], while the recent ones explore graph neural networks [15,86] and nonlocal transformer models [31,63,10] for long-term planning, with the assist of environment map building [95,11,53], cross-modal matching [84,97,90], and multi-modal pretraining [57,26,67,68,3] techniques.Besides the advance in model design, several more challenging VLN datasets, such as R4R [33], RxR [48], and REVERIE [66], are developed to address long-term navigation [33,48] and concise instruction guided navigation [66].</p>
<p>These remarkable efforts follow the seminal work of R2R to assume the agent can 'perfectly' teleport to and from a fixed small set of locations (pre-stored in a sparse navigation graph) in the Matterport3D [8] environment.Although facilitating the evolution and evaluation of VLN algorithms, such discrete task setup is too simplified to cover the practical challenges that a robot would encounter while navigating the real world, such as environment topology acquirement and localization error.In this work, we focus on performing VLN in continuous environments, setup by [47].VLN in Continuous Environments (VLN-CE).Krantz et al. [47] lift the discrete R2R VLN task setup to the continuous setting -VLN-CE, where the embodied agent is initiated in freely traversable 3D environments and must execute low-level actions to follow natural language navigation instructions.After throwing away the unrealistic assumption of the navigation graph, the VLN task becomes more challenging and closer to the real-world.Later approaches attempt to reproduce the success of the abstract VLN in the continuous counterpart, by building a high-level action space based on online prediction of waypoints [70,30,45,83,4].More recently, [46] explores transferring pre-trained VLN policies to continuous environments, demonstrating advantages over training VLN-CE policies from scratch.</p>
<p>Our agent is favored due to i) its model-based nature, and ii) the ability of strategic and interpretable planning.First, our agent learns and builds an explicit model of the world, freeing navigation policy learning from environment modeling.Second, by visualizing possible futures, our agent is able to develop advanced plan before moving and explain its behaviors.In contrast, current agents make only greedy decisions purely relying on their latent state.Their planning ability is rather weak and the decision mode is non-transparent.World Model.Equipping robot machines with the ability to build world models is viewed as a key step towards the nextgeneration of AI [36,52].Capturing high-level aspects of the environment, world models help enable AI agents for reasonable decision-making through simulation (a.k.a., imagination) of possible futures.Towards this direction, a bunch of approaches have been recently developed to predict forward dynamics/representations [62,39,12,20,41,51], perform sampling-based planning [75,64,94], conduct selfsimulation based policy learning [92,88,22], and reconcile the advantages of model-based and model-free RL regimes [69,59,78,49].Some others leverage world models to anticipate (short-term) targets for goal-conditioned policy learning [61,60], boost the learning of world models by introducing belief of state uncertainty [13,20,19], informative state representation [50,25,79], and model regularization [52,14,42].Although world models find successful applications in Atari [38,24,23,25] and robot motion planning [16,72], many of them are restricted to relatively simple tasks or low-dimensional environments, pointed out by [23,25,61,44].To date, world models have been rarely investigated in visually-rich, embodied tasks.The few exceptions [51,44] simply use a viewpoint synthesis network as the world model.They either pre-access the entire environment typology [44], or only treat the synthesized observations as sub-goals without sampling-based planning [51].In [1], NeRF-based world model is adopted for drone navigation planning, which requires pre-exploration of the environment.For ours, in addition to the challenging task setup, our world model is organized as a structured summarization of environment layout and a compact representation of environment evolution, instead of [51,44] encoding all the knowledge about the environment into latent network parameters.More importantly, our explicit and discrete abstraction of the continuous environments offers a suitable testbed for mental experiments.Through sampling and assessing numerous possible solutions entirely in the mind, our agent can conduct strategic and global navigation planning before moving.Monte Carlo Tree Search (MCTS).MCTS [43] is a heuristic search algorithm for approximating optimal choices in large search spaces.It has demonstrated great success in creating game-playing AI agents and solving sequential decision tasks, such as Atari games [21,74] and Go [76,77,74].</p>
<p>In this work, we adopt MCTS to search for possible navigation plans based on the world model.MCTS has so far been rarely explored in the field of VLN-CE.In addition, in many application scenarios of MCTS, all the aspects of the world states are fully observable.However, the continuous VLN-CE environments are partially observable, and hence we run MTCS in a discrete, synthesized world space.</p>
<p>Methodology</p>
<p>Task Setup and Notations.In VLN-CE [47], AI agents are required to navigate in never-before-seen 3D environments to target positions, according to language instructions.The environments are assumed to be continuous open space.At each step, the agent must choose a low-level action from {turn-left 15 • , turn-right 15 • , move-forward 0.25m, stop}, given the instruction X and 360 • panoramic RGBD observation Y .The navigation is successful only if the agent selects stop within 3m of the target location within 500 steps.Waypoint Action Space.Recent VLN-CE solutions [70,30,45,46] adopt a high-level waypoint action space.During navigation, the agent uses a Waypoint Predictor to get a heatmap of 120 angles-by-12 distances, which highlights navigable waypoints in its surrounding.Each angle is of 3 degrees, and the distances range from 0.25 meters to 3.00 meters with 0.25 meters separation, respectively corresponding to the turning angle and forward step size of the low-level action space (Fig. 2(f)).In this way, the problem of inferring low-level controls is translated to the task of waypoint prediction and selection.Please refer to [30] for more details.Algorithmic Overview.Our DREAMWALKER agent solves VLN-CE through i) building a world model ( §3.1) that explicitly abstracts intrinsic properties of continuous environments into a structured, discrete representation; ii) conducting strategic planning in the discrete mental world before taking actual actions in the continuous environment ( §3.2).</p>
<p>World Model: Structured, Discrete, and Compact Abstraction of Continuous Environments</p>
<p>To efficiently plan and act in the highly complex world, humans develop a mental model to represent our knowledge about the surrounding, based on our past daily experiences and current information perceived by limited sense [22].In view of this, our world model is built as combination of two parts: i) An environment graph (EG) -a structured and discrete representation of agent's temporary knowledge about the visual landmarks and typologies of the current partially observed environment; and ii) A scene synthesizer (SS) that encodes agent's stable knowledge about some general rules of environments such as transition dynamics and room layout, which are learned from training experiences and utilized to predict the unobserved portions of the current environment.Environment Graph (EG).EG is organized as an episodic graph G = (V, E), where nodes v ∈ V denote previously visited waypoints and edges e u,v ∈ E store geometric relations between waypoints (Fig. 2(a)(g)).At the beginning of current navigation episode, G only contains one single node -the starting location.Then, the agent predicts a set of accessible waypoints, and selects one of them to navigate.After reaching the selected waypoint, G will be updated by involving this waypoint.Hence EG evolves with the navigation proceeds.Concretely, for each node v ∈ V, its embedding is the feature of the corresponding waypoint observation:    The synthesized and real observations at are given in (i) and (j), respectively.For clarity, only RGB observation is provided.Notably, the imagined scenarios explain agent's inner decision mode in a way that human can understand, leading to improved interpretability.
v = Yp v ,(1)(a) (b) (c) (d) (e) (f) (g) (i) (h) (j)
where p v refers to the location of waypoint v, and the coordinate of the start point is set as (0, 0); Y pv indicates the embedding of the panoramic observation Y pv at location p v .</p>
<p>For each edge e u,v ∈ E, its embedding e u,v encodes topological relations between waypoints u and v:
eu,v = (cos θu,v, sin θu,v, △pu,v),(2)
where θ u,v and △p u,v = p u − p v refer to the angle and distance between u and v, respectively.Note that the connectivity among waypoints is also captured by G.An edge, e u,v , exists only if u and v are connected, that is, they are detected as valid in the waypoint prediction heatmap of each other.</p>
<p>As such, EG is built upon the information gathered during current navigation episode.Hence it represents the agent's temporary knowledge about its observed surroundings, and organizes them in a concise, structured, and discrete manner.Scene Synthesizer (SS).SS is a generative network that predicts future scenes based on agent's past observations only, without having to navigate them (Fig. 2(h-j)).Given panoramic RGBD observation Y p perceived at position p, SS is to render a plausible, full-resolution panoramic RGBD observation Ŷp ′ at an unvisited position p ′ .The position p ′ of interest is a waypoint of p, thus p and p ′ are spatially close.Like previous world structure-aware video synthesis methods [58,44], we first project Y p to a 3D point cloud using the depth information and re-project this point cloud into the observation space at position p ′ , so as to obtain a sparse, geometry-aligned RGBD panoramic image Y p→p ′ .The SS network takes as inputs both the observation Y p perceived at p and the reconstructed observation Y p→p ′ , and synthesizes the observation Ŷp ′ for the unvisited waypoint position p ′ :
Ŷp ′ = Scene Synthesizer(Yp, Y p→p ′ ).(3)
SS learns from experiences of 'viewing' numerousVLN-CE training environments.Its parameters encode agent's statis-tical knowledge about some fundamental constraints in the world, such as the transition dynamics and general rules behind room arrangement (e.g., 'dishwasher is typically located in the kitchen').</p>
<p>Working Mode of World Model.Integrating EG and SS together leads to a powerful world model.EG stores agent's understanding of the observed portions of the environment.</p>
<p>Based on working memory, SS makes use of statistical knowledge to forecast the unexplored portion of the environment (Fig. 2(a-e)).For instance, starting from a waypoint v in G, the world model uses SS to imagine the future (i.e., Ŷp v ′ ) if the agent navigates to a previously unvisited waypoint v ′ of v.With the synthesized observation Vp v ′ at waypoint v ′ , the world model can make further future prediction.Notably, our world model, or more precisely, its parametric part -SS, is trained independently, which lifts the navigation policy from learning inherent structures of environments.</p>
<p>Mental Planning: Forecasting the Future in the World Model</p>
<p>Powered by the world model, DREAMWALKER gains the ability of anticipating the consequences of its actions, for an extended period into the future.This is because the world model can simulate how the environment changes in response to agent's actions.DREAMWALKER can thus make advanced planning, through perform mental experiments in the simulated world.Basically, at every navigation-decision making step, DREAMWALKER first uses the world model to synthesize many future navigation trajectories, and then selects the best one for execution.Here we adopt Monte Carlo Tree Search (MCTS) [43,21], a powerful approach for decision problems, to achieve world model based online planning.MCTS based Mental Planning.As a best-first tree search algorithm, MCTS represents the space of candidates into a tree and finds an optimal solution in an iterative manner.In our case, each tree node s represents a possible world state: the root node s 0 is the world state G at current navigation step, while other nodes indicate future world states, synthesized by the world model.The edge from node s to s ′ denotes the action a taken in s to reach s ′ , and is identified by pair (s, a).</p>
<p>The core idea of MCTS based planning is to gradually expand the search tree and evaluate rewards, i.e., create many plans and assess the outcomes in mind.This is achieved by an iterative tree search process.Each search iteration starts from the root state s 0 , and sequentially samples states and actions, based on the simulation of four phases (Fig. 3): i) Selection: Actions/edges are selected according to a tree policy.A commonly used policy is to select greedily with respect to Upper Confidence bounds for Trees (UCT) [7]:
UCT(s, a) = Q(s, a) + C log N (s) N (s, a) ,(4)
where Q(s, a) is the average accumulated reward of taking action a; N (s, a) and N (s) return the visiting times of edge a and state s, respectively; C is a scalar exploration constant.Starting from the root node, the action is selected as:
a * = argmax a∈A(s) UCT(s, a),(5)
where A(s) is the global action space at state s, which enumerates all the possible waypoints.The selection is performed recursively until an unexpanded edge is selected.ii) Expansion: With the finally selected unexpanded edge, a new leaf node ṡ is further appended, i.e., N ( ṡ) = 1.iii) Rollout: A quick rollout is performed to predict further multiple steps into the future, according to a certain rollout policy.Based on a reward function R, the value of the new leaf node ṡ, i.e., V ( ṡ), is obtained.iv) Back-up: The statistics of nodes and edges are updated bottom-up through the tree from ṡ until reaching the root node.
N (s, a) = N (s ′ ), Q(s, a) = R(s, a) + γV (s ′ ), N (s) = 1 + a∈A(s) N (s, a), V (s) = 1 N (s) a∈A(s) N (s, a)Q(s, a) .(6)
Here R(s, a) gives the reward for taking action a at state s; s ′ = T (s, a), where T (s, a) is the deterministic transition function.The design of our reward function R and the rollout policy will be detailed later.</p>
<p>As such, in each iteration, DREAMWALKER creates and executes a plan in the world model by expanding the search tree, and anticipates the consequence by estimating the rewards.After several times of iterative simulation, the agent conducts a lot of mental experiments, and the best action is selected among the edges starting from the root node:</p>
<p>Here a * corresponds to a certain waypoint observed at current world state s 0 .According to EG G, the agent can reach this waypoint by taking a sequence of low-level actions.If a * is never visited before, EG is updated after its first visit, followed by new-round mental planning and nextstep action.When a * is a visited waypoint, the agent will move to this waypoint and choose stop to terminate navigation.In this way, our agent predicts future in the discrete world model, and navigates in the continuous environment.Reward.The immediate reward R(s, a) is defined according to the change of distance to goal after taking action a.Let G s = (V s , E s ) denote EG corresponding to state s, where V s −V refers to those waypoints visited in dream world state s, the distance to the target location in state s is defined as:
D(s) = min v∈Vs F d (v, Gs, X),(8)
where F d is a learnable distance function that predicts the distance from waypoint v to the target location, conditioned on EG G s and instruction X.More specifically, D(s) is built upon a graph attention (GAT) network [81]:
F d (v, Gs, X) = MLP(ṽ), Ṽ = {ṽ}v = GAT({[v, X]}v, {[eu,v, u, X]}u,v),(9)
where v (u) is the initial embedding of node v (u), e u,v is the edge embedding of e u,v ∈ E. X denotes the textual embedding of the instruction X, and [•, •] refers to concatenation.After fusing node and edge embeddings with the textual context, GAT outputs the collection of improved node embeddings Ṽ .After that, a small multilayer perceptron (MLP) is applied per node for distance regression.</p>
<p>Given the deterministic transition s ′ = T (s, a), we have a = V s ′ − V s .If action a is a previously visited waypoint, i.e., a ∈ V, the immediate reward R(s, a) is given as:
R(s, a) = +5, F d (v, Gs, X) ≤ 3, −5, F d (v, Gs, X) &gt; 3. (10)
Here a ∈ V means the agent chooses stop at waypoint a.We estimate if its distance to the target is within the success cri-teria, i.e., 3m, and assign a constant positive/negative reward (+5/-5) accordingly.If waypoint a is never visited before, we define:
R(s, a) = D(s ′ ) − D(s).(11)
That is to say, the reward is defined as the distance that a can bring the agent closer to the target than before.Rollout Policy.In the rollout phase, a rollout policy is adopted to guide the fast playout starting from the new expanded leaf state ṡ.This can be intuitively viewed as further imagining the future of state ṡ with several steps through fast sampling.For the sake of Monte Carlo property and simplicity, we treat the distance function D(•) (cf.Eq. 8) as the rollout policy.For example, at the first rollout step, the action distribution p over possible waypoints A( ṡ) at state ṡ is given as:
p[a] = softmax a∈A( ṡ) F d (a, G ṡ, X). (12)
The rollout stops when a previously visited waypoint is selected or a maximum rollout depth K is reached.The rewards {R 1 , R 2 , • • • , R K } along the rollout record are collected to compute the accumulated discounted reward as the value of ṡ:
V ( ṡ) = K k=1 γ k−1 R k .(13)</p>
<p>Experiments</p>
<p>After stating our experimental setup ( §4.1) and implementation details ( §4.2), we provide performance comparison results with VLN-CE state-of-the-arts ( §4.3).Then we identify there is still room for improvement in the aspects of world model ( §4.4) and distance estimation ( §4.6), revealing possible future directions.We further verify the contribution of our world model based mental planning ( §4.5).Finally, we evaluate the impacts of core hyper-parameters ( §4.7) and offer visual analyses ( §4.8).</p>
<p>Experimental Setup</p>
<p>Dataset.We conduct experiments on VLN-CE dataset [47].The dataset has 16, 844 trajectory-instruction pairs across 90 Matterport3D [8] scenes, and is divided into four sets, i.e., train (10, 819 pairs, 61 scenes), val seen (778 pairs, 53 scenes), val unseen (1, 839 pairs, 11 scenes), and test (3, 408 pairs, 18 scenes).As the scenes in val unseen and test are not exposed in train, the performances on val unseen and test are more important than val seen.Evaluation Metric.Following [47,45,46], we use five metrics for evaluation, i.e., Navigation Error (NE), Trajectory Length (TL), Success Rate (SR), Oracle success Rate (OR), and Success rate weighted by Path Length (SPL), where SR is the priority.Please see [5,6] for full details on metrics.</p>
<p>Implementation Details</p>
<p>Network Architecture.As in [45,46,47,30], the RGB and depth channels of the panoramic observation Y are respec-tively encoded by ImageNet [71] pre-trained ResNet-18 [27] and PointGoal [93] pre-trained ResNet-50.The language instruction X is encoded through a bi-LSTM with GLoVE [34] word embeddings.Scene Synthesizer is built as a two-stage generator, following [44].Waypoint Predictor is the one used in [30].For the distance function F d , GAT is implemented as standard [81], and MLP has 1, 024 hidden neurons.Network Training.The training ofWaypoint Predictor and Scene Synthesizer are scheduled as in [30] and [44] respectively.For our distance function F d , we progressively construct EGs along the ground-truth trajectory of each training episode.F d is trained by minimizing the L2 loss between the predicted distance and ground-truth distance for each waypoint for each EG.More specifically, for robust distance prediction, for a ground-truth trajectory with L waypoints, we construct L EGs where the l-th EG is constructed by adding the l-th waypoint of the trajectory as well as up to 5 random sampled, accessible waypoints into the (l − 1)th EG.In addition, we randomly replace ground-truth waypoints with the ones created by Scene Synthesizer, making F d better adapted to the working mode of mental planning.Reproducibility.Our hyper-parameters are set as follows.The discount factor γ is 0.98 (Eq.6 and 13).The horizon of mental planning, i.e., the maximum step of imagining the future trajectory, is 4. The number of search iterations for each time of mental planning is 50.Our model is implemented in PyTorch with Habitat [73] simulator, and trained on one NVIDIA RTX 3090 GPU.Our code is released at https: //github.com/hanqingwangai/Dreamwalker.</p>
<p>Comparison with VLN-CE State-of-the-Arts</p>
<p>We compare our agent, DREAMWALKER, with five previously published VLN-CE models [47,45,70,30,46].As illustrated in Table 1, our agent outperforms all the competitors across all the splits, in terms of SR.In particular, DREAMWALKER surpasses BridgingGap [30], the current top-leading VLN-CE model, by 7%, 5%, and 7%, on val seen, val unseen, and test splits, respectively.Since all the involved competitors are model-free approaches, our promising results demonstrate the superiority of our world model based algorithmic design, which relieves the burden on the agent to learn the knowledge about the environments during navigation policy training.In addition, we can find that DREAMWALKER boosts SR score greatly without introducing much extra travel expense.This evidences that, compared to greedily following a sophisticated navigation policy, planning ahead in the mental world enables advanced decision-making with little expense of physical execution.</p>
<p>How Far We Are From a Perfect World Model?</p>
<p>The world model allows our DREAMWALKER to employ 'mental imagery' to perform mental experiments, so as to plan ahead before taking action.The previous experiments  1: Impacts of core method components on VLN-CE dataset [47] ( §4.4- §4.5).(cf.§4.3 and Table 1) primarily demonstrated the power of the world model in strategic navigation planning, through the comparison with existing model-freeVLN-CE agents.To help highlight how far we are from a perfect world model, we derive a variant -"perfect imagination" -from our algorithm, by replacing the future scenarios forecas- sted by our world model with the corresponding actual ahead observations from the environment.For completeness, we also provide a "lazy" world model -"copy memory" -which simply memorizes all past observations and generates future predictions by simply copying the nearest memory.From Table 2 we can find that, compared with DREAMWALKER, "perfect imagination" yields solid performance boost.This is because the agent coupled with a perfect world model can make perfect prediction of the future.This also suggests the upperbound of our performance w.r.t.world model, and is consistent with prior studies [87,44] which find that allowing agent to look ahead at actual observations from environments can facilitate decision-making in the abstract setting.At the other extreme, "copy memory", not surprisingly, gives the worst results, since the agent does not make any imagination of the future.This verifies again the benefit of world model in navigation planning.</p>
<p>Advanced Planning or Greedy Selection?</p>
<p>It is also interesting to quantify the contribution of our world model based mental planning.To this end, in Table 2, we report a baseline -"greedy selection": at each decisionmaking step, the agent greedily selects the waypoint in the view of field for navigation, according to the estimated distance to the target location.This decision-making strategy is adopted by all current VLN-CE agents, yet lacking explicit planning.Our world model enables sampling-based planning -first assessing the consequences of possible navigation solutions in the mind then taking actual action.From Table 2 we can find that, mental planning exhibit significantly better performance compared with the greedy action selection strategy.This evidences our primary hypothesis that imagination of possible futures in the internal world abstract is necessary for strategic navigation planning.</p>
<p>What Is the Impact of Distance Function?</p>
<p>During mental planning, DREAMWALKER makes use of the distance function F d (cf.Eq. 9) to calculate the immedi-ate reward R (cf.Eq. 11) for a certain action.In other words, DREAMWALKER assesses the outcome of mental plans by means of the distance function.We therefore conduct experiments to study the influence of the distance function.Specifically, we report the performance by randomly replacing the distance estimates with the ground-truth in different probabilities.The performance are plotted in curves in Fig. 4. We can observe that, when the distance estimation becomes more accurate, DREAMWALKER performs better.With perfect distance estimation -the agent is allowed to access accurate distance between any waypoint and the target goal, 100% SR can be reached.We also find that, even when the replacement probability is as low as 20%, the agent can achieve a rather high 70 SR.It demonstrates the central role of the distance function in our algorithm and, also, suggests a feasible direction for further improvement.</p>
<p>Hyper-Parameter Study</p>
<p>In this section, we examine our hyper-parameter setup.Search Iterations of MCTS.We first investigate the influence of searching iterations in MCTS based mental planning.Intuitively, with more searching rounds, the search tree has a higher probability to reach a good terminal state and can better estimate the outcome of possible actions, yet, at the expense of larger simulation cost.Therefore, in addition to estimating the navigation performance with different searching rounds (i.e., 10, 30, 50, 70), we also report statistics for the runtime.As shown in Table 3, the runtime grows linearly as the number of searching rounds increases.However, when the number of searching rounds exceeds 50, the performance gain becomes marginal.Thus we finally set the number of searching rounds to 50 to save the unproductive computation cost.We also stress that our mental planning is very fast which typically responds within 1.5s.Horizons of Mental Planning.We report the performance with different planning horizons -imaging future trajectories with a maximum length of 0, 2, 4, or 6 forward steps.From Table 4 we can find that, both the performance and   the runtime generally grows as the horizons increased from 0 to 4 steps.Comparing #3 and #4 rows, we can find that the performance gain becomes marginal or even negative.This happens probably due to that the capacity of the world model is overloaded.As in [44], we assess the fidelity of our synthesized panoramic RGBD views by computing Fréchet Inception Distance (FID) [28] to the ground-truth scenes.</p>
<p>As shown in Fig. 5, the error between simulated views and corresponding actual observations from real environments accumulates as the trajectory rolls out.Hence the scenarios forecasted over long horizons might be useless or even detrimental to mental planning.</p>
<p>Qualitative Result</p>
<p>To better understand the superior performance of our method and demonstrate the interpretability of the mental planning mechanism, we analyse a challenging qualitative case in Fig. 6.In this case, we visualize the navigation trajectory performed by a greedy policy (i.e., greedily selecting the best waypoint predicted by the distance function) and our DREAMWALKER.As seen, given a complicated instruction "Walk out • • • to outside.", the agent with the greedy policy soon gets lost after it enters the dining room, while our DREAMWALKER manages to reach the target location.For an intuitive comprehension of the planning procedure, here we visualize a part of the search tree.The nodes and edges in the search tree are colored according to their corresponding V (s) and Q(s, a) values, i.e., red color indicates high value and blue color indicates low value.We can clearly observe that the search tree is split into two branches when the planning proceeds to "walk into the dining area", i.e., DREAMWALKER starts to imagine the outcomes of two possible actions, and the branch of the correct action finally wins with a large margin as the branch of the wrong action receives rather low rewards (deep blue).We visualize the imagined future waypoints of the correct action and the wrong action respectively and find that the correct action leads to a room which looks more like a "living room" mentioned in the instruction, while the wrong action leads to a corridor.This study demonstrates that our DREAMWALKER can provide strong interpretability of decision making by conducting tractable planning and presenting intuitive visualization for imagined observations.</p>
<p>Conclusion and Discussion</p>
<p>In this article, we devise DREAMWALKER, a world model based VLN-CE agent.Our world model is built as a discrete and structured abstraction of the continuous environment, allowing DREAMWALKER to synthesize, assess, and interpret possible plans in the mind before taking actual actions.We empirically confirm the superiority of DREAMWALKER over existing model-free agents, in terms of performance and interpretability.Building more expressive and versatile world models, and exploring world model based VLN-CE policy learning will be the focus of our future work.</p>
<p>DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation</p>
<p>Supplementary Material</p>
<p>This document sheds more details of our approach and additional experimental results, organized as follows:</p>
<p>• §I Implementation Details.</p>
<p>• §II Qualitative Results.• §III Discussions.</p>
<p>I. Implementation Details</p>
<p>Experimental Configurations.We utilize the observation space adopted in prior works [30,45,46] where the agent perceives 12 single-view RGBD images at horizontal 30 degrees separation for each move.The size of RGB images and depth map is 224×224 and 256×256 respectively.The vertical FOV of the camera is 90 • .Following [47,30], we apply an ImageNet [71]-pretrained ResNet50 [27] for RGB feature extraction, and a modified ResNet50 pretrained in point-goal navigation [73] for depth observation.Those pretrained ResNet50 backbones are frozen during training.Sliding is enabled in the evaluation phase.Details of World Model.Our world model consists mainly of a Waypoint Predictor ( §3.1) and a Scene Synthesizer ( §3.1).An overview of our world model is illustrated in Figure I.The Waypoint Predictor generates a heatmap based on the panoramic RGBD observation.Each pixel in the heatmap represents the probability of a waypoint's existence in a certain direction and at a certain distance.We use non-maximum suppression (NMS) to limit the number of candidate waypoints to 5. The Scene Synthesizer generates a plausible observation for a new waypoint in two steps: 1) The RGBD and semantic map of the initial waypoint, denoted as v, are unprojected to 3D space as point cloud.The point cloud is then reprojected to the image plane of the camera at the new waypoint, denoted as v ′ , as guidance images (i.e.RGB, depth, and semantic images).2) The guidance images and the RGB observation at v is fed into a generative network to synthesize RGBD and semantic images for v ′ .It is worth noting that the input of the Waypoint Predictor is a panorama represented by 12 separated single-view images, whereas the input of the Scene Synthesizer is a single 360 • panoramic image.To align the input format, we perform equirectangular projection to convert the subviews to a panoramic image for the Scene Synthesizer.Following [44], we apply the Red-Net [35] to predict the initial semantic map based on the RGBD observation.Learning of Distance Function.We utilize the R2R training split for the learning of the distance function F d ( §3.2).The learning goal of F d is to minimize the L2 loss between the predicted distance and the GT (ground-truth) distance of each waypoint in the built EG, i.e. environment graph ( §3.1).Concretely, for each training episode, we progressively build EGs as moving along the GT trajectory.The GT trajectory is the shortest path to the target position following the waypoints predicted by the Waypoint Predictor since the topological graphs of environments are not given in VLN-CE.As the EG during mental planning may contain other waypoints besides the GT waypoints, for robust distance prediction, we apply a random strategy to extend the EG during training.Specifically, at each step along the GT trajectory, we extend the EG by adding the next best waypoint and at most 5 other random sampled waypoints accessible from the waypoints in the current graph.The training loss of an episode is formulated as
Loss = G∈G v∈V ∥ F d (v, G, X) − dv ∥, (I)
where G is the set of all constructed EGs in this episode, V is the waypoint set of G, d v is the GT distance of the waypoint v, and X is the language instruction of this episode.As F d predicts the distances of waypoints based on the synthesized observations during mental planning, we randomly replace GT observations in EG with observations synthesized by Scene Synthesizer to make F d better adapted to inference.For fast convergence, we first train the network using the GT observations for 10 epoches and then finetune the network on EGs where the observations of some waypoints are ranodmly replaced by the synthesized ones for another 10 epoches.We adopt AdamW optimizer [54] for network training with the learning rate set to 2.5e-5 and set the batch size to 16.We apply max clip gradient normalization to all parameters to stabilize the training.Mental Planning.For a comprehensive understanding of the mental planning procedure, we present a pythonstyle pseudo code in Alg.I. Our DREAMWALKER agent is equipped with a world model to "imagine" state transitions of taking actions, and a distance function for reward prediction and fast rollout.To perform mental planning for a step of decision making, the agent calls search function with the current EG initState, the discount factor gamma and the exploration constant C as arguments, and gets the best action as the next action to make after rounds of search.</p>
<p>The factor gamma used in experiments is 0.98 and C is 1.0.</p>
<p>II. Qualitative Results</p>
<p>In this section, we present some qualitative results of navigation process on the val unseen split of R2R  We also observe some flaws of synthesized observations.In particular, the synthesized observation is blurry when the projection of the point cloud is sparse.The reason for this issue is three-fold:
Waypoint Net Visited Waypoint Unvisited Waypoint Generator Heatmap Waypoint Predictor Predicted Waypoints Scene Synthesizer Environment Graph Imagined
• The new waypoint is greatly occluded from the initial location.It means that the details of the room is not observable from the opposite perspective, which leads to sparse guidance images.As shown in Several workarounds may help alleviate the aforementioned issues.For instance, to prevent occlusions and threading up, a collision detection and avoidance mechanism could be developed to identify if a predicted waypoint would result in a collision with the environment's mesh.Should a collision be detected, the algorithm can recompute an alternative waypoint or adjust the current one to circumvent the obstacle.Moreover, further study the optimal maximum distance can help mitigate the sparse projection of guidance images caused by long distance point cloud translations.We consider addressing these issues as part of our future work.</p>
<p>III. Discussions</p>
<p>Limitations.Our work represents an initial effort that investigates world models in the context of challenging vision-language embodied tasks.As such, there are many aspects of this framework that warrant further study.We identify the limitations of our work in terms of world model, mental planning, and real-world applicability.At present, the Waypoint Predictor and the Scene Synthesizer are trained on the training split of Matterport3D [8] dataset, which comprises only 61 scenes.The limited training data restricts the quality and diversity of synthesized observations, particularly in terms of layout and appearance.Adding constraints such as interpenetration detection could improve these waypoint predictions.Furthermore, the quality of the synthesized views can be enhanced by replacing the scene synthesizer with diffusion-based methods [29].</p>
<p>The world model based mental planning may encounter computational challenges when scaling up to larger, more complex environments.The increased computational demands for simulating and evaluating potential plans could probably slow down the agent's decision-making process and decrease its overall efficiency.This high computational burden can be mitigated by maintaining a sliding buffer of navigation history.Currently, the agent navigates within static virtual environments.However, in real-world scenarios, its performance may be affected by dynamic elements in the environment, such as moving objects or changing conditions (e.g., lighting or crowdedness).Developing strategies to address these dynamic changes is crucial for the agent's effectiveness in real-world settings, and this will be considered in future work.Social Impact.DREAMWALKER's ability to make strategic planning through mental experiments can lead to more intelligent and interpretable decision-making, which can be beneficial when translating the agent to real-world applications.The agent's capacity to simulate future scenarios and make its decision-making process more transparent can build trust in the technology, making it more acceptable and useful to users.However, the current use of a simulated platform for testing and development may result in discrepancies between the simulation and real-world environments.This can limit the direct applicability of the agent's performance in real-world scenarios, potentially requiring further adaptation and fine-tuning.Additionally, as AI agents like DREAMWALKER continue to advance, it is crucial to consider the importance of accessibility and inclusivity in the design and implementation of these systems.If the technology is not developed with diverse user needs in mind, it may unintentionally exclude certain groups of people, such as those with disabilities or those from non-English speaking backgrounds.Ensuring that the agent can understand and process various languages, dialects, and cultural nuances, as well as catering to the needs of individuals with differing abilities, is essential for promoting equitable access to the benefits of embodied robots.</p>
<p>Head out and turn left.Pass a fireplace and continue towards the sofa.Enter the room and stop when seeing a table.</p>
<p>Figure 1 :
1
Figure 1: In partially observable, continuous VLN environments, DREAMWALKER maps its surrounding into a discrete and structured abstraction.In this internal world, it is able to conduct mental planning () by imagining future scenarios, before taking real action.</p>
<p>Instruction:</p>
<p>Head out and turn left.Pass a fireplace and continue towards the sofa.Enter the room and stop when seeing a table.</p>
<p>Figure 2 :
2
Figure 2: (a) Top-down view of current navigation, where indicates previously visited waypoints and refers to detected but unvisited waypoints.(b) DREAMWALKER synthesizes future observations at unvisited waypoints through SS. (c-d) With the synthesized observations, DREAMWALKER further extends the synthesized trajectories and looks deeper into the future.(e) DREAMWALKER selects the best mental plan for execution.After reaching the selected waypoint, it starts next-round planning.(f) Top-down view of the waypoint action space.(g) EG G of (a).(h-j) DREAMWALKER images its future observation at the unvisited waypoint , based on its current observation.The synthesized and real observations at are given in (i) and (j), respectively.For clarity, only RGB observation is provided.Notably, the imagined scenarios explain agent's inner decision mode in a way that human can understand, leading to improved interpretability.</p>
<p>Figure 3 :
3
Figure 3: MCTS based Mental planning.Each node in the research tree refers to a possible world state, corresponding to a future plan.a * = arg max a∈A(s 0 )Q(s0, a).(7)</p>
<p>Figure 4 :
4
Figure 4: Curves of success rate and distance estimation error ( §4.6).</p>
<p>Figure 5 :
5
Figure 5: FID curve of synthesized panoramic view w.r.t.prediction step ( §4.7).</p>
<p>Figure 6 :
6
Figure 6: (a) Trajectories of an episode navigated by a greedy policy (red) and DREAMWALKER (blue).(b) Current panoramic observation.(c) The search tree rooted by the world state of (a).The nodes and edges are painted according to their V (s) and Q(s, a) respectively.(d) Imagined view at waypoint A. (e) Imagined view at waypoint B. See §4.8 for more details.</p>
<p>Figure I: An overview of the world model.The world model enables the agent to imagine the future state of making an action.</p>
<p>Figure IV, the details of the room is greatly occluded by the wall.• The new waypoint is far away from the initial location.Due to the perspective principle, the point cloud is projected into a small range of pixels in the guidance images.A case is illustrated in Figure V. • The predicted new waypoint is threaded up the mesh of the environment.In this situation, the predicted waypoint is detrimental to navigation.As shown in Figure III, the predicted waypoint locates inside the wall.</p>
<p>Instruction:FigureFigureFigure</p>
<p>Figure II: DREAMWALKER navigates by imagining the future states of executing an action.</p>
<p>table sofa
sofaofaWorld Modelsofatablecontinuous environment
* Corresponding authors.</p>
<p>table</p>
<p>Model NE↓ TL SR↑ OR↑ SPL↑ NE↓ TL SR↑ OR↑ SPL↑ NE↓ TL SR ↑ OR↑ SPL↑ CMA [47][ECCV20] 7.21 9.06 34 44 32 7.60 8.27 29 36 27 7.91 8.85 28 36 25 Waypoint [45][ICCV21] 5.48 8.54 46 53 43 6.31 7.62 36 40 34 6.65 8.02 32 37 30 LAW [70][EMNLP21] 6.35 9.34 40 49 37 6.83 8.89 35 44 31 7.69 9.67 28 38 25 BridgingGap [30][CVPR22] 5.02 12.5 50 59 44 5.74 12.2 44 53 39 5.89 13.3 42 51 36 Sim2Sim [46][ECCV22] 4.67 11.2 52 61 44 6.07 10.7 43 52 36 6.17 11.4 44 52 37 DREAMWALKER (Ours) 4.09 11.6 59 66 48 5.53 11.3 49 59 44 5.48 11.8 49 57 44
val seenval unseentestTable</p>
<p>Table 2 :
2
[47]cts of core method components on VLN-CE dataset<a href="§4.4- §4.5">47</a>.
DREAMWALKER (Ours) 4.09 11.6 5966485.53 11.3 495944Perfect Imagination3.75 10.8 6469604.88 11.1 546349Copy Memory7.10 13.5 3544317.76 13.8 273524Greedy Selection5.22 10.5 4756435.93 10.9 425336
val seen val unseen Variant NE ↓ TL SR ↑ OR ↑ SPL ↑ NE ↓ TL SR ↑ OR ↑ SPL ↑</p>
<p>Table 3 :
3
TL SR ↑ OR ↑ SPL ↑ NE ↓ TL SR ↑ OR ↑ SPL ↑ (s/step) ↓ Impact of sampling iteration for navigation plan searching ( §4.7).TL SR ↑ OR ↑ SPL ↑ NE ↓ TL SR ↑ OR ↑ SPL ↑ (s/step) ↓
Searching # Iteration NE ↓ 1 10 4.43 12.1 55 val seen 6345val unseen 5.76 12.5 44 5538Runtime 0.432304.29 10.9 5765475.62 11.1 4657421.083504.09 11.6 5966485.53 11.3 4959441.434704.02 12.6 5967485.49 12.9 5060441.74Planning # Horizon NE ↓ 1 0 5.22 10.5 47 val seen 5643val unseen 5.93 10.9 42 5336Runtime 0.09224.21 11.8 5664455.66 12.2 4757411.15344.09 11.6 5966485.53 11.3 4959441.43464.18 12.5 5765445.59 12.9 4858412.05</p>
<p>Table 4 :
4
Impact of planning horizon ( §4.7).The maximum searching round is 50.</p>
<p>Algorithm I A python-style pseudo code of our DREAMWALKER.AlgorithmI A python style pseudo code of our DreamWalker.
ICCV #2447ICCV #2447ICCV 2023 Submission #2447. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.108162109163110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128class DreamWalker: def <strong>init</strong>(self, distance_func, world_model, search_round): self.distance_func = distance_func # the distance function self.world_model = world_model # the world model self.search_round = search_round Panoramic RGBD Sub-views # the searching round node = self._selection(self.root, C) reward = self.world_model.rollout(node.state, self.distance_func) self._backup(node, reward, gamma) RGB/Depth/Semantic Panorama RGB/Semantic 3D Point Cloud Panorama Guidance Images RGB/Depth/Semantic RGB/Semantic Synthesized def _executeRound(self, gamma, C): # perform a round of search RedNet bestChild = self._getBestChild(self.root, 0) action = bestChild.action return action Project Unproject self._executeRound(gamma, C) for i in range(self.search_round): self.root = treeNode(initState) def search(self, initState, gamma, C): # perform mental planning Equirectangular Project (Δ𝑥, Δ𝑦, Δ𝑧)164 165 166 167 168 169 170 174 175 176 178 180 181 182 179 177 173 172 171129 130 131 132 133 134 135 136def _selection(self, node, C): while not node.isTerminal: if node.isFullyExpanded: node = self._getBestChild(node, C) # selection phase else: return self._expand(node) return node183 184 185 186 187 188 189 190137 138 139 140 141 142 143 144def _expand(self, node): actions = node.state.getPossibleActions() # expansion phase for action in actions: if action not in node.children: newstate = self.world_model.transition(node.state, action) newNode = node.addChild(treeNode(newstate)) return newNode191 192 193 194 195 196 197 198145 146 147 148 149 150def _backup(self, node, reward, gamma): # backup phase while node is not None: node.numVisits += 1 node.totalReward += reward * (gamma ** node.depth) node = node.parent199 200 201 202 203 204151 152 153 154 155 156 157 158 159 160def _getBestChild(self, node, C): bestValue = float("-inf") for child in node.children.values(): nodeValue = child.totalReward / child.numVisits + C * math.sqrt( # UCT 2 * math.log(node.numVisits) / child.numVisits) if nodeValue &gt; bestValue: bestValue = nodeValue bestNode = child return bestNode205 206 207 208 209 210 211 212 213 214161215</p>
<p>Vision-only robot navigation in a neural radiance world. Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, Mac Schwager, IEEE Robotics and Automation Letters. 32022</p>
<p>Neighbor-view enhanced model for vision and language navigation. Dong An, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, Tieniu Tan, ACMMM. 2021</p>
<p>Bevbert: Topo-metric map pre-training for language-guided navigation. Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, Jing Shao, arXiv:2212.043852022arXiv preprint</p>
<p>Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, Liang Wang, arXiv:2304.03047Etpnav: Evolving topological planning for vision-language navigation in continuous environments. 2023arXiv preprint</p>
<p>On evaluation of embodied navigation agents. Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, arXiv:1807.067572018arXiv preprint</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, CVPR. 201816</p>
<p>Finitetime analysis of the multiarmed bandit problem. Peter Auer, Nicolo Cesa-Bianchi, Paul Fischer, Machine Learning. 2002</p>
<p>Matterport3D: Learning from RGB-D data in indoor environments. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, 20173S11</p>
<p>Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning. Jianyu Chen, Shengbo , Eben Li, Masayoshi Tomizuka, IEEE Transactions on Intelligent Transportation Systems. 22021</p>
<p>History aware multimodal transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev, NeurIPS2021</p>
<p>Think global, act local: Dual-scale graph transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev, CVPR. 2022</p>
<p>Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. Silvia Chiappa, Sébastien Racaniere, ICLR. 2017</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, NeurIPS. 2018</p>
<p>Unsupervised object-based transition models for 3d partially observable environments. Antonia Creswell, Rishabh Kabra, Chris Burgess, Murray Shanahan, NeurIPS. 2021</p>
<p>Evolving graphical planner: Contextual global planning for vision-and-language navigation. Zhiwei Deng, Karthik Narasimhan, Olga Russakovsky, NeurIPS. 22020</p>
<p>Deep visual foresight for planning robot motion. Chelsea Finn, Sergey Levine, ICRA. 2017</p>
<p>Counterintuitive behavior of social systems. Jay W Forrester, Theory and Decision. 221971</p>
<p>Speaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, NeurIPS. 22018</p>
<p>Shaping belief states with generative environment models for rl. Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, Aaron Van Den Oord, NeurIPS2019</p>
<p>Temporal difference variational auto-encoder. Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, Theophane Weber, ICLR. 2018</p>
<p>Deep learning for real-time atari game play using offline monte-carlo tree search planning. Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, Xiaoshi Wang, NeurIPS. 201434</p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jürgen Schmidhuber, NeurIPS. 2018</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, ICLR. 202023</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, ICML. 2019</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, Jimmy Ba, ICLR. 2020</p>
<p>Towards learning a generic agent for visionand-language navigation via pre-training. Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao, CVPR. 2020</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 20166S9</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, NeurIPS2017S8</p>
<p>Text2room: Extracting textured 3d meshes from 2d text-to-image models. Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, Matthias Nießner, 2023S11</p>
<p>Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. Yicong Hong, Zun Wang, Qi Wu, Stephen Gould, CVPR. 2022. 2, 3, 67S9</p>
<p>A recurrent vision-and-language bert for navigation. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, CVPR. 2021</p>
<p>Transferable representation learning in vision-and-language navigation. Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel Magalhaes, Jason Baldridge, Eugene Ie, ICCV. 2019</p>
<p>Stay on the path: Instruction fidelity in vision-and-language navigation. Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, Jason Baldridge, ACL. 2019</p>
<p>Glove: Global vectors for word representation. Richardsocher Jeffreypennington, Christopherd Manning, EMNLP. 2014</p>
<p>Rednet: Residual encoder-decoder network for indoor rgbd semantic segmentation. Jindong Jiang, Lunan Zheng, Fei Luo, Zhijun Zhang, arXiv:1806.010542018S9arXiv preprint</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. Philip Nicholas, Johnson-Laird , 1983Harvard University Press13</p>
<p>Mental models and human reasoning. Johnson-Laird Philip, Proceedings of the National Academy of Sciences. the National Academy of Sciences2010</p>
<p>Model based reinforcement learning for atari. Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, ICLR, 2019. 3</p>
<p>Deep variational bayes filters: Unsupervised learning of state space models from raw data. Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick Van Der Smagt, ICLR, 2017. 3</p>
<p>Tactical rewind: Self-correction via backtracking in vision-and-language navigation. Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa, CVPR. 2019</p>
<p>Learning dynamics model in reinforcement learning by incorporating the long term future. Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, Devi Parikh, Dhruv Batra, ICLR, 2019. 3</p>
<p>Active world model learning with progress curiosity. Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, Daniel Yamins, ICML. 2020</p>
<p>Bandit based montecarlo planning. Levente Kocsis, Csaba Szepesvári, ECML. 200624</p>
<p>Pathdreamer: A world model for indoor navigation. Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, Peter Anderson, CVPR. 2021. 3, 4, 6, 7, S8S9</p>
<p>Waypoint models for instructionguided navigation in continuous environments. Jacob Krantz, Aaron Gokaslan, Dhruv Batra, Stefan Lee, Oleksandr Maksymets, ICCV. 2021. 2, 3, 67S9</p>
<p>Sim-2-sim transfer for visionand-language navigation in continuous environments. Jacob Krantz, Stefan Lee, ECCV. 2022. 2, 3, 67S9</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee, ECCV. 2020. 1, 2, 3, 67S9</p>
<p>Room-Across-Room: Multilingual visionand-language navigation with dense spatiotemporal grounding. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge, EMNLP. 22020</p>
<p>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Alex X Lee, Anusha Nagabandi, Pieter Abbeel, Sergey Levine, NeurIPS2020</p>
<p>Predictive information accelerates learning in rl. Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, Sergio Guadarrama, NeurIPS2020</p>
<p>Walking with mind: Mental imagery enhanced embodied qa. Juncheng Li, Siliang Tang, Fei Wu, Yueting Zhuang, ACMMM. 2019</p>
<p>Improving generative imagination in object-centric world models. Zhixuan Lin, Yi-Fu Wu, Skand Peri, Bofeng Fu, Jindong Jiang, Sungjin Ahn, ICML. 2020</p>
<p>Bird's-eye-view scene graph for vision-language navigation. Rui Liu, Xiaohan Wang, Wenguan Wang, Yi Yang, ICCV. 2023</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. 2019S9</p>
<p>Selfmonitoring navigation agent via auxiliary progress estimation. Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Alregib, Zsolt Kira, Richard Socher, Caiming Xiong, ICLR. 2019</p>
<p>The regretful agent: Heuristic-aided navigation through progress estimation. Chih-Yao Ma, Zuxuan Wu, Ghassan Alregib, Caiming Xiong, Zsolt Kira, CVPR. 2019</p>
<p>Improving visionand-language navigation with image-text pairs from the web. Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra, ECCV. 2020</p>
<p>World-consistent video-to-video synthesis. Arun Mallya, Ting-Chun, Karan Wang, Ming-Yu Sapra, Liu, ECCV. 2020</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, Sergey Levine, ICRA. 2018</p>
<p>Visual reinforcement learning with imagined goals. Vitchyr Ashvin V Nair, Murtaza Pong, Shikhar Dalal, Steven Bahl, Sergey Lin, Levine, NeurIPS2018</p>
<p>Goal-aware prediction: Learning to model what matters. Suraj Nair, Silvio Savarese, Chelsea Finn, ICML. 202023</p>
<p>Action-conditional video prediction using deep networks in atari games. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, Satinder Singh, NeurIPS2015</p>
<p>Episodic transformer for vision-and-language navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, ICCV. 2021</p>
<p>Probabilistic planning with sequential monte carlo methods. Alexandre Piché, Valentin Thomas, Cyril Ibrahim, Yoshua Bengio, Chris Pal, ICLR. 2018</p>
<p>Learning real-world robot policies by dreaming. Alan Aj Piergiovanni, Michael S Wu, Ryoo, IROS. 22019</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, CVPR. 2020</p>
<p>Hop: history-and-order aware pretraining for vision-and-language navigation. Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, Qi Wu, CVPR. 2022</p>
<p>Hop+: History-enhanced and orderaware pre-training for vision-and-language navigation. Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, Qi Wu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 22023</p>
<p>Imagination-augmented agents for deep reinforcement learning. Sébastien Racanière, Théophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, NeurIPS2017</p>
<p>Language-aligned waypoint (law) supervision for vision-and-language navigation in continuous environments. Sonia Raychaudhuri, Saim Wani, Shivansh Patel, Unnat Jain, Angel Chang, EMNLP. 2021. 2, 3, 6, 7</p>
<p>ImageNet Large Scale Visual Recognition Challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, IJCV. 1153S92015</p>
<p>Learning what you can do before doing anything. Karl Oleh Rybkin, Pertsch, G Konstantinos, Kostas Derpanis, Andrew Daniilidis, Jaegle, ICLR. 201823</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, ICCV. 20196S9</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 58878392020</p>
<p>Planning to explore via self-supervised world models. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak, ICML. 2020</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 232016</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Nature. 32017</p>
<p>Universal planning networks: Learning generalizable representations for visuomotor control. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea Finn, ICML. 2018</p>
<p>Decoupling representation learning from reinforcement learning. Adam Stooke, Kimin Lee, Pieter Abbeel, Michael Laskin, ICML. 2021</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. Licheng Hao Tan, Mohit Yu, Bansal, NAACL. 2019</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, ICLR. 201856</p>
<p>Interpretable decision-making for autonomous vehicles at highway on-ramps with latent space reinforcement learning. Huanjie Wang, Hongbo Gao, Shihua Yuan, Hongfei Zhao, Kelong Wang, Xiulai Wang, Keqiang Li, Deyi Li, IEEE Transactions on Vehicular Technology. 22021</p>
<p>Towards versatile embodied navigation. Hanqing Wang, Wei Liang, Luc V Gool, Wenguan Wang, NeurIPS. 22022</p>
<p>Counterfactual cycle-consistent learning for instruction following and generation in visionlanguage navigation. Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool, Wenguan Wang, CVPR. 2022</p>
<p>Active perception for visual-language navigation. Hanqing Wang, Wenguan Wang, Wei Liang, C H Steven, Jianbing Hoi, Luc Shen, Van Gool, International Journal of Computer Vision. 22023</p>
<p>Structured scene memory for visionlanguage navigation. Hanqing Wang, Wenguan Wang, Wei Liang, Caiming Xiong, Jianbing Shen, CVPR. 2021</p>
<p>Active visual information gathering for vision-language navigation. Hanqing Wang, Wenguan Wang, Tianmin Shu, Wei Liang, Jianbing Shen, ECCV. 202027</p>
<p>Exploring model-based planning with policy networks. Tingwu Wang, Jimmy Ba, ICLR. 2020</p>
<p>. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang</p>
<p>Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. Lei Zhang, CVPR. 2019</p>
<p>Lana: A language-capable navigator for instruction following and generation. Xiaohan Wang, Wenguan Wang, Jiayi Shao, Yi Yang, CVPR. 2023</p>
<p>Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang, Wang , ECCV. 2018</p>
<p>Embed to control: A locally linear latent dynamics model for control from raw images. Manuel Watter, Jost Springenberg, Joschka Boedecker, Martin Riedmiller, NeurIPS2015</p>
<p>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, ICLR2019</p>
<p>Experienceembedded visual foresight. Lin Yen-Chen, Maria Bauza, Phillip Isola, CoRL. 202023</p>
<p>Targetdriven structured transformer planner for vision-language navigation. Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong Yang, Haibing Ren, Huaxia Xia, Si Liu, ACMMM. 2022</p>
<p>Vision-language navigation with self-supervised auxiliary reasoning tasks. Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang, CVPR. 2020</p>
<p>Diagnosing vision-and-language navigation: What really matters. Wanrong Zhu, Yuankai Qi, Pradyumna Narayana, Kazoo Sone, Sugato Basu, Xin , Eric Wang, Qi Wu, Miguel Eckstein, William Yang, Wang , NAACL. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5001 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5001</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5001</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-266693275</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.00757v2.pdf" target="_blank">LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs’ prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs’ learning of logical rules, with identified reasoning failures ranging from 29% to 90% across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5%. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs’ formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5001.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5001.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicAsker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicAsker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated Minimum Functionality Test (MFT) framework that generates atomic, natural-language logical inference tests (propositional and predicate logic) to diagnose and improve LLMs' formal reasoning by producing targeted in-context demonstrations and fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicAsker (evaluation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a language model; an automatic test-generation and improvement pipeline. It formalizes 34 atomic rules (propositional + predicate logic) and expands them to 208 extended skills, generates yes/no natural language test cases with varied vocabulary and inference lengths, identifies per-skill weaknesses, and constructs ICL demonstrations and fine-tuning data from failures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicAsker MFT suite (propositional & predicate logic; 34 atomic rules → 208 extended skills)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Minimum Functionality Tests mapping atomic inference/equivalence/fallacy rules into many yes/no natural language problems (inference, contradiction, unrelated, fallacy types), covering propositional operators, quantifiers, and predicate logic; supports variable inference chain lengths (1–9).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Rule-based logic-expression generation → template-based natural language translation; evaluate models zero-shot/CoT/ICL; construct weakness-targeted ICL demonstrations; generate synthetic fine-tuning datasets from tests and fine-tune models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Framework-level results: exposed logical failures with model accuracies ranging ≈78%–98% on sampled 5.2k cases (iteration 1) and as low as 10%–71% on weakness-focused cases (iteration 2); produced ICL and fine-tuning data that improved evaluated LLMs (e.g., GPT-4o from ~91.9% → 97.2% with weakness-targeted ICL; ChatGPT fine-tuned on LogicAsker weak/all data rose from 77.62% → 99.50% on LogicAsker).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generates synthetic natural-language formulations that can still be unnatural in edge cases (human validation shows ≥94% validity); improvements rely on LLMs' in‑context learning capability; remaining gap to human-level reasoning after ICL/fine-tuning; CoT sometimes degrades performance on synthetic strict-logic examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Claims broader, more atomic coverage than prior datasets (e.g., Ontañón et al. LogicInference, Han et al. FOLIO) by including propositional + predicate rules and fallacies and by using test outcomes to construct ICL/fine-tuning data to improve models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Compares Zero-shot, Zero-shot CoT, Random ICL, and Weakness ICL: Weakness ICL most effective; CoT sometimes worse than zero-shot. Fine-tuning on LogicAsker all-rules vs. weakness-only sets improves LogicAsker performance substantially; small generalization gains to LogiQA after fine-tuning were observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5001.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5001.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o (gpt-4o-2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source OpenAI conversational LLM accessed via API and evaluated on LogicAsker tests; used for zero-shot, CoT, ICL experiments and reported fine-tuning/ICL improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (gpt-4o-2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI closed-source conversational LLM (API id used in experiments: gpt-4o-2024-05-13), accessed with temperature=0 and max_tokens=500; exact architecture/parameter count not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicAsker MFT suite (and weakness dataset generated from LogicAsker); also evaluated for generalization to LogiQA after fine-tuning of other models (paper reports GPT-4o improvements via ICL).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary yes/no formal-logic questions derived from propositional and predicate rules, including equivalence, inference, quantifier manipulations, fallacies, with varied vocabulary and inference chain lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated zero-shot and with Zero-shot CoT; provided Random ICL demonstrations and Weakness ICL demonstrations generated by LogicAsker; fine-tuning not reported for this model (ICL used to improve performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot accuracy on sampled LogicAsker data: 91.92%; Zero-shot CoT: 92.94%; Random ICL: 95.77%; Weakness-targeted ICL: 97.23% (improvement ≈ +5.3 percentage points from zero-shot to weakness-ICL). On weakness-focused test subset, accuracy dropped substantially (some skills below random).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on specific atomic predicate rules and recognizing certain fallacies (paper reports weaknesses across skills; longer inference chains reduce accuracy); some rules produce accuracies much lower than model-average; CoT can sometimes induce commonsense heuristics and worsen performance on synthetic strict-logical cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Strong performer among evaluated models but improved notably with targeted ICL; underperforms GPT-4 (in some metrics GPT-4 leads) in Table 5 highest ICL-weak value reported for GPT-4, but GPT-4o shows large gain from ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Table 5 ablation across prompting strategies shows progressive improvements: zero-shot → CoT → Random ICL → Weakness ICL; shows sensitivity to demonstration selection and inference length effects (accuracy declines with longer chains per Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5001.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5001.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-turbo-2024-04-09)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source OpenAI LLM evaluated on LogicAsker; high overall accuracy but with identifiable weaknesses on specific atomic rules and fallacy recognition; benefits from weakness-targeted ICL and shows some CoT sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo-2024-04-09)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI closed-source LLM accessed via API (model id used: gpt-4-turbo-2024-04-09) with temperature=0 and max_tokens=500; architecture/parameter count not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicAsker MFT suite (34 atomic rules → 208 extended rules); per-rule evaluation and weakness dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Yes/no natural language problems reflecting propositional and predicate logic inference/equivalence/fallacy rules, including quantifiers and variable predicates, and varying inference lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated zero-shot, Zero-shot CoT, Random ICL, and Weakness ICL; also included in broader comparisons and per-rule breakdowns; not fine-tuned in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: 97.75% (Table 5); Zero-shot CoT: 96.60%; Random ICL: 97.98%; Weakness ICL: 99.48% (best reported). Overall average accuracy high (~98% on many sampled cases), but per-rule breakdown shows some atomic predicate/inference rules with much lower accuracy (e.g., specific rules where GPT-4 scored 60%–68%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on certain atomic predicate rules and fallacy recognition despite high aggregate accuracy; longer inference chains reduce accuracy; CoT sometimes causes reliance on commonsense heuristics leading to incorrect strict-logical deductions in synthetic cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Top-performing in many settings (highest ICL-Weak value in Table 5) and better than ChatGPT/Gemini/Mixtral/Llama3 on average; still has uneven per-rule competence (some atomic skills poorly learned).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Complete per-rule breakdown (Table 13) and ablations across prompting strategies show that carefully chosen ICL demonstrations (weakness-targeted) outperform Random ICL and CoT; analysis of accuracy vs inference length (Table 12) shows performance decays with chain length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5001.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5001.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI gpt-3.5 family conversational model evaluated and fine-tuned on LogicAsker synthetic data; shows lower zero-shot reasoning performance but large gains after fine-tuning on LogicAsker.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo conversational LLM accessed via API (gpt-3.5-turbo-0125), temperature=0, max_tokens=500; closed-source with smaller capability than GPT-4 series.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicAsker MFT suite (primary) and LogiQA (for generalization after fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary logical inference problems in natural language based on atomic propositional/predicate rules; LogiQA used as independent reading-comprehension logical reasoning benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot / Zero-shot CoT / Random ICL / Weakness ICL evaluations; fine-tuning experiments: fine-tuned on 5.2k LogicAsker-all rules and 2.8k weakness-only datasets produced by LogicAsker.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot on LogicAsker: 77.62% (Table 5). After fine-tuning: FT(All) on LogicAsker = 99.50%; FT(Weak) on LogicAsker = 97.83% (Table 6). On LogiQA test split: vanilla 40.55%, FT(All) 41.01%, FT(Weak) 41.78% (small generalization gain).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relatively weak zero-shot formal logical ability compared to larger models; struggles particularly on predicate logic/quantifier manipulation and fallacy recognition; large improvements require fine-tuning (ICL helps but less than fine-tuning), and generalization to out-of-distribution human datasets (LogiQA) is modest.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs substantially worse zero-shot than GPT-4/4o/Gemini/Llama3 on LogicAsker, but fine-tuning on LogicAsker data can raise performance to near top-tier on the same synthetic suite; generalization to external benchmark (LogiQA) limited.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Fine-tuning ablation: fine-tuning on all-rule LogicAsker data yields larger gains on LogicAsker than fine-tuning only on weakness cases, but weakness-only tuning still improves targeted skills; ICL Weakness demonstrations help more than Random ICL or CoT for ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5001.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5001.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini-1.5 (gemini-1.5-flash-latest)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini-1.5 family model accessed via API and evaluated on LogicAsker under zero-shot, CoT, and ICL settings; shows mid-to-high performance with benefits from weakness-targeted ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5 (gemini-1.5-flash-latest)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google closed-source LLM (model id used: gemini-1.5-flash-latest), accessed via API with temperature=0 and max_tokens=500; exact size not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicAsker MFT suite (propositional & predicate logic tests).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Yes/no tests derived from formal logic rules including equivalences, inference, quantifiers, and fallacies; multiple inference lengths and problem types.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated zero-shot, CoT, Random ICL, and Weakness ICL using LogicAsker-generated demonstrations and weakness dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: 92.06%; Zero-shot CoT: 93.62%; Random ICL: 96.13%; Weakness ICL: 96.67% (Table 5). Performance declines on weakness-only cases and with increasing inference length.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower accuracy on predicate logic relative to propositional tasks in some cases; difficulty recognizing fallacies (paper states most LLMs weak in fallacy recognition, with exceptions including Gemini and Mixtral); accuracy decreases with longer inference chains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Comparable to GPT-4o in zero-shot/ICL performance but slightly below the absolute best (GPT-4) on ICL-weak metric; better than smaller/open-source models overall in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Improvement hierarchy observed: Weakness ICL > Random ICL > CoT > Zero-shot; per-rule breakdowns and inference-length analysis show consistent degradation with longer chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5001.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5001.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3-70B (meta-llama/Meta-Llama-3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Llama3 70B model obtained from Hugging Face and evaluated on LogicAsker; shows good but variable performance on formal logical tasks and benefits from targeted ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B (Meta-Llama-3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM from Meta (70B parameter family indicated by name, accessed via Hugging Face repo meta-llama/Meta-Llama-3-70B); evaluated with temperature=0 and max_tokens=500.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicAsker MFT suite (propositional and predicate logic tests).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary logical inference/equivalence/fallacy problems constructed from atomic rules; includes predicate quantifiers and multi-step inference chains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot / Zero-shot CoT / Random ICL / Weakness ICL evaluations using LogicAsker-generated demonstrations and weakness dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: 91.02%; Zero-shot CoT: 94.54%; Random ICL: 94.83%; Weakness ICL: 93.35% (Table 5). Performance drops with longer inference lengths (Table 12) and on some specific predicate/fallacy rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows weaker performance on complex predicate-logic rules, certain fallacies, and longer inference chains; variability across rules indicates gaps in atomic skill learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs competitively with Gemini and GPT-4o in some settings but below GPT-4 on average; benefits from high-quality demonstrations though gains are model-dependent (Weakness ICL not uniformly best for Llama3 in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Per-rule breakdowns (Tables 16) show specific low-accuracy rules; inference-length analysis confirms monotonic degradation with length; ICL selection (weakness-targeted) yields variable improvement compared to Random ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5001.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5001.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral Mixtral-8x7B (mistralai/Mixtral-8x7B-v0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source expert-mixture model from Mistral evaluated on LogicAsker; shows lower baseline and mixed improvement from ICL; relatively weaker on some strict logical tasks and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B (mistralai/Mixtral-8x7B-v0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source mixture-of-experts LLM provided by Mistral and available on Hugging Face (repo used: mistralai/Mixtral-8x7B-v0.1). Exact aggregate parameter count not specified in paper; model referenced as Mixtral-8x7B.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicAsker MFT suite (propositional & predicate logic yes/no questions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Atomic and extended logical reasoning tasks including inference, equivalence, quantifiers, and fallacy recognition across varying inference lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot / Zero-shot CoT / Random ICL / Weakness ICL evaluations using LogicAsker demonstrations; accessed from Hugging Face with temperature=0 and max_tokens=500.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: 86.77%; Zero-shot CoT: 86.23%; Random ICL: 76.40%; Weakness ICL: 82.02% (Table 5). Random ICL degraded performance for this model in the reported experiments; weakness-targeted ICL recovered some gains but not to top-tier levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows lower baseline reasoning accuracy and sensitivity to demonstration selection (Random ICL sometimes decreased performance); struggles especially on predicate/fallacy tasks and long inference chains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms GPT-4/GPT-4o/Gemini and Llama3 on average in the LogicAsker evaluation; Mixtral is an exception in that it performed relatively better in recognizing some fallacies in earlier analysis but overall weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>ICL ablation shows that Random ICL can harm performance for Mixtral whereas weakness-targeted ICL helps; per-rule breakdowns (Tables 14/15 etc.) reveal specific failure modes; inference-length analysis shows monotonic accuracy decline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logicinference: A new dataset for teaching logical inference to seq2seq models <em>(Rating: 2)</em></li>
                <li>LogiQA: A dataset for evaluating logical reasoning in reading comprehension <em>(Rating: 2)</em></li>
                <li>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 2)</em></li>
                <li>FOLIO: A dataset with first-order logic problems <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Testing the general deductive reasoning capacity of large language models using OOD examples <em>(Rating: 1)</em></li>
                <li>ProntoQA-OOD <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5001",
    "paper_id": "paper-266693275",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "LogicAsker",
            "name_full": "LogicAsker",
            "brief_description": "An automated Minimum Functionality Test (MFT) framework that generates atomic, natural-language logical inference tests (propositional and predicate logic) to diagnose and improve LLMs' formal reasoning by producing targeted in-context demonstrations and fine-tuning data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogicAsker (evaluation framework)",
            "model_description": "Not a language model; an automatic test-generation and improvement pipeline. It formalizes 34 atomic rules (propositional + predicate logic) and expands them to 208 extended skills, generates yes/no natural language test cases with varied vocabulary and inference lengths, identifies per-skill weaknesses, and constructs ICL demonstrations and fine-tuning data from failures.",
            "model_size": null,
            "logical_reasoning_task": "LogicAsker MFT suite (propositional & predicate logic; 34 atomic rules → 208 extended skills)",
            "task_description": "Minimum Functionality Tests mapping atomic inference/equivalence/fallacy rules into many yes/no natural language problems (inference, contradiction, unrelated, fallacy types), covering propositional operators, quantifiers, and predicate logic; supports variable inference chain lengths (1–9).",
            "method_or_approach": "Rule-based logic-expression generation → template-based natural language translation; evaluate models zero-shot/CoT/ICL; construct weakness-targeted ICL demonstrations; generate synthetic fine-tuning datasets from tests and fine-tune models.",
            "performance": "Framework-level results: exposed logical failures with model accuracies ranging ≈78%–98% on sampled 5.2k cases (iteration 1) and as low as 10%–71% on weakness-focused cases (iteration 2); produced ICL and fine-tuning data that improved evaluated LLMs (e.g., GPT-4o from ~91.9% → 97.2% with weakness-targeted ICL; ChatGPT fine-tuned on LogicAsker weak/all data rose from 77.62% → 99.50% on LogicAsker).",
            "limitations_or_failure_cases": "Generates synthetic natural-language formulations that can still be unnatural in edge cases (human validation shows ≥94% validity); improvements rely on LLMs' in‑context learning capability; remaining gap to human-level reasoning after ICL/fine-tuning; CoT sometimes degrades performance on synthetic strict-logic examples.",
            "comparison": "Claims broader, more atomic coverage than prior datasets (e.g., Ontañón et al. LogicInference, Han et al. FOLIO) by including propositional + predicate rules and fallacies and by using test outcomes to construct ICL/fine-tuning data to improve models.",
            "ablation_or_analysis_results": "Compares Zero-shot, Zero-shot CoT, Random ICL, and Weakness ICL: Weakness ICL most effective; CoT sometimes worse than zero-shot. Fine-tuning on LogicAsker all-rules vs. weakness-only sets improves LogicAsker performance substantially; small generalization gains to LogiQA after fine-tuning were observed.",
            "uuid": "e5001.0",
            "source_info": {
                "paper_title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "OpenAI GPT-4o (gpt-4o-2024-05-13)",
            "brief_description": "Closed-source OpenAI conversational LLM accessed via API and evaluated on LogicAsker tests; used for zero-shot, CoT, ICL experiments and reported fine-tuning/ICL improvements.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (gpt-4o-2024-05-13)",
            "model_description": "OpenAI closed-source conversational LLM (API id used in experiments: gpt-4o-2024-05-13), accessed with temperature=0 and max_tokens=500; exact architecture/parameter count not specified in paper.",
            "model_size": null,
            "logical_reasoning_task": "LogicAsker MFT suite (and weakness dataset generated from LogicAsker); also evaluated for generalization to LogiQA after fine-tuning of other models (paper reports GPT-4o improvements via ICL).",
            "task_description": "Binary yes/no formal-logic questions derived from propositional and predicate rules, including equivalence, inference, quantifier manipulations, fallacies, with varied vocabulary and inference chain lengths.",
            "method_or_approach": "Evaluated zero-shot and with Zero-shot CoT; provided Random ICL demonstrations and Weakness ICL demonstrations generated by LogicAsker; fine-tuning not reported for this model (ICL used to improve performance).",
            "performance": "Zero-shot accuracy on sampled LogicAsker data: 91.92%; Zero-shot CoT: 92.94%; Random ICL: 95.77%; Weakness-targeted ICL: 97.23% (improvement ≈ +5.3 percentage points from zero-shot to weakness-ICL). On weakness-focused test subset, accuracy dropped substantially (some skills below random).",
            "limitations_or_failure_cases": "Struggles on specific atomic predicate rules and recognizing certain fallacies (paper reports weaknesses across skills; longer inference chains reduce accuracy); some rules produce accuracies much lower than model-average; CoT can sometimes induce commonsense heuristics and worsen performance on synthetic strict-logical cases.",
            "comparison": "Strong performer among evaluated models but improved notably with targeted ICL; underperforms GPT-4 (in some metrics GPT-4 leads) in Table 5 highest ICL-weak value reported for GPT-4, but GPT-4o shows large gain from ICL.",
            "ablation_or_analysis_results": "Table 5 ablation across prompting strategies shows progressive improvements: zero-shot → CoT → Random ICL → Weakness ICL; shows sensitivity to demonstration selection and inference length effects (accuracy declines with longer chains per Table 12).",
            "uuid": "e5001.1",
            "source_info": {
                "paper_title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "OpenAI GPT-4 (gpt-4-turbo-2024-04-09)",
            "brief_description": "Closed-source OpenAI LLM evaluated on LogicAsker; high overall accuracy but with identifiable weaknesses on specific atomic rules and fallacy recognition; benefits from weakness-targeted ICL and shows some CoT sensitivity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo-2024-04-09)",
            "model_description": "OpenAI closed-source LLM accessed via API (model id used: gpt-4-turbo-2024-04-09) with temperature=0 and max_tokens=500; architecture/parameter count not specified in paper.",
            "model_size": null,
            "logical_reasoning_task": "LogicAsker MFT suite (34 atomic rules → 208 extended rules); per-rule evaluation and weakness dataset.",
            "task_description": "Yes/no natural language problems reflecting propositional and predicate logic inference/equivalence/fallacy rules, including quantifiers and variable predicates, and varying inference lengths.",
            "method_or_approach": "Evaluated zero-shot, Zero-shot CoT, Random ICL, and Weakness ICL; also included in broader comparisons and per-rule breakdowns; not fine-tuned in this study.",
            "performance": "Zero-shot: 97.75% (Table 5); Zero-shot CoT: 96.60%; Random ICL: 97.98%; Weakness ICL: 99.48% (best reported). Overall average accuracy high (~98% on many sampled cases), but per-rule breakdown shows some atomic predicate/inference rules with much lower accuracy (e.g., specific rules where GPT-4 scored 60%–68%).",
            "limitations_or_failure_cases": "Fails on certain atomic predicate rules and fallacy recognition despite high aggregate accuracy; longer inference chains reduce accuracy; CoT sometimes causes reliance on commonsense heuristics leading to incorrect strict-logical deductions in synthetic cases.",
            "comparison": "Top-performing in many settings (highest ICL-Weak value in Table 5) and better than ChatGPT/Gemini/Mixtral/Llama3 on average; still has uneven per-rule competence (some atomic skills poorly learned).",
            "ablation_or_analysis_results": "Complete per-rule breakdown (Table 13) and ablations across prompting strategies show that carefully chosen ICL demonstrations (weakness-targeted) outperform Random ICL and CoT; analysis of accuracy vs inference length (Table 12) shows performance decays with chain length.",
            "uuid": "e5001.2",
            "source_info": {
                "paper_title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (gpt-3.5-turbo-0125)",
            "brief_description": "OpenAI gpt-3.5 family conversational model evaluated and fine-tuned on LogicAsker synthetic data; shows lower zero-shot reasoning performance but large gains after fine-tuning on LogicAsker.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0125)",
            "model_description": "OpenAI gpt-3.5-turbo conversational LLM accessed via API (gpt-3.5-turbo-0125), temperature=0, max_tokens=500; closed-source with smaller capability than GPT-4 series.",
            "model_size": null,
            "logical_reasoning_task": "LogicAsker MFT suite (primary) and LogiQA (for generalization after fine-tuning).",
            "task_description": "Binary logical inference problems in natural language based on atomic propositional/predicate rules; LogiQA used as independent reading-comprehension logical reasoning benchmark.",
            "method_or_approach": "Zero-shot / Zero-shot CoT / Random ICL / Weakness ICL evaluations; fine-tuning experiments: fine-tuned on 5.2k LogicAsker-all rules and 2.8k weakness-only datasets produced by LogicAsker.",
            "performance": "Zero-shot on LogicAsker: 77.62% (Table 5). After fine-tuning: FT(All) on LogicAsker = 99.50%; FT(Weak) on LogicAsker = 97.83% (Table 6). On LogiQA test split: vanilla 40.55%, FT(All) 41.01%, FT(Weak) 41.78% (small generalization gain).",
            "limitations_or_failure_cases": "Relatively weak zero-shot formal logical ability compared to larger models; struggles particularly on predicate logic/quantifier manipulation and fallacy recognition; large improvements require fine-tuning (ICL helps but less than fine-tuning), and generalization to out-of-distribution human datasets (LogiQA) is modest.",
            "comparison": "Performs substantially worse zero-shot than GPT-4/4o/Gemini/Llama3 on LogicAsker, but fine-tuning on LogicAsker data can raise performance to near top-tier on the same synthetic suite; generalization to external benchmark (LogiQA) limited.",
            "ablation_or_analysis_results": "Fine-tuning ablation: fine-tuning on all-rule LogicAsker data yields larger gains on LogicAsker than fine-tuning only on weakness cases, but weakness-only tuning still improves targeted skills; ICL Weakness demonstrations help more than Random ICL or CoT for ChatGPT.",
            "uuid": "e5001.3",
            "source_info": {
                "paper_title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Gemini-1.5",
            "name_full": "Google Gemini-1.5 (gemini-1.5-flash-latest)",
            "brief_description": "Google's Gemini-1.5 family model accessed via API and evaluated on LogicAsker under zero-shot, CoT, and ICL settings; shows mid-to-high performance with benefits from weakness-targeted ICL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5 (gemini-1.5-flash-latest)",
            "model_description": "Google closed-source LLM (model id used: gemini-1.5-flash-latest), accessed via API with temperature=0 and max_tokens=500; exact size not specified in paper.",
            "model_size": null,
            "logical_reasoning_task": "LogicAsker MFT suite (propositional & predicate logic tests).",
            "task_description": "Yes/no tests derived from formal logic rules including equivalences, inference, quantifiers, and fallacies; multiple inference lengths and problem types.",
            "method_or_approach": "Evaluated zero-shot, CoT, Random ICL, and Weakness ICL using LogicAsker-generated demonstrations and weakness dataset.",
            "performance": "Zero-shot: 92.06%; Zero-shot CoT: 93.62%; Random ICL: 96.13%; Weakness ICL: 96.67% (Table 5). Performance declines on weakness-only cases and with increasing inference length.",
            "limitations_or_failure_cases": "Lower accuracy on predicate logic relative to propositional tasks in some cases; difficulty recognizing fallacies (paper states most LLMs weak in fallacy recognition, with exceptions including Gemini and Mixtral); accuracy decreases with longer inference chains.",
            "comparison": "Comparable to GPT-4o in zero-shot/ICL performance but slightly below the absolute best (GPT-4) on ICL-weak metric; better than smaller/open-source models overall in this evaluation.",
            "ablation_or_analysis_results": "Improvement hierarchy observed: Weakness ICL &gt; Random ICL &gt; CoT &gt; Zero-shot; per-rule breakdowns and inference-length analysis show consistent degradation with longer chains.",
            "uuid": "e5001.4",
            "source_info": {
                "paper_title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Meta Llama-3-70B (meta-llama/Meta-Llama-3-70B)",
            "brief_description": "Open-source Llama3 70B model obtained from Hugging Face and evaluated on LogicAsker; shows good but variable performance on formal logical tasks and benefits from targeted ICL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B (Meta-Llama-3-70B)",
            "model_description": "Open-source LLM from Meta (70B parameter family indicated by name, accessed via Hugging Face repo meta-llama/Meta-Llama-3-70B); evaluated with temperature=0 and max_tokens=500.",
            "model_size": "70B",
            "logical_reasoning_task": "LogicAsker MFT suite (propositional and predicate logic tests).",
            "task_description": "Binary logical inference/equivalence/fallacy problems constructed from atomic rules; includes predicate quantifiers and multi-step inference chains.",
            "method_or_approach": "Zero-shot / Zero-shot CoT / Random ICL / Weakness ICL evaluations using LogicAsker-generated demonstrations and weakness dataset.",
            "performance": "Zero-shot: 91.02%; Zero-shot CoT: 94.54%; Random ICL: 94.83%; Weakness ICL: 93.35% (Table 5). Performance drops with longer inference lengths (Table 12) and on some specific predicate/fallacy rules.",
            "limitations_or_failure_cases": "Shows weaker performance on complex predicate-logic rules, certain fallacies, and longer inference chains; variability across rules indicates gaps in atomic skill learning.",
            "comparison": "Performs competitively with Gemini and GPT-4o in some settings but below GPT-4 on average; benefits from high-quality demonstrations though gains are model-dependent (Weakness ICL not uniformly best for Llama3 in Table 5).",
            "ablation_or_analysis_results": "Per-rule breakdowns (Tables 16) show specific low-accuracy rules; inference-length analysis confirms monotonic degradation with length; ICL selection (weakness-targeted) yields variable improvement compared to Random ICL.",
            "uuid": "e5001.5",
            "source_info": {
                "paper_title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mistral Mixtral-8x7B (mistralai/Mixtral-8x7B-v0.1)",
            "brief_description": "Open-source expert-mixture model from Mistral evaluated on LogicAsker; shows lower baseline and mixed improvement from ICL; relatively weaker on some strict logical tasks and CoT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B (mistralai/Mixtral-8x7B-v0.1)",
            "model_description": "Open-source mixture-of-experts LLM provided by Mistral and available on Hugging Face (repo used: mistralai/Mixtral-8x7B-v0.1). Exact aggregate parameter count not specified in paper; model referenced as Mixtral-8x7B.",
            "model_size": null,
            "logical_reasoning_task": "LogicAsker MFT suite (propositional & predicate logic yes/no questions).",
            "task_description": "Atomic and extended logical reasoning tasks including inference, equivalence, quantifiers, and fallacy recognition across varying inference lengths.",
            "method_or_approach": "Zero-shot / Zero-shot CoT / Random ICL / Weakness ICL evaluations using LogicAsker demonstrations; accessed from Hugging Face with temperature=0 and max_tokens=500.",
            "performance": "Zero-shot: 86.77%; Zero-shot CoT: 86.23%; Random ICL: 76.40%; Weakness ICL: 82.02% (Table 5). Random ICL degraded performance for this model in the reported experiments; weakness-targeted ICL recovered some gains but not to top-tier levels.",
            "limitations_or_failure_cases": "Shows lower baseline reasoning accuracy and sensitivity to demonstration selection (Random ICL sometimes decreased performance); struggles especially on predicate/fallacy tasks and long inference chains.",
            "comparison": "Underperforms GPT-4/GPT-4o/Gemini and Llama3 on average in the LogicAsker evaluation; Mixtral is an exception in that it performed relatively better in recognizing some fallacies in earlier analysis but overall weaker.",
            "ablation_or_analysis_results": "ICL ablation shows that Random ICL can harm performance for Mixtral whereas weakness-targeted ICL helps; per-rule breakdowns (Tables 14/15 etc.) reveal specific failure modes; inference-length analysis shows monotonic accuracy decline.",
            "uuid": "e5001.6",
            "source_info": {
                "paper_title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logicinference: A new dataset for teaching logical inference to seq2seq models",
            "rating": 2,
            "sanitized_title": "logicinference_a_new_dataset_for_teaching_logical_inference_to_seq2seq_models"
        },
        {
            "paper_title": "LogiQA: A dataset for evaluating logical reasoning in reading comprehension",
            "rating": 2,
            "sanitized_title": "logiqa_a_dataset_for_evaluating_logical_reasoning_in_reading_comprehension"
        },
        {
            "paper_title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 2,
            "sanitized_title": "language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        },
        {
            "paper_title": "FOLIO: A dataset with first-order logic problems",
            "rating": 2,
            "sanitized_title": "folio_a_dataset_with_firstorder_logic_problems"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Testing the general deductive reasoning capacity of large language models using OOD examples",
            "rating": 1,
            "sanitized_title": "testing_the_general_deductive_reasoning_capacity_of_large_language_models_using_ood_examples"
        },
        {
            "paper_title": "ProntoQA-OOD",
            "rating": 1,
            "sanitized_title": "prontoqaood"
        }
    ],
    "cost": 0.019844499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models
8 Oct 2024</p>
<p>Yuxuan Wan yxwan9@cse.cuhk.edu.hk 
The Chinese University of Hong Kong
Hong KongChina</p>
<p>Wenxuan Wang wxwang@cse.cuhk.edu.hk 
The Chinese University of Hong Kong
Hong KongChina</p>
<p>Yiliu Yang 
The Chinese University of Hong Kong
Hong KongChina</p>
<p>Youliang Yuan youliangyuan@link.cuhk.edu.cn 
The Chinese University of Hong Kong
ShenzhenChina</p>
<p>Jen-Tse Huang jthuang@cse.cuhk.edu.hk 
The Chinese University of Hong Kong
Hong KongChina</p>
<p>Pinjia He hepinjia@cuhk.edu.cn 
The Chinese University of Hong Kong
ShenzhenChina</p>
<p>Wenxiang Jiao wenxiangjiaonju@gmail.com 
Tencent AI Lab
China</p>
<p>Michael R Lyu lyu@cse.cuhk.edu.hk 
The Chinese University of Hong Kong
Hong KongChina</p>
<p>Aida Amini 
Saadia Gabriel 
Shanchuan Lin 
Rik Koncel-Kedziorski 
Yejin Choi 
Mathqa 
Cem Anil 
Yuhuai Wu 
Anders Andreassen 
Aitor Lewkowycz 
Vedant Misra 
Vinay Venkatesh 
Ambrose Slone 
Guy Gur-Ari 
Ethan Dyer 
Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
T J Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeff Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya Sutskever 
Dario 2020 Amodei 
Wenhu Chen 
Xueguang Ma 
Xinyi Wang 
William W Cohen 
Peter Clark 
Isaac Cowhey 
Oren Etzioni 
Tushar Khot 
Ashish Sabharwal 
Carissa Schoenick 
Oyvind 2018 Tafjord 
Kyle 2020 Richardson 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Jacob Hilton 
Reiichiro Nakano 
John 2021 Schulman 
Catherine A Gao 
Frederick M Howard 
Nikolay S Markov 
Emma C Dyer 
Siddhi Ramesh 
Yuan Luo 
Alexander T Pearson 
Shuzheng Gao 
Xinjie Wen 
Cuiyun Gao 
Michael R 2023a Lyu 
Constructing 
Cuiyun Gao 
Michael R 2023b Lyu 
What 
Mor Geva 
Daniel Khashabi 
Elad Segal 
Simeng Han 
Hailey Schoelkopf 
Yilun Zhao 
Zhenting Qi 
Martin Riddell 
Luke Benson 
Lucy Sun 
Eka- Terina Zubova 
Yujie Qiao 
Matthew Burtell 
David Peng 
Jonathan Fan 
Yixin Liu yyiliu@link.cuhk.edu.hk 
Brian Wong 
Mal- Colm Sailor 
Ansong Ni 
Linyong Nan 
Jungo Kasai 
Tao Yu 
Rui Zhang 
Shafiq R Joty 
Alexander R Fab- Bri 
Wojciech Kryscinski 
Victoria Xi 
Caiming Lin 
Dragomir R Xiong 
2022 Radev 
Folio 
LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models
8 Oct 2024F4949178FA93D6A6A1F4E68D4A2F36AAarXiv:2401.00757v3[cs.SE]Towards interpretable math word problem solving with operation-based formalismsArXiv, abs/190513319
We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4.Despite LLMs' prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging.Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes.LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs.Our methodology reveals significant gaps in LLMs' learning of logical rules, with identified reasoning failures ranging from 29% to 90% across different models.Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5%.To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs' formal reasoning capabilities.We make our code, data, and results publicly available 1 to facilitate further research and replication of our findings.</p>
<p>Introduction</p>
<p>Large language models (LLMs), such as OpenAI's GPT series have significantly impacted natural language processing, excelling in a variety of tasks including text generation, machine translation, and code generation (Gao et al., 2022(Gao et al., , 2023a;;Jiao et al., 2023).</p>
<p>Reasoning, defined as the cognitive process of using logic to draw conclusions from given facts (Wei et al., 2022b,a), is crucial for complex interactions that go beyond text generation.Accurately assessing this ability in LLMs is essen-tial, yet challenging, as models may correctly perform tasks merely relying on shortcuts such as pattern recognition without truly engaging in logical reasoning (Huang and Chang, 2022;Huang et al., 2023;Liu et al., 2023a).Consider the following inference example: Either it is raining, or Tom will play football; if it rains, then the floor will be wet; the floor is dry; therefore, Tom will play football.We may encounter the following challenges: 1) It's unclear if a correct LLM response is due to reasoning or simple heuristics like word correlations (e.g., "dry floor" is more likely to correlate with "playing football").2) If an LLM fails, pinpointing the specific breakdown in reasoning is difficult (i.e., inferring not raining from the floor being dry or inferring playing football from not raining).3) Current systems lack comprehensive test cases that encompass various formal reasoning types beyond implication, such as logical equivalence (e.g., A and B are true; therefore, B and A are true.4) Evaluating an LLM's reasoning on such cases offers limited insight into enhancing its reasoning capabilities.</p>
<p>To better handle these challenges, a wellperforming testing framework should be able to define a set of skills that a) directly correspond to the reasoning process, b) cannot be further divided, c) cover all formal logical reasoning scenarios, and d) can identify LLMs' weaknesses and facilitate improving LLMs' performance.Property a) ensures that the task cannot be accomplished by other approaches, such as inferring from the correlations of words, and the evaluation result directly reflects the model's reasoning ability.Property b) and c) ensure that the set of skills is fundamental and comprehensive, which can provide helpful insights to accomplish Property d).</p>
<p>We introduce LogicAsker, an automatic framework designed to evaluate and enhance LLMs' formal reasoning skills using Minimum Functionality Tests (MFTs) (Ribeiro et al., 2020), akin to software engineering's unit tests, which utilize straightforward examples to assess specific behaviors.These tests help identify when models rely on shortcuts rather than genuinely mastering a skill (Ribeiro et al., 2020).Specifically, LogicAsker builds a set of atomic skills from foundational principles of propositional and predicate logic, two fundamental systems used to formalize reasoning procedures (Partee et al., 1990), together with common logical fallacies (Hurley and Watson, 2020).Based on the skill set, LogicAsker generates reasoning questions by translating standard logic expressions into natural language, assesses LLMs' accuracy per skill, pinpoints weaknesses, and creates in-context-learning (Brown et al., 2020) examples and fine-tuning data to bolster reasoning abilities.In addition, for each skill, LogicAsker uses diverse vocabulary to frame various natural language queries, computing average performance to minimize biases from word correlations.</p>
<p>Table 1 demonstrates that LogicAsker complements existing frameworks by providing a comprehensive evaluation scope and utilizing outcomes to enhance LLMs' reasoning capabilities, while other datasets often face data leakage and are scopelimited.LogicAsker serves as an extensive diagnostic tool for LLMs' formal reasoning, significantly exceeding the coverage of comparable tools and enabling detailed assessments across diverse reasoning rules such as inferences, quantifiers, and fallacies.Scaling up the scope presents significant challenges due to the complexity of designing algorithms capable of processing various logical rules and translating them into natural language.Despite these complexities, LogicAsker uniquely integrates all formal logical rules and common fallacies, facilitating robust testing and refinement of reasoning capabilities.</p>
<p>We evaluated LogicAsker's performance through extensive testing on six state-of-the-art (SOTA) LLMs (Hugging Face, 2024), including four closed-source LLMs (GPT-4o, ChatGPT,) and two open-source LLMs (Llama3 and Mixtral).Our findings reveal that LogicAsker's test cases effectively pinpoint logical reasoning failures across these models, with error rates (i.e., 1 − accuracy) between 29% and 90%.These test cases also facilitate the creation of in-context learning examples and fine-tuning data, thereby enhancing logical reasoning capabilities.For instance, applying LogicAsker's cases to GPT-4o improved its reasoning accuracy from 92% to 97%.All resources are released for reproduction and further research2 .</p>
<p>We summarize the main contributions of this work as follows:</p>
<p>• We are the first work that formally defines a comprehensive set of 34 atomic and 208 extended skills necessary for LLMs to execute formal reasoning based on propositional and predicate logic.</p>
<p>• We develop LogicAsker, a fully automatic tool that utilizes atomic skills to generate test cases to assess and enhance LLMs' reasoning abilities, marking a first in utilizing test results to directly improve LLM performance.</p>
<p>• We conduct a thorough empirical evaluation of the logical reasoning abilities of six SOTA LLMs.</p>
<p>We demonstrate that the test results by LogicAsker can be used to effectively evaluate and improve the performance of LLMs.</p>
<p>Preliminaries</p>
<p>2.1 Formal Analysis of Reasoning Abilities "Reasoning" can be characterized into formal reasoning and informal reasoning.The former is a systematic and logical process that follows a set of rules and principles, and the reasoning within these systems will provide valid results as long as one follows the defined rules (e.g., all A are B, all B are C; therefore, all A are C).The latter is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems (Huang and Chang, 2022;Bronkhorst et al., 2020) (e.g., Hong Kong residents have a high life expectancy; this is probably because they have healthy living habits).Generally, formal reasoning is more structured and reliable, whereas informal reasoning is more adaptable and open-ended but may be less reliable.In this paper, we focus on the formal reasoning process to systematically analyze LLMs' reasoning abilities.</p>
<p>To formalize reasoning procedures, two fundamental systems are usually adopted, namely, propositional logic and predicate logic.The former one deals with propositions or statements that can be either true or false, and utilizes logical operators including ∧ (and), ∨ (or), ¬ (not), → (inference), and ↔ (bidirectional) to connect these statements.The latter one, in contrast, extends propositional logic to deal with more complex statements that involve variables, quantifiers, and predicates.Both propositional logic and predicate logic contain various rules for the reasoning process.These rules can be categorized into equivalence rules and inference rules.Equivalent rules summarize the basic expressions that are equivalent in terms of truth value (e.g., ¬(P ∧ Q) ⇔ (¬P ) ∨ (¬Q)).Inference rules summarize the basic valid inference rules (e.g., from the premises: A → B, and A, we can infer B ).</p>
<p>We refer to (Partee et al., 1990) for a more detailed explanation.Table 7-9 in Appendix A list common inference rules in predicate logic and propositional logic.Besides inference rules, formal logic systems can also express common logical fallacies, i.e., arguments that may sound convincing but are based on faulty logic and are, therefore, invalid.We list the common logical fallacies in Table 10.</p>
<p>Minimum Functionality Test</p>
<p>In this paper, we adopted the concept of Minimum Functionality Tests (MFTs), introduced in (Ribeiro et al., 2020), to evaluate the reasoning ability of LLMs.MFTs are analogous to unit tests in software engineering, where a collection of simple examples is used to check a specific behavior within a capability.These tests involve creating small and focused datasets that are particularly effective in detecting whether models resort to shortcuts to handle complex inputs, rather than truly mastering the capability.</p>
<p>To apply MFTs in evaluating the reasoning ability of LLMs, we treated each formal logical rule as an independent task and generated abundant test cases for each task.Each test case was designed to trigger logical failures in the LLMs, allowing us to assess the strengths and weaknesses of LLMs in the logical reasoning process, and providing a solid foundation for further analysis and improvement.</p>
<p>LogicAsker</p>
<p>In this section, we introduce the design and implementation of LogicAsker, a novel tool to trigger logical reasoning failures in large language models.Figure 1 overviews the workflow of LogicAsker, which consists of three main modules: test case generation, weakness identification and in-context learning (ICL) demonstration.In particular, the test case generation module utilizes atomic skills defined on the two formal logic systems and an inference synthesis approach to generate questions as test cases.Then, the generated cases are fed into the LLMs to reveal weaknesses and provide insights into the LLMs by the weakness identification process.Finally, LogicAsker utilizes these insights to construct ICL demonstrations to improve the reasoning abilities of the LLMs.</p>
<p>Reasoning Skills</p>
<p>Atomic skills.As described in Section 2.1, propositional and predicate logic are two fundamental systems that formalize the reasoning process.The inference rules and equivalence laws in these two systems are atomic and can cover all correct reasoning scenarios; therefore, we define these 34 rules as the set of atomic skills an LLM should possess to perform formal reasoning.</p>
<p>Extended skills.Predicate logic extends propositional logic to deal with more complex statements that involve variables, quantifiers, and predicates.
× × × × ✓ × - BERT LogiQA (Liu et al., 2020) × × × × × × - BERT RECLOR (Yu et al., 2020) × × × × ✓ × 2 GPT2 Soft Reasoner (Clark et al., 2020) ✓ × 1 × ✓ × - RoBERTa LogicNLI (Tian et al., 2021) × × 7 × ✓ × - BERT FOLIO (Han et al., 2022) × × × × × × 4 GPT3 LogicInference (Ontañón et al., 2022) ✓ × 19 × × × - T5 ProntoQA-OOD (Saparov et al., 2023) ✓ × 6 × ✓ × 4 GPT3.5 LogicAsker (Ours) ✓ ✓ 34 ✓ ✓ ✓ 6 GPT4 *
We consider language models with more than 1 billion parameters as LLMs.In this regard, besides the unique equivalence and inference laws in predicate logic, we add quantifiers and variables to every rule in propositional logic to form the predicate version of the laws.Using this approach, we expand the set of 34 atomic skills into a set of 208 extended skills.In Appendix B, we provide some concrete examples of these extended rules.</p>
<p>Test Case Generation</p>
<p>To generate logical questions, LogicAsker first adopts a rule-based method to generate logical expressions systematically based on reasoning skills and then translates the logical expressions into natural language.Figure 2 provides an overview of the procedure.</p>
<p>Logic expression generation.To better control the process of logic expression generation, we first define the length of an inference problem by the number of syllogisms it involves.We use the inference rules described in Section 2.1 to generate inference expressions with length one.When a longer inference (&gt; 1) is specified, we start with a base expression E 0 := P 1 ∧ P 2 → C 1 with length one and expand the inference chain.Specifically, we substitute the premises (either or both) of the first inference with the conclusion of some other syllogism and append the premises of those syllogisms into the list of all premises.For example, we can find another syllogism E 1 := P 3 ∧ P 4 → P 2 with P 2 as the conclusion and then obtain a new expression E new := P 1 ∧ P 3 ∧ P 4 → C 1 with the inference length of two.We can obtain inference expressions of any length by recursively expanding the inference chain as above.During the generation process, one can specify the desired rules and length to allow complete control over expected test cases.</p>
<p>In addition to the correct inference expression created above, we generate three kinds of false inference expressions: contradiction, unrelated, and fallacy.A contradiction is generated by negating the conclusion of a correct inference expression and an unrelated is generated by replacing the conclusion of a valid inference expression with an irrelevant statement.For example, for E 0 := P 1 ∧ P 2 → C 1 , a contradiction is E c := P 1 ∧ P 2 → ¬C 1 , an unrelated can be E u := P 1 ∧ P 2 → U 1 .We create a fallacy by directly using the fallacy rules listed in Section 2.1 for an inference length of one.For a fallacy with a more extended length, we select a fallacy rule as the base expression and expand the inference chain using correct rules, ensuring the expression's incorrectness.</p>
<p>Natural language translation.Partially inspired by (Ontañón et al., 2022), translating a clause into natural language involves a series of patterns that depend on the structure of the clause.Simple propositions are transformed into one of the template patterns, such as "subject verb-action", "subject predicate", or "impersonal-action" with a predefined set of subjects, verbs, predicates, and impersonal actions that can be chosen randomly without repetition.For predicate clauses that involve constants or variables, we employ templates "subject verb-action", "subject predicate" to translate them.Furthermore, each clause can be rendered in various modes, such as the present, past, or negated forms.Additionally, connectives like "or," "and," "implies," and "if and only if" also adhere to their designated patterns.For quantified clauses, we adopt patterns like "for all x, X", "there is at least one x for which X", and "some Xs are Y ,".To facilitate the generation process, we curate extensive lists of potential subjects, including common names in English, and compile plausible predicates, actions, and impersonal actions.Compared to Ontanon et al. (Ontañón et al., 2022), our method provides a more natural and less ambiguous translation.We provide a detailed illustration of the translation process in Appendix C.</p>
<p>Weakness Identification</p>
<p>To measure the reasoning abilities of the LLMs, we calculate the accuracy of LLMs' answers Acc = Ncorrect N total .Where N total denotes the total number of responses, and N correct denotes the number of responses that are correct.In particular, since all generated queries are formulated as yes-or-no questions, LogicAsker adopts an automatic approach that searches for pre-defined keywords (e.g., "yes" and "no") in sentences to identify correct answers.</p>
<p>To reveal the weaknesses of LLMs, we generate n test cases for each leaf node in the rule tree depicted in Figure 1.Then, we calculated the response accuracy of an LLM of each leaf node.Based on the result, we can identify the weaknesses of LLMs by listing the leaf nodes that receive the lowest accuracy.In addition, by grouping the accuracy by different attributes in the rule tree, we can gain insights into the strengths and weaknesses of LLMs on these attributes (e.g., performance on predicate logic vs. propositional logic).</p>
<p>Name Rank</p>
<p>GPT-4o (OpenAI, 2024c) 1 GPT-4 (OpenAI, 2024b) 4 ChatGPT (OpenAI, 2024a) 37 Gemini-1.5(Google, 2024) 10 Llama3-70b (Meta Platforms, 2024) 12 Mixtral-8x7b (Mistral AI, 2024) 42</p>
<p>Improving LLMs</p>
<p>In-context learning (ICL) is a paradigm that enables LLMs to learn tasks with examples in the form of demonstrations (Brown et al., 2020).It leverages task instructions and a few demonstration examples to convey the task semantics, which are then combined with query questions to create inputs for the language model to make predictions.ICL has demonstrated impressive performance in various natural language processing and code intelligence.However, the performance of ICL is known to rely on high-quality demonstrations (Gao et al., 2023b) strongly.To fully unleash the potential of ICL, LogicAsker utilizes the weak skills of each LLM to construct both correct and incorrect examples with expected answers and explanations as demonstrations to facilitate the reasoning of LLMs.The generation process follows a similar approach to the test case generation described in § 3.2, with the difference being that we append a brief explanation and the correct answer at the end of each case.We show an instance of the demonstration example in Appendix D. Fine-tuning is another widely used technique to enhance model performance on specific tasks (Moslem et al., 2023;Wei et al., 2021).This process involves taking a pre-trained model and further training it on a smaller, task-specific dataset.The rationale behind fine-tuning is to leverage the learned features and knowledge of the pre-trained model, adapting it to particular nuances and characteristics of a targeted domain or task.In this paper, we directly utilize the data generated by LogicAsker to fine-tune LLMs to improve their reasoning ability.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>We apply LogicAsker to test six state-of-the-art (SOTA) LLMs, including four close-source and two open-source models.Table 2 lists brief information on these systems.All of them are ranked within the top 50 in the LMSYS Chatbot Arena Leaderboard3 according to the assessment results in June 2024.We leave details of how we access the model, the parameters used, and the prompt we used in Appendix E.</p>
<p>We conduct two iterations of experiments for a comprehensive assessment.In the first iteration, we follow the setting in § 3.3 and set n = 25, resulting in 5,200 cases.Statistics of the sampled data are described in Appendix F. The second iteration is based on the first one, which focuses on the identified weaknesses of each LLM, i.e., the ten leaf nodes in Figure 1 with the lowest accuracy.We generated 25 additional test cases for each weakness.These 250 test cases comprise our "weakness dataset, " which will be utilized for further evaluation in § 4.5.</p>
<p>Effectiveness of LogicAsker</p>
<p>We demonstrate the effectiveness of LogicAsker through the overall performance of LLMs on the test cases.The overall performance of LLMs in the first and second iteration is shown in Figure 3.The result reveals that our framework can effectively expose logical failures in the first iteration, with LLM's accuracy ranging from 78%-98%.When focusing on the weak skills of LLMs in the second iteration, we further reduce the accuracy to 10%-71% for the LLMs.What's surprising is that most of these LLMs show accuracy even lower than random guesses (i.e., 50% here) when confronted with logical questions involving specific logical rules.This contradicts their remarkable performance in various LLM benchmarks, for example, achieving top 50 ranks on the LLM Arena Leaderboard.It suggests that existing benchmark datasets are not comprehensive enough to assess the generalization ability of LLMs in reasoning.</p>
<p>Insights into Reasoning Abilities</p>
<p>We conducted a comprehensive analysis to gain insights from the failures exposed by LogicAsker, obtaining three key observations from the evaluation:</p>
<p>Most LLMs are better at easier logical skills.We compared the performance of LLMs on propositional logic and predicate logic, the former of which is simper in form while the latter involves more complex quantifier manipulations.Figure 4 illustrates the difference between the accuracy obtained for the two logic systems.Notably, we observed that most LLMs are better at propositional logic, implying their limited ability in complex reasoning scenarios.</p>
<p>Most LLMs are weak in recognizing logical fallacies.Figure 5 presents the accuracy of LLMs under different skill categories.Interestingly, we discovered that among three types of skills, recognizing fallacies has the lowest accuracy for most LLMs, with GPT-4 and Mixtral-8x7B being the exceptions.It suggests that current LLMs are overconfident even in fallacies, which may be learned from the mistakes in pretraining data.</p>
<p>Case study: GPT-4 did not learn all logic rules well.To provide a direct impression of what skills LLMs cannot perform well, we list three atomic rules in which GPT-4 has the lowest accuracy in Table 3.While GPT-4 has an average accuracy of 98% over all skills, it only achieves 60% -68% accuracy on these skills, indicating that it cannot perform these atomic skills smoothly.</p>
<p>We also discovered that longer inference chains</p>
<p>Existential resolution</p>
<p>Incorrect For all v, v will not play squash or v will go running.There is at least one v for which v will play squash or v will play tennis.Therefore, there is at least one v for which v will go running.</p>
<p>0.60</p>
<p>Universal resolution Correct For all x, x will climb a mountain or x is a police officer.For all x, x will not climb a mountain or x is rich.Therefore, for all x, x is a police officer or x is rich.</p>
<p>The Quality of Test Cases</p>
<p>Since the test cases are automatically generated, we conduct a human evaluation to measure the quality of the generated test cases by LogicAsker.To achieve this, we randomly sampled 10% (520) of the test cases generated during the first iteration of the experiment in 4.2 and conduct manual inspection.Two annotators with bachelor's degrees were recruited to answer the questions manually.Each test case was annotated as either valid or invalid based on the following three questions: a) Is the question grammatically correct?b) Is the question understandable and has only one interpretation?c)</p>
<p>Can the target answer be derived from the question?A test case is considered valid only when both annotator's answer to the above questions are positive.The results of the annotation are presented in Table 4.This result is statistically sufficient to prove that the probability of LogicAsker generating understandable and solvable logical questions is larger than or equal to 0.94 (with p-value 0.05), indicating that the test cases generated by Logi-cAsker are highly reliable and valid.</p>
<p>LogicAsker to Improve Reasoning</p>
<p>In this section, we explore the potential of Logi-cAsker in further improving the reasoning ability of LLMs through in-context learning (ICL) and fine-tuning.We first employ LogicAsker to generate ICL demonstrations tailored to address the weaknesses dataset uncovered in the experiments in § 4.2.For each inference problem, we generated ICL demonstrations that provide both the expected answer and an explanation as described in § 3. We evaluate the effectiveness of the ICL demonstrations generated by LogicAsker by comparing the following prompting strategies: a) Zero-Shot: We provide only task instructions without any ICL demonstrations.b) Zero-Shot Chain-of-Thouhgt (CoT): We use the instruction "Please think step-by-step" (Kojima et al., 2022) to elicit the zero-shot reasoning ability of the LLMs.c) Random ICL Demonstrations: In addition to the task instruction, we also include four ICL demonstrations selected randomly from the available rules with balanced answer labels, i.e., two correct and two incorrect.d) Weakness ICL Demonstration: Instead of random demonstrations, we include four ICL demonstrations using the weakness rules identified in § 4.2 with balanced answer labels.</p>
<p>We perform testing with the 5.2k sampled data on all models and list the result in Table 5.In general, the weakness ICL demonstrations are more effective than those random ICL demonstrations, and both ICL methods bring more performance gain than CoT, indicating that the test cases generated by LogicAsker can improve reasoning.</p>
<p>To further demonstrate the effectiveness of Logi-cAsker, we fine-tune ChatGPT on 5.2k separately generated data on all skills and 2.8k separately generated data on weaknesses of ChatGPT, respectively.We test the two fine-tuned model on both LogicAsker and another dataset, LogiQA, a challenging dataset for machine reading comprehension</p>
<p>Discussion</p>
<p>A surprising result from our experiments is the lower performance of CoT prompting in some LLMs, such as GPT-4, despite the widespread belief in CoT's ability to enhance logical reasoning.One possible explanation lies in the fundamental difference between how CoT operates and the structure of LogicAsker's test cases.CoT functions by breaking down complex reasoning tasks into intermediate, manageable steps, guiding the model toward a final conclusion through a series of natural language reasoning steps.However, LogicAsker tests focus on assessing LLMs' mastery of the most basic, indivisible reasoning skills.These fundamental reasoning skills are the building blocks for more complex reasoning processes like CoT.Since these basic skills cannot be further subdivided into intermediate steps, the benefits of CoT are limited when applied to LogicAsker's simpler test cases.In contrast, improving an LLM's understanding of these core skills can enhance its performance on more complex tasks where CoT is applicable, as demonstrated by ChatGPT's improved results on the LogiQA dataset (see Table 6).</p>
<p>Further analysis of failure cases reveals another intriguing observation.In certain CoT-prompted scenarios, GPT-4 tends to rely on common sense rather than logical deduction.Given that Logi-cAsker's data is synthetic and not aligned with common-sense reasoning, this tendency leads to performance degradation.This is illustrated by the following example, where CoT prompts GPT-4 to incorrectly infer weather conditions based on premises, while a zero-shot approach provides a more accurate deduction by sticking to strict logical interpretation:</p>
<p>Q: Consider the following premises: If Linda were sad, then it is sunny, and if it is sunny, then Linda is sad.Can we infer the following from them?Answer yes or no: It is not overcast.</p>
<p>Ans (Zero-shot): From these premises, we can infer that Linda is sad and it is sunny are equivalent; ... This means that Linda is sad if and only if it is sunny.However, without additional information about whether Linda is actually sad or not, we cannot definitively conclude the state of the weather (overcast or not).Therefore, based on the information given and without knowing whether Linda is sad, the answer is no.</p>
<p>Ans (CoT): ... The statement "It is sunny" directly contradicts the possibility of it being overcast at the same time, assuming a typical understanding of weather where sunny conditions and overcast conditions are mutually exclusive.... Therefore, if either is true, it must be sunny, and thus it cannot be overcast.... Since the premises lead us to conclude that it is sunny, we can infer that it is not overcast.Final answer: yes.</p>
<p>This observation underscores the effectiveness of LogicAsker in revealing situations where models default to heuristics or memorized knowledge rather than true logical reasoning.While CoT has been shown to improve logical reasoning in general tasks, LogicAsker's framework exposes when models fail to genuinely reason and instead fall back on familiar or remembered patterns.This insight suggests that strengthening LLMs' mastery of basic reasoning skills is a necessary foundation for improving performance on tasks that benefit from CoT strategies.</p>
<p>Related Work</p>
<p>Significant advancements in NLP reasoning have been achieved through methods such as Chain-of-Thoughts (CoT) prompting (Wei et al., 2022b), which enables models to generate reasoning steps with minimal training.Enhancing this, the Program of Thoughts (PoT) prompting (Chen et al., 2022) leverages external interpreters like Python to handle complex mathematical problems.Further augmenting reasoning validity, the Logic Agent framework transforms LLMs into logic agents that can dynamically apply propositional logic rules to convert natural language inputs into structured logic forms (Liu et al., 2024).</p>
<p>Recent studies have focused on evaluating the reasoning capabilities of Large Language Models (LLMs) by measuring their performance across various reasoning tasks.These include arithmetic (Cobbe et al., 2021;Hendrycks et al., 2021;Amini et al., 2019;Patel et al., 2021;Miao et al., 2020;Ling et al., 2017;Roy and Roth, 2016), commonsense (Talmor et al., 2019;Geva et al., 2021;Clark et al., 2018), symbolic (Wei et al., 2022b), and table reasoning (Nan et al., 2021), as well as understanding words, dates, and causal relationships (Srivastava et al., 2022), and generalization (Lake and Baroni, 2017;Anil et al., 2022).Despite these efforts, it remains uncertain whether LLMs truly reason or rely on simple heuristics, since most assessments focus only on accuracy and do not thoroughly evaluate the reasoning processes.</p>
<p>Efforts to develop metrics for more formal reasoning analysis in LLMs include creating datasets with first-order logic problems (Han et al., 2022), generating test cases using a single predicate inference rule (Saparov and He, 2022), building instruction-tuning dataset designed for CoT reasoning with GPT-4 (Liu et al., 2023b), and employing propositional logic with randomized methods (Ontañón et al., 2022).These methods, however, often lack generalizability or focus on limited deduction rules.Saparov et al. (Saparov et al., 2023) introduced a comprehensive approach by using all deduction rules in propositional logic to assess LLMs' deductive reasoning across complex proofs.Our research expands further, incorporating all rules and equivalent laws in both propositional and predicate logic, aiming to enhance understanding of each rule's impact on LLM performance and using these insights for improvement.</p>
<p>Conclusion</p>
<p>In this paper, we present LogicAsker, an automated tool designed to comprehensively evaluate and improve the formal reasoning abilities of LLMs under a set of atomic skills.</p>
<p>Our research demonstrated the efficacy of Logi-cAsker in identifying logical reasoning failures in a diverse set of widely deployed LLMs, we achieved a substantial error detection rate in revealing reasoning flaws in these models, ranging from 29% to 90%.Additionally, we utilized the test cases from LogicAsker to design in-context learning demonstrations, which effectively enhance the logical reasoning capabilities of LLMs, e.g., improving from 92% to 97% for GPT-4o.</p>
<p>By providing insights into the strengths and weaknesses of LLMs in reasoning, we are able to improve the reliability and trustworthiness of these models.The release of all the code and data aims to facilitate replication and encourage further research in this crucial area.</p>
<p>Limitations</p>
<p>This paper identifies two primary limitations that highlight areas for future research:</p>
<p>• Although our ICL (In-Context Learning) method significantly enhances the logical reasoning capabilities of large language models (LLMs), there remains a performance gap compared to humanlevel reasoning.Further refinements and innovations in model training and architecture may be necessary to bridge this gap.</p>
<p>• Our method is currently applicable only to LLMs that possess robust in-context learning capabilities.LLMs lacking this feature may not benefit from our approach.Future studies could explore fine-tuning methods to extend the applicability of our improvements across a broader spectrum of LLMs, potentially enhancing models with weaker or no inherent in-context learning abilities.Associative laws
(P ∧ Q) ∧ R ⇔ P ∧ (Q ∧ R)
It is raining and it is cold, and also it is winter ⇔ It is raining, and also, it is cold and it is winter.
(P ∨ Q) ∨ R ⇔ P ∨ (Q ∨ R)
Either I will go to the park or I will go to the library is true, or I will go to the cinema ⇔ I will go to the park or either I will go to the library or I will go to the cinema is true.</p>
<p>Distributive laws
P ∧ (Q ∨ R) ⇔ (P ∧ Q) ∨ (P ∧ R)
It is raining and either I have an umbrella or I have a raincoat ⇔ It is raining and I have an umbrella, or it is raining and I have a raincoat.
P ∨ (Q ∧ R) ⇔ (P ∨ Q) ∧ (P ∨ R
) Either I will go to the park, or it is cloudy and it is cold ⇔ Either I will go to the park or it is cloudy is true, and either I will go to the park or it is cold is true.</p>
<p>DeMorgan's laws
¬(P ∧ Q) ⇔ ¬P ∨ ¬Q
It is not true that it's both cold and raining ⇔ It's not cold or it's not raining.
¬(P ∨ Q) ⇔ ¬P ∧ ¬Q
It's not true that I will study or play ⇔ I won't study and I won't play.</p>
<p>Complement laws ¬(¬P ) ⇔ P It is not the case that it is not raining ⇔ It is raining.</p>
<p>Conditional laws
P → Q ⇔ ¬P ∨ Q If it</p>
<p>A Logical Rules and Fallacies</p>
<p>We list all the logic equivalence rules in Table 7-8, logic inference rules in Table 9, and common logical fallacies in Table 10.</p>
<p>B Extended Rules</p>
<p>B.1 Equivalent Extension</p>
<p>The equivalent rule extension is based on the following fact:
{A ⇔ B, ∀x(A)} ⊢ {∀x(B)}
(i.e., if A and B are equivalent, and for all x, A is true, then for all x, B is also true), and
{A ⇔ B, ∃x(A)} ⊢ {∃x(B)}
(i.e., if A and B are equivalent, and there exist x such that A is true, then there exist x such that B is true).For example, the predicate version of the DeMorgan's law
¬(P ∧ Q) ⇔ ¬P ∨ ¬Q will become ∀x(¬(P (x) ∧ Q(x))) ⇔ ∀x(¬P (x) ∨ ¬Q(x)),and∃x(¬(P (x) ∧ Q(x))) ⇔ ∃x(¬P (x) ∨ ¬Q(x)).
In this example, the goal is to extend the propositional equivalence law to its predicate version by adding quantifiers.To achieve this goal, we first note that DeMorgan's law states that "P and Q cannot both be true" (e.g., Alice is happy and Bob is happy cannot both be true) is equivalent to "either not P or not Q" (e.g., either Alice is not happy or Bob is not happy).Since the two expressions are equivalent, we can add the same quantifier to both sides and the equivalence will still hold.Therefore, by adding a "for all" quantifier to both sides, we obtain "for all x, P(x) and Q(x) cannot both be true" (for all persons in the room, the person likes Charley and the person likes David cannot both be true) is equivalent to "for all x, either not P(x) or not Q(x)" (e.g., for all person in the room, either the person doesn't like Charley or the person doesn't like David).Before the extension, the law can only be applied to simple propositions (e.g., P = "Alice is happy", Q = "Bob is happy"), but after extension, the law can be applied to predicates with variables and quantifiers (e.g., P(x) = "x likes Charley", Q(x) = "x likes David") The same also applies to the It is not the case that all birds can fly ⇔ There exists a bird that cannot fly.</p>
<p>¬∃xP (x) ⇔ ∀x¬P (x)</p>
<p>There is no human that can live forever ⇔ All humans cannot live forever.</p>
<p>Quantifier Distribution ∀x(P (x) ∧ Q(x)) ⇔ ∀xP (x) ∧ ∀xQ(x) Every student is smart and diligent ⇔ Every student is smart, and every student is diligent.
∃x(P (x) ∨ Q(x)) ⇔ ∃xP (x) ∨ ∃xQ(x)
There is a person who is either a doctor or a lawyer ⇔ There is a person who is a doctor, or there is a person who is a lawyer.</p>
<p>Quantifier Movement
∀x(P → Q(x)) ⇔ (P → ∀xQ(x))
For every child, if it is raining then they are inside ⇔ If it is raining, then every child is inside when the notion of raining doesn't depend on the specific child.
∃x(P ∧ Q(x)) ⇔ (P ∧ ∃xQ(x))
There exists a student who is tall and a good basketball player ⇔ There is a tall student and there exists a student who is a good basketball player when the notion of being tall doesn't depend on the specific student.</p>
<p>"exist" quantifier.</p>
<p>B.2 Inference Extension</p>
<p>The inference rule extension is based on the following fact:
{A∧B → C} ⊢ {∀x, (A)∧∀x, (B) → ∀x, (C)},
(i.e., if A and B imply C, then for all x, A is true and for all x, B is true implies for all x, C is true)</p>
<p>{A∧B → C} ⊢ {∃x, (A)∧∀x, (B) → ∃x, (C)}.</p>
<p>(i.e., if A and B imply C, there exists x such that A is true and for all x, B is true implies there exists x such that C is true).Since all propositional inference rules are of the form P ∧ Q → C, we can transform them into their predicate form ∀x, P (x) ∧ ∀x, Q(x) → ∀x, C(x) and ∃x, P (x) ∧ ∀x, Q(x) → ∃x, C(x) following similar procedure in the previous section.</p>
<p>C Natural Language Translation C.1 Algorithm</p>
<p>Given an input: a logic clause of the form [operator, Clause A , Clause B ], where the clauses are also of the form [operator, Clause A , Clause B ], the algorithm will do the following:</p>
<ol>
<li>
<p>Single Proposition Clause: If the clause is just a single proposition, the algorithm finds this proposition's natural language form and returns it.The natural language form is obtained by combining vocabularies according to certain templates (e.g., subject + action).</p>
</li>
<li>
<p>Negation: If the clause starts with a "¬" operator, the algorithm then translates the rest of the clause based on a negation template, making sure to negate the statement.</p>
</li>
<li>
<p>Quantifiers: For clauses that start with "∀" (meaning for all items) or "∃" (meaning there is at least one item), it translates these into natural language, adjusting the phrasing based on whether we're asserting something positively or negating it.</p>
</li>
<li>
<p>Logical Connectives: If the clause combines propositions using logical operators like "∧", "∨", "→" (implies), or "↔" (if and only if), the function translates these into natural language phrases that express the relationship between the propositions.</p>
</li>
</ol>
<p>C.2 Example</p>
<p>Consider the expression:
[∀x, →, A(x), B(x)].
Here's how the function would translate it:</p>
<p>1.It sees the "∀x" quantifier and adds "For all x," to the sentence and continues to process the clause [→, A(x), B(x)].</p>
<ol>
<li>
<p>It sees the "→" operator, which means "if...then...".It connects the two operands with the operator and obtains "For all x, if A(x), then B(x)".Then, it continues to process the clauses A(x), B(x).</p>
</li>
<li>
<p>Since A(x), B(x) are single proposition clauses, the function looks up the vocabulary and synthesizes the natural language versions of the proposition.For example, A(x) = "x drinks water", B(x) = "x is a cashier".</p>
</li>
<li>
<p>It constructs the sentence: "For all x, if x drinks water, then x is a cashier".</p>
</li>
</ol>
<p>C.3 Vocabulary</p>
<p>We list the vocabulary used in our experiment:</p>
<p>Subjects</p>
<p>• x, y, z, James, Mary, Robert, Patricia, John, Jennifer, Michael, Linda, William, Elisabeth, David, Barbara, Richard, Susan, Joseph, Jessica, Thomas, Sarah, Charles, Karen, Alice, Benjamin, Daniel, Emily, George, Helen, Ian, Julie.</p>
<p>Predicates</p>
<p>• a cashier, a janitor, a bartender, a server, an office clerk, a mechanic, a carpenter, an electrician, a nurse, a doctor, a police officer, a taxi driver, a soldier, a politician, a lawyer, a scientist, an astronaut, a poet, an artist, a sailor, a writer, a musician, poor, rich, happy, sad, fast, curious, excited, bored, tired, joyful, intelligent, skilled, efficient, meticulous, creative.</p>
<p>Actions</p>
<p>• make tea, makes tea, making tea, drink water, drinks water, drinking water, read a book, reads a book, reading a book, play tennis, plays tennis, playing tennis, play squash, plays squash, playing squash, play a game, plays a game, playing a game, go running, goes running, running, work, works, working, sleep, sleeps, sleeping, cook, cooks, cooking, listen to a song, listens to a song, listening to a song, write a letter, writes a letter, writing a letter, drive a car, drives a car, driving a car, climb a mountain, climbs a mountain, climbing a mountain, take a plane, takes a plane, taking a plane, paint a picture, paints a picture, painting a picture.</p>
<p>Impersonal Candidates</p>
<p>• snowing, snows, doesn't snow, snow, raining, rains, doesn't rain, rain, sunny, is sunny, is not sunny, be sunny, cloudy, is cloudy, is not cloudy, be cloudy, windy, is windy, is not windy, be windy, cold, is cold, is not cold, be cold, late, is late, is not late, be late, overcast, is overcast, is not overcast, be overcast, foggy, is foggy, is not foggy, be foggy, humid,
(x) → Q(x)), ∀x(P (x) → R(x)) ⊢ ∀x(R(x) → Q(x))
All men are mortal.All men are humans.Therefore, all humans are mortal.Undistributed Middle
∀x(P (x) → Q(x)), Q(a) ⊢ P (a)
All dogs are animals.My cat is an animal.Therefore, my cat is a dog. is humid, is not humid, be humid.</p>
<p>D Prompting LLMs</p>
<p>For all GPT models, we set the system prompt of to blank.</p>
<p>D.1 Zero-Shot Example</p>
<p>Consider the following premises: The claim that John is a poet and the claim that it is cloudy cannot both be true.Can we infer the following from them?Answer yes or no: Jessica is not listening to a song.</p>
<p>D.2 Zero-Shot CoT</p>
<p>For Zero-Shot CoT, we add "Please think step-by-step and answer the following question."To zero-shot queries.A: Let A be the claim that "it is late", B be the claim that "it is windy", then the premises are "A and B".By the simplification rule, we can infer B. Therefore, we can infer that it is windy.</p>
<p>The answer is ==yes==.</p>
<p>Q: Consider the following premises: Jessica is not running.</p>
<p>Jessica is running or it is raining.Can we infer the following from them?Answer yes or no: Sarah is not happy.</p>
<p>A: Let A be the claim that "Jessica is running", B be the claim that "it is raining", C be the claim that "Sarah is happy", then the premises are "not A, A or B".We cannot infer C from the premises.The answer is ==no==.Q: Consider the following premises: It is not raining.It is raining or it is late.Can we infer the following from them?Answer yes or no: It is not late.</p>
<p>A: Let A be the claim that "it is raining", B be the claim that "it is late", then the premises are "not A, A or B".We can infer "B", which is "it is late".Therefore, we cannot infer "it is not late".The answer is ==no==.Q: Consider the following premises: For all x, x will write a letter, and x will climb a mountain and x is a musician.Can we infer the following from them?Answer yes or no: For all x, x will write a letter and x will climb a mountain, and x is a musician.</p>
<p>E Accessing LLMs</p>
<p>For commercial models, the specific models we accessed are gpt-4o-2024-05-13 for GPT-4o, gpt-4-turbo-2024-04-09 for GPT-4, gpt-3.5-turbo-0125for ChatGPT, gemini-1.5-flash-latest(accessed June, 2024) for Gemini-1.5.All accesses are made via their official APIs4 .For the open-source models, we use their respective Hugging Face5 repository, i.e., meta-llama/Meta-Llama-3-70B for Llama-3 and mistralai/Mixtral-8x7B-v0.1 for Mixtral-8x7B.</p>
<p>For model parameters, we set the temperature to 0.0 and max_tokens to 500 for all models.We keep other parameters to the models' respective default values.</p>
<p>F Sampled Data Statistics</p>
<p>We analyzed various statistics of the sampled dataset, classified into different categories as presented in Table 11.The table summarizes the dis-tribution of logic types, categorizes the rule types used in our analysis, and details the types of problems.</p>
<p>G Accuracy Versus Inference Length</p>
<p>To assess the impact of inference length, we generated test cases of varying lengths (i.e., ranging from 1 to 9) using randomly selected rules.For each length, we generated 100 test cases.Table 12 shows the performance of LLMs in these test cases.Generally, LLMs perform gradually worse as the inference length increases, indicating the increased complexity introduced by longer inference chains.</p>
<p>H Complete Break-Down Result for All LLMs</p>
<p>In this section, we list the complete results of all LLMs on the set of 208 atomic rules in Table 13 through Table 18.The results are sorted in ascending order according to the zero-shot performance of the models.</p>
<p>Figure 1 :
1
Figure 1: Overview of the LogicAsker framework.</p>
<p>Figure 2 :
2
Figure 2: Test case generation procedure.</p>
<p>Figure 3 :
3
Figure 3: Overall accuracy.</p>
<p>Figure 4 :
4
Figure 4: Propositional and predicate logic accuracy.</p>
<p>Figure 5 :
5
Figure 5: Accuracy of different rule categories.</p>
<p>Table 1 :
1
Comparison with previous works.
Fully Au-AtomicFormalIncludeIdentifyImproveLLMs  *ExampletomaticSkillsRulesFallaciesWeaknessLLMsTestedTestbedCLUTRR (Sinha et al., 2019)</p>
<p>Table 2 :
2
Conversational LLMs used in the evaluation.</p>
<p>Table 3 :
3
Weakness of
RuleTypeExample</p>
<p>Table 5 :
5
Performance of ICL demonstrations by Logi-cAsker (%).
ModelZero-ShotCoTICLICL(Weak)GPT-497.7596.60 97.9899.48GPT-4o91.9292.94 95.7797.23Gemini92.0693.62 96.1396.67Llama-391.0294.54 94.8393.35Mixtral86.7786.23 76.4082.02ChatGPT77.6278.19 82.9081.04Average89.5290.35 90.6791.63with logical reasoning (Liu et al., 2020). We usethe "test" split of LogiQA which contains 651 testdata. The results are presented in Table 6. We canobserve that models fine-tuned on LogicAsker caneffectively enhance the models' reasoning abilityon both datasets, suggesting the generalizabilityof LogicAsker. These findings demonstrate theeffectiveness of LogicAsker in improving thereasoning ability of LLMs.</p>
<p>Table 6 :
6
ChatGPT performance on LogiQA and Logi-cAsker after fine-tuning (%).
Vanilla FT (All) FT (Weak)LogicAsker77.6299.5097.83LogiQA40.5541.0141.78</p>
<p>Table 7 :
7
Propositional logic equivalence laws.It is cold and it is winter ⇔ It is winter and it is cold.P ∨ Q ⇔ Q ∨ P You can go to the party or you can study ⇔ You can study or you can go to the party.
LawLogical EquivalenceExampleIdempotent laws
P ∧ P ⇔ P I am a teacher and I am a teacher ⇔ I am a teacher.P ∨ P ⇔ P It's raining or it's raining ⇔ it's raining.Commutative laws P ∧ Q ⇔ Q ∧ P</p>
<p>Table 8 :
8
Predicate logic quantifier laws.
LawLogical EquivalenceExampleQuantifier Negation¬∀xP (x) ⇔ ∃x¬P (x)</p>
<p>Table 9 :
9
Propositional and predicate logic inference rules.It is raining and it is cold.Hence, it is raining.{P ∧ Q} ⊢ Q It is raining and it is cold.Hence, it is cold.Conjunction {P, Q} ⊢ P ∧ Q It is raining.It is cold.Hence, it is raining and it is cold.
Inference RuleLogical FormExampleUniversal Instantiation∀xP (x) ⊢ P (c)All humans are mortal. Hence, Socrates is mortal.Existential GeneralizationP (c) ⊢ ∃xP (x)This apple is red. Hence, there exists a red apple.Universal Generalization{P (x)} ⊢ ∀yP (y)Any particular human is mortal. Hence, all humans are mortal.Modus Ponens{P → Q, P } ⊢ QIf it rains, the street gets wet. It is raining. Hence, the street iswet.Modus Tollens{P → Q, ¬Q} ⊢ ¬PIf it rains, the street gets wet. The street is not wet. Hence, itis not raining.{P → ¬Q, Q} ⊢ ¬PIf it rains, the street does not get wet. The street is wet. Hence,it is not raining.Transitivity{P → Q, Q → R} ⊢ P → RIf I study, I will pass the test. If I pass the test, I will get areward. Hence, if I study, I will get a reward.Disjunctive Syllogism{P ∨ Q, ¬P } ⊢ QEither it's raining or it's snowing. It's not raining. Hence, it'ssnowing.Addition{P } ⊢ P ∨ QIt is raining. Hence, it is raining or snowing.Simplification{P ∧ Q} ⊢ P
Resolution {P ∨ Q, ¬P ∨ R} ⊢ Q ∨ REither it is raining or snowing.If it is not raining, then it is cloudy.Hence, either it is snowing or it is cloudy.Disjunction Elimination {P → R, Q → R, P ∨ Q} ⊢ R If it rains, I will stay home.If it snows, I will stay home.Either it will rain or snow.Hence, I will stay home.Biconditional Introduction {P → Q, Q → P } ⊢ P ↔ Q If I study, I pass.If I pass, I studied.Hence, I study if and only if I pass.Biconditional Elimination {P ↔ Q} ⊢ P → Q I study if and only if I pass.Hence, if I study, I pass.{P ↔ Q, ¬P } ⊢ ¬Q I study if and only if I pass.I didn't study.Hence, I didn't pass.{P ↔ Q, ¬Q} ⊢ ¬P I study if and only if I pass.I didn't pass.Hence, I didn't study.</p>
<p>Table 10 :
10
Common fallacies.
NameLogical FormExampleAffirming the Consequent p → q, q ⊢ pIf I study, I will pass the test.I passed the test. Therefore, Istudied.Denying the Antecedentp → q, ¬p ⊢ ¬qIf it rains, the street gets wet.It is not raining. Therefore, thestreet is not wet.Affirming a Disjunctp ∨ q, p ⊢ ¬qEither I will study or I will failthe test. I studied. Therefore, Iwill not fail the test.Denying a Conjunct¬(p ∧ q), ¬p ⊢ qI'm not both hungry and thirsty.I'm not hungry. Therefore, I'mthirsty.Illicit Commutativityp → q ⊢ q → pIf I am in Paris, then I am inFrance. Therefore, if I am inFrance, I am in Paris.Existential Fallacy∀x(P (x) → Q(x)), ¬∃x(P (x)) ⊢ ¬∃x(Q(x))All birds can fly. No birdsare present. Therefore, noth-ing can fly.Illicit Major∀x(P (x) → Q(x)), ∃x(Q(x)) ⊢ ∃x(P (x))All humans are mortal. Some-thing is mortal. Therefore,something is human.Illicit Minor∀x(P</p>
<p>Table 11 :
11
Distribution of logic types, rule categories, and problem types.
Logic TypeCountPredicate146Propositional62Rule Category CountInference108Equivalent81Fallacy19Problem Type CountInference82Unrelated63Contradiction63Total208</p>
<p>Table 12 :
12
LLMs' performance versus inference length.
Len13579GPT40.99 0.88 0.81 0.86 0.76GPT4o0.94 0.81 0.79 0.72 0.63Gemini0.93 0.80 0.73 0.74 0.64Llama30.91 0.78 0.74 0.81 0.67Mixtral0.79 0.67 0.64 0.66 0.62ChatGPT 0.80 0.77 0.71 0.61 0.44</p>
<p>Table 13 :
13
Break-down of the accuracy of GPT-4 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateInferenceUniversal generalizationInference0.520.520.520.52PredicateInferenceExistential resolutionUnrelated0.600.761.001.00PredicateInferenceUniversal resolutionInference0.600.360.081.00PredicateEquivalentLaw of quantifier movementInference0.680.600.600.72PredicateInferenceExistential resolutionInference0.720.520.960.96PredicateInferenceExistential biconditional introductionInference0.760.440.961.00PredicateInferenceExistential biconditional introductionContradiction0.800.801.001.00PredicateEquivalentExistential conditional lawsInference0.800.801.001.00PredicateInferenceExistential transitivityInference0.800.760.961.00PredicateEquivalentUniversal distributive lawsInference0.800.360.200.96PropositionalFallacyDenying a conjunctInference0.880.921.001.00PredicateEquivalentExistential distributive lawsInference0.880.880.960.92PredicateEquivalentLaw of quantifier movementContradiction0.921.001.000.96PredicateInferenceExistential biconditional eliminationInference0.920.921.001.00PredicateInferenceExistential biconditional introductionUnrelated0.920.961.001.00PredicateEquivalentExistential biconditional lawsContradiction0.920.921.001.00PredicateInferenceExistential conjunctionUnrelated0.920.920.920.96PredicateInferenceExistential modus tollensInference0.920.881.001.00PropositionalInferenceResolutionInference0.920.641.001.00PredicateInferenceUniversal disjunction eliminationInference0.920.920.441.00PredicateInferenceUniversal generalizationContradiction0.920.881.001.00PropositionalEquivalentDe morgan's lawsContradiction0.961.001.001.00PredicateEquivalentLaw of quantifier distributionInference0.960.961.001.00PropositionalEquivalentAssociative lawsUnrelated0.960.961.001.00PropositionalEquivalentConditional lawsContradiction0.960.961.001.00PredicateEquivalentExistential associative lawsUnrelated0.960.961.001.00PredicateEquivalentExistential biconditional lawsInference0.960.961.001.00PredicateFallacyExistential denying a conjunctInference0.961.001.001.00PredicateInferenceExistential disjunction eliminationInference0.960.921.001.00PredicateInferenceExistential modus ponensUnrelated0.961.001.001.00PredicateInferenceExistential transitivityContradiction0.960.921.001.00PropositionalEquivalentIdempotent lawsInference0.961.001.001.00PropositionalInferenceModus tollensInference0.961.001.001.00PropositionalInferenceSimplificationUnrelated0.961.001.001.00PredicateInferenceUniversal biconditional eliminationInference0.960.961.001.00PredicateEquivalentUniversal biconditional lawsInference0.960.881.001.00PredicateEquivalentUniversal conditional lawsContradiction0.960.921.001.00PredicateInferenceUniversal conjunctionUnrelated0.960.961.001.00PredicateInferenceUniversal disjunction eliminationUnrelated0.960.960.921.00PredicateInferenceUniversal disjunctive syllogismUnrelated0.960.961.001.00PredicateInferenceUniversal resolutionUnrelated0.961.001.001.00PropositionalEquivalentDe morgan's lawsInference1.001.001.001.00PropositionalEquivalentDe morgan's lawsUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier distributionContradiction1.001.001.001.00PredicateEquivalentLaw of quantifier distributionUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier movementUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier negationContradiction1.001.001.001.00PredicateEquivalentLaw of quantifier negationInference1.001.001.001.00PredicateEquivalentLaw of quantifier negationUnrelated1.001.001.001.00PropositionalInferenceAdditionContradiction1.001.001.001.00PropositionalInferenceAdditionInference1.001.001.001.00PropositionalInferenceAdditionUnrelated1.001.001.001.00PropositionalFallacyAffirming a disjunctInference1.001.001.001.00PropositionalFallacyAffirming the consequentInference1.001.001.001.00PropositionalEquivalentAssociative lawsContradiction1.000.920.961.00PropositionalEquivalentAssociative lawsInference1.001.001.001.00PropositionalInferenceBiconditional eliminationContradiction1.001.001.001.00PropositionalInferenceBiconditional eliminationInference1.001.001.001.00PropositionalInferenceBiconditional eliminationUnrelated1.001.001.001.00PropositionalInferenceBiconditional introductionContradiction1.001.001.001.00PropositionalInferenceBiconditional introductionInference1.001.001.001.00PropositionalInferenceBiconditional introductionUnrelated1.001.001.001.00PropositionalEquivalentBiconditional lawsContradiction1.001.001.001.00PropositionalEquivalentBiconditional lawsInference1.001.000.961.00PropositionalEquivalentBiconditional lawsUnrelated1.000.961.001.00PropositionalEquivalentCommutative lawsContradiction1.000.961.001.00PropositionalEquivalentCommutative lawsInference1.001.001.001.00PropositionalEquivalentCommutative lawsUnrelated1.001.001.001.00PropositionalEquivalentComplement lawsContradiction1.001.001.001.00PropositionalEquivalentComplement lawsInference1.001.001.001.00PropositionalEquivalentComplement lawsUnrelated1.001.001.001.00PropositionalEquivalentConditional lawsInference1.001.001.001.00PropositionalEquivalentConditional lawsUnrelated1.000.961.001.00PropositionalInferenceConjunctionContradiction1.001.001.001.00PropositionalInferenceConjunctionInference1.001.001.001.00PropositionalInferenceConjunctionUnrelated1.001.001.001.00PropositionalFallacyDenying the antecedentInference1.001.001.001.00PropositionalInferenceDisjunction eliminationContradiction1.001.001.001.00PropositionalInferenceDisjunction eliminationInference1.001.001.001.00PropositionalInferenceDisjunction eliminationUnrelated1.001.001.001.00Continued on next page</p>
<p>Table 13 :
13
Break-down of the accuracy of GPT-4 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPropositionalInferenceDisjunctive syllogismContradiction1.001.001.001.00PropositionalInferenceDisjunctive syllogismInference1.001.001.001.00PropositionalInferenceDisjunctive syllogismUnrelated1.001.001.001.00PropositionalEquivalentDistributive lawsContradiction1.001.001.001.00PropositionalEquivalentDistributive lawsInference1.000.961.001.00PropositionalEquivalentDistributive lawsUnrelated1.001.001.001.00PredicateEquivalentExistential de morgan's lawsContradiction1.001.001.001.00PredicateEquivalentExistential de morgan's lawsInference1.001.001.001.00PredicateEquivalentExistential de morgan's lawsUnrelated1.001.001.001.00PredicateInferenceExistential additionContradiction1.001.001.001.00PredicateInferenceExistential additionInference1.001.001.001.00PredicateInferenceExistential additionUnrelated1.001.001.001.00PredicateFallacyExistential affirming a disjunctInference1.001.001.001.00PredicateFallacyExistential affirming the consequentInference1.001.001.001.00PredicateEquivalentExistential associative lawsContradiction1.001.001.001.00PredicateEquivalentExistential associative lawsInference1.001.001.000.96PredicateInferenceExistential biconditional eliminationContradiction1.001.001.001.00PredicateInferenceExistential biconditional eliminationUnrelated1.001.001.001.00PredicateEquivalentExistential biconditional lawsUnrelated1.000.961.001.00PredicateEquivalentExistential commutative lawsContradiction1.001.001.001.00PredicateEquivalentExistential commutative lawsInference1.001.001.001.00PredicateEquivalentExistential commutative lawsUnrelated1.000.841.001.00PredicateEquivalentExistential complement lawsContradiction1.001.001.001.00PredicateEquivalentExistential complement lawsInference1.001.001.001.00PredicateEquivalentExistential complement lawsUnrelated1.001.001.001.00PredicateEquivalentExistential conditional lawsContradiction1.001.001.001.00PredicateEquivalentExistential conditional lawsUnrelated1.001.001.001.00PredicateInferenceExistential conjunctionContradiction1.001.001.001.00PredicateInferenceExistential conjunctionInference1.000.961.001.00PredicateFallacyExistential denying the antecedentInference1.000.961.001.00PredicateInferenceExistential disjunction eliminationContradiction1.001.001.001.00PredicateInferenceExistential disjunction eliminationUnrelated1.001.001.001.00PredicateInferenceExistential disjunctive syllogismContradiction1.001.001.001.00PredicateInferenceExistential disjunctive syllogismInference1.001.001.001.00PredicateInferenceExistential disjunctive syllogismUnrelated1.001.001.001.00PredicateEquivalentExistential distributive lawsContradiction1.001.001.001.00PredicateEquivalentExistential distributive lawsUnrelated1.000.961.001.00PredicateFallacyExistential fallacyInference1.001.000.961.00PredicateInferenceExistential generalizationContradiction1.001.001.001.00PredicateInferenceExistential generalizationInference1.001.001.001.00PredicateInferenceExistential generalizationUnrelated1.001.001.001.00PredicateEquivalentExistential idempotent lawsContradiction1.001.001.001.00PredicateEquivalentExistential idempotent lawsInference1.001.001.001.00PredicateEquivalentExistential idempotent lawsUnrelated1.001.001.001.00PredicateFallacyExistential illicit commutativityInference1.001.001.001.00PredicateInferenceExistential modus ponensContradiction1.001.001.001.00PredicateInferenceExistential modus ponensInference1.000.961.001.00PredicateInferenceExistential modus tollensContradiction1.001.001.001.00PredicateInferenceExistential modus tollensUnrelated1.001.001.001.00PredicateInferenceExistential resolutionContradiction1.001.001.001.00PredicateInferenceExistential simplificationContradiction1.001.001.001.00PredicateInferenceExistential simplificationInference1.001.001.001.00PredicateInferenceExistential simplificationUnrelated1.000.961.001.00PredicateInferenceExistential transitivityUnrelated1.001.001.001.00PropositionalEquivalentIdempotent lawsContradiction1.001.001.001.00PropositionalEquivalentIdempotent lawsUnrelated1.001.001.001.00PropositionalFallacyIllicit commutativityInference1.001.001.001.00PredicateFallacyIllicit majorInference1.001.001.001.00PredicateFallacyIllicit minorInference1.000.961.000.96PropositionalInferenceModus ponensContradiction1.001.001.001.00PropositionalInferenceModus ponensInference1.001.001.001.00PropositionalInferenceModus ponensUnrelated1.001.001.001.00PropositionalInferenceModus tollensContradiction1.001.001.001.00PropositionalInferenceModus tollensUnrelated1.001.001.001.00PropositionalInferenceResolutionContradiction1.001.001.001.00PropositionalInferenceResolutionUnrelated1.001.001.001.00PropositionalInferenceSimplificationContradiction1.001.001.001.00PropositionalInferenceSimplificationInference1.001.001.001.00PropositionalInferenceTransitivityContradiction1.001.001.001.00PropositionalInferenceTransitivityInference1.001.000.961.00PropositionalInferenceTransitivityUnrelated1.001.001.001.00PredicateFallacyUndistributed middleInference1.001.001.001.00PredicateEquivalentUniversal de morgan's lawsContradiction1.001.001.001.00PredicateEquivalentUniversal de morgan's lawsInference1.001.001.001.00PredicateEquivalentUniversal de morgan's lawsUnrelated1.001.001.001.00PredicateInferenceUniversal additionContradiction1.001.001.001.00PredicateInferenceUniversal additionInference1.001.001.001.00PredicateInferenceUniversal additionUnrelated1.001.001.001.00PredicateFallacyUniversal affirming a disjunctInference1.001.001.001.00PredicateFallacyUniversal affirming the consequentInference1.001.001.001.00PredicateEquivalentUniversal associative lawsContradiction1.001.001.001.00Continued on next page</p>
<p>Table 13 :
13
Break-down of the accuracy of GPT-4 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateEquivalentUniversal associative lawsInference1.000.921.001.00PredicateEquivalentUniversal associative lawsUnrelated1.001.001.001.00PredicateInferenceUniversal biconditional eliminationContradiction1.001.001.001.00PredicateInferenceUniversal biconditional eliminationUnrelated1.001.001.001.00PredicateInferenceUniversal biconditional introductionContradiction1.001.001.001.00PredicateInferenceUniversal biconditional introductionInference1.000.801.001.00PredicateInferenceUniversal biconditional introductionUnrelated1.001.001.001.00PredicateEquivalentUniversal biconditional lawsContradiction1.001.001.001.00PredicateEquivalentUniversal biconditional lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal commutative lawsContradiction1.001.001.001.00PredicateEquivalentUniversal commutative lawsInference1.001.001.001.00PredicateEquivalentUniversal commutative lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal complement lawsContradiction1.001.001.001.00PredicateEquivalentUniversal complement lawsInference1.001.001.001.00PredicateEquivalentUniversal complement lawsUnrelated1.000.961.001.00PredicateEquivalentUniversal conditional lawsInference1.001.001.001.00PredicateEquivalentUniversal conditional lawsUnrelated1.001.001.001.00PredicateInferenceUniversal conjunctionContradiction1.001.001.001.00PredicateInferenceUniversal conjunctionInference1.001.000.921.00PredicateFallacyUniversal denying a conjunctInference1.001.001.001.00PredicateFallacyUniversal denying the antecedentInference1.001.001.001.00PredicateInferenceUniversal disjunction eliminationContradiction1.001.000.721.00PredicateInferenceUniversal disjunctive syllogismContradiction1.001.001.001.00PredicateInferenceUniversal disjunctive syllogismInference1.000.881.001.00PredicateEquivalentUniversal distributive lawsContradiction1.001.001.001.00PredicateEquivalentUniversal distributive lawsUnrelated1.001.001.001.00PredicateInferenceUniversal generalizationUnrelated1.001.001.001.00PredicateEquivalentUniversal idempotent lawsContradiction1.001.001.001.00PredicateEquivalentUniversal idempotent lawsInference1.001.001.001.00PredicateEquivalentUniversal idempotent lawsUnrelated1.001.001.001.00PredicateFallacyUniversal illicit commutativityInference1.001.001.001.00PredicateInferenceUniversal instantiationContradiction1.001.001.001.00PredicateInferenceUniversal instantiationInference1.001.001.001.00PredicateInferenceUniversal instantiationUnrelated1.001.001.001.00PredicateInferenceUniversal modus ponensContradiction1.001.001.001.00PredicateInferenceUniversal modus ponensInference1.001.000.881.00PredicateInferenceUniversal modus ponensUnrelated1.001.001.001.00PredicateInferenceUniversal modus tollensContradiction1.001.001.001.00PredicateInferenceUniversal modus tollensInference1.001.000.921.00PredicateInferenceUniversal modus tollensUnrelated1.001.001.001.00PredicateInferenceUniversal resolutionContradiction1.001.001.001.00PredicateInferenceUniversal simplificationContradiction1.001.001.001.00PredicateInferenceUniversal simplificationInference1.001.001.001.00PredicateInferenceUniversal simplificationUnrelated1.001.001.001.00PredicateInferenceUniversal transitivityContradiction1.001.001.001.00PredicateInferenceUniversal transitivityInference1.001.001.001.00PredicateInferenceUniversal transitivityUnrelated1.001.001.001.00</p>
<p>Table 14 :
14
Break-down of the accuracy of GPT-4o on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPropositionalInferenceResolutionInference0.040.641.001.00PredicateInferenceUniversal resolutionInference0.080.000.160.88PredicateInferenceExistential biconditional introductionUnrelated0.200.961.000.56PropositionalInferenceBiconditional introductionUnrelated0.401.001.001.00PropositionalFallacyDenying a conjunctInference0.401.001.001.00PredicateFallacyExistential denying the antecedentInference0.401.000.960.88PredicateInferenceUniversal biconditional introductionUnrelated0.401.001.001.00PredicateInferenceExistential resolutionInference0.520.000.000.56PredicateInferenceUniversal generalizationInference0.520.520.520.52PredicateInferenceUniversal simplificationInference0.560.680.960.96PredicateInferenceUniversal disjunction eliminationInference0.600.320.440.92PredicateEquivalentLaw of quantifier distributionInference0.640.560.600.80PredicateEquivalentLaw of quantifier movementInference0.640.440.720.84PropositionalEquivalentConditional lawsContradiction0.640.960.960.96PredicateEquivalentExistential conditional lawsContradiction0.641.001.001.00PredicateInferenceExistential transitivityInference0.640.000.200.60PredicateInferenceUniversal transitivityInference0.680.681.001.00PropositionalInferenceDisjunctive syllogismContradiction0.721.001.001.00PredicateInferenceExistential disjunctive syllogismContradiction0.721.001.001.00PredicateFallacyExistential fallacyInference0.721.000.961.00PredicateEquivalentUniversal associative lawsInference0.720.960.881.00PredicateInferenceUniversal disjunctive syllogismInference0.720.881.001.00PredicateInferenceExistential biconditional introductionInference0.760.040.160.28PredicateInferenceExistential resolutionUnrelated0.761.000.960.64PredicateFallacyIllicit minorInference0.761.001.001.00PredicateInferenceUniversal biconditional introductionInference0.760.841.001.00PredicateInferenceUniversal disjunctive syllogismContradiction0.761.001.001.00PropositionalFallacyDenying the antecedentInference0.801.001.000.96PredicateInferenceExistential biconditional eliminationUnrelated0.800.961.001.00Continued on next page</p>
<p>Table 14 :
14
Break-down of the accuracy of GPT-4o on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateInferenceExistential disjunction eliminationInference0.800.080.120.76PredicateInferenceUniversal biconditional eliminationInference0.800.880.921.00PredicateInferenceExistential biconditional eliminationContradiction0.841.001.001.00PredicateInferenceExistential conjunctionUnrelated0.840.880.920.92PredicateFallacyExistential denying a conjunctInference0.840.961.001.00PredicateInferenceExistential modus tollensInference0.840.640.801.00PredicateInferenceExistential transitivityContradiction0.841.001.000.92PredicateFallacyUniversal denying the antecedentInference0.841.001.001.00PredicateEquivalentUniversal distributive lawsInference0.840.040.440.60PredicateInferenceUniversal modus tollensInference0.840.640.881.00PredicateEquivalentExistential de morgan's lawsInference0.881.001.001.00PredicateInferenceExistential biconditional introductionContradiction0.881.001.001.00PredicateInferenceExistential disjunctive syllogismInference0.880.961.001.00PredicateInferenceExistential modus tollensUnrelated0.881.001.001.00PredicateInferenceExistential transitivityUnrelated0.881.001.000.88PredicateFallacyUniversal denying a conjunctInference0.881.001.001.00PredicateInferenceUniversal disjunction eliminationUnrelated0.880.800.960.92PredicateInferenceUniversal modus ponensInference0.880.961.001.00PredicateInferenceUniversal modus tollensUnrelated0.880.961.001.00PredicateInferenceUniversal transitivityUnrelated0.880.921.001.00PropositionalEquivalentDe morgan's lawsContradiction0.921.001.001.00PredicateEquivalentLaw of quantifier distributionContradiction0.921.001.000.92PredicateEquivalentLaw of quantifier movementContradiction0.920.840.840.96PropositionalEquivalentAssociative lawsContradiction0.920.921.001.00PropositionalEquivalentBiconditional lawsUnrelated0.920.961.000.96PropositionalInferenceDisjunction eliminationInference0.921.001.001.00PredicateInferenceExistential biconditional eliminationInference0.920.720.561.00PredicateEquivalentExistential commutative lawsUnrelated0.921.001.001.00PredicateInferenceExistential conjunctionInference0.920.961.001.00PropositionalInferenceModus tollensUnrelated0.921.001.000.96PredicateEquivalentUniversal de morgan's lawsInference0.920.961.001.00PredicateInferenceUniversal disjunctive syllogismUnrelated0.920.920.960.96PropositionalEquivalentDe morgan's lawsInference0.960.961.000.96PredicateEquivalentLaw of quantifier negationContradiction0.961.001.001.00PropositionalEquivalentBiconditional lawsInference0.961.001.001.00PropositionalInferenceDisjunction eliminationUnrelated0.961.001.001.00PropositionalInferenceDisjunctive syllogismUnrelated0.961.001.001.00PropositionalEquivalentDistributive lawsInference0.960.681.001.00PredicateEquivalentExistential de morgan's lawsContradiction0.961.001.001.00PredicateFallacyExistential affirming a disjunctInference0.961.001.001.00PredicateFallacyExistential affirming the consequentInference0.961.001.001.00PredicateEquivalentExistential associative lawsUnrelated0.960.961.001.00PredicateEquivalentExistential biconditional lawsContradiction0.961.001.001.00PredicateInferenceExistential disjunctive syllogismUnrelated0.961.001.001.00PredicateEquivalentExistential distributive lawsInference0.960.120.520.80PredicateInferenceExistential generalizationInference0.961.001.000.96PredicateInferenceExistential modus ponensUnrelated0.960.961.000.96PredicateInferenceExistential modus tollensContradiction0.961.001.000.96PredicateInferenceExistential simplificationInference0.961.001.001.00PropositionalInferenceSimplificationUnrelated0.961.001.001.00PropositionalInferenceTransitivityInference0.961.001.001.00PredicateFallacyUniversal affirming a disjunctInference0.961.001.000.96PredicateInferenceUniversal biconditional eliminationUnrelated0.961.001.001.00PredicateEquivalentUniversal biconditional lawsInference0.960.561.001.00PredicateEquivalentUniversal complement lawsUnrelated0.961.001.001.00PredicateInferenceUniversal conjunctionInference0.960.961.001.00PropositionalEquivalentDe morgan's lawsUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier distributionUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier movementUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier negationInference1.000.961.001.00PredicateEquivalentLaw of quantifier negationUnrelated1.001.001.001.00PropositionalInferenceAdditionContradiction1.001.001.001.00PropositionalInferenceAdditionInference1.001.001.001.00PropositionalInferenceAdditionUnrelated1.001.001.001.00PropositionalFallacyAffirming a disjunctInference1.001.001.001.00PropositionalFallacyAffirming the consequentInference1.001.001.001.00PropositionalEquivalentAssociative lawsInference1.001.001.001.00PropositionalEquivalentAssociative lawsUnrelated1.001.001.001.00PropositionalInferenceBiconditional eliminationContradiction1.001.001.000.96PropositionalInferenceBiconditional eliminationInference1.001.001.001.00PropositionalInferenceBiconditional eliminationUnrelated1.001.001.001.00PropositionalInferenceBiconditional introductionContradiction1.001.001.001.00PropositionalInferenceBiconditional introductionInference1.001.001.001.00PropositionalEquivalentBiconditional lawsContradiction1.001.001.001.00PropositionalEquivalentCommutative lawsContradiction1.001.001.001.00PropositionalEquivalentCommutative lawsInference1.001.001.001.00PropositionalEquivalentCommutative lawsUnrelated1.001.001.001.00PropositionalEquivalentComplement lawsContradiction1.001.001.001.00PropositionalEquivalentComplement lawsInference1.001.001.001.00PropositionalEquivalentComplement lawsUnrelated1.000.961.001.00PropositionalEquivalentConditional lawsInference1.001.001.001.00Continued on next page</p>
<p>Table 14 :
14
Break-down of the accuracy of GPT-4o on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPropositionalEquivalentConditional lawsUnrelated1.001.001.001.00PropositionalInferenceConjunctionContradiction1.001.001.001.00PropositionalInferenceConjunctionInference1.001.001.001.00PropositionalInferenceConjunctionUnrelated1.001.001.001.00PropositionalInferenceDisjunction eliminationContradiction1.001.001.001.00PropositionalInferenceDisjunctive syllogismInference1.001.001.001.00PropositionalEquivalentDistributive lawsContradiction1.001.001.001.00PropositionalEquivalentDistributive lawsUnrelated1.001.001.001.00PredicateEquivalentExistential de morgan's lawsUnrelated1.001.001.001.00PredicateInferenceExistential additionContradiction1.001.001.001.00PredicateInferenceExistential additionInference1.000.961.001.00PredicateInferenceExistential additionUnrelated1.001.001.001.00PredicateEquivalentExistential associative lawsContradiction1.001.001.001.00PredicateEquivalentExistential associative lawsInference1.000.881.000.92PredicateEquivalentExistential biconditional lawsInference1.000.401.001.00PredicateEquivalentExistential biconditional lawsUnrelated1.001.001.001.00PredicateEquivalentExistential commutative lawsContradiction1.001.001.001.00PredicateEquivalentExistential commutative lawsInference1.000.921.001.00PredicateEquivalentExistential complement lawsContradiction1.001.001.001.00PredicateEquivalentExistential complement lawsInference1.001.001.001.00PredicateEquivalentExistential complement lawsUnrelated1.001.001.001.00PredicateEquivalentExistential conditional lawsInference1.001.001.001.00PredicateEquivalentExistential conditional lawsUnrelated1.001.001.001.00PredicateInferenceExistential conjunctionContradiction1.001.001.001.00PredicateInferenceExistential disjunction eliminationContradiction1.001.001.001.00PredicateInferenceExistential disjunction eliminationUnrelated1.001.001.001.00PredicateEquivalentExistential distributive lawsContradiction1.001.001.001.00PredicateEquivalentExistential distributive lawsUnrelated1.001.001.001.00PredicateInferenceExistential generalizationContradiction1.001.001.001.00PredicateInferenceExistential generalizationUnrelated1.001.001.001.00PredicateEquivalentExistential idempotent lawsContradiction1.001.001.001.00PredicateEquivalentExistential idempotent lawsInference1.001.001.001.00PredicateEquivalentExistential idempotent lawsUnrelated1.001.001.001.00PredicateFallacyExistential illicit commutativityInference1.001.001.001.00PredicateInferenceExistential modus ponensContradiction1.001.001.001.00PredicateInferenceExistential modus ponensInference1.001.001.001.00PredicateInferenceExistential resolutionContradiction1.001.001.001.00PredicateInferenceExistential simplificationContradiction1.001.001.001.00PredicateInferenceExistential simplificationUnrelated1.001.001.001.00PropositionalEquivalentIdempotent lawsContradiction1.001.001.001.00PropositionalEquivalentIdempotent lawsInference1.001.001.001.00PropositionalEquivalentIdempotent lawsUnrelated1.001.001.001.00PropositionalFallacyIllicit commutativityInference1.001.001.001.00PredicateFallacyIllicit majorInference1.001.001.001.00PropositionalInferenceModus ponensContradiction1.001.001.001.00PropositionalInferenceModus ponensInference1.001.001.001.00PropositionalInferenceModus ponensUnrelated1.001.001.001.00PropositionalInferenceModus tollensContradiction1.001.001.001.00PropositionalInferenceModus tollensInference1.001.001.001.00PropositionalInferenceResolutionContradiction1.001.001.001.00PropositionalInferenceResolutionUnrelated1.000.840.920.88PropositionalInferenceSimplificationContradiction1.001.001.001.00PropositionalInferenceSimplificationInference1.001.001.001.00PropositionalInferenceTransitivityContradiction1.001.001.001.00PropositionalInferenceTransitivityUnrelated1.001.001.001.00PredicateFallacyUndistributed middleInference1.001.001.001.00PredicateEquivalentUniversal de morgan's lawsContradiction1.001.000.961.00PredicateEquivalentUniversal de morgan's lawsUnrelated1.001.001.001.00PredicateInferenceUniversal additionContradiction1.001.001.001.00PredicateInferenceUniversal additionInference1.001.001.001.00PredicateInferenceUniversal additionUnrelated1.001.001.001.00PredicateFallacyUniversal affirming the consequentInference1.001.001.001.00PredicateEquivalentUniversal associative lawsContradiction1.001.001.001.00PredicateEquivalentUniversal associative lawsUnrelated1.000.961.001.00PredicateInferenceUniversal biconditional eliminationContradiction1.001.001.001.00PredicateInferenceUniversal biconditional introductionContradiction1.001.001.001.00PredicateEquivalentUniversal biconditional lawsContradiction1.001.001.001.00PredicateEquivalentUniversal biconditional lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal commutative lawsContradiction1.001.001.001.00PredicateEquivalentUniversal commutative lawsInference1.000.921.001.00PredicateEquivalentUniversal commutative lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal complement lawsContradiction1.001.001.001.00PredicateEquivalentUniversal complement lawsInference1.001.001.001.00PredicateEquivalentUniversal conditional lawsContradiction1.001.001.001.00PredicateEquivalentUniversal conditional lawsInference1.000.921.001.00PredicateEquivalentUniversal conditional lawsUnrelated1.001.001.001.00PredicateInferenceUniversal conjunctionContradiction1.001.001.001.00PredicateInferenceUniversal conjunctionUnrelated1.001.001.001.00PredicateInferenceUniversal disjunction eliminationContradiction1.001.001.001.00PredicateEquivalentUniversal distributive lawsContradiction1.001.001.001.00PredicateEquivalentUniversal distributive lawsUnrelated1.000.961.001.00Continued on next page</p>
<p>Table 14 :
14
Break-down of the accuracy of GPT-4o on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateInferenceUniversal generalizationContradiction1.001.001.001.00PredicateInferenceUniversal generalizationUnrelated1.001.001.001.00PredicateEquivalentUniversal idempotent lawsContradiction1.001.001.001.00PredicateEquivalentUniversal idempotent lawsInference1.001.001.001.00PredicateEquivalentUniversal idempotent lawsUnrelated1.001.001.001.00PredicateFallacyUniversal illicit commutativityInference1.001.001.001.00PredicateInferenceUniversal instantiationContradiction1.001.001.001.00PredicateInferenceUniversal instantiationInference1.001.001.001.00PredicateInferenceUniversal instantiationUnrelated1.001.001.001.00PredicateInferenceUniversal modus ponensContradiction1.001.001.001.00PredicateInferenceUniversal modus ponensUnrelated1.001.001.001.00PredicateInferenceUniversal modus tollensContradiction1.001.001.001.00PredicateInferenceUniversal resolutionContradiction1.001.001.001.00PredicateInferenceUniversal resolutionUnrelated1.001.000.920.76PredicateInferenceUniversal simplificationContradiction1.001.001.001.00PredicateInferenceUniversal simplificationUnrelated1.001.001.001.00PredicateInferenceUniversal transitivityContradiction1.001.001.001.00</p>
<p>Table 15 :
15
Break-down of the accuracy of Gemini-1.5 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateFallacyExistential denying the antecedentInference0.000.600.480.84PredicateInferenceExistential biconditional introductionUnrelated0.080.280.120.32PredicateInferenceExistential biconditional introductionContradiction0.160.520.960.88PredicateInferenceExistential resolutionUnrelated0.200.400.440.80PropositionalFallacyDenying the antecedentInference0.320.800.920.72PredicateFallacyExistential denying a conjunctInference0.320.400.640.88PredicateInferenceUniversal instantiationInference0.321.000.640.64PredicateInferenceUniversal disjunctive syllogismContradiction0.441.001.001.00PredicateInferenceExistential transitivityUnrelated0.480.960.840.68PredicateEquivalentExistential conditional lawsContradiction0.520.880.840.72PredicateFallacyUniversal denying the antecedentInference0.520.880.800.76PredicateInferenceUniversal generalizationInference0.520.520.520.52PredicateEquivalentExistential conditional lawsInference0.560.360.840.88PredicateInferenceUniversal resolutionContradiction0.560.680.800.88PredicateInferenceUniversal biconditional introductionContradiction0.641.001.001.00PredicateFallacyExistential affirming a disjunctInference0.680.640.440.72PredicateEquivalentUniversal conditional lawsContradiction0.680.720.880.92PropositionalEquivalentConditional lawsContradiction0.720.640.880.88PredicateEquivalentExistential biconditional lawsContradiction0.720.960.880.88PredicateEquivalentLaw of quantifier movementInference0.760.480.560.84PredicateEquivalentLaw of quantifier negationContradiction0.760.960.800.88PropositionalInferenceDisjunctive syllogismContradiction0.761.001.001.00PredicateEquivalentExistential associative lawsUnrelated0.761.001.001.00PredicateInferenceExistential conjunctionUnrelated0.760.880.960.92PredicateFallacyExistential fallacyInference0.760.840.800.76PredicateInferenceUniversal transitivityContradiction0.761.000.961.00PredicateEquivalentLaw of quantifier movementContradiction0.800.760.880.96PropositionalInferenceBiconditional introductionContradiction0.800.961.000.92PredicateInferenceExistential biconditional eliminationUnrelated0.801.001.001.00PropositionalInferenceResolutionUnrelated0.800.681.001.00PredicateEquivalentUniversal de morgan's lawsInference0.800.640.881.00PredicateInferenceUniversal generalizationContradiction0.800.961.001.00PropositionalEquivalentConditional lawsInference0.840.641.001.00PredicateInferenceExistential modus tollensUnrelated0.841.000.961.00PredicateEquivalentUniversal biconditional lawsContradiction0.841.000.960.96PredicateEquivalentUniversal idempotent lawsInference0.840.761.001.00PredicateInferenceUniversal simplificationInference0.840.601.001.00PredicateEquivalentExistential biconditional lawsUnrelated0.881.001.001.00PredicateEquivalentExistential distributive lawsUnrelated0.880.961.001.00PredicateEquivalentExistential idempotent lawsUnrelated0.881.001.001.00PropositionalInferenceTransitivityContradiction0.881.001.001.00PredicateFallacyUniversal affirming a disjunctInference0.881.001.001.00PredicateInferenceUniversal disjunction eliminationInference0.881.001.000.96PredicateEquivalentLaw of quantifier distributionContradiction0.921.001.000.96PropositionalEquivalentBiconditional lawsContradiction0.920.960.960.92PropositionalInferenceDisjunctive syllogismInference0.921.001.001.00PredicateInferenceExistential biconditional eliminationInference0.920.960.960.88PredicateEquivalentExistential commutative lawsUnrelated0.920.961.001.00PredicateInferenceUniversal biconditional eliminationInference0.920.841.001.00PredicateInferenceUniversal conjunctionInference0.920.961.001.00PredicateInferenceUniversal disjunctive syllogismInference0.921.001.001.00PredicateInferenceUniversal modus ponensInference0.920.961.001.00PredicateInferenceUniversal modus tollensInference0.920.801.001.00PredicateEquivalentLaw of quantifier distributionInference0.960.960.920.96PropositionalEquivalentAssociative lawsUnrelated0.961.001.001.00PropositionalEquivalentCommutative lawsUnrelated0.961.001.001.00PropositionalInferenceDisjunction eliminationContradiction0.961.001.000.96PredicateEquivalentExistential de morgan's lawsUnrelated0.961.001.001.00PredicateInferenceExistential additionUnrelated0.961.001.001.00Continued on next page</p>
<p>Table 15 :
15
Break-down of the accuracy of Gemini-1.5 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateInferenceExistential biconditional introductionInference0.960.840.960.92PredicateInferenceExistential disjunctive syllogismUnrelated0.961.001.001.00PredicateInferenceExistential modus tollensInference0.960.920.920.96PredicateInferenceExistential simplificationUnrelated0.961.001.001.00PredicateFallacyIllicit minorInference0.961.001.000.96PropositionalInferenceResolutionContradiction0.960.961.001.00PropositionalInferenceResolutionInference0.960.961.001.00PropositionalInferenceSimplificationUnrelated0.960.961.001.00PredicateInferenceUniversal biconditional eliminationUnrelated0.961.001.001.00PredicateInferenceUniversal biconditional introductionInference0.961.001.001.00PredicateFallacyUniversal denying a conjunctInference0.960.960.960.92PredicateInferenceUniversal disjunctive syllogismUnrelated0.960.960.961.00PredicateInferenceUniversal simplificationContradiction0.961.001.001.00PropositionalEquivalentDe morgan's lawsContradiction1.000.800.760.72PropositionalEquivalentDe morgan's lawsInference1.000.921.001.00PropositionalEquivalentDe morgan's lawsUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier distributionUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier movementUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier negationInference1.000.881.001.00PredicateEquivalentLaw of quantifier negationUnrelated1.000.961.001.00PropositionalInferenceAdditionContradiction1.001.001.001.00PropositionalInferenceAdditionInference1.001.001.001.00PropositionalInferenceAdditionUnrelated1.000.961.001.00PropositionalFallacyAffirming a disjunctInference1.001.001.000.92PropositionalFallacyAffirming the consequentInference1.001.001.001.00PropositionalEquivalentAssociative lawsContradiction1.001.001.001.00PropositionalEquivalentAssociative lawsInference1.001.001.001.00PropositionalInferenceBiconditional eliminationContradiction1.000.961.001.00PropositionalInferenceBiconditional eliminationInference1.001.001.001.00PropositionalInferenceBiconditional eliminationUnrelated1.001.001.001.00PropositionalInferenceBiconditional introductionInference1.000.921.001.00PropositionalInferenceBiconditional introductionUnrelated1.001.000.960.88PropositionalEquivalentBiconditional lawsInference1.001.001.001.00PropositionalEquivalentBiconditional lawsUnrelated1.001.001.001.00PropositionalEquivalentCommutative lawsContradiction1.001.001.001.00PropositionalEquivalentCommutative lawsInference1.001.001.001.00PropositionalEquivalentComplement lawsContradiction1.000.961.001.00PropositionalEquivalentComplement lawsInference1.001.001.001.00PropositionalEquivalentComplement lawsUnrelated1.001.001.001.00PropositionalEquivalentConditional lawsUnrelated1.001.001.001.00PropositionalInferenceConjunctionContradiction1.001.001.001.00PropositionalInferenceConjunctionInference1.001.001.001.00PropositionalInferenceConjunctionUnrelated1.000.961.001.00PropositionalFallacyDenying a conjunctInference1.000.800.960.96PropositionalInferenceDisjunction eliminationInference1.001.001.001.00PropositionalInferenceDisjunction eliminationUnrelated1.001.001.001.00PropositionalInferenceDisjunctive syllogismUnrelated1.001.001.001.00PropositionalEquivalentDistributive lawsContradiction1.000.961.000.96PropositionalEquivalentDistributive lawsInference1.000.921.001.00PropositionalEquivalentDistributive lawsUnrelated1.001.001.001.00PredicateEquivalentExistential de morgan's lawsContradiction1.001.000.960.92PredicateEquivalentExistential de morgan's lawsInference1.001.001.001.00PredicateInferenceExistential additionContradiction1.001.001.001.00PredicateInferenceExistential additionInference1.000.761.001.00PredicateFallacyExistential affirming the consequentInference1.001.000.960.88PredicateEquivalentExistential associative lawsContradiction1.001.001.001.00PredicateEquivalentExistential associative lawsInference1.001.001.001.00PredicateInferenceExistential biconditional eliminationContradiction1.001.001.000.96PredicateEquivalentExistential biconditional lawsInference1.000.881.001.00PredicateEquivalentExistential commutative lawsContradiction1.001.001.001.00PredicateEquivalentExistential commutative lawsInference1.000.961.001.00PredicateEquivalentExistential complement lawsContradiction1.001.001.001.00PredicateEquivalentExistential complement lawsInference1.001.001.001.00PredicateEquivalentExistential complement lawsUnrelated1.001.001.001.00PredicateEquivalentExistential conditional lawsUnrelated1.001.001.001.00PredicateInferenceExistential conjunctionContradiction1.000.961.001.00PredicateInferenceExistential conjunctionInference1.000.960.920.96PredicateInferenceExistential disjunction eliminationContradiction1.001.001.001.00PredicateInferenceExistential disjunction eliminationInference1.001.001.001.00PredicateInferenceExistential disjunction eliminationUnrelated1.001.001.001.00PredicateInferenceExistential disjunctive syllogismContradiction1.001.000.960.96PredicateInferenceExistential disjunctive syllogismInference1.001.001.001.00PredicateEquivalentExistential distributive lawsContradiction1.001.001.001.00PredicateEquivalentExistential distributive lawsInference1.000.801.000.96PredicateInferenceExistential generalizationContradiction1.001.001.001.00PredicateInferenceExistential generalizationInference1.000.841.001.00PredicateInferenceExistential generalizationUnrelated1.000.961.001.00PredicateEquivalentExistential idempotent lawsContradiction1.001.001.001.00PredicateEquivalentExistential idempotent lawsInference1.000.921.001.00PredicateFallacyExistential illicit commutativityInference1.001.001.001.00PredicateInferenceExistential modus ponensContradiction1.001.001.001.00Continued on next page</p>
<p>Table 15 :
15
Break-down of the accuracy of Gemini-1.5 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateInferenceExistential modus ponensInference1.000.960.961.00PredicateInferenceExistential modus ponensUnrelated1.001.001.000.96PredicateInferenceExistential modus tollensContradiction1.001.001.001.00PredicateInferenceExistential resolutionContradiction1.001.001.001.00PredicateInferenceExistential resolutionInference1.001.001.001.00PredicateInferenceExistential simplificationContradiction1.001.001.001.00PredicateInferenceExistential simplificationInference1.001.001.001.00PredicateInferenceExistential transitivityContradiction1.000.760.840.92PredicateInferenceExistential transitivityInference1.000.920.960.96PropositionalEquivalentIdempotent lawsContradiction1.001.001.001.00PropositionalEquivalentIdempotent lawsInference1.000.881.001.00PropositionalEquivalentIdempotent lawsUnrelated1.001.001.001.00PropositionalFallacyIllicit commutativityInference1.001.001.001.00PredicateFallacyIllicit majorInference1.001.000.960.92PropositionalInferenceModus ponensContradiction1.001.001.001.00PropositionalInferenceModus ponensInference1.001.001.001.00PropositionalInferenceModus ponensUnrelated1.001.001.001.00PropositionalInferenceModus tollensContradiction1.001.001.001.00PropositionalInferenceModus tollensInference1.000.961.001.00PropositionalInferenceModus tollensUnrelated1.001.001.001.00PropositionalInferenceSimplificationContradiction1.001.001.001.00PropositionalInferenceSimplificationInference1.001.001.001.00PropositionalInferenceTransitivityInference1.001.001.001.00PropositionalInferenceTransitivityUnrelated1.001.000.961.00PredicateFallacyUndistributed middleInference1.001.001.001.00PredicateEquivalentUniversal de morgan's lawsContradiction1.001.000.800.84PredicateEquivalentUniversal de morgan's lawsUnrelated1.001.001.001.00PredicateInferenceUniversal additionContradiction1.001.001.001.00PredicateInferenceUniversal additionInference1.000.961.001.00PredicateInferenceUniversal additionUnrelated1.001.001.001.00PredicateFallacyUniversal affirming the consequentInference1.001.001.001.00PredicateEquivalentUniversal associative lawsContradiction1.001.001.001.00PredicateEquivalentUniversal associative lawsInference1.000.841.000.96PredicateEquivalentUniversal associative lawsUnrelated1.001.001.001.00PredicateInferenceUniversal biconditional eliminationContradiction1.001.000.920.96PredicateInferenceUniversal biconditional introductionUnrelated1.000.961.001.00PredicateEquivalentUniversal biconditional lawsInference1.000.961.001.00PredicateEquivalentUniversal biconditional lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal commutative lawsContradiction1.001.001.001.00PredicateEquivalentUniversal commutative lawsInference1.000.921.001.00PredicateEquivalentUniversal commutative lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal complement lawsContradiction1.001.001.001.00PredicateEquivalentUniversal complement lawsInference1.001.001.001.00PredicateEquivalentUniversal complement lawsUnrelated1.001.001.001.00PredicateEquivalentUniversal conditional lawsInference1.000.841.001.00PredicateEquivalentUniversal conditional lawsUnrelated1.001.001.001.00PredicateInferenceUniversal conjunctionContradiction1.001.001.001.00PredicateInferenceUniversal conjunctionUnrelated1.001.001.001.00PredicateInferenceUniversal disjunction eliminationContradiction1.001.000.920.96PredicateInferenceUniversal disjunction eliminationUnrelated1.001.001.001.00PredicateEquivalentUniversal distributive lawsContradiction1.000.880.921.00PredicateEquivalentUniversal distributive lawsInference1.000.681.001.00PredicateEquivalentUniversal distributive lawsUnrelated1.001.001.001.00PredicateInferenceUniversal generalizationUnrelated1.001.001.001.00PredicateEquivalentUniversal idempotent lawsContradiction1.001.001.001.00PredicateEquivalentUniversal idempotent lawsUnrelated1.001.001.001.00PredicateFallacyUniversal illicit commutativityInference1.001.001.001.00PredicateInferenceUniversal instantiationContradiction1.001.001.001.00PredicateInferenceUniversal instantiationUnrelated1.001.001.001.00PredicateInferenceUniversal modus ponensContradiction1.001.001.001.00PredicateInferenceUniversal modus ponensUnrelated1.001.001.001.00PredicateInferenceUniversal modus tollensContradiction1.001.000.920.96PredicateInferenceUniversal modus tollensUnrelated1.001.001.001.00PredicateInferenceUniversal resolutionInference1.000.881.001.00PredicateInferenceUniversal resolutionUnrelated1.000.961.001.00PredicateInferenceUniversal simplificationUnrelated1.001.001.001.00PredicateInferenceUniversal transitivityInference1.001.001.001.00PredicateInferenceUniversal transitivityUnrelated1.001.001.001.00</p>
<p>Table 16 :
16
Break-down of the accuracy of Llama3 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateFallacyExistential denying the antecedentInference0.000.720.440.40PredicateInferenceExistential resolutionUnrelated0.040.280.160.12PredicateInferenceExistential biconditional introductionUnrelated0.080.640.200.04PredicateFallacyUniversal denying the antecedentInference0.120.760.760.52PredicateFallacyExistential denying a conjunctInference0.160.160.600.48PredicateFallacyUniversal denying a conjunctInference0.160.760.520.12PropositionalFallacyDenying the antecedentInference0.280.880.520.16PredicateFallacyExistential fallacyInference0.280.800.280.28Continued on next page</p>
<p>Table 16 :
16
Break-down of the accuracy of Llama3 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPredicateInferenceExistential transitivityUnrelated0.360.760.760.88PropositionalInferenceTransitivityContradiction0.401.000.961.00PredicateEquivalentExistential distributive lawsInference0.440.600.640.76PredicateEquivalentExistential conditional lawsContradiction0.520.920.680.76PredicateInferenceUniversal generalizationInference0.520.400.520.44PropositionalEquivalentConditional lawsContradiction0.560.760.921.00PredicateEquivalentExistential biconditional lawsContradiction0.600.921.001.00PredicateEquivalentUniversal conditional lawsContradiction0.600.761.001.00PredicateInferenceUniversal transitivityContradiction0.601.001.001.00PropositionalFallacyDenying a conjunctInference0.640.840.600.60PredicateEquivalentExistential conditional lawsInference0.640.480.961.00PropositionalEquivalentDe morgan's lawsContradiction0.680.881.000.52PredicateFallacyIllicit majorInference0.681.000.840.60PredicateEquivalentLaw of quantifier movementInference0.720.480.561.00PredicateInferenceUniversal instantiationInference0.720.760.921.00PredicateInferenceExistential transitivityContradiction0.760.721.000.96PredicateInferenceUniversal disjunctive syllogismContradiction0.761.001.001.00PredicateEquivalentLaw of quantifier movementContradiction0.800.840.960.88PredicateEquivalentLaw of quantifier negationContradiction0.801.000.760.96PropositionalInferenceDisjunctive syllogismInference0.800.961.001.00PredicateInferenceExistential conjunctionUnrelated0.800.960.960.96PredicateInferenceUniversal resolutionUnrelated0.800.960.800.56PropositionalEquivalentConditional lawsInference0.840.961.001.00PredicateEquivalentExistential associative lawsUnrelated0.840.961.001.00PredicateInferenceExistential biconditional eliminationContradiction0.841.000.920.96PredicateEquivalentExistential distributive lawsUnrelated0.841.001.000.96PredicateFallacyIllicit minorInference0.840.920.480.20PropositionalInferenceResolutionUnrelated0.840.720.760.72PredicateEquivalentUniversal distributive lawsInference0.840.640.640.92PredicateEquivalentExistential de morgan's lawsContradiction0.881.001.000.64PredicateFallacyExistential affirming the consequentInference0.881.000.800.48PredicateInferenceExistential biconditional introductionInference0.880.920.961.00PredicateEquivalentExistential commutative lawsUnrelated0.881.001.001.00PredicateInferenceExistential disjunctive syllogismContradiction0.881.001.001.00PredicateInferenceExistential generalizationInference0.880.880.960.92PredicateFallacyUniversal affirming a disjunctInference0.881.000.960.92PredicateInferenceUniversal generalizationContradiction0.880.961.001.00PredicateEquivalentLaw of quantifier distributionContradiction0.921.000.961.00PropositionalEquivalentAssociative lawsContradiction0.920.880.920.92PropositionalEquivalentAssociative lawsUnrelated0.921.001.001.00PredicateFallacyExistential affirming a disjunctInference0.920.840.840.68PredicateInferenceExistential biconditional eliminationUnrelated0.920.961.000.96PredicateInferenceExistential resolutionContradiction0.920.760.960.80PredicateInferenceUniversal biconditional introductionContradiction0.921.001.001.00PredicateInferenceUniversal biconditional introductionInference0.920.960.881.00PredicateEquivalentUniversal biconditional lawsContradiction0.921.001.001.00PredicateEquivalentUniversal biconditional lawsInference0.920.880.921.00PredicateEquivalentUniversal conditional lawsInference0.920.801.001.00PredicateEquivalentUniversal distributive lawsUnrelated0.921.001.000.96PredicateInferenceUniversal resolutionContradiction0.920.720.720.60PredicateInferenceUniversal transitivityUnrelated0.921.001.001.00PropositionalEquivalentDe morgan's lawsInference0.961.001.001.00PredicateEquivalentLaw of quantifier negationUnrelated0.961.001.000.96PropositionalEquivalentBiconditional lawsUnrelated0.961.001.000.96PropositionalEquivalentCommutative lawsUnrelated0.961.001.001.00PropositionalEquivalentConditional lawsUnrelated0.960.961.001.00PropositionalEquivalentDistributive lawsUnrelated0.961.001.000.96PredicateEquivalentExistential de morgan's lawsUnrelated0.960.960.961.00PredicateEquivalentExistential biconditional lawsInference0.960.961.001.00PredicateEquivalentExistential conditional lawsUnrelated0.961.001.001.00PredicateInferenceExistential disjunctive syllogismUnrelated0.961.001.001.00PredicateEquivalentExistential distributive lawsContradiction0.961.001.001.00PredicateInferenceExistential modus ponensUnrelated0.960.961.001.00PredicateInferenceExistential modus tollensUnrelated0.961.000.960.92PropositionalInferenceModus ponensInference0.961.001.001.00PropositionalInferenceSimplificationUnrelated0.960.961.001.00PropositionalInferenceTransitivityInference0.961.001.001.00PredicateEquivalentUniversal de morgan's lawsContradiction0.960.841.001.00PredicateInferenceUniversal disjunction eliminationUnrelated0.960.960.960.96PredicateInferenceUniversal disjunctive syllogismUnrelated0.960.960.960.96PredicateInferenceUniversal modus ponensInference0.960.961.001.00PropositionalEquivalentDe morgan's lawsUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier distributionInference1.000.920.960.88PredicateEquivalentLaw of quantifier distributionUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier movementUnrelated1.001.001.001.00PredicateEquivalentLaw of quantifier negationInference1.001.001.001.00PropositionalInferenceAdditionContradiction1.001.001.001.00PropositionalInferenceAdditionInference1.001.001.001.00PropositionalInferenceAdditionUnrelated1.001.001.001.00PropositionalFallacyAffirming a disjunctInference1.001.001.001.00PropositionalFallacyAffirming the consequentInference1.001.001.001.00Continued on next page</p>
<p>Table 16 :
16
Break-down of the accuracy of Llama3 on all rules (sorted by zero-shot accuracy).
LogicRule categoryRuleProblemZero shotZero shot cotRandom iclWeakPropositionalEquivalentAssociative lawsInference1.001.001.001.00PropositionalInferenceBiconditional eliminationContradiction1.001.001.001.00PropositionalInferenceBiconditional eliminationInference1.001.001.001.00PropositionalInferenceBiconditional eliminationUnrelated1.001.001.001.00PropositionalInferenceBiconditional introductionContradiction1.001.001.001.00PropositionalInferenceBiconditional introductionInference1.001.001.001.00PropositionalInferenceBiconditional introductionUnrelated1.001.001.000.92PropositionalEquivalentBiconditional lawsContradiction1.001.001.001.00PropositionalEquivalentBiconditional lawsInference1.001.001.001.00PropositionalEquivalentCommutative lawsContradiction1.001.001.001.00PropositionalEquivalentCommutative lawsInference1.001.001.001.00PropositionalEquivalentComplement lawsContradiction1.001.001.001.00PropositionalEquivalentComplement lawsInference1.001.001.001.00PropositionalEquivalentComplement lawsUnrelated1.001.001.001.00PropositionalInferenceConjunctionContradiction1.001.000.920.92PropositionalInferenceConjunctionInference1.001.001.001.00PropositionalInferenceConjunctionUnrelated1.001.001.001.00PropositionalInferenceDisjunction eliminationContradiction1.001.001.001.00PropositionalInferenceDisjunction eliminationInference1.001.001.001.00PropositionalInferenceDisjunction eliminationUnrelated1.001.001.001.00PropositionalInferenceDisjunctive syllogismContradiction1.001.000.960.92PropositionalInferenceDisjunctive syllogismUnrelated1.001.001.001.00PropositionalEquivalentDistributive lawsContradiction1.001.000.920.96PropositionalEquivalentDistributive lawsInference1.000.921.001.00PredicateEquivalentExistential de morgan's lawsInference1.001.001.001.00PredicateInferenceExistential additionContradiction1.001.001.001.00PredicateInferenceExistential additionInference1.001.001.001.00PredicateInferenceExistential additionUnrelated1.001.001.001.00PredicateEquivalentExistential associative lawsContradiction1.001.001.001.00PredicateEquivalentExistential associative lawsInference1.000.960.961.00PredicateInferenceExistential biconditional eliminationInference1.001.001.001.00PredicateInferenceExistential biconditional introductionContradiction1.000.761.001.00PredicateEquivalentExistential biconditional lawsUnrelated1.001.001.001.00PredicateEquivalentExistential commutative lawsContradiction1.001.001.001.00PredicateEquivalentExistential commutative lawsInference1.001.001.001.00PredicateEquivalentExistential complement lawsContradiction1.001.001.001.00PredicateEquivalentExistential complement lawsInference1.001.001.001.00PredicateEquivalentExistential complement lawsUnrelated1.001.001.001.00PredicateInferenceExistential conjunctionContradiction1.001.001.001.00PredicateInferenceExistential conjunctionInference1.000.960.920.92PredicateInferenceExistential disjunction eliminationContradiction1.001.001.001.00PredicateInferenceExistential disjunction eliminationInference1.001.001.001.00PredicateInferenceExistential disjunction eliminationUnrelated1.001.001.001.00PredicateInferenceExistential disjunctive syllogismInference1.001.001.001.00PredicateInferenceExistential generalizationContradiction1.001.001.001.00PredicateInferenceExistential generalizationUnrelated1.001.001.001.00PredicateEquivalentExistential idempotent lawsContradiction1.001.001.001.00PredicateEquivalentExistential idempotent lawsInference1.001.001.001.00PredicateEquivalentExistential idempotent lawsUnrelated1.001.001.001.00PredicateFallacyExistential illicit commutativityInference1.001.001.000.96PredicateInferenceExistential modus ponensContradiction1.001.001.001.00PredicateInferenceExistential modus ponensInference1.000.961.001.00PredicateInferenceExistential modus tollensContradiction1.000.961.000.92PredicateInferenceExistential modus tollensInference1.001.000.921.00PredicateInferenceExistential resolutionInference1.000.961.001.00PredicateInferenceExistential simplificationContradiction1.001.001.001.00PredicateInferenceExistential simplificationInference1.001.001.001.00PredicateInferenceExistential simplificationUnrelated1.001.001.001.00PredicateInferenceExistential transitivityInference1.000.961.001.00PropositionalEquivalentIdempotent lawsContradiction1.001.001.001.00PropositionalEquivalentIdempotent lawsInference1.001.001.001.00PropositionalEquivalentIdempotent lawsUnrelated1.001.001.001.00PropositionalFallacyIllicit commutativityInference1.001.001.000.92PropositionalInferenceModus ponensContradiction1.001.001.001.00PropositionalInferenceModus ponensUnrelated1.000.961.001.00PropositionalInferenceModus tollensContradiction1.001.000.921.00PropositionalInferenceModus tollensInference1.001.001.001.00PropositionalInferenceModus tollensUnrelated1.001.001.001.00PropositionalInferenceResolutionContradiction1.001.001.000.96PropositionalInferenceResolutionInference1.000.841.001.00PropositionalInferenceSimplificationContradiction1.001.001.001.00PropositionalInferenceSimplificationInference1.001.001.001.00PropositionalInferenceTransitivityUnrelated1.001.001.001.00PredicateFallacyUndistributed middleInference1.001.001.000.96PredicateEquivalentUniversal de morgan's lawsInference1.001.001.000.92PredicateEquivalentUniversal de morgan's lawsUnrelated1.001.001.001.00PredicateInferenceUniversal additionContradiction1.001.001.001.00PredicateInferenceUniversal additionInference1.001.001.001.00PredicateInferenceUniversal additionUnrelated1.001.001.001.00PredicateFallacyUniversal affirming the consequentInference1.001.001.000.80PredicateEquivalentUniversal associative lawsContradiction1.001.001.001.00Continued on next page</p>
<p>Table 16 :
16
Break-down of the accuracy of Llama3 on all rules (sorted by zero-shot accuracy).</p>
<p>https://github.com/yxwan123/LogicAsker
https://huggingface.co/spaces/lmsys/ chatbot-arena-leaderboard
https://platform.openai.com/docs/ and https:// ai.google.dev/gemini-api
https://huggingface.co/
AcknowledgmentsThe paper is supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14206921 of the General Research Fund).Transformers as soft reasoners over language.In International Joint Conference on Artificial Intelligence.Chatbot arena leaderboard.https://huggingface.co/spaces/lmsys/ chatbot-arena-leaderboard. Accessed: 2024-06-16.
Meta Platforms. 2024. Llama-3. </p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, ArXiv, abs/2106.157722020</p>
<p>A I Mistral, Mixtral of experts. 2024</p>
<p>Fine-tuning large language models for adaptive machine translation. Yasmin Moslem, Rejwanul Haque, Andy Way, ArXiv, abs/2312.127402023</p>
<p>Caiming Xiong, and Dragomir R. Radev. 2021. Fetaqa: Freeform table question answering. Linyong Nan, Chia-Hsuan Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Transactions of the Association for Computational Linguistics10Benjamin Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham</p>
<p>Logicinference: A new dataset for teaching logical inference to seq2seq models. Santiago Ontañón, Joshua Ainslie, Vaclav Cvicek, Zachary Kenneth Fisher, ArXiv, abs/2203.150992022</p>
<p>OpenAI. 2024b. Gpt-4. </p>
<p>Hello gpt-4o. 2024cOpenAI</p>
<p>Barbara H Partee, Alice Ter Meulen, Robert E Wall, Mathematical methods in linguistics. 1990</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, S Bhattamishra, Navin Goyal, North American Chapter. the Association for Computational Linguistics. 2021</p>
<p>Beyond accuracy: Behavioral testing of nlp models with checklist. Marco Tulio Ribeiro, Tongshuang Sherry Wu, Carlos Guestrin, Sameer Singh, ACL. 2020</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, ArXiv, abs/1608.014132016</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, ArXiv, abs/2210.012402022</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, ArXiv, abs/2305.152692023</p>
<p>Clutrr: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, Conference on Empirical Methods in Natural Language Processing. 2019</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, ArXiv, abs/2206.046152022</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, ArXiv, abs/1811.009372019</p>
<p>Diagnosing the firstorder logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Conference on Empirical Methods in Natural Language Processing. 2021</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, V Quoc, Le, ArXiv, abs/2109.016522021</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, ArXiv, abs/2206.07682Emergent abilities of large language models. Donald Metzler, Ed Huai Hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, 2022a</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, F Xia, Quoc Le, Denny Zhou, 2022bNeurIPS</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, ArXiv, abs/2002.043262020</p>            </div>
        </div>

    </div>
</body>
</html>